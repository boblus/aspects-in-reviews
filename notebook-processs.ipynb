{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5b05c-6958-4683-9947-6d9234285022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a92bf-f8e8-4d83-908c-61ff66f1915f",
   "metadata": {},
   "source": [
    "## preprocess\n",
    "\n",
    "This is to segment the review into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41d88a-3d28-4f68-8e1b-7630cfac4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for venue in ['emnlp23', 'iclr20', 'iclr21', 'iclr22', 'iclr23', 'iclr24']:\n",
    "    \n",
    "    with open(f'data/{venue}.json') as file:\n",
    "        data = json.loads(file.read())\n",
    "    with open(f'selected_papers/{venue}.txt') as file:\n",
    "        selected_papers = file.readlines()\n",
    "\n",
    "    selected_paper_ids = []\n",
    "    for title in selected_papers:\n",
    "        for paper_id in data:\n",
    "            if data[paper_id]['Title'] == title.rstrip('\\n'):\n",
    "                selected_paper_ids.append(paper_id)\n",
    "                break\n",
    "\n",
    "    output = defaultdict()\n",
    "    for paper_id in data:\n",
    "        output[paper_id] = defaultdict(dict)\n",
    "        for reviewer_id in data[paper_id]['Reviews']:\n",
    "            if venue not in ['emnlp23', 'iclr24']:\n",
    "                review = data[paper_id]['Reviews'][reviewer_id]['Reasons']\n",
    "            else:\n",
    "                review = data[paper_id]['Reviews'][reviewer_id]['Reasons_to_accept'] + '\\n\\n' + data[paper_id]['Reviews'][reviewer_id]['Reasons_to_reject']\n",
    "            sentences = sent_tokenize(review, language='english')\n",
    "            for sentence in sentences:\n",
    "                if len(sentence) > 2:\n",
    "                    output[paper_id][reviewer_id][str(len(output[paper_id][reviewer_id]))] = sentence\n",
    "\n",
    "    with open(f'preprocessed/preprocessed-{venue}.json', 'w') as file:\n",
    "        json.dump(output, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b0476-2a28-4318-afbe-1c98df06b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for venue in ['nlpeer']:\n",
    "    \n",
    "    with open(f'data/{venue}.json') as file:\n",
    "        data = json.loads(file.read())\n",
    "    with open(f'selected_papers/{venue}.txt') as file:\n",
    "        selected_papers = file.readlines()\n",
    "\n",
    "    selected_paper_ids = []\n",
    "    for title in selected_papers:\n",
    "        for paper_id in data:\n",
    "            if data[paper_id]['Title'] == title.rstrip('\\n'):\n",
    "                selected_paper_ids.append(paper_id)\n",
    "                break\n",
    "\n",
    "    output = defaultdict()\n",
    "    for paper_id in selected_paper_ids:\n",
    "        output[paper_id] = defaultdict(dict)\n",
    "        for reviewer_id in data[paper_id]['Reviews']:\n",
    "            review = data[paper_id]['Reviews'][reviewer_id]\n",
    "            sentences = sent_tokenize(review, language='english')\n",
    "            for sentence in sentences:\n",
    "                output[paper_id][reviewer_id][str(len(output[paper_id][reviewer_id]))] = sentence\n",
    "\n",
    "    with open(f'preprocessed/preprocessed-{venue}.json', 'w') as file:\n",
    "        json.dump(output, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13287a04-eef5-4f10-8005-fa12b12a9559",
   "metadata": {},
   "source": [
    "## postprocess\n",
    "\n",
    "This is to postprocess the LLM annotations to find the most frequent aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774cba5-b242-426b-8aaf-af7cc2c14b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('synonyms.json') as file:\n",
    "    synonyms = {k.lower(): v for k, v in json.loads(file.read()).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bec7fe-9182-44ee-bece-e49fbb9f511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_1, counter = [], Counter()\n",
    "annotation = pd.read_csv('annotation - llm.csv')\n",
    "for i in range(len(annotation)):\n",
    "    item = merge_synonyms(str(annotation['annotation_1'].tolist()[i]).replace(' and ', ', ').split(', '))\n",
    "    annotation_1.extend(item)\n",
    "    counter.update(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd688bd-d351-4eac-80c3-26f8cb790d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
