{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057c638-787e-434d-9f59-5ff5bde68d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c9f66-794c-491b-9b4d-235f5f40ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects_rename = {\n",
    "    'Ablation': 'Ablation',\n",
    "    'Analysis': 'Analysis',\n",
    "    'Comparison': 'Compar.',\n",
    "    'Contribution': 'Contribution',\n",
    "    'Data/Task': 'Data/Task',\n",
    "    'Definition/Description/Detail/Discussion/Explanation/Interpretation': 'DDDDEI',\n",
    "    'Evaluation': 'Eval.',\n",
    "    'Experiment': 'Experi.',\n",
    "    'Intuition/Justification/Motivation/Validation': 'IJMV',\n",
    "    'Methodology': 'Method.',\n",
    "    'Novelty': 'Novelty',\n",
    "    'Presentation': 'Present.',\n",
    "    'Related Work': 'Related Work',\n",
    "    'Result': 'Result',\n",
    "    'Significance': 'Significance',\n",
    "    'Theory': 'Theory'\n",
    "}\n",
    "\n",
    "tracks_rename = {\n",
    "    'Question Answering': 'Question Answering',\n",
    "    'Theme Track: Large Language Models and the Future of NLP': 'Theme Track: LLMs\\n&the Future of NLP',\n",
    "    'Information Extraction': 'Information Extraction',\n",
    "    'Resources and Evaluation': 'Resources & Evaluation',\n",
    "    'Dialogue and Interactive Systems': 'Dialogue & Interactive\\nSystems',\n",
    "    'Machine Translation': 'Machine Translation',\n",
    "    'Multilinguality and Linguistic Diversity': 'Multilinguality\\n& Linguistic Diversity',\n",
    "    'Linguistic Theories, Cognitive Modeling, and Psycholinguistics': 'Linguistic Theories,\\nCognitive Modeling,\\n& Psycholinguistics',\n",
    "    'NLP Applications': 'NLP Applications',\n",
    "    'Sentiment Analysis, Stylistic Analysis, and Argument Mining': 'Sentiment Analysis,\\nStylistic Analysis,\\n& Argument Mining',\n",
    "    'Language Modeling and Analysis of Language Models': 'Language Modeling\\n& Analysis of\\nLanguage Models',\n",
    "    'Computational Social Science and Cultural Analytics': 'Computational Social Science\\n& Cultural Analytics',\n",
    "    'Syntax, Parsing and their Applications': 'Syntax, Parsing\\n& their Applications',\n",
    "    'Interpretability, Interactivity, and Analysis of Models for NLP': 'Interpretability, Interactivity,\\n& Analysis of Models for NLP',\n",
    "    'Summarization': 'Summarization',\n",
    "    'Speech and Multimodality': 'Speech & Multimodality',\n",
    "    'Discourse and Pragmatics': 'Discourse & Pragmatics',\n",
    "    'Natural Language Generation': 'Natural Language Generation',\n",
    "    'Machine Learning for NLP': 'ML for NLP',\n",
    "    'Human-Centered NLP': 'Human-Centered NLP',\n",
    "    'Ethics in NLP': 'Ethics in NLP',\n",
    "    'Phonology, Morphology, and Word Segmentation': 'Phonology, Morphology,\\n& Word Segmentation',\n",
    "    'Efficient Methods for NLP': 'Efficient Methods for NLP',\n",
    "    'Semantics: Lexical, Sentence level, Document Level, Textual Inference, etc.': 'Semantics: Lexical,\\nSentence level,\\nDocument Level,\\nTextual Inference, etc.',\n",
    "    'Information Retrieval and Text Mining': 'Information Retrieval\\n& Text Mining',\n",
    "    'Commonsense Reasoning': 'Commonsense Reasoning',\n",
    "    'Language Grounding to Vision, Robotics and Beyond': 'Language Grounding to Vision,\\nRobotics & Beyond'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b294147-2612-4a5e-845c-bab97bdd207d",
   "metadata": {},
   "source": [
    "## validity check\n",
    "\n",
    "This corresponds to **Section 3.4 Validity check** in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78cad4d-9c75-49ab-aa93-9f3e95b65880",
   "metadata": {},
   "source": [
    "### llm annotation consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d7d28-e3d7-4ebc-83f9-9f4b028442bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = pd.read_csv('annotation - llm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d3783-9234-407b-ac45-c8100f9d058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_i_t0 = [str(_) for _ in annotation['annotation_1'].tolist()]\n",
    "annotation_i_t1 = [str(_) for _ in annotation['annotation_2'].tolist()]\n",
    "annotation_i2_t0 = [str(_) for _ in annotation['annotation_3'].tolist()]\n",
    "annotation_i2_t1 = [str(_) for _ in annotation['annotation_4'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5c002-1512-4d36-ac82-b0b49846de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = annotation_i_t0\n",
    "cand = annotation_i_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39efab-ae6c-43f5-9aef-a95b01827fe8",
   "metadata": {},
   "source": [
    "#### exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5604425-fd7e-4919-96a6-82ed3d98021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = 0\n",
    "for i in range(len(ref)):\n",
    "    if sorted(ref[i].lower().split(', ')) == sorted(cand[i].lower().split(', ')):\n",
    "        em += 1\n",
    "em/len(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1b74b-b030-4044-a62f-3b78db32e5e6",
   "metadata": {},
   "source": [
    "#### bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c41fd9-602f-4f76-ab48-5094e16f4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/bert_score_similarity_llm_annotation_1&2.json') as file:\n",
    "    results = json.loads(file.read())\n",
    "f1 = [_['f1_score'] for _ in results]\n",
    "count = Counter()\n",
    "for _ in f1:\n",
    "    if _ >= 0.99:\n",
    "        count['>=0.99'] += 1\n",
    "    if 0.98 <= _ < 0.99:\n",
    "        count['[0.98,0.99)'] += 1\n",
    "    if 0.97 <= _ < 0.98:\n",
    "        count['[0.97,0.98)'] += 1\n",
    "    if 0.96 <= _ < 0.97:\n",
    "        count['[0.96,0.97)'] += 1\n",
    "    if 0.95 <= _ < 0.96:\n",
    "        count['[0.95,0.96)'] += 1\n",
    "    if 0.90 <= _ < 0.95:\n",
    "        count['[0.90,0.95)'] += 1\n",
    "    if _ < 0.90:\n",
    "        count['<0.90'] += 1\n",
    "1 - (count['<0.90'] / sum(count.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41638a6-6402-4354-90c6-201c5d3e8846",
   "metadata": {},
   "source": [
    "#### aspect consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a4aac-b930-44c8-a0fe-619fc2fe96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_aspect(annotation):\n",
    "    output = []\n",
    "    for i in range(len(annotation)):\n",
    "        items = merge_synonyms(annotation[i].replace(' and ', ', ').split(', '))\n",
    "        entries = set()\n",
    "        for aspect in items:\n",
    "            if aspect in aspect_to_category:\n",
    "                entries.update(aspect_to_category[aspect])\n",
    "            else:\n",
    "                entries.add('-')\n",
    "        if len(entries) != 1 and '-' in entries:\n",
    "            entries.remove('-')\n",
    "        output.append(entries)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec62838-76f1-4fba-b1e8-b9992c468f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_aspect = pd.read_csv('aspects - coarse.csv')\n",
    "aspect_to_category = defaultdict(set)\n",
    "for i in range(len(category_to_aspect)):\n",
    "    aspect_to_category[category_to_aspect['LLM annotation'].to_list()[i]].add(category_to_aspect['COARSE'].to_list()[i])\n",
    "\n",
    "ref_aspects, cand_aspects = match_aspect(ref), match_aspect(cand)\n",
    "em = 0\n",
    "for i in range(len(ref)):\n",
    "    if ref_aspects[i] == cand_aspects[i]:\n",
    "        em += 1\n",
    "print('exact match:', em/len(annotation))\n",
    "\n",
    "scores = []\n",
    "for i in range(len(ref)):\n",
    "    scores.append(jaccard_similarity(ref_aspects[i], cand_aspects[i]))\n",
    "print('jaccard:', sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ba5c3-3c0e-4f44-877f-8fb2ec703c51",
   "metadata": {},
   "source": [
    "### human annotation consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcfd18-ba84-464b-b4d5-987cfd351b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_annotations = defaultdict()\n",
    "for filename in os.listdir('human annotations/'):\n",
    "    if 'annotation' in filename:\n",
    "        human_annotations[filename] = pd.read_csv(os.path.join('human annotations/', filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e4839-b866-489e-9c4c-6308df4de768",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, df in human_annotations.items():\n",
    "    review = df['review'].to_list()\n",
    "    question = [_.replace('Does the review address ', '').replace('?', '') for _ in df['question'].to_list()]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ee731-c112-4b3c-b971-d151d7134c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_yes = defaultdict()\n",
    "for filename, df in human_annotations.items():\n",
    "    percentage_yes[filename] = df['yes'].sum() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d242035-1939-4658-99aa-44917c016576",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c82d4-860d-465d-a31c-7266cf3364b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(percentage_yes.values()) / len(percentage_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f08c6-f7b6-466e-84f8-a2159b3e7ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = defaultdict(Counter)\n",
    "for filename, df in human_annotations.items():\n",
    "    yes = df['yes'].to_list()\n",
    "    for i in range(len(df)):\n",
    "        if yes[i] == True:\n",
    "            matrix['yes'][i] += 1\n",
    "            matrix['no'][i] += 0\n",
    "        else:\n",
    "            matrix['yes'][i] += 0\n",
    "            matrix['no'][i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e6c33-2676-4e92-ae50-88913bf2a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15163a58-4164-47af-bab2-34fc30647e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "x_ticks = [_ for _ in range(n, len(matrix), n)]\n",
    "fleiss = [fleiss_kappa(matrix[:_].values, method='fleiss') for _ in x_ticks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55dc97-d3b2-4f64-8ca3-b4f990c506e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 12\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "sns.lineplot(data=fleiss, marker='o', ax=ax)\n",
    "ax.set_xticks([_ for _ in range(len(x_ticks))])\n",
    "ax.set_xticklabels([str(_) if _ % 400 != 0 else '' for _ in x_ticks])\n",
    "ax.set_ylim(0, 0.8)\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8])\n",
    "\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.fill_between([xmin, xmax], 0.00, 0.20, color='lightblue', edgecolor='lightblue', alpha=0.2)\n",
    "ax.fill_between([xmin, xmax], 0.20, 0.40, color='lightblue', edgecolor='lightblue', alpha=0.4)\n",
    "ax.fill_between([xmin, xmax], 0.40, 0.60, color='lightblue', edgecolor='lightblue', alpha=0.6)\n",
    "ax.fill_between([xmin, xmax], 0.60, 0.80, color='lightblue', edgecolor='lightblue', alpha=0.8)\n",
    "\n",
    "ax.text(x=14, y=0.1, s='slight', ha='right', va='center', color='black', fontstyle='italic')\n",
    "ax.text(x=14, y=0.3, s='fair', ha='right', va='center', color='black', fontstyle='italic')\n",
    "ax.text(x=14, y=0.5, s='moderate', ha='right', va='center', color='black', fontstyle='italic')\n",
    "ax.text(x=14, y=0.7, s='substantial', ha='right', va='center', color='black', fontstyle='italic')\n",
    "\n",
    "ax.set_xlabel('entry')\n",
    "ax.set_ylabel('fleiss\\' kappa')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/fleiss_kappa.png', format='png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4565986-4939-4fdd-99dd-e84fb362c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "fleiss_kappa(matrix.values, method='fleiss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c55b8-25f4-44bb-b5c5-352aaded018b",
   "metadata": {},
   "source": [
    "## aspect analysis\n",
    "\n",
    "This corresponds to **Section 5.1 Aspect analysis** in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de50e1-d5f4-460f-b4aa-1a0a2c3e2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pd.read_csv('config_inference.txt', sep='\\t')\n",
    "run_id = config['run_id'][(config['source'] == 'emnlp23_all') & (config['type_of_labels'] == 'coarse')].to_list()[0]\n",
    "\n",
    "with open('data/emnlp23.json') as file:\n",
    "    data = json.loads(file.read())\n",
    "\n",
    "for paper_id in data:\n",
    "    if data[paper_id]['Submission_Track'] == 'Semantics: Lexical':\n",
    "        data[paper_id]['Submission_Track'] = 'Semantics: Lexical, Sentence level, Document Level, Textual Inference, etc.'\n",
    "\n",
    "with open(f'results/inference-{run_id}.json') as file:\n",
    "    results = json.loads(file.read())\n",
    "\n",
    "number_of_reviews_by_track, number_of_reviews_by_score, counter_aspects_by_track, counter_aspects_by_score = Counter(), Counter(), defaultdict(Counter), defaultdict(Counter)\n",
    "for paper_id in results:\n",
    "    aspects = set()\n",
    "    for reviewer_id, items in results[paper_id].items():\n",
    "        number_of_reviews_by_track[data[paper_id]['Submission_Track']] += 1\n",
    "        number_of_reviews_by_score[data[paper_id]['Reviews'][reviewer_id]['Excitement'].split(': ')[0]] += 1\n",
    "        for _, item in items.items():\n",
    "            aspects.update(item)\n",
    "        if '-' in aspects:\n",
    "            aspects.remove('-')\n",
    "        counter_aspects_by_track[data[paper_id]['Submission_Track']].update(aspects)\n",
    "        counter_aspects_by_score[data[paper_id]['Reviews'][reviewer_id]['Excitement'].split(': ')[0]].update(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5db84-2a19-48f0-bec7-9af4e14e03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tracks = ['Machine Translation', 'Multilinguality and Linguistic Diversity', 'Question Answering', 'Resources and Evaluation']\n",
    "\n",
    "data = []\n",
    "for track in target_tracks:\n",
    "    for aspect, count in counter_aspects_by_track[track].most_common(5):\n",
    "        percentage = count / number_of_reviews_by_track[track] * 100\n",
    "        data.append({'track': track, 'aspect': aspects_rename[aspect], 'percentage': percentage})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(5.6, 5.8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "for ax, track in zip(axes, target_tracks):\n",
    "    subset = df[df['track'] == track]\n",
    "    sns.barplot(\n",
    "        data=subset,\n",
    "        x='aspect',\n",
    "        y='percentage',\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(tracks_rename[track], fontsize=12.6, fontweight='bold')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_yticks([0, 20, 40, 60, 80, 100])\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "fig.supylabel('review (%)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/frequency-target_tracks.png', format='png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdd78e2-576e-40ca-ab1d-bead358dc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tracks = counter_aspects_by_track.keys()\n",
    "\n",
    "data = []\n",
    "for track in target_tracks:\n",
    "    for aspect, count in counter_aspects_by_track[track].most_common(5):\n",
    "        percentage = count / number_of_reviews_by_track[track] * 100\n",
    "        data.append({'track': track, 'aspect': aspects_rename[aspect], 'percentage': percentage})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "fig, axes = plt.subplots(6, 5, figsize=(15, 22))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "for ax, track in zip(axes, target_tracks):\n",
    "    subset = df[df['track'] == track]\n",
    "    sns.barplot(\n",
    "        data=subset,\n",
    "        x='aspect',\n",
    "        y='percentage',\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(tracks_rename[track], fontsize=12.6, fontweight='bold')\n",
    "    ax.set_xlabel('')\n",
    "    if track in ['Question Answering', 'Machine Translation', 'Language Modeling and Analysis of Language Models', 'Speech and Multimodality', 'Ethics in NLP', 'Commonsense Reasoning']:\n",
    "        ax.set_ylabel('review (%)')\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "    ax.set_yticks([0, 20, 40, 60, 80, 100])\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[27].set_visible(False)\n",
    "axes[28].set_visible(False)\n",
    "axes[29].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/frequency-all_tracks.png', format='png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a59d4-04a8-415c-a39d-dbf497c7ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ee465-ec42-40ff-852b-8b0af90399ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_similarity(list1, list2):\n",
    "    len1, len2 = len(list1), len(list2)\n",
    "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
    "    \n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            if list1[i - 1] == list2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
    "    \n",
    "    edit_distance = dp[len1][len2]\n",
    "    max_length = max(len1, len2)\n",
    "    return 1 - edit_distance / max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1968263-546e-4634-bff5-0ab56181bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_commons = defaultdict(list)\n",
    "for track in sorted(list(counter_aspects_by_track.keys())):\n",
    "    most_commons[track] = [_[0] for _ in counter_aspects_by_track[track].most_common(10)]\n",
    "\n",
    "similarities = []\n",
    "for track in most_commons:\n",
    "    entry = []\n",
    "    for _ in most_commons:\n",
    "        entry.append(levenshtein_similarity(most_commons[track], most_commons[_]))\n",
    "    similarities.append(entry)\n",
    "    \n",
    "similarities = np.array(similarities)\n",
    "kmeans_rows = KMeans(n_clusters=2, random_state=2266).fit(similarities)\n",
    "kmeans_cols = KMeans(n_clusters=2, random_state=2266).fit(similarities)\n",
    "\n",
    "row_order = np.argsort(kmeans_rows.labels_)\n",
    "col_order = np.argsort(kmeans_cols.labels_)\n",
    "\n",
    "similarities = similarities[row_order, :][:, col_order]\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax = sns.heatmap(similarities, cmap='coolwarm') #, cbar_kws={'ticks': [0, 2, 4, 6, 8]})#, vmin=0.5, vmax=1.0)\n",
    "ticks = [_+0.5 for _ in range(0, 27, 2)]\n",
    "ticklabels = [_ for _ in range(1, 28, 2)]\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels(ticklabels, rotation=0)\n",
    "ax.set_xlabel('track', fontsize=14)\n",
    "ax.invert_yaxis()\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_yticklabels(ticklabels, rotation=0)\n",
    "ax.set_ylabel('track', fontsize=14)\n",
    "\n",
    "plt.savefig(f'plots/heatmap-track_similarity.png', format='png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604dca24-5e38-48d5-a4e0-c1e74b60df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,i in enumerate(row_order):\n",
    "    print(f'({_+1})', list(counter_aspects_by_track.keys())[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecddfd8-a0bf-4482-af53-9ff6786753ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pd.read_csv('config_inference.txt', sep='\\t')\n",
    "counters, numbers = defaultdict(), defaultdict()\n",
    "venue = 'emnlp23'\n",
    "score_field = 'Excitement' if venue == 'emnlp23' else 'Rating'\n",
    "for source in [f'{venue}_strengths', f'{venue}_weaknesses']:\n",
    "    run_id = config['run_id'][(config['source'] == source) & (config['type_of_labels'] == 'coarse')].to_list()[0]\n",
    "\n",
    "    with open(f'data/{venue}.json') as file:\n",
    "        data = json.loads(file.read())\n",
    "    with open(f'results/inference-{run_id}.json') as file:\n",
    "        results = json.loads(file.read())\n",
    "    \n",
    "    number_of_reviews_by_score, counter_aspects_by_score = Counter(), defaultdict(Counter)\n",
    "    for paper_id in results:\n",
    "        aspects = set()\n",
    "        for reviewer_id, items in results[paper_id].items():\n",
    "            number_of_reviews_by_score[data[paper_id]['Reviews'][reviewer_id][score_field].split(': ')[0]] += 1\n",
    "            for _, item in items.items():\n",
    "                aspects.update(item)\n",
    "            if '-' in aspects:\n",
    "                aspects.remove('-')\n",
    "            counter_aspects_by_score[data[paper_id]['Reviews'][reviewer_id][score_field].split(': ')[0]].update(aspects)\n",
    "\n",
    "    counters[source] = counter_aspects_by_score\n",
    "    numbers[source] = number_of_reviews_by_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a30423-db54-491c-9332-533196ed0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_aspects = ['Analysis', 'Definition/Description/Detail/Discussion/Explanation/Interpretation']\n",
    "\n",
    "data = []\n",
    "for aspect in target_aspects:\n",
    "    for track, counter in counter_aspects_by_track.items():\n",
    "        count = counter.get(aspect, 0)\n",
    "        percentage = round(count / number_of_reviews_by_track[track] * 100, 2)\n",
    "        data.append({'track': track, 'aspect': aspect, 'percentage': percentage})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "for aspect in target_aspects:\n",
    "    subset = df[df['aspect'] == aspect].sort_values(by='percentage', ascending=False)\n",
    "    print('|track|frequency (%)|')\n",
    "    print('|--|--|')\n",
    "    tracks = subset['track'].to_list()\n",
    "    frequencies = subset['percentage'].to_list()\n",
    "    for i in range(len(subset)):\n",
    "        print(f'|{tracks[i]}|{frequencies[i]}|')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d88f6-0e83-4157-b34c-559d3db27a6f",
   "metadata": {},
   "source": [
    "|track|frequency (%)|\n",
    "|--|--|\n",
    "|Computational Social Science and Cultural Analytics|69.23|\n",
    "|Linguistic Theories, Cognitive Modeling, and Psycholinguistics|68.75|\n",
    "|Commonsense Reasoning|62.62|\n",
    "|Multilinguality and Linguistic Diversity|60.68|\n",
    "|Machine Learning for NLP|60.0|\n",
    "|Machine Translation|57.38|\n",
    "|Discourse and Pragmatics|56.86|\n",
    "|Phonology, Morphology, and Word Segmentation|56.67|\n",
    "|Interpretability, Interactivity, and Analysis of Models for NLP|56.37|\n",
    "|Sentiment Analysis, Stylistic Analysis, and Argument Mining|55.66|\n",
    "|Theme Track: Large Language Models and the Future of NLP|55.56|\n",
    "|Information Retrieval and Text Mining|55.5|\n",
    "|NLP Applications|54.48|\n",
    "|Summarization|54.44|\n",
    "|Resources and Evaluation|54.35|\n",
    "|Efficient Methods for NLP|53.07|\n",
    "|Information Extraction|53.06|\n",
    "|Syntax, Parsing and their Applications|52.94|\n",
    "|Language Modeling and Analysis of Language Models|51.85|\n",
    "|Dialogue and Interactive Systems|51.23|\n",
    "|Speech and Multimodality|51.01|\n",
    "|Ethics in NLP|50.96|\n",
    "|Semantics: Lexical, Sentence level, Document Level, Textual Inference, etc.|48.15|\n",
    "|Language Grounding to Vision, Robotics and Beyond|47.91|\n",
    "|Human-Centered NLP|47.78|\n",
    "|Question Answering|45.58|\n",
    "|Natural Language Generation|42.34|\n",
    "\n",
    "\n",
    "\n",
    "|track|frequency (%)|\n",
    "|--|--|\n",
    "|Ethics in NLP|65.38|\n",
    "|Linguistic Theories, Cognitive Modeling, and Psycholinguistics|60.42|\n",
    "|Interpretability, Interactivity, and Analysis of Models for NLP|59.46|\n",
    "|NLP Applications|57.17|\n",
    "|Information Extraction|56.85|\n",
    "|Resources and Evaluation|56.09|\n",
    "|Semantics: Lexical, Sentence level, Document Level, Textual Inference, etc.|56.02|\n",
    "|Human-Centered NLP|54.44|\n",
    "|Machine Learning for NLP|53.56|\n",
    "|Information Retrieval and Text Mining|53.5|\n",
    "|Speech and Multimodality|52.02|\n",
    "|Sentiment Analysis, Stylistic Analysis, and Argument Mining|51.89|\n",
    "|Language Modeling and Analysis of Language Models|51.85|\n",
    "|Language Grounding to Vision, Robotics and Beyond|50.95|\n",
    "|Natural Language Generation|50.9|\n",
    "|Computational Social Science and Cultural Analytics|50.3|\n",
    "|Theme Track: Large Language Models and the Future of NLP|49.9|\n",
    "|Summarization|49.44|\n",
    "|Multilinguality and Linguistic Diversity|49.03|\n",
    "|Dialogue and Interactive Systems|48.77|\n",
    "|Discourse and Pragmatics|47.06|\n",
    "|Efficient Methods for NLP|46.93|\n",
    "|Question Answering|45.94|\n",
    "|Commonsense Reasoning|42.99|\n",
    "|Machine Translation|39.34|\n",
    "|Syntax, Parsing and their Applications|39.22|\n",
    "|Phonology, Morphology, and Word Segmentation|33.33|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
