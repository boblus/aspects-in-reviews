{
    "RndkyLWLHc": {
        "frWd8cEKmp": {
            "0": "The authors contributed a dataset and an annotation protocol.",
            "1": "- The motivation is unclear to me, at least from a practical engineering perspective (e.g., to build an actually useable system).",
            "2": "As the authors pointed out in L120, it was actually unclear how such a system would scale to more general general applications that have external libraries, which are very common in real-life applications.",
            "3": "- There might be computational linguistic/cognitive science motivations, though, and I am not qualified to judge.",
            "4": "I am a bit worried that, by restricting our attention to a closed environment, we are missing the opportunity of collecting data on how language can be used to describe program semantics in a wider range of context."
        },
        "Y0fJXCe7y6": {
            "0": "As the authors note in their motivation, scraping open-source repositories for comments leads to a few issues with the resulting data: comments don't cover all semantic levels of detail, and open-source projects often have complex dependencies that make it difficult to execute them in order to obtain ground-truth semantics.",
            "1": "The proposed annotation scheme addresses the former, and the latter problem is addressed (at least in the authors' HTL dataset) by choosing to annotate standalone programs.",
            "2": "In principle, addressing these issues should make the resulting resource useful to the community.",
            "3": "I don't see any issues in the proposed annotation scheme fundamental enough to merit rejection.",
            "4": "However, the dataset (from what I can tell) appears to cover only 20 programs of the source dataset's ~400 (Schuster et al., 2021).",
            "5": "Based on the histogram in Figure 3, the dataset as a whole contains 209 annotated triples.",
            "6": "This restricted scale limits the usefulness of the primary contribution."
        },
        "NDeU2Vh4gE": {
            "0": "+ A new bi-modal dataset that fills in the gap between informal annotations (e.g., code comments) and formal annotations (e.g., programmatic predicates)\n\n- Lack of clear motivation for building the dataset (HTL): What are the downstream impacts of this dataset?",
            "1": "- No validation of annotation quality: How reliable are the annotations?",
            "2": "- No in-depth descriptions of the annotated dataset: What are the distributions of these annotations?",
            "3": "Are there any patterns and categories?",
            "4": "Any artifacts or shortcuts?"
        }
    },
    "dnQI76LKQy": {
        "3DfKYYDLwD": {
            "0": "1.",
            "1": "The paper is well-written and easy to understand.",
            "2": "2.",
            "3": "This work claims to firstly propose a publicly available large-scale Chinese metaphor dataset, and will publish the data and code.",
            "4": "Although some Chinese metaphor datasets existed in the past, the poor annotation quality of these datasets makes them difficult to use.",
            "5": "Therefore, the new Chinese metaphor dataset proposal in this work is significant to the community.",
            "6": "A contribution of this paper is the metaphor domain classification, which takes identifying the source and target domains of metaphors as a new task.",
            "7": "For example, in “The earth is slowly melting”, the target category for metaphor is natural object, and the source category is substance.",
            "8": "However, I'm not sure about the significance of identifying source categories and target categories for metaphors.",
            "9": "Does it help to identify metaphors better?",
            "10": "But no conclusions can be drawn from the experimental results.",
            "11": "Perhaps the authors could elaborate more on it or demonstrate in experiments that it helps metaphor."
        },
        "mv6a9svMH7": {
            "0": "Taking an advertising perspective, creating a large-scale multimodal dataset is a clever and efficient approach compared to other sources like literary works, songs, or artists' paintings.",
            "1": "Extracting data from creative advertisements and artistic creations offers valuable insights.",
            "2": "Using the domain classification approach further enhances our understanding of metaphorical expressions in different forms of data, mirroring human thinking patterns for problem-solving.",
            "3": "For instance, using animals to represent intoxicated individuals or using \"泥\" (mud) to signify someone heavily drunk in Chinese culture.",
            "4": "Such differences can be applied for interdisciplinary analysis, exploring cultural variances resulting from the distribution of animals, and aiding language learners in memorizing visual representations of expressions.",
            "5": "Building a high-quality dataset and lexicon construction require substantial effort and resources.",
            "6": "In the appendix, I noticed some commercial advertisements, which could lower the dataset's overall quality.",
            "7": "I didn't find relevant statistical data.",
            "8": "Incorporating artwork and portfolios from art and advertising students could improve the dataset's quality."
        },
        "VVNKyq2zK2": {
            "0": "- A multimodal metaphor dataset in non-English could be a valuable resource for future researchers.",
            "1": "- The paper is easy to follow.",
            "2": "- It provides benchmark results, which seem to be from reasonable models.",
            "3": "- I'm not entirely sure how metaphors are defined in the images+texts of this dataset.",
            "4": "I cannot understand why some images are metaphorical in the Appendix when only looking at the images.",
            "5": "The text descriptions are in Chinese.",
            "6": "- This is a dataset paper, and the annotation guidelines are particularly important for metaphors.",
            "7": "Metaphor annotation is notoriously challenging because it is fuzzy and subjective, and multimodal metaphors are even more under-explored.",
            "8": "However, the paper doesn't provide sufficient detail about the annotation guidelines and justifications (e.g., how a metaphor is defined and recognized, how sentiment is defined), although the details of the annotation process are provided.",
            "9": "- As a dataset paper, I expect more insightful discussion about the data so that I can determine whether I want to use this dataset or not.",
            "10": "For example, what are the common disagreements in the annotations?",
            "11": "What kind of characteristics do you observe in the multimodal metaphors?",
            "12": "They are all advertisements.",
            "13": "Are there any interesting things you can observe with regard to that?",
            "14": "This kind of discussion can give a better sense of the dataset and the tasks that can be performed with it.",
            "15": "For these reasons, I think this paper needs more work although I appreciate the work."
        }
    },
    "AptTXihnhH": {
        "pXpYHwQuz7": {
            "0": "1.",
            "1": "This paper proposes an experience reconstruction method to extend a character's profile to many detailed scenes that mimic interactions between the target character and other characters.",
            "2": "The experience can provide more information for character simulation.",
            "3": "This method may inspire future work on character data construction.",
            "4": "2.",
            "5": "This paper proposes constructing and using protective experience to reduce hallucination during role-playing.",
            "6": "Trained on protective experience, the model can pretend to be ignorant of the knowledge that the character does not know.",
            "7": "The character hallucination problem is important, and this work gives some insight.",
            "8": "1.",
            "9": "Lack of important details.",
            "10": "It's unclear which LLM did the authors use for scene extraction and experience generation (both character experience and protective experience).",
            "11": "Does the selection of this model important?",
            "12": "Does the generated experience faithful and without hallucination?",
            "13": "How many scenes are there in the protective experience for training?",
            "14": "For evaluation, how are the questions/topics selected?",
            "15": "2.",
            "16": "Lack of ablation study to show the superiority of the proposed experience construction method.",
            "17": "I think adding a comparison between models fine-tuned on different kinds of character data helps."
        },
        "LdCxndmrbd": {
            "0": "(1) The paper introduces a novel approach for training trainable agents to simulate specific individuals, which is a step closer to character simulacra.",
            "1": "(2) The framework proposed by the authors, including experience reconstruction, experience upload, and protective experiences, is well-structured and practical.",
            "2": "(3) The evaluation process, including interviews and AI-based judging, provides comprehensive insights into the performance of the trained agents.",
            "3": "(1) The paper lacks implementation details, such as the specific architecture and training settings used for the trainable agents.",
            "4": "(2) The evaluation could benefit from comparisons with more baselines and a larger number of interview questions.",
            "5": "(3) The paper could provide more in-depth discussions on the limitations and potential ethical concerns of using trainable agents."
        },
        "WNkPFljv4L": {
            "0": "1.",
            "1": "The paper introduces a novel pipeline for creating trainable agents on LLMs that can emulate specific characters.",
            "2": "This approach is innovative and could have wide-ranging applications in various fields, such as virtual assistants, education, and entertainment.",
            "3": "2.",
            "4": "This paper presents a thorough process that covers everything from gathering character profiles to refining LLMs using those profiles.",
            "5": "This meticulous method could serve as a guide for other scholars in the same field.",
            "6": "3.",
            "7": "The paper also discusses potential flaws and areas for improvement in the proposed approach, indicating a thorough and thoughtful analysis.",
            "8": "1.",
            "9": "While the Protective Experience method is used to reduce hallucination issues, there is a possibility that it may result in bias or incorrect information being incorporated into the agent.",
            "10": "This is a matter of concern for future usage.",
            "11": "2.",
            "12": "Including a human evaluation could potentially increase its credibility."
        }
    },
    "dvDi1Oc2y7": {
        "MXsfdqQbzx": {
            "0": "1)\tThe GBT technique shows improvement (as per the result tables 3 and 4) and could be applied generally to other tasks as well.",
            "1": "The negative and positive paraphrase augmentation seems like a good mix of balanced data augmentation (though we don’t know how much just adding IBH1 and just adding IBH0 independently contribute to improvement).",
            "2": "2)\tThe propose technique seems like a good choice for the PI task and some of the analyses (Figure 2 for boosting interval and ratio, table 5 for why to use GBT for hard examples) highlight interesting takeaways about GBT.",
            "3": "1)\tThere are potentially numerous baselines as data augmentation for hard examples has several work.",
            "4": "Given the closeness with this proposed work of using paraphrases (both negative and positive), some of baselines are necessary for comparison with GBT, especially counterfactual data-augmentation techniques as GBT uses (negative paraphrases in DA i.e., IBH0)\nFeng et.al., A Survey of Data Augmentation Approaches for NLP \nLi  et.al., Data Augmentation Approaches in Natural Language Processing: A Survey\n2)\tSome other, even more simpler baselines could be lower learning rate and training for more number of epochs.",
            "5": "This would even strengthen the claims of GBT if improvements are significant.",
            "6": "3)\tThe ablation study in table 5 seemed more like good baselines which is good to have as it shows GBT is more effective when applied to only hard examples.",
            "7": "A better ablation study could be giving just positive paraphrases (IBH1) and just negative paraphrases (IBH0)\n4)\tSome of the important technical details are unclear.",
            "8": "For example, which datasets and how was the paraphraser trained to generate candidate sentences for selecting IBH0 and IBH1.",
            "9": "In line 268-277, more details would be needed as to how and where the 50K examples were selected from.",
            "10": "5)\tThe text in line 293-295 makes the above point a little bit more unclear.",
            "11": "It would be difficult for readers to understand and evaluate – “we manually observed the generated examples and find the results acceptable.” \n6)\tA very minute point – it may be interesting to compare with openLLM methods like LLaMa (after some instruction tuning for PI  task)."
        },
        "6ULV8ILyyT": {
            "0": "1.",
            "1": "Good motivation to solve the paraphrase identification, with current limitations, detailed examples and vivid diagrams\n2.",
            "2": "Solid experiment on both BERT and GPT, reaching good performance or even state-of-the-art compared to the baseline models.",
            "3": "3.",
            "4": "Explore the detailed of such data augmented boosting training, including but no limited to, boosting time, boosting ratio, different language models, etc.",
            "5": "4.",
            "6": "GBT is efficient since it can be completed 1-hour on GPU.",
            "7": "1.",
            "8": "The advanced data augmentation methods in comparison are from 2018-2020.",
            "9": "It is unclear that whether the paper is comparing with the stat-of-the-art counterparts.",
            "10": "2.",
            "11": "It is not sure that whether part of the performance improvement can be attributed to the learned language ability from the seq2seq model via generated data (like the language model distillation), but not the GBT."
        },
        "bOaE0kWJfE": {
            "0": "1.",
            "1": "The method seems promising in term of the efficiency, which is superior to the adversarial methods in this view.",
            "2": "2.",
            "3": "The paper is well written and easy to follow.",
            "4": "3.",
            "5": "All the source codes will be made publicly available to support reproducible research.",
            "6": "1.",
            "7": "The soundness of mimicking the human learning process should be further discussed in detail.",
            "8": "What about multi-turn rethinking or correction for fine-grained errors？\n2.",
            "9": "More error ratio information and the corresponding examples should be analysed to support the motivation of the idea."
        }
    },
    "iDZQG9aUGH": {
        "LHvfH5WdhT": {
            "0": "1.",
            "1": "Clear writing of the paper.",
            "2": "2.",
            "3": "Thorough experimental analysis that tried multiple model sizes and various source/target tasks.",
            "4": "1.",
            "5": "My main concern of the paper is that the proposed method doesn’t show a significant improvement compared to other existing methods in Table 1 lower part.",
            "6": "2.",
            "7": "I don’t quite understand the motivation of the method.",
            "8": "Intuitively, what are the expected benefits of the proposed method compared to existing methods?"
        },
        "tPuryy82fE": {
            "0": "1.",
            "1": "Acceptable results showing that the method outperforms the existing PEFT methods on GLUE.",
            "2": "2.",
            "3": "This work can encourage more work on Bayesian multi-task prompt training in the future.",
            "4": "3.",
            "5": "The paper is clearn and well-written.",
            "6": "1.",
            "7": "The multi-task results on Super-GLUE are worse than MPT.",
            "8": "2.",
            "9": "The method does not compare to existing work in the literature when scaling the model in section 6.2."
        },
        "jKdocaThrw": {
            "0": "The proposed method demonstrates commendable innovation, standing apart from mere amalgamation of existing models.",
            "1": "The novel approach showcases tangible efficacy without introducing additional parameters.",
            "2": "1.The absence of a computational complexity analysis, coupled with marginal and non-significant experimental performance improvements, raises concerns regarding the practical significance.",
            "3": "If the proposed model introduces high computational complexity and yields only marginal gains, its real-world utility may be limited.",
            "4": "2.The paper lacks comparative analysis with some important baselines, such as P-tuning v2.",
            "5": "3.",
            "6": "Additionally, the theoretical substantiation for the proposed method is insufficiently detailed.",
            "7": "4.",
            "8": "The exclusive use of a single language model, T5-base, as the backbone prompts doubts about the general applicability of the proposed approach across a broader spectrum of models.",
            "9": "5.",
            "10": "Furthermore, the paper lacks visual or interpretable analyses that incorporate concrete natural language statements.",
            "11": "Considering these points, I respectfully recommend that the authors thoroughly address these shortcomings to enhance the paper's overall quality and potential for contribution before reconsidering it for acceptance."
        },
        "Udf60TD9Ey": {
            "0": "1.",
            "1": "A Bayesian multi-task transfer learning approach to prompt tuning that outperforms state-of-the-art methods in many settings.",
            "2": "2.",
            "3": "A soft prompt tuning method that allows for more flexible and efficient transfer learning by using a continuous relaxation of the prompt vectors.",
            "4": "3.",
            "5": "An extensive experimental evaluation on standard benchmark NLP tasks that demonstrate the effectiveness of the proposed approach and its superiority over existing methods.",
            "6": "1.",
            "7": "Technically, the applied approaches are not novel techniques and the main novelty is applying them to solve a multitask learning problem.",
            "8": "Multi-task prompt tuning is only one specific instance of multi-task learning.",
            "9": "2.",
            "10": "Regardless of learning the posterior distribution of source tasks, the practical implementation seems to be kind of similar to the mentioned reference, MPT, esp., in terms of prompt decomposition, parameter efficiency, and the source training/target adaptation process.",
            "11": "The authors might need to explain or clarify clearly the fundamental differences with MPT in the paper.",
            "12": "3.",
            "13": "The empirical results are marginal or nearly the same as the baselines.",
            "14": "The only vast improvement comes from the CoLA task in GLUE, improving it from 60+ in baseline to 86.",
            "15": "The rest of all tasks are nearly the same or very marginal improvement over ATTEMPT or MPT.",
            "16": "Therefore, the empirical improvement isn't impressive."
        }
    },
    "QAZ2QV8SqN": {
        "fTGBnRqLeP": {
            "0": "1.",
            "1": "This paper proposes a versatile framework that utilizes LLMs for tasks that use KGs.",
            "2": "2.",
            "3": "This paper evaluates KG-GPT using KG-based fact verification and KGQA benchmarks, which fills the gap in complex reasoning on knowledge graphs for LLMs.",
            "4": "1.",
            "5": "This paper fails to provide a clear introduction to the sentence segmentation step, which constitutes a crucial aspect.",
            "6": "2.",
            "7": "This paper presents a feasible method for combining LLMs with KGs.",
            "8": "However, it claims that KG-GPT can comprehend graph structures, yet the experiments provided in the paper fail to substantiate this claim.",
            "9": "Additional comparative experiments and case studies are required to support this assertion adequately."
        },
        "ba4yF9lPAP": {
            "0": "S1 The paper designs a framework for reasoning based on knowledge graph.",
            "1": "And the framework shows competitive performance, which gives us some insights of using ChatGPT for reasoning based on knowledge graph.",
            "2": "S2 The details of the way of training the model are provided, for instance, the prompts used in every step.",
            "3": "This level of transparency enhances the reproducibility and understanding of the proposed approach.",
            "4": "W1 The paper makes several claims that some have found to be factual but not entirely accurate.",
            "5": "To strengthen its credibility, a more thorough literature review may be necessary.",
            "6": "---- The paper claims “there is no general framework for performing KG-based tasks (e.g.",
            "7": "question answering, fact verification) using LLMs.” However, I found [1] proposes a framework using RoBERTa-large and KG for question-answering.",
            "8": "There are also some other tasks like knowledge graph completion and graph-to-text generation which are KG-based tasks use LLMs, for example, [2] and [3], respectively.",
            "9": "-------------------------------------------\nPut W1 to presentation improvement.",
            "10": "W2 The formal notations and definitions have various problems related to soundness, clarity, and rigor.",
            "11": "--- line 101-102, “Si consists of a set of entities and a relation”, it is unclear why a sequence of tokens consists of a set of entities and a relation.",
            "12": "And in lines 115-118, it mentions Ri is a set of relations, isn’t each sub-sentence only have one relation according to lines 101-102?",
            "13": "--- Algorithm 1, what is TypeDBpedia and how it is gotten; it is unclear what E1 and E2 are.",
            "14": "What is “Relations (T, TypeDBpedia)” and “Relations (e, DBpedia)”?",
            "15": "W3 One aspect of the paper's precondition raises concerns for me.",
            "16": "It assumes that \"all entities involved in S are given,\" which may not align with real-world applications.",
            "17": "In practice, users may not specify all entities in their questions or claims, making this assumption less realistic.",
            "18": "W4 The choice of datasets for the experiments seems atypical.",
            "19": "For instance, FactKG, released in 2023, might not have gained widespread adoption yet, whereas [4] FEVER serves as a widely used benchmark for claim verification.",
            "20": "W5 The paper appears to lack technical depth, as all three components primarily rely on providing prompts to ChatGPT."
        },
        "3NB8bG6YiE": {
            "0": "The problem addressed in this manuscript is intriguing, and the proposed framework is technically sound.",
            "1": "The results obtained from the experiments are promising, indicating the potential effectiveness of the proposed approach.",
            "2": "The paper would benefit from a more detailed explanation of the results, including how each step contributes to the overall performance and why the methods perform differently.",
            "3": "This could help readers better understand the proposed framework and its effectiveness in addressing the problem of knowledge graph reasoning."
        }
    },
    "GTqt0X2Swn": {
        "rgGTf8Ne9c": {
            "0": "The author believes that creativity is the deficiency of the current generation model, so the author directly chooses to fine-adjust GPT-2 and GPT-3 to build AFFGEN, and the results also show that AFFGEN is better than GPT-2 and GPT-3 in creativity, which provides a new inspiration for story generation.",
            "1": "The proposed AFFGEN model requires the exploration of different beam sizes and affective rearrangements.",
            "2": "It's unclear whether adding these two technologies will outperform a well-fine-tuned model while keeping the system design simple."
        },
        "IFQdZTY7yZ": {
            "0": "The paper proposes Dynamic Beam Sizing and Affective Reranking methods to make the generated sentences more interesting.",
            "1": "The evaluation shows that the proposed method can improve the intrigue and coherence of a generated story.",
            "2": "----- After rebuttal -----\n\nThe paper introduces the Dynamic Beam Sizing and Affective Reranking techniques as a means to enhance the appeal of generated sentences.",
            "3": "The evaluation demonstrates the effectiveness of these approaches in augmenting the intrigue while keeping the coherence of generated stories.",
            "4": "Key contributions lie in its simplicity and adaptability.",
            "5": "The method employs a count-based model to identify one twist sentence and subsequently inserts the intriguing twists.",
            "6": "This infusion elevates the arousal and valence of the sentences and meanwhile introduces variations to the rest of the story.",
            "7": "The inclusion of dynamic beam sizing further bolsters the method's adaptability to different scenarios.",
            "8": "1.",
            "9": "Overall, the idea seems too complicated and not interesting.",
            "10": "AFFGEN needs to train a model to identify the insertion position of the twist and uses an off-the-shelf model to predict trigger likelihood, but simply refer to the arousal and valence scores to define \"interesting\".",
            "11": "Why not directly consider the arousal and valence scores over the whole sentence?",
            "12": "2.",
            "13": "The evaluation results are not convincing.",
            "14": "- The comparison only involved GPT 2 & 3 but no other SOTA methods.",
            "15": "- Table 1 doesn't show significant gains from AFFGEN to the baselines.",
            "16": "3.",
            "17": "Many technical details are missing from the paper.",
            "18": "See `Questions For The Authors`.",
            "19": "----- After rebuttal -----\n\nMore efforts are needed in selecting better baseline results.",
            "20": "If there are no other comparable methods, we could probably carefully prompt GPT 2 & 3 to make it a fair comparison.",
            "21": "For example, since the proposed method explicitly leverage the concepts of arousal and valence, we can also include them in the prompt sentence, something like \"Generate a story that has high arousal and valence scores.",
            "22": "The total length ....\"."
        },
        "63ZAQIr4gz": {
            "0": "Improving the interestingness of the story is an interesting area of text generation but it is not well studied.",
            "1": "The proposed algorithm is relatively simple: just generating one sentence containing an intriguing twist.",
            "2": "The results show the method to be effective, significantly improving upon the base model with the standard decoding algorithm.",
            "3": "The result that dynamic beam sizing improves interestingness compared to using a constant large beam size is somewhat surprising.",
            "4": "Because the motivation to use dynamic beam size is that a larger beam size can generate more intriguing words but at the expense of coherence.",
            "5": "The motivation for using dynamic beam size (as explained in Section 4.3) is to achieve a better trade-off between coherence and interestingness (as well as achieving better decoding efficiency).",
            "6": "The main weakness is that the proposed algorithm seems to be tailored to the five-sentence story dataset, which is the only test dataset.",
            "7": "For example, many examples in the paper show that by generating the second sentence with an intriguing twist, the rest of the story becomes more interesting.",
            "8": "This may be true for the five-sentence story, but it's hard to imagine this is the case for long stories.",
            "9": "Also, the affective reranking contains many hand-crafted rules, which also seem to be tailored to the five-sentence story dataset.",
            "10": "Another weakness is that the comparison between GPT3 and ChatGPT seems to be unfair.",
            "11": "While the authors build many hand-crafted rules  for affective reranking, the prompts for GPT3/ChatGPT are not well optimized.",
            "12": "I played with ChatGPT and I can easily get more interesting stories by changing for prompt slightly from the prompt in Table 8.",
            "13": "For example, by changing \"Write a 5-sentence interesting short story\" to \"Write a 5-sentence interesting short story with a mystery\".",
            "14": "I think the most interesting component is the dynamic beam sizing.",
            "15": "While the ablation study shows it's better than a constant large beam size (in contrast to the original motivation), it does not give more insights into the underlying reason.",
            "16": "So it is still not clear whether this is a generalizable method outside of the five-sentence dataset.",
            "17": "Finally, the clarity can be improved.",
            "18": "It's not very clear how the contextual bandit model works."
        }
    },
    "HtQvhCRTxo": {
        "xJTwWnH6Lc": {
            "0": "The introduction of a new dataset is an important contribution to the field.",
            "1": "In particular, a dataset containing containing the NOTA relation label is particularly valuable.",
            "2": "Additionally, the fact that relations in the chosen domain would be hard to distinguish solely basing the reasoning on the entity types is a valuable characteristic of a dataset for RC.",
            "3": "As mentioned by the authors in the limitations sections, the fact that the dataset is based solely on Wikipedia data limits the variety of styles used in different texts in the data.",
            "4": "Since the authors are presenting a dataset for the few-shot setting, a dataset with a larger number of distinct relation labels would have been more useful to evaluate models on a more diverse range of classes."
        },
        "fH43BCH3jO": {
            "0": "1.",
            "1": "Proposes a new dataset for few-shot relation classification in company domain.",
            "2": "The dataset requires more challenging entity types and a deeper understanding of the context compared to existing few-shot relation classification datasets.",
            "3": "2.",
            "4": "Shows the dataset is challenging for out-of-domain generalization.",
            "5": "3.",
            "6": "Provides insights into how diverse and challenging datasets such as CORE are required for robust domain adaptation.",
            "7": "Encourages future research to consider these factors when generating datasets to advance research on robust few-shot RC models.",
            "8": "1.",
            "9": "The scope of the paper seems a bit narrow.",
            "10": "The dataset focuses only on company relations, but makes strong claims about generalization of RC models.",
            "11": "Perhaps that claim needs to be supported with datasets on a few other domains.",
            "12": "2.",
            "13": "The dataset is limited to company relations from Wikipedia pages.",
            "14": "This may limit the diversity of the dataset.",
            "15": "It may also not be representative of real-world data.",
            "16": "3.",
            "17": "The dataset contains only a limited number of relations and examples, which may limit its usability in different scenarios.",
            "18": "4.",
            "19": "The few-shot RC models considered in the paper are not state-of-the-art models (e.g.",
            "20": "https://aclanthology.org/2022.coling-1.205.pdf, https://ieeexplore.ieee.org/abstract/document/10032649/).",
            "21": "How does the performance compare to relation extraction/generation models in few-shot settings."
        },
        "jqOsRNeRo4": {
            "0": "Although it presented the differences between CORE and FewRel, maybe an explanation is needed why this work was started while FewRel already existed.",
            "1": "There is overlapping, maybe a combination of the two datasets would be better?",
            "2": "The in-domain results presented in shows that the proposed model surpassed other approaches in few-shot settings while out-of-domain evaluation reveals models trained on CORE outperform the in-domain models trained on FewRel.",
            "3": "The work is interesting and the dataset could be a valuable resource for few-shot domain-adaptation in relation classification.",
            "4": "Although it presented the differences between CORE and FewRel, maybe an explanation is needed why this work was started while FewRel already existed.",
            "5": "There is overlapping, maybe a combination of the two datasets would be better?",
            "6": "The in-domain results presented in shows that the proposed model surpassed other approaches in few-shot settings while out-of-domain evaluation reveals models trained on CORE outperform the in-domain models trained on FewRel.",
            "7": "The work is interesting and the dataset could be a valuable resource for few-shot domain-adaptation in relation classification."
        }
    },
    "mRETTyZEJa": {
        "FUecH9AWMF": {
            "0": "1.",
            "1": "It shows that retrieving the stories, asking why, and selecting the proper generated evidence could improve the quality of generated stories.",
            "2": "2.",
            "3": "All the methods make sense.",
            "4": "3.",
            "5": "The experiments can support its main claims.",
            "6": "The evaluation is done by experts with a Master's degrees in English Literature.",
            "7": "Before I start to write my reasons, I would like to first admit that this is my first time to review LLM papers, so I am not familiar with the acceptance standard in the LLM papers.",
            "8": "In short, I believe that proposing a complex way to prompt the LLM and show the improvement is not enough.",
            "9": "I think you need to show that other simpler or popular approaches cannot lead to similar improvement in order to justify the usage of your complex method.",
            "10": "My main complaint to this paper is its experiments.",
            "11": "I discuss its experiment limitations from the following angles.",
            "12": "1.",
            "13": "This paper has similar limitations of other LLM papers.",
            "14": "The proposed methods are not very novel.",
            "15": "It seems to try to make the proposed methods more complex to increase the novelty, but it is unknown if this complex way of prompting is necessary (see the next point).",
            "16": "I think using Alpaca-Plus-7B/ChatGPT for generation and ChatGPT for evaluation is not very costly.",
            "17": "You should be able to try more baselines and conduct more fine-grained ablation study to verify the effectiveness of the propose approach.",
            "18": "For example, you can see how the story quality change as you increase the number of evidence trees.",
            "19": "You can try a baseline that always inserts a fixed movie summary from the IMDB dataset into your prompt to show if your retrieval method is really useful.",
            "20": "2.",
            "21": "The proposed method is complicated and the improvement of each component is not very large.",
            "22": "I wonder if there is some simpler baselines that can achieve similar improvements.",
            "23": "I list some examples below.",
            "24": "a) In the appendix tables, I observe that the final stories are usually longer.",
            "25": "Maybe we actually don't need to use this complex evidence forest.",
            "26": "We can just ask the LLM to revise the initial story to add more details or make it more coherent.",
            "27": "b) In Table 1, ICL seems to give us simpler but more relevant stories.",
            "28": "Then, could we simply do some prompt engineering to encourage the LLM to generate more complex stories?",
            "29": "Or showing one story with more complex plot as an example and encourage the LLM to write something like that.",
            "30": "c) Table 1 seems to suggest that the human evaluation is highly correlated with the automatic evaluation.",
            "31": "Then, I think one simple baseline is to generate several stories and use the automatic evaluation metric to select the best one.",
            "32": "It would be good to compare this simple method with GROVE in terms of generation quality and diversity.",
            "33": "3.",
            "34": "The experiment is not very comprehensive.",
            "35": "All the quantitative results are done on ChatGPT.",
            "36": "Not sure whether it is also applicable to other LLMs.",
            "37": "Furthermore, in my opinion, ablation study in Table 2 is more important than Table 1 because Table 2 supports the main claim of the paper (retrieval and asking why are effective) and the methods in Table 1 are not very comparable.",
            "38": "However, the author does not conduct human evaluation in Table 2."
        },
        "sIYfo4TiwZ": {
            "0": "The proposed method provides more information to the users on knowing what can be improved from the initial story where it might help humans or machines to (re)-write for a better story.",
            "1": "The whole pipeline is very complex.",
            "2": "If we know that the missing background is important for story rewriting, can we prompt the model to iterate on the missing backgrounds in CoT setting.",
            "3": "I believe some performance improvement of the baseline models can be done by simply adding explicit instructions, e.g., “generate a long/creative/complex story”.",
            "4": "For the CoT result, it seems like the problem is that the model failed to finish the story.",
            "5": "I am wondering if this can be solved by the 16k model?",
            "6": "It’s unclear what’s the prompt for CoT baseline.",
            "7": "It will be more useful to have a full log for each method."
        },
        "FzRdFwqzJJ": {
            "0": "Tackling the challenging topic of having both controllability and creativity, the proposed method achieved a good result.",
            "1": "Some information is not fully explained.",
            "2": "The term “a forest of evidence” (or “evidence forest”) should be clearly defined (or cited) because this term seems to be one of the key points of this paper.",
            "3": "Fig.",
            "4": "1 is explained in Section 3.1, but there is no explanation about where is the corresponding part in 3.1 and steps 1 to 3."
        }
    },
    "NPJznfA7ZC": {
        "gtbPX4YJmm": {
            "0": "- Understanding why some prompts might work better than others and finding systematic ways to find better prompts is in fact a very important research direction, especially since prompting (in-context learning) is becoming the de-facto standard for adapting LMs to downstream tasks.",
            "1": "This paper addresses exactly this research question.",
            "2": "- The experiments and analysis are carefully designed while controlling for confounders.",
            "3": "- The paper is generally well-written and easily understandable.",
            "4": "- From line 126 onwards, the authors state that lower perplexity correlated with better performance.",
            "5": "A possible confounder here could be the relevance of the prompts.",
            "6": "While in section 6.2 the authors investigate the effect of noisy prompts, I wonder if the relevance of the prompts to the task can be factored in systematically earlier in experiments in section 5 (like Table 3 results).",
            "7": "- Lines 480-482 compare the variability in accuracy between the two settings, however, the number of prompts assessed in these two settings are not equal, doesn't it introduce some bias?",
            "8": "- In Table 8, for the Emotion task, we see that the delta is negative, this is true while in Table 3 and for the same OPT model and Emotion task, the correlation was negative.",
            "9": "I wonder how this could be the case."
        },
        "UF1jFJHpAd": {
            "0": "* This work presents an analysis of the correlation between LM's perplexity with prompts and downstream performances.",
            "1": "The analysis has some merits for future research.",
            "2": "* The proposed SPELL shows performance gains on some datasets, especially for Newspop (+10 accuracy points across two LMs).",
            "3": "* The problem setting is not well-established.",
            "4": "As this paper focuses on perplexity, there are other factors other than perplexity that could influence downstream performances.",
            "5": "These factors should also be discussed in the paper; otherwise, the authors cannot ensure that it is perplexity really matters.",
            "6": "More details are in Question A in the next section.",
            "7": "* This paper conduct experiments on word translation and text classification.",
            "8": "The two tasks somehow have limited coverage.",
            "9": "It would be better if more tasks could be adopted, especially some more complex tasks like planning and reasoning.",
            "10": "With more complex tasks, the differences among prompts would be more significant, providing more generalizable experimental conclusions.",
            "11": "* The performance gains of SPELL are not consistent across datasets and LMs, raising concern about whether the proposed method could work in a broader scope."
        },
        "IRDlCkQGT3": {
            "0": "1.",
            "1": "The proposed perplexity estimation approach demonstrates wide applicability in reducing the variance of zero-shot learning performance, particularly in multilingual tasks, as evidenced by the strong correlation between perplexity estimation scores and task performance.",
            "2": "2.",
            "3": "The authors introduced a framework that enables automatic rephrasing and adjustment of the seed prompt to enhance compatibility with language models, resulting in improved performance.",
            "4": "3.",
            "5": "Through the authors' perplexity estimation method, it is revealed that certain prompts, despite having similar semantics and appearing acceptable to humans, can lead to poor performance.",
            "6": "These prompts can be identified using the perplexity estimation approach, providing valuable insights for prompt learning debugging without reliance on external knowledge.",
            "7": "1.",
            "8": "While the authors assert that perplexity estimation has the potential for broad application across various tasks, its significance is primarily evident in multilingual tasks.",
            "9": "However, the correlation in classification tasks provides less support for this hypothesis.",
            "10": "2.",
            "11": "The authors concentrate on zero-shot settings across all tasks, which can pose challenges for smaller language models (such as 1.3B and 30B models) in specific tasks like CoLA and Tweet Offensive.",
            "12": "Notably, in these challenging tasks, the performance and correlation may not exhibit a consistent pattern, indicating potential limitations of perplexity estimation."
        }
    },
    "Faxkz2V56o": {
        "CwJdX9Drkw": {
            "0": "Overall, this paper is well motivated and experiments are comprehensive enough.",
            "1": "1.",
            "2": "The motivation of the paper is clear and reasonable.",
            "3": "Overall, the paper is easy to read.",
            "4": "2.",
            "5": "The self training approach can improve both bi-encoder and cross-encoder ranking models and the authors demonstrate the effectiveness of the approach across various datasets.",
            "6": "The paper seems OK.",
            "7": "However the noise induced by synthetic queries and possible solutions have been discussed by GPL and PTR.",
            "8": "Thus, I feel the main contribution is to save training time compared to GPL; thus, I feel the contribution is not enough for a long paper.",
            "9": "1.",
            "10": "I think the proposed method is very similar to Promptagator (PTR) by generating synthetic queries using seq2seq models and cleaning data with self consistency.",
            "11": "And the proposed method is also similar to GPL but replacing the cross-encoder teacher with a bi-encoder teacher.",
            "12": "The main difference is that PTR and GPL train one model for each dataset while the authors train one model for all datasets (but still have two models for MS MARCO and BEIR).",
            "13": "Although the main claim that synthetic queries are noisy and the corresponding solution are reasonable, this claim and solution are well known according to the PTR and GPL.",
            "14": "That is, I do not see any significant impact of the paper rather than just trying the different training settings of GPL.",
            "15": "2.",
            "16": "As for comparison, I think that the authors should also follow GPL’s training strategies and train a single retrievers for all BEIR dataset.",
            "17": "In this way, we can see the importance or the gain of self training.",
            "18": "However, the authors only compare with the single model trained on GenQ."
        },
        "b2EmB68BbL": {
            "0": "1.",
            "1": "This paper is well written and easy to follow.",
            "2": "2.",
            "3": "The idea of improving dense retrieval using self-training sounds reasonable to me.",
            "4": "3.",
            "5": "The experiments are extensive and support the main claim.",
            "6": "1.",
            "7": "The technical novelty of this paper is limited.",
            "8": "But it is a minor issue for an empirical study paper.",
            "9": "2.",
            "10": "The motivation of generating pseudo labels for synthetic queries is not strong enough.",
            "11": "Even though the authors provide a motivation example in Table 1, a comprehensive noise analysis of the generated queries is favorable."
        },
        "ArsNsJrCn7": {
            "0": "- The proposed are evaluated from several perspectives in the experiment.",
            "1": "- The motivation to solve \"out-of-domain\" problems in retrieving is interesting.",
            "2": "- The paper is overall easy to follow.",
            "3": "- The main contribution of this paper is to use self-training framework to learn a student retriever that takes training data query-paraphrase pairs and the negatives given by a teacher retriever in an interactive manner.",
            "4": "The authors claim that using the soft labels generated by teacher retrievers would be more robust to noise.",
            "5": "However, this claim is not well supported or justified in the paper.",
            "6": "More specially, since the teacher model is trained with the labelled data (in the beginning), why using the soft label can be more robust?",
            "7": "- Another main claimed strength of the proposed model is the generalizability to \"out-of-domain\".",
            "8": "However, It is not clear how the model achieves this ability.",
            "9": "There is not specific design in the training algorithm.",
            "10": "Also, the out-of-domain results of student retriever in table 5 seem to be very weak compared to baselines, it is not convincing that the model can really solve \"out-of-domain\" scenarios.",
            "11": "- Since the proposed self-training is an iterative training framework, a convergence analysis (e.g., convergence experiments) is required to understand the training process."
        }
    },
    "aN8zkE15Nx": {
        "gdr3IAzDL8": {
            "0": "The authors:\n - Provides a benchmark for understanding converse relations that poses a challenge for LLMs.",
            "1": "- Conducts a thorough evaluation on the benchmark showing inverse scaling trend.",
            "2": "- Quite clear presentation.",
            "3": "- Not reproducible results on ChatGPT and GPT4 (however, fortunately there are results on the open sourced FLAN-T5)."
        },
        "IgUYOS3vQc": {
            "0": "- The paper presents an interesting analysis of the reasoning capabilities of LLMs by measuring their ability to understand converse relations.",
            "1": "- The authors introduce a new benchmark (ConvRe) for normal and converse relations to assess the semantic comprehension capabilities of future LLMs.",
            "2": "- While this work presents an interesting analysis of the reasoning capabilities of LLMs (even if a few specific) and proposes a benchmark for assessing future language models, I believe that the paper's content might be diluted for its length, and condensing it into a shorter paper could potentially better convey its findings.",
            "3": "There appears to be some redundancy among Figures 2, 3, and 4, as they share a lot of information within themselves.",
            "4": "Additionally, the bottom part of Figure 1 seems somewhat uninformative, with only one line in the leftmost graph and models not clearly represented.",
            "5": "For Figures 5 and 6, I would rather explore the possibility of using tables instead of presenting a whole page of graphs.",
            "6": "I feel it might be easier for readers to follow and compare information.",
            "7": "- I feel like drawing a clear and concise conclusion from this work is challenging due to the vastly different behaviors exhibited by the three models under study.",
            "8": "For instance, Claude shows, in general, insensitivity to the number of parameters, performing well or not regardless of this factor, while Flan-T5 aligns more closely with the initial hypothesis proposed by the authors.",
            "9": "Additionally, in the few-shot setting, Claude models display completely opposite behavior in the Re2Text and Text2Re tasks, whereas GPT and Flan-T5 models exhibit more consistency.",
            "10": "Why is that so?",
            "11": "Is it because of the data used in their pre-training?",
            "12": "Is it because of a difference in the number of parameters of these two different models (how claude-1 compares with Flan-T5 XXL?",
            "13": ")?"
        },
        "47qy0kgXRU": {
            "0": "The paper reveals an important shortcoming of large language model: the reliance on superficial correlations when dealing with some formal-language oriented tasks.",
            "1": "What’s more important is that this problem doesn’t get solved (but gets worse) with scale.",
            "2": "Experiments are carefully-designed and provide sound and convincing justifications to the claims.",
            "3": "The paper is well-written and easy to follow.",
            "4": "The scope of the study is relatively narrow - spurious correlation in LLM might be ubiquitous, and this paper only focuses on the case of relationship triple ↔ text conversion.",
            "5": "Minor:  the paper could be better contextualized among “shortcuts”, “superficial correlations”, “superficial cues”, “spurious correlation”, \"counterfactual\""
        },
        "RzTdeEcSC3": {
            "0": "1.",
            "1": "The paper introduces a novel benchmark called ConvRe, focusing on converse binary relations, which addresses the question of whether LLMs understand the structured semantics of formal languages beyond their pre-training data.",
            "2": "This benchmark provides a specialized evaluation to assess LLMs' ability to match relations with associated text.",
            "3": "2.",
            "4": "The paper presents a comprehensive evaluation protocol that includes different prompting methods, variants of test text, and few-shot example text.",
            "5": "This robust evaluation sheds light on the strengths and weaknesses of LLMs when dealing with structured semantics in formal languages.",
            "6": "3.",
            "7": "The paper's findings reveal that LLMs often resort to shortcut learning and face challenges on the proposed benchmark.",
            "8": "This insight highlights the limitations of current LLMs and can guide future research to improve their understanding of formal language semantics, benefiting the NLP community by driving advancements in model design and evaluation.",
            "9": "1.",
            "10": "The problem of short-cut learning of language models is important and interesting, but converse relations seem to be a minority term.",
            "11": "I believe this work would be more valuable if it could be studied from a more general perspective.",
            "12": "2.",
            "13": "Some details are not clear.",
            "14": "For example (1) In Figure 2 and 3, in the LLM Pre-training corpus, the text corresponding to the triple is “x has a part called y” instead of the sentence “y has a part called x” in the given instruction.",
            "15": "Or some other sentence?",
            "16": "The input context can influence the model's decisions just as much as the pre-trained corpus."
        }
    },
    "UlewKJFkUV": {
        "JRxanE2WsS": {
            "0": "This paper could help researchers better understand the behavior of neural machine translation systems.",
            "1": "The approach is well-motivated and simple.",
            "2": "The analysis is quite thorough and presented logically.",
            "3": "This paper could potentially lead to data up/downsampling techniques to improve machine translation quality (although this is not yet established).",
            "4": "The findings would be more convincing if there were models with English as the target language (i.e.",
            "5": "opposite direction).",
            "6": "The datasets are quite small, although this is understandable given the high compute requirements.",
            "7": "The relationship between TM/GS vs model/dataset sizes is unclear."
        },
        "oa1pXLh5Ms": {
            "0": "(1) Overall, the paper is written well and easy to follow; (2) It shows detailed experimental results and discussions about memorization in Neural Machine Translation; (3) The experiments are sufficient and the results are comprehensive and convincing.",
            "1": "The novelty is limited and the motivation is not strong."
        },
        "c2t6C5gbpy": {
            "0": "1.",
            "1": "Innovative Exploration of Memorization vs. Generalization: The paper offers a groundbreaking perspective on the memorization-generalization continuum in NMT, supported by the novel \"Memorisation Cartography\" approach.",
            "2": "This deep dive into a fundamental dilemma is both timely and essential for the evolving NLP landscape.",
            "3": "2.",
            "4": "Robust Experimental Foundation with Broad Implications: The thorough experiments presented validate the research's claims and offer insights with wide-ranging applications.",
            "5": "These findings can guide the design of efficient NMT architectures, especially beneficial in low-resource scenarios, and pave the way for optimized model performance.",
            "6": "3.",
            "7": "Significant Theoretical and Practical Contributions: Beyond its immediate application to NMT, this research enriches the theoretical understanding of neural network behaviors.",
            "8": "It also opens avenues for future work, with potential extensions to various NLP tasks and strategies\n\n1.",
            "9": "Potential Lack of Generality: While the paper offers insights into the memorization-generalization continuum in NMT, it might be constrained in its applicability.",
            "10": "The findings, although significant, might not extend seamlessly to other NLP tasks or even to all NMT architectures.",
            "11": "2.",
            "12": "Need for Further Validation: Although the paper presents various experiments, additional validation on a broader range of datasets, including more diverse languages and domains, would have strengthened the claims.",
            "13": "The current experimental setup might not capture all nuances and complexities of real-world translation tasks.",
            "14": "3.",
            "15": "Absence of Practical Implementations: The paper, while rich in theoretical content, lacks a clear bridge to practical implementation.",
            "16": "Concrete strategies or tools that harness the presented insights for real-world applications would have elevated the paper's utility.",
            "17": "4.",
            "18": "Depth over Breadth: The paper's deep dive into the memorization-generalization continuum, while commendable, might come at the expense of exploring other equally pertinent aspects of NMT.",
            "19": "A more holistic approach, considering other factors influencing NMT performance, could have provided a rounded perspective."
        },
        "VklKJqF0Sv": {
            "0": "1.",
            "1": "The article is well written and structured.",
            "2": "2.",
            "3": "The article has sufficient experimental studies to demonstrate the findings.",
            "4": "3.",
            "5": "The research question this work explored is useful to understand memorisation behaviour and solve hallucination problem which are widely existed in current LLMs.",
            "6": "The work can also help reseachers to improve the model performance by manipulating used datapoints.",
            "7": "1.",
            "8": "As the authors stated in the Limitations, though computationally expensive experiments are conducted, the applicability of the findings this work observed is questionable as the experimental setup is limited."
        }
    },
    "rwcTxeSsVI": {
        "7dK5ka8XXf": {
            "0": "The paper is written very clearly, is placed well within existing literature and has a clear and well-described experiment design.",
            "1": "It also shuns away from pursuing the SotA and instead focuses on more fundamental links between text generation and information theory.",
            "2": "I believe that as it is, the paper is a good contribution to the community.",
            "3": "While the execution is great, the novelty of this work is disputable as most of the ideas and conclusions could be synthesized from existing works.",
            "4": "That said, I do not believe that this should be a reason to reject this paper."
        },
        "yvolTWp82k": {
            "0": "1 Classify the different errors caused by different probabiltiy settings.",
            "1": "2 By using nature langauge inference module in pipeline, they improve the generation.",
            "2": "Only 50 sampled are evaluated, thus the reliability is not good."
        },
        "ntVA20yDwA": {
            "0": "Using NLI to guide language generation is an interesting idea.",
            "1": "The correlation between NLI relations and the quality of generated text is significant in the experiments.",
            "2": "The paper is well-written.",
            "3": "I'm not sure to what extent the findings in this paper are generalizable to all language generation tasks.",
            "4": "As the authors pointed out in the Limitation section, it seems impossible to justify that the three categories of NLI relations are sufficient to cover all possible relations between generated text and prompt.",
            "5": "In this sense, it is important to include discussions on the generalizability of the method and why the chosen dataset (i.e., SCARECROW) is suitable for this study.",
            "6": "In addition, the reliability of the NLI classifier is also a concern.",
            "7": "Specifically, how many examples are there that cannot be categorized into any of the three categories?",
            "8": "How many of them are misclassified as 'neutral'?",
            "9": "This matters because a major claim of this paper is that 'neutral' is an indicator of high quality."
        }
    },
    "6jik3wCbTr": {
        "twIB49FJil": {
            "0": "1.",
            "1": "Data imbalance and representation degeneration are two important problems to address for multilingual translation.",
            "2": "2.",
            "3": "Experiments are thoroughly done in this paper and demonstrate the effectiveness of Bi-ACL framework.",
            "4": "1.",
            "5": "The method section (Section 3) is not easy to follow.",
            "6": "In particular, I have a hard time understanding how contrastive learning is performed and why the positive (or negative) example is considered positive (or negative).",
            "7": "The notations are confusing.",
            "8": "Please refer to my questions for more details."
        },
        "m9k1hsfYye": {
            "0": "- Although BLEU score is low, but this is a very low resource language, where baseline model BLEU is close to 0.",
            "1": "Experiments showed respectable gains across low, medium, and high resource language.",
            "2": "This showed robustness of the method.",
            "3": "- Works on tail very low resource language is not common.",
            "4": "The paper raises the bar of these languages.",
            "5": "- The intuition behind contrastive learning is hard to follow.",
            "6": "The paper jumps into explaining the technical details of the method before explaining the intution behind it."
        },
        "8EXUfQ75ll": {
            "0": "- the proposed approach is novel;\n- the paper is well written;\n\n- the BLEU scores reported in Table 1 are very low, so it may not be very clear how much the BLEU improvements really imply;"
        }
    },
    "6eBgIRnlGA": {
        "XTm33BqnA5": {
            "0": "- The problem the submission tackles, i.e., temporal misalignment between LM training and use, is highly relevant.",
            "1": "Of course, one paper alone will not solve this problem, but I think it makes a valuable contribution.",
            "2": "- The proposed task, fact duration prediction, is well-defined, feasible, and looks like it could have other uses besides those described in the submission.",
            "3": "- While experiments on more than the two models used in the submission would have made a stronger case, the experiments look like a solid proof-of-concept.",
            "4": "The different ways of using the temporal misalignment prediction for improving calibration (showcased in the second half of the paper) appear very useful.",
            "5": "- The submission is well-written overall and easy to understand.",
            "6": "- none"
        },
        "EWQ6B60Kxa": {
            "0": "This is an interesting and somewhat overlooked problem in pretrained language models.",
            "1": "The distantly supervised model shows moderate performance in fact duration prediction, showing there is a large room for improvement, which can encourage more research in this area.",
            "2": "The paper's exploration of temporal degradation in relation to QA model calibration should be commended.",
            "3": "The scope of the problem is limited, and while LLMs do not entirely resolve the problem, LLMs are able to abstain from answers with potential temporal misalignment."
        },
        "c7dwEiNAre": {
            "0": "* The paper addresses an interesting problem from a new perspective \n* The proposed solution can help make QA-systems more robust by making them more \"aware\" of the validity of their answers with respect to time.",
            "1": "*  Several scenarios are considered in the evaluation\n\nNA"
        }
    },
    "P9V2jcotAF": {
        "X5s72hh9Nb": {
            "0": "The model is based on an intuitive idea, that word similarity should be transitive, and the improvements are large (2+ BLEU).",
            "1": "I have some hesitation about including a word-similarity graph in an MT system, for two reasons.",
            "2": "The first is that the idea that the semantic similarity of words across languages can be quantified is complicated by a few facts:\n- Because of polysemy, some words have multiple meanings\n- Because of subword segmentation (line 159), some \"words\" do not have meanings at all\n\nThe second may be merely aesthetic; one of the benefits of neural MT over phrase-based MT was that it was a single end-to-end model, without separate stages for things like word alignment.",
            "3": "This work brings back a separate word-alignment stage.",
            "4": "Not reasons to reject, but weaknesses:\n- Please state earlier (line 187) what word alignment models are used, since this is part of your model.",
            "5": "Also, please state explicitly what training data is used for the word alignment models.",
            "6": "- BLEU should be computed on untokenized references (e.g., using SacreBLEU), not tokenized references.",
            "7": "- Another metric like BLEU would be helpful."
        },
        "Uu6ZC7TNFj": {
            "0": "1.",
            "1": "The research question addressed by this paper is a valid and important one in the context of multilingual translation models.",
            "2": "2.",
            "3": "All experiments showed improvements over the baseline, both in terms of translation quality and word similarity benchmark.",
            "4": "3.",
            "5": "The presentation of the paper is mostly clear and easy to follow.",
            "6": "1.",
            "7": "My biggest concern of the paper is its evaluation.",
            "8": "(1) The paper only reported tokenized BLEU, which has been shown in Post (2018) (https://aclanthology.org/W18-6319.pdf) to be very hard to reproduce.",
            "9": "Plus, the findings of WMT 2022 metrics shared task (https://www.statmt.org/wmt22/pdf/2022.wmt-1.2.pdf) also suggested against using BLEU as the only metric for evaluation.",
            "10": "I know the authors probably chose to use the tokenized BLEU in order to keep consistent with Lin et al.",
            "11": "(2021), which also reported tokenized BLEU.",
            "12": "Yet, I would still strongly recommend using sacreBLEU and also report at least one of BLEURT or COMET.",
            "13": "If it is hard to get those numbers for the models in Lin et al.",
            "14": "(2021), I would rather drop that baseline.",
            "15": "2.",
            "16": "While I understand the importance of the research question, I don't fully agree with the necessity or the benefit to use GNN for that.",
            "17": "To me, there are simpler ways to inject such information, such as random substitutions (https://aclanthology.org/2020.emnlp-main.210.pdf) or contrastive learning (https://aclanthology.org/2021.acl-long.21.pdf).",
            "18": "Have the authors considered those alternatives?",
            "19": "Has there been any effort to verify that GNN is the better method?",
            "20": "3.",
            "21": "Lots of important related work is omitted, which I'll elaborate on in the missing references section."
        },
        "ax7qjTHFl3": {
            "0": "- The approach is well thought out; on a personal note, I can also say it is likely to be useful in my future work\n- The experiments demonstrate the effectiveness and efficiency of the approach, with a clear improvement in BLEU scores and a low memory footprint.",
            "1": "**A/** there a few methodological points that could be better handled.",
            "2": "In particular:\n\n**A1/** the authors only report 1 seed (as far as I can tell)\n\n**A2/** the baselines do not include comparable methods, and in particular other embedding alignment approaches; e.g., Aji et al (2020) (cited by the authors)\n\n**B/** While the method is proven to be effective, I am not completely sold on the narrative around it; i.e., that it is effective for the reasons suggested by the authors.",
            "3": "This boils down to a few points of comparison that would be valuable, but are regrettably missing.",
            "4": "**B1/** One question that specifically comes to mind is that of the relative importance of the extra embedding weights, vs. that of the implicit random walk in the alignment graph.",
            "5": "As it currently stands, the authors do not disentangle these two factors.",
            "6": "Throughout the paper, the authors seem to attribute the success of their approach to the former, but do not control for the latter (though in fairness they do remark that the added parameters correspond to a fraction (2%) of the total embeddings).",
            "7": "**B2/** Another area where I'm uncertain what to make of the authors' claim concerns the question of the semantic similarity of items with high weighted edges in $G$.",
            "8": "As the authors remark (line 195 sq.",
            "9": "):\n> [The graph definition] _is based on the intuition that when a pair of aligned words frequently co-occur, they 1) have higher confidence as equivalent words, and 2) the knowledge sharing between these two will benefit more context during training._\n\n As such the graph $G$ need not only describe equivalent word classes.",
            "10": "Or put another way: it is plausible that the improvements that this method obtains are mostly due to point 2) in the citation above, rather than point 1).",
            "11": "**B3/** Lastly, it would have been interesting to verify the usefulness of the proposal on bilingual models., as such models constitute a useful point of reference.",
            "12": "Reading from line 110&mdash;111:\n> _our method adapts to massive language pairs and a large vocabulary._\n\nIt is unclear to me (at least unproven) that the method actually fares well on smaller settings.",
            "13": "**Edit after rebuttal:** The authors' rebuttal provided a supplementary set of results that alleviates my concerns as per this last point."
        },
        "5UmdUT3t0F": {
            "0": "I like this paper and I like to see it accepted at this conference.",
            "1": "- Although using GNNs over word alignments to improve the quality of representations has been proposed before (imani-etal-2022-graph), using it in an end-to-end fashion in an MNMT model is novel and could be used more in the community.",
            "2": "- The experiments include several language pairs and show the generalizability of the proposed model.",
            "3": "I like the fact that Table 2 has different sections for high, medium, and low resource performance.",
            "4": "- Including the ablation studies and the additional analysis makes this work more valuable.",
            "5": "- The authors include as many technical details as possible that facilitate to reproduce the results.",
            "6": "They also claim to release the codes upon acceptance of the paper.",
            "7": "- Table 4, 5: It is established to use Precision@k (commonly 1) to check the quality of Bilingual Lexicon Induction.",
            "8": "Using the MUSE dataset with other metrics could be misleading.",
            "9": "Although we know the quality of translations has improved, it could be the case that the nearest neighbors of each word could change during the graph transformation.",
            "10": "Looking at average similarity is not a good metric to check this.",
            "11": "- Section 5.2: Adding more pairs that are extracted from word alignments made these results questionable.",
            "12": "In a way, the word alignment model might have a bias on the graph model to make specific types of words closer to each other (for example only nouns).",
            "13": "Testing the outputs with datasets extracted from the same word aligners is not valid.",
            "14": "- The experiments don't include baselines that use multilingual language models as encoders.",
            "15": "It might be the case that a multilingual LM that only uses monolingual data performs better than the proposed method."
        }
    },
    "TKGgLVYRqJ": {
        "wxTynY8PoU": {
            "0": "The paper is thorough, upfront about the assumptions it needs to make, why they need to be made, and what consequences they may have on results.",
            "1": "The mathematical specifications are dense but overall clear also to a non-specialist like me.",
            "2": "The results are interesting and contribute to present the methodology as an interesting tool to study diachronic phenomena in language, potentially offering other researchers an interesting springboard for future studies.",
            "3": "The limitations section is very thorough and insightful, complementing the results presented in the main paper in important ways.",
            "4": "The paper is very dense in its presentation of the methodology and goes over the results rather quickly.",
            "5": "The discussion is also not particularly insightful, and does not offer a very strong explanation for the tension between the observation that frequency is positively correlated with polysemy under a synchronic perspective and the empirical finding that sense differentiation tends to be stronger for low frequency words.",
            "6": "The account offered by the authors (low frequency words from the past may become more frequent because of sense differentiation) would suggest that over time different word types should be frequent, but the correlation between frequency estimates over time seems to contradict this.",
            "7": "While I don't think a surprising result should justify rejection, I think more care is warranted in interpreting these results than shown in the discussion."
        },
        "E43gbPoRoy": {
            "0": "The paper is generally well-structured and the topic (while not traditionally in the purview of NLP) may be of interest to audience at the conference.",
            "1": "As noted above I have concerns about the status of the phenomenon (polysemy) being modeled.",
            "2": "About how distinguishable different causes of polysemy really are from one another, and whether the present work is really separable from the kind of correlational analysis they mention."
        },
        "wwfYiQnK1p": {
            "0": "The theoretical- formal - approach is very clearly defined, and the topics (diachronic lexical change and polysemy) are important for a better understanding of language structure and use.",
            "1": "Results are very clearly and convincingly presented.",
            "2": "I am not completely sure that a formal approach, rather than a simulative one, would be of interest for the nlp community."
        }
    },
    "2wFVkTDGOZ": {
        "FWX1TURCKg": {
            "0": "The paper is well written and clearly outlines concerns surrounding the model editing methods in question.",
            "1": "The authors provide a concise summary of recent works that have developed both model editing and more post-hoc/retrieval based techniques.",
            "2": "While much of the authors' concerns re model editing seem reasonable, it is unclear what exactly they have identified or are proposing that is novel.",
            "3": "Across the papers that they cite, the main concerns raised by the authors seem to already be recognised and possible solutions are being investigated.",
            "4": "e.g.",
            "5": "Mitchell et al., 2022a highlight the problems arising after many successive edits and then actually propose a potential solution that uses an external system for storing content edits and reason over them when needed (of the category of systems championed by the authors).",
            "6": "The moon landing example given on lns 146-157 seems a bit contrived.",
            "7": "Prompting ChatGPT, for example, with the question they pose does in fact produce the answer they desire.",
            "8": "Also, much of the motivation for fact-oriented model editing is to account for clearly defined instances with a known/desired answer.",
            "9": "The fact that so many alternative methods exist and are referred to makes it somewhat difficult to see what the need for this paper is, when the issues proposed are clearly already being addressed in the ways suggested.",
            "10": "Even though the authors acknowledge it in the limitations section, the fact that this paper also contains no empirical results or examples illustrating the issues discussed doesn't do much to strengthen their argument.",
            "11": "If concrete examples of problems resulting from model editing, or some data illustrating the extent to which it has been adopted under the assumption of blanket reliability, were included it would be easier to accept the gravity of their concerns."
        },
        "kQp7FXgttU": {
            "0": "This is a spirited discussion on a timely topic, and I appreciate the forthright tone that the paper takes when assessing both the editing techniques and the alternatives that the authors prefer (which have major shortcoming as well).",
            "1": "I should say that I share the authors' overall view.",
            "2": "I too think that model editing is the wrong bet to make, for numerous reasons.",
            "3": "I favor the retrieval-augmented approaches described in section 4.",
            "4": "That said, I feel that the current paper misses the mark.",
            "5": "The argument needs to be substantially sharpened in order to have a productive impact:\n\n1.",
            "6": "The stated goal of the paper is to \"call into question the entire _idea_ of model editing\".",
            "7": "This is incredibly ambitious.",
            "8": "I would not recommend such a sweeping scope for such a short paper.",
            "9": "This would require some kind of elegant argument that these approaches cannot, even in principle, serve positive goals.",
            "10": "The paper does not offer such an argument.",
            "11": "2.",
            "12": "The paper blurs together inherent limitations of LLMs with limitations pertaining to model editing.",
            "13": "In particular, the argument in Section 3.1 is not even really about model editing.",
            "14": "It's about LLMs and/or LLM APIs.",
            "15": "For Section 3.2, the same argument could be used to argue against LLMs -- and even against the whole idea of indexing the Web and using it to learn things about the world!",
            "16": "3.",
            "17": "Search engines are vexing for the paper's arguments concerning \"factuality\".",
            "18": "All the concerns raised about factuality hold for traditional search, including the arguments in 3.1.",
            "19": "If lots of people on the Web say that Los Angeles is the capital of California, then n-gram-based search will also surface those results.",
            "20": "The issue here is conflating factuality with grounding in a database.",
            "21": "AI people should never promise factuality, but they should be able to deliver grounding or provenance.",
            "22": "4.",
            "23": "The argument in 3.2 (and the metaphor of \"emptying the ocean with a spoon\") needs to be further substantiated.",
            "24": "After all, changing a single node in a knowledge graph can have wide-ranging effects that spread automatically through the network.",
            "25": "For example, changing the name of the U.S. president could change the name of the dog owned by the president's mother's first cousin, assuming the knowledge graph has the right edges and so forth.",
            "26": "If we could train LLMs to have this kind of structure, then we would be able to change a lot of things with a small database of facts.",
            "27": "But, anyway, all of this needs to be properly benchmarked against what we have now with Web search, Wikipedia editing, etc.",
            "28": "5.",
            "29": "I appreciate that section 4, on alternative approaches, notes that these alternatives suffer from all of the major problems that the paper identifies with model editing.",
            "30": "But this is also a signal that the paper has set its sights too high.",
            "31": "I would encourage the authors to find a more focused sort of criticism and rewrite the paper accordingly.",
            "32": "As I said above, I am on their side!"
        },
        "QSteDtx09e": {
            "0": "The arguments in this paper are overall interesting and quite convincing, with adequate supporting evidence from research studies and other sources.",
            "1": "The topic of factuality in LMs is also of central importance in NLP, especially with the recent rapid developments of large LMs and the new trend of LM-based search engines.",
            "2": "I find the discussion around alternatives to model editing a bit unsatisfying overall - while the content does cover in a rather comprehensive way the possible alternative paths towards factuality in LMs (e.g., retrieval, grounding, continual training, concept erasure, etc.",
            "3": "), the discussion is overall closer to a literature survey and I don't find much critical opinions on the way forward for these alternative directions and approaches of resolving the challenges in these directions.",
            "4": "One example is the issue regarding the difficulty of attributing generations to internal parameters/external stores as discussed in line 284-304, which is arguably a problem of central importance of this topic.",
            "5": "In the end, the authors don't seem to lay out a solution forward, and it seems that these alternative paths face challenges somewhat similar to those in model editing.",
            "6": "I would expect to see more fruitful and critical thoughts & discussions around these key topics, which I believe should be present in a position paper."
        },
        "GwnbD4nC9A": {
            "0": "The paper provides a very strong case against existing model editing formulation.",
            "1": "A nice survey of method have been presented regarding existing model editing methods and problems associated with them such as fact popularity bias, catastrophic forgetting and robustness to changes.",
            "2": "The paper also briefly discusses alternate approaches aimed at mitigating the hallucination issue by decoupling the factual memory component from LLM by relying on external knowledge stores, factual knowledge bases and posthoc editing of model parameters.",
            "3": "The objective mismatch between model pretraining and desired control at inference is very well argued with suitable citations\n\nNo major weaknesses in the position paper however, though not mandatory it would have been nice to provide some more grounding for the arguments and ciatations against implausibility of model editing.",
            "4": "More theoretically grounded works that show that model editing is not posssible (like unlearning has been shown to be theoretically hard which involves model editing) and also how continual learning is hard (https://arxiv.org/abs/2006.05188) would further support the arguments better.",
            "5": "More recent citations on grounding LLM outputs might be relevant like:\n\nhttps://arxiv.org/abs/2302.12813\n\nhttps://arxiv.org/pdf/2302.02662.pdf"
        }
    },
    "PXkS70nuNp": {
        "jNIPgEJcD2": {
            "0": "1.",
            "1": "This paper provides an in-depth empirical analysis of large language models (LLMs) from the perspective of representation and functional similarity.",
            "2": "This sheds light on the internal workings of LLMs as model size increases.",
            "3": "2.",
            "4": "The paper proposes a practical training-free strategy called CRaSh that builds on these insights to improve Offsite Tuning, a method for fine-tuning LLMs without access to the full model.",
            "5": "3.",
            "6": "Experiments demonstrate CRaSh can boost Offsite Tuning performance across multiple datasets.",
            "7": "The analysis of loss landscapes provides theoretical justification for why CRaSh works well.",
            "8": "This paper builds heavily upon the Offsite-Tuning (OFT) method.",
            "9": "However, the motivation for basing the proposed approach on OFT could be clarified further.",
            "10": "In particular, explicitly discussing the advantages of OFT as a privacy-preserving tuning approach and how it enables analyzing and adapting large pretrained models without access to the full model would strengthen the paper.",
            "11": "Additionally, providing more background details on OFT would make the current work more self-contained."
        },
        "PvjrKn7Exy": {
            "0": "- The empirical results is very surprising.",
            "1": "CRaSh even wins full-finetuning for some datasets (Table 1 RACE, SciQ).",
            "2": "- The analysis on redundancy in LLMs is interesting (Figure 1).",
            "3": "- The idea is simple and intuitive.",
            "4": "- The method can effectively reduce the number of parameters to train.",
            "5": "However, it cannot reduce inference cost.",
            "6": "Hence, I don't know if it will be very effective in the long run.",
            "7": "- I want to see more analysis and ablation on the method.",
            "8": "Please check questions."
        },
        "CBirfEvNTu": {
            "0": "1.",
            "1": "The paper conducts an in-depth empirical analysis of LLMs from the perspectives of representation and functional similarity.",
            "2": "This analysis provides valuable insights into the underlying mechanisms of OFT.",
            "3": "2.",
            "4": "The proposed CRaSh strategy is novel and interesting, and the authors present compelling evidence of its effectiveness in improving the performance of OFT with billions of parameters.",
            "5": "3.",
            "6": "The paper's exploration of privacy concerns in the context of tuning publicly accessible, centralized LLMs with private instruction data is an important and interesting topic.",
            "7": "4.",
            "8": "The paper is well-structured and clearly presents complex concepts, making it accessible to readers with varying levels of familiarity with the topic.",
            "9": "This is not necessarily a reason to reject: the paper assumes that LLM providers (e.g.",
            "10": "OpenAI) would be willing to share the compressed emulators.",
            "11": "However, it does not provide a compelling argument or incentive for why they would do so, if they do not want to share the entire model.",
            "12": "Actually, another intriguing question emerges, which, while perhaps extending beyond the scope of this paper, warrants consideration: given the emulators, how challenging would it be to reconstruct the original model?"
        }
    },
    "Ynxo6lene2": {
        "TITINd7Gj7": {
            "0": "This paper demonstrates an interesting task-oriented dialog (TOD) system that follows a neuro-symbolic architecture, in which a neural component (GPT-4) converts from natural language into a probabilistic symbolic representation, which is then operated over by a dialog planning model to control the behavior of the agent towards playing OneCommon successfully.",
            "1": "This is a burgeoning field in using LLMs, in terms of grounding them in environments and ensuring that there is an interpretable and grounded representation of its belief state, leading to a SOTA result on OneCommon.",
            "2": "1.",
            "3": "The specification for how SPC converts from its partner’s natural language utterances to a symbolic representation of the world is not very clear to me.",
            "4": "This is described in Section 4.2 and Section 6 (L404-414), but there doesn’t appear to be a description of how GPT-4 is prompted to convert to the symbolic representation.",
            "5": "From what I understand, the output of GPT-4 will resemble L290-292, which is then parsed by the “partner model” mentioned in L412.",
            "6": "But there are still a lot of questions left unanswered — for example:\n\n    a.",
            "7": "What does GPT-4’s prompt look like?",
            "8": "b.",
            "9": "Are there few-shot examples in the prompt?",
            "10": "c. Is there any regulation on how GPT-4’s output is kept in a parseable format, and what rules does the parser follow?",
            "11": "d. What does the input to the partner model look like, and how is its output converted into a natural language utterance returned to the partner player?",
            "12": "e. Can you provide a step-by-step description of how a partner’s utterance is read, parsed, and fed into the partner model, and how the partner model’s output is converted into generated text?",
            "13": "Much more detail needs to be provided in SPC’s methodology, and I would find it hard to reproduce the SPC system described in the paper.",
            "14": "2.",
            "15": "The contribution that this paper makes over prior work done by Chiu et al., 2022 (https://openreview.net/pdf?id=PkHSHZLig5H) is unclear.",
            "16": "This paper cites Chiu et al., 2022 and reuses its presented dialog planner, and indeed describes the same “information gain” approach when doing dialog planning on OneCommon.",
            "17": "In particular, this paper’s Section 5.2 is identical to the theoretical decomposition described in Chiu et al.’s Section 3 (and there should be a citation here!).",
            "18": "a.",
            "19": "Am I correct in saying that the major contribution is embedding Chiu et al.’s dialog planner with a prompted GPT-4 that also feeds into the dialog planner with code generation?",
            "20": "If so, this also needs to be made more explicit, and again more detail needs to be provided on the code generation aspect (as mentioned in (1)).",
            "21": "If this is true, I'm not sure what the value of this work is with respect to prior works that also prompt large pretrained LLM's to interact with some symbolic task (e.g.",
            "22": "https://arxiv.org/abs/2210.03629)\n\n    b.",
            "23": "What is the value of code generation from GPT-4 when interfacing with the partner model?",
            "24": "Are there other ways to connect the partner model with some LM to do conversation?"
        },
        "GkNRaEZRKo": {
            "0": "The main reason to accept is that the method is effective for the proposed task, achieving near human performance across all partners, and showing that it can adapt to advanced strategies adopted by humans who are highly successful at this task.",
            "1": "Additionally, the method leverages the intrinsic ability of modern LLMs and does not require any new training.",
            "2": "The main weakness, which admittedly is also discussed as a limitation in the paper, is that the proposed method appears to only be realistically applicable to the OneCommon task, and not to more useful real-world grounding tasks.",
            "3": "In particular, the proposed method requires a hand-engineered library of primitive symbolic checks that can be efficiently computed in Python.",
            "4": "It is unclear in which real-world settings this would be possible, for a number of reasons, including the inherent ambiguity and context dependence of concepts such as \"small\" or \"large\", and the fact that such a check might be non trivial to implement for, say, a picture of physical objects rather than an artificial collection of shapes.",
            "5": "Second, the proposed method requires all possible configurations of the shared state to be efficiently enumerable, as the generated code marginalizes over them (Section 4.1).",
            "6": "In reality, this is probably intractable in all but the simplest artificial problems.",
            "7": "To be clear, as the method is effective on the stated task, this is a relatively minor weakness, but if the paper intends to show that the method is applicable to real-world tasks, then it needs an evaluation in a more realistic benchmark than just OneCommon.",
            "8": "A separate but related weakness of the paper is the choice to generate Python directly, as opposed to a dedicated DSL or a more common declarative PL such as SQL.",
            "9": "Given the code generation is the core contribution of this paper, there should be more discussion of what exactly is the expressive power of the generated code, and how this contribution is different from previous work in semantic parsing and dialogue state tracking.",
            "10": "After rebuttal: Thanks for addressing all comments.",
            "11": "I agree that extending the proposed approach to probabilistic programs rather than deterministic would greatly increase the applicability of the method.",
            "12": "I also understand that such an extension could be out of scope for this work.",
            "13": "I would recommend including such discussion, as well as well discussion of prompts and choice of target language, in the final version.",
            "14": "Overall, I am satisfied that the concerns can be addressed in the final revision, and I updated my score."
        },
        "SBMWVcoqdu": {
            "0": "- The proposed method goes seamlessly between language space and symbolic space while utilizing an LLM.",
            "1": "Performing the planning/reasoning explicitly in symbolic space, maximizing an information gain objective, is more effective than planning performed in language space as previous work has done when employing language models for this task.",
            "2": "- A lot of illustrative examples and diagrams that make the understanding of the task and the method easy.",
            "3": "- Reducing the code generated by the LLM down to just the set of the necessary predicates representing the essential information, in order to speed up the generation.",
            "4": "- Outperforming the previous SOTA approach by a large margin in a human evaluation (69% vs. 56% success rate).",
            "5": "- The performance of SPC actually steadily improves with the increasing skill of the human partner (which is the opposite of the previous SOTA method's behavior).",
            "6": "- There is no description whatsoever of how the LLM is prompted to produce the code fragments.",
            "7": "Presumably, some examples are provided, as the LLM would probably not do a good job in a zero-shot setting.",
            "8": "However, we don't know how many examples were provided in the prompt, how complex they were, and whether it was always the same set of examples or varied based on the context.",
            "9": "The choice of examples in the prompt probably also affects the accuracy, but we don't know to what extent.",
            "10": "- The reading phase (~ 5 seconds on average) is still rather slow for real-time application.",
            "11": "- The median number of words per utterance by human partners interacting with SPC is merely a third of the median of the utterances in human-human interactions (4 vs. 14 words).",
            "12": "While this does not necessarily mean anything bad, the large difference makes me wonder if the interaction with SPC feels very unnatural.",
            "13": "Perhaps SPC is too controlling the way it asks questions (perhaps with most humans simply responding \"yes\" or \"no\") and could benefit from an improved response generation to allow for more user initiaitve."
        }
    },
    "9LPJK81xy1": {
        "8ihd4ge5Sr": {
            "0": "The main reasons to accept:\n\n1.",
            "1": "There is little work done on multi-session conversation history in open-domain conversion.",
            "2": "The dataset proposed in this work could be very helpful for further studies.",
            "3": "2.",
            "4": "Dataset is carefully collected in large-scale and it is interesting to see another case where LLM can help reduce the cost of dataset collection.",
            "5": "3.",
            "6": "Paper is well written and presented.",
            "7": "The main reasons to reject:\n\nSome details are missing:\n1.",
            "8": "Despite the fact that LLM is powerful at generation, we should still be careful about the actual outputs.",
            "9": "Is there any mechanism of quality control or verification in LLM (i.e.",
            "10": "ChatGPT) outputs?",
            "11": "2.",
            "12": "Not clear about the model performance.",
            "13": "Although the section 5.6 mentions that MSC 2.7B is used for comparison, there there is no human evaluation reported for MSC 2.7B."
        },
        "1WF91iYl1X": {
            "0": "1.",
            "1": "The paper addressed an important problem in dialogue system and would be of interest to EMNLP audience\n2.",
            "2": "The paper provided a new machine-generated corpus that is provably better than human-generated corpus in certain criteria\n\n1.",
            "3": "The paper contains modeling work only at a bare minimum.",
            "4": "The paper would be stronger if it positions itself as a resource paper, and includes more analysis on data generation.",
            "5": "For example, it would be interesting to see the impact of prompt engineering on the generated corpus.",
            "6": "2.",
            "7": "In the post-processing of the LLM-generated corpus, the human curation part seems weak.",
            "8": "There is no common sense filtering, nor did the authors evaluate the corpus based on common sense.",
            "9": "So the gap between this corpus and human-generated data is unclear."
        },
        "8xerRLxzba": {
            "0": "1.",
            "1": "This paper is well-written and easy to follow.",
            "2": "2.",
            "3": "The authors have constructed multi-session dialogues based on 10 pre-defined relationships between speakers, which presents a novel and intriguing approach.",
            "4": "3.",
            "5": "An extensive human evaluation was undertaken to gauge the quality of both the dataset and the generated dialogues.",
            "6": "Consequently, the Conversation Chronicles dataset and the ReBot outperformed the MSC dataset and the MSC 2.7 model, respectively.",
            "7": "1.",
            "8": "There seems to be a gap in empirical experimentation.",
            "9": "Given that each multi-session dialogue is mapped to one of the 10 predefined relationships, a detailed evaluation of dialogues generated for each specific relationship would enhance the empirical robustness of the study.",
            "10": "2.",
            "11": "How might one measure the impact of speaker relationships on the multi-session conversation dataset?",
            "12": "Specifically, if the dataset did not factor in speaker relationships, would there be a noticeable performance drop in ReBot or other long-term conversation models?",
            "13": "3.",
            "14": "Do the authors operate under the assumption that these relationships remain static throughout multi-session conversations?",
            "15": "I'm curious as to why potential shifts in relationships weren't considered.",
            "16": "For instance, in a \"Co-workers\" scenario, if one or both individuals retire (or quit), they no longer share the same professional bond.",
            "17": "In my opinion, in real-life contexts, relationships often evolve, especially over extended periods, such as several years.",
            "18": "4.",
            "19": "In the comparison between ReBot and MSC 2.7B, do the authors consider this comparison to be fair?",
            "20": "There appears to be a significant discrepancy in the number of sessions between the two.",
            "21": "Could the enhanced performance of ReBot be attributed to the more extensive training data it had access to?"
        }
    },
    "x9BmfezTvD": {
        "htZVOVgNuS": {
            "0": "- paper is well organized and easy to follow\n- very interesting analysis on \"debiasing\" seed-based pseudo-labeling    \n- experimental results are convincing\n\n- I can’t find any"
        },
        "C7pwGBX66c": {
            "0": "The novelty and the contributions of the paper are very well justified.",
            "1": "The analyses are extensive and show a clear advantage of the method over exsiting approaches, including state-of-the-art.",
            "2": "The methods are well described and easy to implement/adjust to wider range of the datasets and classification tasks.",
            "3": "The use of a sinlge classifier - BERT.",
            "4": "The results might differ if different classifiers are used."
        },
        "mUjmmV1PPR": {
            "0": "- The proposed method although simple, is very interesting and shows competitive results or even outperforms other more complicated methods.",
            "1": "- The authors provide an extensive study about how the deletion of seed words should be done and perform experiments that give insights on how the random deletion works and the decision of the deletion ratio.",
            "2": "The proposed method has lower scores than its competitors in 3 out of the five datasets used for evaluation."
        }
    },
    "JwbEwhL3VP": {
        "PxEdoqDan6": {
            "0": "This paper addresses an important problem, i.e., the efficiency of Transformers.",
            "1": "The proposed method, which applies weights to sub-layers during the inference, is an interesting approach to accelerates the inference speed.",
            "2": "This paper is inadequate to publish due to some reasons.",
            "3": "In my understanding, the claims of this paper have not been proved yet.",
            "4": "The authors varied beta during the experiments but did not report the results in fixed beta such as beta=1.",
            "5": "Thus, it is difficult to agree with the claim \"we can accelerate the inference speed by using appropriate beta.\"",
            "6": "In addition, I recommend reporting the portability of beta.",
            "7": "Can we use beta which is adjusted to a specific task for other tasks?",
            "8": "In addition, the authors should compare the proposed method with the early exit techniques.",
            "9": "These method might need extra modules and/or computational costs (especially, during the training) but the proposed method also need additional computational costs to search the appropriate beta before the inference.",
            "10": "The proposed method is more efficient in the total cost?",
            "11": "This paper is hard to follow.",
            "12": "At least, the authors should reduce the ambiguity in notations.",
            "13": "For example, in Equation (3), f_t -> f_i in my understanding.",
            "14": "In addition, the authors defined the number of layers as T in Section 2 but use L in Section 3.3.",
            "15": "Moreover, the description contains several errors.",
            "16": "For example, in Figure 4, vertical axes of (a), (b), and (c) represent perplexity but the correct representation is Accuracy.",
            "17": "Moreover, the authors mentioned figures in Appendices for the main discussion such as Section 4.3.2.",
            "18": "The authors should include them into the main part if the authors use them to support the main claims."
        },
        "Q5hmEYgN0I": {
            "0": "- There is definite merit in reducing the inference cost without the need for additional modules.",
            "1": "- The paper holds value in the presence of several insightful empirical results.",
            "2": "- It is disappointing that there is no method provided to find appropriate values for important variables such as step size or number of iterations, leaving heavy reliance on heuristics.",
            "3": "- Additionally, it would be beneficial to include an analysis of the model's scale.",
            "4": "Given the recent attention towards LLMs that demonstrate diverse generalization abilities, showing the potential effectiveness of this methodology across various scales and applicability in LLMs would enhance the significance of the research.",
            "5": "- As the authors also acknowledge in the limitations section, further validation is required for Partially-shared PLMs."
        },
        "6uhMlLHavg": {
            "0": "1.",
            "1": "Leveraging the connections of Pre-LayerNorm transformer networks to ODEs for reducing inference latency by dynamically scaling step sizes is, to the best of my knowledge, novel and of practical utility.",
            "2": "2.",
            "3": "The inference time step-size scaling results are quite compelling, especially for encoder only models\n3.",
            "4": "The low step-size training experiments are theoretically motivated, and the paper presents strong experiments validating the claims (Section 4.3.2)\n4.",
            "5": "The proposed method for partially shared PLMs is very interesting with compelling results, and demonstrates the possibility of additional future explorations.",
            "6": "5.",
            "7": "The integration with early exit strategies further demonstrates the utility of the presented techniques.",
            "8": "1.",
            "9": "Some results need additional context to better understand the implications, in my opinion.",
            "10": "Specifically, for the section on partial sharing, in addition to Figure 6, it would also be good to \n1.1 Present the upper-bound performance using a 24 layer unshared model\n1.2 Present the iteration speed, as done in Table 1.",
            "11": "Since the paper is mostly presented as a method for improving speed, an iso-parameter unshared model with lower performance somewhat deviates from the main message (since in this case, as per my understanding, the iso-parameter model would technically have lower latency).",
            "12": "While line 495 partially addresses this (w.r.t comparing the performance of a post reduction model comparing to baseline), I think the above two data-points are valuable to get a full picture.",
            "13": "2.",
            "14": "The details about how the beta values are chosen per iteration budget are somewhat fuzzy.",
            "15": "Specifically, what is the set that the beta values are tuned on ?",
            "16": "For the language model setup, where metrics are zero-shot, are the beta values tuned at a per task level, or at an aggregate level ?",
            "17": "This is important since for decoder only language models that operate on a zero-shot setup, one of the primary advantages is the task agnostic nature of the models.",
            "18": "Thus it would be good to see how robust the approach is to task agnostic tuning (eg: tuning on perplexity on the validation set of the pre-training objective), and how that might translate to zero-shot setups.",
            "19": "On a similar note, it would be good to understand how robust the approach is to different sets of beta values.",
            "20": "3.",
            "21": "Appendix G presents an analysis based on Table 5.",
            "22": "However at a perplexity of >= 35, the model performance is really bad.",
            "23": "So any conclusions / trends based on that might not be very robust."
        }
    },
    "vg55TCMjbC": {
        "9YLgjnbli0": {
            "0": "The construction method of the dataset is reasonable.",
            "1": "The experimental results are rich and a detailed analysis has been conducted on the performance of LLM.",
            "2": "There is no comparison with traditional ML methods like BERT or T5.",
            "3": "How does traditional machine learning perform on this issue?",
            "4": "I think it should be reported."
        },
        "3QmeHcqug2": {
            "0": "1) A new multi-modal benchmark for testing visually grounded common sense norms that are defeasible.",
            "1": "2) Study of comparative performance of a set of architecture based on Textual Language models, Socratic models (VLM + LM) and VLM both on classification of judgement and power of explain ability\n3) Multimodal grounded reasoning data generation pipeline with set of tuning and filtration processes and two tests on alignments (High and Medium alignment)\n\n\n1) Not enough discussion on the results.",
            "2": "It appeared the SM with a VLM model with GPT4 can outperform others.",
            "3": "This may be due to GPT4 inherent capability of text grounded reasoning if vision alignment is good enough."
        },
        "FNS5fnjdfz": {
            "0": "- This dataset is a good testbed for the commonsense reasoning abilities of black-box LLMs/VLMs.",
            "1": "- This dataset focuses on social norms/morals which are not heavily covered by prior work (e.g, VCR).",
            "2": "- The data construction process and quality control stages look reasonable.",
            "3": "- This paper is well written and easy to follow.",
            "4": "- Although the situations are checked by human annotators, the seed situations are generated by ChatGPT.",
            "5": "The coverage of situation types might be limited.",
            "6": "- The types of situations/social norms (e.g., physical/psychological safety) are not clear in the main paper.",
            "7": "- It’s a bit hard to interpret precision on NormLens-MA, where the different labels could be considered as gold."
        }
    },
    "L8W6RyMRmL": {
        "T7iV1HfrUe": {
            "0": "1.",
            "1": "Addresses the important challenge of designing labor-efficient evaluation methods for conversational systems.",
            "2": "Reducing human annotation effort would have significant practical impact.",
            "3": "2.",
            "4": "Proposes a novel approach by combining ideas from active learning and human-AI collaboration.",
            "5": "Adaptation of active testing for CIR is innovative.",
            "6": "3.",
            "7": "Comprehensive empirical analysis on multiple datasets and CIR tasks demonstrates the effectiveness over baselines.",
            "8": "The method consistently approximates full human evaluation with minimal labor.",
            "9": "The problem is timely and the solution is novel.",
            "10": "The approach could inspire more work on human-machine collaboration for efficient system evaluation.",
            "11": "The weaknesses are minor compared to the contributions.",
            "12": "One limitation is the need for a differentiable evaluation metric to estimate sample hardness (relevance scores addressed via ChatGPT).",
            "13": "The authors could do more error analysis and discuss scenarios where the approach may not work as well.",
            "14": "But overall the claims seem properly supported."
        },
        "8DGRtUOBJT": {
            "0": "The paper studies an important and timely problem of evaluating model generations with a limited budget for human annotations.",
            "1": "Evaluations are conducted on multiple tasks such as Conversational Question Answering and generation of Clarifying Questions.",
            "2": "Experimental results demonstrate they improve over other baselines for selecting hard samples.",
            "3": "The proposed method has limited novelty and appears incremental compared to HMCEval.",
            "4": "Additionally, treating each sample equally for human labor cost might be a strong assumption.",
            "5": "Would also recommend to validate the performance of the  approach on few other conversational datasets at least."
        },
        "06gTNl0XmW": {
            "0": "- Detailed experiments on three CIR tasks with promising improvement over baselines\n- first approach at leveraging this type of active testing technique for CIR tasks\n\n- The methods section (section 4) was a bit confusing and this may have affected my ability to judge the soundness of the approach.",
            "1": "Some of the variables and equations need to be explained more fully.",
            "2": "In particular, I think there needs to be more explanation given about the usage of the surrogate model.",
            "3": "A few running examples would probably help clarify this.",
            "4": "- I'm also less familiar with the evaluation metrics that are being used here (the stability and consistency ones) and I think that they should be explained a bit more.",
            "5": "Authors could provide references to other works that use these or they could just explain them in more depth.",
            "6": "- How sensitive are the final humcoe scores to the choice of surrogate model g?",
            "7": "I would like to see more discussion or experiments showing how using different models for g (with the same model for f) causes scores to vary."
        },
        "B5qLCs2kQ7": {
            "0": "1.",
            "1": "The problem is interesting and important, which can reduce human labor for the CIR that is considered as the next generation of search engine.",
            "2": "The task definition and evaluation methods are clear.",
            "3": "2.",
            "4": "The proposed method is valid and promising, and the experimental results are effective and sufficient, showing the better results than existing studies.",
            "5": "The chosen tasks are various enough.",
            "6": "3.",
            "7": "The paper structure is good and easy to follow.",
            "8": "The implementation details are clear and the analysis are sufficient to draw the conclusion.",
            "9": "1.",
            "10": "The differences between the proposed Human-Machine Collaboration annotation approach and the traditional human annotated approach is unclear.",
            "11": "I suggest to add more description and the statistic information by comparing both, which can obviously view the advantage of  Human-Machine Collaboration annotation approach.",
            "12": "2.",
            "13": "The motivation of the method design could be more clear, and explain why it is special for CIR system rather than the traditional IR system.",
            "14": "3.",
            "15": "The reason for the chosen datasets and the models are unclear, since they are not directly align to the CIR tasks.",
            "16": "Please also carefully answer the questions for the authors."
        }
    },
    "iMnwXQemEr": {
        "L3olLuWWwI": {
            "0": "1.",
            "1": "This paper has some good findings that pretrained words embedding can be expressed as a composition of a few intrinsic interpretable axes and that remain consistent across different languages, algorithms, and modalities.",
            "2": "2.",
            "3": "The paper is well written and provide many experimental results to validate their assumption.",
            "4": "This findings may be useful for NLP model design.",
            "5": "1.",
            "6": "The pretrained word embeddings are generally coming from complex nonlinear neural network.",
            "7": "However, this work mainly use some linear analysis to get the results, like PCA and ICA.",
            "8": "It is not well understood if those linear model are enough to analyze the nonlinear relationships among pretrained word embeddings.",
            "9": "2.",
            "10": "It is better to provide the source code for  better reproducibility.",
            "11": "3.",
            "12": "The technique contribution is limited as it mainly use PCA/ICA to do post-hot analysis."
        },
        "1visDKeRAP": {
            "0": "* Although there have been other studies that have applied ICA to word embeddings, this is the first to show that it yields embeddings that align to a significant degree across languages without explicit alignment between them (the \"universal\" claim).",
            "1": "* The study comprises an analysis of multiple text and image embedding models, including BERT, fastText, SGNS, ViT, ResMLT, Swin-S, ResNet, and RegNet.",
            "2": "* The findings regarding the degree of alignment of various independently trained embeddings under ICA, without explicit alignment steps, are somewhat surprising, and validate the \"universal\" claim.",
            "3": "* Given how much of the analysis hinges on ICA, it would have been helpful to provide more details in the *main text* about the FastICA approach.",
            "4": "For example, does it make any assumptions?",
            "5": "Is it exact and if not what sort of approximation error is incurred?",
            "6": "* Although ViT-base is extracted from CLIP which learns from both images and text (Appendix C), I think this paper may benefit from more focus on text.",
            "7": "For example, what about popular contrastively trained text embeddings like sentence-bert (SBERT)?",
            "8": "It would be surprising and interesting if the findings applied both to generatively trained and discriminatively trained representations.",
            "9": "What about the impact of model size/capacity?",
            "10": "* It is somewhat well known that, across modalities, higher dimensionality embeddings often result in significantly better downstream performance.",
            "11": "This is somewhat at odds with the conclusions of this paper that suggest that lower dimensional space is sufficient to capture the important information.",
            "12": "What's lost in the ICA project?",
            "13": "Are the embeddings less robust, for example?"
        },
        "ywnueuvz1e": {
            "0": "1.",
            "1": "The finding is interesting and strengthens the understanding of embeddings.",
            "2": "2.",
            "3": "The result in Fig.",
            "4": "6 is encouraging that embeddings can be projected to a lower dimension using ICA while preserving the performance.",
            "5": "1.",
            "6": "Clarity of the paper can be improved (see questions below).",
            "7": "2.",
            "8": "The observation on the composition of semantic axes works the best for English fasttext but is not so clear for other languages and BERT.",
            "9": "===== After Rebuttal =====\n\nThanks to the author for the clear explanations in the response.",
            "10": "All my questions are addressed.",
            "11": "Thus, I decided to raise the Soundness to 4 (strong)."
        }
    },
    "dRlYuG3bj7": {
        "YrEdPCa4Dv": {
            "0": "+ Interesting research direction\n+ Well-motivated and well-structured\n\n- The main concern is about the evaluation methodology, which is not sound and includes unclear parts, to my understanding.",
            "1": "Specifically:\n-- The main dimension of the research, that is time, is not explained.",
            "2": "Simply, how are the reviews ordered?",
            "3": "What is the time interval?",
            "4": "What is the time span of each dataset?",
            "5": "...\n-- [resolved if added to the paper] ~~Not sure I understood the \"throughput\" metric, it's relation with the work, and how it is gauging the efficacy of the proposed model.~~\n-- The logic behind mixing the dataset is confusing.",
            "6": "How such mixture produce topic drift?",
            "7": "-- No competitive baseline is used.",
            "8": "I understand that there might not many works in temporal sentiment analysis, but there are many similar research directions like in temporal recommender systems that could be adopted to benchmark the proposed method against them\n\n-- [resolved] ~~Accuracy is a legit metric in binary classification but in balance datasets.",
            "9": "I'm not sure the datasets in the paper's testbed are balanced.",
            "10": "There are well-established metrics like auc that could be used to add more reliability for the results.~~"
        },
        "gPoLxniijS": {
            "0": "1.",
            "1": "The authors research an interesting topic, that of sentiment analysis of data streams.",
            "2": "2.",
            "3": "The proposed architecture tries to address key issues of stream data, such concept drift and system latency.",
            "4": "1.",
            "5": "In the experimental results, they only compare with some standard baseline and not with the state-of-the-art in sentiment analysis on data streams\n2.",
            "6": "In the way sentiment polarities are approximated (employing sentiment lexicons and computing mean of positive vs mean of negative words), they miss the context of the review (eg a review text of the form \"Even though the restaurant was expensive and the orders were late, I enjoyed the experience\" would be classified as negative, even though it is positive)."
        },
        "k7xvxqGnJn": {
            "0": "1.",
            "1": "The paper is addressing an interesting research problem about the data draft in sentiment analysis.",
            "2": "It is of the interest of the general audience of the conference.",
            "3": "2.",
            "4": "The rationale and the design of the proposed framework is elaborated in detail.",
            "5": "3.",
            "6": "Empirical results show that the proposed method outperforms other baselines.",
            "7": "1.",
            "8": "Empirical results show that the unsupervised module is the major source of improvement in the performance.",
            "9": "The integration of the semi-supervised module can only slightly improve, or even degrade the overall performance.",
            "10": "It cannot justify the need of the semi-supervised module in the framework.",
            "11": "In particularly, without the semi-supervised module, the framework does not need any training examples.",
            "12": "2.",
            "13": "Following this, the proposed framework employs lightweight language model in the unsupervised module.",
            "14": "If a more sophisticated language model is used, it is unclear if the semi-supervised module is still needed.",
            "15": "3.",
            "16": "Authors may consider using a running example to explain the concept draft of sentiment stream.",
            "17": "The experiments use three different dataset to simulate the data stream and sentiment draft.",
            "18": "However, an example of concept draft in a single source can strengthen the need of the application."
        }
    },
    "S81zso7Imh": {
        "rRhjviWIjk": {
            "0": "While a simplistic approach overall (wisdom of the crowd), the detailed presentation of the model will be beneficial to the controllable image captioning community as a whole.",
            "1": "While some papers have looked at slightly similar approaches (as ackknowledged by the authors), this model brings togeter a number of key issues in a fluid way.",
            "2": "The human evaluation is also positive.",
            "3": "Some might argue that the approach lacks overall novelty, but I feel that this is outweighed by the quality of some of the results."
        },
        "gBy0GE1gOX": {
            "0": "Following are the strengths of the paper -\n\n1.",
            "1": "The paper is well written and easy to follow.",
            "2": "2.",
            "3": "The approach is very well thought and backed by a detailed human evaluation study which validates it.",
            "4": "3.",
            "5": "The automatic evaluation results also support that the IC3 approach significantly outperform SOTA captions.",
            "6": "Following are the weaknesses of the paper -\n\n1.",
            "7": "Since the approach uses LLM's, hence it will also suffer from its weakness like hallucinations.",
            "8": "2.",
            "9": "The model isn't controllable and fails to describe the background or contextual information.",
            "10": "3.",
            "11": "Since the model isn't controllable, it might not generalize to the real world scenarios."
        },
        "HIP6KGeuDz": {
            "0": "1.",
            "1": "Although the author didn’t compare the core indicators in the experiment, they also conducted many experiments to prove their performance from other perspectives.",
            "2": "2.",
            "3": "Compared to other single-viewpoint methods, the author's committee consensus method, although relatively simple, has good results.",
            "4": "1.",
            "5": "I personally believe that the entire article seems to have been improved and summarized from practical applications.",
            "6": "There is relatively little theoretical reasoning in the article, which does not reflect the innovation in the theoretical aspect.",
            "7": "And the content of Figure 2 is not concise and clear, it is recommended to supplement the Framework diagram to introduce the model.",
            "8": "2.",
            "9": "The biggest problem with the experiments is that although the model proposes a method based on a large model, general indicator analyses such as BLEU-1, BLEU-4, METR, and CIDEr are not shown in the experimental section for Image Captioning tasks.",
            "10": "Moreover, no BLIP has online test scores based on the MSCOCO dataset, which are not mentioned in the article.",
            "11": "At the same time, there is also a lack of performance comparison with mainstream Image Captioning models in recent years, and the baseline performance of BLIP is not the best in Image Captioning.",
            "12": "Without these key experiments and indicators, it is impossible to show the innovation and progressiveness of the methods in the field of Image Captioning.",
            "13": "3.",
            "14": "Through Rebuttal, the author provided some supplementary explanations for Weakness 2 (experimental design), but objectively speaking, the readability of this article is somewhat lacking, and some essential details are not handled carefully enough."
        },
        "5ycuWA4pyz": {
            "0": "This paper proposes a simple but exciting approach to generating fine-grained image captions using OpenAI GPT APIs.",
            "1": "They conducted detailed experiments to show the values of generated captions on both human and automated evaluations.",
            "2": "The description rating tool is potentially helpful for similar tasks.",
            "3": "The approach is prompt engineering and lacks comparison to LLM-based image captioning models such as BLIP-2."
        }
    },
    "1iQMzgmKeD": {
        "IcZIUm8gsd": {
            "0": "This paper tackle an interesting topic and present a good solution to it, the experimental results look like strong.",
            "1": "I didn't find any obvious reasons to reject this paper directly."
        },
        "W5AIXGIKLk": {
            "0": "1.",
            "1": "The authors propose a novel approach to transform non-autoregressive generation into a step-by-step denoising process, utilizing the denoising ability of a multilingual pre-trained understanding model for implementation.",
            "2": "2.",
            "3": "Authors adopt prompt-based approaches to achieve high parameter-efficiency during semantic alignment and denoising procedures.",
            "4": "3.",
            "5": "The work is solid.",
            "6": "Experiments demonstrate that the proposed method can significantly improve the performance of non-autoregressive generation with only a small number of parameters required.",
            "7": "Additionally, this method can be extended to many other generation tasks and has better zero-shot knowledge transfer capability.",
            "8": "1.",
            "9": "Some parts are not very clear, which caused me confusion:\n  a) Why are the results of mTransformer on the same language direction different between Table 1 and Table 2?",
            "10": "Why are the parameters of mTransformer not labeled in Table 1?",
            "11": "b) The author seems to have not introduced how to deal with the problem of inconsistent input and output lengths, such as how to generate a target sentence shorter or longer than the source sentence?",
            "12": "2.",
            "13": "It is not clear whether authors will release their codes and models."
        },
        "YnbKZOx8pH": {
            "0": "1.",
            "1": "The problem that this paper studies is important: how to modify encoder only models for generations as the non-autoregressive pre-train objective is hard to adapt for generation.",
            "2": "The method is simple and generalizable to various tasks since we only need to train a few set of soft prompts.",
            "3": "2.",
            "4": "The experiments show that their method can outperform simply initializing the encoder with a pre-trained model, direct training a encoder-decoder model on the task albeit training on much less parameters due to the parameter efficient fine-tuning.",
            "5": "3.",
            "6": "The generation speed is accelerated because compared to decoder only models which needs to pass the generated output many times through the model, encoder only model generates multiple tokens simultaneously and iteratively denoises them.",
            "7": "1.",
            "8": "This paper misses some important work in their discussion: The authors fail to compare with or at least cite some important work that utilizes pre-trained encoders for generation.",
            "9": "e.g.",
            "10": "BiBERT[1], which is the current state-of-the art on IWSLT 14 De-En translation.",
            "11": "The authors also compare to mBART and show that although their method underperforms, they win in generation speed and trained parameters.",
            "12": "I want to say that they should compare to doing prompt tuning on mBART as well to make it a fair comparison between number of trained parameters.",
            "13": "If it outperforms, then it strengthens their claim.",
            "14": "2.",
            "15": "Motivation Unclear: The authors mention that an important reason to use encoders for generation is because of their speed.",
            "16": "However, works [2] have demonstrated that standard encoder-decoder models can also be fast and comparable in performance by using a large encoder and a small decoder.",
            "17": "On the other hand, if the argument is that encoder models excel in NLU tasks, then using Pattern Exploit Training ,encoder-decoder models can be comparable to standard encoder models in NLU tasks [3].",
            "18": "My point is, I want to see a more motivating argument for studying how to adapt encoder models for generation aside from performance and efficiency.",
            "19": "3.",
            "20": "The paper is not very well written.",
            "21": "It does not have a clear distinction between model architecture (encoder, decoder, enc-dec) and training objective (autoregressive, auto-encoding).",
            "22": "They use autoregressive (AR) and encoders interchangeably - which should be mentioned in the paper.",
            "23": "Some of the facts need modifying: e.g.",
            "24": "the authors mention that unlike AR models which can only generate from left to right, NAR models as the advantage of generating in arbitrary order.",
            "25": "I don't think this is true since there has been work that generates with permuted orders with encoder-decoder models [4].",
            "26": "[1] BERT, mBERT, or BiBERT?",
            "27": "A Study on Contextualized Embeddings for Neural Machine Translation (Xu et al., EMNLP 2021)\n\n[2] Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation (Kasai et al., ICLR 2021)\n\n[3] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., JMLR 2020)\n\n[4] GLM: General Language Model Pretraining with Autoregressive Blank Infilling (Du et al., ACL 2022)"
        }
    },
    "mQxqo1di63": {
        "6CzNgmsIqj": {
            "0": "•This work copes with the fake news classification task, which is useful in practice.",
            "1": "•The author proposed an effective model for the fake news detection task and verified its effectiveness on the public test set \n\n•The motivation stated in the abstract is not solid enough, and the method proposed in the paper is not novel enough.",
            "2": "For the statement \" majority of these methods focus on news entity information and ignore the structured knowledge among news entities\" in the abstract, some existing methods already do this, like CompareNet[1].",
            "3": "•The comparison of the proposed method with the baseline is not fair enough.",
            "4": "The Bert-base-with-adapter used in the article is inconsistent with the Roberta used by KPL, and it is difficult to prove that KAPALM is more effective than KPL.",
            "5": "What is the motivation for Bert-base-with-adapter?",
            "6": "•The coarse-grained and fine-grained knowledge mentioned in the article is not clear enough.",
            "7": "The knowledge pruning method proposed in the article is intuitive.",
            "8": "[1].",
            "9": "Hu L, Yang T, Zhang L, et al.",
            "10": "Compare to the knowledge: Graph neural fake news detection with external knowledge[C], in ACL2021."
        },
        "UT8FMj1NCh": {
            "0": "* This is a highly relevant research area, and any work that pushes our understanding as to how to identify misinformation of any form is a step forward.",
            "1": "* The work is conducted on a well-known benchmark dataset and is therefore easily contextualisable with other work in the field.",
            "2": "* Conducting an ablation study is a strong aspect of the experimental work as it offers more nuanced insights into what components of the architecture contribute how much  to the overall performance.",
            "3": "There are a number of weaknesses in this work.",
            "4": "Among the biggest issues I consider the following: \n\n* Weak baselines: Given the chosen dataset contains much more information than the textual content I would want to see the results of this work compared to the state of the art reported in the literature that looks at the same dataset.",
            "5": "To pick just one recent example [1], the results reported (such as F1) appear to be way higher than anything in Table 2 (and that is also true for the baselines reported in [1]).",
            "6": "What is needed is a comparison against what the current state of the art is as reported in the literature (ideally reproduced to conduct significance tests where appropriate).",
            "7": "* Only a single dataset is used to explore the problem (well, it is two different parts but in the end it is one fairly specific dataset).",
            "8": "There are many more benchmark datasets for text classification (including fake news detection) that could be included to provide more confidence in the findings.",
            "9": "* There are many missing details (and no supplementary material such as code) making it impossible to replicate the work.",
            "10": "For example, unless I have overlooked it I cannot see what knowledge graphs are actually being used.",
            "11": "* There are no statistical significance tests (and terms such as \"outperform\" should therefore not be used) \n\n\n\n\n[1] Donabauer \"Exploring Fake News Detection with Heterogeneous Social Media Context Graphs\".",
            "12": "ECIR 2023."
        },
        "m18NpFwGiw": {
            "0": "The approach is novel and achieves SOTA on two datasets with full scale training, and show promising performance in the few-shot setting.",
            "1": "Lack statistical significance tests."
        }
    },
    "vU0KbvQ91x": {
        "14x0fCKM77": {
            "0": "1.",
            "1": "The paper builds upon prior research by seamlessly integrating Nonparametric Variational Information Bottleneck (NVIB) into the self-attention layers of a Transformer encoder.",
            "2": "This adaptation broadens the scope of NVIB's utility in natural language processing tasks.",
            "3": "2.",
            "4": "The authors utilize implicit reparameterisation gradients, resulting in more efficient and effective learning processes.",
            "5": "3.",
            "6": "The experiments in the paper are comprehensive, and the analyses are thorough.",
            "7": "In my view, reading this article requires prior knowledge of NVIB, which the paper does not provide a brief introduction to.",
            "8": "This increases the difficulty for readers who are not familiar with NVIB, but it might not be an issue for experts in the field."
        },
        "jfu9zmntFg": {
            "0": "1.",
            "1": "Interesting idea to use the NVIB regularization to learn \"better representations\" in a transformer model.",
            "2": "The representations can be interpreted to learn various levels of abstraction.",
            "3": "1.",
            "4": "The work does not demonstrate any practical benefits of using their method.",
            "5": "The analysis shows interesting trends in the representations, but without any experiments on downstream tasks, using either fine-tuning or prompting.",
            "6": "It is hard to imagine if the method is useful.",
            "7": "Even if the authors could not pre-train using their method, they could have applied this architecture to a supervised learning problem.",
            "8": "2.",
            "9": "A lot of the important paper details are in the appendix.",
            "10": "The paper is not sufficiently self-contained."
        },
        "YwvIMRdtIq": {
            "0": "[+] Research topic.",
            "1": "The research topic of learning different levels of abstraction in the same model is interesting.",
            "2": "[+] Experiments and evaluation.",
            "3": "The authors have conducted experiments both qualitatively and quantitatively for performance evaluation.",
            "4": "[+] Presentation.",
            "5": "The paper is easy to follow in general.",
            "6": "The detailed introduction of the experiment setup is also introduced.",
            "7": "[-] Method: Why applying NVIB to Transformer is challenging is not well-discussed.",
            "8": "This is important to demonstrate the technical contribution of this paper.",
            "9": "[-] Experiment: While the paper mentions applying NVIB to the final three layers of the Transformer, it might benefit from a more detailed explanation or exploration of why only these layers were chosen and how different configurations might impact the results.",
            "10": "[-] Baseline: The method is only compared with Transformer, while there are more advanced models for solving the same task.",
            "11": "It would be better if some other models can also be compared."
        },
        "QqGQRp0R7Z": {
            "0": "The paper is very well written.",
            "1": "Their idea and contribution is explained clearly, and the NVIB regularizer, alongside the changes made to adapt it to stacked self-attention, are well described.",
            "2": "The idea is interesting, and the results are encouraging.",
            "3": "The fact that the attention maps in higher layers better align with words, for example, is a very interesting finding, and the probing results suggest NVIB could lead to better representation learning as a whole.",
            "4": "It would perhaps be interesting to run some more experiments, in particular to see if NVIB regularization improves language modeling."
        },
        "QYUzLpmTai": {
            "0": "1) The authors propose adaptations of NVIB to self-attention and to use it for learning increasingly abstract representations at higher layers of a transformer encoder.",
            "1": "(NVIB was originally introduced for cross-attention.)",
            "2": "2) The authors presented interesting results from intrinsic evaluation of the attention distributions of such a model pretrained on Wikitext-2 and from evaluating the quality of the representation in linguistic probing and text classification.",
            "3": "3) The authors compared the robustness of the NVIB-regularized model with that of a regular transformer and found the former more robust to noisy input.",
            "4": "1) The specific design choices made in adapting the original NVIB to self-attention could benefit from more justification and discussion.",
            "5": "e.g.",
            "6": "Does it matter that the skip connection is multiplicative rather than additive?",
            "7": "Does it matter to choose a strictly increasing strength of regularization as layers get higher?",
            "8": "These could possibly be answered with ablation studies."
        },
        "a2ZqTECaJN": {
            "0": "The strengths of this paper include the introduction of a novel regularization term for character based language representation model, Nonparametric Variational Information Bottleneck (NVIB), that can learn to compress to different levels of abstraction at different layers of the same model, resulting in a more linguistically informed and robust model.",
            "1": "The paper also presents experimental results that demonstrate the effectiveness of NVIB in various NLP tasks, including classification and linguistic probing.",
            "2": "The paper has some weaknesses.",
            "3": "Firstly, the experimental section is relatively simple, and the authors could benefit from comparing their method with more baselines, including sub-word based methods, to demonstrate the benefits of their approach more convincingly.",
            "4": "This would provide a more comprehensive evaluation of the proposed model and its effectiveness in comparison to other state-of-the-art methods.",
            "5": "Secondly, the paper does not provide sufficient information on the decoding and training efficiency of the proposed model compared to sub-word based language models.",
            "6": "This is particularly important in the current era of large language models where sequence length is a crucial factor.",
            "7": "A more detailed analysis of the computational efficiency of the proposed model would be useful to understand its practicality and scalability in real-world applications.",
            "8": "Finally, the paper only evaluates the proposed model on English text, and it would be interesting to see how it performs on other languages such as Japanese and Chinese, which have a much larger character size.",
            "9": "This would provide insights into the generalizability of the proposed model and its effectiveness in capturing the linguistic properties of different languages."
        }
    },
    "dZWiI6A09u": {
        "MH3IXQsgmj": {
            "0": "Claim verification is an important topic with practical significance.",
            "1": "The proposed approach is intuitive, easy to implement, and effective.",
            "2": "The experiments show that it leads to some performance improvements.",
            "3": "Furthermore, the paper is well written and provides a good background for the problem statement.",
            "4": "Weaknesses: \n\nThe reason behind first creating an FOL predicate for each sub-claim is not very convincing.",
            "5": "Since they are not using any logical reasoning tools like CLINGO for solving the predicate.",
            "6": "I don’t see a justification for doing this.",
            "7": "Instead, directly instructing the LLMs with a prompt as simple as “Generate questions verifying the sub-claims of the above claim” does very well in directly generating sub-questions that can be used for verifying the claims.",
            "8": "For Chain-of-Thought and Self-Ask baselines, do you provide the retrieved knowledge to verify the sub-claims as done with FOLK?",
            "9": "If yes, then how would you justify the better performance of your approach since both these approaches also result in accurate questions for verifying the subclaims.",
            "10": "In no, then verifying each sub-claim (decomposed by existing methods) leveraging the retrieved knowledge should be a fair baseline in comparison to the proposed approach that uses the retrieved knowledge.",
            "11": "Furthermore, since creating FOL predicates is not necessarily a trivial task (atleast more complicated than prompting it to decompose the given claim), an error in creating FOL predicates can propagate in generating the followup questions.",
            "12": "On the other hand, directly generating the decomposed sub-claims has been shown to work really well in prior works.",
            "13": "Discrepancies in the presented results of ProgramFC approach.",
            "14": "As reported in the ProgramFC paper, it achieves 60.63 on HoVER 3-hop (as compared to 54.80 of FOLK) and similarly for the FEVEROUS dataset.",
            "15": "However, this paper reports 51.04 on the HoVER 3-hop.",
            "16": "Please provide a justification for this.",
            "17": "A human study on how well the verification process leveraging only the top-1 search result is required.",
            "18": "Also, why not 5 or maybe more search results are used as the knowledge because the first result may not always contain sufficient information to verify the sub-claim.",
            "19": "Analyzing the relation between correct claim verification decision and the correct generated explanation can be added in the analysis section.",
            "20": "A lot of emphasis in the paper has been on generating explanations; however, only 30 examples have been evaluated which may not provide enough evidence to support the claims of the paper."
        },
        "i6XoboxKwM": {
            "0": "1.",
            "1": "The method proposed by the authors is interesting.",
            "2": "2.",
            "3": "Experiments on three datasets (HoVER, FEVEROUS, and SciFact-Open) shows the effectiveness of this paper.",
            "4": "1.",
            "5": "What about the efficiency of this method?",
            "6": "Does calling a search engine cost much more time?",
            "7": "2.",
            "8": "The baseline of this paper is unfairly compared.",
            "9": "Most baselines do not incorporate a search engine and consequently are not conducted in an open book setting, including \"Direct,\" \"CoT,\" and \"Self-Ask.\"",
            "10": "In contrast, the prompt method proposed by the authors uses a search engine to get nearly perfect knowledge.",
            "11": "Such a comparison makes the experimental results highly doubtful.",
            "12": "In my understanding, tool-using works such as Toolformer[1] are compatible with CoT and Self-Ask.",
            "13": "The authors should add retrieved knowledge to those methods too.",
            "14": "Moreover, the authors should split the main result in Table 2 into more tables with different settings, such as one table for models in the open book setting and one for the closed setting.",
            "15": "3.",
            "16": "The ablation study is weak.",
            "17": "Authors only show the performance when Knowledge Grounding (search engine) is removed.",
            "18": "However, I am curious why authors must use the first-order-logic form of atomic claims that must be checked.",
            "19": "Can you use a free text form, like what ToolFormer[1] does, to call API?",
            "20": "Can using the first-order logic give more performance improvement?",
            "21": "4.",
            "22": "The motivations in the Abstract and Introduction don't make sense.",
            "23": "There are two motivations why the authors design this method: 1) how to verify claims without relying on human-annotated data is still unresolved 2) previous models cannot provide comprehensive explanations that can justify their decisions.",
            "24": "I don't see how the prompt method solves these two problems.",
            "25": "Authors should notice that it is the LLMs themselves who solve these two questions, not the prompt method in this paper.",
            "26": "The in-context learning or zero-shot ability is emergent when we scale up LMs.",
            "27": "The ability to generate fluent and logically correct explanations emerges when we scale up the LMs.",
            "28": "5.",
            "29": "A writing tip: you need to explain what \"Followup Question\" and \"Grounded Answer\" are in Figure 1.",
            "30": "It is better to be self-contained.",
            "31": "Ref:\n[1] Schick, Timo, et al.",
            "32": "\"Toolformer: Language models can teach themselves to use tools.\"",
            "33": "arXiv preprint arXiv:2302.04761 (2023)."
        },
        "Cfbt7fI1Qx": {
            "0": "Introduces a simple, but potentially effective, version of LLM prompting which can be of practical use.",
            "1": "Evidence for how LLMs can be helped by forcing it to \"think\" in more structured terms, such as FOL predicates.",
            "2": "Shows the effectiveness of generating simple enough intermediate questions that can be directly answered by a Google search.",
            "3": "It's not clear to what extent this is \"just\" a prompt engineering exercise vs more profound insights from prompting for FOL predicates.",
            "4": "One possible additional experiment might be to use the follow-up questions from FOLK in the self-ask (or CoT) prompts, to see if the performance difference is mostly from FOLK generating better follow-up questions or the predicates also playing a role in constructing the answer."
        }
    },
    "IZzZnp7IUs": {
        "6id96ZqkpM": {
            "0": "1.",
            "1": "The mutual adaptation of knowledge generation and grounding is an important research problem as it helps reveal the reasoning process of an otherwise blackbox model.",
            "2": "2.",
            "3": "The proposed training objectives and iterative training process are both novel and intuitive.",
            "4": "3.",
            "5": "The work carefully examines the method on a wide range of tasks, demonstrating the generalization of the method.",
            "6": "1.",
            "7": "It is unclear how the standard supervision would ensure the model indeed learns to leverage the knowledge instead of ignoring it.",
            "8": "2.",
            "9": "It seems the RL part of the work is largely based on the cited work Rainier, which limits the novelty of the work."
        },
        "v2oDttH5Fe": {
            "0": "The paper is extremely thorough with its experiments, including the diversity in datasets.",
            "1": "The motivation for interdependence on the knowledge introspection with the answer generation is very relevant to current research directions on faithfulness of generated explanations with their answers.",
            "2": "The human evaluations also further reinforce the association between the generated introspections with the label.",
            "3": "RR1 Comparisons with some more fundamental baselines - While the paper compares Crystal to CoT distilled approaches, another line of close work in the introspect then predict paradigm are retrieval augmented generators.",
            "4": "Furthermore, it would be good to compare Crystal with standard CoT approaches."
        },
        "q9Q6BU8yWh": {
            "0": "- Interesting problem\n- Solid results\n\n- Missing details in the evaluation\n- Unclear baseline comparison"
        }
    },
    "iammae3CbG": {
        "oh0DXzbUwN": {
            "0": "1.",
            "1": "The paper is well organized and easy to follow.",
            "2": "2.",
            "3": "The paper presents a new approach to addresse an important issue in multi-task learning.",
            "4": "3.",
            "5": "The paper conducts extensive experiments to demonstrate the effectiveness of the proposed method.",
            "6": "1.",
            "7": "The novelty of the methods is limited, for example, the methods (e.g.",
            "8": "InfoNCE) used in this paper are relatively mature technologies in other fields.",
            "9": "However, considering the different application scenarios, this combination is also acceptable.",
            "10": "2.",
            "11": "The experiment was insufficient, for example, the authors should test the significance of their experimental results.",
            "12": "In addition, the current experiment only involves the NLU task, and testing on the NLG task would be useful, based on different backbone models."
        },
        "0UB2hQ6nUs": {
            "0": "The paper is well-written and easy to follow.",
            "1": "The proposed methods are well-motivated and are shown to be the most efficient ones compared to other methods.",
            "2": "The experimental studies are comprehensive and the proposed methods are evaluated on important benchmark datasets.",
            "3": "The novelties of the proposed methods are not strong.",
            "4": "Most used techniques have been proposed in previous works like INFONCE and HyperNetworks.",
            "5": "Also, 3.3 is more like a description of how to apply the proposed methods in multi-task settings."
        },
        "en746Mx32h": {
            "0": "A.",
            "1": "This paper presents a rather concise approach to tackle the problem of multi-task adaptation for LMs.",
            "2": "B.",
            "3": "The PHA method described in this paper achieves clear improvements with fewer tunable parameters compared to previous methods.",
            "4": "Weaknesses:\nA.",
            "5": "According to Figure 1, the method proposed in this paper is applicable to encoder-decoder-like LMs.",
            "6": "The performance of the approach is unpredictable when applied to language models with other architecture, e.g.",
            "7": "decoder-only LMs.",
            "8": "B.",
            "9": "For Table 1, the improvement on the SuperGLUE benchmark looks marginal.",
            "10": "The improvement on GLUE is larger, even though its absolute number is higher than SuperGLUE.",
            "11": "Do the authors have an explanation of why it is the case?",
            "12": "C. The motivation for introducing prototype for target tasks need further explanation.",
            "13": "For example, whether it will work if we just replace the $k_i$ with the mean instance embeddings of the i-th task?",
            "14": "The relation between task embeddings and instance embeddings needs further explanation.",
            "15": "D. The experimental details for each setting need to be clarified.",
            "16": "I can possibly guess that the results in Table 1 are obtained by training an 8-task adaptation for GLUE and a 4-task adaptation for SuperGLUE, and the results in Table 2 are obtained by training an 8-task adaptation for GLUE and fine-tuned with few-shot samples from BoolQ, CB, SciTail.",
            "17": "But the explanation for these settings is obscure in the paper."
        },
        "1MrTJl7sDF": {
            "0": "1.",
            "1": "Strong motivation.",
            "2": "Existing hypernetwork approaches train task embeddings and hypernetwork end to end so there is no inductive bias that disentangles the task specific knowledge encoded by the embedding and task-shared knowledge encoded by the hypernetwork.",
            "3": "2.",
            "4": "Strong ablation experiments, in particular the ablation on the instance dense retriever module and the t-sne visualization.",
            "5": "I was curious about the necessity of a retriever and its associated loss, as one might consider optimizing equation 5 directly using instance embeddings to learn prototypes.",
            "6": "Yet, the ablation results demonstrate that an additional loss to align task samples in the retrieval space can enhance performance.",
            "7": "The visualization also indicates that without the retriever loss, samples from various tasks can become intertwined.",
            "8": "3.",
            "9": "Strong few-shot results which is comparable/better compared to methods such as ATTEMPT and MPT that pre-train source task prompts.",
            "10": "The paper does not compare their approach with methods such as ATTEMPT and MPT that directly train prompts on source tasks."
        }
    },
    "2U9hDBaOCn": {
        "IzSlUaJg0J": {
            "0": "- The analysis is insightful, I especially like the wide array of tasks and experiments covered and there is substantial evidence that the findings are robust across tasks.",
            "1": "- The paper is furthermore well written and follows a straightforward structure.",
            "2": "- I like the general idea of the analysis, though I feel the insights are not substantial enough for a long paper.",
            "3": "For example, insights into the sizes of generalist data are lacking.",
            "4": "What are the implications of performance on target tasks with different sizes of generalist data?",
            "5": "Figure 3 gives some very brief insights, but the paper would improve given some extra insights for generalist data.",
            "6": "I give several suggestions for making more space for in the section \"Typos Grammar Style And Presentation Improvements\".",
            "7": "- Overall, I feel the paper shows a rather trivial finding: More generalist + specialist data is better.",
            "8": "It doesn't serve insights into the limitations of generalist/specialist data (i.e., convergence) and is not mentioned in the Limitations section.",
            "9": "- The authors mention a systematic guide in their contributions.",
            "10": "However, there are insufficient details in the paper of such a guide.",
            "11": "There is a list of findings from the analysis, but it does not give a systematic guide."
        },
        "wzOVmoqSS7": {
            "0": "This work addresses a long investigated and still important question in transfer learning, which is more prominent in the present with the increasing popularity of \"generalist\" instruction tuning datasets.",
            "1": "The datasets chosen actively used in research and for tuning of current highly capable models.",
            "2": "The experiments run the mixture of training datasets are of the right format to answer these questions.",
            "3": "In general, I find Experiments II to be much weaker than Experiment I.",
            "4": "The authors use a single task for each attribute they are examining, and appear to draw far too strong conclusions for the experiments they have run.",
            "5": "A similar issue of over-extrapolation is found in the section L464-495, where there is a hypothesis that the reason for the difference in performance across datasets is due to being human-authored vs. machine-generated, but they supplement this experiment with a single experiment on an additional pair of datasets.",
            "6": "Given that all four training sets are highly esoteric and greatly different, I do not believe there is any basis to make this claim at this point, and I would recommend the authors heavily soften the claim.",
            "7": "(I acknowledge that the authors are already fairly conservative in their wording (\"hypothesize\", \"are consistent with\", \"suggests\"), but I nevertheless think that they need to water it down further.)"
        },
        "CvQw5kPpbs": {
            "0": "- The paper is timely and relevant to much of the NLP community which is adapting to instruction data and LLMs\n- The work is relatively easy to read and includes lots of great analysis\n- The analysis includes several instruction datasets that are used today (Lima, GPT-instruct, etc.)",
            "1": "and a large number of experiments\n\n- The analysis is only performed with one model.",
            "2": "Although the one model is timely and relevant, it would be nice to see this replicated with another model (or even with different tuning, like LoRA).",
            "3": "- Classification, sentiment, and Yelp are very similar tasks IMO.",
            "4": "Perhaps my fellow reviewers would disagree, but I would like to see something more different like QA or summarization (which are in SuperNI, so I’m not sure where those fit in this paper?)",
            "5": "- There aren’t any generalist datasets used that are published that I can find (are Lima and GPT4-Instruct published?)",
            "6": "and although they are well hyped and (perhaps?)",
            "7": "well used now, it’s hard to assess their quality.",
            "8": "It would have been nice to see some more established generalist fine-tuning datasets like FLAN or those used in T0, etc.",
            "9": "The authors may have had a good reason for this, looking forward to the rebuttal.",
            "10": "Some of these may be a miscommunication, I'm open to increasing my score in Soundness from 3 to 4 on the dataset/modeling issues."
        }
    },
    "jg2WCVrjhS": {
        "5bKdeDBsnQ": {
            "0": "The paper presents an interesting new model for memory augmentation.",
            "1": "The paper is not clearly structured.",
            "2": "For instance, a lot of content in the Results and Discussion section describes the methods used which have not yet been discussed, making it difficult to follow."
        },
        "mNZobUFU1t": {
            "0": "The proposed global memory population mechanism is indeed an evidence collection mechanism with heuristic threshold rules, which requires no additional training.",
            "1": "(1) The heuristic uncertainty threshold used in first-stage of memory population requires specific values for different dataset, which is lack of generalization and reproducibility.",
            "2": "(2) The overall framework perhaps works on general long document question answering and more LLMs.",
            "3": "The authors only use Roberta and multi-hop QA for testing.",
            "4": "In contrast, if the authors believe they made special progress on multi-hop QA, then analysis on hop-depend questions should be provided.",
            "5": "For example, how GEMFormer is superior than other previous LLMs that can deal with multi-hop QA as well.",
            "6": "There're only basic Roberta model be listed as a baseline."
        },
        "xfyZV0kqXE": {
            "0": "Entropy-guided token extraction is a novel way to build a global memory.",
            "1": "1.",
            "2": "It is not clear that how tokens with low entropy can be important to generate answer.",
            "3": "It needs elaborate discussion on what type of tokens are being extracted in the global memory.",
            "4": "2.",
            "5": "Important baseline is missing.",
            "6": "There is no comparision with very obvious method of building memory bank using existing keywords extraction methods as the main purpose of this paper is preparing a set of tokens that would be helpful for answer.",
            "7": "Without further experimental results, it is hard to evaluate the significance of this method."
        },
        "36QvYmPMcS": {
            "0": "The paper is well-written and easy to follow.",
            "1": "The proposed GEMFormer method consistently surpasses the RoBERTa baselines across three MHQA datasets.",
            "2": "Extensive ablation studies analyze the impact of the question context, fine-tuning, and memory content on performance, providing insights into why the proposed approach works.",
            "3": "While results are consistently better than the baseline, absolute improvements are marginal - 1-3 F1 points over a strong RoBERTa baseline."
        }
    },
    "8851TT2R0l": {
        "zm3t3JVRU7": {
            "0": "* Novelty: The new paradigm for zero-shot text classification is interesting and novel based on my knowledge.",
            "1": "This is a new attempt aiming to leverage generic label descriptions to boost zero-shot performance.",
            "2": "* Comprehensive evaluation: The experiments are conducted on both topic and sentiment classification tasks which seem sufficient to me.",
            "3": "There are also analytical studies regarding the sensitivity to prompt patterns/verbalizers and domain transfer ability.",
            "4": "* Effectiveness: The new method is empirically effective across standard text classification benchmarks, outperforming previous standard zero-shot prompting and achieving similar performance to larger GPT3 models with RoBERTa-sized encoders.",
            "5": "* Presentation of the method: While the motivation and intuition is general clear with the label description training, I didn't find a very clear and concrete introduction of how the fine-tuning is exactly conducted (starting Line 271).",
            "6": "Specifically, how would you train the model on class-representative keywords, short templates, and Wikipedia descriptions, respectively?",
            "7": "What is the input format and the prediction target?",
            "8": "This might be straightforward, but I'd appreciate a figure or at least some detailed descriptions with regard to this, which will help the readers understand the method better."
        },
        "84abCRA46i": {
            "0": "This paper proposes a new way of constructing dataset which only consists of the description of the labels for LLMs on zero-shot text classification tasks.",
            "1": "And this method is not strongly dependent on patterns and verbalizers.",
            "2": "It is also hopefully to applied on other tasks.",
            "3": "Though the method proposed by this paper could obviously improve the performance of LLMs on zero-shot text classification tasks, it's a simple method which has a strong rely on the practitioner’s understanding of the labels and usage situation.",
            "4": "It also has a need of a pre-trained model which could produce some potential biases."
        },
        "3k1nt26G15": {
            "0": "* The paper is well-written and easy to understand.",
            "1": "* The approach is straightforward and seems to improve over the pattern-verbalizer approach.",
            "2": "* The idea to construct synthetic data using only the labels is interesting.",
            "3": "* The authors have included ablation tests to demonstrate the robustness of their approach.",
            "4": "* The results are neither surprising nor exciting.",
            "5": "Zero-shot and few-shot learning with GPT-3 using prompting had still a large gap with supervised SotA models.",
            "6": "On the other hand, InstructGPT [1] was much better than GPT-3 and the main reason was that it was trained to follow instructions (prompts).",
            "7": "The proposed approach is very similar to the basic idea of InstructGPT.",
            "8": "The authors fine-tune an MLM using patterns, which can be seen as instructions, and synthetic data.",
            "9": "In effect, the model has learned to use these patterns and is therefore able to use this knowledge for classification.",
            "10": "* The randomized and mismatched ablations are not very well designed.",
            "11": "The authors use the same setting found during hyper-parameter tuning.",
            "12": "However, the ablated versions have a more difficult task to solve.",
            "13": "The randomized version has to learn new embeddings from scratch and the mismatched version has to learn a new meaning for each label.",
            "14": "Thus the model may require more steps to adapt.",
            "15": "I would be interested to see how the ablated versions would perform if the authors tuned the hyper-parameters on 20NG for each of these settings.",
            "16": "* I believe the authors should have included one more baseline, i.e., the model trained on the 20NG dataset.",
            "17": "* I am not entirely convinced by the choice of the authors to use only 4 labels from the 20NG dataset.",
            "18": "For instance, similar labels could have been merged.",
            "19": "In addition, the proposed approach could include more labels, e.g., from Wikipedia, which may or may not contain the labels of the downstream tasks, and create a larger dataset.",
            "20": "This combined with the baseline I proposed above could lead to a general-purpose pattern-based classifier.",
            "21": "* The results with InstructGPT (text-davinci-003) cannot be compared with the other results.",
            "22": "It is understandable that the authors were able to only evaluate 1,000 samples to reduce the cost.",
            "23": "I would expect however to compare these results with the other methods on the same set of samples.",
            "24": "[1] https://arxiv.org/abs/2203.02155"
        }
    },
    "6i98agKoZ1": {
        "unl6ky8eEt": {
            "0": "1.",
            "1": "The paper proposes a simple and effective method that works for multiple NAT models without much tuning.",
            "2": "2.",
            "3": "The proposed method helps mitigate the multi-modality problem better than distillation with autoregressive models.",
            "4": "3.",
            "5": "The paper provided detailed analysis and ablation studies to understand the effect of self-distillation.",
            "6": "1.",
            "7": "The proposed method requires a well-initialized NAT model to generate good candidates.",
            "8": "For example, it might not work for Vanilla NAT, which is not evaluated in the paper.",
            "9": "But this is already acknowledged in the Limitations section.",
            "10": "2.",
            "11": "The design of the selection function does not seem very principled (n-gram-based).",
            "12": "It is well-known that the n-gram-based metric (e.g., BLEU and ROUGE) has a high variance for sequence-level evaluation.",
            "13": "But overall, this is a nice extension over prior work that shows how self-distillation can further simplify the training data and improve NAT models.",
            "14": "The consistent gains across models and datasets are promising."
        },
        "Gk1IidEzl8": {
            "0": "1, The method is simple but useful to improve the NAT models, without using external models\n\n\n1, It is not clear how the proposed method solve the multi-modality issue\n2, The methods need to be evaluated in various tasks"
        },
        "C8JloUo0G1": {
            "0": "It is a clever idea to select good samples from self-distilled data using the reference target, because it is more suitable for fitting NAT models and it is better samples to follow than the self-distilled data itself.",
            "1": "This simple method shows good results in several NAT methods.",
            "2": "* The improvements are relatively small for sophisticated NAT methods (GLAT, CTC, DAT).",
            "3": "This means that the method in this study is not so effective for the state-of-the-art NAT methods.",
            "4": "* The idea is similar to \"Self-Distillation Mixup Training for Non-autoregressive Neural Machine Translation\" (https://arxiv.org/abs/2112.11640) in that the both studies use distilled data from NAT models and use reranking and filtering.",
            "5": "(But the method in this study is simpler and more effective)."
        }
    },
    "BSApuhuM87": {
        "zPI49HCI5Y": {
            "0": "The proposed Anti-CF model sounds reasonable and would be applicable to other tasks.",
            "1": "This paper pointed out the problem of Tent (Wang et al., 2021), which can lead to model collapse.",
            "2": "However, whether Tent was tuned properly in Section 2.2 is still questionable because Tent can be sensitive to hyperparameters (e.g., learning rate, batch size, etc.)",
            "3": "and the optimizer.",
            "4": "The results in Table 1 also show that Tent performs reasonably well on other datasets.",
            "5": "The authors are encouraged to discuss the proposed side block compared to existing PEFT methods (e.g., LoRA) to justify its novelty."
        },
        "54sDvG90fx": {
            "0": "1.",
            "1": "This work proposes an approach to prevent model collapse during test time adaptation.",
            "2": "2.",
            "3": "By introducing a side-block along with the main model, it has significantly reduced the computational cost and presents an efficient approach\n3.",
            "4": "Experiments are carefully designed and presented nicely\n\nAlthough this study shows significant improvements in TTA for extractive QA task, it did not show the performance measure of any other task.",
            "5": "Thus, it is not clear about the generalizability of the proposed method for other tasks.",
            "6": "Offcourse, it is hard to include several tasks within the given space, some discussions along the line would have been great."
        },
        "dWunoXmA9S": {
            "0": "1.",
            "1": "By analyzing, it was discovered that the imbalance in label distribution in the test set of the question answering task is a major cause of model collapse in TTA methods.",
            "2": "This finding is of great significance and provides important guidance.",
            "3": "2.",
            "4": "The Anti-CF method is proposed, which uses the output of the source model as soft label constraints for the adapted model in the TTA process, effectively preventing model collapse.",
            "5": "3.",
            "6": "Anti-CF is demonstrated to achieve stable performance improvements in various distribution shift scenarios, making TTA more reliable in practical applications.",
            "7": "1.",
            "8": "Anti-CF uses KL divergence to constrain the adapted model's output to not deviate too much from the source model, which limits the upper bound of performance to some extent.",
            "9": "More flexible constraint methods could be explored.",
            "10": "2.",
            "11": "Could the author compare with stronger TTA methods?",
            "12": "For example, SAR (https://openreview.net/pdf?id=g2YraF75Tj), AdaNPC (https://arxiv.org/abs/2304.12566), LAME (https://arxiv.org/abs/2201.05718).",
            "13": "3.",
            "14": "Anti-CF is currently validated only in the question answering task, and it can be extended to other NLP tasks in the future.",
            "15": "4.",
            "16": "The selection of hyperparameters in the experiments could be more systematic.",
            "17": "It needs further investigation whether different datasets and models require different combinations of hyperparameters.",
            "18": "The computational and memory complexity of Anti-CF can be analyzed in more detail."
        }
    },
    "7F5w5AQrv7": {
        "SmbQ1yFISz": {
            "0": "A task-aware self-supervised framework is proposed for dialogue discourse parsing.",
            "1": "Experimental results show the effectiveness of the proposed framework.",
            "2": "This paper has the following weaknesses:\n\nA.",
            "3": "The example in Figure 1 lacks the data source.",
            "4": "It does not come from either Molweni or STAC used in this paper.",
            "5": "B.",
            "6": "Lack of a clear motivation.",
            "7": "This paper argued that there are three problems, including error propagation, learning bias, and incompatibility of predefined relations, that need to be addressed.",
            "8": "However, it doesn't give a clear explanation of why these problems need to be solved.",
            "9": "For example, why is learning bias an issue worth considering, given that Molweni and STAC have few multi-head dependencies?",
            "10": "What impact does the incompatibility of predefined relations have on downstream tasks?",
            "11": "C. Unclear experimental settings.",
            "12": "In Section 3.3, how to solve the BERT token limitation, since the whole dialogue is concatenated?",
            "13": "In dataset processing, if you followed Shi et al.",
            "14": "to pre-process a dataset that doesn't consider multi-head dependencies, what is the effect of your top-down and bottom-up strategy that considers multi-head dependencies?",
            "15": "D. Unclear description in Section 3.8.",
            "16": "What is the motivation for transition?",
            "17": "Why does the existing relation taxonomy may not fit the ERC tasks?",
            "18": "Why could this transition better facilitate ERC tasks?",
            "19": "What principles is the transition based on?",
            "20": "E. Unfair comparison.",
            "21": "This paper adopted the BERT-large, which is not fair to directly compare to the works adopting the base version.",
            "22": "Therefore, it is not clear whether the performance improvement comes from the proposed method or from using a larger pre-trained model.",
            "23": "F. An in-depth analysis of each component in the proposed method is required.",
            "24": "Since top-down and bottom-up are used to alleviate the problem of learning bias, does this strategy better identify multi-head dependencies?",
            "25": "Since SWM is devised to alleviate the problem of long-range dependencies, does it help identify longer-distance dependencies?",
            "26": "Besides, what problems in ERC are solved by TA-DialogDP?",
            "27": "The overall performance is not convincing and an in-depth analysis is required."
        },
        "jfljzQD69X": {
            "0": "The strengths of this paper are:\n\n1.",
            "1": "Introducing a task-aware paradigm to improve the versatility of the dialogue discourse parser for downstream tasks.",
            "2": "2.",
            "3": "Designing a graph-based discourse parsing model called DialogDP to alleviate error propagation and learning bias.",
            "4": "3.",
            "5": "Demonstrating the effectiveness and flexibility of the proposed framework through empirical studies on dialogue discourse parsing datasets and a downstream task.",
            "6": "If this paper were to be presented at the conference or accepted into Findings, the main benefits to the NLP community would be:\n\n1.",
            "7": "Advancing the state-of-the-art in dialogue discourse parsing and improving the adaptability of the parser for downstream tasks.",
            "8": "2.",
            "9": "Providing a novel approach to address issues with existing parsing approaches and demonstrating the effectiveness and flexibility of the proposed framework through empirical studies.",
            "10": "1.",
            "11": "The paper lacks sufficient analysis of the experimental results, especially on the Molweni dataset.",
            "12": "2.",
            "13": "There is some overlap between the relevant work section and the introduction section."
        },
        "4bPZDk6CPD": {
            "0": "The paper presents a new framework for dialog discourse parsing, with several interesting novel ideas including a self-supervision objective between top-down and bottom-up approaches and a soft window mask.",
            "1": "These ideas could inspire new approaches not only within DDP but also DAG-parsing tasks in general.",
            "2": "The authors demonstrated the strength of their framework in the experiments, as the framework outperformed all baselines in accuracy of Link and Relations on two datasets.",
            "3": "The new framework also outperformed baselines in downstream ERC task, thus demonstrating the benefit of task-aware setups.",
            "4": "The authors conducted comprehensive ablation studies to justify the various design choices for the framework.",
            "5": "Reproduction of the method could be difficult as some key methodology details and hyperparameters are missing.",
            "6": "For example, the lambda_a and lambda_l of the objective (equation 12) are not provided in the main text or in appendix.",
            "7": "I included more questions about some methodology details in the Questions section.",
            "8": "While the new framework did achieve great performance in both experiments, the improvements are quite marginal (especially in STAC and Molweni experiments) and thus made the results less exciting.",
            "9": "While the ablation studies justified many design choices (such as using the soft window), some choices still seems quite arbitrary, such as selection of specific values of a0,a1,a2,B0,B1,B2 for the soft window."
        }
    },
    "Pu5tJykUeT": {
        "uHGQ9XXceq": {
            "0": "1.",
            "1": "This work propose a different deduction reasoning in multi-modal domain.",
            "2": "2.",
            "3": "This work contribute a meaningful dataset to support this task.",
            "4": "3.",
            "5": "Experimental results shows that the deduction reasoning can be finished in this way.",
            "6": "1.",
            "7": "The task proposed in this paper cannot be applied in real application, since there are no extra annotation, like the rule set.",
            "8": "2.",
            "9": "This work is not the first to explore deductive reasoning in the multi-modal domain, please refer to compositional reasoning in VideoQA and ImageQA for more information.",
            "10": "These task are not same as your task, but they explore programs to model the deductive reasoning in the multi-modal domain."
        },
        "tbDgVB4auy": {
            "0": "- The proposed task is interesting and has the potential to promote rule-based reasoning in the multimodal domain.",
            "1": "- The paper is well-written, offering a comprehensive task definition and clear formulation.",
            "2": "- The setting of confusing items in the candidate events is a critical aspect of the dataset quality.",
            "3": "However, the paper lacks a detailed description of this aspect.",
            "4": "The provided cases in the paper seem to contain relatively unconfusing incorrect candidates, that is, it seems easy to distinguish the correct candidate from the incorrect ones based on relevance or similarity to the rules.",
            "5": "Addressing this concern would strengthen the paper's dataset construction.",
            "6": "- The paper lacks analyses on dataset bias.",
            "7": "First, it remains unclear how crucial each modality (image or text) is to the new task.",
            "8": "Evaluating the performance of models that solely rely on image or text inputs would provide insights into the importance of each modality.",
            "9": "Second, there is a potential issue of models exploiting the relevance or similarity between candidates and rules as a shortcut, rather than genuinely reasoning through the problem.",
            "10": "Evaluating and discussing this dataset bias is essential to ensure the dataset quality.",
            "11": "- It would be better to incorporate potential applications or downstream tasks of the new task, which may enhance the significance of the new task."
        },
        "dv874sKGMd": {
            "0": "The paper formulates the ART task as an early exploration of deductive reasoning in the multi-modal domain.",
            "1": "This work provides a large and rich annotated dataset which could benefit the research related to this topic.",
            "2": "Also, authors develop a well-designed baseline towards better understanding the task.",
            "3": "Overall it is complete and solid from my point of view.",
            "4": "The proposed ART task is close to the well-established video inference task but with more details in commonsense rules and reasoning process.",
            "5": "Though it is an interesting direction, based on the restrictions, it might be a bit hard to scale for future researchers."
        }
    },
    "FgEM735i5M": {
        "0mPr2pz3MJ": {
            "0": "1.",
            "1": "The scene graph-based pseudo-labeling approach is novel and is shown to outperform previous pseudo-labeling methods on standard datasets.",
            "2": "2.",
            "3": "The choice of scene graph as well as the design of various techniques to further improve the performance of the approach is well motivated, described in detail, and the benefits are validated in the ablation studies.",
            "4": "How much performance improvement the approach can provide in more general cases is less clear.",
            "5": "In particular, the RefCOCO dataset contains spatial words in the description that can be better captured by the extracted spatial information and not inherently validating the gain from using scene graphs or capturing relationships.",
            "6": "As expected, we observe a larger performance gain using this approach on the RefCOCO dataset compared to the second place approach.",
            "7": "For the other two datasets (RefCOCO+ and RefCOCOg), which are more complex and do not contain spatial relations, the performance of the system using the proposed approach is very close to the ReCLIP approach."
        },
        "hrEAWQ3Q8G": {
            "0": "- This paper is well-written and easy to follow.",
            "1": "The motivation and design choice of the proposed method is simple and intuitive.",
            "2": "The proposed method shows consistent performance improvement on the three benchmark datasets for referring expression comprehension.",
            "3": "- Although SGEPG is model-agnostic, this paper conduct experiments using a specific model, VLTVG, to validate the proposed method.",
            "4": "The additional experiments using the other VLM would be helpful to emphasize the effectiveness of the scene graph-enhanced pseudo-labeling approach."
        },
        "ttsZ9iqQYB": {
            "0": "1.",
            "1": "The proposed methods demonstrate impressive performance on benchmarks for unsupervised ReC.",
            "2": "2.",
            "3": "Extensive experiments confirm the effectiveness of the proposed model components.",
            "4": "3.",
            "5": "The paper is easily understandable and straightforward to follow.",
            "6": "1.",
            "7": "This paper presents a highly effective engineering method for ReC.",
            "8": "However, it should be noted that the proposed framework incorporates some combinatorial and heuristic aspects.",
            "9": "In particular, the Non-Ambiguous Query Generation procedure relies on a sophisticated filtering template.",
            "10": "It would be helpful if the author could clarify the impact of these heuristic components.",
            "11": "2.",
            "12": "Since the linguistic expression rewriting utilizes the powerful GPT-3.5 language model, it would be interesting to understand the extent of randomness and deviation that may arise from the influence of GPT-3.5.",
            "13": "Is there any studies or analyses on this aspect?"
        }
    },
    "DQ9WeXpgJt": {
        "z0wtKwQzRa": {
            "0": "Strengths:\n\n1.",
            "1": "Novel approach: The paper proposes an unsupervised method USPL for pixel-level sound source localization.",
            "2": "It addresses the challenges of learning accurate pixel-level sounding maps and fine-grained sound source localization in an unsupervised paradigm.",
            "3": "2.",
            "4": "Multi-stage approach: The paper includes three main modules - mask-augmentation based multi-instance contrastive learning, unsupervised sounding map refinement, and sounding pixel segmentation.",
            "5": "This multi-stage approach allows for the alignment of audio-visual features, refinement of coarse sounding maps, and generation of fine sounding maps.",
            "6": "3.",
            "7": "Experimental results: The proposed USPL method outperforms previous state-of-the-art unsupervised methods in terms of mIoU and F-score.",
            "8": "It achieves a significant improvement in sound source localization accuracy on both the AVSBench-S4 and VGGSound datasets.",
            "9": "Benefits to the community:\n\n1.",
            "10": "Unsupervised sound source localization: The USPL method presents a valuable contribution to the community by addressing the challenge of unsupervised sound source localization.",
            "11": "By developing an approach that does not rely on manual annotations, it provides a more efficient and scalable solution for this task.",
            "12": "2.",
            "13": "Accurate pixel-level localization: The USPL method achieves accurate pixel-level sound source localization, which is crucial for various applications, such as audio event detection, speech recognition, and audio-visual scene understanding.",
            "14": "It provides researchers and practitioners with a reliable tool for extracting precise information from audio-visual data.",
            "15": "3.",
            "16": "Superior performance: The experimental results demonstrate the superiority of the proposed USPL method compared to existing unsupervised methods.",
            "17": "The improved localization accuracy can benefit researchers in developing more accurate and robust models for various audio-related tasks.",
            "18": "While the paper presents a detailed description of the proposed method, it lacks a deeper theoretical analysis and explanation of the underlying principles supporting the effectiveness of the approach."
        },
        "fQIbZAtoAX": {
            "0": "1.",
            "1": "To my knowledge, the task of Unsupervised Sounding Pixel Learning (USPL) is first explored by this paper.",
            "2": "This is a novel, interesting and challenging task which may attract more researchers.",
            "3": "2.",
            "4": "Experiments on public datasets show that the proposed method considerably outperforms current systems and achieves SOTA, demonstrating the effectiveness of the paper.",
            "5": "1.",
            "6": "I wonder whether the topic of this paper is suitable for EMNLP, since the paper focuses on sounding pixel location, which has nothing to do with NLP.",
            "7": "2.",
            "8": "To me, the motivation of introduction of the SPS module remains unclear.",
            "9": "What do you mean by ‘Although the SMR produces fine sounding maps, it is too heavy for evaluation’?",
            "10": "More explanation may be needed."
        },
        "Bju3SFJPa2": {
            "0": "1.",
            "1": "The proposed Sounding Map Refinement (SMR) module is novel and reasonable to me.",
            "2": "2.",
            "3": "The experimental results indicate that the proposed method outperforms previous unsupervised methods.",
            "4": "3.",
            "5": "The paper is well-written and easy to follow.",
            "6": "1.",
            "7": "The efficiency of the SMR module appears to be low.",
            "8": "Although the authors suggest using SPS to bypass the SMR during the evaluation stage, there is no corresponding discussion in the experimental section.",
            "9": "I hope to see a comparison of the time and space complexity with and without SPS during evaluation.",
            "10": "2.",
            "11": "Since the SMR module significantly contributes to the performance improvement, has there been any exploration of the hyperparameters used in Eq.",
            "12": "5, 7, and 8?"
        },
        "eDHbUYZlNU": {
            "0": "It's with broad interests to investigate pixel sound source location in an unsupervised manner, which mitigates the challenges of obtaining large volume of annotations.",
            "1": "Overall, this paper clearly presents the proposed methodology, with well-designed experimental design and ablation studies.",
            "2": "I think more details could be added to strengthen the experimental session, including:\n1.",
            "3": "How to determine the values of hyper-parameters, e.g.",
            "4": "as described in Section 4.1.",
            "5": "2.",
            "6": "For methods listed in Table 2, it helps to include computational and memory costs.",
            "7": "3.",
            "8": "For the ablation study summarized in Table 3, why is the test partition (not the dev partition) of AVSBench-S4 used?"
        }
    },
    "hInB4JIQ5P": {
        "4CrOHROdx2": {
            "0": "Rather extensive benchmark comparison is included in this paper with all major editing systems.",
            "1": "The introduced model achieves competitive results with them while maintaining much smaller model size.",
            "2": "The author conducted experiments on composite text editing by human evaluation, which has been less discussed in previous works.",
            "3": "This major change in the proposed work is on fine-tuning text editing models using natural instruction instead of plain prefix, and thus using the newly constructed CoEdit dataset.",
            "4": "I may have missed it but Section 3 didn’t include further details about the model training based on previous work of FlanT5.",
            "5": "It is unclear then what contributes most to the performance variations, and especially in comparing with general purpose LLMs on one specialized task.",
            "6": "The more comparable work of PEER (Schick et al., 2023) also tackles slightly wider range of problems as the author also pointed out in LN139.",
            "7": "Since instruction based tuning has been explored in many works, the novelty at this point can be limited.",
            "8": "In the meantime, the effort implied in the dataset construction is less significant, as it is constructed from public ITERATER+  and as said in LN077 “based on rules that introduce lexical and semantic variations”."
        },
        "nLG4PQzag5": {
            "0": "1.The proposed method is able to achieve better results with less parameter on text-editing task.",
            "1": "2.",
            "2": "The comparison between the proposed method and existing methods is extensive and showing strong support to the claimed contribution in the paper.",
            "3": "3.",
            "4": "There is some insight discussion about the general-purpose and task-specific tuning regarding to text-editing task.",
            "5": "4.",
            "6": "The implementation details including the prompts are clearly described in the paper, making it easier for others to reproduce.",
            "7": "1.",
            "8": "The novelty is limited due to the scope of the idea.",
            "9": "While fine-tuning LLM can be applied to different tasks, the proposed method aligns with the typical approach within this paradigm.",
            "10": "Consequently, the novelty is restricted by the commonly referred method and the limitations of the particular task."
        },
        "AJgLSz7eD1": {
            "0": "1.",
            "1": "The paper has shown instruction-tuning can effectively push a range of text-editing tasks into one LM with good performance.",
            "2": "Extensive experiments are done to support the conclusion.",
            "3": "2.",
            "4": "The paper mentioned an interesting and important topic, generalisability of the instruction tuning.",
            "5": "However, the experiments setting is a bit weak.",
            "6": "1.",
            "7": "Key baseline is missing in the performance experiment:\n    The author has compared with a series of baseline models in Table 2.",
            "8": "However, it could be important to compare with single text-editing task fine-tuned Flan-T5.",
            "9": "This could illustrate if the multi-text-editing-task instruction tuning can effectively improve the performance.",
            "10": "2.",
            "11": "The argument on the performance on generalisation is a bit weak:  \n    - For the experiments on generalisability to unseen tasks, the task choices are sentence compression and politeness transfer.",
            "12": "I have concern that the sentence compression is very similar to the text simplification task, which is used as training data.",
            "13": "The politeness transfer task also has similarity with the formalising task.",
            "14": "If you think there are clear differences, please explain the similarity and difference a bit more, or if possible choose a different task as unseen task.",
            "15": "- The way you collect the training data for compositional task, mentioned in line 1130, is selecting special cases from the current GEC and GYAFC training data.",
            "16": "If they are compositional enough are questionable.",
            "17": "- In line 558, you didn't tell how many new task combinations did you use to evaluate the compositional generalisability.",
            "18": "Furthermore, the test results in Table 7 are only from 30 composite instructions."
        }
    },
    "Dy2mbQIdMz": {
        "YK1e9v2hOF": {
            "0": "1.This paper introduces two novel approaches, DetectLLM-LRR and DetectLLM-NPR, for detecting machine-generated text.",
            "1": "These methods leverage log rank information and outperform baselines in terms of accuracy and efficiency.",
            "2": "2.",
            "3": "The authors provide insights on the efficiency-performance trade-off and show that their methods are more practical for real-world use.",
            "4": "1.",
            "5": "There are already a number of applications for detecting AI-generated text, e.g., https://copyleaks.com/ai-content-detector, https://writer.com/ai-content-detector/.",
            "6": "Even OPENAI released a classifier for indicating AI-written text.",
            "7": "However, authors don't seem to make comparisons.",
            "8": "2.The authors use the area under the receiver operating characteristic curve (AUROC) as the sole evaluation metric for their methods.",
            "9": "While this is a common metric for zero-shot detection, it may not capture all aspects of performance, such as precision and recall.",
            "10": "3.",
            "11": "The experiments in this paper only test the proposed methods on datasets in English.",
            "12": "It is unclear how well these methods would perform on other languages."
        },
        "ys7GXZzVwO": {
            "0": "the proposed methods are simple but useful, outperforming various baselines.",
            "1": "The experimental settings are reasonable.",
            "2": "They test various models and datasets to validate their methods under different decoding schemes such as top-p, top-k, or temperature.",
            "3": "the overall writing and demonstrations are clear and easy to follow.",
            "4": "There is a lack of experiments to show the robustness of their methods, for example, how would their method perform when the text is rephrased by either human or machine?",
            "5": "It is unclear whether their method also works for other languages except English since they only test it in English text.",
            "6": "See more in questions."
        },
        "FtRbosYmPs": {
            "0": "1.",
            "1": "Comprehensive experimental evaluations on three datasets and seven language models provide a thorough analysis of the proposed methods' performance.",
            "2": "The results demonstrate a remarkable improvement over the state-of-the-art by 3.9 and 1.75 AUROC points absolute, highlighting the efficacy and superiority of the proposed techniques.",
            "3": "2.",
            "4": "The authors also showcase the practicality of DetectLLM-NPR, which requires fewer perturbations than previous work while achieving similar performance levels.",
            "5": "This feature makes it more suitable for real-world use, where efficiency is essential in detecting machine-generated text in social media and education settings.",
            "6": "The consideration of efficiency-performance trade-offs and providing insights for effective practical usage further strengthen the paper's quality and relevance to real-world applications.",
            "7": "As mentioned in the paper, one limitation of the zero-shot methods is that it requires statistics from the source model, which doesn't apply to the cases when one has no access to source models.",
            "8": "However, this is not a weakness of the proposed methods."
        }
    },
    "jUrRIcedTN": {
        "qAfVbgz0rE": {
            "0": "1) The newly proposed task is both novel and useful.",
            "1": "2) Good writing.",
            "2": "3) Detailed analysis of experimental results.",
            "3": "4) Good conclusions, especially the \"limitations\" section.",
            "4": "1) The paper has two main contributions.",
            "5": "The first contribution is kicking off a new task, and the second contribution is proposing a multi-task learning approach to accomplish this task.",
            "6": "The second of these contributions mainly follows an ideological approach that has been used for a long time in the NLP community, and the main innovativeness is focused on the first contribution.",
            "7": "However, the analysis part of the experiment is discussed mainly for the second contribution.",
            "8": "From my perspective, it's better to focus more on the first contribution instead of the second one.",
            "9": "2) There is only 1 baseline model in the experimental section."
        },
        "MldLE1OLAu": {
            "0": "This is an interesting task and deepens our understanding of automatic approaches to metaphor.",
            "1": "There is a deep and interesting qualitative analysis of the results.",
            "2": "The paper is well-written and clear.",
            "3": "The focus of this paper is somewhat narrow in that it depends on a corpus with very specific annotations and there is no wider comparison.",
            "4": "In particular, it would be interesting to see if this work could be used to improve a the more general case of annotation.",
            "5": "The only baseline is a single-task baseline, and as the MTL set-up is not really novel, the results are not surprising (as it has been well-established that MTL improves peformance).",
            "6": "This is a nice paper, but I feel it would be better suited to a focused workshop rather than the EMNLP main conference"
        },
        "KRdSnIg2hw": {
            "0": "* A new task is proposed, that of better understanding the metaphorical text span in a metaphorical sentence.",
            "1": "* The comparison between single and multi task learning is interesting, and potentially reaching a wider audience.",
            "2": "* The results are missing statistical significance while the differences in scores are very small.",
            "3": "* A single dataset was used for experiments, and, judging by the annotations presented in this paper, the inter-annotator agreement should also be discussed."
        }
    },
    "M1Nogs3zR5": {
        "MVtZKsq2eO": {
            "0": "1.",
            "1": "The paper is well-written and easy to read.",
            "2": "2.",
            "3": "The empirical study demonstrates the improvement of the proposed methods, especially on character-level and word-level perturbations.",
            "4": "1.",
            "5": "The novelty seems a bit limited.",
            "6": "If you introduce noises in pretaining and fine-tuning, it is expected to perform well on perturbed examples.",
            "7": "2.",
            "8": "I personally think the ablation study/ChatGPT experiments/Ablation study are quite important, not sure why they appear in the appendix."
        },
        "JNrwRVJEd0": {
            "0": "The three main contributions of this paper can be summarized as follows:\n\t• Investigating the Effects of Input Perturbations: This paper is the first to comprehensively investigate the effects of diverse input perturbations on generative frameworks in slot filling tasks.",
            "1": "It highlights the vulnerability of existing prompt-based generative methods when faced with different human expressions.",
            "2": "• Multi-task Demonstration-based Generative Framework: The authors propose a simple yet unified multi-task demonstration-based generative framework called DemoNSF.",
            "3": "It includes three novel noisy auxiliary tasks and a noisy demonstration construction strategy, which enhance the model's robustness and adaptability to perturbed inputs in real-world dialogue scenarios.",
            "4": "• Improved Performance and Generalization: DemoNSF outperforms all baseline methods and demonstrates strong generalization.",
            "5": "The extensive analysis provided in the paper also offers empirical guidance for the practical application of generative frameworks.",
            "6": "The experiment is not enough.",
            "7": "The authors build T5 and BART as baseline models, however, The GPT-style models are not considered in this paper.",
            "8": "Maybe it is interesting to include some GPT-style models."
        },
        "Xju5dQWWSq": {
            "0": "1.",
            "1": "The paper introduces an effective multi-task demonstration-based generative framework for the noisy slot-filling task in practical dialogue scenarios.",
            "2": "2.",
            "3": "The paper is well-written and easy to follow.",
            "4": "The authors provide clear explanations of the proposed framework and the experiments conducted.",
            "5": "No particular reason to reject."
        }
    },
    "Mtgbc9XFPU": {
        "ZA4t3QydMF": {
            "0": "A novel method demonstrating the improvement in representational ability of the LLMs used as encoders in task-oriented dialog task.",
            "1": "The proposed method seems to be pretty close to already published ones [1, 2].",
            "2": "It would be nice to see the comparison.",
            "3": "Another important point is that the authors do not compare their work with state of the art in the field."
        },
        "YR4EDo3H3e": {
            "0": "1.",
            "1": "This paper proposes a new intent-aware contrastive learning objective to leverage gold and pseudo intents and improves intent classification performance compared to baselines.",
            "2": "2.",
            "3": "The motivation of using intent role labeling to generate psudo-intents are well stated and the problems in zero-shot intent classification are identified.",
            "4": "1.",
            "5": "Lack of review and investigations on intent classification and contrastive learning.",
            "6": "2.",
            "7": "The effectiveness of proposed model is not strong enough as it is not demonstrated under the zero-shot settings for intent-classification model.",
            "8": "Instead, this paper only compares to intent-classification models using Prototypical Networks.",
            "9": "3. the quality and amount of pre-training datasets need to be studied."
        },
        "6kxo1WUjGn": {
            "0": "S1: The idea of assigning roles to words using IRL for creating pseudo intents to pre-train the encoder is innovative and interesting.",
            "1": "S2: This paper has good approach presentation and detailed result analysis.",
            "2": "S3: This paper is well-written and has clear structure to follow.",
            "3": "W1: In section 3, the authors annotate the intent roles with 6 types for obtaining the IRL taggers.",
            "4": "However, the definitions of intent roles like Action, Argument, Request, etc., may not capture all nuances across various domains or applications.",
            "5": "Additionally, these roles may overlap in some contexts, potentially complicating the tagging process.",
            "6": "W2: In Table 2, while many IRL labels have high precision, recall, and F1 scores, the 'Problem' label has significantly lower scores, suggesting that this particular aspect of the model might struggle in practical applications.",
            "7": "W3: By formulating IRL as a sequence tagging problem, the complexity of the relationships between different labels in an utterance might be overlooked.",
            "8": "This could lead to suboptimal intent representations.",
            "9": "W4: The IRL tagger uses RoBERTa-base as its foundation.",
            "10": "While RoBERTa is a powerful model, the results might differ if another model was used.",
            "11": "This dependency also means that improvements in RoBERTa (or discovery of issues in it) could directly affect the IRL tagger's performance.",
            "12": "Probably an ablation study is needed for showing the IRL tagger's performance on different models and how the different IRL tagger performance may affect the IC task performance.",
            "13": "W5: For the intent-aware contrastive learning part, the method assumes that randomly sampled utterances that share the same gold intent names can be treated as positive pairs.",
            "14": "This might introduce ambiguity, especially if different utterances with the same intent name have nuanced differences in meaning.",
            "15": "Also, the method treats pairs that are not positive in a batch as negative pairs.",
            "16": "This simplistic approach might introduce noisy negative samples, affecting CL performance."
        },
        "WJgMSCp6Dq": {
            "0": "The paper proposed a novel and inspiring method for the IC task and achieve STOA performance.",
            "1": "The method can potentially be useful for other NLP tasks as well.",
            "2": "No obvious flaws."
        }
    },
    "HUzbEPMd6v": {
        "2yoq60fkTy": {
            "0": "1.",
            "1": "Their asked question is meaningful to the community, as \"how do we find the optimal strategy of prompt injection, given the task at hand?\".",
            "2": "In PETuning (parameter-efficient tuning), it would be interesting to ask and answer this question from a theoretical view, even though this work is more from an empirical view.",
            "3": "2.",
            "4": "For their designs, their optimization objective looks novel and interesting, combing and adapting existing work's key designs to their approach.",
            "5": "3.",
            "6": "The experiments and discussions look convincing, and especially, their conclusion of \"Transferability of the learned PG settings\" is very interesting to me.",
            "7": "I am really looking forward to hearing from you about explaining this phenomenon!",
            "8": "- Honestly, I do think such small modification is very incremental to the community (though still good questions), as you just focus on the simplest classification tasks.",
            "9": "Right now, with very large language models, simple ICL (in-context learning) performance is already very strong.",
            "10": "Therefore, it would be very good to show your advantages on other use cases w/ LMs or even other models."
        },
        "x3jPtLlcaf": {
            "0": "1.",
            "1": "The researchers propose a technique for identifying the optimal layer for prompt integration.",
            "2": "Instead of utilizing an empirical selection process, this approach employs an algorithmic search for finding the most effective layer.",
            "3": "2 .To add clarity and depth to their findings, the authors provide a comprehensive analysis from various perspectives.",
            "4": "3.",
            "5": "The proposed method outperforms the prior related research.",
            "6": "1.",
            "7": "Based on my understanding, the proposed method appears to adaptively update the prefix using a learnable probabilistic gate.",
            "8": "However, the provided figure seems to depict a process of concatenating the prompt instead.",
            "9": "To ensure the illustration aligns with the method's description, I suggest revising the figure.",
            "10": "2.",
            "11": "With regards to training, I'm curious about the requirements for the prompt generator.",
            "12": "Is it necessary to have a separate prompt generator for each layer?",
            "13": "Or, could we train just one prompt generator, with the differing input hidden states provided from layer to layer?",
            "14": "3.",
            "15": "Why the prompt length for the proposed method and other comparing methods are set differently?",
            "16": "(searched as hyperparameter?)",
            "17": "4.",
            "18": "If equations 3 and 8 are accurately bracketed, it implies that you’re highly weighting the previous prompt at the outset of the training.",
            "19": "It would be beneficial to include a discussion or analysis showing the progression of the learnable gates throughout the training.",
            "20": "This would provide valuable insights into how the previous prompts are integrated over time.",
            "21": "5.",
            "22": "I'm interested in how the model performs when consistency regularization is not applied.",
            "23": "If reasonable performance can be achieved without the use of this regularization technique, it could potentially allow for a trade-off between performance and computational cost.",
            "24": "Specifically, this might eliminate the need for multiple forward passes, thereby reducing computational demand."
        },
        "0X49l4Rg6F": {
            "0": "1.",
            "1": "This paper proposes a novel prompt tuning approach, that is, automatically selecting several layers to insert instance-dependent soft prompts.",
            "2": "This paper also proposes the corresponding optimization method.",
            "3": "2.",
            "4": "The results of the experiments verify the effectiveness of proposed method.",
            "5": "3.",
            "6": "The ablation study is comprehensive.",
            "7": "1.",
            "8": "The writing in section 4.3 and 4.4 is confusing.",
            "9": "If my understanding is right, I suggest the author add a subsection in section 4 illustrating that SPT uses only the layers with larger $a_i$ for prompt inserting.",
            "10": "2.",
            "11": "Given the emergent of cutting-edge large language models and their decent in-context performance, the technique proposed in this paper seems to be redundant and outdated."
        }
    }
}