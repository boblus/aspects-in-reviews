{
    "BJg7x1HFvB": {
        "H1xlKlknFr": {
            "0": "The authors investigate the problem of training compact pre-trained language model via distillation.",
            "1": "Their method consists of three steps: \n1. pre-train the compact model LM\n2. distill the compact model LM with a larger model (teacher)\n3. fine-tune the compact model on target task \n\nThis idea is not significantly new since it is quite common to apply distillation to compress models, and the results are largely empirical.",
            "2": "From Table 3 the results on test sets are better than previous works, but not by much.",
            "3": "The authors spend quite a of space on ablation studies to investigate the contribution of different factors, and on cross-domain transfers.",
            "4": "They do manage to show that using a teacher for distilling a compact student model does better than directly pre-training a compact model on the NLI* task in section 6.3.",
            "5": "It would be better if they could show it for other tasks on the benchmark as well.",
            "6": "Overall I think this work is somewhat incremental, and falls below the acceptance threshold."
        },
        "rkgyLdyntS": {
            "0": "This submission revisits the student-teacher paradigm and shows through extensive experiments that pre-training a student directly on masked language modeling is better than distillation (from scratch).",
            "1": "It also shows that the best is to combine both and distill from that pre-trained student model.",
            "2": "My rating is Weak Accept.",
            "3": "I think the submission highlights a very useful observation about knowledge distillation that I imagine is overlooked by many researchers and practitioners.",
            "4": "The decision of Weak as opposed to a Strong accept is because the submission does not introduce anything truly novel, but simply points out observations and offers a recommended training strategy.",
            "5": "However, I do argue for its acceptance, because it does a thorough job and presents many interesting findings that can benefit the community.",
            "6": "Comparison with prior work:\n\nThe submission focuses on comparison with Sun et al.",
            "7": "and Sanh.",
            "8": "These comparisons are important, but not the most compelling part of the paper.",
            "9": "Comparison with more prior work that show large benefits would make the paper even stronger.",
            "10": "Interesting experiments:\n\nThe paper presents many interesting experiments useful for anyone trying to develop a compressed model.",
            "11": "First, it shows that distillation (from scratch) by itself may be overrated, since simply repeating the pre-training+fine-tuning procedure on the small model directly is effective.",
            "12": "However, distillation remains relevant since it also shows that pre-training the student, then distilling against a teacher, is a potent combination.",
            "13": "In the case when the transfer set is the same size as the pre-training set, it surprisingly still has some benefits.",
            "14": "This is not experimentally explained, but I suspect there are optimization benefits that are hard to pin down exactly.",
            "15": "The paper hypothesizes that the two methods learn different “linguistic aspects,” but I think it is a bit too speculative to put it in such terms.",
            "16": "The experiments are thorough, with many student sizes, transfer set sizes, transfer set/task set correlation, etc.",
            "17": "It also compares against the truncation technique, where the student is initialized with a truncated version of the teacher.",
            "18": "There are no error bars in the plots, but there are so many plots with clear trends, that this is not a big concern.",
            "19": "I can’t think of any experiments that are obviously missing.",
            "20": "Misc:\n\n- The introduction says that the pre-training+fine-tuning baseline has been overlooked.",
            "21": "It would be great to point out papers that has actually overlooked this baseline.",
            "22": "Including this in the results would be even better.",
            "23": "- During my first read-through, I got confused because I didn’t realize “pre-training” in most of the paper refers to “student pre-training” (as opposed to simply training the teacher).",
            "24": "Making this a bit more explicit here and there can avoid this confusion."
        },
        "SJe6IsB6KH": {
            "0": "This paper proposes to pre-train a student before training with a teacher, which is easy to understand.",
            "1": "Although the authors provide extensive empirical studies, I do not think they can justify the claims in this paper.",
            "2": "** Argument\n\nOne concern is that compared to other baselines such as \"Patient knowledge distillation\" [1], the proposed method is not consistently better.",
            "3": "The authors argue that [1] is more sophisticated in that they distill task knowledge from intermediate teacher activations.",
            "4": "However, the proposed method introduces other extra complexities, such as pre-training the student.",
            "5": "I do not agree that the proposed method is less elaborate than previous methods.",
            "6": "Although the investigation on influence of model size and the amount/quality of unlabeled data is interesting in itself, this does not help justify the usefulness of pre-training a student.",
            "7": "I hypothesize that when considering the intermediate feature maps as additional training signals, randomly initialized students can catch up with pre-trained students.",
            "8": "Furthermore, the mixed results shown in Table 3 do not justify the proposed method well enough.",
            "9": "[1] Patient Knowledge Distillation for BERT Model Compression, https://arxiv.org/abs/1908.09355"
        }
    },
    "SylVJTNKDr": {
        "B1exhXP6KB": {
            "0": "The authors present a series of simple experiments to characterize the objective that emergent languages are optimizing and why we see various behaviors (not aligned with natural language) when training them to play these language games.",
            "1": "The paper is clearly written, the tasks are simple and minimalist (in a good way) and the experiments are often followed by the exact ablation I was just noting that I wanted to see.",
            "2": "Questions:\n1.",
            "3": "What happens if the vocabulary is too small to completely communicate the space?",
            "4": "Do models accept that some concepts are not expressible, discover a random behavior that overloads a lexical item, or fail to learn entirely?",
            "5": "2.",
            "6": "Why are values for lamda_r not investigated or plotted?",
            "7": "3.",
            "8": "How many runs are averaged for each experiment?",
            "9": "Should I assume one per seed?",
            "10": "why are there different numbers of seeds for each experiment?",
            "11": "4.",
            "12": "Page 5 -- \"We conclude that in this case there is no apparent entropy minimization pressure on Sender simply because there is no communication\" -- Was any analysis performed on the gradients to see if they are completely random or what kind of signal they are getting?",
            "13": "5.",
            "14": "How important is the architectural design to these experiments?",
            "15": "e.g.",
            "16": "number of hidden layers, etc?"
        },
        "SJgZchUCFH": {
            "0": "This paper investigates the phenomenon of entropy minimization in emergent languages.",
            "1": "The paper studies two simple speaker-listener prediction tasks where the amount of information that’s needed to be encoded in the speaker’s message can be manipulated.",
            "2": "They find, in both games and across training methods, that speaker agents generally learn to minimize the amount of information conveyed in their messages (i.e.",
            "3": "minimize the entropy), such that they are still able to solve the task.",
            "4": "The paper then conducts some experiments measuring the robustness to overfitting, and find that more discrete channels leads to better generalization when training on partially shuffled labels.",
            "5": "Overall, this paper is very well written.",
            "6": "I think the main result, that speaker agents learn to only convey as much information as is needed to solve the task, is interesting and insightful.",
            "7": "I do, however, have a slight concern about the novelty of this result.",
            "8": "Given that the environments are very simple (consisting of a single message sent by a speaker and a prediction made by a listener), the line between ‘multi-agent emergent communication’ and ‘neural network with discrete representations’ is very blurred.",
            "9": "This is addressed only briefly in the related work section, with a handful of post-2016 papers cited.",
            "10": "I’d personally want to see a more expanded related work section that goes into some more depth into some of these papers, to be able to better judge the novelty of this contribution.",
            "11": "I found the second result, that making the representations more discrete (by lowering the temperature in Gumbel-Softmax) leads to increased robustness to overfitting, to be less clearly explained, especially how it relates to the first result.",
            "12": "It seems obvious to me that, if you can’t pass enough information from the speaker to the listener (when there’s a low temperature), then you won’t be able to solve the task at training time if a lot of information needs to be conveyed (as in the case of randomly shuffled labels).",
            "13": "The authors describe this by saying: “With a low temperature (more closely approximating a discrete channel), this is hard, due to stronger entropy minimization pressure.” But this seems misleading to me, since it’s very different from the ‘entropy minimization pressure’ that was discovered in the first result (which comes about when both agents are able to solve the task, but the listener has redundant information).",
            "14": "Thus, I don’t see how this result is either surprising or connected to the first result.",
            "15": "Further, the main claim to novelty of this second result is that it tests the information bottleneck principle in a ‘language learning’ set-up.",
            "16": "However, since now the communication is continuous, this setup resembles the classic ‘neural network prediction’ even more closely, and calling it a language learning setup seems down to semantics.",
            "17": "Given this, I’m unsatisfied with the comparison to previous work on the information bottleneck (which is not my area of expertise).",
            "18": "Given the above points, I’d say the paper is borderline it its current form, with a tendency towards rejection.",
            "19": "However, I’d be willing to increase my score if the authors can clarify some of my confusion around the second experiment.",
            "20": "UPDATE: I've increased my review to a 6 after the author rebuttal, although I still feel the paper is borderline."
        },
        "HkebXr-r9r": {
            "0": "This paper sets up a couple discrete communication games in which agents must communicate a discrete message in order to solve a classification task.",
            "1": "The paper claims that networks trained to perform this case show a tendency to use as little information as necessary to solve the task.",
            "2": "I vote to reject this paper.",
            "3": "The experiments are not very interesting and I don't at all agree with the assertion of the paper.",
            "4": "The paper claims the networks use only the entropy necessary to solve the task, but there are two main problems with this assertion.",
            "5": "(1) their own experiments don't support this all that strongly, as in the limit of few hidden bits (left half of the x axis in Figure 1), the networks all had noticeable excess information, and (2) and perhaps most damning the paper applies entropy regularization on the sender during training?",
            "6": "Could it perhaps instead be the fact that the entropy of the sender was penalized as an explicit regularization term that the entropy of the senders messages tended to be small?",
            "7": "I also find the experimental design puzzling.",
            "8": "Why both reinforce and the 'stochastic computation graph' approach?",
            "9": "Treating the receiver's output as binary and stochastic without using the log loss of the bernoulli observation model is just giving up on a good gradient as far as the receiver is concerned.",
            "10": "The experiments done are much to simple and the protocol flawed.",
            "11": "The second set of experiments in Figure 3 were not left to converge, so I'm not sure how we can derive a great deal of insight.",
            "12": "Additionally, relaxing the gumbel softmax channel to being continuous rather than discrete technically ruins any argument that there is an entropy bottleneck present anymore, as theoretically even a single dimensional continuous signal could store an arbitrary amount of information.",
            "13": "If the paper wanted to, it could have upper bounded the mutual information between the input and the message using a variational upper bound.",
            "14": "--------------- Response to Response ---------------------------------\n\nI'm editing here in light of continuing to look at the paper and the responses from the author below.",
            "15": "I have to still argue for a rejection of this paper.",
            "16": "I thank the authors for addressing my comments and I admit that at first I thought the paper was minimizing the entropy during training which would have been particularly bad.",
            "17": "While I was mistaken on that point, I still believe the paper is deeply flawed.",
            "18": "In particular, the paper makes a very bold claim, namely that \"We find that, under common\ntraining procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual\ninformation between the communicating agent’s inputs and the messages is minimized, within the range afforded by the need for successful communication.\"",
            "19": "But if we are being honest here, the experiments are very lacking to support such a bold claim.",
            "20": "In particular there was one thing I was worried about upon reading the paper again, and is similar to the point raised by the other reviewers.",
            "21": "In Figure 1, we are shown the entropy only of those networks that have succeeded.",
            "22": "Naturally to succeed, the entropy i the message must be large enough to accomodate the size of the remaining bits we are trying to reconstruct.",
            "23": "That is why Figure 1 includes the dotted line, since the networks must be above that line to have good performance.",
            "24": "And the main evidence for the main claim of the paper is that the trained networks are above that line and arguably close to it.",
            "25": "Now, we know that there are clearly solutions to these tasks (in particular the Guess Number task) which could achieve good performance at noticeably higher entropy.",
            "26": "For instance we could take any minimal solution and simply split up each message into 8x different buckets, each of which had exactly the same behavior from the decoder.",
            "27": "This would give us a +3 in the entropy of our message space while having no effect whatsoever on the loss.",
            "28": "The claim of the paper is that under normal training procedures it seems like we don't find those solutions and instead seem to find minimal ones.",
            "29": "But after implementing a simplified version of the experiment in the paper (Notebook available here: https://nbviewer.jupyter.org/urls/pastebin.com/raw/ZF7g34GN ) I suspect something much simpler is going on.",
            "30": "The reason the solutions look minimal in Figure 1 is probably because the initialization chosen for the encoder they used in the paper tended to start at low entropies.",
            "31": "Imagine if all of the networks started out with an initial message entropy of around 3 bits.",
            "32": "Then Figure 1 could be explained by the problems with hidden bits ~< 3 bits simply preserved their entropy, which in order to solve the task with higher numbers of digits hidden we know requires some minimal budget, so they get sort of pushed up.",
            "33": "This could explain the figure, but we wouldn't claim this explains why we observe small entropy for the high number of hidden digits case.",
            "34": "In particular, if we initialized the encoders with higher entropy, we might expect that we fail to see this phenomenon.",
            "35": "That is exactly what I was able to show for myself in that notebook.",
            "36": "If you simply initialize the encoder to have high entropy, all of the solutions have high entropy and the observed effect goes away.",
            "37": "Overall, the paper as I said is low quality.",
            "38": "Several choices were made that don't make a lot of sense.",
            "39": "With the experiments being as small scale as they were, why not explicitly marginalize out the message (as I did in the notebook)?",
            "40": "Why use single layer neural networks to predict 256 x 1024 parameters?",
            "41": "Why not just learn them directly?",
            "42": "If the paper aimed to mimic more standard setups and show that under those setups we observe this kind of minimal message entropy, then it would have to much better tease out the effects of all of these choices.",
            "43": "Why does the decoder use a mean field sigmoid bernoulli observation model to try to predict something is in one of ~32 states?",
            "44": "The missing digits are not independent given the message, why model them as so?",
            "45": "Is that part of the purported reason why these models show minimal entropy (cause it isn't discussed).",
            "46": "For such a simple problem, you could presumably analytically compute the gradient with respect to the loss and study whether that correlates with the gradient of the entropy.",
            "47": "There are several things I could imagine checking, none of which are checked in the paper.",
            "48": "The primary question the paper addresses is an interesting one.",
            "49": "But this paper does very little to carefully investigate that question.",
            "50": "I maintain my vote to reject."
        },
        "S1gTnJOuqr": {
            "0": "What is the specific question/problem tackled by the paper?",
            "1": "The paper studies whether discrete communication channels between agents are low-entropy.",
            "2": "The claim is that agents that try to solve a prediction task subject to a communication bottleneck will exchange low entropy messages, even if these messages are not explicitly encouraged to have low entropy.",
            "3": "Is the approach well motivated, including being well-placed in the literature?",
            "4": "The paper is well motivated, though I am not an expert in the area.",
            "5": "Does the paper support the claims?",
            "6": "This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.",
            "7": "The support for the claims is almost adequate.",
            "8": "The entropy of the messages was only analyzed for the runs where agents have successfully learned to communicate in order to solve the task.",
            "9": "Part of the paper's conclusion is that an entropy constraint on messages is not necessary, but maybe it still is necessary to increase the frequency of successful runs, or help faster learning.",
            "10": "This means that successful runs lead to low-entropy messages, but what about the unsuccessful runs?",
            "11": "Do messages have low entropy as well?",
            "12": "I am also somewhat confused by the second set of experiments.",
            "13": "The discussion seems to suggest that setting higher temperature in GS creates pressure for lower-entropy messages.",
            "14": "Buf if that's the case, then there's a controllable parameter that implicitly controls an entropy constraint and it's no longer clear to me that low-entropy is emerging.",
            "15": "Summarize what the paper claims to do/contribute.",
            "16": "Be positive and generous.",
            "17": "I think the paper does an interesting analysis and makes an interesting point about the problem being studied.",
            "18": "I am a bit confused by how the experimental setup supports the claims and their consequences.",
            "19": "In particular, I have some doubts about the claim that entropy regularization is unnecessary.",
            "20": "Clearly state your decision (accept or reject) with one or two key reasons for this choice.",
            "21": "I am voting for acceptance.",
            "22": "Provide supporting arguments for the reasons for the decision.",
            "23": "The paper sets up a clear problem to study and focuses on increasing our understanding around the issue.",
            "24": "After reading the paper a few times I am a bit confused about how the experimental setup supports the claims & conclusions.",
            "25": "I think the results in Figs.",
            "26": "1-2 adequately support the claim, but the results in Fig.",
            "27": "3 make it unclear whether the temperature parameter is implicitly controlling entropy.",
            "28": "The fact that unsuccessful runs were discarded for the first set of experiments limits the implications of the main claim that low entropy emerges, because an entropy regularization might still meaningfully improve the frequency of successful runs.",
            "29": "Provide additional feedback with the aim to improve the paper.",
            "30": "Make it clear that these points are here to help, and not necessarily part of your decision assessment.",
            "31": "I think if the paper will be improved if it resolves the lack of clarity around the temperature in GS being an implicit entropy-regularization parameter.",
            "32": "Perhaps an entropy-regularized setup is a useful comparison to show that it provides marginal benefit over the setup studied, and this might resolve the lack of clarity around the implications of the claims made from the first set of experiments.",
            "33": "Apart from these issues I am happy with the choice of topic and execution of the paper.",
            "34": "I also appreciate that due care has been taken to present the work as understanding a phenomenon, to avoid any misconceptions about a new method being proposed."
        }
    },
    "HJeYalBKvr": {
        "rJxkd5v0Fr": {
            "0": "This paper addresses an issue of compositionality in self-attention models such Transformer.",
            "1": "A simple idea of composing multiple words into a phrase as a hypernode and representing it using a non-linear function to capture the semantic mutation is proposed.",
            "2": "In the machine translation and PoS tagging tasks, the proposed PhraseTransformer achieves impressive gain, especially +13.7 BLEU score compared to the Transformer.",
            "3": "The motivation of the paper is very clear, and I love this kind of paper; with a simple idea, making a huge impact on the field.",
            "4": "I appreciate the real example to compare how word-level self-attention is different from the phrase-level self-attention in Abstract and Figure 1.",
            "5": "The problem itself; tackling the semantic compositionality of self-attention, is a very important problem, and I like the part that authors described it as an inductive bias as a model perspective.",
            "6": "However, this work seems to be problematic in terms of presentation, clarity, and meaningful comparisons.",
            "7": "Please see my detailed comments below.",
            "8": "First, what exactly is “semantic mutation”?",
            "9": "The term has been used here and there to describe the inductive bias in semantic compositionality and to show how the nonlinearity can effectively capture it.",
            "10": "But, I couldn’t find any definition from the paper, couldn’t find any formal definition from any ACL papers, and couldn’t guess myself based on the context.",
            "11": "I am guessing it is probably some sort of combination of meaning in phrase-level words.",
            "12": "If so, more importantly, how does the simple non-linear function (i.e., sigmoid) can capture such semantic combinations of words?",
            "13": "How could it make such a huge gain (PhraseTransformer vs Linear PhraseTransformer) in Table 1 on MT task?",
            "14": "This seems to be the most important contribution of this paper, of which I don’t understand yet.",
            "15": "What is the “concept of hypernodes in hypergraph”?",
            "16": "I think it is not a common word that I can understand it clearly without any references or any background.",
            "17": "It would be better to add some references for the concept.",
            "18": "Again, I am guessing it is a sort of graph theory that decomposes a large node into small pieces but keeps their connectivity.",
            "19": "But, then how is that exactly linked to phrases of words?",
            "20": "If you make phrases only on consecutive words, it is basically just chunking.",
            "21": "I don’t find any relevance of phrases in a sentence with the (hyper)graph something.",
            "22": "In Figure 2, how is the bidirectional path made between the word representations and phrase representations?",
            "23": "In my understanding of your algorithm based on Equation 2-6, I only see the attention of phrases is computed by word attention, but not the other way.",
            "24": "Please clarify this.",
            "25": "If so, how does the gradient back-propagate to each other?",
            "26": "The biggest concern of this work is the scores reported in Table 1.",
            "27": "I have checked the recent papers which used the Mutli30K(de-en) and other results from WMT[16-18] reports, but the BLEU score reported in Table 1 (20.90) seems way lower than the scores reported by any systems trained by either non-Transformer or Transformer systems.",
            "28": "For that reason, it would be fair to include some results from the state-of-the-art systems on the same dataset.",
            "29": "A minor point but in the complexity analysis, your m is basically n because you take consecutive words from n length of sentence.",
            "30": "You better distinguish which variables are dependent on each other first.",
            "31": "There are MANY typos, missing captions, grammatical errors in the paper.",
            "32": "Here are only some of them:\nThere is no caption for Figure 1.",
            "33": "When you cite a reference with its model name, you better make the citation under parenthesis.",
            "34": "“equation equation 5” -?",
            "35": "“equation 5”\n“Wee” -> “We”"
        },
        "BkxidjcRtB": {
            "0": "This submission proposes to consider to put attention on \"phrases\" in NLP.",
            "1": "The phrases are generated by taking consecutive words in sentences.",
            "2": "Each phrase is treated as a \"node\" in the same way as words.",
            "3": "Then representations of phrases are learned in the network.",
            "4": "The algorithm is applied to two applications, translation and pos tagging.",
            "5": "The proposed method achieved better performance than transformer.",
            "6": "Critics: \n\n1.",
            "7": "In the abstract and the introduction, the submission argues that usefulness of phrases, which are sementic units represented by word groups.",
            "8": "However, in the model development, \"phrases\" are really bigrams and trigrams.",
            "9": "I don't know how much the previous argument is still valid.",
            "10": "Particularly, there are so many bigrams and trigrams.",
            "11": "The effect from these word combinations should have strong effect on the model, but the effect may not be explained as the argument.",
            "12": "2.",
            "13": "I think transformer can somewhat capture word combinations in bigrams and trigrams.",
            "14": "In higher layers, transformer actually combine words in representations.",
            "15": "What is the advantage of the proposed method over the type of combination done in transformer?",
            "16": "3.",
            "17": "The experiment only compares to transformer in the translation task.",
            "18": "It only compares to transformer and semantic phrase transformer.",
            "19": "Other SOTA methods (e.g.",
            "20": "different versions of transformers) should be compared.",
            "21": "4.",
            "22": "The comparison is not really fair.",
            "23": "In each \"layer\" of the proposed phrase transformer, it has actually two self-attention layers, but the baseline has only one self-attention layer."
        },
        "ryg1sde8qB": {
            "0": "I think the paper needs a deep review in the English part.",
            "1": "For example, in the abstract, they repeat \"In this paper\" a couple of time and it is complicated to understand the introduction and methodology.",
            "2": "Also, I think the paper needs a better structure.",
            "3": "The related work should be first in order to understand the relevance of this paper.",
            "4": "From the experiment point of view, it is necessary a better explanation about the hyperparameters or the experiments which were carried out.",
            "5": "In addition, the single database was used to evaluate the technology which is not enough to show the big different respect to the transformed paper.",
            "6": "Also, the comparison is not really fair.",
            "7": "In each \"layer\" of the proposed phrase transformer, it has actually two self-attention layers, but the baseline has only one self-attention layer.",
            "8": "In addition more methodologies should be necessary to compare the results of the experiment.",
            "9": "The architecture part is complicated to follow and I don't understand the big contribution of this paper.",
            "10": "For that reason, I recommend a reject the paper and work more for the final version"
        }
    },
    "H1lTQ1rFvS": {
        "SJx3GtTwtH": {
            "0": "The authors propose R2D2 layers, which are trained to reduce and re-use existing parameters of a neural network layer, and apply this to Transformer and LSTM architectures.",
            "1": "The authors conduct experiments on various NLP tasks, including NLI and NMT.",
            "2": "The main benefit of the proposed R2D2 layer is that the number of parameters can be reduced, as the existing parameters can be reused.",
            "3": "I find this motivation compelling, particularly as it is well known Transformer networks are largely overparameterized.",
            "4": "Comments:\n1.",
            "5": "There is no analysis on the specific choices made for dynamic weight diffusion- the way the partitioning is done could have a large effect on the end result.",
            "6": "There's also little comparison to other ways to share weights across a model besides the proposed weight diffusion method.",
            "7": "2.",
            "8": "Sharing parameters contributes a regularization effect - it is difficult to untie the contributions of increased regularization from the proposed method.",
            "9": "This is particularly problematic as the majority of the datasets used are \"small\" by current standards.",
            "10": "WMT en-de (authors do not include the sizes of the datasets, but this is 4.5 million sentences) is the only large scale dataset, and the BLEU drop is quite large on this dataset compared to the smaller ones such as IWSLT.",
            "11": "To tie my points #1 and #2 together, I feel the authors did experiments on a variety of different tasks, but these style transfer and subject verb agreement tasks are not particularly interesting or realistic - instead this space should be devoted to discussions of the advantages of their method and analysis on its performance, which is quite lightly covered.",
            "12": "3.",
            "13": "The authors claim that the R2D2 Transformer outperforms standard Transformer models on 5 out of 7 NMT tasks.",
            "14": "This appears true if up-sampling with a factor of 2 is used to make the models larger again.",
            "15": "The authors should compare to factorized/quaternion baselines which have a larger quantity of parameters as well.",
            "16": "4.",
            "17": "Table 3, where results are reported on the competitive WMT en-de benchmark, lacks comparison for number of parameters and decoding speed.",
            "18": "This table would probably have the most compelling and impactful results for this paper as this is the most competitive task (aside from the pre-training regime on MNLI/QNLI as part of GLUE).",
            "19": "Can the authors complete this table so readers can understand the parameter reduction and inference speed possible from this method on this benchmark?",
            "20": "(As an aside, the technique should be applicable to the DynamicConv model, which is a Transformer variant?)",
            "21": "5.",
            "22": "The related work section is quite light on other approaches to reducing model size, such as knowledge distillation or quantization?",
            "23": "While the approach taken in this paper leverages parameter sharing, the motivation is similar and I feel acknowledging this entire area of work would be relevant.",
            "24": "6.",
            "25": "I'm not clear on why we see inference time decoding speed improvements based on the description of the method.",
            "26": "Can the authors clarify this point for me?"
        },
        "B1xr0FcpFS": {
            "0": "This paper proposes a new Reuse and Reduce with Dynamic weight Diffusion (R2D2) layer as an alternative to feed-forward layers in neural networks.",
            "1": "The layer is inspired by the Hamilton Product in a hypercomplex space (where numbers have multiple imaginary components).",
            "2": "The main idea is to have two smaller parameter blocks that are partitioned, multiplied, and concatenated together in order to form the full weight matrix of the feed-forward layer.",
            "3": "In extensive experiments on NLI, NMT, text style transfer and subject-verb agreement, feed-forward layers in LSTMs and Transformers are replaced with R2D2 layers.",
            "4": "The modified models achieve similar performance to the originals, while being more than 50% smaller.",
            "5": "Overall, the proposed method is presented clearly and the experiments are comprehensive and convincing.",
            "6": "For these reasons, I am leaning towards accepting this paper.",
            "7": "The proposed method is well explained.",
            "8": "In particular, Figure 1 is helpful to obtain a conceptual picture of the method.",
            "9": "This is in contrast to some of the previous methods based on hypercomplex operations, which often seem harder to grasp and visualize.",
            "10": "In addition, it is helpful that connections to other operations such as matrix multiplication and the Hamilton product are highlighted.",
            "11": "The proposed method is evaluated extensively.",
            "12": "It is applied to different models (LSTMs and Transformers) and on different tasks.",
            "13": "Results are mostly convincing, as performance numbers are competitive with the baselines, while the models are much smaller.",
            "14": "In addition, it compares to previous work, which it outperforms.",
            "15": "The main thing that I'm missing is some analysis of the dynamics of the model, what it is learning (in comparison to using FC layers) or why a smaller number of parameters is still competitive with the standard FC layers.",
            "16": "Are feed-forward layers over-parameterized and only a smaller number of their weights are actually used in practice, similar to lottery tickets (https://arxiv.org/abs/1803.03635)?",
            "17": "How do the learned A and S blocks look like?",
            "18": "Is the entire model learning a different function or do the R2D2 layers just find a way to approximate a feed-forward layer?",
            "19": "Overall, as the method seems straightforward enough to implement and achieves promising results, it has the potential to have some practical impact."
        },
        "BJer5FcCYH": {
            "0": "Paper Summary:\n\nThis paper proposes to train smaller models by decomposing the weights of fully connected networks as the product of smaller matrices, along with a reordering/transpose of the outcome.",
            "1": "The experiments shows that models with less parameters yield comparable performance with their larger counterparts.",
            "2": "Review Summary:\n\nThe method is technically sound and the paper reads well.",
            "3": "Experiments demonstrate the efficacy of the method, although some ablations are missing (see below).",
            "4": "The paper is however not clear on the ultimate objective of the method (speed/accuracy/generalisation?)",
            "5": "and does not compare with alternatives.",
            "6": "Detailed Review:\n\nThe introduction does not make clear if your motivation to make model smaller is training speed, inference speed, memory usage, generalization accuracy.",
            "7": "Please clarify.",
            "8": "The explanation of the method, i.e.",
            "9": "Section 2.2.1, is not clear, in particular for the mapping \\psi.",
            "10": "I feel it would cleared if somewhere in the paper there was an equation with the element-wise correspondence, i.e.",
            "11": "H_{?,?}",
            "12": "= \\sum_k A_i,k S_k,j\nIn that section, you should introduce that n is a hyperparameter before using it as well.",
            "13": "In that section, you could also discuss parameter initialization, and whether this model can use weight decay over H or A/S.",
            "14": "it is also not clear to me if you control the norm ratio between A and S given the weight magnitude is over parameterized.",
            "15": "The experimental section lack a validation/ablation study to help the reader understand the interplay between the number of blocks and the number of latent dimensions.",
            "16": "It will also be good to show learning curves to compare training speed of different parameterization.",
            "17": "Also no training errors are reported, does your method can be seen as a regularizer, i.e.",
            "18": "is training objective closer to valid objective when n grows?",
            "19": "Did you have to change other regularization parameters like dropout.",
            "20": "To me the main weakness of the paper lies in the lack of comparison with alternatives.",
            "21": "Replacing fully connected layers with alternative has a rich literature that the authors ignore.",
            "22": "I feel it is necessary to compare the approach with\n(i) block diagonal approaches, popular since ResNext for convolutions but equally applicable to linear layers.",
            "23": "https://arxiv.org/abs/1611.05431\n(ii) other form of structured sparsity.",
            "24": "https://arxiv.org/abs/1902.09574 (survey).",
            "25": "https://arxiv.org/abs/1812.08301 (squantizer) https://arxiv.org/abs/1802.08435 (block sparsity)...\n(iii) distillation of large models into smaller models.",
            "26": "https://arxiv.org/abs/1503.02531 https://arxiv.org/abs/1702.01802\n(iv) it might not be necessary to compare, but at least mentioning approaches which predict weights from a meta network would be good.",
            "27": "https://arxiv.org/abs/1609.09106\n\nAs a reviewer, I am a bit annoyed that made no effort to have a decent list of related work and that they delegate that work to the reviewers to do so.",
            "28": "Details:\n\"transformation layer\": this is not common terminology, please prefer linear layer or fully-connected layer.",
            "29": "please define all acronyms, e.g.",
            "30": "FC.",
            "31": "The experimental section does not define \\alpha (end of page 6)."
        }
    },
    "HylZIT4Yvr": {
        "rJeyiVOptH": {
            "0": "The paper proposes a model to address the Any-Code Generation (AnyGen) task, which basically to fill missing code from a given program.",
            "1": "The model makes use of partial Abstract Syntax Tree (AST) as input.",
            "2": "The model learns representation for partial AST paths and use the learnt representation to generate AST node at masked steps.",
            "3": "The conducted experiments show that using AST paths from root and leaves are good for AST node generation, but whether those inputs are robust and sufficient should be further explored.",
            "4": "There are some restrictions to the method, for example,  the input is only a single function, and the missing expression is not that complex.",
            "5": "Nevertheless this work presents a novel method towards code generation.",
            "6": "The paper also introduces a new metric to evaluate the prediction accuracy for generated expressions.",
            "7": "Writing is clear.",
            "8": "Evaluation is fairly comprehensive.",
            "9": "Questions:\n1.",
            "10": "Did the author test the method without the camel notation assumption, i.e.",
            "11": "the data contains non-camel notation or mixed notations?",
            "12": "2.",
            "13": "In the Restrict Code Generation test, it seems that the author filters out non-primitive types and user-defined functions.",
            "14": "Therefore, does the experiment on Java-small dataset fully show the proposed model’s strength?",
            "15": "3.",
            "16": "Can the author explain why the SLM model fail on Figure 6?",
            "17": "Is it because of dividing token into sub tokens?",
            "18": "4.",
            "19": "How big is the token vocabulary?",
            "20": "How does the vocab size affect the performance?"
        },
        "BJe4jZK0FH": {
            "0": "This paper proposes a generative task for programming code where an expression from the program is generated given the rest of the program (minus the expression).",
            "1": "This is in line with language modeling for natural language.",
            "2": "The proposed method generates the AST corresponding to the program by generating one node at the time for the missing/to-be-generated expression by approximating the probabilities of the generated notes.",
            "3": "Again, this is similar to the prediction of words in language models.",
            "4": "The method takes into account the AST corresponding to the program.",
            "5": "However, when representing the program, the structure of the AST is not preserved, instead, the AST is represented by generating several sequential paths by traversing paths between connected nodes in the tree.",
            "6": "It would be nice if the paper provided some intuition why generating such connecting paths in the tree are relevant for representing the code, specially for nodes that do not have a direct relationship between them (e.g., the nodes are distant enough in the code that the corresponding probabilities of their nodes do not seem/appear related).",
            "7": "The paper presents results for 2 datasets (comparing with various related work methods).",
            "8": "The results for the Java dataset improve state of the art by 1-2%, while the results for the restricted C# dataset show a much more significant improvement (in the order of 10-15% improvement, depending on the metric).",
            "9": "I would have liked to see a qualitative analysis of the results.",
            "10": "In particular, I would have liked to understand how the predictions differ between acc and tree metrics.",
            "11": "In other words, when the prediction looking at the tree structure is correct and the overall prediction is not, what goes wrong?",
            "12": "It was not clear to me why or if all the paths between 2 nodes are necessary when encoding the partial AST and predicting the missing nodes.",
            "13": "I was not convinced that the ablation studies were relevant.",
            "14": "I would have liked to see ablation studies that considered a subset of the paths in the graph.",
            "15": "The elimination of the methods with more than 20 lines of code seems ad-hoc to me and biases the evaluation with relatively short methods (how many methods were eliminated this way?).",
            "16": "One thing that I struggle with is understanding how useful the proposed task is and how it can be generalized/used in practice for some relevant higher level task in AI4code."
        },
        "BJx4Rjq85B": {
            "0": "This paper presents a grammar-based generation approach for \"slot-filling\" style code generation tasks.",
            "1": "Given a context AST with opening non-terminal node, the model completes the opening node by predicting a sequence of child nodes, which forms a sub-AST rooted at the original opening node.",
            "2": "The proposed model encodes context ASTs using a path-based approach (Alon et al., 2019a), essentially generalizing the previous model of Alon et al., 2019a from a code-to-sequence setting (e.g., generating natural language comments from code) to a \"code-to-code\" setting (i.e., code completion given contextual snippets).",
            "3": "Strong Points:\n\n* The paper is very well written.",
            "4": "The idea of formalizing code completion as structured language modeling and extending Alon et al., 2019a for the task is natural and well executed, with strong models and significantly improved results on two code completion benchmarks for both Java and C#.",
            "5": "* The authors attempted to establish comparisons with most existing code generation models.",
            "6": "Detailed Review:\n\n*Technical Contribution* I have a very mixed feeling with this paper, while the model registers high empirical performance, the technical contribution is a bit limited, as detailed below:\n\n    - *Path-based Context Encoding* The most important contribution in this submission is the application of path-based AST encoding model of Alon et al., 2019a to encode context (the given contextual and partially generated ASTs) for code generation.",
            "7": "While the path-based encoding scheme is indeed a powerful model that intuitively encapsulates and generalizes over most previous approaches (Section 7), applying the model to a different task without significant task-specific adaptation or in-depth analysis might not sound technically novel.",
            "8": "Meanwhile, the core idea of modeling/encoding the information flow in both the given context AST and partially generated programs for opening node expansion has already been explored in Brockschmidt et al.",
            "9": "(2019a), albeit using a different encoding approach (GNNs) and in a relatively restricted setting of generating arithmetic expressions.",
            "10": "- *Node-based Tree Generation Model* Apart from the path-based context encoding model, the node-based generation model presented in Section 2 also seems interesting.",
            "11": "However, it might take longer time-steps to generate the node sequence instead of the sequence of production rules (composed of multiple child nodes), which could make optimization and inference more difficult.",
            "12": "On the other hand, to control arity, the node-based approach need to inject synthetic \"EOS\" nodes to signal end of generating an argument sequence, while existing production rule-based systems could easily generate arbitrary number of argument nodes using either a transition system (e.g., Yin and Neubig, 2018) or a special neural component to compute end-of-argument-list probability (e.g., Rabinovich et al.",
            "13": "(2017)), without using separate production rules of different arity.",
            "14": "- *Syntactic Copy Mechanism* While the proposed syntactic terminal token copy mechanism (Section 3.3) could be better than the vanilla sequential one, there have already been syntactic copying models capable of copying both terminal tokens and partial ASTs from the context (Yin et al., 2019).",
            "15": "How to Improve: to better understand the different technical contributions outlined above and their relative impacts, the following ablation studies would be helpful:\n\n        - Importance of Path-based Context Encoding: the Seq→Path ablation in Table 3 alone might not be adequate to demonstrate the importance of path-based encoding of AST contexts for code generation tasks.",
            "16": "The authors should compare with the GNN-based context encoding approach in Brockschmidt et al.",
            "17": "(2019a) as this is the most relevant work.",
            "18": "The original GNN→NAG model cited in Table 2 used a much simpler copying mechanism and a vanilla production-based tree generation model, and therefore not directly comparable with a tuned SLM.",
            "19": "- Importance of Node-based Tree Generation Model: If possible, the authors might consider swapping their node-based tree generation model with a state-of-the-art production-based approach (e.g., Yin et al., 2019) to demonstrate its effectiveness.",
            "20": "*Claims* The authors claimed in the beginning of the paper that previous program synthesis approaches are either restricted in domains (e.g., DSLs like SQL) or grammars (e.g., restricted grammar of the full language), therefore coining the proposed approach as \"any-code generation\".",
            "21": "However, there are indeed code generation systems (some of them cited in this paper) that synthesize open-domain code snippets in general-porpuse programming languages without restriction on vocabulary or grammar.",
            "22": "To give a few examples, Iyer et al.",
            "23": "(2018) generate open-domain Java class member functions; Rabinovich et al.",
            "24": "(2017) and Zhao et al.",
            "25": "(2019) predict full Python classes or partial snippets, while Yin et al.",
            "26": "(2019) synthesize open-domain short C# code diffs observed in GitHub commits.",
            "27": "In fact, the the proposed \"any-code generation\" benchmark is limited to sub-expressions defined within a function, whose scope is more restricted than other benchmarks like CONCODE.",
            "28": "How to improve: the authors might present more evidence to substantiate the their claim on the novelty of the AnyGen benchmark compared with existing open-domain, general purpose code synthesis benchmarks, or consider revising the claim and the title.",
            "29": "References:\n\n* Zhao et al., Neural Networks for Modeling Source Code Edits.",
            "30": "2019\n* Yin et al., Learning to Represent Edits.",
            "31": "2019"
        }
    },
    "BkepbpNFwr": {
        "Hklyku7AYB": {
            "0": "This paper proposes an extensible attention mechanism applied on the previous hidden state of an RNN and resulting in supplementary input for the next RNN step.",
            "1": "For each added domain, new pairs of attentions key and values can be added to provide more capacity for the model.",
            "2": "This method is applied in the context of incremental domain adaptation for NLP without the possibility of storing of old samples (episodic memory).",
            "3": "Pros:\n- Extensive ablation study with the different possible combinations of methods\n- Very interesting comparison between expanding the memory (i.e.",
            "4": "attention) and expanding the hidden states.",
            "5": "Using the attention results in better results for a same number of added parameters and the activations sizes stay the same even when the attention is extended with new pairs.",
            "6": "- Paper is well written/motivated\n\nWeaknesses:\n- MultiNLI seem to have too much correlation between tasks.",
            "7": "It would have been better to be able to observe catastrophic forgetting for the source domain.",
            "8": "In the appendix, the metrics have really strong disagreement so it is tough to judge for these two corpuses.",
            "9": "- When you give the numbers for multi-task learning, you should use your extended memory method to be fair with MT learning.",
            "10": "I would just be interested to see it, just as a proper upper bound.",
            "11": "Otherwise, the paper proposes a novel method which works well in practice so I am leaning towards acceptance."
        },
        "r1gIw_JlcB": {
            "0": "*** Summary\n\nThis work proposes to use an augmented RNN model to address the incremental domain adaptation problem.",
            "1": "In particular, it designs the progressive memory bank approach which expands the memory capacity by adding parameters every time a new task comes in.",
            "2": "The RNN retrieves knowledge from the memory bank via key-value attention.",
            "3": "A proof in a highly simplified case is given in addition to empirical results showing that expanding the memory bank is better than expanding the RNN states.",
            "4": "*** Strengths\n\n1.",
            "5": "Section 3 is well-written.",
            "6": "The methods and motivations are illustrated clearly.",
            "7": "2.",
            "8": "Comprehensive experiments are conducted.",
            "9": "Supportive results for the arguments presented in Section 3 are therefore demonstrated.",
            "10": "*** Weaknesses\n\n1.",
            "11": "Regarding Table 2., multiple runs of different sources and targets would be helpful to better understand the effectiveness of the proposed methods in the 2-domain set-up.",
            "12": "2.",
            "13": "The choice of key-value memory bank is not intuitive.",
            "14": "A comparison between this memory and the traditional attention can help demonstrate the validity of this choice."
        },
        "r1gnymt9cH": {
            "0": "\n###Summary###\nThis paper introduces incremental domain adaptation for natural language processing, assuming that each domain comes one after another and only the current domain can be accessed in the application scenario.",
            "1": "The basic framework of this paper is based on RNN but augmented with the directly parameterized memory bank.",
            "2": "The memory bank of this paper is a set of distributed, real-valued vectors capturing domain knowledge.",
            "3": "When the model is adapted to the new domain, the model progressively increases the slots in the memory bank.",
            "4": "The paper evaluates the proposed approach on an NLP classification task, i.e.",
            "5": "multi-genre natural language inference (MultiNLI).",
            "6": "The dataset used in this paper includes 5 genres: Slate, Fiction, Telephone, Government and Travel.",
            "7": "In the experiments, the paper performs the dynamics of the progressive memory network for IDA as well as compares the proposed method with variants and previous work in the multi-domain setting.",
            "8": "### Novelty ###\n\nThis paper proposes incremental domain adaptation, which is inspired by Li & Hoiem's work.",
            "9": "The setting assumes that each domain comes one after another and only one domain can get accessed.",
            "10": "This setting is interesting as we will encounter this setting in the real application scenarios, i.e., the domain knowledge in the real domain is unpredictable.",
            "11": "Thus, the problem setting provides some novelty.",
            "12": "However, I am not sure whether assuming that we can only get access to one domain is reasonable as we can always save the data for the domain we have already seen.",
            "13": "From the perspective of the method, this paper incorporates the memory bank to the RNN, which is not new in the machine learning research area, but heuristic enough for the transfer learning community.",
            "14": "###Clarity###\n\nOverall, the paper is well organized and logically clear.",
            "15": "The proposed claims are well supported by the experiments and analysis.",
            "16": "The images are well-presented and well-explained by the captions and the text.",
            "17": "###Pros###\n\n1) The paper proposes an incremental domain adaptation scenario where one domain appears another and only the data from the current domain can be accessed, which is interesting and heuristic to the domain adaptation research community.",
            "18": "2) The paper is applicable to many practical scenarios since the data from the real-world application is typically from multiple domains and the data is from one domain at a time.",
            "19": "3) The paper is overall well-organized and well-written.",
            "20": "The claims of the paper are verified by the experimental results.",
            "21": "###Cons###\n\n1) The paper has a good motivation for the setting, however, one of the critical drawbacks of this paper is that the papers fail to compare with the state-of-the-art baselines.",
            "22": "I understand that this paper has a new setting, but since the authors also compare the proposed method with the \"multi-task\" learning, it will be helpful to compare with state-of-the-art multi-task or multi-source baselines.",
            "23": "2) The experimental results provided in this paper are weak.",
            "24": "In Table 4, we found that sometimes, the IDA method performs worse than the multi-task baselines.",
            "25": "3) The paper presents no ablation study or analysis of the experimental results.",
            "26": "The effectiveness of the memory bank, fine-tuning/freezing learning parameters is unclear.",
            "27": "It will be also interesting to see how does the proposed method performs on large-scale visual datasets.",
            "28": "Based on the summary, cons, and pros, the current rating I am giving now is weak reject.",
            "29": "I would like to discuss the final rating with other reviewers, ACs.",
            "30": "I am willing to improve my rating if the authors can address my following concerns.",
            "31": "To improve the rating, the author should explain the following questions:\n1).",
            "32": "Why assuming that we can only get access to the data from one domain is a reasonable setting, since we can always save the data (or at least the statistics about the data) form the domains we have already observed.",
            "33": "2).",
            "34": "Can the proposed approach generalize to visual domain adaptation, i.e.",
            "35": "on the visual task instead of NLP task?",
            "36": "3).",
            "37": "The drawbacks I mentioned in the paper Cons section.",
            "38": "#################### Updated Review  ###########\nThe authors have addressed most of my concerns I proposed in the initial review, thus I raise my score to 6 weak accept.",
            "39": "1) In the response to the review, the authors claim that from the data privacy's perspective, it's reasonable to assume that we can only get access to one dataset at one time, which makes sense.",
            "40": "2) I would be really interested in how the similar idea works on vision data, which could be a future work for this paper."
        }
    },
    "Byg1v1HKDB": {
        "BJlqUqW3tH": {
            "0": "Summary: the paper purposes a dataset of abductive language inference and generation.",
            "1": "The dataset is generated by human, while the testing set is adversarially selected using BERT.",
            "2": "The paper experiments the popular deep learning models on the dataset and observe shortcoming of deep learning on this task.",
            "3": "Comments: overall, the problem on abductive inference and abductive generation in language in very interesting and important.",
            "4": "This dataset seems valuable.",
            "5": "And the paper is simple and well-written.",
            "6": "Concerns: I find the claim on deep networks kind of irresponsible.",
            "7": "1.",
            "8": "The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage.",
            "9": "After all, the paper says BERT scores 88% before the dataset is attacked.",
            "10": "2.",
            "11": "The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction.",
            "12": "To compare the author should use the average score of human.",
            "13": "3.",
            "14": "The ground truth is selected by human.",
            "15": "On a high level, the main difficulty of abduction is to search in the exponentially large space of hypothesis.",
            "16": "Formulating the abduction task as a (binary) classification problem is less interesting.",
            "17": "The generative task is a better option.",
            "18": "Decision: despite the seeming unfair comparison, this task is novel.",
            "19": "I vote for weak accept."
        },
        "BklqO2_AKH": {
            "0": "This paper introduces two new natural language tasks in the area of commonsense reasoning: natural language abductive  inference and natural language abductive  generation.",
            "1": "The paper also introduces a new dataset, ART, to support training and evaluating models for the introduced tasks.",
            "2": "The paper describes the new language abductive tasks, contrasting it to the related, and recently established, natural language inference (entailment) task.",
            "3": "They go on to describe the construction of baseline models for these tasks.",
            "4": "These models were primarily constructed to diagnose potential unwanted biases in the dataset (e.g., are the tasks partially solved by looking at parts of the input, do existing NLI models far well on the dataset, etc.",
            "5": "), demonstrating a significant gap with respect to human performance.",
            "6": "The paper, and the dataset specifically, represent an important contribution to the area of natural language commonsense reasoning.",
            "7": "It convincingly demonstrates that the proposed tasks, while highly related to natural language entailment, are not trivially addressed by existing state-of-the-art models.",
            "8": "I expect that teaching models to perform well on this task can lead to improvements in other tasks, although empirical evidence of this hypothesis is currently absent from the paper.",
            "9": "Below are a set of more specific observations about the paper.",
            "10": "Some of these comments aim to improve the presentation or content of the paper.",
            "11": "1.",
            "12": "In Section “5.1 - Pre-trained Language Models” and attendant Table 1describe results of different baselines on the ART inference task.",
            "13": "The results in the table confused me for quite some time, I’d appreciate some clarifications.",
            "14": "With respect to the differences with columns 1 (GPT AF) and 2 (ART, also BERT AF) I would like the comparison to be made more clear.",
            "15": "As far as I understand it, there are 2 parts of the dataset that can be varied: (1) the train+dev sets and (2) the test set.",
            "16": "Furthermore, it seems that it makes sense to vary each of these at a time, if we are to compare results with variants.",
            "17": "For example: we can fix the test set, and vary how we generate training and dev examples.",
            "18": "If a model does better with the same test set, we can assume the train+dev examples were better for the model (for whatever reasons, closer distribution to test, harder or more \ninformative training examples, etc).",
            "19": "We can also keep the train+set constant, and vary the test set.",
            "20": "This allows us to evaluate which test set is harder with respect to the training examples.",
            "21": "The caption of Table 1 implies that both columns are evaluations based on the “ART” test set.",
            "22": "If that is correct, then the train+dev set generated from the GPT adversarial examples is of better quality, generating a BERT-ft (fully connected) model that is 3% better.",
            "23": "But the overall analysis seems to indicate that this is not what was done in the experiment.",
            "24": "Rather, it seems that *both* the train+dev _and_ the test sets were modified concurrently.",
            "25": "If that is the case, I would emphasize that the text needs to make this distinction clear.",
            "26": "Furthermore, I would say that varying both train and test sets concurrently is sub-optimal, and makes it a bit harder to draw the conclusion that BERT adversarial filtering leads to a stronger overall dataset.",
            "27": "2.",
            "28": "Along the lines of the argument in (1), above, I would urge the authors to publish the *entire* set of generated hypotheses (plausible and implausible) instead of only releasing the adversarially filtered pairs.",
            "29": "Our group’s experience with training inference models is that it is often beneficial to train using “easy” examples, not only hard examples.",
            "30": "I suspect the adversarially filtered set will focus on hard examples only.",
            "31": "While this is fine to do in the test set, I think if the full set of annotated/generated hypotheses are released, model designers can experiment with combining pairs of hypothesis in different ways.",
            "32": "3.",
            "33": "Furthering the argument of (2): in certain few-shot classification tasks, one is typically asked to identify similarity between one test example and different class representatives.",
            "34": "Experience shows that it is often beneficial to train the model on a larger number of distractor classes than what the model is eventually evaluated on (e.g., https://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning).",
            "35": "In the alpha-NLI setting, have you experimented with training using multiple distractors, instead of only 1, during training (even if you end up evaluating over 2 hypotheses)?",
            "36": "4.",
            "37": "One potential argument for introducing a new natural language task is of transfer learning: learning to perform a complex natural language task should lead to better natural language models more generally, or, for some other related tasks.",
            "38": "This paper does not really touch on this aspect of the work.",
            "39": "But, potentially, one investigation that could be conducted is through a reversal of the paper’s existing NLI entailment experiment.",
            "40": "The paper shows that NLI entailment models do not perform well on alpha-NLI.",
            "41": "But it would be interesting to see if a model trained on alpha-NLI, and fine-tuned or multi-tasked on NLI entailment, does better on NLI entailment (i.e., is there transfer from alpha-NLI to entailment NLI?).",
            "42": "5.",
            "43": "Another option is to evaluate whether alpha-NLI helps with other commonsense tasks.",
            "44": "One other example is Winograd Schema Challenge, which current systems also perform well below human performance.",
            "45": "It also seems that the Winograd Schema Challenge questions are not too far from abductive inference.",
            "46": "6.",
            "47": "In the abstract of the paper, before the paper defines the abductive inference/generation task specifically, the claim that abductive reasoning has had much attention in research seemed awkward.",
            "48": "Informally, most commonsense reasoning (including NLI entailment) could be cast as abductive reasoning.",
            "49": "7.",
            "50": "In at least one occasion, I found an acronym which was hard to find the definition for (“AF” used in Section “5.1 - Pre-trained Language Models”; I assumed it was “adversarial filtering”.)",
            "51": "8.",
            "52": "In Section “5.1 - Pre-trained Language Models” it seems that the text quotes an accuracy for BERT-ft (fully connected) of 68.9%, but Table 1 indicates 69.6%.",
            "53": "9.",
            "54": "In Section “5.1 - Learning Curve and Dataset Size”, there is a claim that the performance of the model plateaus at ~10,000 instances.",
            "55": "This does not seem supported by Figure 5.",
            "56": "There appears to be over 5-7% accuracy (absolute) improvements from 10k to 100k examples.",
            "57": "Maybe the graph needs to be enhanced for legibility?",
            "58": "10.",
            "59": "It is great that the paper includes human numbers for both tasks, including all the metrics for generation.",
            "60": "11.",
            "61": "Period missing in footnote 8.",
            "62": "12.",
            "63": "The analysis section is interesting, it is useful to have in the paper.",
            "64": "However, Table 3 is a bit disappointing in that ~26% of the sampled examples fit into one of the categories.",
            "65": "It would be great if the authors could comment on the remaining ~74% of the sampled dataset."
        },
        "ByltNYc0tr": {
            "0": "This paper proposes a new task/dataset for language-based abductive reasoning in narrative texts.",
            "1": "Pros: \n\n-\tThe proposed task is interesting and well motivated.",
            "2": "The paper contributes a dataset (20,000 commonsense narratives and 200,000 explanatory hypotheses).",
            "3": "The construction of the dataset was performed carefully (e.g., avoiding annotation artifacts).",
            "4": "-\tThe paper established many reasonable baselines.",
            "5": "-\tThe paper conducted detailed analysis, which invites more research on this task: despite the strong performance of many existing systems on NLI/RTE, there are larger gaps between the performance of these models and human performance on the proposed task.",
            "6": "The experiments well support the conclusions made in the paper.",
            "7": "-\tThe paper is well structured and easy to follow.",
            "8": "It is well written.",
            "9": "Cons/comments: \n\n-\tWhile this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited.",
            "10": "I also suggest the paper discusses e-SNLI a bit more.",
            "11": "-\tThe paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between.",
            "12": "I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.",
            "13": "-\tShould the title of the paper specify the paper is about “language-based” abductive reasoning.",
            "14": "-\tA minor one: “Table 7 reports results on the αNLI task.” Should it be “Table 2”?"
        }
    },
    "rkgc06VtwH": {
        "Byx39qufKS": {
            "0": "This paper proposes a reranking architecture with a LogicForm-to-NaturalLanguage preprocessing step for semantic parsing.",
            "1": "The authors experiment their method on three datasets and get the state of the art results.",
            "2": "The proposed method is natural.",
            "3": "But using neural models to rank (or rerank) is a long-existing technique, regardless of the chosen parametrization of the reranking model.",
            "4": "This paper chose BERT.",
            "5": "See section-2.6 of this tutorial for more details about using neural models to rank: https://www.microsoft.com/en-us/research/uploads/prod/2017/06/INR-061-Mitra-neuralir-intro.pdf.",
            "6": "Overall, I think the paper is not ready to publish for the following reasons.",
            "7": "1.",
            "8": "The method relies much upon manual designs that seem hard to generalize.",
            "9": "By converting the logic forms to natural languages, the authors can leverage paraphrase datasets and pre-train the critic as a paraphrase model.",
            "10": "However, the way they convert the logic forms is different for each dataset and they have to manually design rules for each logic form.",
            "11": "2.",
            "12": "It is not clear how certain experimental designs were made.",
            "13": "The authors chose to not rerank if the candidates' scores are too low or high but close.",
            "14": "Such choice and associated thresholds seem arbitrary: how were they actually found out?",
            "15": "Were they tuned on a development set?",
            "16": "How does the method work if the candidate with the highest score is always picked: in the end, this is what the model is supposed to learn, correct?",
            "17": "Other designs include beam size, whether or not to use a pretrained model, etc.",
            "18": "How were such decisions made?",
            "19": "Tuned on a development set?",
            "20": "3.",
            "21": "The results are not sound enough.",
            "22": "Given the issued pointed out in 1 and 2, I am not sure if the results are really sound as the authors claimed.",
            "23": "For example, what if the authors don’t use a LogicForm-to-NaturalLanguage conversion?",
            "24": "What is the result if we directly learn to match input and logic forms?",
            "25": "Moreover, the authors better answer questions in 2 so I can gauge if their hyper-parameters were chosen in the principled ways.",
            "26": "Once those are answered, a significant test had better be done since the improvement seems small.",
            "27": "4.",
            "28": "Claiming Shaw et al.",
            "29": "2019 in table-3 as ``our methods’’ is wrong.",
            "30": "It is clear that Shaw et al.",
            "31": "(2019) didn't experiment on OVERNIGHT dataset, but setting up the baseline on a dataset should not be classified as ``our method’’.",
            "32": "Moreover, I have some comments on the model and experiments.",
            "33": "These are not weakness, but I think some work in this direction may help improve the paper.",
            "34": "1.",
            "35": "The model architecture should be better justified.",
            "36": "In its current form, the two arguments (input query and output sequence translated from a logic form) are interchangeable.",
            "37": "Why so?",
            "38": "Why isn’t an asymmetric architecture more natural?",
            "39": "How can the authors use a pair of logic forms as negative examples (in figure-2)?",
            "40": "Why do the authors use the Quora dataset in particular?",
            "41": "2.",
            "42": "The error analysis might be better to be a bit more quantitative.",
            "43": "Its current form doesn’t seem to give insight on how the proposed method really helps.",
            "44": "What the authors can do is: you can sample some sentences from the test/development set and count how many comparative words are misused in the original model, among which how many are corrected by reranking."
        },
        "BkgMQmijtH": {
            "0": "This paper proposes a framework for semantic parsing, which includes a neural generator that synthesizes the logical forms from natural language utterances, and a neural reranker that re-ranks the top predictions generated by beam search decoding using the neural generator.",
            "1": "While the neural generator is the same as prior work, the main novelty is the reranker design, which is a binary classifier that takes a pair of natural language utterance/logical form, and predicts the similarity between them.",
            "2": "This reranker could also be pre-trained using auxiliary data sources, e.g., Quora question pairs benchmark for paraphrasing.",
            "3": "They evaluate their approach on 3 semantic parsing datasets (GEO, ATIS, and OVERNIGHT), and show that their reranker can further improve the performance of the base generator.",
            "4": "I think the general motivation of the framework is sound.",
            "5": "Although the idea of reranking is not new in the semantic parsing community, with the most recent work [1] already shows the promise of this direction, the concrete approach described in this paper is different, seems simple yet effective.",
            "6": "The most interesting part is to transform the generated logical form into a pseudo-natural language text, so that it becomes a paraphrase of the input natural language utterance in some sense, which enables the re-ranker to be pre-trained with auxiliary data sources, and to use the wordpiece tokenizer that is effective in understanding natural language.",
            "7": "In their evaluation, they indeed show that this transformation helps improve the performance of the reranker.",
            "8": "My main concern of  this paper is about evaluation.",
            "9": "First, although they already evaluate on 3 datasets, all of them are not among the most challenging benchmarks in semantic parsing.",
            "10": "In [1], they also evaluate on Django and Conala, which are 2 benchmarks to translate natural language to Python, and also are more complicated than the benchmarks in this paper.",
            "11": "It would be helpful for the authors to show results on such datasets that the results of baseline neural generators are less satisfactory, which may also make more room for the possible improvement using a re-ranker.",
            "12": "On the other hand, they also lack a comparison with existing re-ranking approaches.",
            "13": "For example, it will be helpful to compare with [1], given that they also evaluate on GEO and ATIS.",
            "14": "Right now the results are not directly comparable because: (1) the base generators are different; and (2) the beam size used in this paper (10) is larger than the beam size (5) in [1].",
            "15": "It will be helpful if the authors can at least provide results with a smaller beam size, and would be better if they can provide results that are directly comparable to [1].",
            "16": "[1] Yin and Neubig, Reranking for Neural Semantic Parsing, ACL 2019.",
            "17": "------------\nPost-rebuttal comments\n\nI thank the authors for the response.",
            "18": "However, I don't think my concerns are addressed; e.g., without a comparison with previous re-ranking methods, it is hard to justify their proposed approach, given that other re-ranking methods are also able to improve over an existing well-performed generator.",
            "19": "Therefore, I keep my original assessment.",
            "20": "------------"
        },
        "HygYH9sl5H": {
            "0": "In this paper, a method for re-ranking beam search results for semantic parsing is introduced and experimentally evaluated.",
            "1": "The general idea is to train a paraphrase critic.",
            "2": "Then, the critic is applied to the each pair (input sentence, logic form) in the beam to determine if they are close.",
            "3": "The main problem with the proposed method is that the critic does not receive high quality negative examples.",
            "4": "The generator is never trained to adapt to the critic.",
            "5": "Second big problem is that the critic trained on two sources of data: the original dataset and the Quora paraphrasing dataset.",
            "6": "It is very unclear what is the impact of each of the data sources.",
            "7": "Also, it is unclear how the critic works in this case.",
            "8": "It seems to be an easy task to distinguish a logical form from a natural sentence.",
            "9": "In general, the paper is well written.",
            "10": "I would suggest to reduce the size of the introduction and dedicate this space to more detailed explanation how reranking works and the experimental details.",
            "11": "Figures don't add much to understanding.",
            "12": "The experimental part is rather weak.",
            "13": "The error analysis part is great, but not very methodical.",
            "14": "It is not clear is these examples are cherry picked or it is frequent mistake of the baseline.",
            "15": "I would like to the accuracy of the critic and the analysis of its performance.",
            "16": "The critic is the main contribution of this paper and it is strange that so little attention is dedicated to it.",
            "17": "Other aspects that need to be highlighted in the experimental section: \n- how the Quora pretraining helps\n- do other strategies for negative sample work\n- how important is not to rerank in certain cases (Sec 3.3)\n\nIn conclusion, I encourage the authors to develop the idea further.",
            "18": "Taking in into account the issues with the method (or its presentation) and the experimental weaknesses I recommend reject for now.",
            "19": "Typos:\n- [CLS] is not defined in text\n- Shaw et al.",
            "20": "should be in Previous methods in Table 3"
        }
    },
    "S1eYKlrYvr": {
        "r1l12CqnYr": {
            "0": "This paper aims to identify the primary source of transfer error in vision&language navigation tasks in unseen environments.",
            "1": "The authors tease apart the contributions of the out-of-distribution severity of language instructions, navigation graph (environmental structure), and visual features, and conclude that visual differences are the primary form in which unseen environments are out of distribution.",
            "2": "They show that using ImageNet class scores as visual features results in significantly less transfer gap than using low-level visual features themselves.",
            "3": "Experiments then show that semantic-level features dramatically reduce the transfer gap, although at a cost of absolute performance.",
            "4": "I recommend this paper for acceptance; my decision is based on the thorough analysis of the ultimate cause of a recurring problem in this field.",
            "5": "These results, if shown to hold across a significant number of datasets and tasks, would significantly change the focus of research in this field toward a focus on robust high-level visual representations (as opposed to e.g.",
            "6": "better spatial awareness or better language understanding).",
            "7": "This work represents an important step in this direction.",
            "8": "The description of the 'learned' features in 6.3 could use more elaboration.",
            "9": "Since it is the best performing approach by a large margin (as measured by transfer gap), it should probably get more than one sentence.",
            "10": "In particular, what do the authors mean by \"train a separate multi-layer perceptron to predict the areas of these semantic labels\"?",
            "11": "Does that mean the predicted pixel-level semantic segmentation map is used as input to the navigating agent?",
            "12": "Or is it an auxiliary task for representation learning?",
            "13": "etc.",
            "14": "This should be clarified.",
            "15": "I anticipate this paper to significantly influence future work in this area.",
            "16": "--------\n\nAfter discussing with the reviewers about the methodological issue of the validation set, I have lowered my score to a weak accept, but I think this paper should still be published."
        },
        "S1gbNt0nYr": {
            "0": "This paper has two main contributions.",
            "1": "First, the authors perform an extensive study to understand the source of what they refer to as 'environment bias', which manifests itself as a gap in performance between environments used for training and unseen environments used for validation.",
            "2": "The authors conclude that of the three sources of information provided to the agent (the natural language instruction, the graph structure of the environment, and the RGB image), the RGB image is the primary source of the overfitting.",
            "3": "The second contribution is to use semantic information, compact statistics derived from (1) detected objects and (2) semantic segmentation, to replace the RGB image and provide input to the system in a way that maintains state-of-the-art performance but shrinks the performance gap between the seen and unseen data.",
            "4": "This paper has some pretty exhaustive treatment diagnosing the source of the agent's 'environment bias' (which, as I discuss below, I believe is more accurately referred to as 'overfitting') in Sec.",
            "5": "4.",
            "6": "To me, this is this highlight of the paper, and some interesting work; the investigation of the behavior of the system is interesting and informative.",
            "7": "It provides a framework for thinking about how to diagnose this behavior and identify its source.",
            "8": "The authors use this rather extensive study to motivate the need for new features (semantic features) to replace the RGB image that their investigation finds is where much of this 'environment bias' is located.",
            "9": "Unfortunately, it is here that the paper falls flat.",
            "10": "The authors proposal methods perform nominally better on the tasks being investigated, but much of the latter portion of the paper continues to focus on the 'improvement' in the metric they use to diagnose the 'bias'.",
            "11": "As I mention below, the metric for success on these tasks is performance on the unseen data, and, though an improvement on their 'bias' metric is good anecdotal evidence their proposed methods are doing what they think, the improvements in this metric are largely due to a nontrivial decrease in performance on the training data.",
            "12": "Ultimately, this is not a compelling reason to prefer their method.",
            "13": "I go into more details below about where I think some of the other portions of the paper could be improved and include suggestions for improvement.",
            "14": "High-level comments:\n- I am uncertain that 'bias' is the right word to describe the effect under study.",
            "15": "In my experience, environment bias (or, more generally, dataset bias) usually implies that the training and test sets (or some subset of the data) are distinct in some way, that they are drawn from different distributions.",
            "16": "The learning system cannot identify these differences without access to the test set, resulting in poor performance on the 'unseen' data.",
            "17": "In the scenario presented here, the environments are selected to be in the train/test/validation sets at random.",
            "18": "As such, the behavior described here is probably more appropriately described as 'overfitting'.",
            "19": "The shift in terminology is not an insignificant change, because using 'bias' to describe the problem incorrectly suggests that the data collection procedure is to blame, rather than a lack of data or an overparamatrized learning strategy; I imagine that more data in the training set (if it existed) could help to reduce the gap in performance the paper is concerned with.",
            "20": "That being said, I imagine some language changes could be done to remedy this.",
            "21": "- Perhaps the biggest problem with the paper as written is that I am not convinced that the 'performance gap' between the seen and unseen data is a metric I should want to optimize.",
            "22": "This metric is instructive for diagnosing which component of the model the overfitting is coming from, and Sec.",
            "23": "4 (devoted to a study of this effect) is an interesting study as a result.",
            "24": "However, beyond this investigation, reducing the gap between these two is not a compelling objective; ultimately, it is the raw performance on the unseen data that matters most.",
            "25": "The paper is written in a way that very heavily emphasizes the 'performance gap' metric, which gets in the way of its otherwise interesting discussion diagnosing the source of overfitting and some 'strong' results on the tasks of interest.",
            "26": "The criteria should be used to motivate newer approaches, rather than the metric we should value for its adoption.",
            "27": "This narrative challenge is the most important reason I cannot recommend this paper in its current state.",
            "28": "- Using semantic segmentation, rather than the RBG image, as input seems like a good idea, and the authors do a good job of motivating the use of semantics (which should show better generalization performance) than a raw image.",
            "29": "However, the implementation in Sec.",
            "30": "6.3 raises a few questions.",
            "31": "First (and perhaps least important) is that 6.3 is missing some implementation details.",
            "32": "In this section, the authors mention that 'a multilayer perceptron is used' but do not provide any training or structure details; these details should be included in an appendix.",
            "33": "More important is the rather significant decrease in performance on the seen data (11% absolute) when switching to the learned method.",
            "34": "Though the performance on the unseen data does not change much, it raises some concerns about the generalizability of the learning approach they have used: in an ideal world with infinite training data, the network would perfectly accurately reproduce the ground truth results, and there should be no difference between the two.",
            "35": "Consequently, the authors should comment on the discrepancy between the two and the limits of the learned approach, which I worry may limit its efficacy if more training data were added.",
            "36": "Smaller comments:\n- I do not fully understand why the 'Touchdown' environment was included in Table 1, since the learned-semantic agent proposed in the paper was not evaluated.",
            "37": "The remainder of the experiments are sufficient to convince the reader that this gap exists, and I would recommend either evaluating against the proposed technique or removing this task from the paper.",
            "38": "- Figure captions should be more 'self-contained'.",
            "39": "Right now, they describe only what is shown in the figure.",
            "40": "They should also describe what I, as a reader, should take away or learn from the figure.",
            "41": "This is not always necessary, but in my experience improves readability, so that the reader does not need to return to the body of the text to understand.",
            "42": "- The use of a multilayer perceptron for the Semantic Segmentation learned features, trained from scratch, stands out as a strange choice, when there are many open source implementations for semantic segmentation exist and could be fine-tuned for this task; a complete investigation (which may be out of scope for the rebuttal period) may require evaluating performance of one of these systems."
        },
        "HyeuvT_0Kr": {
            "0": "Summary: This paper provides a thorough analysis of why vision-language navigation (VLN) models fail when transferred to unseen environments.",
            "1": "The authors enumerate potential sources of the failure--namely, the language, the semantic map, and the visual features--and show that the visual features are most clearly to blame for the failures.",
            "2": "Specifically, they show that by removing the low-level visual features (e.g.",
            "3": "the fc17 or similar) and replacing with various higher-level representations (e.g.",
            "4": "the softmax layer of the pretrained CNN, or the output of a semantic segmentation system) dramatically improves generalization without a meaningful drop in absolute performance.",
            "5": "Evaluation: The paper is easy to follow and interesting.",
            "6": "Some results presented have been show previously (e.g.",
            "7": "that removing visual features doesn't drastically hurt performance of VLN models) but overall, the paper presents the results in a clear and thorough manner that will be beneficial to the community.",
            "8": "A few small questions/comments below.",
            "9": "* I am confused by how you compute BLEU in Section 4.1.",
            "10": "You say you compute corpus BLEU but Eq.",
            "11": "2 suggests you compute the BLEU for a single instruction against a set of training instructions.",
            "12": "I think corpus BLEU is usually corpus vs. corpus (e.g.",
            "13": "all generated sentences vs. all reference sentences) not one generated sentence against all reference sentences.",
            "14": "Is this right?",
            "15": "It also seems odd that your BLEU scores are distributed the way they are (Fig.",
            "16": "2).",
            "17": "Can you explain why you did this the way you did?",
            "18": "* nit: Sec.",
            "19": "5 heading.",
            "20": "Your grammar is backwards.",
            "21": "The question you are trying to express is \"bias is attributed to what\" not \"what is attributed to bias\".",
            "22": "So heading should be \"to what inside the environments is bias attributed\" (which is admittedly a clunky title)\n* another nit: \"suggest a surprising conclusion: the environment bias is attributed to low-level visual information carried by the ResNet features.\"",
            "23": "--> idk that this is that surprising, it was kind of natural given the result that removing visual features entirely doesn't hurt performance and helps generalization.",
            "24": "So maybe rephrase this sentence."
        }
    },
    "ryx6daEtwr": {
        "SJgLFl3CFB": {
            "0": "This paper introduces a method to detect cars from a single image.",
            "1": "The method imposes several handcrafted constraints specific to the dataset in order to achieve higher improvement and efficiency.",
            "2": "These constraints are quite strong and they not generalize to new situations (eg.",
            "3": "a car in the sky, a car upside down a car with multiple wheels).",
            "4": "The results do not seem particularly strong because the dataset seems easy and the improvements over previous works is small.",
            "5": "I would suggest to emphasise the improvements over previous works and send this paper to a specialized journal or venue in vehicle detection."
        },
        "HkgHJ8y1cB": {
            "0": "I find it very hard to review this paper.",
            "1": "The idea of using keypoints to carry pose estimation is is more than 15 years old, and for the car examples reported in this paper, I'm wondering why not just you SURF or SIFT - these would certainly have been reasonable baselines.",
            "2": "The convnets cited in this paper are mostly targeted at the harder problem of estimating human body poses.",
            "3": "The paper is very hard to read.",
            "4": "It is full of typos and far from ready for submission to ICLR.",
            "5": "The equations (eg eqn 1) are impossible to parse.",
            "6": "Based on all this I find it hard to trust the results."
        },
        "rkg6-V6B9B": {
            "0": "General:  The proposed method tries to improve vehicle identification and tracking by combining model-based and data-driven methods.",
            "1": "The idea is appealing, especially the use of domain knowledge combined with a data driven method to improve a model-based task.",
            "2": "The authors propose a set of model configurations on the waypoint detection, optimization techniques, a deep learning network topology and a data driven and domain knowledge based wheel detection mechanism.",
            "3": "Improvements on vehicle identification and tracking are not shown in the study, but different computer vision methods  are compared to each other.",
            "4": "The study has several shortcomings in consistently stating the problem and the result, completeness, reproducibility, quantification of results and the experimental methodology lacks  a systematic approach to isolate the effects of the different components of the proposed method.",
            "5": "It seems that the full method proposed yields better results compared to other way point based methods in a certain phase space.",
            "6": "However, from the material shown, especially the lack of the experimental description and its systematic shortcomings,  I cannot judge if the components of the method can contribute to improved vehicle detection and tracking.",
            "7": "The paper can be strengthened by the following:\n    Better framing of the problem\n    Improve the descriptions on the experiments carried out\n    Including a quantitative discussion on the results and the corresponding uncertainties\n    Systematic studies on the behavior of the proposed method components including a mathematical description in the probability space\n\n \n\nMore detailed comments:\n\nBetter framing of the problem:\n\n    The framing of the problem and the task lack  at least a qualitative discussion on the issues arising in using visual based vs Lidar/Radar.",
            "8": "I disagree with the authors, that a visual system can replace these other systems, but rather enhance results under certain conditions.",
            "9": "I am missing the discussion on the shortcomings of camera sensors with regards to the overall task: Day/Night, Fog, wheels are not visible from all angles and the impact on the target phase space.",
            "10": "The authors give an outline, that there is a study to come tackling the performance on vehicle identification, therefore a qualitative discussion could be enough at this point.",
            "11": "Improve the descriptions on the experiments carried out\n    So that someone else can reproduce it\n\nSystematic studies on the behavior of the proposed method components including a mathematical description in the probability space\n\n    The Experiment is not described at all and is therefore not reproducible.",
            "12": "The results shown are in general the overall Precision and Recall on some dataset.",
            "13": "The authors state at several occasions, that isolated measures on the method show better performance.",
            "14": "E.g.",
            "15": "the choice of the \"..online hard keypoints mining method..\", \"..the fixed range of softmax inputs..\", \"..ensure the accuracy io vehicle yaw angle…\".",
            "16": "However, I cannot find proof of these statements in the material shown.",
            "17": "Usually only the overall task performance is stated from which deductions on isolated effects cannot be drawn.",
            "18": "Albeit mentioning in the introduction priors in a Gaussian Process framework, I have not noticed a notion of a random variable, a probability density distribution or the impact of the experiment on the prior pdfs.",
            "19": "A systematic study on the behavior of those pdfs is missing and only a few example picture of bounding boxes are shown.",
            "20": "Including a quantitative discussion on the results and the corresponding uncertainties\n    Insert a quantitative  discussion on the experiments.",
            "21": "Insert estimates on uncertainties on the results or give at least a qualitative statement, if these are negligible.",
            "22": "Other: The paper is an application paper and does not offer novel advances for the ICLR community."
        }
    },
    "SyxS0T4tvS": {
        "ByeB-Xqatr": {
            "0": "This paper is a replication study of BERT for training large language models.",
            "1": "Its main modification is simple: training longer with more data.",
            "2": "Significantly improvements have been reported, and the work achieves on-par or higher accuracy over a large set of downstream tasks compared to XLNet, which is a state-of-the-art autoregressive language model.",
            "3": "Pros:\n+ The paper incorporates robust optimization into BERT training with more data, and shows that together it significantly improves BERT's performance on downstream tasks.",
            "4": "+ The experimental results show that RoBERTa can significantly advance the baseline BERT model and achieve on-par or new state-of-the-art accuracy on a large range of downstream tasks.",
            "5": "Cons:\n- While the replication study is well appreciated, the novelty contribution of the paper is marginally incremental as the model structure is largely unchanged from BERT.",
            "6": "The other techniques applied also are somewhat trivial.",
            "7": "- Very little can be deduced from the experiments, as performance is often improved by training over more data.",
            "8": "Overall, I believe this paper comes at the right time and is addressing an interesting problem.",
            "9": "The paper is well-organized and well-written.",
            "10": "The contribution of the paper comes mostly from carefully taking into account several additional design choices and show that they could help train BERT with more data and can achieve SoTA performance on downstream tasks.",
            "11": "Those modifications can be summarized as: (1) large-batch training with batch size 8k; (2) no mixed sequence length training only used 512 for the entire run; (3) no next sentence prediction; (4) dynamic masking instead of static; (5) larger byte-level BPE vocab (which increases BERT-large size by 20M parameters).",
            "12": "Although they are interesting, a major concern is that it is difficult to find one thing that would have catapulted it over others to ensure publication.",
            "13": "Question:\nDo you plan to release the datasets used for training in this work?"
        },
        "r1gzpOvAFH": {
            "0": "This paper presents a detailed replication study of the BERT pre-training model considering alternative design choices such as dynamic masking, removal of next sentence prediction loss, longer training time, larger batch sizes, additional training data, training on longer single document or cross-document sequences etc.",
            "1": "to demonstrate their efficacy on several benchmark datasets and tasks by achieving the new state-of-the-art results.",
            "2": "Overall, the paper is very well-written and the experimental setups are reasonable and thoroughly presented that would benefit the community for future research and exploration.",
            "3": "However, I am not sure if the paper presents a case of adequate novelty in terms of ideas as many of them are rather obvious and the current state-of-the-art models could also improve considerably using similar experimental setups, which authors also acknowledged in footnote 2.",
            "4": "Other comments:\n\n- Section 3.1: please clarify how exactly setting Beta2=0.98 would improve stability when training with large batch sizes.",
            "5": "- It's not clear what exactly was the motivation for proposing full-sentences and doc-sentences input formats.",
            "6": "Please explain.",
            "7": "- Although the presented metrics show that the removal of NSP loss helps, however, no explanation was provided based on qualitative evaluation as to why this is the case.",
            "8": "Some task-specific examples would have been nice to discuss the effects of NSP loss.",
            "9": "- Section 5: Please provide details on why you think the longest-trained model does not appear to overfit."
        },
        "Byx4-_ZZcS": {
            "0": "This paper presents a replication study of BERT pretraining and carefully measures the impact of many key hyperparameters and training data size.",
            "1": "It shows that BERT was significantly undertrained and propose an improved training recipe called RoBERTa.",
            "2": "The key ideas are: (i) training longer with bigger batches over more data, (ii) removing NSP, (iii) training over long sequences, and (iv) dynamically changing the masking pattern.",
            "3": "The proposed RoBERTa achieves/matches state-of-the-art performance on many standard NLU downstream tasks.",
            "4": "The in-depth experimental analysis of the BERT pretraining process in this paper answers many open questions (e.g., the usefulness of NSP objective) and also provide some guidance in how to effectively tweak the performance of pretrained model (e.g., large batch size).",
            "5": "It also further demonstrates that the BERT model, once fully tuned, could achieve SOTA/competitive performance compared to the recent new models (e.g., XLNet).",
            "6": "The main weakness of the paper is that it is mainly based on further tuning the existing BERT model and lacks novel contribution in model architecture.",
            "7": "However, the BERT analysis results provided in this paper should also be valuable to the community.",
            "8": "Questions & Comments:\n•\tIt is stated that the performance is sensitive to epsilon in AdamW.",
            "9": "This reminds us of the sensitivity of BERT pretraining to the optimizers.",
            "10": "Since one of the main contributions of this paper is the analysis of the BERT pretraining process, more experimental analysis on the optimizer should also be included.",
            "11": "•\tIt is stated that (page 7) the submission to GLUE leaderboard uses only single-task finetuning.",
            "12": "Is there any special reason for restraining it to single-task finetuning if earlier results demonstrates multi-task finetuning is better?",
            "13": "Of course, it is valuable to see the great performance achieved by single-task finetuning for RoBERTa.",
            "14": "But there should be no reason that it is restricted to be so.",
            "15": "An additional experimental results with multi-task finetuning should also be added."
        }
    },
    "rJxwDTVFDB": {
        "BkenstNatH": {
            "0": "This paper proposes a new understanding of dropout on top of variational dropout, which shows that training with dropout equals to maximizing an empirical variational lower bound on the log-likelihood.",
            "1": "This paper shows that the log posterior have the same lower bound when the inference model p(y|x) is defined by different methods, i.e., the arithmetic mean of predictions with different dropout masks, the geometric mean, and a power-mean family as an interpolation between these two cases.",
            "2": "This indicates that with the same training objective, different inference methods have different gaps to the posterior lower bound.",
            "3": "Intuitively, a smaller gap might lead to better performance.",
            "4": "The paper then uses an existing result from Liao & Berg (2017) to show that the gap can be bounded by the variance of prediction probability.",
            "5": "With empirical observations, the paper gives an unrigorous conclusion that the deterministic inference with dropout rate 0 achieves the smallest gap.",
            "6": "However, this does not hold theoretically due to the extra bias on expectation.",
            "7": "The optimality of deterministic inference does not hold empirically due to class imbalance or discrepancy between training and test sets.",
            "8": "The paper then proposes two practical solutions for better inference: 1) tuning dropout rate, softmax temperature, and the power mean parameter; and 2) deterministic inference with tuned softmax temperature.",
            "9": "By using the first inference solution, the performance on PTB and Wikitext2 LM can be improved by 2-3 on perplexity but is still slightly worse than the SOTA achieved by the mixture of softmaxes.",
            "10": "The idea of analyzing the gap to variational posterior lower bound for different dropout inference model is interesting.",
            "11": "The derivations are correct.",
            "12": "The organization is not perfect and readers might find it hard to follow here and there, but the main idea is understandable.",
            "13": "Experiments show that the suggested tuning of inference hyperparameters can bring improvements to LM tasks, which is convincing.",
            "14": "However, there are still major gaps between the theoretical analysis, the conclusion and the empirical solution (please see the detailed comments).",
            "15": "Such gaps make the main contribution questionable and make it as a pure empirical paper on its value.",
            "16": "Detailed comments:\n\n1) Reducing the variance of output prediction can reduce the gap on variational posterior, but how does the gap relate to the generalization error?",
            "17": "The current paper only indicates that a small gap gives more consistency between the true objective and the optimized objective defined on the training set: they can be still far away from the expected posterior over data distribution.",
            "18": "Hence, it is hard to directly relate \"reducing the gap\" and \"improve the test-set performance\".",
            "19": "2) As the author mentioned in Section 3.4, reducing the dropout rate causes a bias issue on the expectation.",
            "20": "So it is not clear whether deterministic inference with zero dropout rate can achieve the smallest gap or not.",
            "21": "In this way, the conclusion is only supported by the empirical observations but not the presented theoretical analysis.",
            "22": "3) One main contribution of this paper is the power-mean family of dropout.",
            "23": "However, only one member (alpha=0.5) from the family has been evaluated in the experiments, and it does not achieve the best performance in most experiments.",
            "24": "So this contribution seems not practically useful according to the empirical result.",
            "25": "4) It is not clear how the prediction variance is reduced gradually in order to generate the results in Figure 1(b).",
            "26": "I guess reducing dropout rate is not the correct way to do so since it causes the bias issue and the tightness will be influenced."
        },
        "rJlsryRpFB": {
            "0": "Authors consider dropout as MAP for conditional model and consider different types of averaging to obtain predictive distribution p(y|x, theta) during inference stage.",
            "1": "The paper proposes power mean family and shows that well-known types of MC averaging (arithmetic and geometric) are particular cases of proposed family.",
            "2": "Authors show that power mean family objective is lower-bounded by the original dropout lower-bound.",
            "3": "Therefore it is consistent to use original dropout on the training stage and do any kind of averaging from power mean family during inference stage.",
            "4": "Concerns:\n1) In general, paper is hard to follow and main motivation of the accomplished work is not clearly stated in the paper.",
            "5": "2) One of the most confusing things about this paper is the analysis of the lower bound tightness.",
            "6": "The authors state that the quality of fit for models in the power mean family depends on the Jensen gap ln(E[L]) - E[ln(L)] where L = p(y|x, w).",
            "7": "This gap reflects the difference between the training objective and the objective which is used at the evaluation stage.",
            "8": "From this perspective the expectation for the second term E[ln(L)] (which corresponds to the training objective and is the same in all settings) is fixed because we change the dropout scheme only in the inference.",
            "9": "Therefore, the Eq.",
            "10": "10 for the gap from this paper is misleading.",
            "11": "From this equality the authors derive that reducing the variance leads to decreasing the gap.",
            "12": "However, from the Bayes inference it is known that the zero gap between the training objective and the one at evaluation will be reached when we use expectation with respect to the true posterior distribution on the model weights given training data.",
            "13": "The authors however claim that the gap is zero when deterministic dropout (that is weight scaling rule) is used.",
            "14": "This would be true if we used the same deterministic objective during training stage (that would correspond to no dropout).",
            "15": "But we use expectation wrt non-degenerate noise distribution during training and at evaluation stage we take expectation wrt degenerate distribution (i.e.",
            "16": "apply deterministic mode).",
            "17": "Since the distributions are different we cannot conclude that the gap is zero.",
            "18": "Moreover it can be even negative.",
            "19": "Hence the statement about zero gap and justification of deterministic mode seems to be wrong.",
            "20": "3) Experiments results are also hard to follow.",
            "21": "In Tables 2, 3, 4 the differences in metric values are insignificant and there are no error bars.",
            "22": "From such empirical results it is difficult to draw any conclusions.",
            "23": "Overall, the motivation of the paper is not clear, the analysis for the lower bound tightness and following conclusions are misguided and experiments results are unconvincing.",
            "24": "Therefore, I would suggest rejecting the current version."
        },
        "SyluRQZf9r": {
            "0": "Summary:\nThe paper aims to develop a more principled framework for choosing between different inference procedures in neural network models employing dropout as a stochastic regularizer.",
            "1": "In particular, they posit a family of conditional models and show that the learning objectives of all these models are lower bounded by the usual dropout training objective with L2 regularization on weights.",
            "2": "They proceed to show empirically that the deterministic inference procedure (multiplying the node's output by the droput rate) achieves the tightest lower bound.",
            "3": "From this observation the authors conclude that deterministic inference should be seen as the best available approximation to the true dropout objective rather than an approximation to Monte Carlo averaging.",
            "4": "Strengths: The paper builds on recent works viewing dropout as a Bayesian approximation to the predictive posterior distribution.",
            "5": "Introducing a conditional model and showing that the dropout objective is akin to MAP estimation of the parameters of this model is interesting.",
            "6": "The result that dropout simultaneously optimizes a lower bound to an entire family of conditional distributions is novel.",
            "7": "Weaknesses:\nThe writing is often not clear, ambiguous or misleading and needs improvement.",
            "8": "For instance: \nIn Section 2.1, comments on weaknesses of variational dropout seems out of place.",
            "9": "It should either be ommitted or shifted to the previous section where variational dropout is introduced.",
            "10": "In Section 2.1, \"Consider a conditional model p(Y |X, Θ) as a crippled generative model with p(X) constant, X and Θ independent.\"",
            "11": "Assuming p(X), the input features, to be constant is a very strong assumption.",
            "12": "The variational lower bound is still true if p(X) is assumed arbitrary independent of Θ.",
            "13": "In Section 3.3, last paragraph, \"Suppose we pick a base model from the power mean family and have a continuum of subvariants with gradually reduced variance in their predictions but the same expectation.\"",
            "14": "It is not clear what the authors mean by continuum of subvariants?",
            "15": "Is it the dropout rate?",
            "16": "Several statements are made without any citations or explanations.",
            "17": "In Section 3.",
            "18": "\"While it is easy to argue in general that objectives of more than one model may share any given lower bound\" How?",
            "19": "More explanation needed.",
            "20": "In Section 3.1, \"Notice how with SGD and multiple epochs, for each data point several dropout masks are encountered, and the approximating quantity is the geometric mean of the predicted probabilities\".",
            "21": "Citation needed.",
            "22": "In Section 3.2, \"because M_α is monotonically increasing in α\".",
            "23": "Proof (in appendix) or citation needed.",
            "24": "In Section 5, \"The construction of a conditional model family with a common lower bound on their objectives is applicable to other latent variable models with similar structure and inference method.\"",
            "25": "What general structure and inference method are the authors referring to?",
            "26": "Technical Concerns:\nSection 3.3, last paragraph.",
            "27": "The entire paragraph is extremely convoluted.",
            "28": "It is not clear how one achieves deterministic dropout inference by reducing variance of the prediction y keeping its expectation constant.",
            "29": "Section 3.3, \"A similar argument based ... shows that Z monotonically increases... \" How?",
            "30": "The authors should deliberate more on this statement since, the Z term is also important in the difference between the true objective of each conditional model and the droput training objective.",
            "31": "In Section 5, \"The gains reported in those works might be explained by reducing the bias of deterministic evaluation and also by encouraging small variance in the predictions and thus getting tighter bounds.\"",
            "32": "Isn't this contradictory to Section 3.3, where from Eq.",
            "33": "10 and Eq.",
            "34": "11?",
            "35": "If the bias is reduced and variance increases, according to Eq.",
            "36": "10, the lower bound would become looser.",
            "37": "How is Fig.",
            "38": "1a and 1b computed?",
            "39": "Conclusions drawn from experiments not convincing.",
            "40": "Section 3.4, last line.",
            "41": "\"Having trained a model with dropout, the best ﬁt is achieved by the deterministic model with no dropout.",
            "42": "This result isolates the regularisation effects from the biases of the lower bound and the dropout family.\"",
            "43": "How does this isolate the regularization effects?",
            "44": "What biases of the lower bound?",
            "45": "Do you mean the difference between the model's true objective and the dropout training objective?",
            "46": "Table 4 indicates that not only AMC, also power with alpha=0.5 is better than deterministic in some cases.",
            "47": "Did the authors try other values of alphas?",
            "48": "Deterministic seems to be good just for MNIST.",
            "49": "This strongly refutes the most important claim of this paper, written in the abstract, \"Together, these results suggest that the predominant view of deterministic dropout as a good approximation to MC averaging is misleading.",
            "50": "Rather, deterministic dropout is the best available approximation to the true objective.\"",
            "51": "Changing the dropout multiplier and adjusting the softmax temperature of the network output layer, to achieve comparable performance to AMC on several datasets seems to support the existing hypothesis that deterministic dropout is a good approximation to MC average, and not the other way around!",
            "52": "Summary: \nThe paper introduces some interesting ideas about dropout but suffers from bad writing and presentation of results.",
            "53": "One of the most important claims made in this paper, \"dropout trains a deterministic model ﬁrst and foremost and a continuum of stochastic ones to various extents\", is not well-motivated theoretically in Section 3.3.",
            "54": "Consequently, this seems to be purely an empirical observation, which is contradicted by further experiments on linguistic datasets.",
            "55": "Several conclusions made from experiments seem adhoc."
        }
    },
    "rklraTNFwB": {
        "HyeF7bRoKH": {
            "0": "This work proposes applying natural language encoders pre-trained on a large text corpora (e.g.",
            "1": "BERT) to training agents to follow natural language instructions in a simulated environment.",
            "2": "Overall, I greatly enjoyed reading this paper: clear exposition of the idea, sensible model architecture, reasonable baselines and good experimental performance.",
            "3": "I only have minor questions & feedback, see below.",
            "4": "Pros\n- Well designed experiments with sensible baselines.",
            "5": "- Strong transfer learning results.",
            "6": "- Illuminating analysis where using BERT indeed performs better on capturing phrasal and sentence-level equivalence in natural language instructions.",
            "7": "Questions\n- In Table 5, BERT doesn't give performance improvement on natural instruction (over Word embedding+Transformers) until BERT+CMSA+TN.",
            "8": "Why do you think this is the case?",
            "9": "To phrase this in a different way, why do you think BERT+MP doesn't perform well on this?",
            "10": "- Are the results in Figure 2 computed from a BERT+MP model or a BERT+CMSA model?",
            "11": "- In the lifting results in Table 4, why doesn't BERT+CMSA+MP outperform BERT+MP?",
            "12": "------ \n\nUpdates:\n\nHaving read other reviewers' comments and also the authors' response, I would also like to call into question the difficulty of the experiments in the paper -- for the lifting task, the model is always presented with a command \"Lift X\", and essentially only needs to identify the correct object out of two at test time.",
            "13": "Also, for the putting task, the model only needs to disambiguate between 6 possible combinations (3 movable and 2 fixed objects).",
            "14": "Especially with a sophisticated model like BERT, I would have liked to see tasks where human instructions are more complex than this simple task.",
            "15": "Hence, I'm changing my score to 6."
        },
        "HJeZVasCFB": {
            "0": "The authors present a method of transferring template-based instruction following agents to natural language instructions by using language encoders trained on large text corpora.",
            "1": "They explore different ways of combining text-based language encoders with visual representations and compare them.",
            "2": "They find that contextual phrase-based representations learned by BERT significantly improve the performance on natural language instructions.",
            "3": "Strengths:\n- The paper is written well, it is easy to understand and follow.",
            "4": "- The task setup is good, authors collect natural language instruction data from humans.",
            "5": "- The paper presents several language encoding methods for the task and systematically evaluates them in a scientific manner.",
            "6": "- The experimental results indicate that it is possible to transfer an agent trained on template-based instructions to natural language instructions using language models trained on large text corpora.",
            "7": "It is not necessary to train the agent on natural language instructions.",
            "8": "I find this result important and useful.",
            "9": "Weaknesses:\n- The paper lacks significant technical novelty.",
            "10": "It essentially combines known reinforcement learning based instruction following agents with known language models.",
            "11": "The different ways of combining language encoding with visual representations are either trivial or adapted from prior work.",
            "12": "- A major concern is that the natural language instructions considered in the paper do not have much diversity with respect to language.",
            "13": "The paper only considers lifting and putting tasks and trains a separate model for both the tasks.",
            "14": "-- The lifting task always uses the verb 'lift' and replaces the object word with synonyms or referring expressions.",
            "15": "There are 80 objects in the lifting task and I suspect there are very few referring expressions for these objects and they mostly involve a synonym.",
            "16": "Furthermore, at test time, the agent only needs to distinguish between 2 objects.",
            "17": "The performance with random embedding is around 50% for this task and the best model is around 76% which means the agent is not recognizing the correct object around 50% of the time.",
            "18": "-- For the putting task, authors consider synonyms for object words and natural instructions which involve changing the verb ‘put’.",
            "19": "It seems like humans mostly use only 4 verb words for this task, ‘put’, ‘keep’, ‘move’, ‘place’.",
            "20": "This might be an artifact of the examples given to the human annotators.",
            "21": "In any case, this word is inconsequential as the agent always lifts one of 3 available objects on one of 2 fixed objects.",
            "22": "- It seems like the most diversity is coming from synonyms which can probably be handled with a dictionary or wordnet rather than requiring a language model.",
            "23": "There is also some prior work on handling synonyms (https://arxiv.org/pdf/1902.04546.pdf).",
            "24": "I would have liked to see many more tasks and a multi-task learning model which is also able to distinguish between the task based on natural language instructions in addition to understanding object word synonyms and referring expressions.",
            "25": "More objects would also help.",
            "26": "- The authors claim to tackle “more behavioural and environmental challenges than previous work”.",
            "27": "I do not agree with this claim.",
            "28": "It is true that this paper handles object interaction and natural language instructions in a partially observable setting, however, previous work has tackled other challenges which this work does not tackle.",
            "29": "For example, Oh et al.",
            "30": "2017 generalize to new sequence of instructions, Hermann et al.",
            "31": "2017 and Chaplot et al.",
            "32": "2018 also handle compositionality and generalize to unseen instructions referring to new objects, Hermann et al.",
            "33": "2017 handle negation, Chaplot et al.",
            "34": "2018 handle instructions involving ‘largest’ or ‘smallest’ objects, Misra et al.",
            "35": "2018 handle more diverse natural language and so on.",
            "36": "- I wouldn’t call moving objects using high-level symbolic actions as ‘manipulation’.",
            "37": "This is a whole research area in robotics involving taking low-level actions to move an object.",
            "38": "Also, the environment used in the paper is not visually realistic in my opinion.",
            "39": "It looks game-like and visual encoders trained in this environment are unlikely to generalize to the real world.",
            "40": "This is fine as it is mostly irrelevant to handling natural language instructions, but authors should not claim visual realism and object manipulation in my opinion.",
            "41": "Comments/Questions\n- I do not understand the meaning and purpose of some actions.",
            "42": "Why are there GRAB + actions?",
            "43": "Doesn’t the object move with the agent once it is grabbed?",
            "44": "What is SPIN_OBJECT?",
            "45": "Why is it needed?",
            "46": "It seems like there is no ‘place’ action, how does the agent place the object?",
            "47": "I am guessing when the agent stops taking GRAB+ actions.",
            "48": "If that is the case, then wouldn’t it be easier to just have Grab and Place actions rather than 16 GRAB+ actions?",
            "49": "- The meaning of ‘(sub)-’ in (sub)-word is not described.",
            "50": "- What is the probability of typo noise in the experiments?",
            "51": "- Many experimental details are missing.",
            "52": "How long was the model trained for both the tasks?",
            "53": "How many training samples/episodes?",
            "54": "What were the hyperparameters used for reinforcement learning?",
            "55": "Learning rate, optimizer, discount value and so on.",
            "56": "Updates after author response:\nI have examined the author response and additional experiments carefully.",
            "57": "I am maintaining my score due to the following reasons:\n- The authors seem to agree that the paper does not provide any 'substantial algorithmic advance'.",
            "58": "-  The authors argue that the number of objects in the paper is much more than prior work.",
            "59": "However, the focus of prior work (referenced by authors) was not to tackle natural language.",
            "60": "Since the focus of this paper is to tackle natural language, I believe the number and diversity of objects and tasks need to be much higher than 80 objects and 2 tasks used in the paper.",
            "61": "- \"We would language referring to an even wider range of motor-behaviours, e.g.",
            "62": "more 'verbs'\" -> I do not understand what the authors are trying to say here, but if \"learning the motor programmes for such concepts in an environment\" is not \"the focus of the present paper\", I believe the focus is only handling synonyms and referring expressions for objects.",
            "63": "In my opinion, these are relatively easier to tackle, (for example using a dictionary or wordnet) than grounding 'verbs' into sequence of actions, which limits the scope of this paper further.",
            "64": "- The authors still claim to tackle object manipulation and visual realism which I do not agree with.",
            "65": "I do not believe taking high-level \"GRAB\" action can be called object manipulation.",
            "66": "The objects are taken from shape net with relatively realistic shapes, but neither the appearance of objects (textures, shadows) nor the relative arrangement of objects is realistic.",
            "67": "In my opinion, a model trained in this environment has no hope of generalizing to the real-world.",
            "68": "- I do not agree with the authors' argument against chance performance in the lifting task.",
            "69": "From my experience, I believe an RL agent trained without any language input would perform at 50% if it receives a reward for lifting one of the two available objects.",
            "70": "I do not understand why the authors chose to put only 2 objects in the environment.",
            "71": "Why not put 5 or more objects?",
            "72": "- I appreciate authors' efforts towards multi-task learning results, however, tackling only 2 tasks is not convincing enough."
        },
        "r1epwfv15r": {
            "0": "This paper considers the task of instruction following where an agent navigates/interacts with a 3D environment conditioned on goals provided in natural language.",
            "1": "While several existing approaches use synthetic language for instructions, the authors tackle this problem under the setting of noisy instructions provided by humans in natural language.",
            "2": "For this, they use large-scale pre-trained representations (e.g.",
            "3": "BERT) as initial parameters for representing the textual instructions.",
            "4": "Their main result is the demonstration of transfer from agents trained using synthetic instructions to environments with more variation (e.g.",
            "5": "synonyms) or natural instructions provided by humans on two tasks involving object manipulation.",
            "6": "Pros:\n1.",
            "7": "Nice application of BERT to grounded instruction following tasks\n2.",
            "8": "Good empirical results\n\nCons:\n1.",
            "9": "Not much technical novelty\n2.",
            "10": "Empirical experiments could use a bit more rigor in terms of disentangling the major factors that contribute to performance (e.g typo noise)\n\n\n\nOther comments:\n1.",
            "11": "Are the BERT weights frozen or finetuned along with the rest of the model?",
            "12": "Does the performance depend on this?",
            "13": "2.",
            "14": "The typo noise (TN) seems to be a key driver of performance.",
            "15": "Have you tried adding it to the other baselines like wordPiece Transformer?",
            "16": "3.",
            "17": "What are the scores when training on the test tasks (D.O synonym, natural instructions, etc.)",
            "18": "directly?",
            "19": "It would be good to establish how well the transfer setup is doing compared to the best RL agent trained directly on the test scenarios.",
            "20": "—————\nPost rebuttal update:\nThanks to the authors for their response and for updating the paper!",
            "21": "I especially appreciate the additional experiments, but I’m still confused why the authors do not perform a clear ablation study to support their claims.",
            "22": "It seems like the main claimed novelty of the paper is the proposed CMSA method.",
            "23": "However, the empirical results are not convincing/rigorous enough to provide the reader information on 1) whether CMSA is a useful method (since it is used only with BERT and does not seem to affect results on its own compared to MP, SA, TN, etc.)",
            "24": "and 2) when should one use/not use BERT and CMSA (BERT+CMSA actually does quite poorly acc.",
            "25": "to table 5).",
            "26": "Further, the other reviewers also pointed out concerns regarding the difficulty of the task and complexity of language used.",
            "27": "Hence, I feel the paper still requires some revision to form a coherent story — updating my score accordingly."
        }
    },
    "BylPSkHKvB": {
        "rkxqciuCtH": {
            "0": "This paper considers the challenging problem of learning to generate programs from natural language descriptions: the inputs are sequences of words describing a task and the outputs are corresponding programs solving the task.",
            "1": "The proposed approach elegantly relies on tensor product representations.",
            "2": "Inference with the proposed model is done in 3 steps: (1) encode the symbolic information present in the text data as a TPR, (ii) maps the input TPR to an output TPR encoding the symbolic relations of the output programs (here the authors use a simple MLP), and (iii) decode the output TPR into an actual program.",
            "3": "The parameters of the models used in the 3 steps are learned jointly.",
            "4": "For step (iii), the authors proposes a novel way of encoding an n-ary relation into a TPR which facilitates the recovery of the relation's arguments using unbinding operations: this is a neat trick (though I think it increases the number of parameters and may limit the expressiveness of the TPR, since reaching \"full-rank\" of the TPR will occur faster than with the encoding used in [Smolensky et al., 2016]).",
            "5": "Experiments on two datasets demonstrate the validity of the approach.",
            "6": "The paper is very well written and easy to follow.",
            "7": "The idea seems original and well executed but I think the experimental section could be improved.",
            "8": "In particular, adding/reporting stronger baselines to the comparison would straighten the paper.",
            "9": "I also feel some relevant literature may be missing from the related work.",
            "10": "Nonetheless, I think it is a good paper which will be relevant to the community, I thus recommend acceptance.",
            "11": "* Comments / Questions *\n\n- Section 3.1.1: if I understand correctly, the length of the sequence affects the rank of the TPR.",
            "12": "Could that be a problem in practice?",
            "13": "E.g., the capacity of the TPR could likely be saturated quickly for long sequences?",
            "14": "- Section 3.2.1: the filler vector f_t = Fu is computed as a convex combination of the learned filler vectors.",
            "15": "Is it a design choice to choose a convex combination rather than taking the column corresponding to the argmax of the vector u?",
            "16": "Or is it because otherwise the model can not be trained using the classical backprop approach?",
            "17": "- The results of the Seq2Tree+Search model from (Bednarek et al.",
            "18": "(2019)) is not reported in Table 2.",
            "19": "Why?",
            "20": "I believe it should be included (it is ok that it outperforms the proposed method.",
            "21": "In addition you can maybe identify clear advantages of your method illustrating a trade-off, e.g., running time, end-to-end, scalability ...).",
            "22": "- A more thorough ablation study could also improve the strength of the experiments.",
            "23": "For example, do you know to which extent the attention model in the decoder is necessary to achieve good performances?",
            "24": "- I am not very familiar with the literature but it seems some relevant work may be missing from the review.",
            "25": "In particular, I believe there are many papers tackling the problem of learning programs from input output examples or execution traces, e.g.",
            "26": "\"DeepCoder: Learning to Write Programs\", \"Neural Turing machines\",  \"Inferring algorithmic patterns with stack-augmented recurrent nets\", \"Inferring and Executing Programs for Visual Reasoning\", \"Learning to infer graphics programs from hand-drawn images\"...",
            "27": "This list is by no means meant to be exhaustive in any way, just to illustrate a large body of work that seems relevant to the present paper (even though I understand that those papers do not consider natural language description as inputs).",
            "28": "* Typos *\n\n- Eq.",
            "29": "(5) Should be r' instead of r_i' (?)"
        },
        "SygoAfzI9r": {
            "0": "This paper proposes a sequence-to-sequence model for mapping word sequences to relation-argument-tuple sequences.",
            "1": "The intermediate representation (output of the encoder) is a fixed-dimensional vector.",
            "2": "Both encoder and decoder internally use a tensor product representation.",
            "3": "The experimental results suggest that the tensor product representation is helpful for both the encoder and the decoder.",
            "4": "The paper is interesting and the experimental results are positive, but in my opinion the exposition could use some substantial work.",
            "5": "Fixing the most substantial flaws in the exposition would be sufficient to warrant an accept in my view.",
            "6": "Major comments:\n\nI found the mix of levels of detail in the model specification in section 3 confusing.",
            "7": "It would be extremely helpful to have a straightforward high-level mathematical description of the key parts of the encoder, mapping (which could be considered part of the encoder), and decoder in standard matrix-vector notation.",
            "8": "While equations (7), (8), (9), (10), (11) and appendix A.2 go some way toward this, key high-level details seem to be missing, and I feel like the exposition would benefit from simply stating the matrix-vector operations that are performed in addition to describing their interpretation in terms of the semantics of the tensor product representation.",
            "9": "Specific examples are noted below.",
            "10": "It would be helpful to be explicit about the very highest-level structure of the proposed model.",
            "11": "If I understand correctly, it is a probabilistic sequence-to-sequence model mapping a word sequence to a probability distribution over relation-argument-tuple sequences.",
            "12": "It uses an encoder-decoder architecture with a fixed-dimensional intermediate representation, and an autoregressive decoder using attention.",
            "13": "Both the encoder and decoder are based on the tensor product representation described in section 2.",
            "14": "Stating these simple facts explicitly would be extremely helpful.",
            "15": "Especially for the encoder, the learned representation is so general that there seems to be no guarantee that the learned roles and fillers are in any way related to the syntactical / semantic structure that motivates it in section 2.",
            "16": "There doesn't seem to be any experimental investigation of the learned TPR in the encoder.",
            "17": "If I understand correctly, the way encoder roles and fillers are computed and used is symmetric, meaning that the roles and fillers could be swapped while leaving the overall mapping from word sequences to relation-argument-tuple sequences unchanged.",
            "18": "This suggests it is not possible to interpret the role and filler vectors in the encoder in an intuitive way.",
            "19": "Minor comments:\n\nIn section 2, \"R is invertible\" should strictly be \"R has a left inverse\".",
            "20": "In section 3.1.1, the claim that \"we can hypothesize to approximately encode the grammatical role of the token and its lexical semantics\" is pretty tenuous, especially given the apparent symmetry between learned roles and fillers in the encoder and given the lack of experimental investigation of the meaning of the learned encoder roles and fillers.",
            "21": "In section 3.1.2, my understanding is that the relation-argument tuple (R, A_1, A_2, A_3), say, is treated as a sequence of 3-tuples: (A_1, R, 1), (A_2, R, 2), (A_3, R, 3).",
            "22": "Each of these 3-tuples is then embedded using learned embeddings (separate embeddings for argument, relation and position).",
            "23": "If correct, it would be helpful to state this explicitly.",
            "24": "In section 3.1.2, it is stated that contracting a rank-3 tensor with a vector is equivalent to matrix-vector product, which is not the case.",
            "25": "In section 3.1.3, both high-level and low-level details of the MLP module are omitted.",
            "26": "High-level, I presume that the matrix output by the encoder is reshaped to a large vector, the MLP is applied to this vector to produce another vector, then this is reshaped to a rank-3 tensor to input to the decoder.",
            "27": "It would be helpful to state this.",
            "28": "Low-level, the number of layers, depth and activation function of the MLP should be specified somewhere, at least in the appendix.",
            "29": "Did the authors consider using a bidirectional LSTM for the encoder?",
            "30": "This might improve performance.",
            "31": "In section 3.1.2 and appendix A.2, why use the LSTM hidden state for subsequent processing rather than the LSTM output (which would be more conventional).",
            "32": "The LSTM output is defined in appendix A.2 but appears not to be used for anything.",
            "33": "Please clarify in the paper.",
            "34": "Did the authors consider passing the output of the reasoning MLP into every step of the tuple LSTM instead of just using it to initialize the hidden state?",
            "35": "It would be helpful to state the rank of the tensors H, B, etc in section 3.2.2.",
            "36": "In section 3.2.2, what does \"are predicted by classifiers over the vectors...\" mean?",
            "37": "This seems quite imprecise.",
            "38": "What is the form of the classifier?",
            "39": "My best guess is that the vector a_i^t is passed through a small MLP with a final softmax layer which outputs a probability distribution over the 1-of-K representation of the argument.",
            "40": "The main text says \"more details are given in the appendix\", but appendix A.2 just has \"Classifier(a_1^t)\".",
            "41": "Please clarify in the paper.",
            "42": "What is the attention over in equation (9)?",
            "43": "Attention needs at least two arguments, the query and the sequence being attended to.",
            "44": "It seems that (9) only specifies one of these.",
            "45": "It would also be helpful to be explicit about the form of attention used.",
            "46": "What is f_linear in (11)?",
            "47": "It seems unnecessarily confusing to switch notation for the arguments from A_1 in section 3.1.2 to a r g_1 in section 3.2.2, and similarly for the relations.",
            "48": "For the decoder tuple LSTM, how exactly is the previous relation-argument tuple (R, A_1, A_2, A_3), say, summarized?",
            "49": "Are each of R, A_1, A_2 mapped to a vector, these vectors concatenated, then passed into the LSTM?",
            "50": "Or is the positional decomposition into (A_1, R, 1), ... used?",
            "51": "Please clarify in the paper.",
            "52": "Based on section 3.3, it seems that the model assumes that, in the decomposition of (R, A_1, A_2, A_3) into a sequence (A_1, R, 1), (A_2, R, 2), (A_3, R, 3) of 3-tuples at each decoder output step, the three 3-tuples are conditionally independent of each other and the three entries of each 3-tuple are conditionally independent of each other.",
            "53": "Is this indeed assumed?",
            "54": "If so, it would be helpful to state this explicitly.",
            "55": "It seems like this is likely not true in practice.",
            "56": "Section 3.3 refers to \"predicted tokens\".",
            "57": "Where are these predicted tokens in (9), (10) or (11)?",
            "58": "In section 3.3, it seems the loss at each decoder step is the log probability of the relation-argument tuple at that step.",
            "59": "Thus, by the autoregressive property, the overall loss is the log probability of the sequence of relation-argument tuples.",
            "60": "If so, it would be helpful to state both these facts explicitly.",
            "61": "Section 3 seems to be missing a section, which is how decoding is performed at inference time.",
            "62": "For the output of the decoder at each step, is random sampling used, if so with a temperature, or is greedy decoding (selecting the most likely class, equivalent to a temperature of 0) used?",
            "63": "Also, what is done if decoding outputs different R's for (A_1, R, 1), (A_2, R, 2), (A_3, R, 3)?",
            "64": "The three R values here should be equal in order for this to represent a relation-argument tuple (R, A_1, A_2, A_3), but there is no guarantee the model will respect this constraint.",
            "65": "Unless I missed it (apologies if so), many experimental architecture details were omitted.",
            "66": "For example, how many hidden cells were used for the LSTMs, etc, etc?",
            "67": "These should at least be stated in the appendix.",
            "68": "It would be interesting to investigate how long input / output sequences need to be before the fixed-dimensional internal representation breaks down.",
            "69": "In section 4.1.1, it was not clear to me what \"noisy examples\" means.",
            "70": "Does this mean that the dataset itself is flawed, meaning that the reference sequence of operations does not yield the reference answer?",
            "71": "Please clarify in the paper.",
            "72": "In table 1, please state the total size of the fixed-dimensional intermediate representation for all systems.",
            "73": "This seems crucial to ensure the systems can be meaningfully compared.",
            "74": "In figure 4, left figure, the semantic classes don't apper to be very convincingly clustered.",
            "75": "(And it seems like K-means clustering could easily have selected a different clustering given a different random initialization.)",
            "76": "In appendix A.2, mathematical symbols are essentially meaningless without describing what they mean in words.",
            "77": "Please explain the meaning of all the symbols that are not defined in terms of other symbols, e.g.",
            "78": "w^t, T_{t-1}, ..., f_s m (is this softmax???",
            "79": "), f_l i n e a r (what does this mean?",
            "80": "), C o n t e x t, C l a s s i f i e r, etc, etc.",
            "81": "C o n t e x t in particular doesn't even have a hint of a definition.",
            "82": "In (19) and (27), why would a temperature parameter be helpful?",
            "83": "This can be absorbed as an overall multiplicative factor in the weight matrix of the previous linear layer.",
            "84": "Is this temperature parameter learned during training (I presume so)?",
            "85": "Please clarify in the paper.",
            "86": "Usually * is used for convolution, not simple multiplication (e.g.",
            "87": "equation (17)).",
            "88": "Throughout the main body and appendix, there are lots of instances of poor spacing.",
            "89": "For example, $f_{linear}$ should be written as something like $f_\\text{linear}$ in latex to avoid it coming out as l i n e a r (which literally interpreted means l times i times n times e, etc).",
            "90": "Please fix throughout."
        },
        "SyecPABa9S": {
            "0": "The authors propose a binding-unbinding mechanism for translating natural language to formal language.",
            "1": "The idea is good and novel.",
            "2": "As far as I know, this is indeed the first work for handling this task using binding-unbinding mechanism.",
            "3": "The experimental results also look promising in compared with the exsiting models.",
            "4": "However, the designed specific neural network does not support the claimed binding-unbinding theory very well.",
            "5": "Moerover, there seem to be some errors about the correctness of the theory (See the first point below).",
            "6": "Firstly, in the last paragraph of Section 2, the authors claim that the role matrix $R$ would be invertible such that there exists a matrix $U = R^{-1}$ such that the fillers would be recovered.",
            "7": "However, $R$ is defined as a non-square matrix in the previous paragraph.",
            "8": "How can a non-square matrix be invertible?",
            "9": "Secondly, the design of the specific neural network cannot describe the theory behind proposed binding-unbinding mechanism properly.",
            "10": "The authors try to interpret the design of the neural networks using the concepts in the proposed binding-unbinding theorybut are not convincible.",
            "11": "In Section 2, the basics of binding-unbinding are introduced and many mathematical properties are required to make the binding-unbinding work.",
            "12": "However, all the parameters/variables in the neural networks are freely designated and are not correlated to each other, thus they cannot work together to meet the requirements in the binding-unbinding mechanism.",
            "13": "According to my understanding, at least there should be some direct connections between the parameters in the encoder and decoder.",
            "14": "For example, is there any restriction on the parameters in encoder and decoder respectively to reflect the property $UR=I$ as in Section 2.",
            "15": "Lastly, in the encoder part, the role and filler are learned in an unsupervised without any evidence.",
            "16": "The input for the decoder is an \"assumed\" TPR, thus the only evidence from the objective function are cut-off by the assumed TPR.",
            "17": "Given that there are no other connections between encoder and decoder, the design of the encoder cannot learn role and filler properly.",
            "18": "Other suggestions:\nThe natural language to formal language problem is named semantic parsing in natural language processing field.",
            "19": "In semantic parsing problem, langugae to programatic language is a typical task.",
            "20": "I would recommend include some references in semantic parsing."
        }
    },
    "Bkle6T4YvB": {
        "SyxkmrNAFH": {
            "0": "This paper proposes a method to adapt a pretrained BERT model from English to another languages with a limited time/GPU budget.",
            "1": "Evaluation on 6 target languages shows good performance for natural language inference and dependency parsing.",
            "2": "Concretely, the proposed approach consists of, starting from a pretrained English language model, first training language-specific embeddings and then fine-tuning the entire pretrained model on English *and* the target language, using those embeddings.",
            "3": "The language-specific embeddings are initialized based on the English embeddings (the authors propose two different ways for doing that).",
            "4": "I like about the paper that the approach is simple and fast.",
            "5": "The experiments seem reasonable, too.",
            "6": "The only minor negative point is that the approach is not particularly exciting."
        },
        "r1lhNKE0Yr": {
            "0": "This paper presents a method to efficiently transfer pre-trained english language model to bilingual language model.",
            "1": "The obtained representations are evaluated on downstream NLP task (natural language inference and dependency parsing) with state-of-the-art performances.",
            "2": "Pros:\n\n- Experiments clearly show that, using the proposed method, stronger pre-trained English embedding leads to stronger bilingual language model and thus to better performances for downstream foreign tasks.",
            "3": "Cons: \n\nWhile it is generally  intelligible, some structural modifications could be done to  improved the clarity of the paper.",
            "4": "For instance, the method used to align foreign word vectors with English word vectors, when no aligned corpus is available, should appear sooner.",
            "5": "It is described in 3.1 but should probably appear in 2.1 subsection Learning from Monolingual Corpus.",
            "6": "Minor issues:\n\n- in section 3: RoBERA -> RoBERTa\n- in section 5.1: the third sentence is syntactically incorrect\n- in Conclusion: our approach produces better than -> our approach performs better than"
        },
        "rylWllu1qB": {
            "0": "In this work, the authors propose a way to transfer a pre-trained English BERT model to a new language within a short amount of time.",
            "1": "The key insight is to map English embeddings to the foreign language and have separate embeddings for both English and the foreign language.",
            "2": "The resulting bilingual LM is evaluated for zero-shot transfer learning on two tasks: XNLI and dependency parsing.",
            "3": "Pros:\n- The authors provide good details into their hyperparameter settings and about how the obtain the foreign language word embeddings.",
            "4": "- By leveraging existing pre-trained models, they’re able to do pre-training for their bilingual LM within 2 days.",
            "5": "Cons:\nI find that a key comparison point in this paper is missing, which is Bilingual BERT trained on just the two languages that are being considered for their RAMEN system.",
            "6": "This is not a fair comparison while mBERT which is trained on 100+ languages is not.",
            "7": "All comparisons are not fair since a simple baseline of just training mBERT on two languages with monolingual data and with a shared WPM is not evaluated here.",
            "8": "The proposed system has an unfair advantage over mBERT since it’s initialized from BERT/RoBERTA and fine-tuned only on two languages.",
            "9": "Hence most of the parameters are used for just the two languages while mBERT uses the parameters for 104 languages.",
            "10": "Given this unfair comparison, I’m not sure if we can draw a meaningful conclusion from all the experiments.",
            "11": "Rating justification:\nGiven the lack a fair comparison between the bilingual and multilingual BERT models, I don't think the conclusions are insightful."
        }
    },
    "rkx1b64Fvr": {
        "HJl-jdkaKB": {
            "0": "Summary\n\nThis paper introduces a new model architecture for doing text classification.",
            "1": "The main contribution is proposing a deeper CNN approach, using both word and character embeddings (as well as label embeddings with attention).",
            "2": "The paper claims improved performance over baselines.",
            "3": "Decision\n\nI reject the paper for 3 main reasons:\n1) Very misleading claims regarding establishing a new state of the art.",
            "4": "The baselines used for comparison don't include any of the best existing published results.",
            "5": "2) Lack of positioning within the literature.",
            "6": "In particular, no mention nor discussion of Transformers (self-attention) networks, including BERT and XLNet approaches, which are the state of the art in text classification.",
            "7": "3) Lack of justification/explanation for the proposed architecture.",
            "8": "One key argument made is that current models are shallow, but it appears that only CNN models are considered for that comparison.",
            "9": "More discussion is needed to understand why the new aspects of the proposed network are importantly different from other existing approaches.",
            "10": "Additional details for decision\n\nThe results from this paper are significantly inferior to the best results published.",
            "11": "With a few quick searches, I found that there are several approaches performing better than the proposed model on every dataset considered in the analysis, as you can see below.",
            "12": "http://nlpprogress.com/english/text_classification.html\nhttps://github.com/sebastianruder/NLP-progress/blob/master/english/sentiment_analysis.md\nhttps://paperswithcode.com/sota/text-classification-on-yahoo-answers\n\nExtra notes (not factoring in decision)\n\n- Consider spacing out the 3 rightmost blocks in Figure 1, I found the layout confusing and there's space available.",
            "13": "- In section 3, I would have liked more explanation for the motivation of the various design choices."
        },
        "BJg7Gw_0KB": {
            "0": "This paper presents a multi-input model for text classification.",
            "1": "The presentation of this paper is very poor.",
            "2": "The novelty is very limited.",
            "3": "The justification of the proposed architecture is not persuasive.",
            "4": "The experiment design has many flaws.",
            "5": "All the compared baselines are extremely weak.",
            "6": "The authors need to follow the experiment setup in more recent text classification papers.",
            "7": "Authors need at least to compare their methods to BERT.",
            "8": "The current version of this paper is definitely not an ICLR publication."
        },
        "SygNcwUaqH": {
            "0": "This paper described a multi-input model for text classification.",
            "1": "This paper is a bit hard to read compared to other submissions.",
            "2": "I highly recommend authors to have native speaker to proofread this before submission.",
            "3": "The paper mentioned \"labels\" as input to embedding layer as well as attention mechanism.",
            "4": "However, it will be more helpful if authors can provide examples of labels.",
            "5": "Moreover, the experiment results (e.g., Table 3 and 5) did not include label.",
            "6": "I wonder if the methods used as baselines are still the state-of-the-art as language model pre-training might have better results.",
            "7": "It will be more convincing if authors include additional baselines such as fine-tuning BERT.",
            "8": "Minor issue: the references in Table 3 (in brackets) are hard to find.",
            "9": "Based on above reason, I would reject this paper."
        }
    },
    "SklGryBtwr": {
        "HyeAa6v9Fr": {
            "0": "This paper studies systematic generalization in a situated agent.",
            "1": "The authors examine the degree to which various factors influence systematic generalization, including 2D vs. 3D environments, egocentric vision,  active perception, and language.",
            "2": "The experiments reveal that the first three factors, but not language, promote systematic generalization.",
            "3": "The experiments are well-done and worthwhile, and identifying the key factors that affect generalization is a strength of the paper.",
            "4": "I have two main criticisms.",
            "5": "First, the model's abilities for systematic generalization are overstated.",
            "6": "Second, critical details about the experiments are omitted that make them difficult to evaluate.",
            "7": "Let's start with the abilities of the model.",
            "8": "The title of the paper is \"Emergent systematic generalization in a situated agent,\" which of course implies that the agent has \"systematic generalization.\"",
            "9": "The authors go on to say, in the abstract, that \"we demonstrate strong emergent systematic generalisation in a neural network agent\".",
            "10": "The results, however, fall short of these statements.",
            "11": "The strongest results pertain to generalizing a highly-practiced action such as \"lifting\" or \"putting\" to novel objects.",
            "12": "In this case, highly-practiced means that the actions have been trained on 31 unique objects for millions of steps.",
            "13": "However the paper does not study whether or not the agent can learn a novel action (e.g.",
            "14": "\"lifting\" or \"putting\" with only a few examples) and generalize it systematically to familiar objects.",
            "15": "Nor does it study whether novel actions can be combined systematically in new ways using relations and modifiers such as \"finding the toothbrush ABOVE the hat\" or \"finding AND putting\" or \"putting to the right of.\"",
            "16": "Benchmarks for systematic generalization such as SQOOP and SCAN include these types of generalizations, and an agent with systematic generalization should handle them as well.",
            "17": "To be clear, I don't think it's necessary to add additional experiments to the paper, but the current results should not be overstated in their generality.",
            "18": "Even within the reported experiments, the results suggest that systematicity is lacking in several places.",
            "19": "In the negation task, where chance is 50% accuracy, the agent achieves only 60% correct after learning from 40 unique words and 78% performance with 100 unique words (doesn't systematic generalization imply 100%?)",
            "20": "For the putting tasks, the agent achieves 90% correct in one experiment (section 3.2) and then only achieves 63% correct in another (section 4.2).",
            "21": "Again, the generalization abilities seem far from systematic.",
            "22": "Critical details about the action space and the simulation parameters are needed.",
            "23": "The action-space has 26 actions, but the paper does not say what these actions are.",
            "24": "These details are crucial to understanding what is required to generalize \"lift\" or \"put\" to new objects -- instead the paper only says that \"in particular the process required to lift an object does not depend on the shape of that object (only its extent, to a degree)\" and that \"shape is somewhat more important for placement\" compared to lifting.",
            "25": "I would consider updating my evaluation if the authors make revisions to ensure that the evidence supports their conclusions.",
            "26": "The paper's title should also be supported by the findings; to offer a suggestion, something like \"Richer environments promote systematic generalization in situated agents\".",
            "27": "Other suggestions\n- The axis on Fig.",
            "28": "2 is too small to read.",
            "29": "Also, it should be mentioned in text that the network is trained for 100 million+ steps (also, what is a step?",
            "30": "how many episodes was it trained for?)",
            "31": "- The number of objects in sets X_1 and X_2 is important and should be mentioned in the main text.",
            "32": "------\n\n** Update to review **\n\nThanks for your response to my review.",
            "33": "It's clear that the authors have made considerable effort to improve the paper.",
            "34": "In particular, the revised title, abstract, and introduction now more accurately reflect the contributions of the paper.",
            "35": "It's not perfect, but the paper is improved and I revised my score accordingly.",
            "36": "While it did not affect my final score, not all my suggestions were incorporated and I hope the authors will make further improvements in their revisions.",
            "37": "The number of objects in sets X_1 and X_2  (Sections 3.1 and 3.2) are not mentioned and are tucked away in the appendix' this should be in the main text.",
            "38": "Thanks for providing the list of 26 actions, but it's still not completely clear what makes a successful \"lift\" or \"put\" in terms of the sequence of actions.",
            "39": "Finally, rather than simply saying that your agent \"in no way exhibits complete systematicity\" (Discussion), I hope the authors will expand on this and discuss the limitations of their experiments, and the kinds of systematicity not addressed and which could be the focus of future work."
        },
        "Byeq7rzTYr": {
            "0": "\n=============================== Update after revisions =====================================================\n\nIn my initial review, I had raised some issues with the interpretation of the results and suggested some control experiments to tighten the conclusions.",
            "1": "The authors chose to weaken their initial claims by rephrasing their conclusions instead.",
            "2": "I understand that there may not have been enough time to run many of the experiments I suggested, but I still think they are worth considering for the future.",
            "3": "I'm mostly satisfied with the rephrasing of the conclusions in the revised paper, so as promised, I'm happy to increase my score and recommend acceptance.",
            "4": "I spotted several typos in the revised paper, however: section 4.1: \"we choose to consider negation ...\", p. 5: \"for for ...\", a citation on p. 5 is not compiled correctly.",
            "5": "There may be more.",
            "6": "For the final version please make sure to go through the paper thoroughly a couple of times and fix all the typos.",
            "7": "========================================================================================================\n\nThe authors present a systematic study of generalization in agents embedded in a simulated 3d environment.",
            "8": "I think there are some interesting results in this paper that might be useful for people to know about.",
            "9": "I appreciate the thoroughness of the experiments, in particular.",
            "10": "I have, however, some issues with the interpretation of several of the main results.",
            "11": "I would be happy to increase my score if we can resolve some of these issues.",
            "12": "Here are my main concerns:\n\n1) In the experiments in section 3, only a limited test set is used.",
            "13": "How is the train/test split decided in these experiments?",
            "14": "Table 6 suggests that you have a much larger repository of objects.",
            "15": "Why not use all possible objects in the test set?",
            "16": "It is a bit premature to declare your results as systematic generalization if you can’t show that it actually works for a much larger set of test objects (ideally for all possible objects).",
            "17": "2) Section 4.1: in these experiments, the training set size is increased, but the test set size is kept constant (and small), so the train/test size ratio also increases.",
            "18": "So, an alternative explanation of the results in this section is that the model behaves largely according to visual similarity and as the training set size is increased, it becomes easier to find a training set object that is visually similar to any test set object.",
            "19": "I think the authors should run an experiment where both training and test set sizes increase by the same amount so that the train/test set size ratio stays constant.",
            "20": "If the model can’t achieve systematic generalization in that case, it would be wrong to conclude, as the authors do now, that increasing the training set size itself improves systematic generalization.",
            "21": "The correct conclusion would rather be that increasing the train/test size ratio improves generalization, which is a weaker conclusion.",
            "22": "Please note that the results in this section are quite similar to those in Lake & Baroni (2018) and in Bahdanau et al.",
            "23": "(2018) (see their Figure 3).",
            "24": "Bahdanau et al.",
            "25": "(2018), for example, also show that increasing train/test set size ratio (their “rhs/lhs” ratio) improves generalization in generic neural networks.",
            "26": "It is interesting to note, however, that neither Lake & Baroni (2018) nor Bahdanau et al.",
            "27": "(2018) interpret these results positively (i.e., these results don’t show systematicity), whereas the current paper seems to put a more positive spin on essentially the same result.",
            "28": "I think these earlier results should be explicitly discussed here and the authors should justify why they are interpreting the results differently (if they are).",
            "29": "It should also be noted that in the real world the train/test size ratio for humans is presumably very small, perhaps zero (given the compositional abilities of humans).",
            "30": "3) Section 4.3: I don’t think the results in this section are sufficient to establish the egocentric frame per se as the key factor.",
            "31": "One possibility is that perhaps the frame doesn’t have to be centered on the agent, but as long as it has some systematic relationship to the agent’s location (for example, the center of the visibility window could be some distance away from the agent, and the agent itself may or may not be inside this window), that’s good enough to get generalization improvements.",
            "32": "An even weaker possibility is that simply a moving frame is enough for improved generalization.",
            "33": "In this case, the reference frame doesn’t even need to have a systematic relationship to the agent’s location.",
            "34": "For example, the frame could be relative to a fictitious agent that randomly explores the environment.",
            "35": "I think the authors should run some experiments to rule out these possibilities if they want to claim that the egocentric frame itself is responsible for generalization improvements.",
            "36": "4) Section 4.4: In the experiments in this section, I think there are two relevant factors that need to be better disentangled: 1) the number and variability of image frames experienced by the two models; 2) the active perception aspect (the fact that the agent interacts with the environment and affects its own perceptual experience in one case).",
            "37": "The authors claim the second factor as the key aspect enabling better generalization, but 1) is equally likely (this would be more in line with a standard data augmentation type result).",
            "38": "A good control experiment here would be to not just use the first frame but a larger number of more variable frames for training the non-situated agent (for example, one can use image frames that would be seen by a camera that more or less randomly moves in front of the objects perhaps with the constraint that both objects are always at least partially visible).",
            "39": "If the classification model generalizes as well as the situated agent in this control condition, you cannot claim active perception as the key factor.",
            "40": "5) As a more general point, it’s a bit frustrating to have to judge systematic generalization by only looking at the results of some limited set of experiments.",
            "41": "How do I interpret the results if the agent achieves only 84% accuracy in some experiment (as opposed to 100%)?",
            "42": "It would be much better if the authors could somehow more rigorously prove systematicity.",
            "43": "Here, I don’t necessarily mean “prove” in a mathematical sense, but just analyzing the learned representations a bit more rigorously and being able to say something along the lines of: here’s exactly how the trained model represents “lift”; because of reason X, Y, Z, this representation is completely disentangled from all object representations in the dataset (and ideally from all possible object representations, because that’s really what true systematicity entails, although I highly doubt that any generic model of the type studied in this paper will be able to achieve this, regardless of the amount and type of input it receives).",
            "44": "More minor issues:\n\n6) In Table 5, “table lamp” appears both in training and test sets.",
            "45": "Is this a typo?",
            "46": "7) Some results are presented in the appendix without any mention in the main text (Appendix D. 2).",
            "47": "I think this is not a good practice in general.",
            "48": "In the main text, please make sure to mention, however briefly, every result that appears in the appendix (something along the lines of \"This result could not be explained by confound X or Y (Appendix Z)\" would suffice).",
            "49": "8) Font size in Figure 2 is tiny (axis labels are impossible to read), please make it bigger.",
            "50": "You don’t need that many ticks on the axes."
        },
        "rJxuMyU1ir": {
            "0": "This work studies factors which promote combinatorial generalization in a \"neural network agent\" embodied in a 3d simulation environment.",
            "1": "The authors present interesting experiments and some insightful empirical findings on how a richer environment and a first-person egocentric perspective can aid a simple neural net to generalize better over previously unseen tasks.",
            "2": "While I truly commend the effort undertaken to perform the experiments, I have several concerns which I explain below and would be happy to raise my score if they can all be addressed satisfactorily:\n\n1) While the authors interpret the experiment results in sec 4.1 in a positive way, the results don't seem to necessarily indicate good systematic generalization.",
            "3": "For instance, after learning with 40 words the agent only achieves 60% test accuracy.",
            "4": "While the accuracy increases to 78% on training with 100 words, the training and test accuracy gap indicates that the performance is still far from any kind of systematic generalization.",
            "5": "The results instead seems to be hinting that neural nets don't indeed perform combinatorial generalization on their own, but can be forced towards it by supplying them huge amounts of diverse data (which is not true for humans).",
            "6": "Also, the fact that increasing the number of words helps in generalizing better is true for most ML models and does not come as a surprise.",
            "7": "So the results in this subsection are somewhat trivial and do not necessarily contribute any new understanding.",
            "8": "2) For the experiments regarding egocentric frame in sec 4.3, I feel that the results are not really conclusive (even including the control exps in appendix D).",
            "9": "Could it be that if one uses any frame rigidly attached (i.e.",
            "10": "fixed displacement and rotational coordinates) to the agent's egocentric frame, one would achieve the same generalization performance?",
            "11": "It is also possible that as suggested by authors in sec 4.4, it is just the motion of the egocentric frame which might be giving diverse views of the environment to the agent.",
            "12": "So the frame might not even need to be egocentric, but just a moving frame which gives richer and diverse views whenever the agent moves.",
            "13": "Please include experiments to test for these possibilities.",
            "14": "3) In section 4.4, the authors have trained the non-embodied classifier with just a single image frame.",
            "15": "But this does not necessarily justify the conclusion that active perception helps in generalization.",
            "16": "This is because the motion of the RL agent gives it both a varied set of views AND also control over what views to obtain by taking actions.",
            "17": "In order to better understand which of these factors (or perhaps both) aid in generalization, another set of experiments is required which shows the classifier agent more images while keeping the desired object in view.",
            "18": "In one experiment, these images should be chosen with random movements but the number of such images provided to the classifier should be increased in sub-experiments to gauge if giving more varied views bridges the performance gap between the classifier and the RL agent's generalization performance.",
            "19": "In a second experiment, one might want to first train the RL agent, then extract a few (say 10) frames out of its enacted policy for all pairs of objects and use these frames as a part of the training set for the classifier agent.",
            "20": "This would allow one to gauge if both varied views and actively selecting to interact with the environment can help bridge the generalization gap.",
            "21": "4) Lastly, sec 4.5 seems to be hinting at a potentially very incorrect conclusion: \"language is not a large factor (and certainly not a necessary cause) of the systematic generalisation...\".",
            "22": "This cannot be said from the small single experiment presented in sec 4.5.",
            "23": "For instance, that experiment has been devised in a way that an optimal policy can be found with/without language.",
            "24": "However, if a language input is provided to explicitly state the desired object, that might speed up the training of the RL agents significantly.",
            "25": "In such a case, it might be helpful to see if learning the policy with the language input is being accomplished with a much lower number of frames during training, as opposed to when no language input is provided.",
            "26": "Please provide the training error plots.",
            "27": "But regardless of the plots, the experiments can still be quite inconclusive since language helps in systematic generalization in a variety of other ways apart from what has been tested for.",
            "28": "In general, language starts helping humans once it has been acquired to a sufficient extent since one needs noun-concept linkages, verb-action linkages etc.",
            "29": "to have been acquired a priori before the benefits of language emerge in combinatorial generalization.",
            "30": "Training an LSTM to understand the language commands in tandem with learning policies for picking desired objects could lead to sub-optimal or heavily over-fitted language models which may not help in generalization.",
            "31": "Testing for the true role of language will require many more experiments, which may be somewhat out of scope for this paper given the space constraints for a single paper.",
            "32": "But, I would advise the authors to refrain from drawing hasty inferences about the role of language without thorough experimentation.",
            "33": "Minor issues:\n\n1) What are the 26 actions in the Unity 3D environment in section 3?",
            "34": "It is important to know the action space to understand how easy or hard it is for the agent to learn generalizable policies.",
            "35": "2) The x-axis of Figure 2 is not readable at all.",
            "36": "Please rectify those graphs and reduce the number of ticks.",
            "37": "-------------------------- Update after interaction during author feedback period -------------------------------\nI appreciate the efforts that the authors have undertaken to address my concerns.",
            "38": "While the paper is far from perfect, it is still a very thought provoking work and I believe that it would make a valuable contribution to the line of works on systematic generalization in embodied agents.",
            "39": "I am updating my score to reflect the same."
        }
    },
    "Byl5NREFDr": {
        "r1exx7fTKS": {
            "0": "This paper studies the effectiveness of model extraction techniques on large pretrained language models like BERT.",
            "1": "The core hypothesis of the paper is that using pretrained language models, and pretrained contextualized embeddings, has made it easier to reconstruct models using model stealing/extraction methods.",
            "2": "Furthermore, this paper demonstrates that an attacker needn't have access to queries from the training set, and that using random sequences of words as a query to the \"victim\" model is an effective strategy.",
            "3": "They authors also show that their model stealing strategies are very cost effective (based on Google Cloud compute cost).",
            "4": "The basic set up of their experiments has a fine-tuned BERT model as the victim model, and a pre-trained BERT model as a the attacker model.",
            "5": "The attacker model is assumed to not have access to the training set distribution and the queries are randomly generated.",
            "6": "There are 2 strategies for query generation (with additional task specific heuristics): 1) randomly selecting words from WikiText-103, and 2) randomly selecting sentences or paragraphs from WikiText-103.",
            "7": "The victim model is passed a generated query and the attacker model is fine-tuned using the output from the victim model.",
            "8": "Overall, this paper find that this simple strategy for query generation is effective on 4 different datasets: SST2, MNLI, SQuAD 1.1 and BooolQ.",
            "9": "The method is also cost-effective, a few hundred dollars depending on the dataset and the number of queries used to train the attacker model.",
            "10": "The paper also present some analysis.",
            "11": "They find that queries with higher agreement across victim models (5 BERTs with different random seeds) also leda to better results for the attacker model.",
            "12": "The authors also run some experiments with humans to test the interpretability of the queries they generated.",
            "13": "They collect annotations on SQuAD using questions that were generated with the WIKI and RANDOM strategies (they also compare highest agreement and lowest agreement queries), and also collect a control with the original SQuAD questions.",
            "14": "While this is an interesting analysis to present, showing that most of the generated queries are nonsensical to humans and there is low inter-annotator agreement, I have an issue with the experimental procedure here: the victim model is fine-tuned on the original data, therefore it has picked up some of the data heuristics used to generate the queries, the annotators are not trained on, or shown any of the original examples (there is a control run, but these are presumably a separate set of annotators).",
            "15": "Through interviews, the authors learn that the annotators were using word overlap heuristics, but perhaps training the annotators on a small set of the original data would draw a closer example to the victim model.",
            "16": "Either way, while this is an interesting result, it seems a bit misplaced in this paper.",
            "17": "I'm not sure this human annotation experiment is contributing in any real way to the core thesis of the paper.",
            "18": "The authors also test the results of having a mismatch between the victim and attacker model.",
            "19": "They consider the mismatch of BERT-base and BERT-large models.",
            "20": "They conclude that \"attackers can maximize their accuracy by fine-tuning more powerful language models, and that matching architectures is a secondary concern.\"",
            "21": "This conclusion feels like a bit of a stretch.",
            "22": "I would suggest that the authors add another few rows of experiments comparing less similar model architectures.",
            "23": "The paper's finding that  a model trained from scratch, QANet on SQuAD, suffers significantly without access to the training set inputs is strong supportive evidence for their hypothesis that using pretrained language models has made model extraction easier.",
            "24": "The authors also present a few defense strategies, membership inference, implicit membership classification, and watermarking.",
            "25": "They also discuss the limitations of these strategies and do not claim to have solved the problem at hand.",
            "26": "Overall, I think this paper makes a useful  contribution to the field and I would accept this paper.",
            "27": "While I have a couple of issues with some of the experiments (human evaluation and architecture mismatch), I think this paper is thorough and the experiments are well presented.",
            "28": "This is the first paper, to the best of my knowledge, showing the efficacy of model extraction of large pretrained language models using rubbish/nonsensical inputs."
        },
        "SJlh2oSRKH": {
            "0": "This authors introduce a novel approach to successful modern extraction.",
            "1": "The paper is well written and easy to follow (the two exceptions/oddities are Figure 1 & Table 1, which appear one page before they are refered, which makes them initially hard to understand because they are out of context).",
            "2": "The experimental evaluation is both well-thought and convincing.",
            "3": "Given the \"unreasonable effectiveness\" of the proposed approach, one is left to wonder whether/how it is possible to systematically close the performance gap between the extracted model and the victim one.",
            "4": "Would well formed queries help?",
            "5": "how about random/smartly chosen training examples from the training/tuning set of the victim model?",
            "6": "or anything else?"
        },
        "HklZG620Fr": {
            "0": "The authors explore how well model extraction works on recent BERT-based NLP models.",
            "1": "The question is: how easy is it for an adversary model to learn to imitate the victim model, only from novel inputs and the corresponding outputs?",
            "2": "Importantly, the adversary is supposed to not have access to the original training set.",
            "3": "The authors state that this is problematic because such techniques could be used in order to gain information about (potentially private!)",
            "4": "training data of the victim model.",
            "5": "In the experiments, two different settings are studied: one where the output probabilities are known and one where only predicted classes (by the victim model) are available.",
            "6": "In either case, the adversary model achieves high agreement with the victim model.",
            "7": "One interesting finding is that random queries (i.e., inputs to the victim model) work well, too.",
            "8": "So, the main conclusion is that the possibility of such attacks is a problem for natural language processing.",
            "9": "Finally, the authors study two methods to help against the problem of potential model extraction: one that helps avoiding it and one that detects model copies.",
            "10": "This paper is technically not very novel, but asks interesting questions.",
            "11": "The methodology seems sound."
        }
    },
    "Syl5o2EFPB": {
        "B1xsvvvVtB": {
            "0": "Overall this is a good paper.",
            "1": "Such compact rewards over image captions are useful for evaluation and diagnosis.",
            "2": "*concerns:\n1. method:\n   1.1 At first, I think section 4 could be re-written to corporate more concepts in image captioning (such as replacing action a to word w) so that it becomes more readable for related readers.",
            "3": "1.2 The motivation for adding a constant to shift nash equailibrium could be stated clearer.",
            "4": "1.3 what is the state s in the context of image captioning?",
            "5": "the hidden vector h ?",
            "6": "1.4 since the reward is computed for each pair of (a, s), how to get the reward for the whole sentence?",
            "7": "Sum them up?",
            "8": "I wonder this because it seems you compute this in Table 1.",
            "9": "1.5 in Eq.",
            "10": "(5) and Eq.",
            "11": "(11), there are expectations.",
            "12": "Do you need to compute them using Monte Carlo Rollouts?",
            "13": "Or just averaging over mini-batchs?",
            "14": "Because G-GAN (Dai et al) also utilizes rollouts to estimate different values for different words.",
            "15": "2. experiments:\n  1.1 the experiment in terms of \"compactness\" may not  reflect well the concept of compactness.",
            "16": "While compactness means similar words may obtain similar rewards in the same caption,  top-k captions for an image may be different in multiple words, or different in formats rather than word choices.",
            "17": "How about top-k words in a given position of a given caption?",
            "18": "Or replace a given word with different words and compare the correlation between reward differences and semantic differences.",
            "19": "1.2  qualitative samples in terms of caption diversity could be included.",
            "20": "1.3 since the rewards are computed for each word separately,  the authors may include experiments on using such rewards to diagnose captions, such as replacing a word to make the caption better, etc."
        },
        "S1eMFynatr": {
            "0": "The authors propose using a recent method for adversarial inverse reinforcement learning (AIRL) for the task for generating high-quality image captions.",
            "1": "Leveraging the GAN framework, a discriminator is trained to distinguish real captions from those produced by the generator, while the generator is optimized with policy-gradients (REINFORCE) to maximize the pseudo-reward from the discriminator.",
            "2": "The main difference from prior work seems to be that the discriminator acts on a word-level, rather than sentence-level (as done, for instance in Dai et.",
            "3": "al.",
            "4": "2017).",
            "5": "Correspondingly, the generator policy is updated with the objective of 1-step reward maximization (more like contextual bandits), rather than with a long-term sequential decision-making objective (as done in Dai et.",
            "6": "al.",
            "7": "2017).",
            "8": "The evaluation is done using 2 data splits – standard and robust, with various metrics such as SPICE, CIDEr, BLEU, CHAIR.",
            "9": "Diversity analysis and ablations are also performed to dissect the performance of the proposed approach.",
            "10": "My 2 main issues with the paper are confusing motivation (in section 1) and various imprecise parts (in section 3 and 4).",
            "11": "1.",
            "12": "The authors argue that current GAN-based captioning models provide ill-defined rewards due to the “reward ambiguity problem”.",
            "13": "This problem is not explained or motivated well in the paper, but instead the readers are referred to the AIRL paper.",
            "14": "“Reward ambiguity” in inverse-RL arises because there could be many reward functions that yield the same optimal policy.",
            "15": "The AIRL algorithm recovers one of these possible reward functions, and since such a recovered reward could be shaped by the environment dynamics, AIRL attempts to disentangle the reward from the dynamics.",
            "16": "The motivation there is to use the recovered reward on a new system with different transition dynamics.",
            "17": "In the context of this paper though, I would like to understand the angle of reward ambiguity.",
            "18": "The authors disentangle the sentence reward into word-wise rewards; however, I’m not sure if there’s any relation between this and the disentanglement done in AIRL for solving reward ambiguity.",
            "19": "2.",
            "20": "One of the objectives is learning compact rewards.",
            "21": "It is claimed that addition of a constant term to the reward provided to the generator policy results in this, but what’s the intuition behind this?",
            "22": "As for evaluation, it needs to be shown that words with similar semantics have similar discriminator score.",
            "23": "How do we conclude this from Figure 2.?",
            "24": "Also, please include Up-Down method results in Figure 2.",
            "25": "3.",
            "26": "Section 3 questions\n     a.",
            "27": "How is state s_t defined?",
            "28": "It is very hard to follow sections 3 and 4 without a clear definition and example for this.",
            "29": "b.",
            "30": "“Finn et.",
            "31": "al.",
            "32": "2016 proved that IRL is mathematically equivalent …” --- this is imprecise.",
            "33": "Maximum-Entropy-IRL is equivalent to the GAN formulation, not general IRL.",
            "34": "c.\tp_theta(a,s) is referred to as “reward distribution”.",
            "35": "I don’t think it’s a distribution.",
            "36": "d.\tEquation 3.",
            "37": "AIRL defines g only as a function of state (s_t) for the disentanglement, and not like what the authors have written.",
            "38": "e.\tEquation 4.",
            "39": "How is s_t sampled?",
            "40": "4.",
            "41": "Section 4 questions\n     a.",
            "42": "4.1 says discriminator “maximizes the divergence”.",
            "43": "This doesn’t seem correct.",
            "44": "b.\tf is referred to as state-value.",
            "45": "This doesn’t seem correct.",
            "46": "c.\tShouldn’t the -1 term in Equation 8 disappear under expectation?",
            "47": "d.\tDon’t understand how second line of Equation 11 is arrived at.",
            "48": "There are quite a few other sources of mathematically imprecise writing that I noticed.",
            "49": "I would recommend the authors to be more robust in their presentation."
        },
        "BJlRe9-RFS": {
            "0": "This paper proposes a refined Adversarial Inverse Reinforcement Learning (rAIRL) to remedy the reward ambiguity by decoupling the reward for each word in a sentence, while the existing methods that utilize reinforcement learning to optimize evaluation score handle only sentence-level rewards.",
            "1": "Furthermore, a conditional term is introduced in the loss function to avoid mode collapse and to increase the diversity of the generated captions.",
            "2": "Throughout experiments on MS COCO show that the proposed method achieves state-of-the-art performance with several evaluation scores.",
            "3": "I think this is a good paper.",
            "4": "The idea to disentangle the sentence-level reward into word-level ones with Adversarial Inverse Reinforcement Learning (AIRL) is highly motivated.",
            "5": "Although the original AIRL has a problem that the convergence is slow, the authors introduce a constant term to shift on of the stationary points.",
            "6": "As reported, this refinement surprisingly improves the performance and achieves state-of-the-art performance, while the original AIRL degraded the performance.",
            "7": "Although I lean to accept this paper, I have two comments.",
            "8": "- I would like to recommend that the source code to reproduce the result should be released.",
            "9": "- As discussed by the authors, the introduced constant term can be regarded as a baseline in REINFORCE.",
            "10": "In (Rennie et al., 2017), a self-critical sequence is introduced as a baseline in REINFORCE.",
            "11": "Therefore, it is notable that this paper proposes another type of baseline.",
            "12": "I would like the authors to compare Att2in (Rennie et al., 2017) to the proposed method, and to discuss why rAIRL outperforms Att2in as shown in Table 5."
        }
    },
    "Bke02gHYwB": {
        "Byg0bMQ5YS": {
            "0": "Summary: This paper proposed a variational word embedding method, by using vMF distribution as prior and adding an entropy regularization term.",
            "1": "Strengths:\n[+] In the experiment parts, the authors describe the experiment settings in detail.",
            "2": "[+] Detailed descriptions for each equation is given.",
            "3": "Weaknesses:\n[-] Template: It seems that the authors use the wrong template, which is not the template for ICLR2020.",
            "4": "[-] Appendix: Although the author mentioned 'appendix' many times, I cannot see the appendix.",
            "5": "[-] Motivation: The connection between motivation and proposed method seems weak.",
            "6": "The authors argue 'interpretable' in their title and abstract, but their method and experiments do not show this point explicitly.",
            "7": "Questions\n[.]",
            "8": "The experiments seem a little weak.",
            "9": "The vocabulary size is 10K and the corpus is not so big, and I wonder whether the performance of the proposed method will be better for large corpus and longer training time.",
            "10": "[.]",
            "11": "For Equation 8, we should have a guarantee on the concentration of partition functions.",
            "12": "Is it still true for vMF distribution?",
            "13": "[.]",
            "14": "What is the advantage of vMF distribution?"
        },
        "rkxk7rD6tS": {
            "0": "The paper addresses the problem the problem in word embeddings where \"word-word\" or \"context-context\" inner products are relied upon in practice when the embeddings are usually only optimized for good properties of \"word-context\" inner products.",
            "1": "The new approach to training word embeddings addresses interpretability in the sense of having a theoretically grounded interpretation for the good properties that the inner products should possess -- namely that they capture the PMI between words.",
            "2": "By changing the embedding training procedure (to include a step that samples vectors from each word's distribution representation) this interpretation is made possible without problematic theoretical assumptions.",
            "3": "Now trained for the purpose they are intended to be used for, the approach yields strong SOTA results on the non-small challenge datasets.",
            "4": "The word is well exceptionally well motivated (we should directly optimize for the properties we want!)",
            "5": "and situated in the literature.",
            "6": "The opposition of Arora 2016 (getting at \"interpretable\" word embeddings with a latent variable model) and Mimno & Thompson 2017 (problematic assumptions in skip-gram embeddings) is particularly convincing.",
            "7": "The vMF distribution is underexamined as tool for analyzing high-dimensional semantic vector distributions, and it is an excellent fit for the purposes of this project.",
            "8": "To back up claims of theoretical interpretability, a derivation (building on Ma 2017) proceeds without problematic leap thanks to a property introduced by a carefully selected (if straightforward) regularization term.",
            "9": "This reviewer did not read the appendix (not sure where this would be located -- first time reviewing with OpenReview here), but the intuition doesn't seem to be much different than Ma.",
            "10": "To back up claims of applied performance, appropriate experiments are conducted on multiple evaluation datasets.",
            "11": "The results seem to be fairly interpreted.",
            "12": "This reviewer decides to accept this paper for its balance of theoretical and empirical contributions and for the role it might play in reducing dependence on mysticisms in word embeddings (relying on accidental / uncharacterized properties of previous embedding strategies).",
            "13": "Suggestions for improvement:\n- Run a spell checker.",
            "14": "It will catch a large number of problems that weren't ever big enough to hurt my appreciation of the paper.",
            "15": "- Consider the creation of an adversarial dataset (of ~3 words in ~3 contexts) where techniques that optimize for the wrong thing will catastrophically fail but the proposed approach succeeds.",
            "16": "- Write one or two more sentences about the fate of \\kappa_c -- is it ever updated or am I just missing something?",
            "17": "Making a bigger point about leaving \\kappa un-optimized shows there is room for additional depth in this line of thinking.",
            "18": "(If you start learning concentration parameters, check out the Kent distribution for an even more expressive distribution on the unit hypersphere: https://en.wikipedia.org/wiki/Kent_distribution)\n- Figure out what happened to your appendix.",
            "19": "- Watch out that readers/reviewers of similar work may try to read the word \"interpretable\" in the title as a reference to the much broader topic of interpretability in ML related to explaining a model's behavior.",
            "20": "Is there a synonym for \"interpretable\" that won't raise this false link?"
        },
        "ryg1XGKOcB": {
            "0": "This paper is about learning word embeddings.",
            "1": "In contrast to previous work (word2vec's Skipgram) where a word is represented by a fixed word and context vector, here each word is represented by a von Mises-Fisher distribution and a context vector.",
            "2": "The paper claims that the resulting embeddings are more interpretable, and that the inner product of two context vectors represents their point-wise mutual information.",
            "3": "My main concerns are the following:\n\n- Representing words by distributions is not a novel idea; it was previously done by Brazinskas et al.",
            "4": "(2017): Embedding words as distributions with a bayesian skip-gram model (BSG).",
            "5": "They however used Gaussian distributions and not von Mises-Fisher.",
            "6": "BSG is acknowledged in the paper, but only small scale comparisons are performed (15 million) while the BSG paper uses a lot larger data sets.",
            "7": "There is therefore not a meaningful comparison to the most relevant previous work.",
            "8": "- Word similarity experiments are not enough to justify this approach.",
            "9": "BSG at least showed the strength of using distributions to represent words by showing that different samples could constitute different word senses/meanings.",
            "10": "There is no such analysis here.",
            "11": "- The claim that the resulting representations are more \"interpretable\" is not backed up by any evidence at all, even though the word \"interpretable\" is in the title and the list of contributions.",
            "12": "Generally this paper could benefit from proof reading and editing."
        }
    },
    "H1gx1CNKPH": {
        "H1xMxNR2FB": {
            "0": "Summary:\n- The paper proposes augmenting transformer neural networks with KNN-based information fetching modules that can access relevant external knowledge, combine knowledge from different sources, and integrate the information into seq-to-seq architectures.",
            "1": "The authors apply their proposal to generative dialog modeling, and apply it to two dialog datasets.",
            "2": "Strengths:\n- The paper is well-written and well-motivated.",
            "3": "- The authors evaluate their approach on 2 publicly available datasets and compare it to existing approaches, showing improvements in terms of F1.",
            "4": "The authors conduct a human evaluation to compare their approach against other approaches.",
            "5": "Weaknesses:\n- There are some details that are missing from the paper, for example details about the mapping operator, and the specific representations of E_i for the datasets used.",
            "6": "- Parts of the analysis are rushed (e.g., sections 6.2 and 6.3).",
            "7": "Questions/Comments:\n- In the human evaluation, is there a difference between the ratings for seen and unseen topics for the Wizard of Wikipedia dataset?",
            "8": "- Section 6.3 (especially the part on the effect of gating) can be improved with additional information/analysis."
        },
        "HJxDDJkX9r": {
            "0": "This paper uses KNN-based retrieval method for extracting relevant information from different sources for the task of generating response in a dialogue task.",
            "1": "They utilized different sources of information from Wikipedia, YFCC image set and dialogue utterances.",
            "2": "The method is very straight forward and using nearest neighbors from external information sets as auxiliary information and encoding that information via same neural net to produce the encoded external information.",
            "3": "The concatenation of encoded input and auxiliary information is concatenated to produce the output in the dialogue via decoder network.",
            "4": "The novelty of the proposed method is incremental over Dinan et al.",
            "5": "(2018).",
            "6": "The KNN-based retrieval module can produce some drift in dialogue from the actual context of the input which can result in irrelevant response in dialogue.",
            "7": "Some failure cases can show the quality changes with those drifts in dialogues."
        },
        "B1lDPWhucr": {
            "0": "### Summary\n​\nThis paper provides a framework to augment dialogue generation with external data sources using K-Nearest Neighbors in the embedding space.",
            "1": "The idea seems simple and intuitive, and the results show improvements over prior work in dialogue generation and retrieval.",
            "2": "​\n​\n### Strengths\n- The paper shows quantitative improvement over some prior works on dialogue agents (however this needs to be correctly validated).",
            "3": "- The reviewer appreciates the human study and conversation example provided in the paper to qualitatively evaluate their model.",
            "4": "- The paper provides ablation studies of the various training tricks for each dataset they have proposed.",
            "5": "​\n### Weaknesses\n- There are certain changes in the training pipeline that makes the comparison with prior work difficult (Tables 1 and 2), and find the real contribution of data-augmentation caused by KNN-based information fetching (KIF).",
            "6": "e.g., In Wizard of Wikipedia experiment, most recent dialogue utterance and turn number are used as salient features.",
            "7": "Similarly, the paper utilizes a \"personality\" feature in ImageChat dataset.",
            "8": "It seems the results taken from prior papers have different training settings.",
            "9": "Could the authors verify that the extra assumptions made for their model are equivalently applied to other generative baselines?",
            "10": "If not, the reviewer recommends to provide some experiments with same conditions.",
            "11": "- Could the authors provide the embedding dimensions and other training details, to assess the contribution of different components of the input's embeddings.",
            "12": "​\n​\n#### Minor:\n- Is there a sound reason behind using concatenation of input representation and fetched representations?",
            "13": "This design choice makes the architecture inflexible to the number of data sources.",
            "14": "Another way to combine embeddings - e.g.",
            "15": "addition or inner product, can also be tried to see if they provide performance improvements.",
            "16": "- It is mentioned that attention based modules scale poorly with large sized datasets.",
            "17": "If the authors conducted a quantitative evaluation to test this, it would be valuable to add it to the paper.",
            "18": "- The title and abstract should be limited to generative dialogue modeling, instead of transformers, since the contributions proposed are more suited for this particular application and are quite engineered.",
            "19": "Hence, it is not correct to make this claim for transformers in general.",
            "20": "​\n​\n### Score\nWeak Reject (Leaning towards Accept if appropriate experiements can be provided)"
        }
    },
    "r1xQNlBYPS": {
        "H1g0adr3Kr": {
            "0": "[Paper summary]\nThis work is an extension of KERMIT (Chan et al., 2019) to multiple languages and the proposed model is called “multichannel generative language models”.",
            "1": "KERMIT is an extension of “Insertion Transformer” (Stern et.",
            "2": "al, 2019), a non-autoregressive model that can jointly determine which word and which place the translated words should be inserted.",
            "3": "KERMIT shares the encoder and decoder of insertion Transformer, and the source sentence and target sentence are concatenated to train a generative model (also, various loss functions are included).",
            "4": "In this work, parallel sentences from more than two languages are concatenated together and fed into KERMIT.",
            "5": "Each language is associated with a language embedding.",
            "6": "This work demonstrates that a joint distribution p(x1, .",
            "7": ".",
            "8": ".",
            "9": ", xk) over k channels/languages can be properly modeled through a single model.",
            "10": "The authors carry out experiments on multi30k dataset.",
            "11": "[Pros] Some discoveries of this work are interesting, including: (1) It is possible to use a single model to translate a sentence into different languages in a non-autoregressive way.",
            "12": "(2) The unconditional multilingual generation in Section 4.5 is interesting, especially, the generation order is determined by the model rather than left-to-right.",
            "13": "[Questions]\n1.",
            "14": "The authors work on multi30k dataset, which is not a typical dataset for machine translation.",
            "15": "(A)\tThe dataset and the corresponding information is at https://github.com/multi30k/dataset.",
            "16": "The number of words in a sentence is smaller than 15, which is too short for a machine translation.",
            "17": "Also, the pattern of sentences is relatively simple.",
            "18": "(B)\tFor real world application, I am not sure whether it is possible to collect a large amount of k-parallel data where $k>2$.",
            "19": "Therefore, the application scenario is limited.",
            "20": "What if we have a large amount of bilingual data instead of k-parallel data?",
            "21": "How should we leverage the large amount of monolingual data?",
            "22": "2.",
            "23": "For novelty, this is an extension of KERMIT to a multilingual version, which limits the novelty of this wok.",
            "24": "3.",
            "25": "The best results on En->De in Table 1 are inconsistent.",
            "26": "On tst16, bilingual en<->de is the best; on tst17, en<->{rest} is the best; on mscoco, any<->rest is the best.",
            "27": "In Table 2, seems using bilingual data only is the best choice.",
            "28": "This makes me confuse about how to use your proposed method.",
            "29": "However,"
        },
        "S1gYvmxpFB": {
            "0": "This paper proposes a multichannel generative language model (MGLM), which models the joint distribution p(channel_1, ..., channel_k) over k channels.",
            "1": "MGLM can be used for both conditional generation (e.g., machine translation) and unconditional sampling.",
            "2": "In the experiments, MGLM uses the Multi30k dataset where multiple high quality channels are available, in the form of multilingual translations.",
            "3": "I feel that this paper is not ready for publication at ICLR due to the following major issues:\n\n* Missing important related work: This paper seems unaware of an important related work \"Multi-Task Learning for Multiple Language Translation\" by Dong et al, ACL 2015.",
            "4": "In fact, Dong et al.",
            "5": "investigated the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages.",
            "6": "Although machine translation is just an example of MGLM, Dong et al.",
            "7": "is highly relevant to the conditional generation with MGLM, needless to say that they share the same multi-language translation problem domain.",
            "8": "Thus, this paper will be much stronger if comparison with important baseline methods is provided.",
            "9": "* Limited novelty: This paper extends Chan et al.",
            "10": "'s KERMIT by applying its objective on tasks with more than 2 sequences, in order to learn the joint distribution p(channel_1, ..., channel_k) over k channel sequences.",
            "11": "Most of the math in this paper can be found in the original Chan et al.",
            "12": "'s paper.",
            "13": "The extension to the multichannel case is incremental as it is hard to justify the challenge of such extensions.",
            "14": "Besides, as minor suggestions, it would help readers if more illustrations of Figure 1 (especially the inference part) can be provided."
        },
        "SJlT6ASJcS": {
            "0": "This submission belongs to the area of multi-view modelling.",
            "1": "In particular, the submission describes construction of multi-view language models that (i) can generate text simultaneously in multiple languages, (ii) can generate text in one or more languages conditioned on text from another language.",
            "2": "This submission extends previously proposed KERMIT from two views to more than two views.",
            "3": "I believe this paper could be of interest to multi-view modelling/learning community.",
            "4": "Though the original KERMIT approach is very interesting and you application of it to more than two views is also interesting I find the presentation to be poor.",
            "5": "In particular I find section 2 to be hard if not impossible to understand without referring to the original paper where the story, equations, nomenclature are much more clearly explained.",
            "6": "Even though your extension from two views to multiple is simple I find reliance on a diagram to be a mistake as I find your description not to be very clear.",
            "7": "Given that there are no equations to support the reader and that the original equations are not adequate I find it hard to understand Sections 2 and 3.",
            "8": "The key experimental result in Table 1 is only briefly commented on despite featuring multiple models with different strength and weaknesses, multiple types of inference.",
            "9": "If space is of concern I would suggest removing Figure 2 (or changing input from non-English to English and removing or removing another qualitative table)."
        }
    },
    "Bkga90VKDB": {
        "B1lOWHeJ9H": {
            "0": "\n\nThis paper proposes a method for compressing embedding matrices of both encoder/decoder embeddings.",
            "1": "The basic idea of the proposed method is to reconstruct the embedding matrix by what they called the “funneling decomposition” method, whose parameter shape is identical to the SVD (low-rank matrix) decomposition with additional non-linear function.",
            "2": "Therefore, the idea itself is not so novel and innovative.",
            "3": "Moreover, their method requires the embedding matrix as the teacher signal for calculating the reconstruction loss.",
            "4": "We need to note that the memory requirement of the proposed method during training will increase.",
            "5": "One of the notable advantages of the proposed method is that their proposed method seems to successfully reduce the embedding matrix even if it shares the parameters with the output layer, which is a de-facto standard model architecture for NMT.",
            "6": "As pointed out by the authors, this seems to be the first success of reducing the embedding matrix with a tied embedding setting.",
            "7": "1,\nThe authors claim that “We demonstrate that at the same compression rate our method outperforms existing state-of-the-art methods.” at the end of the Introduction section.",
            "8": "However, according to Tables 1, 2, and 3, it seems that the performance gain is marginal compared with similar methods.",
            "9": "For example, \n37.78 (proposed) <=> 37.78 (Shi & Yu (2018)) \n26.97 (proposed) <=> 26.75 (Chen et al.",
            "10": "(2018)\nand\n 42.62 (proposed) <=> 42.37 (SVD with rank 64),\nwhich are the at most 0.25 BLEU gain.",
            "11": "I believe that most of MT researchers hardly say that BLEU 0.25 difference is a significant improvement.",
            "12": "Besides, the authors should perform a statistically significant test if they say “our method outperforms existing state-of-the-art methods.”\n \n \n2\nI am a bit confused about the following inconsistency;\nThe authors say that “Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment” in the abstract.",
            "13": "However, according to Table 1, the number of parameters for embeddings is 16.3M, which is only 27% of the total number of parameters in Transformer base.",
            "14": "By this fact, compressing embedding matrices seems not essential for successful commercial edge deployment.",
            "15": "In Table 6, it is explicitly unclear what is the difference between \n“Funneling with Emb.",
            "16": "Distillation”, “Funneling (with non-linearity),” and “Funneling (with retraining all weights).”\nPlease give us a more precise explanation."
        },
        "HJl-Mn5kcS": {
            "0": "The paper proposes to use low-rank matrix decomposition for embedding compression, with relu in the reconstruction layer to gain non-linearity.",
            "1": "Experiments on machine translation task shows improvement compared with state-of-the-art methods with different compression rates.",
            "2": "Detailed comments:\n1)\tThe technical contribution seems to be a bit limited.",
            "3": "Using relu in the reconstruction function looks straightforward and adding reconstruction loss in objective function is also common practice.",
            "4": "Also, not much insight is provided on why such approach works better than other baselines.",
            "5": "2)\tExperiments:\na.",
            "6": "It is good to see such simple approach outperforms several more sophisticated baseline methods.",
            "7": "Also, ablation study is also performed to show the effect of different components.",
            "8": "b.",
            "9": "How does the time complexity and running time of the proposed method compared to the baselines?",
            "10": "c.\tThe paper only evaluates distilled embedding on one task (i.e., machine translation).",
            "11": "The experiments would be more convincing if evaluated on more tasks as well.",
            "12": "d.\tIt could be helpful to include some sensitivity analysis on the hyperparameters such as \\alpha which controls the weight of reconstruction loss.",
            "13": "In conclusion, this paper seems to be below the bar and I would recommend a ‘weak reject’ for the paper."
        },
        "BJgi5BU8qS": {
            "0": "There are many ways to reduce the memory footprint and increase speed of a neural network: weight quantisation, compression, coarse-to-fine, knowledge distillation, etc.",
            "1": "The method proposed in this work is a specific case of knowledge distillation that focuses on the discrete-input-to-first-layer and output-layer-to-discrete-output transformations, which represent a large portion of the parameters.",
            "2": "The authors propose to use a variant of SVD (which can be viewed as 2 linear transformation, with a middle dimension that represents an embedding), where the first transformation is linear with a ReLu, and the second is linear.",
            "3": "By approximating the learned matrices of the model, the experiments show that using the proposed variant of SVD gives similar predictive performance compared to the original model, with a fraction of the parameters.",
            "4": "However, it seems that the authors could have simply replaced the input by a 2-layer NN (first a linear+ReLu, then a Linear) to obtain the same parametrisation, but they could have learned the parameters in a end-to-end fashion.",
            "5": "It is not clear to me why using a surrogate L2 loss within the model should give better predictive performance than a fully end-to-end trained neural network.",
            "6": "Without this comparison, I do not think the proposed experiments are conclusive enough."
        }
    },
    "SygcCnNKwr": {
        "H1lWSY0fKH": {
            "0": "Summary\n\nStrength:\n1.",
            "1": "This topic studied in this paper is interesting and is helping to promote the following developing of algorithms with compositional generalization ability.",
            "2": "2.",
            "3": "The experimental results show the effectiveness of the proposed metric for measuring compositional diversity.",
            "4": "comments:\n1.",
            "5": "How to control the trade-off between the atom and compositional divergence?",
            "6": "It's interesting to show how different\ncompositional divergence can affect the performance of different models.",
            "7": "2.",
            "8": "Many previous works are proposed for improving the generalization ability of the seq2seq models[1].",
            "9": "More experiments need to\nbe conducted using these previous methods.",
            "10": "[1]Compositional generalization in a deep seq2seq model by separating syntax and semantics"
        },
        "BJgDGC9RFB": {
            "0": "This paper first introduces a method for quantifying to what extent a dataset split exhibits compound (or, alternatively, atom) divergence, where in particular atoms refer to basic structures used by examples in the datasets, and compounds result from compositional rule application to these atoms.",
            "1": "The paper then proposes to evaluate learners on datasets with maximal compound divergence (but minimal atom divergence) between the train and test portions, as a way of testing whether a model exhibits compositional generalization, and suggests a greedy algorithm for forming datasets with this property.",
            "2": "In particular, the authors introduce a large automatically generated semantic parsing dataset, which allows for the construction of datasets with these train/test split divergence properties.",
            "3": "Finally, the authors evaluate three sequence-to-sequence style semantic parsers on the constructed datasets, and they find that they all generalize very poorly on datasets with maximal compound divergence, and that furthermore the compound divergence appears to be anticorrelated with accuracy.",
            "4": "This is an interesting and ambitious paper tackling an important problem.",
            "5": "It is worth noting that the claim that it is the compound divergence that controls the difficulty of generalization (rather than something else, like length) is a substantive one, and the authors do provide evidence of this.",
            "6": "At the same time, I think the authors could possibly do more to show that the trend in the plots in Figure 2 can't be explained by something else: for example, the authors could show that the length ratios remain constant as the compound divergence is varied.",
            "7": "I think it is also not necessarily clear how easily the notion of differing compound distributions generalizes to other types of tasks.",
            "8": "Presentation-wise, much of the paper is clear and well written, though I think the discussion of weighted frequency distributions of compounds (top of page 3) could be clarified further, and in particular an example subgraph of a rule application DAG should be highlighted here."
        },
        "Skgd0wo0tS": {
            "0": "This paper introduces a method for generating training/test data for measuring the model's ability of \"compositional generalization\" in complex compositional tasks/domains such as natural language understanding, visual understanding and other domains.",
            "1": "The idea of the proposed method is to essentially keep the \"atom\" distribution unchanged between the training and test data, but maximize the divergence between the \"compound\" distributions between them.",
            "2": "The authors have conducted a thorough and systematic experimentation, comparing the proposed approach with a number of other heuristic approaches for train test splits (such as random and input/output length, etc.)",
            "3": "and using both a new large data set they generated (CFQ) and existing data set (SCAN).",
            "4": "The experimental results verify that using their method they can obtain train test data sets with uniform atom distributions with large divergence in compound distributions, and they find that there is a surprisingly large negative correlation between the accuracy of existing state-of-the-art learning methods and the compound divergence.",
            "5": "The data generation mechanism is systematic and involved, consisting of different categories of rules (logical form, grammar, rule application DAG's, etc.)",
            "6": "and it would seem that the generation method/system and the generated data would be useful as benchmark data for the community.",
            "7": "The paper lacks technical novelty other than the training and test data generation approach, but having one available to the community with these apparently desirable characteristics as benchmark data for measuring complex, compositional generalization capabilities, and that could be invaluable to the research community."
        }
    },
    "B1elCp4KwH": {
        "HJlEmq1jFS": {
            "0": "Overview:\n\nThe paper proposes a method to learn discrete linguistic units in a low-resource setting using speech paired with images (no labels).",
            "1": "The visual grounding signal is different from other recent work, where a reconstruction objective was used to learn discrete representations in unsupervised neural networks.",
            "2": "In contrast to other work, a hierarchy of discretization layers are also considered, and the paper shows that, with appropriate initialization, higher discrete layers capture word-like units while lower layers capture phoneme-like units.",
            "3": "Strengths:\n\nThe paper is extremely well-written with a clear motivation (Section 1).",
            "4": "The approach is novel.",
            "5": "But I think the paper's biggest strength is in its very thorough experimental investigation.",
            "6": "Their approach is compared to other very recent speech discretization methods on the same data using the same (ABX) evaluation metric.",
            "7": "But the work goes further in that it systematically attempts to actually understand what types of structures are captured in the intermediate discrete layers, and it is able to answer this question convincingly.",
            "8": "Finally, very good results on standard benchmarks are achieved.",
            "9": "Weaknesses:\n\nAlthough I think the paper is very well-motivated, my first criticism is that discretization itself is not motivated: why is it necessary to have a model with discrete intermediate layers?",
            "10": "Does this give us something other than interpretability (which we obtain due to the sparse bottleneck)?",
            "11": "In the detailed questions below, I also specifically ask whether, for instance, the downstream speech-image task actually benefits from including discrete layers.",
            "12": "My second point is that it is unclear why word-like units only appear when the higher-level discrete layers are trained from scratch; as soon as warm-starting is used, the higher level layers capture phoneme-like units (Table 1).",
            "13": "Is it possible to answer/speculate why this is the case?",
            "14": "Overall assessment:\n\nThe paper presents a new approach with a thorough experimental investigation.",
            "15": "I therefore assign an \"accept\".",
            "16": "The weaknesses above asks for additional motivation and some speculation.",
            "17": "Questions, suggestions, typos, grammar and style:\n\n- Section 3.3: It maybe makes less sense for the end-task, but did the authors consider discretization on the image side of the network?",
            "18": "This could maybe lead to parts of objects being composed to form larger objects (in analogy to the speech network).",
            "19": "- Section 3.3, par.",
            "20": "3: \"with the intention that they should capture discrete word-like and sub-word-like units\" -> \"with the intention that they should capture discrete *sub-word-like and word-like units*\" (easier to read with first part of sentence)\n- Section 3.3: The more standard VQ-VAE adds a commitment loss and a loss for updating the embeddings; was this used or considered at all, or is this all captured through the exponential moving average method?",
            "21": "- Section 3.4: \"with same VQ layers\" -> \"with *the* same VQ layers\"\n- Section 3.5: Can you briefly outline the motivation for adding the two losses (so that it is not required to read the previous work).",
            "22": "- Section 4.1: Following from the first weakness listed above, the caption under Figure 2 states that the non-discrete model achieves a speech-image retrieval R@10 of 0.735.",
            "23": "This is lower than some of the best scores achieved in Table 1.",
            "24": "Can this be taken as evidence that discretization actually improves the downstream task?",
            "25": "If so, it would be worth highlighting the point more; if there is some other reason, that would also be worth knowing.",
            "26": "- Figure 1: Did the authors ever consider putting discrete layers right at the top of the speech component, just before the pooling layer?",
            "27": "Would this more consistently lead to word-like units?"
        },
        "SyxuhAn2FS": {
            "0": "This paper attempts to learn discrete speech units in a hierarchical (phone and word) fashion by incorporating multiple vector quantization layers into the audio encoder branch of a model that visually grounds speech segments with accompanying images.",
            "1": "The model has been tested and compared against two algorithms and implementations that set the SOTA on the Zero Speech 2019 challenge (further improving one of them in the process, it seems), and outperforms these significantly using the ABX metric, so the proposed method seems to perform well (the model is using additional supervision, though).",
            "2": "In addition, this is an interesting and timely research problem with implications far beyond the core machine learning setup.",
            "3": "The hierarchical setup, and the finding that successful learning here depends on the curriculum, is intriguing indeed.",
            "4": "The paper is a pleasure to read and provides a rich set of results and analyses.",
            "5": "A few remarks:\n- It is probably worth explaining how the ABX test is performed, i.e.",
            "6": "that features are extracted from some layer of a model, and then a time alignment is performed to get the score - this is in the text somehow, but i had to read it multiple times.",
            "7": "- Did you try other architectures like 5 layers (rather than 4) in Figure 2\n- Figure 2 is a bit hard to interpret.",
            "8": "Maybe plot log of ABX error rate or something, to pull apart the different layers?",
            "9": "- Could you explain the difference between cold-start and warm-start?",
            "10": "One is adding the discretization to a pre-trained model, the other is training from the start?",
            "11": "- When you measure ABX at layer 2 and 3, in a model trained with quantization, do you measure ABX on the features before or after quantization?",
            "12": "does it make a difference?",
            "13": "- Table 7: some of the top word hypothesis pairs make sense acoustically (building-buildings, red-bed, ...), some could be neighboring words (large-car, ...), but some are just weird (people-computer) - any intuition as to what is going on?"
        },
        "HJxsX9aL9B": {
            "0": "Pretty interesting paper attempting to learn discrete linguistic units via vector quantization of visually grounded, speech related features.",
            "1": "I think this is a worthwhile contribution.",
            "2": "My main complaint is that the exposition is a bit diffuse and fails to crystallize the essence of the work.",
            "3": "In particular, the claim is that the novelty is from the use of a \"discriminative, multi-modal grounding objective\".",
            "4": "Reading the work, this seems to be the triplet loss described in Section 3.5.",
            "5": "Is that the novel objective?",
            "6": "In my my the really interesting aspect that should be stressed is the visual grounding -- I encourage the authors to highlight that aspect more directly.",
            "7": "I fear that essential and interesting point is somewhat diluted in the detailed exposition of results."
        }
    },
    "BkePneStwH": {
        "rJewS5lLtB": {
            "0": "This paper proposes two improved strategies for fine-tuning XLM (a multilingual variant of BERT) for cross-lingual NLI.",
            "1": "First of all, it shows that fine-tuning a single model on the combination of all languages (the original English data from MultiNLI and their MT translation into the rest of languages) performs better than fine-tuning a separate model for each language.",
            "2": "Furthermore, they show that minimizing the L2 distance between the English training sentences and their MT translation into the rest of languages, which does not explicitly use any labels in the foreign languages and is presented as a way of performing cross-lingual knowledge distillation, also performs better than zero-shot transferring a regular model fine-tuned in English.",
            "3": "I think that the paper makes some interesting contributions and, in particular, I think that the finding that multilingual fine-tuning performs better than the standard approach of fine-tuning a separate model for each language is important.",
            "4": "Nevertheless, I am not convinced that there is enough novelty and substance on this, I have some concerns on the evaluation, and I think that the overall presentation should also be improved:\n\n- I am not convinced by the \"knowledge distillation\" approach.",
            "5": "First, although I see the connection, I do not think that presenting this as \"knowledge distillation\" is consistent with the common use of this term in the literature.",
            "6": "More importantly, I do not see what is the value of this approach considering that multilingual fine-tuning performs better, and combining them both does not bring any clear improvement.",
            "7": "The authors motivate it as a form of performing zero-shot cross-lingual transfer as, unlike the multilingual fine-tuning, this approach does not use any label in the foreign languages.",
            "8": "However, I am not convinced at all by this reasoning, as it still relies on the translation of the English labeled data into the other languages.",
            "9": "So, from a practical perspective, it requires the exact same resources as the other approach, as you would always be able to use the English labels for the rest of the languages, while being more complex and worse.",
            "10": "- It looks like the IndT results, which is the real baseline, are taken from the original XLM paper, while the rest of the results come from the authors' own runs, who use a different implementation.",
            "11": "I think that you should also report IndT results from your own runs to make sure that your improvements come from the actual method, and not from small implementation details.",
            "12": "- You are trying small variations of your method (e.g.",
            "13": "removing a particular language from the multilingual training) to support your claims, and it is not clear if the (rather small) differences in the results are significant.",
            "14": "It would be good if you at least run the baseline multiple times and show the variance.",
            "15": "- It is unfair to try so many variants of your method in the test set, and then pick the best and claim SOTA as done in Table 6.",
            "16": "Your final system looks rather ad-hoc and arbitrary: it is doing multilingual fine-tuning in all languages but Urdu, and cross-lingual knowledge distillation in a subset of 4 languages out of 15.",
            "17": "It might get SOTA results in this particular scenario, but what if we move to a different set of languages, a different task, or even a different test set?",
            "18": "- The authors claim that \"Urdu (ur) is an unrelated language\" and \"Swahili (sw) is loosely between French and Urdu in terms of relatedness to English\", which they use to justify why Urdu behaves differently in their experiments.",
            "19": "I do not speak neither Swahili nor Urdu, but from what I know this statement looks wrong.",
            "20": "Swahili and English belong to completely different language families, and from what I know their grammar is very different.",
            "21": "In contrast, Urdu at least belongs to the Indoeuropean language family.",
            "22": "- This is not relevant at all, but I would suggest the authors to find a different acronym instead of XD, which happens to be a widely used emoticon.",
            "23": "I assume that the authors deliberately made this choice thinking that it would be funny, but I just find it confusing to see XD all over the place in a formal paper."
        },
        "BygAwA40FH": {
            "0": "First, the authors propose to train a model for natural language inference (NLI) on multiple languages simultaneously.",
            "1": "In particular, they translate English examples into all target languages and fine-tune a pretrained language model on all thereby obtained data at once.",
            "2": "This is different from the previous state-of-the-art approach which consisted of, after translating from English into target languages, fine-tuning one NLI model for each language individually.",
            "3": "The authors show that their approach is superior to training individual models for each language.",
            "4": "For evaluation, XNLI is used.",
            "5": "Second, they introduce cross-lingual knowledge distillation (XD), where the same polyglot model is used both as teacher and student across languages to improve its sentence representations without using the target task labels.",
            "6": "The main idea is that the same sentence in all languages should receive output representations as similar as possible.",
            "7": "The paper seems okay to me and the experiments seem solid.",
            "8": "However, the results are not particularly surprising and the methods are not very innovative.",
            "9": "The writing could be improved.",
            "10": "This paper could further be improved in the following ways:\n- A more detailed investigation which combination of languages improve performance (and why?).",
            "11": "- Similarly: A combination of MTL and XD doesn't seem straightforward.",
            "12": "Why?",
            "13": "What is learned?",
            "14": "Smaller comments:\n- Articles are missing frequently (e.g., \"we substitute the word prediction head with classification layer\" -> \"we substitute the word prediction head with a classification layer\")\n- Table 5: \"w/0\" -> \"w/o\"?",
            "15": "- Have you run any significance tests?"
        },
        "S1e0ZJj35B": {
            "0": "What is the task?",
            "1": "Multilingual natural language inference (NLI)\n\nWhat has been done before?",
            "2": "Current state-of-the-art results in multilingual natural language inference (NLI) are based on tuning XLM (a pre-trained polyglot language model) separately for each language involved, resulting in multiple models.",
            "3": "What are the main contributions of the paper?",
            "4": "[Not novel] Significantly higher average XNLI accuracy with a single model for all 15 languages.",
            "5": "[Moderately novel] Cross-lingual knowledge distillation approach that uses one and the same XLM model to serve both as teacher (for English sentences) and student (for their translations into other languages).",
            "6": "The approach does not require end-task labels and can be applied in an unsupervised setting\n\nWhat are the main results?",
            "7": "A single model trained for all 15 languages in the XNLI dataset can achieve better results than 15 individually trained models, and get even better when unrelated poorly-translated languages are removed from the multilingual tuning scheme.",
            "8": "Using XD they outperformed the previous methods that also do not use target languages labels.",
            "9": "Weaknesses :\n1.",
            "10": "Combining XD with multilingual tuning is not effective in improving average results or even in case of target languages\n2.",
            "11": "Final system is adhoc as experiments on a particular set of languages have been used to support claims.",
            "12": "For example, Urdu was excluded to get the best MLT model.",
            "13": "Only 4 languages were used while combining XD and MLT\n3.",
            "14": "Findings, methods and experiments are not strongly novel.",
            "15": "4.",
            "16": "Paper was not an easy read.",
            "17": "Strengths:\n Using XD they outperformed the previous methods that also do not use target languages labels."
        },
        "rJgnL8waqB": {
            "0": "The paper addressed multilingual natural language inference.",
            "1": "The motivations of the authors are two folds: 1/ to have only one model for all the languages instead of one model per language and 2/ achieving good results in a zero shot setup where only the english labels are available.",
            "2": "Previous work proposed first to learn multilingual language model in a self-supervised manner for the initialization.",
            "3": "Then, fine-tuning it on the final task, separately for each different language.",
            "4": "It results in multiple models, one for each language.",
            "5": "The authors proposed to fine-tune the model with all the data from the different languages simultaneously, resulting in only one model with comparable results.",
            "6": "In addition, they also proposed a method based on distillation where only the english targets are used.",
            "7": "While the model doesn't require the label data for the other languages, the scores remained similar to the previous experiments.",
            "8": "Finally the authors proposed to combine both methods.",
            "9": "It compares favorably, obtaining 4 points of improvement over SOTA in the zeroshot setup.",
            "10": "Pros:\n-the motivation for having only one model is interesting\n-the results are promising\n\nCons:\n-one of main motivation of the paper is to achieve zeroshot as opposed to previous work.",
            "11": "To that purpose, the authors chose to keep only the translated non-en input and not the non-en target.",
            "12": "In XNLI, all the non-en training data, including target, are automatically translated from the English data.",
            "13": "Therefore, the authors did not use less human annotated data and their approche still requires automatic translation.",
            "14": "Hence, the motivation to perform the task in a zero-shot scheme, as opposed to previous work, doesn't seem correct.",
            "15": "-the paper was not always easy to follow and would benefit from more clarity.",
            "16": "'large margin of 5.9 and 4.2 points' in 3.2.1, please add the reference to table 6.",
            "17": "-the method 'significantly' improved results.",
            "18": "I didn't see any significance measurements and it would be important to add them.",
            "19": "Overall, using all the data together seems like a natural and effective approach to and achieves good results through only one model.",
            "20": "However, the motivation behind 'distillation' is to perform in a zeroshot scheme.",
            "21": "It seems abusive to me since it actually requires the exact same amount of human labels than the previous works."
        }
    },
    "Hkx6hANtwH": {
        "ByxVe3vnOS": {
            "0": "This paper presents a GNN-based method for predicting type annotations in JavaScript and TypeScript code.",
            "1": "By constructing a \"type dependency graph\" that encodes the relationships among variables and appropriately modifying GNNs to the hypergraph setting, the authors show that the good performance improvements can be made.",
            "2": "Overall, this paper is well-written, the experiments are quite convincing and the methods are reasonable and interesting.",
            "3": "I believe that there are a few things that need to be clarified within the evaluation, but I would argue that this paper should be accepted.",
            "4": "* It is unclear to me what is the scope of the type dependency graph construction.",
            "5": "Is it a whole file?",
            "6": "Is it a single function/class?",
            "7": "A whole project?",
            "8": "* The DeepTyper paper seems to suggest that it predicts type annotations one function at a time.",
            "9": "Does the comparison (in 5.1) use the same granularity as LambdaNet?",
            "10": "If for example, LambdaNet looks at whole files, then the comparison is not exact.",
            "11": "Could you please clarify?",
            "12": "Performing the comparison on equal-sized samples would make sense.",
            "13": "* If my reading of DeepTyper is correct, it processes all identifiers as a single unit.",
            "14": "In contrast, this work (correctly, in my opinion), breaks the identifiers into \"word tokens\".",
            "15": "This may be an important difference between the two methods.",
            "16": "To test this, an ablation where LambdaNet does *not* split the identifiers, would provide a better comparison among the sequential representation of DeepTyper and the type constraint graph of LambdaNet.",
            "17": "* Some comparison with JSNice is missing (Raychev et al.",
            "18": "2015).",
            "19": "The DeepTyper paper suggests that the two approaches are somewhat complementary.",
            "20": "It would be useful to know how LambdaNet compares to JSNice too.",
            "21": "* There is some literature that suggests that code duplication exists in automatically scraped corpora and that it hurts the evaluation of machine learning models [a,b].",
            "22": "At the very least, the authors should report the percent of duplicates (if any) in their corpus.",
            "23": "Another option would be to _not_ evaluate predictions in any duplicated file.",
            "24": "## Secondary question:\n\n* I do not understand why a separate edge is needed for Subtype() and Assign().",
            "25": "Isn't Assign (a,b) == Subype(a,b)?",
            "26": "* The authors correctly exclude the `any` annotations produced by the TypeScript compiler.",
            "27": "Do they also exclude any other annotations?",
            "28": "For example, functions that do not return a value (i.e.",
            "29": "their return value is `void`) would also need to be excluded.",
            "30": "What about `object`?",
            "31": "* I would encourage the authors to make the dataset (source code and extracted graphs), and the LambdaNet code public upon acceptance.",
            "32": "## Minor\n\n* Please capitalize GitHub, TypeScript, etc throughout the paper?",
            "33": "* Sec 4: \"we first to compute\" -> \"we first compute\"\n\n\n\n[a] Lopes, Cristina V., et al.",
            "34": "\"DéjàVu: a map of code duplicates on GitHub.\"",
            "35": "Proceedings of the ACM on Programming Languages 1.OOPSLA (2017): 84.",
            "36": "[b] Allamanis, Miltiadis.",
            "37": "\"The Adverse Effects of Code Duplication in Machine Learning Models of Code.\"",
            "38": "arXiv preprint arXiv:1812.06469 (2018)."
        },
        "BJgF7g7jFr": {
            "0": "= Summary\nA method to predict likely type of program variables in TypeScript is presented.",
            "1": "It consists of a translation of a program's type constraints and defined objects into a (hyper)graph, and a specialised neural message passing architecture to learn from the generated graphs.",
            "2": "Experiments show that the method substantially outperforms sound typing in the TypeScript compiler, as well as a recent method based on deep neural networks.",
            "3": "= Strong/Weak Points\n+ The graph representation of the problem is novel, and draws both on core ideas from Hindley-Milner typing (in the subtyping/assignment graph bits) as well as neural ideas (in name similiarity)\n+ The neural message passing architecture is adapted to the problem, handling features not present in the standard GNN literature (hyperedges, ...)\n+ Experiments compare with relevant baselines and consider interesting ablations, studying the effect of the GNN extensions in detail.",
            "4": "- The hyperparameter selection regime (and the experiments used to find them) is not described\n\n= Recommendation\nThis is an application-driven paper with nice practical results.",
            "5": "The fact that standard neural architectures are extended and adapted to the task, and the way domain knowledge is used to design the graph representation makes this interesting even to people outside the task-specific audience, and hence I strongly recommend acceptance.",
            "6": "= Minor Comments\n- page 2: \"network's type to be class\" -> \"to be a class\"\n- Evaluation Datasets: Did you take duplication in the crawled datasets into account?",
            "7": "(Lopes et al.",
            "8": "2017 (DéjàVu: a map of code duplicates on GitHub) suggests that this is particularly problematic for JavaScript/TypeScript)"
        },
        "Sklm0w_6tr": {
            "0": "This paper proposed to use Graph Neural Networks (GNN) to do type inference for dynamically typed languages.",
            "1": "The key technique is to construct a type dependency graph and infer the type on top of it.",
            "2": "The type dependency graph contains edges specifying hard constraints derived from the static analysis, as well as soft relationships specified by humans.",
            "3": "Experiments on type predictions for TypeScript have shown better performance than the previous methods, with or without user specified types.",
            "4": "Overall this paper tackles a nice application of GNN, which is the type prediction problem that utilizes structural information of the code.",
            "5": "Also the proposed type dependency graph seems interesting to me.",
            "6": "Also the pointer mechanism used for predicting user specified types is a good strategy that advances the previous method.",
            "7": "However, I have several concerns below:\n\nAbout formulation:\n1) I’m not sure if the predicted types for individual variable would be very helpful in general.",
            "8": "Since the work only cares about individual predictions while no global consistency is enforced, it is somewhat limited.",
            "9": "For example, in order to (partially) compile a program, does it require all the variable types to be correct in that part?",
            "10": "If so, then the predicted types here might not be that helpful.",
            "11": "I’m not sure about this, so any discussion would be appreciated.",
            "12": "About type dependency graph:\n1) Comparing to previous work (e.g, Allamanis et.al, ICLR 18), it seems the construction of the task specific graph is the major contribution, where the novelty is a bit limited.",
            "13": "2) The construction of the dependency graph is heuristic.",
            "14": "For example, why the three contextual constraints are good?",
            "15": "Would there be other good ones?",
            "16": "Also why only include such limited set of logical constraints.",
            "17": "For example, would expression like (x + y) induce some interesting relationships?",
            "18": "Because such hand-crafted graph is lossy (unlike raw source code), all the questions here lead to the concern of such design choices.",
            "19": "3) The usage of graph is somewhat straightforward to me.",
            "20": "For example, although the hard-constraints are there, there’s no such constraints reflected in the prediction.",
            "21": "Adding the constraints on the predictions would be more interesting.",
            "22": "About experiments:\n1) I think one ablation study I’m most interested in is to simply run GNN on the AST (or simply use Allamanis et.al’s method).",
            "23": "This is to verify and support the usage of proposed type dependency graph.",
            "24": "2) As the authors claimed in Introduction, ‘plenty of training data is available’.",
            "25": "However in experiment only 300 projects are involved.",
            "26": "Also it seems that these are not fully annotated, and the ‘forward type inference functionality from TypeScript’ is required to obtain labels.",
            "27": "It would be good to explain such discrepancy.",
            "28": "3) Continue with 2), as the experiment results shown in Table 2, TS compiler performs poorly.",
            "29": "So how would it be possible to train with poor annotations, while generalize much better?",
            "30": "Some explanations would be helpful here.",
            "31": "4) I think only predicting non-polymorphic types is another limitation.",
            "32": "Would it be possible to predict structured types?",
            "33": "like nested list, or function types with arguments?"
        }
    },
    "ryenvpEKDr": {
        "rkeru_QAYH": {
            "0": "This work proposes a functional form for the relationship between <dataset size, model size> and generalization error, and performs an empirical study to validate it.",
            "1": "First, it states 5 criteria that such a functional form must take, and proposes one such functional form containing 6 free coefficients that satisfy all these criteria.",
            "2": "It then performs a rigorous empirical study consisting of 6 image datasets and 3 text datasets, each with 2 distinct architectures defined at several model scales, and trained with different dataset sizes.",
            "3": "This process produces 42-49 data points for each <dataset, architecture> pair, and the 6 coefficients of the proposed functional form are fit to those data points, with < 2% mean deviation in accuracy.",
            "4": "It then studies how this functional form performs at extrapolation, and finds that it still performs pretty well, with ~4.5% mean deviation in accuracy, but with additional caveats.",
            "5": "Decision: Accept.",
            "6": "This paper states 5 necessary criteria for any functional form for generalization error predictor that jointly considers dataset size and model size, then empirically verifies it with multiple datasets and architectures.",
            "7": "These criteria are well justified, and can be used by others to narrow down the search for functions that approximate the generalization error of NNs without access to the true data distribution, which is a significant contribution.",
            "8": "The empirical study is carefully done (e.g., taking care to subsample the dataset in a way that preserves the class distribution).",
            "9": "I also liked that the paper is candid about its own limitations.",
            "10": "A weakness that one might perceive is that the coefficients of the proposed functional form still needs to be fit to 40-ish trained NNs for every dataset and training hyperparameters, but I do not think this should be held against this work, because a generalization error predictor (let alone its functional form) that works for multiple datasets and architecture without training is difficult, and the paper does include several proposals for how this can still be used in practice.",
            "11": "(Caveat: the use of the envelope function described in equation 5 (page 6) is not something I am familiar with, but seems reasonable.)",
            "12": "Issues to address:\n- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.",
            "13": "Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.",
            "14": "The extrapolation section did provide evidence that there probably isn't /that/ much overfitting, but cross validation would directly address this concern.",
            "15": "- In addition, the paper provides the standard deviation for the mean deviations over 100 fits of the function as the measure of its uncertainty, but I suspect that the optimizer converging to different coefficients at different runs isn't the main source of uncertainty.",
            "16": "A bigger source of uncertainty is likely due to there being a limited amount of data to fit the coefficients to.",
            "17": "Taking the standard deviation over the deviations measured on different folds of the data would be better measure of uncertainty.",
            "18": "Minor issues:\n- Page 8: \"differntiable methods for NAS.\"",
            "19": "differentiable is misspelled."
        },
        "BJxakKXRFS": {
            "0": "Summary:\nThis paper proposes a functional form to model the dependence of generalization error on a held-out test set on model and dataset size.",
            "1": "The functional form is derived based on empirical observations of the generalizing error for various model and dataset sizes (sections O1, O2, and O3) and on certain necessary criteria (C1, C4 and C5).",
            "2": "The parameters of the function are then fit using linear regression on observed data.",
            "3": "The authors show that the regressed function \\(\\epsilon(m,n)\\) is able to predict the generalization error for various \\(m\\) and \\(n\\) reasonably accurately.",
            "4": "Major Points:\n- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \\(\\epsilon(m,n)\\).",
            "5": "I would ideally like to see results on more optimizers, at the very least for Adam, even if for fixed hyper-parameters.",
            "6": "As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.",
            "7": "If the form still holds true then the results from this work can be more reliably used for small-scale network development and in making trade-off choices (as discussed in section 8).",
            "8": "- Given the current form of the paper, the abstract and introduction should be modified to reflect the fact that only limited architectures and optimizers were experimented with, and the claims of the paper are not experimentally validated in general.",
            "9": "Minor Points:\n \n- It would be nice if more network architectures were analysed (such as VGG and DenseNets).",
            "10": "- It would be nice if different stopping criteria were analysed.",
            "11": "- It would greatly benefit the reader if eq.",
            "12": "5 were expanded.",
            "13": "Overall, I think this is a well written paper and provides good insight into the behaviour of the error landscape as a function of model and dataset size.",
            "14": "The paper’s primary drawback is the restrictive setting under which the experiments are performed.",
            "15": "Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).",
            "16": "I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.",
            "17": "Rebuttal Response\nI would like to thank the authors for their response.",
            "18": "The results of additional experiments as described in Section 6.2 and in Figure 5 do indeed provide stronger evidence of the power-law form of the error function.",
            "19": "In light of this, I have changed my original rating."
        },
        "HJl_1dRl5r": {
            "0": "This paper explores the relation among the generalization error of neural networks and the model and data scales empirically.",
            "1": "The topic is interesting, while I was expecting to learn more from the paper, instead of some well-known conclusions.",
            "2": "If the paper could provide some guidance for model and data selection, that would be an interesting paper for the ICLR audience.",
            "3": "For instance, how deep should a model be for a classification or regression task?",
            "4": "What is the minimum/maximum layers of a deep model?",
            "5": "How much data is sufficient for a model to learn?",
            "6": "What is the minimum/maximum size of the data set?",
            "7": "Do we really need a large data set or just a subset that covers the data distribution?",
            "8": "What's the relation between the size of a model and that of a data set?",
            "9": "By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?",
            "10": "How about the gain of the task performance?"
        }
    },
    "Bkg5LgrYwS": {
        "BJg1cbsTtB": {
            "0": "This work uses imitations learning (from synthetic data) to train a deep model which takes a natural language instruction, and a visual representation of a robot's environment, and outputs a trajectory for the robot to follow which executes this instruction.",
            "1": "The work focuses on a robotic pick-and-place task, where the instruction indicates which of the available bins an item should be placed in.",
            "2": "In addition to the trajectory model, a second model is trained which allows the agent to predict whether a given command is actually feasible (i.e.",
            "3": "whether the target bin exists).",
            "4": "Empirical results show a reasonably high success rate in placing objects in the bin specified by the instruction, though there is still room for improvement in cases where the shape o a combination of features is important to the selection of the correct bin.",
            "5": "Rather than mapping directly from instructions and observations to control signals, the model trained in this work translates from an instruction, and an image of the agent's environment, to the parameters of a DMP controller.",
            "6": "The network therefore outputs the entire motion for the task in a single inference pass.",
            "7": "This approach would have advantages and disadvantages.",
            "8": "The DMP formulation ensures that the resulting trajectory is relatively smooth.",
            "9": "It also means that the network outputs a distinct goal configuration, which the DMP should reach (assuming the goal is feasible) regardless of the other motion parameters.",
            "10": "The use of a DMP output space, however, limits the model to generating relatively simple, goal-directed motions, and does not allow the agent to adapt to changes in the layout of the environment (which would only be observed in the static visual input).",
            "11": "As other work has considered visual instruction following (e.g.",
            "12": "Misra et.",
            "13": "al.",
            "14": "\"Mapping Instructions and Visual Observations to Actions with Reinforcement Learning\") it would strengthen this work considerably to see a direct comparison between this method and existing approaches.",
            "15": "It is likely that the approach presented in this work is better suited to the specific problem of robot control, but it would be helpful to see if learning a low-level control policy directly can be successful in this context.",
            "16": "The work needs to expand on the discussion in the second paragraph of section 4, where human annotators were used to generate natural language instructions for different tasks.",
            "17": "The paper suggests that this data was not used directly to train the model, but was instead used to build a template for generating natural language instructions.",
            "18": "What this template looks like, and how it was constructed based on the human-generated data, remains unclear, and needs to be described in much more detail."
        },
        "BylouRApKH": {
            "0": "*Summary \n\nThe paper describes a new end-to-end imitation learning method combining language, vision, and motion.",
            "1": "A neural network architecture called Multimodal Policy Network is proposed.",
            "2": "That can extract internal representations from language and vision to condition the generated motions.",
            "3": "It enables an end-user to influence a robot's policy through verbal communication.",
            "4": "The experiments demonstrate the generalization performance of the method.",
            "5": "That can generate behaviors towards different goals depending on different sentences.",
            "6": "*Decision and supporting arguments\n\nI think the paper is just below the borderline.",
            "7": "The reason is as follows.",
            "8": "The concern is about evaluation.",
            "9": "They demonstrated the method could work, and the robot can move to appropriate goals.",
            "10": "However, there is no comparative methods in the experiment.",
            "11": "Related to this point, the problem was not identified in the Introduction.",
            "12": "The authors might assume that introducing language into behavioral cloning itself is qualitatively new work.",
            "13": "However, such a study has a long history.",
            "14": "For example, please refer to Tani's pioneering works.",
            "15": "Sugita, Yuuya, and Jun Tani.",
            "16": "\"Learning semantic combinatoriality from the interaction between linguistic and behavioral processes.\"",
            "17": "Adaptive behavior 13.1 (2005): 33-52.",
            "18": "The author should specify a current challenge or problem in pre-existing studies about imitation learning with language input, clarify their claim, and give empirical support for the claim."
        },
        "HJecYd1RFS": {
            "0": "The paper addresses the problem of using multiple modalities for learning from demonstration.",
            "1": "Approaches that take in task or joint space data to learn a policy for replicating that task are numerous.",
            "2": "Doing the same with multiple modalities involved, in particular vision, language and motion, has only been recently considered, so this is a timely paper.",
            "3": "The core contribution is pretty well summarised by the architecture in figure 1, which involves a combination of encodings of the words and sentences, images and parameters of a DMP in order to generate movement commands from a high level instruction.",
            "4": "Unless I have missed something in the experimental setup, all of the considered task variations are movement commands of the form <Move> to <Object>.",
            "5": "The network setup allows for synonyms of two kinds, so <Move> can be replaced by numerous verbal synonyms such as advance and go, and the object can be specified in terms of shapes, colors and so on, but otherwise this is the only specification of the task.",
            "6": "This has been addressed in the recent literature using neural network architectures similar to the one being proposed here, e.g., see the following papers.",
            "7": "These papers already solve the proposed problem and provide similar explanations.",
            "8": "It would be helpful to see comparative discussion with respect to those methods and a clear statement of novelty with respect to such prior work:\n[R1] M. Burke, S. Penkov, S. Ramamoorthy, From explanation to synthesis: Compositional program induction for learning from demonstration, Robotics: Science and Systems (R:SS), 2019.",
            "9": "[R2] Y. Hristov, D. Angelov, A.Lascarides, M. Burke, S. Ramamoorthy, Disentangled Relational Representations for Explaining and Learning from Demonstration, Conference on Robot Learning (CoRL), 2019.",
            "10": "An interesting feature in R2 that the authors do not explicitly address here is the issue of relational specifications in the language, e.g., in addition to saying \"move to the red bowl\", we may also wish to say \"place on top of red block\".",
            "11": "In the way that MPN is currently set up to map from the language input directly to hyperparameters of the DMP, and considering the embedding structure, it is not clear if MPN is capable of handling such specifications.",
            "12": "If so, the claim of generalisation on the language input should be stated more clearly.",
            "13": "The ablation study is setup somewhat differently than what I would have expected.",
            "14": "The authors consider the effect of changing the training set size and if the language input includes synonyms or not.",
            "15": "Those two aspects seem to produce the expected results.",
            "16": "It would also be interesting to see an ablation study in the sense of replacing or removing aspects of the architecture to see its relative effect on the overall model performance.",
            "17": "So, for instance, if one did not have a DMP with the hyperparameters being estimated by a network and instead had a more straightforward encoding of where to move to - does it make a difference and how much?",
            "18": "Likewise, how much performance benefit, if any, is being derived from an uninterpreted image I being combined as described in the embedding as opposed to an alternative that detects an objects and combines that position differently.",
            "19": "The paper would have been stronger if such architectural choices were better justified and also demonstrated in the experiments."
        }
    },
    "B1x6w0EtwH": {
        "rJxq_Sx0FB": {
            "0": "This paper proposes a knowledge graph advantage actor critic (KG-A2C) model to allow an agent to do reinforcement learning in the interactive fiction game.",
            "1": "Under the general framework of A2C, the core contribution of the paper is to apply a graph attention network on the knowledge graph to help learn better representation of the game state and reduce the action space.",
            "2": "Experiments on Zork1 game environment are done to verify the effectiveness of the proposed method.",
            "3": "Overall, this paper presents a novel contribution to reinforcement learning with augmented memory/world-state.",
            "4": "However, I do have a few concerns regarding the baselines and other details.",
            "5": "Given these clarifications and or comparisons in an author response, I would be willing to increase the score.",
            "6": "Pros:\n\n1, I like the idea of constructing the knowledge graph as the agent roll out.",
            "7": "I think it is a better way to construct a structural representation of the world rather than assuming the agent gonna learn everything via single hidden state vector.",
            "8": "It also permits more explainable policy in the future.",
            "9": "Authors do make good progress along this line.",
            "10": "2, The paper is well written and the design of the proposed new model seems technically reasonable.",
            "11": "Cons & Questions:\n\n1, The main concern I had is regarding to the baseline.",
            "12": "I think it is more convincing to have a baseline which leverages the same entity extraction and template-action space.",
            "13": "In particular, it should have the same model architecture except that it uses maybe a LSTM to decode the action rather than a GAT applied on knowledge graph.",
            "14": "Note that the baseline I am referring to is different from the LSTM-A2C baseline reported in the paper as: (1) with entity extraction, although you may not get a graph mask, but you can still have a object-mask which also reduces the action space; (2) it is not clear to me that LSTM-A2C uses the same template-action mechanism as the KG-A2C, e.g., the valid action construction procedure described in section 4.1.",
            "15": "Without such a baseline, it is hard to fully judge how helpful the knowledge graph is.",
            "16": "2, How do you test the generalization of the proposed models?",
            "17": "In particular, do you use different maps during training and testing?",
            "18": "If the model is merely trained on one map as shown in figure 5, it may just memorizes it in the knowledge graph and overfit to this map.",
            "19": "3, The details of the interaction fiction problem setup are sparse.",
            "20": "It would be very helpful to explain what exactly the observations are in the example of Figure 2.",
            "21": "For example, what are the game description, game feedback are in this case?",
            "22": "4, In the caption of figure 1, “Solid lines represent gradient flow” is misleading.",
            "23": "If I understood correctly, solid lines refer to the computation flow which has gradient back-propagated in the backward pass.",
            "24": "5, Could you explain why KG-A2C converge slower than DRRN in figure 3?",
            "25": "6, Do you think having a fully differentiable mechanism of building knowledge graph would help or not?",
            "26": "Why?",
            "27": "======================================================================================================\n\nMost of my concerns are addressed by the authors' response.",
            "28": "I increased my score."
        },
        "BygA6oW15H": {
            "0": "This paper tackles the problem of developing agents to solve interactive fiction (IF) games.",
            "1": "The authors propose an agent that builds a dynamic knowledge graph of each state from the textual observation provided by the games, while choosing actions from a template-based action space.",
            "2": "While both these directions have been explored in prior work as pointed out, this paper combines them effectively to produce an agent that outperfoms existing methods on a benchmark of different IF games.",
            "3": "Pros:\n1.",
            "4": "Writing is clear, method is easy to understand and design choices are clearly specified.",
            "5": "2.",
            "6": "Empirical results are good and presented on real IF games.",
            "7": "Cons:\n1.",
            "8": "(minor) While the authors test their method on a suite of human-made IF games, it would also be great to have a study on synthetic cases like the Microsoft TextWorld environments, if only to see which aspects of the method are crucial to making the jump from synthetic to real IF games.",
            "9": "Other comments:\n1.",
            "10": "It looks like the knowledge graph is constructed for every state separately.",
            "11": "The DRRN on the other hand incorporates more of the state history.",
            "12": "From Figure 3(b), both DRRN and KG-A2C seem to do well - have you analyzed whether these methods are complementary?",
            "13": "In general, it would be nice to have some more analysis on all the models across the different games rather than just Zork1."
        },
        "r1ga-eIL5H": {
            "0": "This paper considers the problem of interactive fiction games in which an agent interacts with the world purely through natural language.",
            "1": "The problem is challenging because one has to map natural language as observations to appropriate representations to partially infer the state space of the world, and actions are also defined in vast space of natural language sentences.",
            "2": "One of the main contribution in the paper is to represent the partial observations of the world as a knowledge graph, and so as to efficiently infer appropriate actions.",
            "3": "The paper is very well written, especially the introduction section, demonstrating novelty in the context of fictional games literature, and showing good empirical results.",
            "4": "However, I don't have any background in fictional games but dialog modeling.",
            "5": "So it is hard for me to fairly assess how novel this work is.",
            "6": "Ideas are simple and incremental, even if i rely upon literature overview provided by the authors in the related work section.",
            "7": "Though, it can be effective.",
            "8": "The problem is challenging, and yet narrows down to fictional games.",
            "9": "The proposed solution doesn't seem generic to be applied in other NLP or ML problems.",
            "10": "Authors argue that action space is super large even if generating sentences of length upto 5.",
            "11": "Even though true, I think, this argument doesn't hold in the context of recent progress in NLP for problems like dialog modeling, where the action space of generating responses is even larger.",
            "12": "I suggest the authors to relate their work to dialog models, as some of the ideas can be borrowed from there to simplify the solution for the considered problem setting."
        }
    },
    "ByxODxHYwB": {
        "SkgKHY_aKS": {
            "0": "On the basis of existing topic modelling approaches, the authors apply a transfer learning approach to incorporate additional knowledge to topic models, using both word embeddings and topic models.",
            "1": "The underlying idea is that topic models contain a global view that differs on a thematic level, while word embeddings contain a local, immediate contextual view.",
            "2": "The combination of both local and global view transfer to enhance a topic model is the main contribution of this paper, especially when using multiple sources (therefore the title: multi-source multi-view transfer).",
            "3": "Given a document collection, DocNADE is used to generate the topic-word matrix.",
            "4": "In the local view transfer step, the pre-trained WordPool is used, from which knowledge is transferred on the target document.",
            "5": "The global view transfer is done by transferring knowledge from the pre-trained TopicPool to the target.",
            "6": "As described in Algorithm 1 in the paper, both Word- and TopicPool are jointly used in the transfer learning process.",
            "7": "For evaluation, three different measures are taken into account: Perplexity, Topic Coherence and Precision (Information Retrieval).",
            "8": "In comparison to a DocNADE only approach, all values are better in the settings that use the transfer learning approach.",
            "9": "Compared to DocNADE + word embeddings, the results are competitive as well.",
            "10": "In both experiments, the multi-source setting evaluates best overall.",
            "11": "In conclusion, the paper shows that exploiting multiple sources and views in transfer learning leads to an overall improvement in the given tasks.",
            "12": "The main contribution is the usage topic models in a transfer learning framework.",
            "13": "Additionally the use of multi-source word embeddings is novel too, especially in the joint setting with the topic model transfer.",
            "14": "The paper shows how the DocNADE approach is enhanced to make use of both local and global view transfer and how this enhancement leads to improved performance on various related tasks.",
            "15": "Still, the overall contribution is mostly in combining existing methods and can be judged as rather incremental.",
            "16": "Minor note: A small mistake has been found in Table 5.",
            "17": "The best perplexity value in the first column is not the bold 638, but the 630 in the local-view transfer setting.",
            "18": "Edit after rebuttal: In my review I did not value the contribution of the transfer learning approach enough.",
            "19": "So, when also considering the extensive evaluation I am now leaning towards accept."
        },
        "HyeGN5C85B": {
            "0": "The paper proposes a multi-source and multi-view transfer learning for neural topic modelling with the pre-trained topic and word embedding.",
            "1": "The method is based on NEURAL AUTOREGRESSIVE TOPIC MODELs --- DocNADE (Larochelle&Lauly,2012).",
            "2": "DocNADE learns topics using language modelling framework.",
            "3": "DocNADEe (Gupta et al., 2019) extended DocNADE by incorporating word embeddings, the approach the authors described as a single source extension of the existing method.",
            "4": "In this paper, the proposed method adds a regularizer term to the DocNADE loss function to minimize the overall loss whereas keeping the existing single-source extension.",
            "5": "The authors claimed that incorporating the regularizer will facilitate learning the (latent) topic features in the trainable parameters simultaneously and inherit relevant topical features from each of the source domains and generate meaningful representations for the target domain.",
            "6": "The analysis and evaluation were presented to show the effectiveness of the proposed method.",
            "7": "However, the results are not significantly improved than the based line model DocNADE.",
            "8": "Overall, the paper is written well.",
            "9": "However, it is not clear to me that the improved results are resulted due to multi-source multi-view transfer learning or for the better leaning of the single-source model due to the incorporation of the regularizer."
        },
        "SygTrGoa9r": {
            "0": "This is an emergency review.",
            "1": "This work proposes a novel method to use pre-trained topic embeddings and pre-trained word embeddings obtained from various corpora in the transfer learning framework.",
            "2": "Their model architecture is based on DocNADE, unsupervised neural-network based topic model, and the authors propose two strategies to use pre-trained topic embeddings and pre-trained word vectors.",
            "3": "1) Addition of a weighted sum of pre-trained word embeddings and the hidden vector of DocNADE.",
            "4": "2) L2-Regularization term between topic embedding of DocNADE and pre-trained topic embeddings.",
            "5": "They propose to align these two embeddings by multiplying align matrix \"A\" to the topic embedding of DocNADE.",
            "6": "They show the transfer learning performance of their model on various source/target domain datasets, including medical target corpora, and verify that their model outperforms on a short text and small document collection.",
            "7": "Strengths.",
            "8": "1.",
            "9": "Comparison with the data augmentation baseline shows the performance gain is not only from bigger training data.",
            "10": "Even though comparison with the naive baseline (data augmentation) seems too obvious, I think the results clearly show their claim about the importance of using transfer learning in neural topic modeling domain.",
            "11": "2.",
            "12": "As the first approach that introduces a novel transfer learning framework with pre-trained topic embeddings, they show tons of experimental results with various datasets and metrics to show the specification of their method.",
            "13": "Their experimental setting is well designed.",
            "14": "Weaknesses and comments:\nTheir method to combine pre-trained word embeddings and pre-trained topic embeddings is too simple.",
            "15": "Since this is the first approach to use topic embedding in the transfer learning field, the simplicity of the proposed method is somewhat necessary.",
            "16": "However, a weighted sum of pre-trained topic/word vectors seems not enough to transfer multisource knowledge.",
            "17": "For instance, word vectors obtained from individual training processes do not share embedding vector space.",
            "18": "As you apply the alignment method to topic embeddings from various sources, you should align word embeddings too."
        }
    },
    "Syxwsp4KDB": {
        "B1xJ3ZoaFS": {
            "0": "POS-DISCUSSION\nI thank the authors for their answer.",
            "1": "I updated my score assuming ryxAY34YwB does not exist, and would encourage authors to discuss in more details the relationship with MeanSum if this gets accepted\n\nPRE-DISCUSSION\n\nThis is an important contribution for the field of unsupervised summarization.",
            "2": "\"Unsupervised *\" is trendy in NLP so this is a timely contribution.",
            "3": "Furthermore, doing this for summarization is important because of the cost of getting gold summaries and the model used in translation is harder (impossible?)",
            "4": "to adapt to this setting where there is information loss in one direction.",
            "5": "However, I find major drawbacks in the current state of this paper.",
            "6": "They are best related to the three contributions the author claim:\n - Contribution3: the use of BPE.",
            "7": "\"BPE for X\", with X being an NLP task can hardly count as a contribution today.",
            "8": "If we are counting who did it first, then this is taken at least by Liu & Lapata 2019 through their use of BERT\n - Contribution1: leveraging the lead bias for pre-training.",
            "9": "This is a great idea!",
            "10": "However, this seems to be covered by an accompanying paper (ICLR submission ryxAY34YwB) which is not referenced.",
            "11": "Because of common paragraphs and experimental setting I am assuming there is an overlap of the author sets in two papers.",
            "12": "PLEASE CORRECT IF THIS IS NOT THE CASE.",
            "13": "As you don't get to claim the same contribution twice, this contribution should go all to the benefit of the other paper.",
            "14": "- Contribution2: the use of combining reconstruction loss and theme loss for summarization is another great idea.",
            "15": "However, the paper that introduced this for summarization (as far as I know) is not cited nor compared too (MeanSum: https://arxiv.org/abs/1810.05739).",
            "16": "This seems like a major issue considering the similarity in the approach (including the use of the straight-through Gumbel softmax estimator).",
            "17": "Other comments:\n\n - Being a growing topic of study, I appreciated in particular the care taken to report a number of other approaches.",
            "18": "Could you please clarify which version of ROUGE was used in each case?",
            "19": "There are significant differences in the different implementations being used.",
            "20": "- Please also specify the version of ROUGE you used.",
            "21": "- Your numbers in Table 2 do not coincide with Table 3 of ryxAY34YwB (eg: LEAD-3 for CNN/DM).",
            "22": "Can you explain?",
            "23": "- Your ablation study (Sect 4.1) focuses on CNN/DM (NOTE: the caption of Table 4 says NYT, but the number correspond to CNN/DM.",
            "24": "I guess this is an error), where the topic & reconstruction loss indeed helps.",
            "25": "However this is not the case for NYT, where LEAD-3 actually beats any of your approach.",
            "26": "This is not mention nor discussed.",
            "27": "- The example of Fig 4 reveals a major problem.",
            "28": "The summary states an incorrect fact: the gov accountability had indeed released a report earlier that week; but this was NOT a few hours before the reported incident.",
            "29": "What happened a few hours before was a report on Fox News.",
            "30": "In a summary: a good idea combining ideas of ryxAY34YwB and adapting MeanSum.",
            "31": "However, this is in my opinion not enough material for a full paper."
        },
        "Hke_3SSAYr": {
            "0": "Paper's Claims\n\nThe paper introduces a new unsupervised abstractive summarization approach called TED, using a Transformer encoder and decoder.",
            "1": "Their main contributions are as follows:\n1) Pretraining the encoder and decoder on news articles using the first beginning as the target summary.",
            "2": "2) Fine-tune on other datasets using so-called theme modeling, and separately a denoising loss.",
            "3": "3) TED's performance is claimed to significantly improve over GPT-2 while not being too far from the best unsupervised extractive summarization results.",
            "4": "Decision\n\nEdit: After revisions and discussions, I recommend we accept this paper.",
            "5": "I am leaning towards accepting this paper mostly because of the contribution #1 above.",
            "6": "Unsupervised learning using large quantities of text that have the property of being typically written in a style that synthesizes information in the first 1-3 sentences is a powerful idea.",
            "7": "That the performance is improved compared to other unsupervised abstractive summarization confirms the importance of this approach.",
            "8": "However the importance of and justification for the fine-tuning steps are comparatively much more limited in my opinion.",
            "9": "Also, some important details about the preprocessing for pre-training appear to be missing and they could be quite important.",
            "10": "Detailed arguments for decision\n\nI view this effort as aiming to reproduce the BERT approach in the context of abstractive summarization, which is a good idea.",
            "11": "The most clever contribution is in leveraging un-labeled text using the first few sentences as the target summary for pretraining.",
            "12": "The results of just this part are already beating previous approaches, while not requiring any in-domain data, which is quite powerful.",
            "13": "However, some relatively important details regarding the methodology are omitted or only glossed over and it would greatly contribute to making this work more reproducible if the details were included (see my detailed notes below, notably regarding section 2.2).",
            "14": "On the fine-tuning steps, I have several worries.",
            "15": "First, why not fine-tune using supervised learning, as would be the analog to the BERT approach?",
            "16": "Instead the authors go out of their way to do in-domain unsupervised learning, which provides a boost, yes, but still doesn't compare positively to extractive and/or supervised methods.",
            "17": "Second, why not perform the theme modeling and denoising also -- or rather only -- on the unlabelled pretraining data?",
            "18": "Why should it be done on the in-domain fine-tuning data instead (while not using the most valuable piece of in-domain information, namely the example summaries)?",
            "19": "After all, it's a fully unsupervised approach and it can actually be performed on any text at all, whether a summary for it exists or not.",
            "20": "Again regarding the unsupervised approach, and to push the BERT analogy further, I'm wondering why not initialize the pretraining model with a BERT-style trained model?",
            "21": "After all we could imagine building a system that adds more and more in-domain characteristics sequentially: first pretrain a BERT model, then fine-tune to summarization using what this paper calls pretraining, and then finally fine-tune again to a specific summarization domain.",
            "22": "So, to conclude, I find that this paper goes in the right direction and introduces important ideas for pretraining and fine tuning unsupervised abstractive summarization models, but that some decisions about how to use the various ideas (theme and denoising but no supervised learning, in-domain vs during pretraining) have not been explored enough.",
            "23": "Extra notes\n\npage 2, second line: pretrainleverages (typo)\nsection 2.1: fix first sentence to make it an actual sentence.",
            "24": "section 2.2: \"we obtain three years of online new articles ... via a search engine\" please be more specific about your methodology.",
            "25": "section 2.2: You should double check more throughly that there is no data leakage in test.",
            "26": "There could be articles about the same exact events, years apart, for example.",
            "27": "I doubt that this would be a big effect, but there are easily ways to find highly similar articles between the pretraining data and test data to make sure.",
            "28": "section 2.2: \"Next we conduct following data cleaning\" fix (typo?).",
            "29": "Also that sentence probably belongs to the next paragraph.",
            "30": "section 2.2: Why did you pick the values that you did for the preprocessing heuristics (such as between 10-150 words, 150-1200 words, 3 sentences and not 2 or 1 or 4, the ratio 0.65, etc.)?",
            "31": "Were other values tried?",
            "32": "section 2.2: You mention you end up with 21.4M articles.",
            "33": "How many were there to start with?",
            "34": "What's the filtering ratio?",
            "35": "section 2.2: You mention that you pick the model with the best ROUGE-L score on the validation set.",
            "36": "How many models were there?",
            "37": "What was different between them?",
            "38": "section 2.2, OOV Problem: the information in this whole subsection would fit better in 2.1 where 'tokens' are left generic without specifying which type of token you're considering.",
            "39": "Figure 1: I find the upper part of this figure very confusing.",
            "40": "Why are there arrows going from the encoder/decoder to a summary, to theme loss, to article and back to encoder/decoder?",
            "41": "It's important that the summary is never seen by the theme loss otherwise it's not unsupervised anymore, and I also don't see why the arrow would go through article *after* theme loss.",
            "42": "I assume there must have been a mistake, please fix.",
            "43": "section 2.4: \"the sequence is slightly shuffled by applying a permutation /sigma such that ...\" The formula given here tells me that all token indices are shuffled with another token within a window k. That seems like a lot of moving around, and also depending on the implementation a token from the beginning could possibly end up at the very tail of the sentence by being picked iteratively again and again, thus falling outside the permutation distance k. Please provide more details on how this is done and a justification for why it was decided to do it this way.",
            "44": "Section 3.1: I'd like to know how long (preferably number of words, or at least number of wordpiece tokens) the summaries generated are.",
            "45": "What determines how long they are, is it a fixed size, or the model decides to stop on his own (or when hitting some limit), or something else?",
            "46": "section 4.2: Do you have any idea why your unsupervised approach yields more novel n-grams than a the supervised model you compare against?",
            "47": "This can be good as much as it can be bad, in that it could be going off-track.",
            "48": "Yes humans have high novelty, but high novelty in itself isn't necessarily good.",
            "49": "I don't find the argument that have more novel ngrams is intrinsically, necessarily good, compelling.",
            "50": "If I'm wrong, then it would be nice to have better explanation in the paper."
        },
        "rJxaA5iecS": {
            "0": "The authors propose to improve abstractive summarization models by using pretrained embeddings, theme modeling and denoising.",
            "1": "They propose a very interesting idea: to leverage the lead bias in news article to build supervized summarization task from 21.4 M of articles.",
            "2": "Details are given how to produce this supervized data using simple heuristics.",
            "3": "The  model is  train with a denoising loss, by introducing 2 types of noise (tokens from other article and sequence shuffle).",
            "4": "Theme modeling is also introduced as a classification problem  (same as BERT) :  the system must learn to classify pairs of sentences from the same article and pairs from different articles.",
            "5": "Experiments are conducted on 3 datasets.",
            "6": "The proposed model outperforms the other unsupervized abstractive models and provides results closed to unsupervized extractive models, with a metrics which favors extractive models.",
            "7": "Ablation study shows that pretraining yields most of the impact, whereas improvements due to theme modeling and denoising loss are marginal.",
            "8": "In the Article example : \n\"in the wold\"  ?",
            "9": "Conclusion : \n- dataset-agnostic : I don't see why since the approach take advantage of the lead bias.",
            "10": "- \"outperforms previous systems by significant margins\" : excessive."
        }
    },
    "BygSXCNFDB": {
        "BJldEiamKH": {
            "0": "This paper applies the Go-Explore algorithm to the domain of text-based games and shows significant performance gains on Textworld's Coin Collector and Cooking sets of games.",
            "1": "Additionally, the authors evaluate 3 different paradigms for training agents on (1) single games, (2) jointly on multiple games, and (3) training on a train set of games and testing on a held-out set of games.",
            "2": "Results show that Go-Explore's policies outperform prior methods including DRRN and LSTM-DQN.",
            "3": "In addition to better asymptotic performance Go-Explore is also more efficient in terms of the number of environment interactions needed to reach a good policy.",
            "4": "Broadly I like how this paper shows the potency of Go-Explore applied to deterministic environments such as the CoinCollector/CookingWorld  games.",
            "5": "It is an algorithm that should not be ignored by the text-based game playing community.",
            "6": "I also like the fact that this paper clearly explains and demonstrates how efficient and effective Go-Explore can be, particularly when generalizing to unseen games.",
            "7": "The major drawback of the paper is a lack of novelty - the Go-Explore algorithm is already well known, and this paper seems to be a direct application of Go-Explore to text-based games.",
            "8": "While the results are both impressive and relevant for the text-game-playing community - it's my feeling that this work may not be of general interest to the broader ICLR community due to the lack of new insights in deep learning / representation discovery.",
            "9": "However, I am open to being convinced otherwise.",
            "10": "Minor Comments:\n\nThe textworld cooking competition produced at least one highly performing agent (designed by Pedro Lima).",
            "11": "While I'm not sure if the code or agent scores are available, it would be a relevant comparison to see how well Go-Explore compared to this agent.",
            "12": "(See https://www.microsoft.com/en-us/research/blog/first-textworld-problems-the-competition-using-text-based-games-to-advance-capabilities-of-ai-agents/)"
        },
        "Byxbair19H": {
            "0": "\nThis paper considers the task of training an agent to play text-based computer games.",
            "1": "One of the key challenges is the high-dimensional action space in these games, which poses a problem for many current methods.",
            "2": "The authors propose to learn an LSTM-based decoder to output the action $a_t$ by greedily prediction one word at a time.",
            "3": "They achieve this by training a sequence to sequence model on trajectories collected by running the game using a previously proposed exploration method (Go-Explore).",
            "4": "While the results are promising, there might be limited novelty beyond training a sequence to sequence model on pre-collected trajectories.",
            "5": "Further, the experiments are missing key elements in terms of proper comparison to baselines.",
            "6": "Pros:\n1.",
            "7": "Nice idea for tackling the unbounded action space problem in text-based games.",
            "8": "Cons:\n1.",
            "9": "The method depends on the assumption that we can get a set of trajectories with high rewards.",
            "10": "This seems a pretty strong assumption.",
            "11": "In fact, the authors use a smaller set of admissible actions in order to collect these trajectories in the first place - this seems to not be in line with the goal of solving the large action space problem.",
            "12": "If we assume access to this admissible action function, why not just use it directly?",
            "13": "2.",
            "14": "Some of the empirical results may not be fair comparisons (unless I'm missing something).",
            "15": "For example, all the baselines for the CookingWorld games use $\\epsilon$-greedy exploration.",
            "16": "Since the Go-Explore method assumes access to extra trajectories at the start, this doesn't seem fair to the other baselines which may not observe the same high-reward trajectories.",
            "17": "Other comments:\n1.",
            "18": "Do you use the game rewards to train/finetune the seq2seq model or is it only trained in a supervised fashion on the trajectories?",
            "19": "(like an imitation learning setup)\n2.",
            "20": "How critical is the first step of producing high reward trajectories to the overall performance?",
            "21": "Some more analysis or discusssion on this would be helpful to disentangle the contribution of GoExplore from the seq2seq action decoder."
        },
        "ByljVLmfcS": {
            "0": "This paper proposes an exploration approach of Go-Explore together with imitation learning for playing text games.",
            "1": "It is shown to outperform existing solutions in solving text-based games with better sample efficiency and stronger generalization ability to unseen games.",
            "2": "Pros:\nSeq2seq imitation learning + Go-Explore is applied to more challenging text games and achieves better performance, higher sample complexity and better generalization ability.",
            "3": "Cons: \n•\tFrom modeling perspective, the policy network uses the standard sequence-to-sequence network with attention.",
            "4": "And it is trained on the high-reward trajectories obtained with Go-Explore method using imitation learning.",
            "5": "From this perspective, there is not much novelty in this paper.",
            "6": "Detailed comments:\n•\tMore details about the mapping function f(x) in Phase 1 should be given.",
            "7": "•\tIt is not clear why Phase 2 should be called “Robustification”.",
            "8": "It seems to be just standard imitation learning of seq2seq model on the high-reward trajectories collected in Phase 1.",
            "9": "•\tIn the paragraph after eqn.",
            "10": "(1), H is defined to be the hidden states of the decoder.",
            "11": "Shouldn’t it be the hidden states of the encoder?",
            "12": "•\tIt seems to be unfair to compare the proposed method with advanced exploration strategy to other model-free baselines that only have very simple exploration strategies (e.g., epsilon-greedy).",
            "13": "It is not surprising at all that Go-Explore should outperform them on sparse reward problems.",
            "14": "More baselines with better exploration strategies should be compared."
        }
    },
    "rygGQyrFvH": {
        "rJeVL4b6KS": {
            "0": "Contributions:\n\nThis paper studies an important problem, i.e., how to find a good decoding strategy for open-ended text generation.",
            "1": "To this end, the authors provide a deep analysis of the most common decoding methods, and propose Nucleus Sampling, a very simple yet effective method to generate higher-quality text.",
            "2": "Compared with top-k sampling, the key idea behind the proposed method is to sample from the dynamic nucleus of tokens containing the majority of the probability mass.",
            "3": "Experiments demonstrate that nucleus sampling is an effective decoding strategy in practice.",
            "4": "Strengths:\n\n(1) Writing & Clarity: The proposed method is well motivated, the paper is carefully written, and clearly presented.",
            "5": "I enjoyed reading the paper.",
            "6": "(2) Experiments: The experiments are also carefully designed.",
            "7": "Both quantitative and human evaluation are provided.",
            "8": "Quality examples are also shown.",
            "9": "Weaknesses:\n\n(1) Novelty: The biggest concern that I have is its technical novelty.",
            "10": "The proposed method is effective, but it acts more like a useful trick.",
            "11": "Also, no theoretical justification is provided, but only some intuitions.",
            "12": "So, I would say the novelty is indeed limited.",
            "13": "However, given the comprehensive evaluation, and high writing quality, I lean to accept this paper due to its empirical contribution.",
            "14": "It seems that this nucleus sampling method can be applied in a wide range of text generation applications.",
            "15": "** Minor **\nTypo: In the line below Eqn.",
            "16": "(2), \"x \\in V^{(k)}\" => \"x \\in V^{(p)}\", same typo in Eqn.",
            "17": "(3)."
        },
        "Skxc3_RpFB": {
            "0": "This paper is motivated by an observation that maximization-based decoding approaches such as beam search can lead to incoherent and repetitive sentences when open-ended long-form text generation based on neural language model such as GPT-2 is performed.",
            "1": "To solve the problem, this paper proposes a sampling method called Nucleus Sampling.",
            "2": "Similar to Top-k sampling, Nucleus Sampling truncates the probability distribution of the words in the vocabulary.",
            "3": "Instead of re-normalizing the probabilities for the top-k words, Nucleus Sampling re-normalizes the original probabilities for the words with values above a pre-chosen threshold p. Some quantitative and qualitative results show that the proposed sampling method can generate long-form texts with some nice properties.",
            "4": "Pros:\n\nThe problem addressed in this paper is highly interesting, and the proposed method is simple and intuitive.",
            "5": "The paper is well motivated and the method is clearly presented.",
            "6": "Extensive quantitative and qualitative experiments are conducted to compare different sampling methods.",
            "7": "Cons:\n\n1) Although the raised problem in this paper is interesting, the proposed Nucleus Sampling seems to be a trivial variant of Top-k sampling.",
            "8": "With a reasonably large k suitable for different practical problems in question, it is unclear that Nucleus Sampling produces significant advantages over commonly used Top-k sampling.",
            "9": "2) The argued difficulty in choosing k in Top-k sampling is not that different from that of choosing the threshold p in Nucleus Sampling.",
            "10": "3) In section 4.3, the argument that natural language rarely remains in a high-probability zone is questionable.",
            "11": "This happens only because our current neural language models are not well-specified for generating long texts and modeling long-range contexts.",
            "12": "4) In section 6.2, the qualitative comparison between Nucleus Sampling and Top-k sampling might be caused by randomness.",
            "13": "With a large k, there is no technical barrier that prevents Top-k sampling from generating the sentences produced by Nucleus Sampling.",
            "14": "5) A recent stochastic beam search method based on Gumbel-max-k (Kool, Hoof, and Welling, ICML 2019) should be discussed and compared.",
            "15": "In summary, although the studied problem in this paper is highly interesting, the proposed Nucleus Sampling is not technically significant compared to Top-k sampling."
        },
        "S1lugevycS": {
            "0": "In the domain of language models, the paper introduces a new heuristic sampling method called top-p sampling, or nucleus sampling (NS).",
            "1": "It is a variant of top-k sampling where the smallest k is selected to ensure the combined likelihood is no less than p. The paper centers on claiming and showing that the generated samples are of higher quality and more diverse than common alternatives such as beam search, pure sampling, top-k sampling, and low-temperature sampling.",
            "2": "While overall I think the proposed method is sound as an alternative to other heuristics such as beam search, I have reservations on the presentation and arguments made in the paper.",
            "3": "Pros:\n1.",
            "4": "NS is sound as a heuristic sampling method.",
            "5": "2.",
            "6": "The paper contains many interesting experimental observations and I speculate that some of them will find future uses.",
            "7": "For example, the selection of parameter values (not just for NS, also for top-k) and the nontrivial perplexity of generated text.",
            "8": "Cons:\n1.",
            "9": "The ultimate performance measure (open-ended generation) of “high quality” and “diversity” is very vague.",
            "10": "It seems that the authors end up doing is to evaluate by high self-BLEU, HUSE, few repetitions, and perplexity.",
            "11": "Furthermore, it is unclear why one _should_ train with cross-entropy (trying to match the distributions) and then rely on the sampling procedure to fulfill these desiderata (See also Min1 and Min2).",
            "12": "2.",
            "13": "The comparison with beam search (BS) is not well motivated.",
            "14": "BS is devised to find the maximal sentence and it is not stochastic.",
            "15": "It seems out of place in the context of generating a “diverse” set of samples.",
            "16": "3.",
            "17": "The arguments in the comparison with pure sampling is vague and sometimes misplaced.",
            "18": "The key argument seems to hinge on the idea that the low likelihood tail is of “low confidence.” But this claim is problematic.",
            "19": "If the estimate is wrong on the low probability tail, then so is the estimate on p(head) = 1-p(tail) by virtue of p being a probability measure.",
            "20": "4.",
            "21": "The arguments in the comparison with top-k is vague and sometimes misplaced.",
            "22": "The main argument against top-k is the “[d]ifficulty in choosing a suitable value of k” but the same can be said for choosing p. After all, top-k and top-p (NS) can be thought of as a variant of each other (by dynamically choosing k or p respectively).",
            "23": "Moreover, in Figure 5, a selection for k value is suggested.",
            "24": "I agree that this value might not _appear_ as intuitive as p, and maybe other works have chosen a smaller k than they should have, but similarly, people might intuitively choose too high a value for p (Figure 5).",
            "25": "Possible mistakes/typos:\n1.",
            "26": "(2), “>=“ -> ≥.",
            "27": "2.",
            "28": "Figure 7, the human self-BLEU4 < human self-BLEU5 and that seems wrong, especially when all other bars show the opposite ordering.",
            "29": "3.",
            "30": "In References, “Angela Fan, Mike Lewis, and Yann Dauphin.",
            "31": "Hierarchical neural story generation.",
            "32": "In ACL, 2018a” is duplicated.",
            "33": "4.",
            "34": "In References, the citation of “Unifying human and statistical evaluation for natural language generation” is from NAACL 2019, not “2018.”\n5.",
            "35": "In References, the first names are shown as initials in “Sparse forward-backward using minimum divergence beams for fast training of conditional random fields.”\n\nQuestions:\n1.",
            "36": "In Table 1, how is Human perplexity estimated?",
            "37": "Minor issues:\n1.",
            "38": "Partly due to what the authors position NS to solve, i.e.",
            "39": "open-ended generation, the core arguments is not as precise or rigorous as it could have been in my opinion.",
            "40": "I feel that focusing on comparing NS to other heuristics as a heuristic might make the text appeal to a wider audience and the discussion more precise.",
            "41": "2.",
            "42": "The distinction drawn between open-ended generation and directed generation is unpersuasive to me.",
            "43": "In the context of language modeling, the former is to approximate a distribution (over an extended alphabet) whereas the latter is to approximate a conditional distribution (given the input).",
            "44": "However, the most common formulation to solve the former is to decompose the distribution into a product of conditional distributions (1).",
            "45": "3.",
            "46": "The caption in Figure 1 draws a misleading comparison.",
            "47": "The “admirable” generation (presumably referring to the OpenAI blog post) was from the full GPT-2 model, not the initially released GPT-2-117M.",
            "48": "Please point out my misunderstanding directly.",
            "49": "I am open to acknowledging them and revising my assessment."
        }
    },
    "rkg-mA4FDr": {
        "SkekhUVKKH": {
            "0": "This paper studies a query-related document retrieval problem using a framework which they call “two-tower retrieval method”.",
            "1": "The task is to learn query representation and document representation in order to retrieve query-related documents by the maximum inner product.",
            "2": "This is a realistic setting for large-scale retrieval problem since it enables document representations to be computed once regardless of the question, and obtaining query-sensitive document representations is very expensive.",
            "3": "Then, the paper studies three different pretraining methods for this task, ICT (previously proposed by Lee et al 2019), and BFS & WLP (proposed by this paper).",
            "4": "For evaluation, the paper considers the retrieval task of question answering, based on SQuAD and Natural Questions.",
            "5": "The combination of ICT, BFS and WLP achieves remarkable improvement over the number of baselines including BM25 and other neural-based models.",
            "6": "The strength of this paper is that it includes comprehensive studies on the two-tower retrieval problem.",
            "7": "In particular, they have conducted extensive ablation studies with different train/test ratios.",
            "8": "However, there are some notable weaknesses of this paper as follows.",
            "9": "First, the benchmark relies on the recall rate instead of the end task (open-domain QA).",
            "10": "Recall rate is not a good way to evaluate the retrieval result since a system may retrieve text which contains the answer text but is not semantically related to the question.",
            "11": "(I understand that the paper follows Admad et al (2019), but I believe this is not a published paper.)",
            "12": "In addition, this paper did not empirically demonstrate the relatedness between the recall rate and the end performance.",
            "13": "This makes it very hard to compare with other papers in open-domain QA, which has been extensively studied for a few recent years.",
            "14": "Second, despite comprehensive studies, the fact that ICT+BFS+WLP is almost the same as ICT (93.91 vs. 94.37) means that the method does not give improvement over ICT which was already proposed in the previous study.",
            "15": "Third, the gap between BM25 and ICT+BFS+WLP in Table 3 and 4 are very significant (e.g.",
            "16": "27 vs 94 on Natural Questions), but this doesn't seem to be consistent to Lee et al (2019).",
            "17": "(There are some differences: (1) Lee et al (2019) compares BM25 vs. ICT, but according to this paper, ICT and ICT+BFS+WLP are similar.",
            "18": "(2) Lee et al (2019) reports the end QA performance while this paper reports the recall rate, but one of the assumptions in this paper is that recall rate and the end performance is related.)",
            "19": "What is the explanation for this discrepancy?",
            "20": "(I am happy to increase the rating if my concerns are resolved during rebuttals and/or the paper includes performance on the end QA performance.)",
            "21": "Some questions:\n1) Section 4.1 says ICT is sentence-level, BFS is paragraph-level and WLP is document-level.",
            "22": "What does it mean?",
            "23": "I thought, according to Section 3, all methods are paragraph-level.",
            "24": "2) Section 4.1: it looks like Ahmad et al (2019)’s setting is actually not entirely open-domain.",
            "25": "Their candidate sentences/paragraphs are much less than the entire Wikipedia.",
            "26": "Did this paper also use the same set of the candidate?",
            "27": "In that case, it should be clearly mentioned in the paper.",
            "28": "In addition, the data statistics are different across two papers.",
            "29": "Did Ahmad et al (2019) include only train set whereas this submission reports train+test?",
            "30": "In case there is an official split of train/test, why were different splits used for evaluation?",
            "31": "3) Also regarding the split: for each split, how much was used for development?",
            "32": "I believe data used for the development and test should be different.",
            "33": "In fact, rather than experimenting on different ratios of train/test, is it possible to report on official test set, while splitting the train set into 90/10 for training and development?",
            "34": "Or, split the entire data to 90/5/5 for training/development/test?",
            "35": "Update on Nov 15: The revised paper resolves most of my concerns, so I am updating the score from 3 to 6."
        },
        "HJlvYglaKS": {
            "0": "The paper provides a comprehensive study on the two-tower Transformer models in terms of the impact of its pre-training tasks on large-scale retrieval applications.",
            "1": "The studies here show that, pre-training with Inverse Cloze Task (ICT) the two-tower Transformer models significantly outperform the widely used BM-25 algorithm for large-scale information retrieval.",
            "2": "The authors also propose two novel pre-training settings which also show improvement over the baseline BM-25.",
            "3": "In addition, the authors empirically demonstrate that the token-level masked-LM model used by BERT is not a good choice as pre-training task for the two-tower Transformer when deployed for large-scale information retrieval applications.",
            "4": "The paper is well written and easy to follow.",
            "5": "The Ablation Study of the paper also provides useful insights about the impact of different pre-training schemas on large-scale information retrieval tasks.",
            "6": "I think the studies here will benefit the communities where large-scale information retrieval is required such as open-domain question answering.",
            "7": "The main limitation to me is that, the two novel pre-training tasks proposed in this paper are specific for Wikipedia and they are less effective than the ICT strategy (as shown in Table 5).",
            "8": "I hope the authors will release the source codes to the community."
        },
        "S1eXkuHEcS": {
            "0": "This paper proposes a solution to the large scale query-document retrieval problem.",
            "1": "The proposed method was shown to be a better alternative to the classic information retrieval approach such as BM-25 (token marching + TF-IDF weights).",
            "2": "The proposed method is based on two separate transformer models which has computational benefit over one cross-attention model.",
            "3": "For fast training, they have also used the sampled softmax.",
            "4": "For pre-training tasks, Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki LinkPrediction (WLP) were studied.",
            "5": "The paper is written well, easy to follow and well-motivated.",
            "6": "However, there is a major technical problem in the proposed method.",
            "7": "In the proposed approach, the query embedding (q_emb) and the document embedding (d_emb) train separately by two transformer models (two towers --- Query-tower and Doc-tower).",
            "8": "After that, the similarity was measured through a dot product.",
            "9": "Two embedding models are, therefore, represented by separate vector space representation.",
            "10": "Applying dot product to find the similarity does not make much sense to me, as the embedding is not comparable in two different vector representations."
        }
    },
    "rJxGLlBtwH": {
        "SkeTV81RKr": {
            "0": "This paper explores the effect of ordering supervised learning and self-play on the resultant language learnt between agents.",
            "1": "The topic is of high relevance to the ICLR community and makes several interesting insights useful to anyone learning control of a multi-agent system where communication amongst agents is applicable.",
            "2": "I have several suggestions for improvements below, but all I believe are feasible to make within the time period of the rebuttal with the most necessary being:\n\n1) The naming of methods in Section 5 is not consistent with those introduced in section 3.3.",
            "3": "For example, in the first paragraph ec2supervised is presumably sp2sup and sched is presumably sup2sp?",
            "4": "Similarly, on page 7 (S2P and Pop-S2P) and Figure 4.",
            "5": "Please revise and ensure consistency throughout.",
            "6": "2) Figures 4b, 6 and 7 only present single values.",
            "7": "Are these average values from repeated runs?",
            "8": "If so please quantify variance.",
            "9": "3) The conclusion in Section 7 at the bottom of page 8 that \"S2P performs much worse than the other options\" is contrary to previous results.",
            "10": "Can the authors please comment on what features of the environment caused this difference?",
            "11": "4) Appendix A includes details of hyperparameters, but some details remain unclear.",
            "12": "Specifically, hyperparameter ranges swept over are shown but how were they then chosen from?",
            "13": "Are they optimised for each environment and algorithm?",
            "14": "What does the bold text in the table represent?",
            "15": "If it is chosen values, why do only some parameters have chosen values?",
            "16": "These are important details to enable reproduction of the paper.",
            "17": "Minor Comments:\nIn Section 3.3, if all these methods are \"well known ways to combine self-play and supervised learning\" can all be supported by an (or preferably multiple) exemplar publications that used these method previously.",
            "18": "Directly linking each to the previous work will further clarify the contribution this specific paper makes and help readers new to the area gain insight across the multiple papers this work builds upon.",
            "19": "Figure 3b, colour is representative of performance.",
            "20": "Is this mean accumulated reward?",
            "21": "Please clarify to increase how informative this visualisation is, as currently it is unclear if yellow or blue is the desired value.",
            "22": "Page 5, small typo \"introduced in the Lee et al.",
            "23": "(2017)\" should be \"introduced in Lee et al.",
            "24": "(2017)\".",
            "25": "Figure 4b, the legend is blocking two bars and their corresponding value.",
            "26": "It looks like moving to the bottom right may help, or placing above the plot.",
            "27": "Figure 4 caption refers to a subfigure (d) that is not included.",
            "28": "On Page 6, the reference to babbling equilibrium should include a citation for interested readers to learn more about this well established concept.",
            "29": "On Page 7 there is a reference to Figure r4b, is this intended to be a reference to Figure 4b right?",
            "30": "Figure 6 appears after Figure 7.",
            "31": "Maintaining ordered numbering would be preferable.",
            "32": "On Page 9 it is noted some experimental results are in the Appendix but as there is a page and a half of space remaining before the 10 page limit, I would encourage to include all results in the main body of the paper.",
            "33": "Multiple references do not list a publication venue (e.g.",
            "34": "Evtimova et al., Lazaridou et al.",
            "35": "2018, Tieleman et al.",
            "36": "2018) or cite Arxiv versions when the work has been later published (e.g.",
            "37": "Jacques et al.",
            "38": "2018 was published at ICML 2018).",
            "39": "Figure 9 caption should state the environment."
        },
        "HJgVjdDRFB": {
            "0": "\nSummary\n---\n\n(motivation)\nTo develop language speaking agents we can teach them to mimic human language\nor to solve tasks that require communication.",
            "1": "The latter is efficient, but\nthe former enables interpretability.",
            "2": "Thus we combine the two in an attempt\nto take advantage of both advantages.",
            "3": "This paper studies a variety of ways to\ncombine these approaches to inform future work that needs to make this tradeoff.",
            "4": "(approach)\nThe trade-off is studied using reference games between a speaker and a\nlistener.",
            "5": "Goal oriented _self-play_ and human _supervision_ are considered two contraints one\ncan put on a network during learning.",
            "6": "This work considers algorithms that vary\nwhen self-play and supervision are used (e.g., training with self-play then supervision,\nor supervision then self-play, or alternating back and forth between the two).",
            "7": "Additional variations freeze the speaker or distill an ensemble of agents into one agent.",
            "8": "(experiments)\nA synthetic Object Reference game (OR) and a Image-Base Reference game (IBR) with real images are used for evaluation.",
            "9": "Performance is accuracy at image/object guessing.",
            "10": "1.",
            "11": "(OR) Like previous work, this work finds that emergent languages are imperfect at supporting their goals and cannot be understood by agents that only understand a human language like English.",
            "12": "2.",
            "13": "(OR) Pre-training with supervision then fine-tuning with self-play is superior to pre-training with self-play then fine-tuning with supervision.",
            "14": "This is presented as surprising from the perspective of language emergence literature, which is though of as pre-training with self-play.",
            "15": "3.",
            "16": "(IBR) Distilling an agent from an ensemble of 50 independently trained agents outperforms training single agents from scratch, but is still not as good as the whole ensemble.",
            "17": "Self-play vs supervision schedules:\n4.",
            "18": "(IBR) Supervision (using image captions) followed by self-play performs much worse than all other approaches.",
            "19": "5.",
            "20": "(IBR) Alternating between supervision and self play (e.g., randomly choosing supervision or self-play every iteration) performs best.",
            "21": "Strengths\n---\n\nThe curricula considered by this paper seem to have a sigificant impact on performance.",
            "22": "These are new and could be important for future work on language learning, which may have considered the sup2sp setting from figure 7a without considering the sched setting.",
            "23": "The diversity of experiments provided and the analysis help the reader get a better sense for how emergent communication models work.",
            "24": "It's nice to see experiments on both a toy setting and a setting with realistic images.",
            "25": "Future directions suggested throughout the paper are interesting.",
            "26": "Weaknesses\n---\n\n\n* The 3rd point of section 5 is presented as a major conclusion of this paper, but it is not very surprising and I don't see how it's very useful.",
            "27": "The perspective of language emergence literature is presented a bit strangely.",
            "28": "The self-play to supervision baseline seems to be presented as an approach from the language emergence literature.",
            "29": "I don't think this is what any of that literature promotes exactly, though it is close.",
            "30": "Generally, I (and likely others) don't think it's too surprising that trying to fine-tune a self-play model with language supervision data doesn't work very well, for the same reasons cited in this paper (point 3 of section 5).",
            "31": "I think the general strategy when trying to gain practical benefits from self-play pre-training is a translation approach where the learned language is translated into a known language like English rather than trying to directly align it to English as does the supervision approach in this paper.",
            "32": "This particular baseline would be more useful if the paper considered learning some kind of translation layer on top of the self-play pre-trained model.",
            "33": "* How significant are the performance differences in figure 7a, especially those between the frozen and non-frozen models?",
            "34": "Is the frozen model really better or this performance difference just due to noise?",
            "35": "* I'm somewhat skeptical that these trends will generalize to other tasks/models.",
            "36": "The main goal of this paper is to inform future work.",
            "37": "That makes it even more important than normal that the trends identified here are likely to generalize well.",
            "38": "Are these trends likely to generalize well?",
            "39": "Does the paper address when these trends are expected to hold anywhere?",
            "40": "Minor Presentation Weaknesses:\n\n* Figure 4: I think the sub-figures are mis-labeled in the caption.",
            "41": "* In the related work I'm not sure the concept of generations is right.",
            "42": "I think it should refer to different languages of different agents across time rather than different languages of the same agent across time.",
            "43": "Missing details / clarification questions:\n\n* What exactly does Figure 4c compare?",
            "44": "Are both methods distilled from ensembles or is the blue line normal S2P while the other is distilled from an ensemble of compositional languages?",
            "45": "It's not clear since point (3) in section 5 refers to the S2P result (not Pop-S2P) in that plot.",
            "46": "I'm also assuming that PB-S2P means the same thing as Pop-S2P, but that's not made clear anywhere.",
            "47": "Does PB stand for Population Based?",
            "48": "* In the rand setting how is convergence defined?",
            "49": "Do both objectives need to converge or just one?",
            "50": "* In the sched_rand_frz setting what is r?",
            "51": "* In the IBR how are the distractor images picked?",
            "52": "Suggestions:\n\n* Can't both self-play and supervision be used at the same time (just use a weighted combination of the two objectives)?",
            "53": "I don't think the paper ever did this but it seems like a very useful variation to consider.",
            "54": "Preliminary Evaluation\n---\n\nClarity: The writing is fairly clear, though some details are lacking.",
            "55": "Significance: This work could help inspire some future models in the language emergence literature.",
            "56": "Quality: Experiments are aligned with the paper's goals and support its conclusions.",
            "57": "Originality: The distillation approach and curricula are novel.",
            "58": "Overall the work could prove to be an interesting and useful reference point inside the language emergence literature so I recommend it for acceptance."
        },
        "BJxoajDCtS": {
            "0": "This paper investigated how two conflicting learning objectives; supervised and self-play updates could be combined with a focus on visual-grounded language tasks.",
            "1": "With a different set of their combinations, the authors empirically found that alternating two learning updates may result in the best equilibrium state; consistency with samples in the supervised dataset and optimal state with high rewards in the task environment.",
            "2": "The paper is very well-written, and I really enjoyed reading it overall.",
            "3": "There are some typos, presentation issues, and minor format issues (e.g., wrong naming) though.",
            "4": "I do like this kind of simple but insightful result with enough empirical observations and discussions.",
            "5": "Even though there is not that novel method proposed, the overall message found from the experiments, their interpretation by the authors, and meaningful comparisons to the past works in emergent communication are fair enough to learn high scientific values from it.",
            "6": "The design of the experiment is again very simple (e.g., changing the size of data, switching two setups in different ways) but clear to understand.",
            "7": "This work is a good example of how well-designed hypotheses and their empirical validation could contribute to the field.",
            "8": "I also appreciate the large spectrum of literature surveys including from the recent advances (Lewis et al., 2017, Lee et al., 17) to the past literature in emergent communications such as Littman (1994) and (Farrell & Rabin, 1996).",
            "9": "One of my concerns is the lack of applications, especially on the tasks using more natural language.",
            "10": "The two tasks; OR and IBR, seem to be very limited settings to evaluate how self-play operates with data supervision.",
            "11": "As pointed out by the authors, supervision from the training data itself may include most of the unexplored cases of the task, leading a less chance to learn policies from the high rewards.",
            "12": "I think more realistic tasks using natural language need to be considered: negotiation (e.g., Lewi’s task, “Decoupling strategy and generation in negotiation dialogues”), recommendation (e.g., “Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue”), and more.",
            "13": "I agree with the point made by the authors that this work mainly focuses on investigation rather than exploitation.",
            "14": "But, then it would be adding another emergent task where the self-play can learn many more policies than one in the supervised dataset.",
            "15": "Adding to the point, I was expecting to see non-task related metrics to measure the effectiveness of their appropriate combinations.",
            "16": "For example, it would be better to add language-side metrics (e.g., perplexity, fluency, consistency) to measure how language degeneration varies by the different combinations.",
            "17": "This issue is not addressed in the paper, and I guess this is mainly because of the limited usage of language in the two limited tasks.",
            "18": "If the paper is only focusing on emergent language which is related to specific tasks, it would be better to tone-down a little bit and state the major difference of it with natural language.",
            "19": "The population-based S2P seems to be a bit incremental and unrelated to the main theme of the paper.",
            "20": "To me, the motivation of adding POP into S2P based on the policy variability is somewhat different from the original claim about the combination of supervised and selfplay.",
            "21": "Also, the improvements on IBR in Figure 7 are incremental, making the major claim of this work little divergent.",
            "22": "In terms of presentation, if you like to show how performance changes over the different sizes of data, it would be better to show it by graphs over different variations instead of the bar charts only with 10k and 50k sizes.",
            "23": "In addition, the figures and captions need to be improved for better interpretation.",
            "24": "I think they are written in a hurry or changed a lot in the last minutes.",
            "25": "Please see some minor formatting issues below.",
            "26": "Minor comments:\nDuplicate reference of (Lewis et al., 2017)\nSome names defined in Section 3.3 and Section 5 are not exactly matched.",
            "27": "Figures and fonts in Figures 4 and 7 are a little difficult to understand.",
            "28": "Especially, I can’t understand the two upper figures in Figure 4a\nCaptions in Figure 4 are not matched with the sub-figures.",
            "29": "Figure r4b -> Figure 4b"
        }
    },
    "rkgqm0VKwB": {
        "HJxKkZIRtB": {
            "0": "The paper presents an end-to-end methods for jointly training named entity recognition (NER) and relation extraction (RE).",
            "1": "The model leverage pre-trained BERT language models, making it very fast to train.",
            "2": "The methods is evaluated on 5 standard NER+RE datasets with good performances.",
            "3": "Pros:\n\n- the paper is well written and very clear\n- the proposed model has two main advantages: (1) it is very fast to train due to the use of pre-trained BERT representations and (2) it does not depends on any external NLP tool (such as dependency parser)\n\nCons: \n\n- I think the main source of improvement comes from the BERT representations used as input.",
            "4": "As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.",
            "5": "- Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train.",
            "6": "This is not really surprising..."
        },
        "BJeklIo0tr": {
            "0": "The paper proposes a new joint learning algorithm that works for two tasks, NER and RE.",
            "1": "The model is based on a pre-trained BERT model, which provides the word vectors of the input word sequence.",
            "2": "Then it solves two tasks with two network branches: the first branch minimizes the loss for NER, and the second branch minimizes the loss for RE.",
            "3": "The second branch uses entity labels predicted by the first branch, so joint learning may benefit both tasks.",
            "4": "The design of the architecture is novel, but it is also not groundbreaking.",
            "5": "Each network branch is from known structures, but the combination is not proposed before.",
            "6": "The submission has evaluated the proposed algorithms on four datasets and improved SOTA performances.",
            "7": "The ablation study justifies the design details.",
            "8": "The writing is generally clear.",
            "9": "Now critics: \n\nAblation study: \n1.",
            "10": "As pointed by one public comment, the ablation study should show how much improvement is from BERT vectors.",
            "11": "2.",
            "12": "I'd like to see another ablation study of whether RE helps NER.",
            "13": "If you remove the RE component, does the NER performance suffer?",
            "14": "Writing: \n3. how are predicted labels embedded?",
            "15": "Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?"
        },
        "SJlvvnJkcB": {
            "0": "The paper proposes an end-to-end joint model for named entity recognition (NER) and relation extraction (RE), using pre-trained language models.",
            "1": "The model is very simple, with the key is to use BERT and take NER output as input to RE.",
            "2": "The experimental results show the model, without the need for handcrafted features, get state-of-the-art results on five datasets.",
            "3": "Although the paper is well written and shows good results, I would reject the paper because: \n- the idea is trivial and simple.",
            "4": "I don't think there's significant novelty here: all the components are existing and combining them seems very trivial to me.",
            "5": "- the good performance seems to be from BERT rather than the model's structure (table 2 suggests that).",
            "6": "I thus think the contribution of the paper is pretty not significant.",
            "7": "I think the paper does not fit this conference.",
            "8": "It is better to be presented in a Demonstration section at a *ACL conference."
        }
    },
    "H1ggKyrYwB": {
        "B1l7jf5rFS": {
            "0": "The authors propose a framework to incorporate additional semantic prior knowledge into the traditional training of deep learning models such that the additional knowledge acts as both soft and hard constraints to regularize the embedding space instead of the parameter space.",
            "1": "To illustrate the idea, the authors use 3 different annotated knowledge that are already available in a public dataset that contains equivalent statements, entailed statements as well as functional programs and show that the final performance indeed increases.",
            "2": "In general, the paper is well-written and easy to follow.",
            "3": "The motivation is clear, i.e., to boost the performance of supervised learning tasks with additional knowledge constraints in a hard way.",
            "4": "Compared with the existing models that treat the constraints as soft regularizers, the authors propose to additionally distill the knowledge using teacher-student framework.",
            "5": "And this paper contributes in a novel way to incorporate the constraints with both soft and hard training strategies.",
            "6": "However, there are several considerations which limits the contribution of this paper:\n\n1.",
            "7": "As a teach-student distillation framework, there are several papers using a posterior regularizer with hard constraints, e.g., \"Harnessing deep neural networks with logic rules\", \"Constrained Convolutional Neural Networks for Weakly Supervised Segmentation\".",
            "8": "More discussions and comparisons with these models should be addressed, and even experimental comparisons if possible, since they also use knowledge distillation to convey the knowledge expressed in the constraints.",
            "9": "2.",
            "10": "The proposed model differs with other soft-regularization-based methods in terms of an additional distillation process.",
            "11": "The authors state that the combination of task loss with soft regularization lead to over-fitting.",
            "12": "To my point of view, the distillation step actually makes similar effect with the case when only optimize the regularizer without the task loss.",
            "13": "Hence, I am wondering what's the performance of first using the combined loss and then fix the subsequent layers to only optimize the embedding layers using only the regularization loss.",
            "14": "This could demonstrate the difference between the distillation process and the regularization process.",
            "15": "3.",
            "16": "Many recent models for VQA have been proposed, e.g, \"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences from Natural Supervision\" which also combines extra knowledge as symbolic reasoning.",
            "17": "The authors should also compare with such models.",
            "18": "4.",
            "19": "It seems the model need to sample a pair of data each time at training to compute the regularizer and also conducting the distillation process.",
            "20": "In this case, the time cost should be non-trivial because the distillation process requires optimizing the distance between the current embedding with the hard constraint.",
            "21": "Then the question comes as how's the time complexity of the model?",
            "22": "What's the convergence speed?"
        },
        "SyxXGwWjKB": {
            "0": "The paper argues for encoding external knowledge in the (linguistic) embedding layer of a multimodal neural network, as a set of hard constraints.",
            "1": "The domain that the method is applied to is VQA, with various relations on the questions translated into hard constraints on the embedding space.",
            "2": "A technique which involves distillation is used to satisfy those constraints during learning.",
            "3": "The question of how to encode external knowledge in neural networks is a crucial one, and the limitations of end-to-end learning with supervised data is well-made.",
            "4": "Overall I feel that this is a potentially interesting paper, addressing an important question in a novel way, but I found the current version a highly-frustrating read (and I read the paper carefully a number of times); in fact, so frustrating that it is hard for me to recommend acceptance in its current form.",
            "5": "More detailed comments below.",
            "6": "Major comments\n--\nThe main problem I have with the paper lies with the first part of section 3, which is a key section describing the main method by which the constraints are satisfied during learning.",
            "7": "This is very confusing.",
            "8": "The need for the two-step procedure, in particular, and the importance of distillation needs much more explanation, and not relegated to the Appendix (which reviewers are not required to read - see call for papers).",
            "9": "I'm not suggesting that the whole of the appendix needs moving to the body of the paper, but I would suggest perhaps 1/2 a page.",
            "10": "A related comment is the use of the distillation technique.",
            "11": "This looks crucial, but I don't believe distillation is mentioned at all until the end of the related work section, and even there it comes as a bit of a surprise since there's no mention anywhere of this technique in the introduction.",
            "12": "I would say a little more about the distinction between the embedding space and parameter space, since you say that the external knowledge is encoded in the former and not the latter, and this is important to the overall method.",
            "13": "Since embeddings are typically learned (or at least fine-tuned) it's not clear where the boundary is here.",
            "14": "Another comment is that embedding space in this paper means the linguistic embedding space.",
            "15": "Since this is ICLR and not, eg, ACL, I would make clear what you mean by embedding space.",
            "16": "I don't understand the diagram in Fig.",
            "17": "3 of the architecture, nor the explanation.",
            "18": "What's an operation here?",
            "19": "Is it *, or *6?",
            "20": "I don't get why 3 is embedded by itself in the diagram, and then combined with the remainder using the MLP.",
            "21": "Why not just run the RNN over the sequence?",
            "22": "Why are the training instances {3,+1...} and {4,*2,...} equivalent.",
            "23": "I stared at this a while, and still have no idea.",
            "24": "Also, how are these \"known to be equivalent\" - what's the procedure?",
            "25": "Minor comments including typos etc.",
            "26": "--\nThe paper has the potential to be really nicely written and well-presented.",
            "27": "Currently it reads like it was thrown together just before the deadline (which only adds to the overall frustration as a reader).",
            "28": "In fig.",
            "29": "1 the second equivalent question example is interesting, since strictly speaking \"box\" and \"rectangular container\" are not synonyms (e.g.",
            "30": "boxes can be round).",
            "31": "Since strict synonymy is hard to find, does that matter?",
            "32": "(I realise the dataset already exists and was presented elsewhere, but this might be worth a footnote).",
            "33": "missing (additional) right bracket after Herbert (2016)\n\nNot sure footnote 1 needs to be a footnote.",
            "34": "It's already been said, I think, but if it does need repeating it probably deserves to be in the body of the text.",
            "35": "between pairs questions\n\nsee Fig.3 -> figure 2?",
            "36": "see Fig.1 -> Tab.",
            "37": "1?",
            "38": "(on p.5)\n\nfootnote 1 missing a right bracket\n\nusually involve -> involves\n\n+9]) - extraneous bracket\n\nFig.",
            "39": "4.1 -> Fig.",
            "40": "3?",
            "41": "(p.6)\n\np.7 wastes a lot of space.",
            "42": "In order to bring some of the appendix into the main body, I would do away with the very large bulleted list.",
            "43": "(I don't mean lose the content - just present it more efficiently)\n\nRemember than\n\nFinally in Fig.",
            "44": "4.2 - some other figure\n\ndue of the long chains"
        },
        "rkgQW97pKS": {
            "0": "This paper proposes the incorporation of “prior knowledge” which enters in the form of the relations between training instances in neural network training.",
            "1": "The proposed method is tested on VQA problem, bringing improvements upon the popular soft regularizer.",
            "2": "The authors claim that their method is a general technique but in fact, the constraints are drawn from specific tasks (VQA for example).",
            "3": "So, I believe the contribution is rather domain-dependent and not general.",
            "4": "Can you explain more how this method can be applied to general problems?",
            "5": "Other than that, I have some concerns:\n1.",
            "6": "Although the authors claim that they are the first to bring these annotations to VQA, I see their training procedure is closely related to cycle-consistent learning.",
            "7": "Recent work in VQA also applied cycle consistency as an online data-augmentation technique (See Shah et al.",
            "8": "2019).",
            "9": "“Shah, M., Chen, X., Rohrbach, M., & Parikh, D. (2019).",
            "10": "Cycle-consistency for robust visual question answering.”\n2.",
            "11": "In Section 2, the authors say “constraints on the parameter space of a model are often non-intuitive”.",
            "12": "How are they \"non-intuitive\" and why the proposed method is more intuitive in terms of theory?",
            "13": "Please clarify this.",
            "14": "3.",
            "15": "Each question in Hud et al.",
            "16": "is associated with a functional program, therefore, questions are compositional.",
            "17": "However, arbitrary questions don’t need to strictly follow this constraint.",
            "18": "Natural language is not exactly suited to functional programming I think.",
            "19": "I have doubts about the claim in Section 4 “Our method can use partial annotations and should more easily extend to other datasets and human-produced annotations”.",
            "20": "Also, the definition “A question is defined as a set of operations” does not seem correct.",
            "21": "A question can be translated into a program that is composed of a set of operations.",
            "22": "4.",
            "23": "Experimental results are not strong enough for such strong claims I believe.",
            "24": "Regarding GQA dataset, the authors should compare the proposed method with more works, for example, Hu et al.",
            "25": "2019 and Hudson et al.",
            "26": "2019 achieve much favorable performance upon MAC.",
            "27": "\"Hu, R., Rohrbach, A., Darrell, T., & Saenko, K. (2019).",
            "28": "Language-Conditioned Graph Networks for Relational Reasoning.\"",
            "29": "\"Hudson, D. A., & Manning, C. D. (2019).",
            "30": "Learning by abstraction: The neural state machine.\"",
            "31": "Minor comments: The paper is not really well written.",
            "32": "I even found a wrong reference (Section 3)."
        }
    },
    "rylMgCNYvS": {
        "BkgskIy2YB": {
            "0": "Motivated by a link between LSTMs and counter machines (suggested by recent work, e.g.",
            "1": "Merrill, 2019 et al.",
            "2": "), this paper studies the formal properties of counter machines (and LSTMs by extension) as grammars, in hopes of discovering why LSTMs perform particularly well in language tasks despite having no obvious hierarchical structure.",
            "3": "It makes the following contributions.",
            "4": "It shows that: (1) many variants of counter machine converge to the same formal language, (2) the counter languages are closed under common set operations (e.g.",
            "5": "intersection, union, and complement), (3) counter machines are incapable of evaluating boolean expressions, and (4) only a weak subclass of CLs are sublinear (and most are not).",
            "6": "While this paper gives thorough proofs, I would have liked to see more connection to practical NLP with some experiments.",
            "7": "Also, I would have liked to see more concrete takeaways from this paper: if correctly detecting surface patterns doesn't mean that LSTMs build correct semantic representations, what can ensure that LSTMS do have a correct semantic representation?",
            "8": "As this paper is far from my area of expertise, I'm willing to change my score based on my co-reviewers."
        },
        "H1ll3f5JqS": {
            "0": "The paper proof properties of counter machines that have in recent work be suggest that LSTMs can model those.",
            "1": "The papers starts to mentions related work that relates automaton's and counter machines with LSTMs.",
            "2": "These related work papers do some correlational experiments partly restricted in size, layers and architecture.",
            "3": "They provide mostly empirical evidence that some behaviour is related to performance seen in LSTMs, GRU, etc.",
            "4": "and similar to those in counter automata.",
            "5": "The paper makes then the point to take  the counter machine as a simplified formal model of the LSTM.",
            "6": "However, I would read Merrill 2019 that counter machines could be model via LSTMs but are not limited or they are not an upper bound what they can compute.",
            "7": "The authors does some proves on counter automata and hopes to gain insights into the properties of LSTMs used for NLP or semantic analysis and this would provide insights for the use in NLP.",
            "8": "It seems to me that the paper claims that counter automatons are an upper bound for the computation power of LSTMs.",
            "9": "In the way I read this seems at least not well formulated or too strong.",
            "10": "I would not follow the conclusion that 'A general take-away\nfrom our results is that just because a counter machine (or LSTM) is sensitive to surface patterns in\nlinguistic data does not mean it can build correct semantic representations'.",
            "11": "The argumentation is flawed as counter machines are not an upper limit of expressiveness of LSTMs nor do they describe well what they do.",
            "12": "That one can use LSTMs to compute languages that counter automata can do too means not that they could do more.",
            "13": "The property of Counter automata are useful for instance to build phrase structures meaning they can be use to express scope and keep track of.",
            "14": "However, deeper layered networks are widely used to put structure over the scopes (arguments) to connected them in a higher order fashion.",
            "15": "There are many paper which show empirical how to build semantic or syntactic structures using LSTMs - also in already quite well in seq2seq fashion.",
            "16": "The more theoretical part looks fine to me and could be of value to readers.",
            "17": "Nevertheless, the authors could considere to revise  their claims as they are not well supported by the evidence provided in the paper nor pervious literature cited."
        },
        "SylMDPRYcr": {
            "0": "Summary\n-------\n\nThe authors investigate (subclasses of) generalized counter machines with respect to their weak generative capacity, their ability to represent structure, and several closure properties.",
            "1": "This is motivated by recent indications that LSTMs have comparable expressivity to counter machines, so that the formal properties of these machines might provide indirect insights into the linguistic suitability of LSTMs.",
            "2": "Evaluation\n----------\n\nI also reviewed this paper for SCiL a few months ago.",
            "3": "While I had major reservations back then, I am happy to provide a more positive evaluation this time as the authors have done some revisions that clear up many points of confusion.",
            "4": "I have to add two caveats, though.",
            "5": "First, I am a bit disheartened that the authors chose not to adopt many of the excellent changes suggested by another SCiL reviewer (who went way beyond the call of duty with their multi-page review).",
            "6": "Second, I did not have sufficient time to check all proofs for their correctness.",
            "7": "In many cases the strategies strike me as intuitively sound, but my intuition tends to miss edge cases.",
            "8": "Nonetheless, I think that this paper, albeit a bit of a gamble, would make for an interesting addition to the program.",
            "9": "1) Weakness: Link to neural networks still unclear\n\nThe central weakness of the paper is still the link between neural networks and counter automata.",
            "10": "Based on what is said in the paper, this is merely a conjecture at this point, not a well-established fact.",
            "11": "Without this link, the value of the paper is unclear.",
            "12": "If, however, this conjecture should turn out to be true, the paper would mark a very strong starting point for further exploration.",
            "13": "This makes it a gamble worth taking.",
            "14": "2) Strong results, but lack of examples\n\nThe results are not trivial and provide deep insights into the inner workings of counter machines.",
            "15": "In particular the fact that counter machines cannot correctly represent Boolean expressions reveals key limitations on their representational power.",
            "16": "The semilinearity result is less impressive because of how limited the machines are that it applies to, and I'm not sure that the proof provides a good basis for generalization to more complex machines.",
            "17": "The authors might consider removing this part to clear some space for examples, which are sorely needed.",
            "18": "The formalism is abstract and unfamiliar to most readers, and a few concrete examples would greatly strengthen the readers' intuition.",
            "19": "3) No investigation of linguistically important string languages\n\nAs the authors make claims about linguistic adequacy, it is surprising that there is no discussion of TALs, MCFLs or PMCLFs.",
            "20": "The grammar formalism of GPSG was abandoned because it was limited to context-free languages and could not handle those more complex language classes.",
            "21": "So if counter machines fail here, the issue of their linguistic adequacy is already decided without further probing semilinearity or representational power.",
            "22": "As far as I can tell, real-time counter machines cannot generate the PMCLF a^{2^n}, which is an abstract model of unbounded copying constructions in natural language (see Radzisnky on Chinese number names, Michaelis & Kracht on Old Georgian case stacking, and Kobele on Yoruba).",
            "23": "Nor is it obvious to me that counter machines can handle the copy language {ww | w \\in \\Sigma^*}, a model of crossing dependencies, although they can handle a^n b^n c^n (a TAL).",
            "24": "It should also be possible to generate the linguistically undesirable MIX language, which is a 2-MCFL but not a TAL.",
            "25": "Minor comments\n--------------\n\n- As noted in my SCiL review, your definitions still differ from those of Fischer et al.",
            "26": "1968.",
            "27": "What is the reason for this?",
            "28": "- Theorem 3.1: \\subsetneq would be clearer than \\subset\n\n- p4, typo: the the\n\n- Proof of Theorem 3.2: Unless I misunderstand your modulo construction, your ICL only has resolution up to mod n. For instance, with mod 2 it can distinguish 2 from 3, but not 2 from 4.",
            "29": "The CL can do that.",
            "30": "Don't you need a second counter c_i' for each c_i, then, to keep track of how often you have wrapped around modulo n in c_i?",
            "31": "That would still be incremental as you can never wrap around by more than 1 in any given update.",
            "32": "- Sec 6.1: in all those definitions, if should be iff\n\n\nReferences\n----------\n\n@ARTICLE{Radzinski91,\n  author = {Radzinski, Daniel},\n  title = {Chinese Number Names, Tree Adjoining Languages, and Mild Context\n\tSensitivity},\n  year = {1991},\n  journal = {Computational Linguistics},\n  volume = {17},\n  pages = {277--300},\n  url = {http://ucrel.lancs.ac.uk/acl/J/J91/J91-3002.pdf}\n}\n\n@INPROCEEDINGS{MichaelisKracht97,\n  author = {Michaelis, Jens and Kracht, Marcus},\n  title = {Semilinearity as a Syntactic Invariant},\n  year = {1997},\n  booktitle = {Logical Aspects of Computational Linguistics},\n  pages = {329--345},\n  editor = {Retor{\\'e}, Christian},\n  volume = {1328},\n  series = {Lecture Notes in Artifical Intelligence},\n  publisher = {Springer},\n  doi = {10.1007/BFb0052165},\n  url = {http://dx.doi.org/10.1007/BFb0052165}\n}\n\n@PHDTHESIS{Kobele06,\n  author = {Kobele, Gregory M.},\n  title = {Generating Copies: {A}n Investigation into Structural Identity in\n\tLanguage and Grammar},\n  year = {2006},\n  school = {UCLA},\n  url = {http://home.uchicago.edu/~gkobele/files/Kobele06GeneratingCopies.pdf}\n}"
        }
    },
    "H1eA7AEtvS": {
        "rJlMUMIoFH": {
            "0": "The authors present ALBERT, a modification of the BERT architecture with substantially fewer parameters.",
            "1": "They show that despite being much smaller, the performance is very strong and achieves state of the art on a variety of different tasks.",
            "2": "There are several ideas proposed here: embedding factorization, sharing layers, and sentence ordering as a training objective.",
            "3": "1.",
            "4": "The point that naively increasing the size of the BERT architecture does not work is a good one, but the authors don't acknowledge that this is tied up in the effect of regularization.",
            "5": "Cross layer parameter sharing has a regularization effect that simply scaling up BERT large to x-large or such sizes does not have.",
            "6": "This is also an issue with the authors making the statement that they are the first to show that dropout is harmful for Transformers.",
            "7": "This is a large generalization that seems to be a special case of not only the regularized architecture they propose but also the large quantity of data that the model still underfits to.",
            "8": "2.",
            "9": "The authors propose embedding factorization to reduce the number of parameters in the embedding dimension.",
            "10": "This is very intuitive, but the authors do not cite or compare to related approaches.",
            "11": "I understand these models are computationally intensive and thus do not expect large quantities of detailed ablations.",
            "12": "However, this kind of dimensionality reduction has been explored with other techniques, for example for knowledge distillation, quantization, or even adaptive input/softmax (and with subword as well, not just whole word modeling).",
            "13": "These techniques have also been applied to machine translation models, which do not use them to learn rare words.",
            "14": "I believe a better discussion of these methods should be added to the paper, as this is not a novel proposition.",
            "15": "3.",
            "16": "A large takeaway I have from this paper is that parameter size is not a good metric.",
            "17": "While ALBERT is substantially smaller, the authors do not make it clear that this model is very slow at inference time due to the large size.",
            "18": "This raises several questions: is it better to have models that are deeper or more wide?",
            "19": "Can the authors actually report the latency in a comparative table next to BERT?",
            "20": "Can the authors provide a sense of how large this model is in MB - e.g.",
            "21": "presumably a goal of less parameters would be to have a model with less memory, but then the decision between memory and latency that different models make should be made more clear.",
            "22": "4.",
            "23": "Section 4.8 is not clear.",
            "24": "Exactly how much data, in terms of GB of uncompressed text, is used here?",
            "25": "Is it the data of XLNet and RoBERTa, so larger than both of those settings individually?",
            "26": "Further, the authors train for 1 million steps.",
            "27": "This is larger than both XLNet and RoBERTa, is that correct?",
            "28": "Or there is some detail about the size of the batch that actually makes it comparable?",
            "29": "The many small tables where the changes are not clearly delineated makes it difficult to compare results."
        },
        "SygC0QIhFB": {
            "0": "Summary: This paper investigates improving upon BERT by reducing complexity in terms of free parameters and memory footprint as well as computation steps.",
            "1": "They propose 2 strategies for doing this: 1) Splitting the embedding matrix into two smaller matrices (going from V x A to V x B + B x A where B <<<< A); 2) layer-wise parameter sharing.",
            "2": "They also utilize sentence order prediction to help with training.",
            "3": "These coupled with a bunch of other choices such as using the lamb optimizer, certain hyperparameters etc help show dramatic empirical gains across the board on a wide variety of NLP/NLU tasks.",
            "4": "Positives: This paper has a dramatic, seemingly statistically significant reduction in error across a wide-variety of tasks.",
            "5": "It provides a thorough experimental plan and approaches the few addendums to training (splitting the embedding matrix, the layer-wise parameter sharing, and the sentence order prediction).",
            "6": "Concerns & Questions: There's a lot of experimentation here and a lot of seemingly deliberate choices after seeing empirical results during the research phase.",
            "7": "How crucial are the choices of optimizer and other specific hyperparameters?",
            "8": "Were there ones you observed that were more brittle than others?",
            "9": "Any specific 'reasonable' configurations/settings that caused degenerate solutions?"
        },
        "Hkl2NgFecr": {
            "0": "This paper proposes a new pre-trained BERT-like model called ALBERT.",
            "1": "The contributions are mainly 3-fold: factorized embedding parameterization, cross-layer parameter sharing, and intern-sentence coherence loss.",
            "2": "The first two address the issue of model size and memory consumption in BERT; the third corresponds to a new auxiliary task in pre-train, sentence-order prediction (SOP), replacing the next sentence prediction (NSP) task in BERT.",
            "3": "These modifications lead to a much leaner model and improved performance.",
            "4": "As a result, ALBERT pushes the state of the art on GLUE, RACE, and SQuAD while having fewer parameters than BERT-large.",
            "5": "This is a well-written paper which is easy to follow even for readers without deep background knowledge.",
            "6": "The proposed method is meaningful and effective.",
            "7": "Its empirical results are impressive.",
            "8": "Other comments:\n\n- Section 4.9.",
            "9": "Why use the all-share condition for state-of-the-art ALBERT results (as indicated in Table 2)?",
            "10": "Judging from Table 4 and 5, shouldn't the non-shared condition give better results?",
            "11": "The number of parameters would be larger, of course.",
            "12": "- I like the justification/motivation given for replacing NSP with SOP.",
            "13": "I wonder if the authors have tried other objectives (but didn't work out).",
            "14": "Such negative results are valuable to practitioners.",
            "15": "- Typo in Sec.",
            "16": "4.1: x1,1, x1,2 should be x2,1, x2,2."
        }
    },
    "HkePNpVKPB": {
        "SkgR8TeAKH": {
            "0": "The paper \"Compositional languages emerge in a neural iterated learning model\" address the problem of language emergence in two-players games.",
            "1": "In particular, the authors proposed a neural iterated learning model which seeks comopsitional languages.",
            "2": "Authors claim that compositional languages are easier to be learned and that they allow listeners to more easily understand provided messages.",
            "3": "The problem of language emergence is interesting since it refers to the problem of finding efficient ways to communicate.",
            "4": "At first I was wondering why such compositional language messages would be desirable and was a bit negative on this work.",
            "5": "But in Table 2 authors give results for zero-shot performance which emphasize the benefits resulting from finding such composition properties in language: compositional languages have greatly better generalization properties.",
            "6": "I like the parallel that we can make with humans, that have to learn to understand language for achieving tasks when they are childs, which is simulated here in the reset and re-training performed  at the start of each generation, and which explains the natural emergence of such compositionality.",
            "7": "This is not a big surprise, but I like the simple but clever idea of reset that the paper exploits.",
            "8": "What I like less is the inequality (5) that not fully convinced me.",
            "9": "I am not sure why this should hold.",
            "10": "Moreover, authors claim that if Ia is too long, no improvement of the topology can be made.",
            "11": "I cannot understand why.",
            "12": "For instance if there are ambiguous messages in D, the interaction phase can radically change the language even if the pre-training has converged... And why should weak pre-training favor low-p languages?",
            "13": "Also, the considered learning scheme is that in the transmitting phase Alice records messages for all objects in D. But is it realistic ?",
            "14": "A study of the impact of the size of vocabulary would also be useful (since it must have a big impact on the results)   \n\nAt last, why considering such discrete messages while for agents communication  would be easier with continuous messages ?",
            "15": "When considering continuous messages, the problem relates with disentanglement which is a current hot-topic: having one factor controling one specific aspect of the object would also be useful for improving zero-learning."
        },
        "SyeiU2UAtr": {
            "0": "This paper studies the emergence of compositional language in neural agents.",
            "1": "They propose an iterated learning method that consists of three phases: a supervised learning phase for a randomly-initialized speaker and listener, a self-play phase (where both agents are updated together), and a phase where a new dataset is created based on the current speaker’s language.",
            "2": "This dataset is then passed on to the next ‘generation’ of speaker and listener.",
            "3": "The paper finds that this procedure, with the right hyperparameters, leads to the emergence of more compositional languages in a simple symbolic referential game.",
            "4": "The question of how to emerge a compositional language is indeed interesting.",
            "5": "This paper does a good job of conducting a careful set of ablations and analyzing the results.",
            "6": "In my mind, the main scientific contribution of this work is the empirical verification of the principle ‘compositional languages are easier to learn’.",
            "7": "While this principle is intuitive, it’s good to see it confirmed via experiments.",
            "8": "The paper’s description of the ‘interval of advantage’ --- the range of updates where a compositional language performs better on the task than a non-compositional language --- is insightful to me.",
            "9": "I do have concerns for this paper around utility and novelty.",
            "10": "As the paper mentions, it has already been shown that iterated learning procedures give rise to more compositional languages in non-neural models.",
            "11": "While there are some things to consider in adapting this to neural networks, to my eye they seem rather straightforward (i.e.",
            "12": "tuning the number of updates of the speaker and listener, the values I_a and I_b), contrary to the paper’s assertion.",
            "13": "From a utility perspective, the paper doesn’t go into how this might be practically applied in general to train neural agents to learn compositional languages in more complex environments (where they might be simultaneously speakers and listeners), as they stick to a very simple symbolic referential game.",
            "14": "The main contribution of this paper is really: “studying how neural networks behave when trained in an iterated learning setting in a simple referential game“.",
            "15": "I think this is a nice contribution, but the main question for me is whether this is enough for an ICLR acceptance.",
            "16": "My other concern is around the length of the paper.",
            "17": "In my opinion, while the paper is well-written, it’s quite bloated and there is a lot of repetition.",
            "18": "I think the paper could easily be condensed to 8 pages and retain the same information.",
            "19": "Alternatively, some of the graphs in the Appendix (which are quite nice) could be added to the main paper to give more insight about how neural networks behave in this iterated learning procedure.",
            "20": "Finally, the paper shows that compositional languages generalize better to the held-out validation set.",
            "21": "While this is also an intuitive result, it’s nice to have in the paper.",
            "22": "I’d encourage the authors to remove the ‘zero-shot’ terminology though (which usually refers to predictions on new samples outside of the training distribution), and just stick to what is actually being shown, which is improved generalization.",
            "23": "Overall, I like the paper, but due to the concerns mentioned above I think it’s borderline, with a slight lean towards rejection."
        },
        "Skev5xvMcB": {
            "0": "This paper proposed a neural iterated learning algorithm to encourage the dominance of high compositional language in the multi-agent communication game.",
            "1": "The author shows that the iterative training of two agents playing a referential game can incrementally increase the agent to use the language with high topological similarity.",
            "2": "The authors also demonstrated that topological similarity is correlated with zero-shot performance.",
            "3": "And Experiment results show the authors could propose alternative pre-training strategies for the neural agent can prefer high compositional language and achieve high task performance.",
            "4": "Emerging the compositional language from uniform prior can be very challenging, as mentioned in the paper, \"high-\\rho language only represents a small portion of all possible unambiguous language\" and \"high-\\rho do not seem to be directly preferred during the interaction phase, they can be favored by the neural agent during the learning phase.\"",
            "5": "I agree with the authors with respect to the difficulties of generating high-\\rho language, but I have questions about the designed learning phrase, especially how to avoid the mode collapse during training.",
            "6": "With the first hypothesis, \"high topological similarity improves the learning speed of the speaking neural agent\", I agree that high topological language has less low sample complexity compared to random sampled low topological language.",
            "7": "However, low topological language didn't necessarily lead to low sample complexity, for example, given a D consists of {a, a, a, a ...}, the sample complexity can be quite low and also with a high topological score.",
            "8": "I wonder is the hypothesis still true in this case?",
            "9": "On the second hypothesis, a high-\\rho language will be faster to success choosing the right object using fewer samples.",
            "10": "I agree with the authors that compositional language can and will lead to better generalization ability.",
            "11": "However, from Algorithm1, it seems Bob receives the message only update with its parameters.",
            "12": "There is no change of language generation.",
            "13": "I wonder how the update of Bob will help Alice to speak the more compositional language?",
            "14": "More explicitly, to avoid mode collapse.",
            "15": "Exp 3 mainly tests the model with different \\rho as the posterior probability.",
            "16": "In Exp 4, I assume the posterior probability of the mapping is random (uniform), is that correct?",
            "17": "It will be great if the confirm this since this is my major doubt when reading the paper.",
            "18": "As mentioned above, I understand that high topological similarity language both benefit from the speaker and listener.",
            "19": "However, there are some corner cases that mode collapse will happen and It seems the model will hardly recover from that.",
            "20": "From Table 3, it seems the authors have fixed vocabulary size 8, I'm wondering what happens with a large vocabulary size?",
            "21": "will the model still learn to emerge the compositional language with a large vocab size?"
        }
    },
    "BJx4rerFwB": {
        "ByxSBaCJqr": {
            "0": "\nSummary:\nThis paper proposes a method for aligning an input text with the frames in a video that correspond to what the text describes in a weakly supervised way.",
            "1": "The authors propose a combination of a “Frame-By-Word” (FBW) representation and a Word-Conditioned Visual Graph (WCVG).",
            "2": "The proposed method outperforms the weakly supervised baseline presented in the paper in experiments by a large margin.",
            "3": "In addition, it quantitatively performs close to previous strongly supervised methods.",
            "4": "Pros:\n+ New Word-Conditioned Visual Graph representation\n+ Outperforms weakly supervised baseline\n+ Ablation study of the moving parts\n+ Interesting use of positional embeddings for multi-modal learning\n\nWeaknesses / comments:\n- What is the processing speed of the method compared to the baseline?",
            "5": "The proposed method makes multiple comparisons while computing the attention weights over all words and frames.",
            "6": "Does this cause the method to be slower than the baseline?",
            "7": "If so, how much slower is it?",
            "8": "Answers to these questions can help readers to keep in mind the trade-off of the proposed method for achieving the accuracy presented in the paper.",
            "9": "- Number of parameters comparison with baseline:\nDid the authors make sure to have similar number of model parameters for the baselines and the proposed method?",
            "10": "Maybe I missed it, but I couldn’t see a mention of this anywhere.",
            "11": "It would be useful to state this so that readers are sure that it’s not the number of parameters that is helping the method.",
            "12": "- Assumption that sentences are only associated with its ground truth video:\nThe authors mention that they have the same assumption as Mithun et al., 2019.",
            "13": "Can this assumption be detrimental if the dataset does not follow it?",
            "14": "Say there are sentences in the dataset that could describe segments in multiple videos.",
            "15": "Could this assumption lead to suboptimal representation learning / relationship learning for words / video frames?",
            "16": "- Determining the size of the sliding window:\nFrom reading the paper, it looks like the sliding window used for computing the word / frame relationships has to be manually defined.",
            "17": "This seems a bit suboptimal for the generalizability of this method.",
            "18": "Do the authors have any comments on this?",
            "19": "- Can this model be supervised?",
            "20": "If so, how does it compare to the supervised baselines?",
            "21": "The authors point out that their weakly supervised method performs close to the strongly supervised previously proposed.",
            "22": "This is a nice finding, however, have the authors try to answer the question of what would happen if the proposed model is supervised?",
            "23": "Will the proposed model outperform the strongly supervised baselines?",
            "24": "Or at least perform the same?",
            "25": "Conclusion:\nIn conclusion, the proposed method makes sense and it has been shown to empirically outperforms a previous weakly supervised baseline.",
            "26": "The authors also provide an ablation study of the moving parts to show that the entire pipeline is important to achieve the highest performance in the hardest setting.",
            "27": "It would be nice if the authors successfully answer / address the questions / concerns mentioned above in the rebuttal."
        },
        "Byx0x0jDcB": {
            "0": "The paper proposed a weakly-supervised wMAN model for moment localization in untrimmed videos.",
            "1": "Only the video-level annotation is available for training, and the goal is retrieving the video segment described by the sentence.",
            "2": "The proposed model explored to utilize better context information and captured the relation between video and sentence/word via graph neural networks.",
            "3": "In particular, instead of modeling the context information between the sentence and each video frame, wMAN tried to learn the representation with multi-level and co-attention, which considers all possible pairs between the word and the frame.",
            "4": "The proposed model was evaluated on two publicly-available dataset and achieved reasonable results.",
            "5": "Pros:\n- Weakly-supervised method for video moment localization is a reasonable and important direction.",
            "6": "- wMAN explicitly utilized multi-level context information between the sentence and the video frame, and used the graph neural network and the message passing to model the representation.",
            "7": "I think this is a reasonable direction.",
            "8": "- wMAN is evaluated with two publicly available datasets, and is compared with state-of-the-art methods and other \"oracle\" baselines.",
            "9": "The performance is impressive and could be a better baseline for the future work.",
            "10": "Cons:\n- wMAN model the relation for all possible pairs of the word and the video frame.",
            "11": "However, if the video is quite long, say 10 minutes, 30 minutes, or even few hours, will the method still be efficient and effective?",
            "12": "- When building the relation between the word and the frame, is there any emphasis on verb, some particular word, or self-learned attention?",
            "13": "For some particular word, say \"people\" and \"cup\", won't it have strong connection with many frames?",
            "14": "But for some of the words, say \"hold\" and \"sits\", could it play a more important role?",
            "15": "- Followed by previous question, in the qualitative results, it seems the boundary parts of the predicted video segments are less accurate.",
            "16": "Is it because some of the words case these false positive results?",
            "17": "What do you think the reason is?",
            "18": "- Experimental results: I suggest the author to provide more ablation analysis to the experiment section.",
            "19": "For example, the full model of wMAN works better than FBW on R@1, but worse on R@5 and R@10.",
            "20": "Is there a particular reason about this?",
            "21": "PE seems to be important for wMAN, and the authors provides few sentences analysis about this, but I don't think I fully understand this part.",
            "22": "Another problem is that there is only few qualitative results, and in both these two examples, predicted results cover the GT segments.",
            "23": "Is this always the case for wMAN?",
            "24": "Why?",
            "25": "Some failure cases could also be very helpful.",
            "26": "- Less technical comments: The paper writing is fine to me, but I don't like the typesetting.",
            "27": "I suggest to put the model figure more close to the methodology section and the qualitative results on page 8.",
            "28": "Overall, I think the paper is marginal above the accept line."
        },
        "Skeukb_ncH": {
            "0": "This work presents a model for text based video clip (video moments or text-to-clip) retrieval.",
            "1": "The goal is to identify a video segment within a longer video that is most relevant to an input sentence.",
            "2": "Authors propose a new model based on a weakly-supervised training approach.",
            "3": "This model does not require explicit temporal annotations to align text and video, but it only needs as an input the full video and sentence pairs.",
            "4": "Key aspects of the model are: i) A coattention step frame-by-word and word-by-frame that produces the basic embeddings of the model, which is enriched with positional information, and ii) A contextual step that aggregates contextual information from all the frames using graph propagation.",
            "5": "Afterwards, they use a  LogSumExp pooling strategy to score similarity among the input sentence and video frame.",
            "6": "The main contribution of the paper is incremental (specially respect to Mithun et al., 2019), I do not see a ground-breaking contribution.",
            "7": "One of the main novelties with respect to previous text-to-clip models is the use of co-attention schemes at the level of words and frames.",
            "8": "However, the idea of co-attention at different grain-levels have been proposed before.",
            "9": "Actually, while the model makes an extensive use of frame-to-word encoding, it is not clear to me what is the role of the word-to-video representation in Eqs.",
            "10": "5 and 6.",
            "11": "In general, the paper is well written.",
            "12": "The experimental evaluation is convincing.",
            "13": "However, it is not clear why authors change the structure of the evaluation among the experiments.",
            "14": "As an example, for the experiments in Charades-STA dataset, they include scores for different IOUs levels, but they do not repeat this for DiDeMo dataset.",
            "15": "Similarly, for DiDeMo dataset, results in Table 3 are for the test set, while the ablation study in Table 4 is for the validation set.",
            "16": "I will recommend to standardize the evaluations.",
            "17": "Another comment is that in several experiment best performance is obtained using just the FBW module, it will be interesting to further analyze why the contextual cues hurt performance in some cases, maybe at least a qualitative analysis.",
            "18": "Also, in some part of the papers, authors state that the proposed model does better than strongly-supervised state-of-the-art methods on some metrics, looking all the reported tables, I do not think that this is the case.",
            "19": "Authors show qualitative results about cases where the model perform well, it will be good to also analyze failure cases, actually, according to the final scores, there is still lot of cases that the model can't handle properly.",
            "20": "I rate the paper as borderline, but there is not such a rating at ICLR 2020, so I will lean to weak reject."
        },
        "rJxsIwXpcB": {
            "0": "Overview: \nThe authors proposed a weakly-supervised method to localize video moments given text queries.",
            "1": "The model builds multi-level relational graphs among pairs of word and video frame, and the graph is used to aggregate visual-semantic feature for each word and each frame.",
            "2": "Then the attentive features are used to localize the sentence query in videos by calculating the similarity of words and frames.",
            "3": "In summary, the proposed weakly-supervised Moment Alignment Network (wMAN) utilizes a multi-level co-attention mechanism to learn richer multimodal representations for language based video retrieval..\n\nPros:\n1.",
            "4": "Significant performance improvement on Didemo and Charades-STA datasets.",
            "5": "The authors achieved very good performance on both dataset, even higher than some of the full-supervision methods, such as CTRL and MLVI.",
            "6": "Cons:\n1.",
            "7": "The overall novelty of the proposed methods is limited.",
            "8": "Essentially, the key points of the model is hierarchical visual semantic co-attention.,which is proposed originally in [Hierarchical Question-Image Co-Attention\nfor Visual Question Answering], although the original application is VQA in image domain.",
            "9": "So in this way, the novelty is only marginal.",
            "10": "2.",
            "11": "Paper writing can be improved.",
            "12": "Figure 2 shows the overall structure of the model, however, the caption doesn't explain all the notations in the figure, such as WCVG, and the equations.",
            "13": "Additionally, the reference is very far away from Figure 2, which makes the whole paper hard to read.",
            "14": "3.",
            "15": "For evaluation part, one important ablation study is missing: the number of steps T for message passing.",
            "16": "This eval is important, as it shows the necessity of using \"multi-level\" attention.",
            "17": "Minor comments:\n1.",
            "18": "Make the caption of Figure 2 self-explainable, e.g.",
            "19": "the meaning of LSE.",
            "20": "2.",
            "21": "There is a \"word-conditioned\" visual graph network, why not the other way, \"frame-conditioned\" semantic graph net and iterate over it?"
        }
    },
    "SyxhVkrYvr": {
        "rkxuhTae5S": {
            "0": "This paper proposes a model to verify the robustness of NLP models (change in the original probability), more specifically DAM, in the case of word removals in the input.",
            "1": "The idea is given the lower and upper bound on the hidden state at previous layer, compute the new bound by propagating the bounding box around the hidden state at previous layer.",
            "2": "The upper bound at the final layer is then compared with the label probability of the original input to assess if the probability increases or not.",
            "3": "By training model with a hinge loss based on this verification method, they show that the model becomes more robust to word removals.",
            "4": "Overall, the paper is well written and the idea of using IBP with an attentive model seems to work empirically for SNLI datasets.",
            "5": "But, the technical contribution feels incremental over previous approaches, especially Huang (2019).",
            "6": "I have several questions related to some parts of the paper:\n\n- Since upper and lower bounds are also propagated, do you backpropagate the gradients via these bounds or only via the original inputs?",
            "7": "- How sensitive is the label in SNLI dataset to word removal?",
            "8": "For some label types, such as entailment, it might have less of an effect that for the others.",
            "9": "- How is the accuracy distributed wrt different label types?",
            "10": "- Since the accuracy of the proposed model drops the most, I am wondering how the verfied accuracy and accuracy are related during training?",
            "11": "For example, can you show what is the verified accuracy with accuracy being close to the standard training?"
        },
        "HJefc93Z5S": {
            "0": "This works considers the task of Natural Language Inference (NLI).",
            "1": "The question addressed is that SOTA NLI models tend to lead to\nhigher confidence when some parts are deleted from the \"premise\".",
            "2": "It is a problem known as under-sensitivity.",
            "3": "A method based on IBP is proposed to address this issue.",
            "4": "The idea of Interval Bound Propagation (IBP) is to use interval arithmetic to propagate\nintervals and bound the variation of the target based\non variation of the input.",
            "5": "In other words, one propagates\nupper and lower interval bounds through the network.",
            "6": "The DAM model from (Parikh et al., 2016)\nis studied in particular.",
            "7": "The paper is well written and easy to follow.",
            "8": "My only concern is about the relevance of approach based on DAM when\nthere are now more accurate models for this task.",
            "9": "The paper is however\ninteresting and addressed a relevant topic.",
            "10": "Misc:\n- transpose should be written with $^\\top$ (not $^T$)."
        },
        "ryxiFWbHcH": {
            "0": "-- Overall --\nThis submission tackles to verify the “under-sensitivity” problem of neural network models in the natural language inference by ensuring modes do not become more confident in the predictions when arbitrary subsets of words from the input text are deleted.",
            "1": "The authors developed new verification approaches based on decomposable attention mechanism with interval bound propagation (IBP), which can prove the under-sensitivity issue given a model and a particular sample.",
            "2": "The experimental results on SNLI and MNLI show that the proposed approach leads to a much improved verified accuracy.",
            "3": "-- In general, “under-sensitivity” is a very critical problem for applying neural models in natural language understanding where powerful neural networks tend to capture spurious correlations from the biased datasets.",
            "4": "This submission formulates “under-sensitivity” as a mathematical specification and then try to verify it with IBP verification.",
            "5": "Although the used technique IBP is not new, it would interesting to have the verification in NLI models.",
            "6": "-- Section 5 is a bit unclear how to compute the IBP for deleting several words, and what is the output.",
            "7": "It would be better to have a clear example for how this was computed.",
            "8": "-- As the author mentioned, the verification of under-sensitivity can also be done by using beam-search, although it is costly and not accurate.",
            "9": "IBP is another more efficient option, but not the optimal neigher.",
            "10": "Maybe consider to change the title as “efficient verification”?",
            "11": "-- Specific Questions -- \nThe entire paper builds on decomposable attention.",
            "12": "Is the same approach also applicable to other model types, or only single layer attention-based models?",
            "13": "Also, how this methods work for other NLI or NLU tasks?",
            "14": "In experiments, how the data augumentation penalize the model with a loss for specification violation?",
            "15": "What does the equation look like?",
            "16": "Can you explain a bit more for IBP-training?",
            "17": "How that hinge loss applies to the objective function?",
            "18": "Is the IBP training differentiable?"
        },
        "HJlme7pccr": {
            "0": "This work is an application of interval bound propagation on evaluating the robustness of NLI model.",
            "1": "This work is well-motivated, assuming that the confidence of a neural model should be lower when part of the sentence is missed.",
            "2": "However, the application of vanilla IBP is quite limited in certain model architectures.",
            "3": "In this work, the author considers specifically the decomposable attention model, which is a very shallow network, and not a state-of-the-art model anymore.",
            "4": "It is non-trivial to adapt the proposed method to other more advanced models, such as the ones based on the Transformer model.",
            "5": "Hence, this work does not make enough contribution to be accepted."
        }
    },
    "rJeXS04FPH": {
        "Syl00P_TFH": {
            "0": "This paper describes a new method for learning deep word-level representations efficiently.",
            "1": "The architecture uses a hierarchical structure with skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods.",
            "2": "1.",
            "3": "From table 1a or table 2, the training time of the proposed method is not reduced compared with existing methods.",
            "4": "2.",
            "5": "It seems the number of parameters in DeFINE still depends directly on vocabulary size.",
            "6": "Methods proposed in [1] and [2] do not depend directly on the vocabulary size.",
            "7": "For dataset that has very large vocabulary size, [1] and [2] could potentially have larger compression rate.",
            "8": "3.",
            "9": "The experiments are detailed, and includes ABLATION studies.",
            "10": "[1] Variani, Ehsan, Ananda Theertha Suresh, and Mitchel Weintraub.",
            "11": "\"WEST: Word Encoded Sequence Transducers.\"",
            "12": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
            "13": "IEEE, 2019.",
            "14": "[2] Li, Z., Kulhanek, R., Wang, S., Zhao, Y., & Wu, S. (2018, April).",
            "15": "Slim embedding layers for recurrent neural language models.",
            "16": "In Thirty-Second AAAI Conference on Artificial Intelligence."
        },
        "HJgUu7or9B": {
            "0": "The paper proposes a novel sparse network architecture to learn word embeddings more effectively.",
            "1": "I am not an expert in the area of machine translation, so I am able to sanity-check the results and the reasoning and motivation given in the paper.",
            "2": "Generally I failed to find motivation as to why this specific architecture was chosen out of many others.",
            "3": "I also do not understand the purpose of doing aggressive embedding expansion before another contraction.",
            "4": "Why would this allows to learn a more efficient low dimensional embedding than the original one?",
            "5": "This may happen to be the case, but why?",
            "6": "Overall, the results seem to be a bit inconclusive.",
            "7": "Table 1: b) DeFINE uses less parameters but also gives worse results.",
            "8": "This does not allow me to conclude anything.",
            "9": "Table 1: c) DeFINE seems to give better perplexity results, while using less parameters.",
            "10": "This is good.",
            "11": "Table 2: DeFINE uses more parameters and gives better perplexity results.",
            "12": "I do not know what to conclude, as ideally I would like to see how would DeFINE do with the same number of parameters.",
            "13": "Table 3: \"our implementation\" seems to provide much lower scores than the ones found in the literature and thus can not be used as an fair baseline.",
            "14": "Once this baseline is discarded, DeFINE seems to be producing worse results while using less parameters.",
            "15": "Is this good?",
            "16": "I do no know.",
            "17": "But certainly this is inconclusive.",
            "18": "I do not see how this table allows to conclude the following: \"DeFINE improves the performance by 2% while simultaneously reducing the total number of parameters by 26%, suggesting that DeFINE is effective\".",
            "19": "A few other comments:\n\nFigure 1: if m >> n, why is the bottom (green) of DeDINE network wider than the top (tellow)?"
        },
        "SJeIxKeYqr": {
            "0": "This paper describes an approach to learn word embedding functions more efficiently and with fewer parameters.",
            "1": "This is done by replacing the embedding lookup function which is typical in NLP tasks such as language modeling and machine translation with a hierarchical embedding model.",
            "2": "This allows for a low dimensional embedding layer, reducing total parameters and training time.",
            "3": "A novel skip-connections architecture is introduced as a part of the \"embedding generation model\".",
            "4": "Experiments are conducted for language modeling and machine translation tasks and performance improvements are observed with a reduction in parameters and lesser training time.",
            "5": "The direction of this work is nice, the problem that is being tackled is indeed important.",
            "6": "The obtained results are nice (though this can be improved) and there is indeed some potential value in this work.",
            "7": "However, I have the following concern.",
            "8": "The paper completely ignores a lot of previous and concurrent work in reducing the size of the embedding layer.",
            "9": "These works are in most cases not even cited and no empirical comparisons are provided.",
            "10": "For example, please see below works in matrix factorization approaches, sparse word representation learning, codebook learning and other quantization approaches for compressing word embeddings:\n\nhttps://www.aclweb.org/anthology/P16-1022/\nhttps://aaai.org/ojs/index.php/AAAI/article/download/4578/4456\nhttp://web.cs.ucla.edu/~chohsieh/papers/Mulcode_Compressor.pdf\nhttps://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17042/16071\nhttps://storage.googleapis.com/pub-tools-public-publication-data/pdf/f158f7c81ed8e985fd51a20d193103ce427cad51.pdf\nhttps://arxiv.org/pdf/1711.01068.pdf\nhttps://arxiv.org/abs/1510.00149\n\nI would appreciate if comparisons with some of these approaches is provided in the next iteration of this work.",
            "11": "Other suggestions:\n\n1.",
            "12": "I think the paper would benefit from some analysis of the differences in the word embeddings learnt by a general lookup table learning model in comparison with the word embeddings learnt by this model.",
            "13": "How are the embeddings compressed?",
            "14": "How do the decompressed embeddings compare to the embeddings learnt by the lookup approach?",
            "15": "More insights in the machinery via some visualizations would help.",
            "16": "2.",
            "17": "How do the gains of this method change as more or less training data is provided.",
            "18": "For example, are the gains lesser on Gigaword?",
            "19": "This would be interesting to know.",
            "20": "3.",
            "21": "GLT is mentioned twice in this paper.",
            "22": "Perhaps a slightly more detailed explanation of the same would help improve the readability of this paper.",
            "23": "Based on the presentation improvements and new experiments added to the paper in the rebuttal time period, I am updating my evaluation of this work."
        }
    },
    "H1lac2Vtwr": {
        "r1g_66GAKr": {
            "0": "This paper proposes a novel BERT based neural architecture, SESAME-BERT, which consists of “Squeeze and Excitation” method and Gaussian blurring.",
            "1": "“Squeeze and Excitation” method extracts features from BERT by calculating a weighted sum of layers in BERT to feed the feature vectors to a downstream classifier.",
            "2": "To capture the local context of a word, they apply Gaussian blurring on output layers of the self-attention layer in BERT.",
            "3": "The authors show their model’s performance on GLUE and HANS dataset.",
            "4": "Strengths\n*This paper claims the importance of the local context of a word and shows an effect of their method on the various datasets: GLUE, and HANS.",
            "5": "Weaknesses\n* It seems like the self-attention layer can learn the local context information.",
            "6": "Finding important words and predicts contextual vector representation of a word is what self-attention does.",
            "7": "So, if using local-context information, which is information in important near words, is an important feature for some downstream tasks, then the self-attention layer can learn such important near words by training the key, query, and value weight parameters to connect the near important words.",
            "8": "It would be nice if the authors provide some evidence that self-attention can't learn such a local-context feature.",
            "9": "*In table 1, their experimental results show a slight improvement by using their method, but it's not significant.",
            "10": "* On HANS dataset, they show using local-context can prevent models from easily adopting heuristics.",
            "11": "How Gaussian blurring can prevent that problem?",
            "12": "More explanation about the relation between local-context and adopting heuristics is required."
        },
        "rJezoUU0tr": {
            "0": "The paper proposes fine-tune methodologies for BERT-like models (namely, SeasameBERT).",
            "1": "This includes a method that considers all BERT layers and captures local information via Gaussian blurring.",
            "2": "The methods were evaluated on several baseline datasets (e.g., GLUE, HANS)\n\nStrengths: \n\n* The paper is easy to follow.",
            "3": "*  Squeeze-and-extraction was used to incorporate all hidden layers instead of the common-practice of averaging last 4-layers.",
            "4": "I find it both logical and useful.",
            "5": "* The suggested gaussian blurring method is able to capture local dependencies, which is missing in attention-based transformer layer.",
            "6": "*  SesameBERT improves performance on some GLUE metrics and on HANS dataset.",
            "7": "Also ablation analysis suggests squeeze-and-extraction is a good technique to extract features from BERT model compared to other common practices.",
            "8": "Weaknesses:\n\n* In my opinion, the paper novelty is not significant enough.",
            "9": "Although useful, the suggested techniques are based on existing methods.",
            "10": "*  Incorporate spatial/context-information is usually done by concatenating a location-based embedding with the original word embedding.",
            "11": "I’m curious if the blurring Gaussian will be as useful compared to such version.",
            "12": "* Since the suggested methods are generic, It can be more convincing to see results on recent models, and not only BERT.",
            "13": "Currently, the results are not significantly better.",
            "14": "* The HANS DATASET RESULTS section seems rushed, will be good to elaborate more about HANS.",
            "15": "also the first sentences of the section discusses GLUE results not HANS.",
            "16": "To conclude: The paper is easy to follow, suggests two nice methods for fine-tune BERT.",
            "17": "But although useful, the suggested methods are not novel enough.",
            "18": "The performance does not significantly improves, and the methods are applied only to BERT model."
        },
        "rkeEfJhCKS": {
            "0": "Summary:\nThe paper proposes adding two mechanisms to the BERT architecture for NLU.",
            "1": "The first is based on integrating information from all layers of the encoder via a method called Squeeze and Excitation.",
            "2": "The second uses Gaussian blurring to encourage information sharing among neighboring words.",
            "3": "The proposed method improves modestly on BERT on the GLUE suite of problems.",
            "4": "It also substantially improves on BERT with respect to a class of examples that are designed to confound models that learn superficial heuristics based on word occurrence.",
            "5": "I learn toward rejecting this paper.",
            "6": "The method shows some performance gains over BERT on some GLUE tasks, but these are fairly small for the most part, and BERT outperforms the proposed method by a similar amount on a similar number of tasks.",
            "7": "The strongest result is the HANS \"lexical_overlap\" case, where the proposed method has a clear advantage.",
            "8": "I have no experience with these kinds of NLU models, so I can't say with confidence whether the architectural additions proposed are well-motivated, but to me it feels like there is not a strong justification for adding these particular features to the BERT architecture, and the results do not clearly demonstrate their utility except in the \"lexical_overlap\" case.",
            "9": "Details / Questions:\n* It seems to me that the GLUE results might be within the margin of error.",
            "10": "Is it feasible to replicate training with different random seeds to see what the variance in the performance numbers might be?",
            "11": "I suspect that a statistical analysis [1] might conclude that BERT and the proposed method are indistinguishable on the GLUE suite.",
            "12": "* Were the proposed architectural additions conceived with the HANS \"counterexamples\" in mind (i.e.",
            "13": "is there a specific reason to think that these types of methods would avoid the \"superficial\" reasoning that these examples are supposed to reveal)?",
            "14": "Were other methods of adding context considered?",
            "15": "* I suggest using the same x-axis scale on the two charts in Figure 3 to avoid confusion about the magnitudes of the differences.",
            "16": "References:\n[1] Demšar, J.",
            "17": "(2006).",
            "18": "Statistical comparisons of classifiers over multiple data sets.",
            "19": "Journal of Machine Learning Research, 7(Jan), 1-30."
        }
    },
    "HkxARkrFwB": {
        "Hyl12frTtr": {
            "0": "The paper presents two methods to learn word embedding matrices that can be stored in much less space compared to traditional d x p embedding matrices, where d is the vocabulary size and p is the embedding size.",
            "1": "Two methods are proposed: the first method estimates a p-dimensional embedding for a word as a sum of r tensor products of order n (tensor product of n q-dimensional embeddings).",
            "2": "This representation takes rnq parameters which can be much less than p, since p = q^n.",
            "3": "The second method factorizes a full d x p embedding matrix jointly as a tensor product of much smaller t x q matrices and can obtain even larger space savings.",
            "4": "Algorithms for efficiently computing full p-dimensional representations are also included.",
            "5": "When only dot products are needed, the p-dimensional representations do not need to be explicitly constructed.",
            "6": "In my opinion the terminology from quantum computing and entanglement is an unnecessary complication.",
            "7": "It would be better to simply talk about the special parametric form of the embeddings , which allows efficient storage.",
            "8": "Tensor product representations have been used for embeddings before (but not with the goal of efficiency) (e.g.",
            "9": "Arora et al 2018) https://openreview.net/pdf?id=B1e5ef-C-\nThe paper covers related work briefly and does not compare experimentally to any other work aiming to reduce memory usage for embedding models (e.g.",
            "10": "using up-projection from lower-dimensional embeddings, or e.g.",
            "11": "this paper: Learning Compact Neural Word Embeddings by Parameter Space Sharing by Suzuki and Nagata.",
            "12": "The experimental results on summarization, machine translation, and QA show that the methods can obtain comparable results to models using traditional word embeddings while obtaining savings of up to one-thousand fold decrease in space needed for the embeddings.",
            "13": "The experimental results seem to conflate the issues of the dimensionality of the word embeddings versus that of the higher layers.",
            "14": "For example, in the summarization experiments, word2ketXS embeddings corresponding to 8000-dimensional embeddings are compared to a standard model with embeddings of size 256.",
            "15": "The LSTM and layers for the word2ketXS model would become quite large but their size is not taken into account.",
            "16": "In addition, the activation memory is often the major bottleneck and not the parameter memory.",
            "17": "These issues are not discussed or made explicit in the experiments.",
            "18": "Overall the paper can be a strong contribution if the methods are stated with less quantum computing jargon, the overall parameter size and speed of the different models is specified in the experiments, and more specific connections to related work are made.",
            "19": "Ideally, an experimental comparison to a prior method for space-efficient embeddings.",
            "20": "Question: What is the role of pre-trained Glove embeddings in the word2ket models?",
            "21": "Was any pre-training done on unlabeled text?",
            "22": "Some typos:\n\nSection 1.1\n\n“matrix, as the cost ..”  -> “matrix, at the cost”\n\nUnder Eq (2)\nI think you mean w instead of u \n\nSection 3.2\n\nF_j: R^t -> R^p , do you mean R^q"
        },
        "r1lY-EZJqH": {
            "0": "\nThis paper proposes word2ket - a space-efficient form of storing word embeddings through tensor products.",
            "1": "The idea is to factorize each d-dimensional vector into a tensor product of much smaller vectors (either with or without linear operators).",
            "2": "While this results in a time cost for each word lookup, the space savings are enormous and can potentially impact several applications where the vocabulary size is too large to fit into processor memory (CPU or GPU).",
            "3": "The experimental evaluation is done on several tasks like summarization, machine translation and question answering and convincingly demonstrates that one can achieve close to original model performance with very few parameters!",
            "4": "This approach would be very useful due to growing model sizes in many areas of NLP (e.g.",
            "5": "large pre-trained models) and more broadly, deep learning.",
            "6": "Pros:\n1.",
            "7": "Novel idea, clear explanation of the method and the tensor factorization scheme.",
            "8": "2.",
            "9": "Convincing experiments on a variety of NLP tasks that utilize word embeddings.",
            "10": "Cons:\n1.",
            "11": "(Minor) While this is not the focus of the paper, it would be useful to have at least one experiment with a state-of-the-art model on any of these tasks to further strengthen the results (most of the baseline models used currently seem to be below SOTA).",
            "12": "Minor comments:\nAbstract: stain -> strain\nPage 2: $||u|| \\rightarrow ||w||$"
        },
        "HJlANh2NqH": {
            "0": "\nThis paper explores two related methods to reduce the number of parameters required (and hence the memory footprint) of neural NLP models that would otherwise use a large word embedding matrix.",
            "1": "Their method, inspired by quantum entanglement, involves computing word embeddings on-the-fly (or by directly computing the output of the \"word embedding\" with the first linear layer of network).",
            "2": "They demonstrate their method can save an impressive amount of memory and does not exhibit big performance losses on three nlp tasks that they explore.",
            "3": "This paper is clearly written (with only a couple of typos) but does not yet reach publication standard.",
            "4": "Whilst the empirical performance of their approach is promising from the perspective of saving reducing memory requirements, more experiments are required and more careful comparisons to baselines and other methods in the literature for saving memory/parameters.",
            "5": "In general the related work and experimental sections are weak and brief, with only superficial analysis.",
            "6": "There is  lack of careful analysis and insight into their results, as well as a careful comparisons to other work in this area.",
            "7": "The choice of tasks to evaluate on is broad, which is a strength, but is missing simpler tasks that one would expect to see, such as a text classification dataset, or simple bag-of-vectors style models.",
            "8": "In addition, the choice of models are somewhat outdated baselines.",
            "9": "It seems that transformers would be an ideal setting for their approach, as transformers have rather high dimensional word embedding matrices, but the authors do not run experiments with the now-ubiquitous Transformer.",
            "10": "The quantum inspiration is largely a distraction, and I think the paper would benefit from this element being scaled back or removed in order to free up space for more experiments.",
            "11": "The authors acknowledge one key weakness of their approach, that both training and inference time are increased (by 28% or 55% longer for DocQA depending on compression)  but much more work could be done to understand the best way to  mitigate for longer training and inference times.",
            "12": "The authors argue that reducing the memory footprint of models is vital to address hardware limitations for training and inference for large models like BERT or ROBERTA, but this argument is not particularly strong.",
            "13": "Generally current limitations for training these kinds of models  are the long training times and being able to fit large batches onto our hardware, and the vocabulary matrix is only a constant factor here.",
            "14": "And since training time is a bottleneck, the added value of saving memory vs slowing the training speed by 30-50% is debatable.",
            "15": "Here are some questions for the authors that come to mind when reviewing:\n\nHow does your method compare to other published methods on your benchmarks?",
            "16": "which choices for r and k lead to the best time/memory/performance tradeoff?",
            "17": "how does this compare to other compression methods (on your tasks)\n\nSeq2Seq models usually involve multiplying the the output hidden state with a vocab matrix before softmaxing over all the vocabulary produce word probabilities - did you account for this?",
            "18": "Does your method work for the output vocab matrix?",
            "19": "Did you investigate pre-training word2ket like word2vec or Glove?"
        }
    },
    "ByeL1R4FvS": {
        "r1lbeiL5KH": {
            "0": "The paper \"Unsupervised Data Augmentation for Consistency Training\" marries two recent ideas of \n1.",
            "1": "\"Data Augmentation\" (DA) from supervised learning: The authors explore various methods for \"DA\" mostly inspired by much recent work such as Random image transformations, Backtranslation, and TF-IDF based word replacement.",
            "2": "2.",
            "3": "\"Consistency Training\" (CT) from semi-supervised learning: CT tries to minimize the divergence between the output distributions of the classifiers that are produced by adding noise to the input.",
            "4": "The key insight in this paper is that, data augmentation methods that work well during supervised training should also work equally well as the noise distribution for consistency training on unlabeled data.",
            "5": "The authors support this claim empirically through the experiments in table 1 and 2.",
            "6": "The paper is well written and the authors present extensive comparative and ablation tests to demonstrate that their proposed method works well with both low and high amounts of labeled data.",
            "7": "This paper should be accepted into the conference."
        },
        "rkxWsB9TKS": {
            "0": "In this paper, the authors present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising plays a crucial role in semi-supervised learning.",
            "1": "By substituting simple noising operations with advanced data augmentation methods, their method brings substantial improvements across six language and three vision tasks under the same consistency training framework.",
            "2": "I think the topic itself is interesting and I have the following concerns.",
            "3": "(1) The first is about the contribution of this paper.",
            "4": "In this paper, all the results, including the augmented methods are all well established approaches.",
            "5": "The authors have just employed them in solving a new problem, without support about why they work.",
            "6": "Thus, the results are only strategies, without theoretical guarantee or insights.",
            "7": "It is difficult to convince the reviewers.",
            "8": "(2) Although the authors have achieved seemingly promising results, I think it can not convince me since the authors have not answered the questions about why and when.",
            "9": "I think this paper likes a technical report, not a research paper.",
            "10": "(3) I have also noticed the discussions among the authors and other readers.",
            "11": "It seems that the large improvement depends on the parameters heavily.",
            "12": "So, why not to share the parameters directly?"
        },
        "Byxu8IcTKH": {
            "0": "The paper proposes to substitute simple noising operations with many data augmentation methods in consistency-based semi-supervised learning.",
            "1": "The main idea is the same as previous work: constrain the model predictions of unlabeled examples to be invariant to different noise.",
            "2": "The proposed UDA is evaluated on a wide range of language and vision tasks.",
            "3": "Overall, the paper is well-written and clear.",
            "4": "The most impressive point of this paper is its strong empirical results.",
            "5": "However, it looks not surprising to me that more data augmentations found in supervised learning are also effective in semi-supervised learning.",
            "6": "The paper fails to provide any theoretical insights but a thorough empirical evaluation.",
            "7": "One of my concerns is that the hyperparameters on vision tasks follow those of AutoAugment, which is carefully tuned on supervised tasks.",
            "8": "Apparently, their hyperparameters are based on the whole labeled training dataset.",
            "9": "In this case, the adopted hyperparameters include sort of information of the whole labeled dataset.",
            "10": "Is it fair?",
            "11": "Another concern is how to control the strength of augmentations.",
            "12": "For example, for digit images like SVHN, a \"6\" rotates by 180 degree is \"9\", whose prediction should change correspondingly.",
            "13": "In this case, the assumption of invariance does not hold when the augmentation is too strong.",
            "14": "I'm willing to increase my score if the authors address my concerns."
        }
    },
    "HygnDhEtvr": {
        "BklwQSsJtH": {
            "0": "The authors propose a Graph-to-Sequence Reinforcement Learning Model for Natural Question Generation, evaluated on SQuAD benchmark in for Question Generation.",
            "1": "An interesting aspect of the work is related to the Graph2Seq model, and the use of the Reinforcement Learning to fine-tune the model.",
            "2": "The latter stage seems to improve the structure of the answers considerably.",
            "3": "An interesting use of RL algorithm and apparently a good choice of reward functions.",
            "4": "Questions: in the combined loss used in the RL run:\n1.",
            "5": "Have you managed to have a successful run with gamma = 1?",
            "6": "2.",
            "7": "I understand that the L_rl factor is computed based on the sampling, and the L_lm is computed based on the top variant from the nbest list?"
        },
        "SJg13QwHFH": {
            "0": "This paper focuses on improving the performance on the task of natural language generation.",
            "1": "To this end, they propose a  graph-to-sequence (Graph2Seq) model for the task of question generation which exploits the rich structure information in the text as well as use reinforcement learning based policy gradient approach to address the exposure bias and inconsistency between test/train distributions in cross-entropy optimization setup.",
            "2": "The Graph2Seq model has a bidirectional gated graph neural network on the encoder side, which is an extension of traditional gated graph neural network.",
            "3": "To exploit the rich hidden structure information in the input text, they explore two different methods: (1) syntax-based static graph; (2) semantics-aware dynamics graph.",
            "4": "The proposed model achieves state-of-the-art results on question generation, which are further validated with human evaluations.",
            "5": "Overall, The paper should be rejected because the paper have minor extensions to each of their modules but lacking any major important contribution.",
            "6": "Some major concerns: \n1) The bidirectional gated GNN doesn’t seem novel enough in comparison to previous work\n2) I believe RL to Graph2Seq is a minor extension from Seq2Seq, since RL mostly deals with the decoder part which is common in across both Graph2Seq and Seq2Seq\n\n\nArguments:\n\n1) Adding the structure information to the encoder via the GNNs is an interesting angle for question generation.",
            "7": "Compared to previous work, this paper proposes an additional deep alignment network on the encoder side to align paragraph and answer.",
            "8": "However, the importance of this module is not well studied in the experiments section.",
            "9": "I see that there is an ablation with/without this module but its not fairly compared with other aligning or simple techniques like in Zhao et al.",
            "10": "(2018).",
            "11": "2) The addition of RL component to Graph2Seq is a minor extension from the Seq2Seq model, because both of these models have similar decoder and RL mainly deals with it.",
            "12": "Also the importance of each reward component or the effect of each phrase-matching automatic metrics is missing.",
            "13": "3) Open part I am unclear about the dataset is which dataset version did you use sentence-level or paragraph level?",
            "14": "I see that the baselines correspond to sentence-level, but the Figure-1 alignment module has input paragraph.",
            "15": "Also I couldn’t find the SeqCopyNet (Zhou et al., 2018) split-2 BLEU4 score=13.02 in the original paper!",
            "16": "4) Some of the latest papers which use BERT based models are not discussed in the paper which achieve state-of-the-art-results: “Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering”\n\n5) For Table-2 results are the differences in the scores for the two models statistically significant?",
            "17": "6) Table-3: First of all, evaluating only one metric is no sufficient.",
            "18": "Please see latest papers that have also introduced new metrics that are good for QG evaluation, e.g., Q-BLEU.",
            "19": "The gap between G2Ssta+BERT vs. G2Ssta+BERT+RL seems negligible, and missing statistical significance.",
            "20": "7) Minor comments: BLUE -> BLEU; please cite KNN-style graph sparsification; the color choices in Figure-3 are creating confusion in understanding the model."
        },
        "HyeyXb4TFS": {
            "0": "The paper proposes two modules to improve the performance of the Natural Question Generation task: (1) deep alignment network and (2) passage graph embeddings.",
            "1": "The idea of generating passage graph is novel.",
            "2": "The authors experiment with SQuAD and the numbers look good.",
            "3": "I have a few questions regarding the model and experiments.",
            "4": "First, a reasonable baseline could be using Transformer-based sequence to sequence model.",
            "5": "Could you fine tune the embedding of CLS token and use that as a summary of the document?",
            "6": "It seems that the construction of the passage graph is basically sparsifying a multi-head attention in the BERT model.",
            "7": "I think you should justify why graph-structure is important in your experiment.",
            "8": "Second, if the Graph2Seq is particularly important for Natural Question Generation, the author should clarify it more.",
            "9": "If the Graph2Seq model is generally applicable to replace the Seq2Seq model, the author should experiment with more tasks.",
            "10": "The paper seems not well motivated."
        }
    },
    "rkeYL1SFvH": {
        "ryecO-JoYS": {
            "0": "The paper presents WikiMatrix, an approach to automatically extract parallel sentences from the free text content of Wikipedia.",
            "1": "The paper considers 1620 languages and the final dataset contains 135M parallel sentences.",
            "2": "The language pairs are general and therefore the data does not require the use of English as a common language between two other languages.",
            "3": "To evaluate the quality of the extracted pairs, a neural machine translation system has been trained on them and tested on the TED dataset, obtaining good results in terms of BLEU score.",
            "4": "The article provides information on the system used to extract parallel sentences and opens up different directions for future investigations.",
            "5": "The dataset seems, from the given details, useful.",
            "6": "However, without access to the data and, more importantly, extensive testing of it, it is difficult to say how and how much it would help the advancement of the field.",
            "7": "For the moment it seems to be good.",
            "8": "However, I am not really sure that this paper could be of interest to a wide audience, except for those involved in machine translation.",
            "9": "In general, the article describes everything at a high level, without going into the real details.",
            "10": "An example of this is on page 6, section 4.2, where the article says that its purpose is to compare different mining parameters, but I do not see any real comparison.",
            "11": "Some words are spent for the mining threshold, but there is no real comparison, while other possible parameters are not considered at all.",
            "12": "For this reason, I would tend to give a low score, which does not mean that the dataset is not good.",
            "13": "It means that the real content of the paper seems to me to be too little to be published at ICLR, since the paper only informs about the presence of this new dataset, saying that it contains a large number of sentences and seems to allow good translations based on the results of a preliminary test.",
            "14": "Typos:\n- on page 9 \"Aragonse\"\n- on page 9, end penultimate line, the word \"for\" is repeated."
        },
        "B1eiO93RYB": {
            "0": "The paper creates a large dataset for machine translation, called WikiMatrix, that contains 135M parallel sentences in 1620 language pairs from Wikipedia.",
            "1": "The paired sentences from different languages are mined based on the sentence embeddings.",
            "2": "Training NMT systems based on the mined dataset, and comparing with those trained based on existing dataset, the authors claim that the quality of the dataset is good.",
            "3": "The effect of thresholding values of similarity scores for selecting parallel sentences is studied.",
            "4": "Since the data is huge, dimension reduction and data compression techniques are used for efficient mining.",
            "5": "The study is the first one that systematically mine for parallel sentences of Wikipedia for a large number of languages.",
            "6": "The results are solid and the dataset is valuable for research in multilinguality."
        },
        "SklmxWok9S": {
            "0": "This ICLR submission deals with an strategy for the automatic extraction of parallel sentences from Wikipedia articles in 85 languages, based on multilingual sentence embeddings.",
            "1": "The review is delivered with the caveat that I am not an expert in this particulat field.",
            "2": "The paper is well written and structured, being within the scope of the conference.",
            "3": "The literature review is very up to date and overall relevant to provide an appropriate context to the investigation.",
            "4": "I reckon this is a very interesting piece of work, but also that it draws too heavily on previous work from which the study is just an incremntal extension.",
            "5": "Minor issues:\nAll acronyms in the text should be defined the first time they appear in the text.",
            "6": "1st sentence of section 2: typo on “comparable coprora”."
        },
        "HygOmHZm2H": {
            "0": "The paper presents a multi-lingual multi-way pseudo-parallel text corpus automatically extracted from Wikipedia.",
            "1": "The authors use a variety of pre-existing techniques applied at large scale with substantial engineering effort to extract a large number of sentence pairs in 1620 language pairs from 85 languages.",
            "2": "In the proposed method 1) raw sentences are extracted from a Wikipedia dump, 2) LASER sentence embeddings and language IDs are computed for each sentence, 3) for each language pair candidate sentence pairs are extracted using a FAISS approximate K-nearest neighbor index on the cosine distance between sentence embeddings, 4) sentence similarity scores are computed between the candidate pairs using the \"max margin\" criterion of Artetxe & Schwenk, 2018 and finally 5) sentence pairs are selected according to a language-pair-agnostic threshold on the similarity scores.",
            "3": "The extraction method is symmetric w.r.t.",
            "4": "language directions for each language pair.",
            "5": "Structural metadata of Wikipedia, such as cross-lingual document alignments, is deliberately not exploited (some discussion is provided but I would have preferred an empirical comparison of local vs global extraction).",
            "6": "The similarity threshold is determined by evaluating training corpora extracted at different thresholds on a machine translation task on De->En, De->Fr, Cs->De and Cs->Fr translation directions, evaluated on WMT newstest2014, and manually selecting the threshold based on BLEU scores.",
            "7": "The paper also reports that combining the automatically extracted corpora with Europarl results in strong BLEU improvements over training only on Europarl.",
            "8": "BLEU scores on TED test sets obtained using only the automatically extracted corpus are also reported.",
            "9": "The corpus has been released.",
            "10": "Overall the methodology presented in the paper is strong and the corpus is likely going to become a valuable tool to build machine translation systems and other multi-lingual applications.",
            "11": "However, I am concerned that ICLR 2020 may not be the appropriate venue for this paper, as in my understanding dataset release papers are not explicitly solicited in the Call for Papers https://iclr.cc/Conferences/2020/CallForPapers .",
            "12": "The corpus generation method is based on existing techniques, and to the extent that the engineering effort is innovative, it might not necessarily transfer well to data sources other than Wikipedia, thus limiting its broad scientific value.",
            "13": "Therefore I suggest to submit the paper to a different venue."
        }
    },
    "Sklgs0NFvr": {
        "HJgaPkj0YH": {
            "0": "This paper seeks to separate \"causal\" features from ones with spurious correlations in the context of natural language machine learning tasks.",
            "1": "The proposed approach is to ask human annotators to alter examples in a minimal way that changes the label.",
            "2": "Thereby the humans separate out the causal features (those changed) from the spurious or irrelevant features (those left unchanged).",
            "3": "Experiments show that classifiers trained on the original data perform poorly on the altered data and vice versa, but (unsurprisingly) training on the union of the two datasets results in a classifier that performs well in both cases.",
            "4": "Furthermore, training an SVM on the original results in irrelevant attributes (such as movie genre) being weighted, whereas these weights are largely removed when training on the union of the datasets.",
            "5": "This suggests that the augmented training data results in weighting the \"right\" features more.",
            "6": "Overall, I think this paper should be accepted because it makes several interesting contributions: It proposes an interesting approach, shows intriguing experimental results, and produces an interesting dataset (size ~2k) that may be useful for future testing.",
            "7": "The main limitation of the paper is that the evidence is largely circumstantial.",
            "8": "The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.",
            "9": "My suggestion for a further experiment would be to apply the movie review classifiers to, say, book reviews -- something where the task is fundamentally the same but the context is different.",
            "10": "If the classifier trained on the union of the original and altered datasets performs better than a classifier trained on only on dataset, then that is strong evidence that this approach yields better extrapolation."
        },
        "rylen8dl5H": {
            "0": "The authors propose a new way to augment textual datasets for the task of sentiment analysis, in order to help the learning methods to generalize better by concentrating on learning the different that makes a difference.",
            "1": "The main idea of the paper is to augment existing datasets with minimally counteractual versions of them, that change the sentiment of the documents.",
            "2": "In this way, all spurious factors will naturally cancel out.",
            "3": "The authors use the newly created datasets and show that indeed, the retrained algorithms on the augmented datasets generalize much better.",
            "4": "The main contribution of the paper is the introduction of the idea of counterfactual datasets for sentiment analysis.",
            "5": "Overall, I find the idea of the paper quite interesting and I’m excited to use the datasets they have created.",
            "6": "However, I think the relative novelty of the paper does not meet ICLR standards, and it’s better suited as a whitepaper attached to an open dataset release."
        },
        "B1eNnTeLqS": {
            "0": "This paper addresses the problem of building models for NLP tasks that are robust against spurious correlations in the data by introducing a human-in-the-loop method: annotators are asked to modify data-points minimally in order to change the label.",
            "1": "They refer to this process as counterfactual augmentation.",
            "2": "The authors apply this method to the IMDB sentiment dataset and to SNLI and show (among other things) that many models cannot generalize from the original dataset to the counterfactually-augmented one.",
            "3": "This contribution is timely and addresses a very important problem that needs to be addressed in order to build more robust NLP systems.",
            "4": "Because, however, of a few limitations, I recommend weak acceptance.",
            "5": "My main hesitation comes from a lack of clarity about the main lesson we have learned.",
            "6": "In particular, if the goal is to use this method to augment the data we use to train NLP systems in order to make them more robust, it seems that the time cost of the process will be prohibitive.",
            "7": "On the other hand, perhaps these methods could be used to identify the kind of spurious correlations that models tend to rely on, which could then be used in a more automated data augmentation process.",
            "8": "If that's the goal, however, a more detailed error analysis would need to be included.",
            "9": "A few small comments:\n\n* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.",
            "10": "I would love to see a more detailed investigation of what annotators usually did.",
            "11": "For instance, a reason that hypothesis-only models do well is that certain words are very predictive of certain labels (e.g.",
            "12": "\"not\" and contradiction).",
            "13": "Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?",
            "14": "That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.",
            "15": "* The BiLSTM they use is very small (embedding and hidden dimension 50).",
            "16": "Given that BERT is most robust against their manipulation, it would be good to see a more powerful recurrent model for comparison.",
            "17": "It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.",
            "18": "Some very minor / typographic comments:\n\n* abstract: \"with revise\" should be \"with revising\"\n* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause\n* page 2, \"We show that...\" I'd break this into two sentences to make it easier to parse.",
            "19": "* Table 3: I would make two columns for each model with accuracy on original versus revised.",
            "20": "With the current table, one has to compare cells in the top half of the table to those in the bottom half of the table, which is quite difficult to do."
        },
        "rylDGP7Zjr": {
            "0": "Summary:\n       The authors take two tasks,sentiment analysis and natural language inference, and identify datasets for them which they counterfactually augment it by asking people over the Amazon Mechanical Turk Platform to change either the sentiment (in the case of sentiment analysis) or the nature of relationship in the NLI task by making minimal changes to the text that produce the targeted changes.",
            "1": "Authors find that popular models trained on either fail on the other dataset while the models trained on both actually generalize much better.",
            "2": "This is because the original sample and its counterfactual pair the label changed , has the difference in the text that matters to the change and this pair could reduce spurious correlations that models might find in the data distribution.",
            "3": "Pros: \n This is a very interesting experiment and certainly the dataset that will be released would be extremely valuable to the community.",
            "4": "The one part (I dont have much NLP background but I do have a causality background) that I like most is that the new text generated are counterfactual in some real sense with respect to a real world generating process - that is people modifying text with changed targets.",
            "5": "A lot of existing work that claim to do counterfactual changes do not specify assumptions about the generating mechanism.",
            "6": "For counterfactuals to be valid they have to be intervention on the actual generating mechanism (or an assumed one) acting on a given unit (latent) that produced the current sample.",
            "7": "The paper in that respect (even if it does not explicitly specify relationship between counterfactuals and generating mechanisms) tries to be faithful to a \"strict causal notion\" by actually asking people to modify the text.",
            "8": "Cons:\n    - I think the authors want to make an explicit connection to counterfactuals as understood in the causality community.",
            "9": "Then they shy away from it saying they are inspired by it.",
            "10": "May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples.",
            "11": "Its good to say what exactly in a counterfactual generation process, the \"people\" in amazon turk were substituting.",
            "12": "-  Is the romantic/ horror flips and their absence the only spurious thing in Figure 4 ?",
            "13": "-  In figure 6, it appears that BERT is sensitive to the domain - does it mean that it is bad ?",
            "14": "- Authors indicate that ideally it must not be so.",
            "15": "Because Table 3 results seem to indicate that BERT performs the best in almost all the cases .",
            "16": "-  Can the authors highlight the best performances in each case in the Tables by a bold face.",
            "17": "It helps easily eye ball the best performing model."
        }
    }
}