{
    "9ITXiTrAoT": {
        "XbwJSltVS5V": {
            "0": "With strong arguments I could be persuaded to change my score.",
            "1": "The paper investigates how well LSTM language models learn the known statistical temporal dependency distributions of languages, which follows a power law.",
            "2": "The authors deduct the expected distribution of LSTM unit timescales and present a method for manually controlling them by setting the forget gate bias using an inverse Gamma distribution.",
            "3": "They show that while the timescales of a standard LSTM LM follow the expected distribution, manually controlling it instead of learning it still increases LM performance, especially for infrequent words.",
            "4": "The findings are also validated on a formal language, where the time scales are explicitly known.",
            "5": "What this paper excels at is a theoretical formulation of the proposed method and a motivation grounded in real contemporary problems in the field of modeling sequential data.",
            "6": "However, I am not confident that the baselines and test suite employed by the authors is sufficient for me to accept their hypothesis.",
            "7": "The LSTM baselines in Table 1 are about 5 ppl points below the reportings of the paper they use as a baseline.",
            "8": "Thus the authors \"improvement\" is actually significantly below the original paper from almost three years ago,which is a long time in this field, current models are now below half of the reported perplexity.",
            "9": "\"Test data were divided into 100-word sequences and resampled with replacement 10,000 times.",
            "10": "For each sample, we computed the difference in model perplexity (baseline − multi-timescale) and reported the 95% confidence intervals (CI) in Table 1.",
            "11": "Differences are significant at p < 0.05 if the CI does not overlap with 0.\"",
            "12": "- This test suite is unfamiliar to me.",
            "13": "Please link a paper which uses the same test suite or follow the standard practices in the field of language modeling.",
            "14": "Also if the authors are looking to argue for better handling longer time sequences, why cut samples off at 100 words?",
            "15": "Shouldn't your model be better at longer sequences?",
            "16": "\"This suggests that the performance advantage of the multi-timescale model is highest for infrequent words, which require very long timescale information\" - It is not intuitive to me that rare words have more long-term dependencies than non-rare words?",
            "17": "Also, I am unsure if the improvement in performance on rare words might just be attributed to the fact that the baseline model is significantly underperforming the original (Merity et al., 18) model.",
            "18": "Do you have confidence intervals for the DYCK language?",
            "19": "Training models on DYCK-2 can be highly seed dependant.",
            "20": "5-10% improvement in performance seems like it could just be random noise.",
            "21": "I don't get Figure 3.",
            "22": "In general the paper is well-written and easy to understand.",
            "23": "UPDATE:\n\nI do not believe that Merity 18's results are not reproducible.",
            "24": "Just look at the vast amount of work after the publication that builds on top of the results.",
            "25": "I don't doubt that your way of evaluating the performance of language models can lead to hypothesis testing, but it is not what the field employs thus your results are not comparable to others, which is why I cannot accept this paper.",
            "26": "If your method really does work as well as you argue it should be no issue obtaining an improvement over the actual baseline.",
            "27": "UPDATE 2:\n\nThe new results with Merity's original benchmark leads me to increase my score from 4 to 7.",
            "28": "I appreciate the effort in reproducing Merity's results."
        },
        "jG0Z5eBi4L": {
            "0": "## Summary\n\nThis work investigates representational power of LSTM to model natural language, in particular how well it models temporal dependencies within text.",
            "1": "They define a notion of timescale of each LSTM unit and analitycally show that LSTM memory exhibits exponential decay, while natural language tends (based on prior work) to decay following the power law.",
            "2": "Based on this, they figure that LSTM memory may decay following the power law *if the timescales approximate samples from the particular Inverse Gamma Distribution*.",
            "3": "To achieve that they propose the multi-timescale LSTM unit, where the desired timescale is explicitly controlled via the forget gate bias.",
            "4": "Authors empirically validate their theoretical claims and show improvements in language modeling (PTB, Wikitext2) over the baseline LSTM using the proposed multi-timscale LSTM.",
            "5": "Importantly, they show how multi-timescale LSTM gives improvement in modeling rare words, which are known to require longer temporal dependencies.",
            "6": "## Strong points\n\n1.",
            "7": "This work investigates the important (though not that popular) question of discrepancy between the temporal dependencies existing in natural language and the abilities of models we use to learn these dependencies in practice.",
            "8": "2.",
            "9": "The idea of including explicit control of the timescale (i.e.",
            "10": "temporal horizon) of each LSTM unit is interesting and well-motivated.",
            "11": "3.",
            "12": "Experiments use a formal language too in addition to natural language modeling which allows to check if proposed approach generalizes in case of exactly computable timescale distribution.",
            "13": "## Weak points\n\n1.",
            "14": "There is **no code** available.",
            "15": "I was interested in the way how test set bootstraping was performed in the experiments (see comments below for details) and found out there is no code, which is really sad.",
            "16": "I hope authors will submit the reproducible code in the near future.",
            "17": "2.",
            "18": "Theoretical part gives some essential quantities while no derivations are given.",
            "19": "Given that there is some free space left in the paper and the fact of unlimitied supplement material I don't see any reason to omit derivations (I struggle with some transitions between equations as you may found in the comments below).",
            "20": "3.",
            "21": "I am not sure how useful is the proposed approach on new tasks given hardly tuned hyperparameters for the Inverse Gamma Distribution proposal and LSTM model architecture for each task (more details in the comments below).",
            "22": "## Recommendation\n\nI vote for accepting **upon fixing major weak points**: uploading reproducible code with experiments and adding all derivations necessary for essential theoretical claims in this work.",
            "23": "Overall this is decent work which will be useful for future research in studying representational power of models we are using to learn complicated dependencies of natural language.",
            "24": "## Questions\n\nAll the questions below are welcome to be used as **suggestions** to provide more details in the manuscript.",
            "25": "### Theoretical part\n\n1.",
            "26": "Eq.3: why can we simply average forget gates?",
            "27": "Out of 'free input' regime $c_t$ from eq.1 would have more dependencies.",
            "28": "Could you elaborate why can we estimate it like this.",
            "29": "2.",
            "30": "How to solve Eq.4 as getting Inverse Gamma Distribution?",
            "31": "I struggle to find an obvious/trivial solution, please elaborate this in the manuscript.",
            "32": "### Experiments\n3.",
            "33": "From 3.1.1.",
            "34": "'*Training sequences were of length 70 with a probability of 0.95 and 35 with a probability of 0.05.",
            "35": "During inference, all test sequences were length 70.",
            "36": "*' Why are you making such explicit scheduling?",
            "37": "Given that each training sequence from WT2 is some excerpt from wikipedia (often longer than 70 words), how do you deal with tails of sequences?",
            "38": "More detailed description of data loading will be helpful.",
            "39": "4.",
            "40": "Table 1: from my understanding columns with rare words attract most interest, and I wonder if you could add the varaiance among different training instances in Table 1 like you reported in the appendix (Table 2)?",
            "41": "Or refactor Table 2 such that it has same freq based columns.",
            "42": "5.",
            "43": "Did you think of tokenization other than word-level?",
            "44": "BPE for example: it gives more balanced token distribution for WT2 due to absence of UNK token there.",
            "45": "I wonder if improvements in terms of rare words PPL will hold.",
            "46": "How do you think (speculatively)?",
            "47": "6.",
            "48": "PTB results: >10K bin PPL got higher with your approach (also ratio drop below 1 in routing study).",
            "49": "Why do you think this happens?",
            "50": "Why do you think this does not happen with WT2 task?",
            "51": "As I see with PTB the fixed forget bias underestimates true/gold high freq word probabilities on average, but why?",
            "52": "### Other\n\n7.",
            "53": "Is it possible to estimate/learn the IGD alpha parameter from the data itself of the task you work on?",
            "54": "This grid search you provide makes me less convinced in how useful this approach is for the new task, where the $\\alpha$ is not known.",
            "55": "8.",
            "56": "How important is the model layers tuning you do?",
            "57": "E.g.",
            "58": "only specific layers have fixed forget bias, but others not, **why is that?",
            "59": "** I am really interested in knowing if other ways of defining your model hurt the performance or keep it on the baseline level?",
            "60": "I am sure this will be useful for all other readers too.",
            "61": "9.",
            "62": "Is it possible to apply this timescale control for other units e.g.",
            "63": "GRU (no explicit forget gate)?"
        },
        "geg1PEZQPtL": {
            "0": "This paper points out the relationship between words in natural language usually follow the power law.",
            "1": "Gated recurrent neural networks such as LSTMs excel in modelling natural language, however, the forgetting mechanism of LSTMs  is ruled by the exponential decay.",
            "2": "This work demonstrates a way to engineer the forgetting mechanism of LSTMs to mimic the power law relationship that is more presented in natural language.",
            "3": "By applying their technique, the modified LSTM model can do better in modelling rare tokens, which usually span for longer timescales, hence the model can score lower perplexities on less frequent words.",
            "4": "The key contribution of the paper is the derivation which shows that the forget gates of LSTMs are subject to exponential decay in zero-input regime after the first input token is given.",
            "5": "And the expected value of exponential decay functions exp(-t/T) can approximate the power law when T is sampled from the Inverse Gamma distribution.",
            "6": "The experiments demonstrate that that drawing T from the Inverse Gamma distribution is a natural fit for natural language.",
            "7": "Then, the authors propose a multiscale LSTM model that exploits this property.",
            "8": "Each timescale T is drawn from the inverse Gamma distribution, which essentially becomes a forget bias term and is fixed during the training.",
            "9": "Multiple Ts are drawn to mimic the power law.",
            "10": "The multiscale LSTM captures the right inductive bias to perform better in modelling less frequent words which might be useful to keep in the memory for longer time.",
            "11": "The paper is well written and both the motivation and explanation of the approach are clear.",
            "12": "The experiments are appropriately designed, and the results support the main claim nicely.",
            "13": "I do have some comments and questions on what's written in the paper though.",
            "14": "It is conventional to use notation h_{t-1} in the update rules for the input, forget and output gates since h_t is obtained from c_t, or at least clarify the update rule of the hidden state as h_{t+1} = o_t * tanh(c_t).",
            "15": "Instead of not learning the forget bias at all, have you tried adding a regularisation loss that forces the forget bias term to stay closer to the prior (initially drawn T from the Inverse Gamma distribution)?",
            "16": "What is the motivation of using smaller hidden size for the topmost layer for both the baseline and the multiscale LSTM?",
            "17": "For the multiscale LSTM, why were the timescales not sampled from the Inverse Gamma distribution and fixed during the training for the topmost layer?",
            "18": "But instead, they were learned like the standard LSTM?",
            "19": "How were T=3 or T=4 chosen for the first layer of the multiscale LSTM?",
            "20": "Were they found by evaluating performance on the validation set or were they chosen based on domain knowledge?"
        },
        "a1e3B8PAKrA": {
            "0": "This paper proposes a novel variant of LSTM by analyzing its behavior against\nscale-free distributions generally found in natural languages.",
            "1": "Since the\nprediction of LSTM is essentially a convolution over each hidden unit, the\nauthors derived that the bias parameter should obey an inverse Gamma \ndistribution.",
            "2": "This is a very neat and interesting result, which is also \nvalidated by a number of experiments in natural languages with scale-free\ndistributions and artificially generated corpus with non scale-free \ndistributions.",
            "3": "My only question is the setup of the proposed LSTM: in Section 3.1.1, the\nauthors say that the first layer of LSTM has a fixed timescale, and only the\nsecond layer has an inverse Gamma bias parameters.",
            "4": "The third layer does not\nhave inverse-Gamma distribution and simply optimized.",
            "5": "Is this architecture necessary for the result?",
            "6": "If so, why the third layer\nshould not have the proposed inverse-Gamma time scales?",
            "7": "Finally, in Figure 3, infrequent words actually use longer time scales, but\nthey also leverage short scales (i.e.",
            "8": "red lines are U-shaped, not linear for\nlonger scales).",
            "9": "I would like to know why this phenomenon happens.",
            "10": "That being said, this is a very interesting paper leveraging the structure of\nLSTM and scale-free property of natural languages.",
            "11": "In addition to Dyck \nexperiments, some other languages, such as a generation from PCFG or some\nprogramming languages might be also interesting for experimentation."
        }
    },
    "xpFFI_NtgpW": {
        "LlWEQtK73-r": {
            "0": "This work studied the impact of embedding coupling in pre-trained language models, by taking a multilingual model as backbone.",
            "1": "The major finding is that decoupling the input and output embedding shapes can bring benefits, and the output embedding plays an important role in the transferability of pre-trained representation.",
            "2": "A rebalanced mBERT is designed by combing and scaling up the investigated techniques, achieving strong results on the XTREME benchmark.",
            "3": "This paper is well written, and the proposed strategy is simple yet effective for obtaining more transferable language representations.",
            "4": "The insights of model design for more efficient fine-tuning are well supported by the analysis.",
            "5": "My concern is about the model efficiency and the true source of performance improvement.",
            "6": "It seems that the number of the parameters is much more than those public ones.",
            "7": "I am curious if it is a fair comparison in Table 7.",
            "8": "The current approach does help in reducing the number of parameters in the fine-tuning stage by increasing E_{out}.",
            "9": "Even in your all experiments, the optimal value of E_{out} appears to be 768 (similar in the baseline), where performance is quite similar to baseline.",
            "10": "However, increasing E_{out} to a much larger value (i.e 3072) drastically increases the number of parameters in pre-training, even as compared to baseline, which seems like a trade-off between fine-tuning and pre-training.",
            "11": "The performance with much larger E_{out} is a marginal improvement over baseline unless the saved parameters are reinvested.",
            "12": "The reinvestment of parameters creates a larger improvement over baseline (Table 6).",
            "13": "The authors must conclude with optimal values of E_{in} and E_{out}, otherwise the paper is merely a series of experiments with different values of E_{in}, E_{out}, and # of layers in the baseline.",
            "14": "Try referencing Table 7 in the main explanation of RemBERT rather than in the Appendix, as it is your major contribution.",
            "15": "But again my concern is the same, the number of parameters in fine-tuning is larger than XLM-R plus the additional pre-training time (more than XLM-R).",
            "16": "Yes, the performance increases but the main objective of the paper is not quite satisfied (reducing the number of parameters in FT which are actually larger in RemBERT than XML-R).",
            "17": "Maybe you can alter the abstract accordingly and focus on reinvestment, which actually is helping in making your point in the experiments.",
            "18": "Minor comments:\n\nCheck the use of \\citep{} and \\citet{}, e.g., in the last sentence of Section 2 and Footnote 10.",
            "19": "Table 7 as main results was never cited in this paper (but only in appendix), which is not a proper organization way.",
            "20": "Page 3-> \"It consists of 12 Transformer layers with a hidden size H of 768 and 12\" the sentence seems incomplete.",
            "21": "Page 5 -> \"increasing the number of Transformer layers L results in the best performance\" I am wondering if reinvestment in L gains higher performance than in H because of the larger number of parameters (10M more).",
            "22": "Maybe you can keep reinvestment the same in both H and L for fair comparison and better results projection.",
            "23": "Section 4: How about a proportional increase in E_{in} and E_{out} affects the performance (e.g increasing or decreasing both with the same proportion, especially decreasing which lines up with paper goal i.e providing flexibility with less number of parameters.)",
            "24": "Page 7 -> \"For both E_{out} = 128 and E_{out} = 768, removing the last layer improves performance\".",
            "25": "Seems in disagree with Figure 1.",
            "26": "Acc with 10 layers is lower than that with 12 layers for E_{out}=128.",
            "27": "Similar is the case with E_{out}=768.",
            "28": "How about the extra pre-training and fine-tuning time compared with the baseline?",
            "29": "For the final rebalanced mBERT described in page 5, how are the hyper-parameters decided?"
        },
        "0r5Css9Y2qM": {
            "0": "This paper systematically studies the impact of embedding coupling with multilingual language models.",
            "1": "The authors observe that while na¨ıvely decoupling the input and output embedding parameters does not consistently improve downstream evaluation metrics, decoupling their shapes comes with a host of benefits.",
            "2": "Moreover, they achieve significantly improved performance by reinvesting saved parameters to the width and depth of the Transformer layers on the XTREME benchmark.",
            "3": "This paper is well-written and strongly motivated.",
            "4": "The idea of decoupling embedding is novel, and the evaluation results are strong.",
            "5": "Strength:\n\n+ The systematical study of the impact of embedding coupling on state-of-the-art pre-trained language models.",
            "6": "This paper also thoroughly investigates reasons for the benefits of embedding decoupling and observes that an increased output embedding size enables a model to improve on the pre-training task, which correlates with downstream performance.",
            "7": "They also find that it leads to Transformers that are more transferable across tasks and languages.",
            "8": "Those empirical results will promote future model design and the understanding of the transformer for textual representation learning.",
            "9": "+ The paper proposes a method to reinvest saved parameters to the width and depth of the Transformer layers and achieve significantly improved performance on the XTREME benchmark over a strong mBERT.",
            "10": "Weakness:\n\n- Some model details are missing.",
            "11": "Although I know what the input and output embedding are, it is still a bit hard to follow in Section 4 and 5.",
            "12": "I strongly recommend that the authors revise those parts and introduce the model details, such as decoupling and model architecture in your experiments.",
            "13": "- Lots of your model designs are empirical such as the embedding size, and it is a bit boring to optimize those hyperparameters, and sometimes we even do not know why it works.",
            "14": "Questions:\n\n- Could you please introduce the model details of your decoupled model in Section 4?"
        },
        "959iTeqc8ov": {
            "0": "Summary:\n \nThis work investigated the strategy of reallocating parameters of multilingual language models for improving their cross lingual transferability.",
            "1": "Authors first decoupled the input and output embeddings and showed that the capacity of output embedding is more important than input embedding.",
            "2": "Then,  they proposed a Rebalanced mBERT (RemBERT) model that reallocates the input embedding parameters of mBERT model to the output embedding and additional layers.",
            "3": "Experimental results on XTREME benchmark showed that RemBERT significantly outperformed XLM-R with similar model size.",
            "4": "Pros:\n \n- The paper is well written and easy to follow.",
            "5": "The comparison of embeddings parameters ratios of different language models in Table 1 gives a very good motivation of reallocating the parameters of embeddings.",
            "6": "- Authors conducted several ablation studies to understand how the capacity of  different parts of the model (e.g., input and output embeddings, transformer layers) contributes to the final performance.",
            "7": "Cons:\n\n- Reallocating the parameters of input embedding to additional transformer layers might affect the pre-training and inference speed, it will be helpful to show the training and inference speed of different parameter reallocation strategies.",
            "8": "Questions:\n\n- Why are the pre-training and fine-tuning parameters of coupled and decoupled models the same in Table 2?",
            "9": "Shouldn’t decoupling the input and output embeddings double the parameters of the embeddings?"
        },
        "HN8h7VWbbR7": {
            "0": "Transformer based bidirectional LMs pre-trained using Masked Language Model loss typically share input and output token embeddings.",
            "1": "This paper makes an interesting investigation about decoupling input and output embeddings and gains which can be obtained out of this decoupling.",
            "2": "In particular, this paper shows that the pre-training performance of transformers and the transferability of the learned representations can be improved by increasing the dimension of output embeddings while reducing the dimension of input embeddings.",
            "3": "Better performance while pre-training and improved transferability further helps the performance while finetuning on downstream tasks.",
            "4": "Parameters saved while reducing the dimensions of input embeddings can be re-invested into increasing the depth or width of the transformer layers.",
            "5": "I believe that the findings in this paper are going to be useful in practice to the general NLP community working on Transformers and Multilingual problems.",
            "6": "Weak Points:\n1.",
            "7": "In table 3, (E_in=768,E_out=128) is just 0.1 point worse on average than (E_in=128, E_out=768).",
            "8": "Yet, authors draw some conclusions on Page 4 based on this table, without reporting any variance or statistical significance tests.",
            "9": "E.g.",
            "10": ">  the model pretrained with a larger output embedding size slightly outperforms the comparison method on average despite \n    having 77M fewer parameters during fine-tuning\n\n    > Reducing the input embedding dimension saves a significant number of parameters at a noticeably smaller cost to accuracy \n    than reducing the output embedding size.",
            "11": "I would request the authors to report the variance in Table 3 and also in Table 9.",
            "12": "Otherwise, it’s hard to rely on conclusions drawn from such minor differences in performance.",
            "13": "Strong Points:\nStrong empirical results.",
            "14": "1.",
            "15": "Table 4 clearly shows that increasing the dimension of output embeddings improves transfer to downstream tasks while having the same number of trainable parameters during the finetuning stage.",
            "16": "2.",
            "17": "Table 6 shows that reinvesting the parameters saved from reducing the dimension of input embeddings into additional transformer layers yield improved performance.",
            "18": "Further, RemBERT performs at par with XLM-R while having a comparable number of trainable parameters during the finetuning stage.",
            "19": "3.",
            "20": "Sufficient amount of analysis in Section 6 to establish the usefulness of having higher dimensional output embeddings for improved pre-training and better transferability of the learned representations across tasks.",
            "21": "Other comments/questions:\n1.",
            "22": "Shouldn’t #PT params in table 2 for the “Decoupled” method be more than #PT params for the “Coupled\" method ?",
            "23": "2.",
            "24": "More figures like Figure 1, on other tasks in addition to XNLI, would be really helpful in making the observations more conclusive."
        }
    },
    "XPZIaotutsD": {
        "tAB4kErmoV": {
            "0": "The paper proposes a BERT-inspired model that adds a two main different architectural decisions: different content and position representations (instead of a sum), and absolute positions in the decoding layer.",
            "1": "The authors run the standard suite of GLUE benchmark experiments, on both “large” and “base” setups, as well as a generation setup (Wikitext-103).",
            "2": "The modifications proposed are not game-changing, but the evaluations are interesting in terms of understanding the impact of these modifications.",
            "3": "One thing that I find disingenuous is fact that their disentangled approach does introduce additional parameters, which is not quantified (or even mentioned) in the main paper.",
            "4": "I had to dig into the Appendix to see that this introduces about 49M additional parameters (increment of 13%).",
            "5": "Another problem that I have is with their experimental comparisons, especially the ones in main part, Sec 4.1.1.",
            "6": "I’m listing below the most important issues in this section:\n\n“RoBERTa and XLNet are trained for 500K steps with 8K samples in a step, which amounts to four billion passes over training samples”.",
            "7": "This is confusing; what you mean to say is that the models see about four billion training examples.",
            "8": "The term “passes” is used usually as an equivalent to “epochs”, ie how many times the model goes over the entire training set.",
            "9": "“[...] Table 1, which compares DeBERTa with previous models with around 350M parameters: BERT, RoBERTa, XLNet, ALBERT and ELECTRA.” Note that ALBERT is actually around 235M parameters, significantly less than all the others.",
            "10": "You cannot simply bundle all together and claim they are equivalent parameter-size--wise.",
            "11": "“DeBERTa still outperforms them [ALBERT_xxlarge] in term  of the average “GLUE” score.” Note that the difference here wrt ALBERT_xxlarge is from 89.96 to 90.00, ie 0.04 for the average, with a tie 3-3 in terms of wins for specific tasks.",
            "12": "Unless you can show that the 0.04 difference is statistically significant, you need to tone down the claim about “outperforming”.",
            "13": "“We summarize the results in Table 2.",
            "14": "Compared to the previous SOTA models with similar sizes, including BERT, RoBERTa, XLNet and Megatron336M, DeBERTa consistently outperforms them in all the 7 tasks.",
            "15": "Taking RACE as an example, DeBERTa is significantly better than previous SOTA XLNet with an improvement of 1.4% (86.8% vs. 85.4%).”\nFor whatever reason, the authors omit ALBERT from the comparison done for Table 2, in spite of its even smaller size compared to the included ones, and the fact that the ALBERT numbers for these tasks are readily available in the paper.",
            "16": "Taking RACE as an example: ALBERT (single model) has 86.5% accuracy, therefore nullifying the claim of 1.4% improvement.",
            "17": "Re: References\n\nA lot of the references use the Arxiv version for papers that have been peer-reviewed and published.",
            "18": "Please fix."
        },
        "tM7jxs-mDwp": {
            "0": "In this paper, an improvement of BERT model is proposed.",
            "1": "It relies on the disentanglement of contents and relative positions in the encoding layers and on the incorporation of absolute positions in the decoding layer.",
            "2": "Strengths:\n* The paper is well written, the positioning to the state of the art is clear and the method is rigorously described.",
            "3": "* The paper provides a complete evaluation using the existing benchmarks for NLP and including ablation studies and evaluation of pre-training efficiency and Deberta improves results in the major part of the cases.",
            "4": "Weaknesses:\n* The proposed method is a relative increment of previous methods.",
            "5": "* In Section 4.1.1., the way performance increase or decrease is reported is not exact (1.1% -> 1.1 points)\n* Do we have an idea of the statistical significance of the improvements?",
            "6": "* It would be interesting to have the rationale for the mitigated result obtained on Table 1.",
            "7": "Is Deberta more relevant for specific tasks?",
            "8": "* The authors claim that they evaluate their results on generation task but it rather seems that they evaluate language modeling using perplexity.",
            "9": "*The use of non documented acronyms (ppl, for example) that could be not understandable outside the NLP community.",
            "10": "*They are some redundancy in the text (second paragraph of 3.2 and fourth paragraph of the introduction) that is not necessary."
        },
        "WYwTmQDzGb3": {
            "0": "Summary and Contributions\n\nThe authors proposed an extension to the word representation transformer architecture  that takes into account disentangle features for position and content.",
            "1": "The disentangle of attention is based on the composition of a content and position parameter matrices, in addition with combinations of both.",
            "2": "The main contribution is to tackle issues with the relative position embeddings used on standard transformer architectures.",
            "3": "The proposed model shows improvements on some benchmarks by using less pre-training data compared to the baseline.",
            "4": "Strengths\n\n- The proposed model tackles a known issue in transformer architectures.",
            "5": "- The authors perform a comprehensive comparison on standard text benchmarks as well as an ablation study.",
            "6": "- The findings show that disentangle attention improves results on some text benchmarks.",
            "7": "Weaknesses\n\n- Related work on disentangle representations for text, and the further motivation for using disentanglement into the attention model are not discussed.",
            "8": "- Missing results of the variance in metrics with multiple runs on the downstream tasks.",
            "9": "As an extra contribution, the authors could  show if the improvements are due to the proposed model or variance in parameter initialisation.",
            "10": "Questions to the Authors\n\n- Could you elaborate on disentangled representations and how they relate to the proposed attention model?",
            "11": "- How does it compare the enhanced masked language model with the masked language model?",
            "12": "- How does the relative position parameter matrix is initialised, and how does it affect the language model performance?"
        },
        "hbRWP5lM16H": {
            "0": "The paper proposed a novel attention mechanism and a new objective function that mitigates the distribution shifts caused by masked tokens for downstream tasks in MLM.",
            "1": "It demonstrates superior performance across benchmarks.",
            "2": "Pros:\n1.",
            "3": "Good empirical results are demonstrated across an extensive suite of benchmarks.",
            "4": "Ablation studies are well done.",
            "5": "Hence I am willing to give a score of 6 despite of the following concerns.",
            "6": "Cons:\n1.",
            "7": "My major concern is about the novelty of this paper.",
            "8": "In transformer-XL[1], the idea of relative positional information in the form of Eq (2) was already introduced.",
            "9": "The paper somehow intentionally omit the discussion following (2), only mentioning two earlier works of (Shaw et al., 2018; Huang et al., 2018).",
            "10": "I think the author should be honest and compare with relative positional information introduced in transformer-XL in the forefront.",
            "11": "That being said, there is obviously still differences between transformer-XL and the proposed methods.",
            "12": "And also the introduction of novel objectives in addition to the attention mechanism.",
            "13": "2.",
            "14": "However, the previous concern brought up the second concern I have about the evaluations.",
            "15": "Since the modification relative positional information of transformer-XL to the proposed method is not too large, I wonder if there is a reason to explain the better performances of the proposed methods.",
            "16": "Hence I am worried if the baseline such as XLNet was well-tuned.",
            "17": "We can see that for example in [2], the performance of XLNet was much better than originally reported.",
            "18": "I think the author should try to carefully evaluate the relative positional mechanisms of prior works with authors' own infrastructure, while having everything else fixed.",
            "19": "3.",
            "20": "I find the word \"disentangled\" a bit misleading in this context.",
            "21": "Disentanglement in ML [3] often refers to the ability to disentangle factors of variations of the data.",
            "22": "The work does not make use of any disentangled techniques, or have disentanglement representation/architectures.",
            "23": "It simply use a relative position mechanism that's the sum of four matrix products.",
            "24": "[1] Dai et.",
            "25": "al.",
            "26": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\n\n[2] Dai et.",
            "27": "al.",
            "28": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing\n\n[3] Locatello et.",
            "29": "al., Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations"
        }
    },
    "KxUlUb26-P3": {
        "Gn3_Bi66Lan": {
            "0": "##########################################################################\nSummary: \nThis paper proposes a unified PAC-Bayesian-based informativeness measure (PABI) to quantify the value of incidental signals.",
            "1": "PABI can measure various types of incidental signals such as partial labels, noisy labels, constraints, auxiliary signals, cross-domain signals, and their combinations.",
            "2": "In NER and QA tasks, they showed the strong correlation signals between PABI and the relative improvements for various incidental signals.",
            "3": "##########################################################################\nReasons for score: \n \nOverall, my score is marginally below than acceptance threshold.",
            "4": "Pros:\n1.",
            "5": "I enjoyed reading the paper, and I like the idea of covering various types of supervision signals at one unified measure.",
            "6": "2.",
            "7": "The definition and approximation of PABI and its generalization to different inductive signals look sound to me.",
            "8": "Cons: \n \n1.",
            "9": "My biggest concern about this paper is the lack of clarity and presentation.",
            "10": "In the introduction, I do understand how conceptually PABI is different from others, but do not know what it is.",
            "11": "It would be better describing how PABI works in the introduction, Also, it would be better understanding the Section 2 and 3, if authors provide high-level insights of why each part of PABI’s description is important.",
            "12": "Similarly, in the experiment, it was quite difficult to follow the text and capture the main claim.",
            "13": "For instance, it would be better to understand if how Figure 2 and 3 should look like first and what trends of the points support the main claim of PABI, etc.",
            "14": "Similarly, visual interpretation without specific guidelines make Figure 3 really difficult to understand.",
            "15": "I guess some quantitative numbers would be very helpful like the linear regression slope, etc.",
            "16": "2.",
            "17": "Besides the presentation, I don’t quite understand how PABI can be used as a practical measure for other applications.",
            "18": "Does the strong correlation with relative improvement mean that it can be used as an alternative measure of mutual information and further applied to other applications using such information measures in their optimization?",
            "19": "If so, it would be nice to describe potential applications of these measures and other benefits of PABI in general.",
            "20": "This also requires additional experiments that show its effectiveness in other applications.",
            "21": "#########################################################################\nSome typos:\na widely used measure for for noisy signals -> a widely used measure for noisy signals\nthe SQuAD dataset servers as the main dataset -> the SQuAD dataset serves as the main dataset"
        },
        "8nM23j0OJjT": {
            "0": "This paper proposes PABI (PAC-Bayesian Informativeness?",
            "1": "), a way of measuring and predicting the usefulness of “incidental supervision signal” for a downstream classification task.",
            "2": "In particular, when labeled data is only available in noisy or partial form, or over a different domain than the target test domain, this data may still be used to improve a classifier, but it’s unclear how to tell which forms of incidental supervision will be most useful.",
            "3": "Having a measure which allows us to compare different types of such supervision enables us to make intelligent tradeoffs.",
            "4": "PABI is proposed as a very general framework.",
            "5": "The most general form of the measure, dealing with updates to the concept class prior, seems that it could capture any kind of incidental supervision.",
            "6": "However, this means most of the work is in understanding how to apply and approximate it.",
            "7": "This paper provides several such methods, particularly focusing on “inductive” learning (from constraints or partial/noisy gold labels) or “transductive” learning (from complete gold labels on different input domains).",
            "8": "Mathematical developments of PABI are given for these cases, and experiments show that PABI is nicely positively correlated with the relative improvement that comes with various methods for integrating incidental supervision signal (including one which is developed as a side note by the authors).",
            "9": "Computing PABI may be challenging in some cases.",
            "10": "In the case of transductive learning, it seems that a model needs to be trained on the incidental signal, although this is better than the combinatorial explosion of jointly trained models that would be required to test relative improvements directly.",
            "11": "However, it’s not clear if efficient approximations for PABI will be feasible in all cases.",
            "12": "This and other questions about the breadth of application of PABI are left for future work.",
            "13": "### Strengths\n\nI think this paper is very well-motivated, situates itself well with respect to previous work, and presents clear advantages.",
            "14": "Having a unified framework for comparing the utility of different kinds of incidental supervision signals seems potentially very useful, especially these days when incidental supervision of various sorts is instrumental in state-of-the-art models.",
            "15": "It is also extremely relevant for data annotation and task design, which often has to make tradeoffs between these factors (i.e., noise versus partial annotation or dataset size).",
            "16": "There is a lot of content in this paper, including mathematical developments, algorithms, and experimental results.",
            "17": "While I did not carefully check the proofs in the appendix, and I am not familiar with PAC-Bayesian theory or the associated literature, the paper seems technically sound to me.",
            "18": "### Weaknesses\n\nWhile the generality of the proposed PABI framework is great and improves over existing work, I think this paper could be scoped more carefully and the scope could be clarified better.",
            "19": "As proposed, the PABI framework seems very general—which is good.",
            "20": "But the paper only shows how to realize the framework in a couple specific cases, for “inductive” and “transductive” learning independently.",
            "21": "This is still more general than previous work, but from the first few pages of the paper I was expecting something even more general.",
            "22": "* It seems to me that the combination of inductive and transductive learning may be possible using something close to the paper's proposed methods , but this isn’t addressed by the paper except a glancing mention in Footnote 6.",
            "23": "* It also is not clear to me from the paper’s text whether something close to the PABI framework can apply in broader settings like language modeling style pretraining, where the input-output format of the incidental supervision signal is different than that of the target task.",
            "24": "In particular, it seems that in this case the approximation method proposed for transductive learning would indeed have to reduce to training a combined model.",
            "25": "Related issues were finally mentioned briefly in the last paragraph of the paper, and something along these lines appears in appendix A.3, but I think a more up-front clarification of the limitations is warranted.",
            "26": "More broadly, the question in the back of my head when I began reading the paper was if this would help explain why and when language model pretraining (and other more flexible related-task pretraining) works well.",
            "27": "The paper points to related work in this area, such as Gururangan et al 2020 (“Don’t Stop Pretraining”), leading me to think this paper would shed light on the issue, but in the end the issue was not mentioned and seems perhaps out of scope.",
            "28": "This is fine.",
            "29": "All I would ask of the authors is to be more explicit about the limitations of PABI (or the proposed realizations of it) from the beginning, laying out the scope of this work and stating the limitations outright instead of only pointing to the appendix.",
            "30": "It seems to me like PABI is more of a foundational framework which is ideal for future work to build into, rather than already being a general solution in itself.",
            "31": "I think it would be best to pitch the paper this way.",
            "32": "### Recommendation\n\nAccept.",
            "33": "Important problem, lots of solid content, clear benefits over previous work and directions for the future.",
            "34": "Great work.",
            "35": "### More comments/questions\n\nI think the point of the formulation in Section 2.2 can be made a bit more explicit.",
            "36": "It seems like the point is for applying PABI to partial labels.",
            "37": "If that’s true (or there’s more to it) then might as well just say it there, or at least give this case as a motivating example.",
            "38": "Regarding the cross-domain results: why are the incidental supervision sets so small?",
            "39": "It seems that there is a ton more incidental supervision available for NER, and in both cases the incidental supervision data is even smaller than the test set.",
            "40": "Why not use more?",
            "41": "It seems to me that the use case here is when a large amount of incidental supervision is available anyway.",
            "42": "It also seems like the low-data setting is not totally fair to the vocabulary overlap baseline.",
            "43": "### Typos, style, etc.",
            "44": "When describing your experiments, I think it’s worth mentioning that they are on English text.",
            "45": "Figure 3: I don’t understand which numbers correspond to which model in the caption.",
            "46": "This would be much easier to read in a table.",
            "47": "* P. 7: something’s wrong with “twitter(Strauss et al., 2016)”\n* P. 7: The FitzGerald et al 2018 dataset is called “QA-SRL Bank 2.0”.",
            "48": "* P. 7: servers -> serves\n* P. 7: “the lower bound for is”"
        },
        "MPyUxej_rG6": {
            "0": "\n#### Summary\n\nThis paper proposes a unified measure for the informativeness of incidental signals (ie, not standard ground truth supervised labels) derived from the PAC-Bayesian theoretical framework.",
            "1": "Instantiations of the score are derived for a variety of these signals, and experiments show good agreement between the measure and true performance improvements.",
            "2": "#### Strong and weak points\n\nThis problem setting is well-positioned as complementary to the growth in \"alternative supervision\" in both research and industry.",
            "3": "Besides the directions identified in the paper, one could easily imagine using this kind of a measure in ML applications as a tool to help guide economic decisions about what kinds of datasets or annotations to pursue.",
            "4": "The explanatory potential of this approach with respect to observed gains using incidental signals is exciting as well, especially the agreement with empirical findings from Mayhew 2019.",
            "5": "I found Figure 1 to give helpful context, and I found the core technical content in Section 2 to be clear and precise.",
            "6": "The experimental results were a bit intricate to follow.",
            "7": "A key result is Figure 2f and the associated correlations, which show strong correlation between the PABI scores and true performance improvements, this could perhaps be higlighted or emphasized more.",
            "8": "Likewise the meaning of Figure 3 is a bit obscured by the poor correlations of the\nbaselines.",
            "9": "One weakness of the evaluation was that, while the Related Work coverage seemed sufficient, only Gururangan 2020 is included in the experiments.",
            "10": "Of course the other approaches have the limitations well-captured in Table 1, but it would have been nice to have some restricted experiments crafted in order to give direct comparisons.",
            "11": "The supplemental appendix was comprehensive with respect to theoretical derivations and experimental details.",
            "12": "#### Recommendation (accept or reject) with one or two key reasons for this choice.",
            "13": "I would recommend to accept, the work represents an advance across both theory and practice on an important problem.",
            "14": "#### Supporting arguments\n\nThe work leverages a well-studied framework to answer important questions about understanding the utility of non-standard supervision signals, enabling us to reason in a unified way about varied kinds of these signals as well as their combinations.",
            "15": "Experimental results\n\n\n#### Questions to clarify / additional evidence required\n\nThe approximation in Definition 2.2 was a little strange for me, and seemed kind of circular: for calculating our approximate PABI, we are approximating the target (gold) distribution with our approximatively improved prior ($\\tilde{pi_0}$)?",
            "16": "Is there anything we can say about how good/accurate this approximation is?",
            "17": "Section 3.2: \"much cheaper\" - how or why would we say this is true, can we quantify it?",
            "18": "Or are there cites to see?",
            "19": "Is it possible to frame the PABI measures in terms of testable hypotheses about true generalization error, or are the bounds too loose in practice to say anything meaningful here?",
            "20": "#### Additional feedback to improve\n\nSpace permitting, a small diagram of the mappings between different domains and the restrction trick would make Section 3.2 much clearer.",
            "21": "Another possibility is some symbol table to keep straight which versions of $c()$ correspond to gold vs silver, incidental, etc.",
            "22": "The code was great to see as well but is missing dependencies:\n\n- seqeval\n- tqdm\n- transformers\n\nI might suggest adding a requirements.txt or similar."
        },
        "lrCxMJ9PX-C": {
            "0": "The use of PAC-Bayes theory for NLP tasks is rare.",
            "1": "Although I know little on NLP, the paper proposition to leverage on  PAC-Bayes for evaluating the benefit of various incidental supervision signals seems promising.",
            "2": "However, even if the empirical results are good, the connection between PAC-Bayes and the proposed informativeness measure (named PABI) is vague.",
            "3": "The paper needs to better situate the proposed analysis compared to classical PAC-Bayesian generalization risk bounds.",
            "4": "**Section 2 contains many assertions that are questionable.",
            "5": "**\n1.",
            "6": "*\"The training samples [are] generated i.i.d.",
            "7": "\"*: It is the case for most PAC-Bayes analysis, but I wonder to which extent this assumption holds for the NLP problems studied as experiments.",
            "8": "In a sentence, words are highly dependent on each other.",
            "9": "2.",
            "10": "*\"In the common supervised learning setting, we usually assume the concept that generates data comes from the concept class\"*: This is a surprising claim as the **PAC**-Bayes framework differs from the Bayesian one namely by the fact that we usually don't need to make assumptions about the data-generating distribution other than being i.i.d.",
            "11": "In particular, the model does not need to be well specified.",
            "12": "This makes me wonder if PABI would not better fit in the purely Bayesian framework (see other comments below).",
            "13": "3.",
            "14": "*\"the generalization bounds in both PAC-Bayesian and PAC frameworks have the square root function\"* : There exist several forms of the PAC-Bayes theorem in the literature, not only the square root ones (e.g., Seeger 2002).",
            "15": "In fact, the square root bounds are not the tightest, particularly when the model is well specified.",
            "16": "**Is PABI really backed by PAC-Bayes theory?",
            "17": "**\nAs far as I understand, the procedure PABI is only remotely inspired by the PAC-Bayes bound,  but is not truly justified by it.",
            "18": "No PAC-Bayes bounds are fully optimized; PABI borrows from PAC-Bayes the sole idea of relying on the KL between distribution.",
            "19": "For this reason, I think that the introduction sentence \"Previous attempts are either not practical or too heuristic\" is harsh, because the proposed method turns out to be a heuristic too.",
            "20": "**Is PABI a more Bayesian method than a PAC-Bayes one?",
            "21": "**\nI wonder if one could not do the same analysis in a fully Bayesian setting, maximizing a Bayesian information criterion.",
            "22": "This should be appropriate since PABI and the Bayesian setting assume that the model is well specified.",
            "23": "Note that there is a direct link between the Bayesian Marginal Likelihood and the PAC-Bayes generalization bound (e.g., Germain, et al., 2016: \"PAC-Bayesian Theory Meets Bayesian Inference.\")",
            "24": "Overall, I think that the paper explores a new and exciting territory, but needs a deeper analysis to support the connection with the PAC-Bayes theory."
        }
    },
    "vVjIW3sEc1s": {
        "7c16GW4v4AU": {
            "0": "**Summary**\nThis work relates a pre-training performance with a downstream performance for tasks that _can_ be reformulated as next word prediction tasks.",
            "1": "The authors show that for such tasks, if the pre-training objective is $\\epsilon$-optimal, then the downstream objective of a linear classifier is $\\mathcal{O}(\\sqrt{\\epsilon})$-optimal.",
            "2": "**Strengths**\n- To the best of my knowledge, this is the first work that _mathematically_ justifies the connection between the pre-training objective and the downstream performance.",
            "3": "- The proof technique (pre-training performance $\\to$ covariance of pre-training errors $\\to$ covariance of downstream errors $\\to$ downstream performance) is itself interesting.",
            "4": "If the paper is accepted I am looking forward to seeing a high-level proof sketch in the main part as is done in Section 2.1 of [Arora et al.",
            "5": "(2015)](https://arxiv.org/abs/1502.03520).",
            "6": "A three-line explanation at the end of Section 4.1 seems a bit scarce to me.",
            "7": "- The paper is well written: it gives an appropriate context, presents the main theoretical results, and verifies _some_ of the claims experimentally.",
            "8": "**Major concern**\nIf I understand correctly (and please correct me if I am wrong), in Theorem B.1, the ratio between the downstream error $\\ell_\\mathcal{T}(\\\\{p_{\\cdot\\mid s}\\\\}) - \\tau$ and the pre-training error $\\ell_\\text{xent}(\\\\{p_{\\cdot\\mid s}\\\\})-\\ell_\\text{xent}^\\ast$ is _hidden_ in the $\\gamma(p_{\\mathcal{T}}; \\\\{p_{\\cdot\\mid s}\\\\})$ coefficient.",
            "9": "Let me elaborate on this:\n1.",
            "10": "In Lemma D.1 (with the help of Lemma D.9) you show that $\\frac{1}{\\gamma(p_{\\mathcal{T}}; \\\\{p_{\\cdot\\mid s}\\\\})}$ is an upper bound for the ratio $\\frac{\\boldsymbol{v}^\\top\\Sigma_{p_{\\mathcal{T}}}\\boldsymbol{v}}{\\boldsymbol{v}^\\top\\Sigma_{p_{L}}\\boldsymbol{v}}$\n2.",
            "11": "The latter ratio seems proportional to the ratio $\\frac{\\ell_\\mathcal{T}(\\\\{p_{\\cdot\\mid s}\\\\}) - \\tau}{\\ell_\\text{xent}(\\\\{p_{\\cdot\\mid s}\\\\})-\\ell_\\text{xent}^\\ast}$.",
            "12": "I am not sure on this---my intuition is based on your Lemma D.2 and the fact that for a $p_{\\cdot\\mid s}$ with full support a non-precise reverse version of [Pinsker's inequality](https://en.wikipedia.org/wiki/Pinsker%27s_inequality#Inverse_problem) holds.",
            "13": "In a nutshell, aren't you showing that\n$$\\text{downstream error}=\\mathcal{O}\\left(\\sqrt{\\text{pre-training error}\\cdot\\frac{\\text{downstream error}}{\\text{pre-training error}}}\\right)\\qquad ?$$\n\n**Issues**\n- Why don't you verify the main claim---$\\epsilon$-optimality in pre-training propagates as $\\mathcal{O}(\\sqrt{\\epsilon})$-optimality on downstream---empirically?For this, you may want to vary the language modeling performance (e.g.",
            "14": "by pruning the language model) and then verifying that the downstream loss increase is indeed $\\mathcal{O}(\\sqrt{\\text{pre-training loss increase}})$.",
            "15": "I believe such an experiment will definitely make the submission stronger.",
            "16": "- I don't see why your theory does not generalize to a _masked_ language modeling (MLM).",
            "17": "Why do we need to treat $s$ as the left-context only?",
            "18": "Given the success of MLM as a powerful pre-training objective, please consider formulating your claims in a more general way.",
            "19": "**Minor issues**\n- At the beginning of Section 2.3, $p_{\\mathcal{T}}$ is introduced as a distribution over $\\mathcal{S}\\times\\\\{\\pm1\\\\}$, but later (e.g.",
            "20": "in formula (5)), it is used as a distribution over $\\mathcal{S}$ only.",
            "21": "Please clarify/fix this.",
            "22": "- What is the \"margin of task $\\mathcal{T}$\" mentioned on p.5?",
            "23": "Is it a margin of an SVM classifier that solves $\\mathcal{T}$?",
            "24": "**Limitations**\n- The authors admit that their work is limited to a particular type of downstream tasks.",
            "25": "Indeed, it is not clear how one can reformulate e.g.",
            "26": "linguistic tasks (like POS-tagging or dependency parsing) as a next word prediction task.",
            "27": "**Update (after the author's response)**: During the rebuttal, the authors clarified my major concern, as well as provided additional experiments that verify the main claim of the paper.",
            "28": "I am totally satisfied with the author's response, hence I am changing the score 6 $\\to$ 7"
        },
        "Ljbfz8JWK8D": {
            "0": "This paper studies why language model pre-training has been such an effective technique in improving downstream performance across a wide range of NLP tasks recently.",
            "1": "In particular, it considers language models which compute a probability distribution over the next word in a text, given the previous context.",
            "2": "Then, taking inspiration from recent work that shows that many downstream tasks can be reframed as sentence completion tasks, it defines a “natural task” as one on which a sparse linear model over the output of the “true” language model (next word probability distribution, conditioned on context) attains strong performance.",
            "3": "Theoretically, it shows that language models which are close to the “true” language model are guaranteed to attain strong performance on natural tasks.",
            "4": "Empirically, it demonstrates that several NLP tasks are “natural”.",
            "5": "**Strengths**\n- The paper is generally quite clearly written and the claims are well-validated.",
            "6": "- The definitions, models, and assumptions in the paper are intuitive and clear (e.g., natural task).",
            "7": "- The analysis which builds on these definitions/models/assumptions provides meaningful theoretical insight into why language model pre-training may be so beneficial for downstream training.",
            "8": "It provides a nice theoretical framework for thinking about the connection between language models and downstream tasks, which future work could build on.",
            "9": "- The empirical validation is thoughtful and relatively thorough.",
            "10": "Even though the results don’t show that the proposed loss function and proposed “conditional mean features” give improvements over baselines, the empirical results show that the basic assumptions and definitions in the theoretical analysis are relatively realistic.",
            "11": "For example, Figure 4 validates Assumption 4.1 (log-partition function is roughly quadratic in theta), and Table 1 shows many real tasks are approximately “natural”.",
            "12": "Furthermore, when there is a gap between the empirical results and the theoretical results (e.g., validation of Lemma 4.3 at the end of Section 4), the paper makes these limitations clear, which I appreciated very much as a reader (paper does not over-claim its contributions).",
            "13": "**Weaknesses**\n* Unclear if there are real practical applications to the insights from this paper.",
            "14": "Neither the proposed “Quad” loss function, nor the theoretically inspired “conditional mean features”, perform better than the baselines.",
            "15": "* The current analysis doesn’t apply directly to BERT, which is trained to predict masked words in a sentence, instead of the next word.",
            "16": "Furthermore, BERT doesn’t predict these masked words using a linear softmax model over a contextual embedding for the whole sentence, which is the assumed structure for the softmax language models considered in the analysis.",
            "17": "(This limitation is acknowledged in the conclusion, which is good).",
            "18": "* The paper doesn’t explain why learning a linear model directly on the context embeddings f(s) performs better than using the contextual mean embeddings.",
            "19": "* One idea I had here: Could you define a natural task as one for which there exists a sparse linear model over the *logits* of p*( .",
            "20": "| s) which performs well, instead of a model directly over p*( .",
            "21": "| s)?",
            "22": "Due to the very “flat” portions of the softmax function, there can be meaningful differences between the logits corresponding to 2 different words, but the LM probabilities for those words are extremely similar (and thus, harder for a linear model to distinguish).",
            "23": "With this definition, a linear model of the logits is also a linear model over the context embeddings f(s) directly.",
            "24": "* There are some points in the paper that could be made clearer.",
            "25": "* I think it should be discussed earlier (in intro/related work) why the paper focuses on language models which do next word prediction via linear softmax models over fixed dimensional context embeddings, and that BERT is out of scope.",
            "26": "* I think there should be more discussion about the implications of Proposition 2.2.",
            "27": "As I understand it, this result shows that any part of p_f(s) orthogonal to row-span(Phi) doesn’t affect the cross-entropy of the language model (first order optimality condition would still be satisfied).",
            "28": "However, this doesn’t necessarily imply that p_f(s) will be in span(Phi) for all contexts s.  In particular,  the architecture of the embedding model f likely constrains f in such a way that makes it impossible for p_f(s) to be in span(Phi) for all contexts s.  Furthermore, at the end of section 3 it should be better explained why the assumption that p_f(s) is in span(Phi) for all s implies that Definition 3.2 should only consider sparse models v which are in this span as well (decompose v = v_in + v_out (component of v in the span, and orthogonal to the span), v^T p = (v_in + v_out)^T p = v_in^T p).",
            "29": "* I found the discussion in Section 4.1 pretty confusing.",
            "30": "In particular the part that argued why B = O(1/alpha).",
            "31": "Overall, I really enjoyed reading this paper, and found it to be quite insightful.",
            "32": "It provided me with a much more thoughtful explanation for why language model pre-training improves downstream task performance, beyond simply “it helps learn good general representations of language using large amounts of unlabeled text data” (my previous reasoning).",
            "33": "As a result, I recommend acceptance for this paper.",
            "34": "NIT: \n- Grammar last sentence of Section 1.1 (“…analyze the efficiency *of* …”)\n- Proposition 2.2: Maybe write “\\forall s \\in S” instead of “\\forall s ~ p_L”.",
            "35": "- Section 3: ...append a prompt like “This movie is” (the final quotation mark is on the next line).",
            "36": "- Equation (5).",
            "37": "Use “sup” instead of “max”.",
            "38": "- Discussion in Section 4.1\n- I think Figure 4 should be explained in more detail (in caption and/or text).",
            "39": "- Using capital and lower case tau in Theorem 4.2 is confusing notation.",
            "40": "- Similarly, using bold and not-bold B in Theorem 5.2 is confusing notation.",
            "41": "- After definition 5.1, what does Omega[w] = Omega[w’] mean?",
            "42": "- In Table 1, can you explain more explicitly (in caption and text) what “subset” and “class words” means?",
            "43": "Also, can you add a column where a dense linear model over p_f(s) is used?"
        },
        "T6O2wO-1p9L": {
            "0": "Summary: This paper presents an explanation of why pretraining on language modeling (LM) helps performance on downstream text classification tasks.",
            "1": "The explanation relies on formulating classification tasks as next word prediction tasks (i.e.",
            "2": "language modeling).",
            "3": "They use their theoretical results to design the Quad objective and experiment with it on SST and AG news, finding that it performs close but slightly worse than standard cross entropy training of classifiers.",
            "4": "Overall, this work contributes an interesting framework for analysis.",
            "5": "However, I have one large conceptual concern about the framework.",
            "6": "Central to the proposed explanation is the ability to formulate text classification tasks as next word prediction, possibly with a prompt appended to the input (e.g.",
            "7": "for sentiment analysis of movie reviews, “This movie is “).",
            "8": "In a trivial sense, this is always possible: We can simply append the task definition to the end of an input as a question (e.g.",
            "9": "“Is the sentiment of the review positive?”) and check the probabilities of “yes”/”no”.",
            "10": "Then predicting the answer to this prompt is equivalent to performing the task, and a perfect LM is of course able to perform the task perfectly.",
            "11": "This formulation makes Sections 3 and 4.1 feel trivially true.",
            "12": "Though, to the authors’ credit, they do have to do additional work to extend an LM that is eps-optimal in next word cross entropy (i.e.",
            "13": "on average) to optimality on the specific task formulation.",
            "14": "However, the authors don’t mention this trivial reformulation strategy and instead base their argument on the existence of heuristic words (e.g.",
            "15": "for sentiment analysis, the probability of “:)” or “:(“ after a review).",
            "16": "This strategy introduces the potential for spurious correlations: The heuristics might be strongly correlated with the task in general, but might be off due to other factors like sarcasm.",
            "17": "Additionally, relying on these single-word heuristics seems a bit off to me, as many text classification tasks don’t readily admit single words that encapsulate the task definition or label semantics.",
            "18": "There’s an argument, then, that the theory described here doesn’t apply to these tasks (i.e.",
            "19": "they’re not (t, B)-natural), but what’s frustrating about this argument is that this theory doesn’t provide us a way to distinguish which tasks fall in the category of single-word predictable or how to find such words other than trial and error.",
            "20": "It is very likely I am misunderstanding something about this paper.",
            "21": "I am not sure what it means for a task to “lie in the row span of word embeddings”."
        },
        "3cDis2cmU1g": {
            "0": "Summary of review:\n\nThere have been lots of interests to understand why self-supervised learning approaches such as the next word prediction task learn a useful representation for downstream tasks.",
            "1": "This paper provides a mathematical framework to understand this question.",
            "2": "One novel finding of this paper is that the distribution of the next word, conditional on the context, can provide a strong discriminative signal for the downstream task.",
            "3": "In particular, using a carefully selected subset of \"prompt\" words, the authors observe that learning a linear predictor over the next word distributions of these words achieves performance close to a pre-trained GPT-2 model.",
            "4": "Setting and Main Result:\n\nThis paper focuses on classification tasks, and the bulk of the work goes into how to model the next word distributions as features or representations.",
            "5": "For this purpose, the authors introduced the definition of a \"natural\" task.",
            "6": "Informally , a task is defined as natural if, just by using the next word distributions as features, the downstream task can be solved with a small loss.",
            "7": "Result 1: Under the above assumption over the downstream task, this paper provides a bound on the empirical loss of the downstream prediction task.",
            "8": "This bound consists of two parts:\n- The first part measures how \"natural\" the task is, that is, how well can the task be solved using the next word distributions as features.",
            "9": "- The second part measures the difference between the \"empirical\" next word distributions and the \"optimal\" next word distributions.",
            "10": "Result 2: The authors further extend this result to word embedding features, which are obtained by a weighted average of word embedding vectors based on the next word distributions.",
            "11": "There are several follow-up results built on these two results, such as a new loss objective for predicting the downstream task, but to the best of my understanding, these two results are the main claims of this paper.",
            "12": "A key parameter that occurs in obtaining the above results is a worst-case coefficient that bounds the distributional shift between language model distributions of the training dataset and that of the downstream task.",
            "13": "Intuitively, this parameter arises from translating the \"natural\" task assumption, which only guarantees transfer on average to the downstream task.",
            "14": "Pros:\n\n- A new framework for understanding why learning how to predict the next word helps the downstream task.",
            "15": "This paper finds that the next word distributions of a subset of \"prompt\" words contain discriminative signals and are good features.",
            "16": "This seems to be a novel finding and may help inspire future work in this important direction.",
            "17": "Cons:\n\n- The main result (Thm 4.1) applies to next/conditional word distributions that are very close to the optimal distribution.",
            "18": "It is unclear to me how the authors are going to justify this \"assumption\".",
            "19": "Should we expect the empirical distribution to converge to the true distribution when there are an infinite amount of samples?",
            "20": "- Secondly, this main result depends on the worst-case coefficient, which is also unclear to me.",
            "21": "For the transferability coefficient proposed in Section 5.1, is it possible to measure it in experiments?",
            "22": "How large should we expect this coefficient to be?",
            "23": "Writing:\n\nThe writing is overall clear and easy to follow, although it took me quite some time to map out the definitions of various notations.",
            "24": "Many of the notations look cumbersome and I suspect that there is still room for making the notations more accessible for new readers.",
            "25": "Detailed comments:\n\n- P2, Sec 1.1: \"analyze the efficiency language model features\" -> analyze the efficiency of language model features\n\n- P2, Sec 2: you started introducing these notations without explaining what they mean.",
            "26": "For example, the $p^{\\star}$ notation is also defined in Sec 2.1.",
            "27": "- P2, Sec 2: \"where $p^{\\star}_{\\cdot | s}$ is used as a vector on the left and distribution on the right\".",
            "28": "What does this sentence mean?",
            "29": "- P3, Sec 2.2: \"... achieve lower test perplexity than traditional n-gram models\" Why is this true?",
            "30": "Could you add a reference?",
            "31": "- P5, Sec 4.1: \"The result suggests that small test cross-entropy (hence test perplexity)...\" Same question as above.",
            "32": "- P6, Sec 4.3: \"In fact, $f$ almost always performs better than ...\" This part seems intriguing despite the linear relationship shown in figure 1.",
            "33": "Could you discuss this more here?",
            "34": "- P8, Table 2: The results from using Quad look worse than the above two.",
            "35": "Could you explain the significance of this result again?",
            "36": "- P24, Figure 2: What are the x and y axis, and what does each dot mean in this figure?"
        },
        "7s1JHlVw-Ua": {
            "0": "Summary.",
            "1": "This work tries to understand why features from trained language models can be used to solve classification tasks effectively.",
            "2": "A language model (LM) in the analysis is modeled as a feature map $f : S \\rightarrow \\mathbb{R}^d$, a word embedding $\\Phi \\in \\mathbb{R}^{d \\times V}$, and a trained language model is thought of as $\\epsilon$-optimal in terms of its cross-entropy (from the true distribution over S).",
            "3": "The work shows that for classification tasks approximately solvable by linear functions over the distributions of the next token the best linear classifier based on $(f, \\Phi)$ will suffer error of $O(\\sqrt{\\epsilon})$.",
            "4": "The authors also propose an additional assumption where the log partition function is quadratic in $f$ based on which some improvements can be obtained.",
            "5": "Being inspired by this assumption, a new objective function Quad where the partition function is directly replaced by a quadratic of $f$ is proposed.",
            "6": "Experiments seem to support key assertions in the theoretical analysis.",
            "7": "Strengths.",
            "8": "1.",
            "9": "The authors’ approach to a well-posed question seem original to me.",
            "10": "In particular, some proposed concepts such as the refined transferability coefficient, conditional mean features, substitutability matrix might be useful for future studies.",
            "11": "2.",
            "12": "The article is precise, well-written and cautious in its tone.",
            "13": "The accompanying experiments are informative and supportive of the main theoretical claims.",
            "14": "I enjoy the overall journey the authors presented and would love to see more well-reasoned articles like this in ICLR.",
            "15": "Weaknesses.",
            "16": "1.",
            "17": "The presentation can be improved by allocating more space to ideas in the Extensions section.",
            "18": "This part seems more creative (perhaps a little less coherent but expected for “a mathematical exploration”) but is too compressed as it stands.",
            "19": "(See Suggestion1)\n\nMinor issues.",
            "20": "1.",
            "21": "Consider to replace “partial sentences” with prefixes, which is more technically accurate.",
            "22": "2.",
            "23": "The many notations involving $p$ has inconsistent meanings for their subscripts.",
            "24": "I would suggest to consolidate them to reduce confusion.",
            "25": "For example, consider to use notations of this form $p(w|s; \\theta)$.",
            "26": "Perhaps boldface for when it is viewed as a vector.",
            "27": "Similarly for $\\ell$.",
            "28": "(See Suggestion2)\n3.",
            "29": "Below section 2.1, “trained to learn” -> “trained to fit”.",
            "30": "Suggestions.",
            "31": "1.",
            "32": "I think a moderate revision reducing some (parallel) elaboration on “unconstrained language model” should provide the space needed for Extensions (and other novel ideas).",
            "33": "After all, I found results on unconstrained LMs somewhat trivial and I suspect that you hope to use it only as an instructional tool.",
            "34": "Perhaps a serial layout would save space and even improve the perceived emphasis.",
            "35": "2.",
            "36": "I strongly suggest a review of the notations and to adopt a more consistent scheme.",
            "37": "One trick I found useful is to follow the notational convention of a textbook or a classic paper.",
            "38": "The current notation has too much overloading and variability.",
            "39": "Questions.",
            "40": "1.",
            "41": "It seems to me that the central question lacks strong practical motivation.",
            "42": "The NLP community seems to move to prefer a _natural_ answer (in the form of a generated sentence) instead of a label (from a classifier).",
            "43": "As you have argued, many classification tasks can be framed as predicting the next token (perhaps in the presence of a prompt).",
            "44": "What is your opinion?",
            "45": "2.",
            "46": "How realistic is the $\\epsilon$-optimality condition on cross-entropy for LMs?",
            "47": "Can you comment on any associated sample complexity bounds?",
            "48": "// Post-rebuttal update:\nThank you for replying to my questions.",
            "49": "I am still concerned about the sample complexity associated with $\\epsilon$-optimality in cross-entropy (even in the trivial case, and perhaps impossible for some low-dim representations) as LMs are over a countably infinite extended alphabet."
        }
    },
    "2Ey_1FeNtOC": {
        "_vUn7TRwMpQ": {
            "0": "This paper presents an approach to training recurrent neural networks with the mix objective of minimizing cross entropy (or something similar, which is not clearly defined) and minimizing minimum description length (MDL) by using a binary representation of the network which is optimized with a genetic algorithm.",
            "1": "The model is evaluated in contrast to RNN, GRU and LSTM  baselines on modelling a number of formal languages.",
            "2": "- (+) MDL is a powerful inductive bias which has been explored extensively in Machine Learning and Pattern Recognition, and it is interesting to see it revisited in the context of neural networks and in contrast to deep learning models.",
            "3": "- (+) The paper exhibits numerous examples in which the presented system produces simple hypothesis to model (also relatively simple) data.",
            "4": "- (-) Nonetheless, the studied tasks are good for a start, but insufficient to motivate the approach because actually neural networks can deal with them quite well [e.g., 1].",
            "5": "Moreover, the claims about the binary addition experiments are notably flawed.",
            "6": "In particular, the task that is considered in this paper involves two bit sequences that are fed *bit by bit* in parallel to the system.",
            "7": "This poses no serious challenge to no RNN, and in fact you can find example code directed at students that solves this task: https://github.com/mineshmathew/pyTorch_RNN_Examples.",
            "8": "(I have personally tried this code on the same setup of using sequences of up to 20 bits and evaluating on a test set of up to 250 bits with 100% accuracy -- It only required updating a few lines to more modern versions of python/pytorch and changing the loss to BCE).",
            "9": "The version of binary addition which is actually more challenging for RNNs is when the output is produced _after_ all operands are given [2].",
            "10": "Nonetheless, other neural network architectures deal with this task effectively [3].",
            "11": "I can see two directions in which this work could be improved moving forward:\n\n   - If the goal is to improve interpretability of models, the authors could aim at tackling problems in gradient-based neural network remain obscure (e.g.",
            "12": "see https://blackboxnlp.github.io/)\n   - If on the other hand the goal is improving generalization, the authors could consider tackling tasks in which neural networks have been shown to be deficient in their generalization skills [2].",
            "13": "- (-) Furthermore, RNN baselines might not be properly trained across the paper.",
            "14": "First, as mentioned in the previous point, RNNs can in fact learn the binary addition task, and thus it is unclear why the authors report that they fail.",
            "15": "Second, the authors report that the RNNs perform worse than chance on some other tasks, which could be explained by divergent training.",
            "16": "Since no code was provided it is not possible to assess how the models were trained.",
            "17": "- (-) The authors enumerate some works related to theirs, but they do not comment in which ways they are similar or different from theirs.",
            "18": "Also, given that whole books have been written on the MDL principle, the related work section could be considerably more detailed.",
            "19": "The relation to neural architecture search (NAS) is missing too.",
            "20": "- (-) It is also unclear how the training objective is quantified.",
            "21": "It is only mentioned that |G : D| relates to cross-entropy, but no precise definition is given, nor it is detailed in which ways it differs from the former.",
            "22": "**Questions for the authors**\n\n1) Do the addition RNN models reach 100% accuracy on the training data?",
            "23": "2) The cross-entropy numbers are quite high for binary classification problems.",
            "24": "Could you report how you computed them?",
            "25": "3) For the test data you mention that you test the model on \"an unseen sequence of length X\".",
            "26": "By \"an unseen\" you mean 1 sequence?",
            "27": "4) Regarding footnote 7, why not properly quantifying the number of operations needed to train each type of model?",
            "28": "**References**\n\n[1] Weiss, Gail, Yoav Goldberg, and Eran Yahav.",
            "29": "\"On the practical computational power of finite precision RNNs for language recognition.\"",
            "30": "arXiv preprint arXiv:1805.04908 (2018).",
            "31": "[2] Joulin, Armand, and Tomas Mikolov.",
            "32": "\"Inferring algorithmic patterns with stack-augmented recurrent nets.\"",
            "33": "Advances in neural information processing systems.",
            "34": "2015.",
            "35": "[3] Kaiser, Łukasz, and Ilya Sutskever.",
            "36": "\"Neural gpus learn algorithms.\"",
            "37": "arXiv preprint arXiv:1511.08228 (2015).",
            "38": "[4] Lake, Brenden, and Marco Baroni.",
            "39": "\"Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks.\"",
            "40": "International Conference on Machine Learning.",
            "41": "PMLR, 2018."
        },
        "xCGOFHrFxqK": {
            "0": "\nThe authors proposed a new training framework for recurrent neural networks that involves updating the weights with genetic algorithm under a minimum description length principle.",
            "1": "They evaluated it on a syntheic mini task of languange modeling and showed a better performance over classical RNNs trained by backpropagation.",
            "2": "Strength\n\n+ it is a good thing to read about alternative approach to backpropagation in training deep networks.",
            "3": "+ tackling the interpretablity issue of deep networks with symbolic knowledge is a great approach.",
            "4": "+ the problem of generalization in recurrent networks is an important topic to study.",
            "5": "+ As in the discussion section, this line of research has great potential in future extensions if implemented and understood well.",
            "6": "Weakness\n\n- the previous work offers an intriguing task on the historical attempts in applying genetic algorithms and MDL principles in neural networks.",
            "7": "However, it would be better if what works are done in these related work.",
            "8": "For instance, the authors wrote \"These challenges were already taken up by early work on XXX\", but failed to say how they tackled these challenges.",
            "9": "- the technical sections are very hard to follow.",
            "10": "It is hard to decode in section 3 how the algorithm work.",
            "11": "Although mentioned some details on the genetic algorithm in the Appendix, the main text should be self-contained.",
            "12": "- Following the previous point, how is the MDL metric computed exactly?",
            "13": "Is it a bitwise estimation?",
            "14": "If so, is the RNN here merely a boolean network?",
            "15": "- Why is the task descriptions in section 3.5 placed in the \"Learner\" section?",
            "16": "And why are some results also included here in the method section?",
            "17": "The writing needs some improvement to increase clarity and structure.",
            "18": "- Still this section, \"Again, this network is transparent, the task is learned perfectly well, and no RNNs would do as well.\"",
            "19": "Would the author mind explaining where this is coming from?",
            "20": "- The result section is also lack of important details on the tasks and evaluations.",
            "21": "Unlike section 4.3 where the experiments are adequately introduced, the setup in section 4.1 is far too brief for the authors to grasp the task (especially if it is not usually used in the field).",
            "22": "- table 4.4. the cross entropy of MDL model is quite close with the best RNN in all tasks except addition (where MDL model kills) and a^nb^nc^n (where MDL model sucks).",
            "23": "This could indicates an coincidence -- did the authors run multiple runs with different random seeds?",
            "24": "(the plots in the appendix suggests that they only ran once).",
            "25": "Summary\n\nOverall, the project introduced an interesting approach with great potential, but the results and writing appear to be preliminary.",
            "26": "We suggest the authors to add more evaluations and improve the writing."
        },
        "_5g5ZpJVMxX": {
            "0": "This paper describes a method for training neural network based on\ngenetic algorithms that minimizes the minimum description length\n(MDL).",
            "1": "The aim of the study is to obtain smaller, explainable networks\nthat learn generalizations from sequences.",
            "2": "The paper shows that the\nprocedure, indeed, learns compact and interpretable networks\nrecognizing a number of (artificial) formal languages, as well as\naddition.",
            "3": "The networks are also compared to common RNN models\ntuned/trained on the same task, indicating that the networks\ndiscovered by MDL outperforms RNNs in majority of the experiments.",
            "4": "Although study uses well-known methods (MDL, genetic algorithms), the\napplication and premise is interesting.",
            "5": "Without doubt, both smaller\nand explainable models are good, and the paper demonstrates this\nnicely with the included experiments.",
            "6": "It is not specific to the\nparticular study, but strengths of the method also include learning\nstructure of the network as well as its weights and possibility of\nlearning a larger class of network architectures.",
            "7": "My main criticism likes within two aspects of the paper/study:\n\n- Although the authors touch upon this in the concluding remarks, I'd\n  be interested to see a more through proof or demonstration of\n  scalability of the method.",
            "8": "All the experiments provided can be\n  solved by relatively simple networks.",
            "9": "With increasing problem\n  complexity, the networks size (as a result its encoding) and the\n  number of alternative networks to test before finding a solution is\n  expected to increase as well.",
            "10": "In short: the usability of the method\n  would be more convincing if one of the problems had a larger\n  alphabet, and less well-defined solutions (e.g., a linguistic\n  problem as elaborated by the authors).",
            "11": "- I was very surprised to find the description of the proposed\n  method/model in an appendix.",
            "12": "Although the method is relatively\n  straightforward for most ICLR audience, I thin the paper needs a\n  reasonably detailed description of the method in the main text.",
            "13": "If\n  the space is the issue here, one can shorten the descriptions of the\n  artificial language experiments, or even push part of them to an\n  appendix.",
            "14": "I also have some minor remarks/suggestions:\n\n- From the description in the paper, I am not sure if the baseline\n  RNNs got the same care and love as the proposed system.",
            "15": "If not tuned\n  properly, it would not be surprising that they do not necessarily\n  perform well.",
            "16": "For example, for these simple problems there is a fair\n  chance that at 1000 epochs some of the networks badly overfit.",
            "17": "Hence\n  while comparing the method with RNNs the study may be basing the\n  conclusions on weak baselines.",
            "18": "- Similar to some of the points noted above, I'd be also interested to\n  see more discussion/comparison with (L1) regularized learning.",
            "19": "- Some readers may benefit from a better description of the network\n  representation in the figures.",
            "20": "For example, it is not clear to me\n  what an output unit with no input produces (assuming that there is\n  an intercept/bias term, but an explicit remark would be useful).",
            "21": "- A few typographic/language suggestions/issues:\n\n    - Footnote marks should go after punctuation (e.g., footnote mark\n      2).",
            "22": "- The fonts on figures are sometimes unreadably small, and\n      resolution is not optimal (especially fig.",
            "23": "6 in the appendix,\n      but others may also benefit from high-resolution or vector\n      graphics).",
            "24": "- Aligning numbers in table 1 properly would increase their\n      readability.",
            "25": "- The color choice on figures (although it did not seem\n      significant) was not kind to this color blind reader."
        },
        "-qXy79O-55": {
            "0": "The importance of finding minimum description RNNs is definitely unquestionable for several reasons.",
            "1": "Thus the authors address an important problem.",
            "2": "They decide to use GA in order to find such a representation.",
            "3": "My main reservation is that the paper is a vanilla application of GA without any enhancements or tailored aspects.",
            "4": "In other words, I can't find methodological or algorithmic contributions.",
            "5": "The experimental results also don't stand out.",
            "6": "The tasks selected are all artificial (learning mathematical operations).",
            "7": "To this end it would be great to include some practical cases."
        }
    },
    "fylclEqgvgd": {
        "a2VIZand6bJ": {
            "0": "## Summary\nThe paper shows that Transformers trained unsupervised on millions of protein sequences learn information about protein contacts by using attention maps for contact prediction.",
            "1": "The paper is mostly clearly written and discusses server interesting ablation experiments.",
            "2": "However, two recent papers that appeared on arXiv before the ICLR submission deadlines also use Transformers for protein contact prediction.",
            "3": "These papers and other methods for contact prediction beyond Gremlin are not described.",
            "4": "I therefore consider the contributions as insufficient for an ICLR submission.",
            "5": "## Major comments\n\n1.",
            "6": "Using Transformer attention maps for protein contact prediction is not new.",
            "7": "See Rives et al, 2020, ‘Biological structures and functions emerge…’, section 5.2, and Vig et al, 2020, ‘Bertology’ section 4.2.",
            "8": "Both publications appeared on arXiv at least one month before the ICLR submission deadline and are not clearly discussed in the paper.",
            "9": "2.",
            "10": "The introductions discusses existing work on Transformers for protein languages models.",
            "11": "Existing methods for contact prediction (beyond Gremlin), however, are not described sufficiently.",
            "12": "3.",
            "13": "It is unclear which sequences were used for training the Transformer models and how similar they are to test sequences.",
            "14": "4.",
            "15": "The paper compares Transformers to Gremlin.",
            "16": "However, it is unclear how well they perform to the CASP state-of-the art (see also Rives et al, 2020).",
            "17": "5.",
            "18": "Section 3.4  does not describe clearly enough how attention maps were used for predicting contact maps.",
            "19": "How were attention maps symmetrized?",
            "20": "Which layers and heads were used and how were they aggregated?",
            "21": "What is the number of resulting features that were used to train the logistic regression model?",
            "22": "APC is not described or cited.",
            "23": "6.",
            "24": "Section 4.5 discusses that Transformers can be also used for secondary structure prediction.",
            "25": "This is not new (see Rives 2020 and Vig 2020) and does not fit well to the rest of the paper, which is about contact prediction.",
            "26": "6.",
            "27": "Section 4.8: Using transformers for generating proteins with natural properties is not new (see Madani et al, 2020, ‘ProGen’ or Rives et al, 2020).",
            "28": "‘Wang & Cho’ were not the first who used Transformers generativity (see Vaswani, 2017)."
        },
        "AsQVaEgEL_": {
            "0": "In this paper, the authors show that transformer protein language models can learn protein contacts from the unsupervised language modelling objectives.",
            "1": "They also show that the residue-residue contacts can be extracted by sparse logistic regression to learn coefficients on the attention heads.",
            "2": "One of the advantages of using transformers models is that they do not require an alignment step nor the use of specialized bioinformatics tools (which are computationally expensive).",
            "3": "When compared to a method based on multiple sequence alignment, the transformers models can obtain a similar or higher precision.",
            "4": "Contributions of this paper are:\n- showing that the attention maps built in Transformer-based protein languages learn protein contacts, and when extracted, they perform competitively for protein contact prediction;\n- a method for extracting attention maps from Transformer models;\n- a comparison between a recent protein transformer protein language model (which does dot require sequence alignment), and a pseudo-likelihood-based optimization method that uses multiple sequence alignment;\n- an analysis of how much the supervised learning (logistic regression) contributes to the results.",
            "5": "The paper covers a relevant topic and it is easy to read.",
            "6": "However, I have a number of concerns.",
            "7": "The main contribution of the paper is that attention maps built in Transformer-based protein languages learn protein contacts and can be used for protein contact prediction.",
            "8": "However, this was reported before in Rives et al.",
            "9": "(2019) (doi: 10.1101/622803).",
            "10": "Also, several methods have been developed for this problem, but are not included in the comparisons.",
            "11": "Finally, the provided implementation details are not sufficient to reproduce the results of the paper.",
            "12": "I detail some of these concerns below, together with questions/suggestions for improvements:\n\n1) I would recommend comparing transformers to other methods besides Gremlin, or justify why other methods were not included.",
            "13": "This review can be helpful:\n\n(Adhikari B, Cheng J., 2016.. doi: 10.1007/978-1-4939-3572-7_24)\n\nAlso, more recent methods that were published after the review are:\n\n(Badri Adhikari, 2020. https://doi.org/10.1093/bioinformatics/btz593)\n\n(Luttrell  et al., 2019. https://doi.org/10.1186/s12859-019-2627-6)\n\n(Gao et al.,2019.",
            "14": "https://doi.org/10.1038/s41598-019-40314-1)\n\n(Ji S et al., 2019. https://doi.org/10.1371/journal.pone.0205214)\n\n2) On page 7, the authors state that \"We find that the logistic regression probabilities are reasonably well calibrated estimators of true contact probability and can be used directly as a measure of the model's confidence (Figure 10a)\".",
            "15": "However, from the plot in Figure 10a, it is not totally clear that the probabilities are well calibrated.",
            "16": "Could the authors add more justifications of why they consider it well calibrated?",
            "17": "Could they also show a comparison of the calibration of the other transformer models, perhaps using MSE as a calibration metric?",
            "18": "3) To understand the occurence of false positives, the authors analyze the Manhattan distance between the predicted contact and the true contact, which is between 1 and 4 for most false positives.",
            "19": "They also show an example of a homodimer, for which predictions were far from the true contacts, and explain that the model is picking up inter-chain interactions.",
            "20": "Could the authors report how many predictions have a Manhattan distance larger than 4?",
            "21": "Is this one example representative of the group of false positives far from the true contact?",
            "22": "Maybe the authors could analyse whether this happens in most of the cases.",
            "23": "4) While ESM-1 is open-source and publicly available, this is not the case for ESM-1b.",
            "24": "In section A.5, the authors provide implementation details as differences between ESM-1 and ESM-1b, stating “Compared to ESM-1, the main changes in ESM-1b are: higher learning rate; dropout after word embedding; learned positional embeddings; final layer norm before the output; and tied input/output word embeddings.",
            "25": "The weights of all ESM models throughout the training process were provided to us by the authors.”.",
            "26": "In my opinion, this is not enough to reproduce the results in this paper.",
            "27": "To make it reproducible, the authors need to provide a detailed enough description of the differences to make the reader able to implement ESM-1b, or provide the weights and hyperparameters required to reproduce their results."
        },
        "goICfmHPbVf": {
            "0": "**Summary**\nThe paper performs a number of analyses centered around the ability of transformer-based language models trained on protein sequence data to learn representations useful for predicting protein secondary and tertiary structure (the latter as contact maps).",
            "1": "Specifically, the paper studies several pre-trained transformer models by fitting an L1-penalized logistic regression to amino acid pair contacts.",
            "2": "Several experiments are performed to showcase that (i) transformer-based representations can outperform state-of-the art methods based on MSA in terms of contact prediction precision; (ii) that the necessary information for contact predictions in these representations is learned in an unsupervised manner (and not by the logistic regression put on top of these representations); and (iii) that the contact prediction probabilities are reasonably well calibrated.",
            "3": "**Score justification**\nIn its current form the paper presents interesting analyses, but has overall limited novelty.",
            "4": "The ability of transformer models to learn representations predictive of secondary and tertiary structure has been demonstrated before (including in the papers proposing the models used by the authors).",
            "5": "Furthermore, I have some questions regarding the methodology employed by the authors.",
            "6": "**Major comments**\n* The main metric employed by the authors is the precision of the top L (protein length) contact prediction for a given range (P@L).",
            "7": "I wonder why the authors do not also consider recall at L as an accompanying metric for reporting the results.",
            "8": "* When comparing ESM to the baseline Gremlin method, the authors consider two scenarios: (i) Gremlin trained on the trRosetta data; and (ii) Gremlin trained on the same data as the ESM transformer model.",
            "9": "Overall, Gremlin trained on the ESM data - which is arguably the correct baseline for the ESM model -  performs worse than Gremlin trained on the trRosetta data.",
            "10": "Why is that the case?",
            "11": "How does the procedure for preparing MSA for the ESM data compare to that of the trRosetta data?",
            "12": "Can it be tuned to improve Gremlin's performance?",
            "13": "* The paper compares several transformer models that differ primarily in the model size, dataset size and hyper-parameters.",
            "14": "As can be seen from Table 1 of the manuscript, these differences are clearly important for the contact prediction task and thus should be summarized and discussed in more detail.",
            "15": "* From what I understand the sequences from the testing set of the contact prediction problem (or sequences highly similar to them) could appear in the training sets of the considered transformer models.",
            "16": "This creates some information leakage.",
            "17": "It's unclear from the results presented in the paper whether it is an issue or not - how does contact prediction precision / recall change as sequence similarity to the ESM training set drops?",
            "18": "* The authors present analysis on the usefulness of the representations learned by various attention heads for contact prediction; and on robustness of such predictions.",
            "19": "I wonder how robust the results of these analyses are - they appear to have been performed using a single checkpoint of the ESM model, which is a result of stochastic training from random initialization.",
            "20": "* In the Appendix the authors talk about the benefit of using predicted contact maps for inferring the all-atom protein 3D structure.",
            "21": "However no results on this are presented.",
            "22": "I would be very eager to see the comparison of 3D structure accuracy inferred with ESM-predicted and Gremlin-predicted contacts.",
            "23": "**Minor comments**\n* Introduction talks about the ESM-1b model but (as far as I can tell) a reference isn't provided until a later section."
        },
        "34HFsILFxM": {
            "0": "In this manuscript, the authors present a method for predicting residue-residue contacts within protein structures using the attention layers learned by transformer language models.",
            "1": "Using the largest transformer language models trained to data, the authors show good performance for contact prediction.",
            "2": "The paper is clearly written and easy to follow.",
            "3": "The general concept of fine tuning protein language models for contact prediction has circulated for some time which lessens the core contribution, but the authors approach is surprisingly data efficient and accurate.",
            "4": "Overall this is an interesting work, though there is quite a bit of background on contact prediction missing.",
            "5": "This paper is also very application specific and may not present new machine learning methods of general interest to the ICLR community.",
            "6": "The existence of previous language model-based contact prediction methods reduces the novelty of this work, especially given that the model used here is from Rives et al.",
            "7": "2019, who already look at contact prediction.",
            "8": "Furthermore, no comparisons with state-of-the-art evolutionary coupling-based or language model-based contact prediction methods are performed.",
            "9": "With this in mind, the manuscript may be better suited to submission at a biology specific venue.",
            "10": "Additional specific comments follow below.",
            "11": "Major comments:\n1.",
            "12": "Missing related work: there are a number of highly relevant prior works that are not mentioned/discussed.",
            "13": "In particular, “Deep generative models of genetic variation capture the effects of mutations” – Riesselman et al.",
            "14": "2018 was, as far as I know, the first paper to show that deep generative models capture structure information (see Figure 6).",
            "15": "Following that, “Learning protein sequence embeddings using information from structure” – Bepler & Berger 2019 was, to my knowledge, the first paper to propose deep language models (alignment free) for learning protein sequence representations and used those unsupervised representations for contact prediction.",
            "16": "Furthermore, there has been extensive work in improving contact prediction using sequence + co-evolutionary features.",
            "17": "See, for example, “Enhancing Evolutionary Couplings with Deep Convolutional Neural Networks” Liu et al.",
            "18": "2018 and “Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model” Wang et al.",
            "19": "2017.",
            "20": "Other papers looking at protein structure prediction from sequence with deep learning, though they are less directly relevant, include “End-to-End Differentiable Learning of Protein Structure” AlQuraishi 2018 and “Learning Protein Structure with a Differentiable Simulator” Ingraham 2019.",
            "21": "2.",
            "22": "Before this work, others have looked at fine tuning language models for contact prediction.",
            "23": "How do those approaches compare with the approach presented here?",
            "24": "Rives et al look at contact prediction in their manuscript describing the transformer model (which is the same model used here) on CASP 11-13 (see Table 5 in their manuscript).",
            "25": "How does that approach compare with this one?",
            "26": "Likewise for Bepler & Berger\n3.",
            "27": "Many methods have surpassed GREMLIN for contact prediction using evolutionary couplings.",
            "28": "How do those approaches compare with this one?",
            "29": "It would be helpful to see how this approach compares with truly state-of-the-art contact prediction methods.",
            "30": "Reporting results on the CASP data would help to make this comparison.",
            "31": "Minor Comments:\n1.",
            "32": "Although multiple sequence alignment methods have challenges especially as related to evolutionary coupling prediction, these methods have been heavily optimized for decades.",
            "33": "The authors should provide citations for claimed failings such as “failure to find an optimal alignment” and “suboptimality of the substitution matrix and gap penalty.” Certainly, these may be sources of error in alignments, but I am not aware of any studies of the frequency or impacts of these errors on evolutionary coupling analysis.",
            "34": "If these studies exist, I encourage the authors to cite them.",
            "35": "If they do not exist, I suggest the authors focus on well known sources of error here (namely, alignment depth) and provide references.",
            "36": "2.",
            "37": "The authors use the language model without fine tuning, but the model could be fine tuned for each protein using its MSA.",
            "38": "It’s great that contacts can be predicted without fine tuning, but it would be interesting to investigate whether additional gains can be made.",
            "39": "3.",
            "40": "Eight iterations of jackhmmer is a lot.",
            "41": "In my personal experience, jackhmmer often diverges at 3+ iterations.",
            "42": "By this I mean, the set of sequences and HMM learned by jackhmmer drift far away from the original sequence/family.",
            "43": "Did the authors perform and quality checks of these alignments to ensure jackhmmer did not diverge?",
            "44": "4.",
            "45": "How are sequence depths in Figure 3 calculated?",
            "46": "Is this the raw number of sequences in each MSA or is it after applying some sort of neighborhood weighting to calculate an effective number of sequences?",
            "47": "Things that would improve my rating:\n1.",
            "48": "Provide a more comprehensive background review.",
            "49": "2.",
            "50": "Compare with state-of-the-art evolutionary coupling-based contact prediction methods.",
            "51": "3.",
            "52": "Compare with other language model-based contact prediction methods.",
            "53": "4.",
            "54": "What should interest the general machine learning community about this paper?",
            "55": "What can we learn that might lead to better ML methods in the future?",
            "56": "Convince me that this doesn’t belong in a bioinformatics venue!"
        }
    },
    "oVz-YWdiMjt": {
        "AowgLbK53ne": {
            "0": "Summary\n=======\nTransformers models have been recently shown to capture protein contact information in their attention maps when trained unsupervised of millions of protein sequences.",
            "1": "This paper draws parallels between Transformers and Potts models (fully-connected pairwise MRF)--the current standard approach for protein contact prediction--and shows empirically that Transformers are competitive with Potts models.",
            "2": "Understanding the differences and similarities between Transformers and Potts models makes Transformers less of a ‘black-box’ and helps to establish them as a principled method for contact prediction.",
            "3": "The paper is clearly written and the evaluation is solid.",
            "4": "I have only a few comments.",
            "5": "Major comments\n=============\n1.",
            "6": "What is the maximum sequence similarity between the training sequence of ProtBERT and sequences in TrRosetts alignments that were used for testing?",
            "7": "Sequences must not overlap have a maximum similarity of let’s say 80%.",
            "8": "2.",
            "9": "You describe that you used three sets of families from the TrRosetta dataset (A.4.1).",
            "10": "Why did you use only 732 families for testing (set 3)?",
            "11": "Were these all families that were not included in the first two sets?",
            "12": "How many families do the first two sets include and how similar are families of different sets?",
            "13": "Ideally, train, tune, and test families belong to different super families.",
            "14": "3.",
            "15": "You describe in section A.3 how you extracted protein contact maps from the attention maps of ProtBERT.",
            "16": "This is an important detail that must be described in the main text.",
            "17": "How did you choose the 6 heads?",
            "18": "Did you choose them manually or, for example, by training a linear model to predict contacts from attention maps and using the weights for identifying important heads, or computing the weighted average of attention maps?",
            "19": "Minor comments\n=============\n4.",
            "20": "Section 3.2, ‘x = E_seq(x_i) + E_pos(i)’: How did you compute positional embeddings and why do and add embeddings instead of concatenating them?",
            "21": "5.",
            "22": "Section 3.2, ‘We treat the positional embedding E_pos as an overall summary per-position information’.",
            "23": "Please describe more clearly what this summary is.",
            "24": "6.",
            "25": "Section 4, first paragraph: The L of the precision at L metric is not the sequence length but the number of top sequences.",
            "26": "You describe L as being both.",
            "27": "7.",
            "28": "Figure 6 is not discussed.",
            "29": "Instead of showing this figure, I suggest quantifying the correlation depending on the number of heads by computing and discussing  the Spearman correlation.",
            "30": "8.",
            "31": "Rives et al  2020 ‘Biological structure and function emerge…’ have recently shown in addition to Vig et al that protein contact can be predicted from attention maps, which must be also pointed out in the ‘Background’ section."
        },
        "A7p-BK8Hhd": {
            "0": "Recently, some researchers tried to apply attention models into the protein field, using self-supervised learning to predict protein contacts.",
            "1": "In this work, the author attempt to build the connection between such works and the old-school model, Potts model.",
            "2": "By simplifying some operations within the attention model, the author managed to build an analog between the simplified model and the Potts model.",
            "3": "The analog is intuitive and easy-to-understand.",
            "4": "The authors further compare the simplified model and the Potts model on 748 protein families, showing that they are similar.",
            "5": "Or probably the simplified attention model is even better.",
            "6": "This is an interesting work.",
            "7": "However, I also have a number of concerns.",
            "8": "The advantages and disadvantages are listed below.",
            "9": "Pros:\n1.",
            "10": "The manuscript is concise and easy-to-understand.",
            "11": "2.",
            "12": "The idea is intuitive and reasonable, with experimental support.",
            "13": "Cons:\n1.",
            "14": "The analog between the simplified attention model and the Potts model is intuitive but not rigorous.",
            "15": "The authors claim that they provide a theoretical connection between the two models.",
            "16": "However, that part is not strong enough, without proof.",
            "17": "2.",
            "18": "There are two assumptions in this work, which make the simplified model different from the attention models that the previous researchers used.",
            "19": "Firstly, they train the model on multiple sequence alignment instead of the raw sequences.",
            "20": "If they train the model on the raw sequences, the performance is unacceptable, as shown in Figure 16, which is consistent with the previous research.",
            "21": "Secondly, they removed the sequence embedding in queries and keys.",
            "22": "This simplification makes the model only consider the statistical pattern in the MSA.",
            "23": "To me, this one is a too strong assumption.",
            "24": "3.",
            "25": "The running time and hardware comparison is missing.",
            "26": "If the single layer of attention is comparable to the Potts model, not outperform it significantly, while it would take much more time to train, the researchers would need to think twice if they want to use the attention model.",
            "27": "4.",
            "28": "The ablation study makes me feel that the results are on the opposite of the conclusion.",
            "29": "Here is my logic.",
            "30": "With the above two assumptions, the attention model can achieve similar performance as the Potts model, or a little bit better.",
            "31": "However, when we train on the unaligned sequences, which is the usual case that we would use the attention model, the performance becomes unacceptable.",
            "32": "Then why we want to use the more expensive attention model?",
            "33": "The attention model in the NLP field is a different story.",
            "34": "Those models are refreshing the STOA performance all the time.",
            "35": "However, in the protein field, the attention model can still only achieve comparable performance as the classific models, after a two-year study.",
            "36": "They seldom outperform classic algorithms.",
            "37": "The results in this manuscript are consistent with the previous research.",
            "38": "So I am not convinced regarding the conclusion in the abstract:\n\"Taken together, these results provide motivation for training Transformers on large protein datasets.\"",
            "39": "5.",
            "40": "The potential audience of this paper would be those who are specialized or interested in bioinformatics and protein."
        },
        "35c7F7Aganm": {
            "0": "Summary:\nThis paper explores the connection between the classic Potts model-based approaches and modern Transformer-based approaches for protein contact map prediction.",
            "1": "To this end, the authors introduce a simplified variation of the attention layer called factored attention, and show that a single layer of factored attention performs operations similar to those performed by the Potts model-based methods.",
            "2": "Pros:\n- The paper attempts to connect classic and modern approaches to protein contact map prediction, which might be interesting to the people working in this field.",
            "3": "The evidence presented (simplifying attention layer so that the equations look similar to the classic methods, numerical results of the simplified attention layer close to the classic methods) is reasonably convincing.",
            "4": "- The topic of the paper is quite timely, there has been a lot of interest recently in modelling proteins using the latest NLP techniques.",
            "5": "- The paper is well written.",
            "6": "I appreciate the effort put in by the authors to define basic protein terminologies which might not be obvious to readers without biology background.",
            "7": "Cons:\n- The contributions of the paper would have been more interesting if the proposed modifications of the attention layer led to increased prediction performance of models which are representative of the state-of-the-art.",
            "8": "Specifically, if retraining ProtBERT-BFD using the modified attention layer led to further improvement in performance, that would have been a solid contribution.",
            "9": "- Are MRF models really that competitive for contact map prediction?",
            "10": "From what I understand, deep neural networks have been far better at this task for quite some time now.",
            "11": "At multiple places in the paper, the authors give the impression that MRF models are close to state-of-the-art.",
            "12": "- In the last paragraph of the introductory section, the idea of encoding the MSAs is introduced which seemed interesting.",
            "13": "However, from what I understood from the rest of the paper, the queries and keys are extracted solely based on the position of the amino acid.",
            "14": "Is that right?",
            "15": "If so, does the position correspond to the position in the sequence or in the MSA?",
            "16": "Are the actual alignments used in any of the results in the paper?",
            "17": "Please clarify.",
            "18": "Comments:\n- Section 3.1: \"each edge\" should have a capital e.\n- Section 3.3, specifically the part where you show that factored attention is a pairwise MRF, is too brief.",
            "19": "Given that this is a main contribution of the paper, it would be worthwhile to explain this connection in a more detailed manner."
        },
        "uBqGHjM1FQb": {
            "0": "This manuscript describes a connection between Potts models and attention as implemented in modern transformers.",
            "1": "The authors then present an attention model in which positional encodings are defined as one-hot vectors indicating fixed positions in the multiple sequence alignment and train single layer attention models.",
            "2": "These models, unsurprisingly, perform similarly to Potts models without APC correction for contact prediction.",
            "3": "The methods section is somewhat confusingly written.",
            "4": "I think the factored attention model would benefit from being described on it’s own terms rather than in connection with typical multiheaded attention, especially because the isolation of position encodings and amino acids at those positions dramatically simplifies the understanding of W_Q, W_K,  and W_V.",
            "5": "The authors also spend a long time describing well known methods, but without providing additional insight.",
            "6": "The connection between the Potts model and attention described in this paper should be obvious to those who already understand attention models and Potts models and the empirical results of the factored attention model don’t make this approach seem compelling.",
            "7": "In the discussion, the authors make several broad future speculations.",
            "8": "Some of these would be interesting contributions and I encourage the authors to develop this work further.",
            "9": "Maybe factored attention could be promising for better capturing dependencies between positions for deeper transformers on MSAs, but it isn’t likely that this work will be of broad interest to the machine learning community.",
            "10": "This manuscript seems better suited to a workshop or other specialized venue.",
            "11": "Some specific comments on this work follow below.",
            "12": "1.",
            "13": "In the factored attention model, the authors use one-hot encoding of the position index as the position encoding.",
            "14": "This is equivalent to learned position embeddings as in BERT which is worth mentioning.",
            "15": "2.",
            "16": "The authors discuss single-site potentials as a difference between Potts models and single layer attention models and then show a comparison of attention models with and without single-site potentials showing little difference.",
            "17": "However, attention models already implicitly have single-site potentials which arise from the positional encoding input features.",
            "18": "Granted, this is not the case for the factored attention model where single-site potentials seem to have more effect, though in the negative direction.",
            "19": "3.",
            "20": "The authors state that “The ability of factored attention to capture similar contacts to Potts without use of APC suggest that it may be more suitable for protein design.” I don’t follow this conclusion.",
            "21": "If the factored attention model performs equivalently to the Potts model alone and worse than the Potts model with APC correction, why would it be more suitable for protein design?",
            "22": "4.",
            "23": "What makes the single-layer attention or factored attention models compelling for protein modeling?",
            "24": "What problems do these models solve that are not better solved by the Potts model or traditional transformers?",
            "25": "What would raise my score:\n1.",
            "26": "Present a compelling use case for the factored attention model.",
            "27": "What questions can be answered (or better answered) with this model over the Potts model or other alternatives?",
            "28": "One idea is to use the factored attention model as the layers in a full deep transformer model and see if this architecture can improve tasks where MSA training data is available.",
            "29": "Edit: I have increased my score in light of the response and manuscript edits.",
            "30": "The manuscript is improved, but I think the method still needs more development.",
            "31": "There are a number of interesting pieces but the final picture of an improved protein model is not fully resolved."
        }
    },
    "wOI9hqkvu_": {
        "VZIslghG1gD": {
            "0": "The paper creatively extends text GAN by introducing non-autoregressive generator, which is a well-known notion in translation and VAE like generation but not often applied in a GAN setting.",
            "1": "The paper argues that a non-autoregressive generator brings more effective gradient-based optimization and also good latent representation learning capability.",
            "2": "The comparison between NAGAN and other text GANS reads okay, but the reviewer concerns the limited scope and significance of this paper.",
            "3": "1, Given very strong text generation capability of MLE learning and pre-training, NAGAN makes little contribution to push the generation SOTA.",
            "4": "Audiences of this approach are also limited.",
            "5": "In this paper, given a very old baseline of MLE and a bunch of text GANs, the overall performance of NAGAN is still not much leading.",
            "6": "Let alone compare it to other strong pre-trained generators.",
            "7": "2.",
            "8": "When claiming good latent representation learning capability, there should be a big gap between NAGAN and text VAEs in this aspect.",
            "9": "If the author adds more control and manipulation experiments in text VAE, NAGAN will be not as shining as now.",
            "10": "3.",
            "11": "Non-autoregressive generator has difficulties in generalizing to long text generation and conditional generation.",
            "12": "How does the author consider such settings, instead of simple unconditional generation in toy datasets like COCO?",
            "13": "Overall, the reviewer thinks this is a well-written paper, but a boardline one considering its limited significance for the venue."
        },
        "vuZv-au-5jy": {
            "0": "This paper introduces the non-autoregressive generator in the GAN-based text generation, making textGAN can be trained without pre-training and better utilize latent variables to control the style of generated text.",
            "1": "Introducing non-autoregressive architectures into GAN-based text generator is a natural idea, and the modelling ability of non-autoregressive generator has been verified at BERT.",
            "2": "I think that this paper is above the average because it provides comprehensive experiments (including unconditional text generation, unsupervised decipherment and sentence manipulation), showing the significant improvement in various evaluation metrics compared to the text model without NAR and baselines.",
            "3": "However, this paper should have more analysis of how non-autoregressive architectures work in the GAN-based text generation.",
            "4": "1.",
            "5": "How latent variables in the non-autoregressive generator influence the content of the generated text?",
            "6": "Can you provide some analysis or examples about it?",
            "7": "(e.g.",
            "8": "change the value of some latent variable continually (from 0 to 1?)",
            "9": "and give the generated text).",
            "10": "2.",
            "11": "Can you give some analysis about the generation process of the non-autoregressive generator, (e.g.",
            "12": "attention map), which makes the generator more interpretable.",
            "13": "3.",
            "14": "The user study is absent.",
            "15": "4.",
            "16": "Is dropout necessary for the non-autoregressive generator?",
            "17": "What if the dropout rate is 0, how the performance of generator changes?",
            "18": "5.",
            "19": "Can you give more details about experiments, such as model parameters, training time, inference time and GPU you used?"
        },
        "HLdDYmY3A": {
            "0": "**Summary**\nThis paper proposed a new text GAN framework by combining non-autoregressive text generator based on transformer, straight-through gradient approximation, and various regularization techniques such as gradient penalty and dropout.",
            "1": "The paper demonstrates the superiority of non-autoregressive generator in the context of text GANs through various experiments including unconditional text generation, latent space manipulation and unsupervised decipherment.",
            "2": "**Pros**\n- This work narrows the gap between image GAN and text GAN by leveraging recent advances on non-autoregressive text generators and a straight-through gradient approximation.",
            "3": "While these components are well studied in previous works, I think this work presents a neat combination of them in order to solve a well-known problem.",
            "4": "- The paper provides rich discussions in training text GAN and comprehensive experiments and ablations to demonstrate the usefulness of an implicit text generator in different contexts.",
            "5": "**Concerns & Questions to Answer during rebuttal**\n- The original text GAN papers were mainly motivated to address the *exposure bias* problem in maximum likelihood estimation for autoregressive generators.",
            "6": "In other words, when we use an autoregressive generator to sequentially generate tokens one by one, there is a distribution mismatch between training and test phase.",
            "7": "In your case, now that you already have a non-autoregressive text generator, is there any *theoretical motivation/insights* for using the adversarial training framework?",
            "8": "We know that MLE is statistically efficient (achieving Cram´er–Rao Lower Bound) and possesses many good properties, maybe training the non-autoregressive text generator with MLE (e.g., FlowSeq [1] or variational inference) is a better choice?",
            "9": "- The non-autoregressive (NA) text generator have been well studied recently so the novelty of this work is more on the integration of NA generator with adversarial training.",
            "10": "Thus the main challenge here is how to solve the non-differentiability problem.",
            "11": "The paper directly leverages a traditional workaround, the straight through estimator, which is a biased gradient approximation.",
            "12": "Is the bias going to be an issue and is there any better strategy?",
            "13": "I think the paper need to provide more discussions on this aspect.",
            "14": "Overall the method section need to be polished with more details, as I feel this part is currently hard to follow.",
            "15": "- In figure 1, $z_1, \\ldots, z_L$ are sampled independently, which are sent to a transformer and later produced the sample.",
            "16": "Is the independence between $z_1, \\ldots, z_L$ going to be a problem?",
            "17": "When we use transformer to do neural machine translation, the attention mechanism will capture the dependence in the input sentence ($z_1, \\ldots, z_L$ in this context) and then produce the output correspondingly.",
            "18": "Hence will the independence in $z_1, \\ldots, z_L$ lead to a less expressive sample distribution in your text generator (although this is not an issue in image GAN)?",
            "19": "- The paper also propose to use the Max Gradient Penalty from image GAN domain.",
            "20": "The Max Gradient Penalty was introduced under the framework of Wasserstein GAN or Lipschitz GAN framework, which aims to constraint the function space to be Lipschitz smooth.",
            "21": "However this work uses the vanilla GAN objective (eq 12 and eq 13), which is not the WGAN or LGAN framework.",
            "22": "Thus the regularization may not be theoretically correct.",
            "23": "Also why not instead use the WGAN objective which is empirically more stable and theoretically sound?",
            "24": "- Experiments: In table 2, all the results for NAGAN use the dropout with a positive ratio.",
            "25": "How does NAGAN perform without dropout?",
            "26": "Also I wonder if the comparison in the table and figures are fair, since most previous methods.baselines such as MLE or SeqGAN only use a vanilla RNN/LSTM, while NAGAN has a more complicated structure with transformers, and additional regularization such as gradient penalty and dropout.",
            "27": "Perhaps we should control at least the number of parameters to be in the same level.",
            "28": "[1] FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow\n\nUpdate after rebuttal: \nAfter seeing the author response below, no change to my score."
        }
    },
    "-gabSeMKO4H": {
        "8Dx8xypI6k": {
            "0": "** Summary **\n\n(1) The authors proposed a translation system with an external memory.",
            "1": "Given a sentence $x$ to be translated, they first retrieve a $(tx, ty)$ sentence pair from the training set through ``SEGMENT-BASED TM RETRIEVAL’’ defined in Section 4.1, where $tx$ and $ty$ are from source and target languages respectively.",
            "2": "Then the $(x,tx,ty)$ are fused together to get the eventual translation, where authors design several ways to achieve that.",
            "3": "(2) Specifically, in the encoder side, the M-BERT model is leveraged to jointly encode the $x,tx,ty$.",
            "4": "(3) The improvement in Table 1 is significant.",
            "5": "** Clarity **\n1.",
            "6": "Section 4.1 is unclear to me.",
            "7": "In the 2nd paragraph of Section 4.1, ``For each s_k in s, we try to find a matched example (tx, ty) from D what tx contains the s_k’’: (1) There should be many sentence pairs (tx,ty) that tx contains s_k.",
            "8": "Which ones should be kept?",
            "9": "(2) What’is more, if tx contains s_k, can we say that the selected $tx$ is similar to $x$?",
            "10": "(3) in experiments, the $n$ in  n-gram is set as?",
            "11": "And what if we choose different $n$?",
            "12": "2.",
            "13": "Which script did you choose to evaluate BLEU score?",
            "14": "** Significance **\n1.",
            "15": "The idea itself is not novel.",
            "16": "Compared to the related work, the novel part of this work is: (i) a new retrieval way, which is not quite clear and convincing to me.",
            "17": "(ii) a new way to aggregate multiple inputs (using M-BERT) and several different decoding methods.",
            "18": "In experiments, there is no comparison with previous retrieval based methods.",
            "19": "Similar idea also exists in [R3], which is missing from this paper.",
            "20": "The differences with [R3] should be discussed.",
            "21": "2.",
            "22": "The authors did not provide what the retrieved sentences are like.",
            "23": "Given a validation corpus (X,Y), and the corresponding retrieved (tX, tY), the authors should at least show the similarity between (X,tX), (Y,tY), which measures the retrieval quality.",
            "24": "3.",
            "25": "Please report the total training and total inference time, and make a comparison with standard Transformer model.",
            "26": "Specifically, for Table 1, the inference time of each algorithm should be reported (retrieval time included).",
            "27": "4.",
            "28": "Why do you choose case-insensitive BLEU score for En->Fr, which is not commonly used in previous baselines.",
            "29": "5.",
            "30": "Considering the BERT is leveraged, you should discuss the relation with BERT + NMT [R1,R2].",
            "31": "6.",
            "32": "The authors should conduct experiments beyond English-to-French.",
            "33": "More languages pairs should be verified.",
            "34": "Typos:\n1.\tcompared with Enhanced baseline… -> Comparison with enhanced baseline\n\n\n** Refereces **\n\nR1: Zhu, Jinhua, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu.",
            "35": "\"Incorporating bert into neural machine translation.\"",
            "36": "ICLR’20, https://arxiv.org/pdf/2002.06823.pdf\n\nR2: Yang, Jiacheng, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Yong Yu, Weinan Zhang, and Lei Li.",
            "37": "\"Towards making the most of bert in neural machine translation.\"",
            "38": "AAAI’20, https://arxiv.org/pdf/1908.05672.pdf\n\nR3.",
            "39": "Eriguchi, A., Rarrick, S. and Matsushita, H., 2019, November.",
            "40": "Combining Translation Memory with Neural Machine Translation.",
            "41": "In Proceedings of the 6th Workshop on Asian Translation (pp.",
            "42": "123-130)."
        },
        "FV8mgwcEn_": {
            "0": "`The paper argues that the existing way of using Translation Memory (TM) in neural machine translation (NMT) is sub-optimal.",
            "1": "Therefore it proposes TMG-NMT, namely Translation Memory Guided NMT, which consists of two parts, a universal memory encoder and a TM guided decoder.",
            "2": "Experiments are performed to demonstrate that their method can significantly improve the translation quality and show strong adaptation for a new domain.",
            "3": "Pros: None\n\nCons:\n\n1.",
            "4": "The main concern of this paper is that, the contributions are quite limited.",
            "5": "The authors claimed three contributions: n-gram retrieval, universal encoder, and using the copy mechanism.",
            "6": "Basically, none of them is novel.",
            "7": "- Bapna & Firat (2019) and Xu et al.",
            "8": "(2020) have used n-gram matching for retrieval.",
            "9": "- In late 2020, what is the novelty of using a multilingual BERT when encoding source sentences and retrieved TM sentences?",
            "10": "Very little if any.",
            "11": "- Likewise, using the copy mechanism to tackle rare word problems seems a regular approach.",
            "12": "Overall, I don't see any of these so-called contributions are truly technically original.",
            "13": "This paper seems a very hurry combination of some existing techniques.",
            "14": "I basically learn nothing new from reading this submission.",
            "15": "2.",
            "16": "Another one of the key concerns about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method.",
            "17": "Despite the paper stating that there have been earlier work (Gu et al.",
            "18": "2017, Can & Xiong.",
            "19": "2018, Xia et al.",
            "20": "2019) that explore Translation Memory in NMT, the paper does not compare with them and only compare to non-TM-guided baselines, making the improvement less convincing.",
            "21": "In addition, what is the language pair evaluated in this paper, which was not even mentioned...\n\n3.",
            "22": "Considering the limited results, a deeper analysis of the proposed method would have been nice.",
            "23": "Is the semantic relationship between the source sentence and TM sentences well learned in TMG-NMT?",
            "24": "What kind of translation error can be well addressed with the help of TM?",
            "25": "Further analysis of the proposed model would provide greater insight to the community.",
            "26": "4.",
            "27": "Section 5.4: The results would have been more complete if another setting is considered where the transformer is adapted to the target domain without using the TM mechanism, such as fine-tuning the vanilla transformer on the provided TM parallel sentences.",
            "28": "In this way, the adaptation ability of TMG-NMT could be better proven.",
            "29": "5.",
            "30": "To be honest, the writing of the current version seems a disaster.",
            "31": "Not to mention the impressive amount of grammar errors, many parts of the storytelling are logically incoherent.",
            "32": "For example,\n  - \"Although Bapna & Firat (2019) and Xu et al.",
            "33": "(2020) **also** use the n-gram method to search, they still need to select the corresponding sentence that maximizes the n-gram similarity with the source sentence. \"",
            "34": "- The usage of \"also\" is so weird, when you didn't even mention you are using n-gram method in advance... And by the way, what is the difference between your ngram matching and theirs?",
            "35": "You should've made it clear.",
            "36": "- \"To obtain sufficient training corpus and train network parameters more fully and effectively, in this paper, we also modify the retrieval algorithm and use a pre-trained language model (PLM) to initialize the encoder’s parameters.",
            "37": "Partially inspired by phrase-based SMT, we don’t compute the sentence level similarity score between two sentences in our retrieved method.",
            "38": "If two sentences have a common n-gram segment, we assume that they are similar, and the sentence pairs of the TM database can provide a useful segment to help improve the translation quality.",
            "39": "Currently, many studies have proven that PLM can offer valuable prior knowledge to enhance the translation performance of NMT (Weng et al., 2020; Song et al., 2019), So we also employ PLM to initialize the parameters of the encoder and give encoder well-trained parameters as a starting point.\"",
            "40": "- What is the logical relationship b/w the first sentence and the second one?",
            "41": "- Please check minors for more details.",
            "42": "*****\nMinors:\n1) It would have been nice to see that the format of the reference are unified.",
            "43": "2) Khandelwal et al, 2020 [1] propose a novel way to incorporate Translation Memory into NMT which may bring you more thoughts towards using TM.",
            "44": "Typos: too many.",
            "45": "1.",
            "46": "Equation (2): a redundant close paren\n2.",
            "47": "Section 3.1 penultimate paragraph: l-th encoder layer -> L-th encoder layer\n3.",
            "48": "Section 4.2 First paragraph: N->M n->m\n\nGrammar errors:\n\nToo many.",
            "49": "E.g., In the contribution part in the intro, the second and the third items start with \"does\" and \"apply\".",
            "50": "What are the subj of these two verbs?",
            "51": "Please try to properly use Grammarly to check your writing before submission.",
            "52": "[1] \"Nearest Neighbor Machine Translation.",
            "53": "Khandelwal et al.",
            "54": "2020 arXiv.\"",
            "55": "******\nReasons for score:\nThe novelty of this paper is basically none.",
            "56": "The experimental results are limited and the comparison with prior work is none, which cannot fully demonstrate the effectiveness of the proposed method."
        },
        "Tu1a3WkxQ14": {
            "0": "This paper describes several improvements on using information from a Translation Memory (TM) in Neural Machine Translation (NMT).",
            "1": "In the spirit of several prior work, the approach relies on 1) a retrieval step to obtain TM content that is related to the current source sentence to translate, 2) an encoder combining the source sentence with retrieved TM content, and 3) a decoder using the joint encoded information to produce a (target) translation.",
            "2": "Experiments are conducted on benchmark French-English data, showing consistent improvement over classical baselines.",
            "3": "Translation Memories are important Computer-Aided Translation (CAT) tools, likely the most widely used CAT tools by translation professionals and agencies.",
            "4": "As such, it is important to study how they can be used to improve translation quality for example through inclusion in NMT.",
            "5": "This study is therefore a welcome addition to the relatively limited work investigating this topic.",
            "6": "I personally wish there was more work on integrating existing translation resources in MT.",
            "7": "However, the experimental setup does not really correspond to a typical TM use.",
            "8": "It essentially leverages the idea of reusing close matches to a source sentence in the training data.",
            "9": "The idea is interesting, but only loosely related to TM.",
            "10": "On the other hand, the experiments in Section 5.4 are much closer to a TM, it is unfortunate that these experiments are quite limited.",
            "11": "Despite the general relevant and interesting focus of this work, there are a number of issues discussed below, related mainly to modeling and to the experimental evaluation.",
            "12": "MODELLING:\nEach of the three components on the method (retrieval, encoding, and decoding) introduces some novelty.",
            "13": "There are also a number of issues to resolve.",
            "14": "1) The retrieval (sec.",
            "15": "4.1) uses an n-gram matching technique, which is contrasted with the usual computation of sentence similarities (edit distance, fuzzy match or idf-based score).",
            "16": "I basically don’t buy the advantages put forward in the paper:\n- The cost of retrieval in a TM is dominated by the requirement to go over the entire memory for each source sentence, not by the computation of the score.",
            "17": "The n-gram matching would still incur that cost, unless some smart way to retrieve matching sentences (such as an inverted n-gram index) is implemented.",
            "18": "Unfortunately there is no detail at all in the paper about how the ngram matching is performed.",
            "19": "- The fact that one can retrieve N matched sentence pairs for each (x,y) pair in the training data is no different from retrieving the top-N sentence pairs using any of the usual similarity metrics.",
            "20": "Additionally, when retrieving N>1 sentence pairs from the TM, it is not entirely clear how the N pairs are used.",
            "21": "One interpretation of the second paragraph of 5.1 is that this actually yields N different training instances at training time, while one match is randomly picked at prediction time.",
            "22": "This should be clarified.",
            "23": "In addition, this would introduce additional randomness at prediction time, producing possibly different target translations.",
            "24": "It would be good to assess the impact of this choice on the performance, and compare against the obvious choice of picking the closest match.",
            "25": "2) The encoding is straightforward but clever.",
            "26": "It is not entirely clear how the encoder keeps track of the split of the context into I+M+N (one assumes here that N is no longer the number of matched pairs, but the length of the TM-target context) — is it through propagating the separation marker, hardcoded in the encoder, through some other way?",
            "27": "3) The decoding is done in four different manners, offering various ways to integrate the TM-target information into the prediction.",
            "28": "However, the description of « TM-pointer Decoder » in Section 4.3 seems faulty: Eq.",
            "29": "11 shows how to get S_x^l through the self-attention mechanism and Eq.",
            "30": "10 illustrates the concatenation mechanism in the TM-concat decoder, they can’t help get g_t and the attention distribution vector a.",
            "31": "EVALUATION:\n\nStrengths:\n+ Shows consistent non trivial gains\n+ Uses a large corpus, with several domains\n+ Interesting « domain adaptation » mode shows good results [Sec.",
            "32": "5.4]\n\nWeaknesses:\n- Limited to French-English (close languages with lots of cognates, lots of resources, high performance) — it would be interesting to show how this works on radically different languages, especially in a lower resource setting where an existing TM may greatly help.",
            "33": "- Limited comparison to a couple baselines.",
            "34": "None of the methods cited in the related work is tested against.",
            "35": "- It is unclear what significance test was used, if any, to back the claim of « significantly outperforms » (e.g.",
            "36": "end Section 5.4).",
            "37": "Small typos and clarifications:\n« by reuse existing » (Sec 1)\n« nmt » (Sec 2) -> NMT\n«  Formally, We » (Sec 4.2)\nWhat is the sentence after Eq.",
            "38": "15 (« The p_copy… ») trying to tell us.",
            "39": "This is not clear.",
            "40": "« we set the N to 10 » (Sec 5.1) \n« set the size … is » (Sec 5.2)\n« we valid the translation performance » (Sec 5.4)"
        },
        "nt7WTftPnWQ": {
            "0": "This paper presents a way to integrate a translation memory into a neural machine translation model.",
            "1": "They introduce a novel method for finding matching sentences (which they do not properly describe) and they also have 4 methods from including this information into an NMT model, with the best approach using multiple sources combined with gated attention and a pointer network for copying rare words.",
            "2": "They show improvements of up to 2.64 BLEU over a reasonable but not SOTA baseline for English-French task.",
            "3": "My major problem with this paper is that the model and the experiments are not adequately described.",
            "4": "The segment based retrieval function is one of the two core contributions of the paper, but it cannot be fully described.",
            "5": "They claim that any sentence in the TM that matches any n-gram in the sentence x.",
            "6": "This would result in a huge set of matching sentences, where 1-gram that matches could be the work \"the\".",
            "7": "So there must be a way of ordering the matches or scoring the matches to result in a good set of N matches where N=10 here.",
            "8": "How do they do this?",
            "9": "At no point to they describe where they get the translation memory from and/or how big it is.",
            "10": "This is a fundamental part of the model.",
            "11": "I have searched and can't find this - did I miss it?",
            "12": "I find this to be an extremely worrying omission - especially as there should be a discussion about the properties of this TM and how it compares to for instance using a phrase-table/lexicon etc.",
            "13": "Their translation baselines are reasonable but probably significantly lower than SOTA (44.6 BLEU vs highest published 45.6 on WMT14 from http://nlpprogress.com/english/machine_translation.html)\nThey are not directly comparable however because they should be reporting detokenized BLEU using sacreBLEU.",
            "14": "Their scores are tokenised lowercase text which means that their numbers are artificially elevates when compared to true cased tokenised text."
        }
    },
    "a9nIWs-Orh": {
        "UHtN1YWqVb": {
            "0": "\n### Summary \nIn this paper, the authors propose a fine-tuning scheme for pre-trained language models, called Hidden Representation Extractor (HIRE).",
            "1": "The proposed method consists of two components.",
            "2": "1) Hidden representation extractor (HIRE): a two-layer bi-GRU model summarizes the hidden representation of the encoder with a weighted-sum scheme.",
            "3": "2) Fusion network: combine the weighed representation with original representation via the bi-GRU model.",
            "4": "The authors apply the proposed approach on top of RoBERTa and evaluated on general benchmark GLUE.",
            "5": "The experimental results show that the proposed method could give moderate improvements over RoBERTa baseline system.",
            "6": "The authors' main contribution lies in two components: HIRE and fusion network.",
            "7": "However, unfortunately, both components are not novel to me.",
            "8": "1) The idea of HIRE model is very similar to utilizing ELMo in down-stream tasks, where the hidden representations of each layer are weighted.",
            "9": "2) The central part of the fusion network utilizes a concatenation scheme of [A;B;A+B;A*B], which is also a widely used approach, similar to Chen et al.",
            "10": "(2017), Seo et al.",
            "11": "(2017), Hu et al.",
            "12": "(2019), etc.",
            "13": "Besides the novelty of the proposed components, the experiments are also not well-designed.",
            "14": "I assume the proposed method could apply to various pre-trained language models (PLMs), as this is an approach that aims to apply on the fine-tuning stage.",
            "15": "However, the authors only present the results on top of RoBERTa model, which cannot demonstrate that the proposed model can be generally applied to various PLMs.",
            "16": "I would suggest the authors evaluate their approach in a more general way, such as build on top of various PLMs, or apply on PLM in another language.",
            "17": "These would have helped to better demonstrate the effectiveness of the proposed method.",
            "18": "Considering the novelty and generalizability of the proposed method, I recommend rejection for this paper.",
            "19": "[1] Chen et al.",
            "20": "Enhanced LSTM for Natural Language Inference.",
            "21": "[2] Seo et al.",
            "22": "Bidirectional attention flow for machine comprehension.",
            "23": "[3] Hu et al.",
            "24": "Read+ verify: Machine reading comprehension with unanswerable questions.",
            "25": "### PROs\n1.",
            "26": "A possible useful framework for fine-tuning down-stream tasks for pre-trained language models.",
            "27": "### CONs\n1.",
            "28": "The design of the components is not novel.",
            "29": "2.",
            "30": "The experiments are not well-designed, and the results only show marginal improvements over RoBERTa.",
            "31": "3.",
            "32": "The generality of the proposed method is not well-studied, as the experiments are only performed on top of RoBERTa.",
            "33": "### Questions\n1.",
            "34": "Unlike Transformer-based models, RNN models (such as Bi-GRU) are not computationally efficient.",
            "35": "How about your training time compared to the baselines?",
            "36": "According to Table 2, using HIRE will increase the total parameter from 355 to 437, which will slow down the inference speed.",
            "37": "2.",
            "38": "The authors did not cite ELECTRA (Clark et al., 2020) for comparison.",
            "39": "After comparing the results, the proposed model is not competitive against ELECTRA, while it would cost more training and inference time, which will weaken the impact of the paper.",
            "40": "I strongly recommend the authors also carry out experiments on top of ELECTRA (or ALBERT-xxlarge, etc.)",
            "41": "to see if your approach generalizes well on various PLMs.",
            "42": "### Minor Reviews\nI did not go through every detail of the writing and only list a few of the issues here.",
            "43": "1. page 1, section 1: CoNNL-2003 -> CoNLL-2003\n2. page 1, section 1: (Peter et al., 2018b) points out ... -> Peter et al.",
            "44": "(2018b) points out ...\n3. page 3, section 2.3: Tansformer -> Transformer; yeild -> yield; elementwise -> element-wise;"
        },
        "srwFlBgZ_G3": {
            "0": "This paper presents a new mechanism, called HIRE, to extract more information from the intermediate layers of pre-trained models, which will be further fused with the last layer of pre-trained models.",
            "1": "The main contribution of this work is the newly proposed dynamic feature extractor HIRE and the fusion network.",
            "2": "Experiments confirmed the effectiveness of the proposed method, and some interesting observations on the importance of different layers for different tasks were given (i.e.",
            "3": "Figure 2).",
            "4": "However, the proposed HIRE and Fusion modules can be viewed as the combination of some widely used deep learning mechanisms (e.g.",
            "5": "Bi-GRU, softmax), using Bi-GRE to represent a sequence is a widely adopted choice, so the contribution in modeling is quite limited.",
            "6": "Some existing works, like Self-Adaptive Hierarchical Sentence Model https://arxiv.org/pdf/1504.05070.pdf, have already proposed similar ideas to use more information from the intermediate layers of deep models.",
            "7": "The paper is well-written and organized.",
            "8": "I have several concerns:\n1.",
            "9": "All the models are evaluated on the GLUE dataset, experiments on more challenging tasks like QA (e.g.",
            "10": "SQuAD 1.1/2.0) should be added.",
            "11": "It would be interesting to visualize the importance scores of each layer for this challenging task.",
            "12": "2.",
            "13": "As illustrated in Figure 2, different downstream task highly depends on different intermediate layers, the authors are suggested to conduct experiments under the multi-task setting, since the HIRE can adaptively select the intermediate layers, the proposed method should have a big advantage for this setting."
        },
        "ik-lwhK9Or1": {
            "0": "The paper proposes a method to improve the downstream performance of a pretrained Transformer on NLP tasks.",
            "1": "The core idea is to not only use the output of the last Transformer layer for prediction, but let the model decide how to fuse the information from intermediate layers as well.",
            "2": "To dynamically decide which intermediate layers to use depending on the input example, the model uses a mechanism conceptually similar to self-attention, which yields a normalized importance score for each layer.",
            "3": "The importance-weighted sum then yields a complementary representation to the last layer.",
            "4": "Lastly, another network produces a final, integrated representation from the output at the last layer and the complementary representation, which is then used for prediction.",
            "5": "The model is evaluated on the GLUE benchmark.",
            "6": "Strengths:\nThe paper is well written and the experimental evaluation seems correct.",
            "7": "The paper has a nice ablation study which shows that the learned importance scores, the complementary representations, and the fusion network are needed to reach the model's full performance.",
            "8": "Weaknesses:\nThe main weakness is that the proposed extension to the baseline is relatively complex and rather heavy-weight in terms of new parameters (introducing ~25% more parameters compared to the baseline according to Table 2), yet only achieves a very marginal relative improvement of 0.2 percent over the baseline.",
            "9": "It seems likely that this improvement could be achieved through much simpler means, e.g., additional self-attention layers on top of the last pretrained layer.",
            "10": "This is supported by the fact that the paper's analysis of the proposed importance score mechanism doesn't show comprehensible patterns (or the paper doesn't talk about it).",
            "11": "I lean slightly towards rejection, because, although the proposed model is reasonable and could have potential, the experiments do not currently demonstrate that potential, and I hence expect it hard for the community to learn from this paper.",
            "12": "Questions:\n\n* You cite several papers which demonstrate that different layers of pretrained Transformers encode different information, which is the motivation for your architecture.",
            "13": "However, the cited sources use a feature extracting approach, i.e., they don't fine tune the encoder on their target task.",
            "14": "In the finetuning scenario (yours), can't we expect the model to learn to simply forward the relevant information from intermediate layers to the last layer?",
            "15": "* In Section 5 you analyze which intermediate layers are used for which task.",
            "16": "You state that we can see that the model learns to prefer different layers for each task, but don't go into detail.",
            "17": "Can you relate the nature of the task to the layers it produces?",
            "18": "For example, why does QNLI rely on layers 2-4 to some extent despite being a rather high-level understanding task?",
            "19": "* In Figure 2, right side you observe that different examples tend to different layers.",
            "20": "Did you qualitatively inspect the examples and try to find a pattern/link to their preferred intermediate layers?",
            "21": "Suggestions:\n\n* RoBERTa already achieves super-human results on the GLUE benchmark, meaning that it is probably already close to some upper limit.",
            "22": "This makes it even more difficult for your model to substantially improve upon it.",
            "23": "I would consider evaluating it on the SuperGLUE benchmark instead, since there RoBERTa is not so close to human performance yet.",
            "24": "* Your model is designed to make use of representations at lower layers if a task requires it.",
            "25": "But all the tasks in your experiments are evaluated on high-level natural language understanding tasks, which typically require representations at higher layers.",
            "26": "This makes it more likely that your model does not improve much over the baseline, because the last layer will arguably already contain much of the information needed for the tasks.",
            "27": "I think you should consider different tasks in your experiments to increase the chance of significant improvements.",
            "28": "* In the current state, it is unclear whether your improvements are coming from the specific model you chose or merely from the fact that you added a lot of parameters.",
            "29": "As a control, you could include an experiment where you simply add several self-attention layers on top of RoBERTa such that you match the number of parameters of your model."
        }
    },
    "JAlqRs9duhz": {
        "3_jEVP1T5t": {
            "0": "## Summary\n\nThis work proposes an effective modification of language model token-level distribution during the training which prevents some forms of degeneration such as repetitions and dullness.",
            "1": "The approach is based on the idea of encouraging the model to use tokens which were not observed in the previous context so far.",
            "2": "In other words, this method changes softmax distribution such that unseen/novel tokens is being rescaled with a given hyper-parameter $\\gamma$ (eq.4).",
            "3": "Authors conduct several experiments using different tasks such as open-ended generation, image captioning and abstractive text summarization.",
            "4": "As a result they confirm substantial improvement over the standard mle training and **token-level** unlikelihood training.",
            "5": "In addition to analysis of their method, authors discuss a potential issue of unlikelihood training criterion and how their approach avoids this issue.",
            "6": "## Strong points\n\n1.",
            "7": "Main method is easy to understand and I believe is easy to implement: this work can be a motivating example for future research towards degenerative text generation.",
            "8": "From my understanding there are some interesting future work such as setting individial $\\gamma$ for some tokens, specific masks for novel token sets etc.",
            "9": "2.",
            "10": "Large-scale experiments **with some human evaluation**.",
            "11": "I enjoyed seeing improvements on multiple tasks including summarization (with automatic metrics at least).",
            "12": "In addition, analysis of stochastic decoding used in directed generation is meaningful and highlights the importance of this work.",
            "13": "Appendix includes detailed description of each experiment protocol including the protocol of human evaluation.",
            "14": "Convincing examples of generated continuations are given in the appendix.",
            "15": "3.",
            "16": "Code with implemented method and experiments is provided: code is based on fairseq custom module which makes it relatively easy to extend and do more research with it.",
            "17": "## Weak points\n\n1.",
            "18": "Misleading comparison choice: authors claim to compare their approach with unlikelihood training (UL) and choose **token-level** UL loss without even mentioning the existence of **sequence-level** UL loss which works better based on the original paper.",
            "19": "In fact, the whole narrative looks like sequence-level version does not exist.",
            "20": "Simply stating that ScaleGrad is being compared with token-level UL (which works worse than sequence-level) would make future conclusions more clear.",
            "21": "2.",
            "22": "Some relevant work got completely ignored.",
            "23": "I am not aware of the full variety of prior work for this popular problem these days, but there is one i am aware of: https://arxiv.org/pdf/2003.11963.pdf, where authors *do similar gradient analysis* as here.",
            "24": "If this one is missed, I wonder what else may be missing in the related work here.",
            "25": "3.",
            "26": "No human evaluation for text summarization.",
            "27": "Given known weakness of automatic metrics in text summarization task and the fact that authors did human eval for text completion, I wounder why they decided to exclude it from here (I can totally see budget limitation as one of the factors, and saying this explicitly would be helpful).",
            "28": "4.",
            "29": "The potential issue of UL (sec.",
            "30": "5.4) does not look convincing.",
            "31": "From my understanding the main line there is \"*UL essentially rejects the ground truth token in such special cases (subject to the choice of the value of $\\alpha$).*\".",
            "32": "This statement on its own is not clear to me and seems to be disconnected from the previous one: \"*In this case, the norm increases as pk increases, which contradicts with the optimization principle.",
            "33": "*\" I agree that in this case ($\\alpha=1, p_\\text{neg}>0.5$) the norm increases as $p_k$ (prob of ground truth token) increases, but I don't see any problem or contradiction here.",
            "34": "From my understanding when $p_\\text{neg}$ goes above some threshold, then norm of the gradient of $p_k$ is growing proportionally to $p_k$.",
            "35": "Keep in mind that as $p_k$ is increasing, $p_\\text{neg} > \\frac{1}{1+\\alpha}$ is eventually dissatisfied (because of softmax property), i.e.",
            "36": "I don't see any issue.",
            "37": "Would be great if authors can elaborate more about this.",
            "38": "## Recommendation\n\nOverall I vote for accepting this work as long as main concerns will be addressed/discussed.",
            "39": "This is a decent approach with a strong experimental evidence and it will be useful for text generation community in the future research.",
            "40": "I would be even more satisfied if authors can discuss / clarify / address the points I highlighted above.",
            "41": "## Comments and questions\n\n1. stochastic beam search was mentioned as one of the efforts to solve text generation issue, but I believe it is more about doing sampling without replacement on the sequence-level.",
            "42": "I am curious if authors may provide some perspective on how stochastic beam may reduce the degeneracy (e.g.",
            "43": "compared to simple beam search).",
            "44": "2.",
            "45": "In sec.",
            "46": "2.1 teacher forcing is described as being used \"*used to train neural text generation models for faster convergence.*\".",
            "47": "I wonder how can one use MLE criterion on the token level without teacher forcing?",
            "48": "E.g.",
            "49": "if we use predicted token as the context in the next time step, then we have no target truth token for the next time step (since the context is changed).",
            "50": "In other words I think that teacher forcing is essential if we aim to maximize the probability of training sequences.",
            "51": "3.",
            "52": "In section 2.2: \"*thus reformalizing the probability distribution*\", this wording reformalizing sounds a bit weird to me, but it is clear what authors had in mind.",
            "53": "4.",
            "54": "Did you think about combining UL loss (both seq level and token level) with ScaleGrad?",
            "55": "From my understanding it is possible since ScaleGrad emphasize novel tokens, and this softmax from scalegrad may be used in the UL loss, which may help even further!",
            "56": "Importantly, sequence-level UL would allow to use ScaleGrad on the sequence-level, since there is no need for ground truth target there, and while penalizing the repeating words with UL, ScaleGrad would emphasize the novel words (sounds promising to me).",
            "57": "Overall the paper narrative looks as combating the UL method (with some misleading gaps about token vs. sequence level), but to me it looks like they may work together!"
        },
        "hZDHvFAk2G": {
            "0": "The authors propose to modify a language model's token-level distributions by rescaling the output probability of tokens that do no appear in the context ('novel tokens').",
            "1": "The authors show improvements over MLE and token-level unlikelihood in terms of repetition, with increases in perplexity.",
            "2": "#### Clarity and significance\n- **ScaleGrad motivation**.",
            "3": "There are many different ways to change the gradient, e.g.",
            "4": "any regularization function, any scaling of the output probabilities, or even gradient clipping modifies the gradients that are used to update the model.",
            "5": "As a result, the presentation of their method as \"a modification straight to the gradient of the loss function\" seems odd, and the name ScaleGrad suggests that they are proposing the general notion of rescaling gradients.",
            "6": "Instead, they propose to scale a novel set's output probabilities then renormalize.",
            "7": "- **Specific solution**.",
            "8": "The method is specifically designed around the 'novel set', which could limit its significance.",
            "9": "The authors speculate that they can alter the novel set (e.g.",
            "10": "for importance or factual correctness), but this appears to be nontrivial.",
            "11": "- **Unlikelihood discussion**.",
            "12": "The discussion in 5.4 deals with the case of $p_{neg}>0.5$, meaning that the probability of the ground-truth token $p_*$ is $<0.5$ (due to normalization).",
            "13": "If I'm understanding their argument, the authors argue that the resulting gradient contradicts the fact that the gradient should go to zero at an optimum.",
            "14": "However, *the model is not at an optimum* if $p_<0.5$.",
            "15": "Could the authors clarify the statements \"the model is not to learn to predict the ground truth tokens correctly\", \"contradicts the optimization principle\", and \"essentially rejects the ground truth token\"?",
            "16": "- **Method for promoting novelty**.",
            "17": "It's unclear why this specific method (renormalizing over the novel set) is the best or simplest method for promoting novelty.",
            "18": "A downside is that we no longer know which objective the model is optimizing.",
            "19": "In the appendix, the authors discuss a variant that uses an additional loss (Section I), yet do not perform an empirical comparison with that or other 'novelty promoting' variations.",
            "20": "They argue in Section I that this suffers 'the similar issue as the UL loss', but that issue was unclear (see point above).",
            "21": "Overall I'm borderline on this paper: the authors do perform a lot of experiments and show improvements, but I'm hesitant that scaling novel tokens and renormalizing the model's output distribution is significant."
        },
        "Y7_SCEsK8og": {
            "0": "**I have updated this review after noting the authors’ detailed response.",
            "1": "**\n\nThis paper focuses on the problem of “Neural Text Degeneration”—where text sampled from a language model can either be too repetitive and bland or too random and nonsensical.",
            "2": "The authors focus largely on the former problem, proposing a finetuning loss that specifically incentivizes the use of tokens that have not yet been decoded in the given document.",
            "3": "The authors test whether this improves repetition and unique token coverage with greedy decoding in open-ended generation.",
            "4": "A small human study is conducted and the proposed method, ScaleGrad, is found to outperform MLE and Unlikelihood Training (UT).",
            "5": "Similarly good results are obtained on Image Captioning with and without trigram repetition blocking.",
            "6": "On Abstractive Summarization BeamSearch is used and again outperform MLE and UT.",
            "7": "Analysis attempts to make comparisons across different decoding strategies, though coverage of different variations is limited.",
            "8": "The authors argue that stochastic decoding is outperformed by ScaleGrad, though they note that trigram blocking still helps ScaleGrad.",
            "9": "Multiple hyperparameter settings are shown, with some analysis on how gamma can be chosen to get a desired behavior.",
            "10": "Finally, the authors analyze why UT may not be as effective: it penalizes gold repetitions too much and does little for other tokens.",
            "11": "Strength:\n- The results are good for greedy decoding\n- The method is well motivated and well explained\n- The analysis regarding Unlikelihood Training is interesting\n\nWeaknesses\n-  The results shown do not make proper comparisons across models, baselines, and hyperaparameters over all tasks.",
            "12": "- Results for stochastic decoding should have been shown across tasks.",
            "13": "- Despite citing the need for awkward rules such as trigram repetition blocking as a reason to propose ScaleGrad, trigram repetition blocking still helps significantly.",
            "14": "- Some details are hidden away in the appendix, which I had to read thoroughly in order to fully understand the comparison.",
            "15": "I recommend to reject this paper, because the experimental comparisons are not quite fair and because of implicit argumentation about what Greedy decoding can or should do that is never made explicit.",
            "16": "The following two paragraphs are obsolete, because the authors shared experimental results from a larger set of experiments.",
            "17": "> The results in Table 1—which show the main metrics of interest on open-ended generation—are missing two key points of comparison: ScaleGrad is only show with gamma=0.2, even though gamma=0.5 & gamma=0.8 are used for the rest of the experiments, giving us little idea of how these metrics change over hyperparameter settings.",
            "18": "This is despite the fact that two hyperparameter options for Unlikelihood Training are shown.",
            "19": "In a footnote on page 6, for directed generation, the authors state “Although UL was originally proposed for open-ended generation, it is applicable to directed generation.",
            "20": "We did  the same scale hyper-parameter search for UL.",
            "21": "Details can be seen in Appendix E.” However, in Appendix E two hyperparameter settings for alpha are shown, the same two as used in Table 1, but two hyperparameter settings for gamma in ScaleGrad are shown _neither of which are shown in Table 1_ nor are repetition or uniqueness numbers shown for these hyperparameters settings anywhere in the paper or the appendices.",
            "22": "This makes me question whether the improvements shown in Table 1 hold across hyperparameter settings as the authors claim in their analysis of Figure 1.",
            "23": "> However, Figure 1 is missing necessary data points and comparison.",
            "24": "First of all gamma=0.2 is not shown, though at least gamma=0.1,0.3 are so it can be somewhat inferred.",
            "25": "That is suboptimal, but this graph does not even go up to gamma=0.8, which is what is used in the Abstractive Text Summarization experiment!",
            "26": "Furthermore, the number in Figure 1 (b) cannot be directly compared to other decoding methods, because they are an average of repetition metrics shown in Table 1.",
            "27": "Luckily, Figure 1 (c) can be compared, and if cross-referenced with Table 1, shows that Unlikelihood Training does better than ScaleGrad with a higher gamma.",
            "28": "However, Figure 1 has no data on either Unlikelihood Training or a human baseline.",
            "29": "It really should not be necessary to go looking through Table 1, Figure 1, and Appendix F to see that Unlikelihood Training is outperforming ScaleGrad on some metrics.",
            "30": "Worse, the data presented in Figure 1 (b) actually makes comparison impossible, which makes me uncomfortable about the universally positive results in Table 1.",
            "31": "On page 4 the authors write “Following Welleck et al.",
            "32": "(2020), we apply greedy decoding in our experiments in this section.",
            "33": "This allows us to evaluate the modeling capability exclusively.” We will get into the matter of comparison to Welleck et al.",
            "34": "2020, but I would like to begin by addressing whether Greedy Decoding is a neutral choice that only tests modeling capability, because it is clearly not.",
            "35": "There is a spectrum of generation algorithms between probability maximization and straight-forward sampling.",
            "36": "Greedy is closer to probability maximization, but it only maximizes local probabilities (Meister et al., 2020) and inevitably comes-up with lower probability outputs than Beam Search or Bound & Branch (Stahlberg & Byrne, 2019).",
            "37": "Welleck et al., 2020 show that Greedy Decoding results in better text along their proposed metrics for open-ended generation.",
            "38": "Since Greedy Decoding is not a “neutral” choice, I do not believe it is appropriate to exclude stochastic decoding baselines from the given comparisons.",
            "39": "Stochastic decoding algorithms such as sampling, top-k sampling, and Nucleus Sampling usually do very well on repetition and uniqueness metrics.",
            "40": "Indeed, they can be seen to outperform all the other models on Table 16 in Appendix H.\n\nIn the analysis section, tables are quite limited in their coverage.",
            "41": "In Table 6 no comparisons are made to systems that have not been trained with ScaleGrad, and these algorithms were not reported on in Table 1 so no comparison can properly be made even if the reader goes searching for the data.",
            "42": "In Table 8, Unlikelihood Training is not included in the comparison even though it does very similarly to ScaleGrad on the same task in Table 5.",
            "43": "Finally, Table 5 shows that trigram blocking still helps significantly on ScaleGrad trained systems.",
            "44": "This is understandable, but disappointing since getting rid of these kind of rules is described as the reason for proposing ScaleGrad.",
            "45": "Altogether, I feel the comparisons made in this paper are not quite convincing and the argument about why Greedy decoding, a deterministic algorithm, should even be able to match the properties of a large, noisy distribution is not properly fleshed-out.",
            "46": "Meister, Clara, Ryan Cotterell, and Tim Vieira.",
            "47": "\"If Beam Search Is the Answer, What Was the Question?.\"",
            "48": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "49": "2020.",
            "50": "Stahlberg, Felix, and Bill Byrne.",
            "51": "\"On NMT Search Errors and Model Errors: Cat Got Your Tongue?.\"",
            "52": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).",
            "53": "2019."
        },
        "84s-GIHdgFU": {
            "0": "The paper presents a  technique to encourage generating certain tokens (i.e.",
            "1": "non-repetitive ones) in text generation.",
            "2": "The idea is to scale the softmax probability  for certain words (in the novel set) by a factor of gamma.",
            "3": "The authors show how this affects learning by deriving the effect on the gradient.",
            "4": "Many experiments are presented both on open ended generation (language modeling, story telling) as well as abstractive text summarization to justify the method.",
            "5": "The model seems to encourage more diversity than unlikelihood training in open ended generation (while still maintaining a lower perplexity).",
            "6": "The gains in summarization are marginal.",
            "7": "I have two questions:\n(1) How are lambda and alpha connected to \\gamma in Section 3?",
            "8": "This make the method section clearer.",
            "9": "(2) How much is the model discouraged from generating stop words like \"the\" or \"a\" (and how does this affect fluency)\n\nPros:\n-Well justified and simple method to solve a relevant problem in text generation.",
            "10": "-Lots of experiments, gains in open ended generation seem decent.",
            "11": "Cons:\n-Gains on summarization are marginal / non-existent suggesting that this is not as large of a problem for more constrained tasks.",
            "12": "-Some clarity on the questions above would be helpful."
        }
    },
    "I-VfjSBzi36": {
        "LjymxTRiccT": {
            "0": "This paper tackles a very important and under-studied problem: reducing the cost of training NLP models.",
            "1": "The authors present a method that builds on the lottery ticket hypothesis (LTH).",
            "2": "The authors first identify redundant structures early during training, then prune these structures, which leads to faster training.",
            "3": "The authors experiment both with pre-training and fine-tuning of contextual models (BERT-{base,large}) and claim large reduction in training time, with reasonable loss in performance.",
            "4": "Despite very encouraging results, several important methodological questions about the source of the efficiency gains and other aspects of the paper are left unanswered.",
            "5": "I cannot recommend accepting this paper in its current form, but am looking forward to reading the authors' response which might clarify things.",
            "6": "Detailed comments:\n\nTraining (especially pretraining) costs have been going wild in AI and NLP more particularly, which leads to large costs ([1]) as well as potential environmental problems ([2]).",
            "7": "Reducing these costs could have a very high impact on the field, allowing many more researchers to participate in state-of-the-art research [3].",
            "8": "As a result, this paper has a great potential, and its results seem very promising.",
            "9": "Nonetheless, the current paper leaves too many open questions regarding the validity of the experiments.",
            "10": "First, much of the improvement (I think) comes from reducing the number of epochs and/or the number of steps.",
            "11": "For fine-tuning, the authors run their model for 2.2 epochs, while their baseline model runs for 3 epochs, roughly 30% more which accounts for much of the reduction observed in Table 2.",
            "12": "Similarly, for pretraining, the model runs 80% of the training steps (20% reduction), which accounts much of the training time reduction reported on section 4.3.",
            "13": "Running a baseline model that runs *for the same amount of time* is essential to appreciate the contribution of this work (e.g., repeat the same analysis in Figure 3 for the vanilla BERT).",
            "14": "Second, the authors argue for a large reduction in runtime, but are very cryptic about how they actually measure this reduction (footnote 2), while reporting number in ranges of 5%.",
            "15": "As the main contribution of this paper is the increased efficiency of the proposed approach, it must be clear how efficiency is measured.",
            "16": "Finally, writing in general can be made clearer:\n\n1.",
            "17": "Last sentence of intro: \"without scarifying accuracy\" seems like an inaccurate description of the results presented in this paper.",
            "18": "around 1% might be reasonable for 30-40% reduction in training time, but it is certainly a reduction in accuracy.",
            "19": "2.",
            "20": "Some description of Network sliming would help\n3.",
            "21": "The term \"intermediate neurons\" (section 3.2) was unclear to me.",
            "22": "4.",
            "23": "Section 3.3: how is mask difference defined?",
            "24": "5.",
            "25": "Figure 1 was unclear to me.",
            "26": "What do the axis represent?",
            "27": "The authors say \"the axes in the plots are the number of training steps finished.\"",
            "28": "so why do you need two of them?",
            "29": "6.",
            "30": "The \"Non-trivial Sub-network\" paragraph feels like it should be part of the Experiments section.",
            "31": "7.",
            "32": "Implementation details are only given for the vanilla BERT Are they similar to the EarlyBERT model as well?",
            "33": "8.",
            "34": "\"Since we observe that the randomly pruned models do not competitive performance ...\": how uncompetitive?",
            "35": "I would have liked to see these results (also, please fix grammar in this sentence)\n9.",
            "36": "\"Reducing it to 80% seems to be a sweet point with the best balance between performance and efficiency.\"",
            "37": "-> I would disagree.",
            "38": "The graph indicates that for MNLI and QNLI 60% seems like a better choice.",
            "39": "Questions:\n1.",
            "40": "\"We observe empirically that if pruned globally, the attention heads in some layers may be completely removed, making the network un-trainable.",
            "41": "\": in this case, couldn't the authors remove a full layer?",
            "42": "2.",
            "43": "The difference in the ablation results seem quite small (tables 3 and 4).",
            "44": "Are they statistically significant?",
            "45": "Minor:\n1.",
            "46": "Missing period at the end of the first paragraph in the related work section.",
            "47": "2.",
            "48": "Second paragraph of related work: McCarley et al.",
            "49": "(2019) appears twice with different descriptions, is this intentional?",
            "50": "3.",
            "51": "The authors say \"we focus on larger datasets from GLUE (MNLI, QNLI, QQP and SST-2), as it is less meaningful to discuss efficient training\", but then report and analyze results from other GLUE datasets as well.",
            "52": "4.",
            "53": "\"Hon downstream tasks with smaller learning rate\" -> Do you mean smaller datasets?",
            "54": "References:\n[1] Sharir, O., Peleg, B., and Shoham, Y.",
            "55": "(2020).",
            "56": "The cost of training NLP models: A concise overview.",
            "57": "arXiv:2004.08900.",
            "58": "[2] Strubell, E., Ganesh, A., and McCallum, A.",
            "59": "(2019).",
            "60": "Energy and policy considerations for deep learning in NLP.",
            "61": "In Proc.",
            "62": "of ACL.",
            "63": "[3] Schwartz, R., Dodge, J., Smith, N. A., and Etzioni, O.",
            "64": "(2019).",
            "65": "Green AI.",
            "66": "arXiv:1907.10597."
        },
        "LxOVxNXxUNX": {
            "0": "The main contribution of this work is to use Early Bird Lottery Tickets to reduce pre-training and fine tuning time for BERT.",
            "1": "In order to reduce computation, the authors propose pruning the number of attention heads and neurons in the fully connected layers.",
            "2": "They demonstrate that the technique works for BERT-Base and BERT-Large for GLUE and SQUAD tasks.",
            "3": "The work is well-thought through and authors do a good job of explaining their approach and other existing works using lottery tickets.",
            "4": "The usage of lottery tickets during the pre training phase is the biggest strength of the paper since it can result in significant computational savings.",
            "5": "The authors perform interesting ablation studies but they could be augmented by a few more experiments (see below).",
            "6": "The paper could also be improved by comparison with relevant prior work.",
            "7": "Here are some thoughts and questions that could help improve the paper: \n\n* Experiments in Section 4.2 are only during the fine tuning stage.",
            "8": "How do these results compare with prior work for drawing lottery tickets in transfer learning for NLP (Chen et al, Prasanna et.",
            "9": "al)?",
            "10": "Particularly, it would be good to compare against Chen et al since their work is very relevant.",
            "11": "* How long is the searching stage followed by the efficient training stage?",
            "12": "Does this remain the same for all the tasks?",
            "13": "Or did it require tuning?",
            "14": "It would be interesting to see how soon we can switch from the searching stage to the efficient training stage.",
            "15": "* The paper proposes two different approaches for pruning: pruning attention heads and removing neurons from fully connected layers.",
            "16": "It would be interesting to see how the accuracy of the approach changes if only neurons or attention heads are pruned.",
            "17": "* In Section 4.3, the authors discuss reducing training time by reducing training steps for EarlyBERT.",
            "18": "I think a similar analysis for BERT would be helpful to understand if the training time can be reduced for the baseline model as well.",
            "19": "Also, pre-training techniques have tended to show improvements in downstreams tasks with longer pre-training as the long as the pre-training dataset is large enough.",
            "20": "So, perhaps this reduction in pre-training steps might not be applicable for larger pre-training datasets.",
            "21": "Overall, I find the approach interesting and the authors show computational savings in the pre-training models.",
            "22": "I recommend accepting the paper."
        },
        "WJCsET4sSBq": {
            "0": "Summary:\nThis paper uses the Lottery Ticket Hypothesis to compress BERT.",
            "1": "More specifically, they adapt EarlyBird lottery tickets to the BERT setting in order to find winning configurations in early stages of training combine it with structured pruning methods to ensure the resulting network is more efficient to train.",
            "2": "The method is a three-stage process: (1) searching - this phase involves training full BERT with some coefficient parameters to learn the mask, (2) drawing - use the mask to “draw a ticket” or select the sub-network to train, (3) training.",
            "3": "Experiments show that performance isn’t that much worse when EarlyBERT is used for fine-tuning and for pre-training.",
            "4": "The goal of the paper is to find structured winning tickets for BERT in the early stages of training/fine-tuning.",
            "5": "Strengths:\n1.",
            "6": "Compressing BERT using the lottery ticket hypothesis has been getting a lot of attention recently, and doing so relatively efficiently is an exciting and interesting contribution.",
            "7": "2.",
            "8": "Compared to previous and contemporary work, the combination of EarlyBird lottery tickets (You et al., 2020) to detect tickets early and network slimming (Liu et al., 2017) for structured pruning, is an interesting one.",
            "9": "Prasanna et al., (2020) are doing structured pruning too, but via an iterative pruning method.",
            "10": "3.",
            "11": "Experiments for both pre-training (the first of its kind) and fine-tuning show that performance does not drop all that much for GLUE and Squad which are the main set of tasks BERT is typically evaluated on.",
            "12": "Weaknesses:\n1.",
            "13": "The paper isn’t very clear in some places.",
            "14": "It starts out explaining things well but then it gets harder to follow the details.",
            "15": "Eg: need to add more information on the mask - it’s binarized at some point?",
            "16": "The distance metric is still Hadamard like in EarlyBird?",
            "17": "2.",
            "18": "(More of a question/nit) Is it possible to also show what happens when a winning ticket for BERT fine-tuning is selected based on the pre-training objective?",
            "19": "Chen et al., (2020) showed that these make for better tickets that are performant on many of the downstream tasks.",
            "20": "It would be interesting to see if this holds true for EarlyBert and would add another layer of fine-tuning efficiency.",
            "21": "The work isn't terribly novel, but it's still interesting.",
            "22": "Questions and comments:\n1.",
            "23": "Why does the mask distance diverge for FC in pre-training (Figure 1b)?",
            "24": "Does it somehow indicate that the training run is degenerate?",
            "25": "2.",
            "26": "The ablation on the regularization parameter didn’t give a clear indication of how important selecting this is.",
            "27": "It seems to not be that important, but would using separate values for attention and FC make a difference?",
            "28": "3.",
            "29": "It would probably be helpful to expand the table/figure captions, make them a bit more detailed.",
            "30": "4.",
            "31": "Curious, why did you only use the largest tasks from GLUE?"
        },
        "_QHZA65mG4K": {
            "0": "This paper proposes an approach to sparsifying BERT.",
            "1": "What sets this work apart from prior work on model compression is that while prior works attempt to compress a pre-trained BERT, the authors in this work attempt to learn a sparsified BERT for the purpose of speeding up pre-training.",
            "2": "The method essentially involves learning an independent bernoulli mask for all BERT heads along with Bernoulli masks for some later intermediate neurons.",
            "3": "This model is then trained, along with the masks, for a few epochs.",
            "4": "Then, heads/neurons corresponding to a small value of mask are pruned out.",
            "5": "This results in a sparser BERT model, leading to faster pre-training.",
            "6": "#### Pros\nThe paper is well written and the presentation of the contribution is simple and well-motivated.",
            "7": "#### Cons\n1.",
            "8": "Since one of the main contributions of this work is to make progress on improving the training / inference speed of large transformers, the authors could spend more time going over how the “Time Saved” column is computed.",
            "9": "As of now, it seems casual and hand-wavy.",
            "10": "2.",
            "11": "Experiment protocol:\n  * How were the hyperparameters decided?",
            "12": "Could we have uncertainty estimates for all results by reporting mean/std dev.",
            "13": "across 3-5 runs (atleast for fine-tuning if doing this for pre-training is too compute intensive).",
            "14": "This is especially useful since some of the numbers between EarlyBERT and Random appear very close in Table-1.",
            "15": "* While a central argument is that most model distillation techniques still require expensive pre-training, it would still be useful to include some of those results in Table-2 since EarlyBERT is comparable to those techniques for the purpose of Table-2.",
            "16": "* One way to contextualize how much time is saved during pre-training would be to report total time required for fine-tuning+pretraining on the entire glue benchmark.",
            "17": "This would involve computing the pre-training time (time to learn BERT parameters on Wikipedia) + total fine-tuning time across all datasets (QQP/CoLA/MNLI etc) considered.",
            "18": "This could then be compared against alternatives (such as DistilBert and DeeBERT).",
            "19": "3.",
            "20": "Baselines: Experiments on pre-training compare with no baselines.",
            "21": "Some possibilities:\n  * Using DeeBERT / other early exit approaches at pre-training time.",
            "22": "* Training a BERT model for some epochs, and then distilling it into a smaller network for the rest of the training.",
            "23": "#### Misc Nitpicks\n1.",
            "24": "The phrase “structured sparsity’ is used in multiple places, but never defined.",
            "25": "2.",
            "26": "You et al.",
            "27": "pioneers $\\rightarrow$ You et al.",
            "28": "pioneer\n3.",
            "29": "Could Prasanna et al.",
            "30": "2020 be extended to sparsify pre-training?"
        },
        "DfnIh_mRH-": {
            "0": "Summary:\n\nThe authors propose a technique for reducing the computational requirements of training BERT early in training to reduce the overall amount of resources required.",
            "1": "Pros:\n\nThe paper is well written and clear for the most part.",
            "2": "The authors do thorough experimental evaluation.",
            "3": "Cons:\n\nI have two primary concerns about the paper and the proposed technique.",
            "4": "1.",
            "5": "The positioning of the technique is not entirely clear to me.",
            "6": "The authors pitch it as a technique for reducing the training time of BERT and use LayerDrop as a baseline technique that also removes network components.",
            "7": "However, it feels like another baseline that should be considered is neural architecture search, which also seeks to automatically find a more efficient model to train.",
            "8": "The difference here is that the authors find the model early in the training run, but it seems like the EarlyBERT procedure could be run once and the resulting model architecture could be saved and re-trained like NAS models are.",
            "9": "2.",
            "10": "I found the experimental results to be lacking detail and breadth necessary to establish the value of the technique.",
            "11": "Firstly, the rough time estimates in Table 2 are very odd given the primary value of the proposed technique is to reduce training time.",
            "12": "The accuracy of EarlyBERT is close enough to LayerDrop that accurate training cost numbers are needed to differentiate between the techniques.",
            "13": "Secondly, quoting training time reductions over the dense baseline when the EarlyBERT mode does not achieve the same accuracy makes the comparison very difficult to make.",
            "14": "This problem shows up quite commonly in the model compression literature [1] and I’d encourage the authors to show full accuracy-training time tradeoff curves so that the training time savings for a given accuracy can be more clearly established.",
            "15": "Lastly, I found the use of reduced training epochs in EarlyBERT to be odd because you do not evaluate whether or not this can be done for the baseline models and there isn’t clear evidence as to why your model would be able to do this while others (e.g., DropLayer) cannot.",
            "16": "Figure 2 also does not seem to corroborate that higher learning rates can be used with shorter training time to achieve better accuracy.",
            "17": "The data in your figure shows that the best learning rate achieves the best model quality independent of the number of training epochs.",
            "18": "References:\n1. https://arxiv.org/abs/2003.03033"
        }
    },
    "sxZvLS2ZPfH": {
        "gaNvv0yVkv9": {
            "0": "This paper compared three different Chinese language pretraining methods.",
            "1": "The paper is not well organized.",
            "2": "Although the paper mentioned the char and word are both important in the representation of the Chinese language, it is not clear why the author(s) used the current three pretraining methods.",
            "3": "More intuitive explanation of the design of the pretraining structure should be added.",
            "4": "The most intuitive way of combining the pretrained word and character information is to pretrain them separately and concatenate them together, the proposed models should compare with this most intuitive method and also explain why the proposed models are better than this simply pretraining concatenation method.",
            "5": "The experiments were not persuasive.",
            "6": "Although it is not necessary to beat all the state-of-the-art models, the comparison with other models should be given.",
            "7": "For example, the MSRA NER in this paper is only 82% which is largely behind the SOTA models (>93%).",
            "8": "Given the poor baseline performance, it is hard to persuade the readers the conclusions in this paper are still hold in the most recent/advanced models.",
            "9": "Page 2, citation error \"Albert ?\"",
            "10": "Page 5, format error, the sentence is over the page boundary."
        },
        "f9jTV2ZKPcA": {
            "0": "This paper proposes to use multiple levels of language units, including characters, subwords, and words, for Chinese language modeling.",
            "1": "Three versions of multi-vocabulary pretraining methods are also studied.",
            "2": "Experiments show that using the optimized seg_tok units could improve the model performance in various downstream tasks, and the MVP strategies boost the seg_tok’s results on sequence labeling tasks.",
            "3": "Strengths:\n1.",
            "4": "Empirical studies on building vocabularies with three different granularities of language units for Chinese language model pre-training.",
            "5": "2.",
            "6": "Evaluations on three multi-vocab pre-training strategies.",
            "7": "My most concerns are:\n1.",
            "8": "The idea of modeling different levels of language units has been widely studied before.",
            "9": "Although they have not been comprehensively evaluated on pre-trained LMs, the findings are quite similar with previous studies, without much new highlights.",
            "10": "The claim of the contribution, “the combination of CWS and subword tokenization is novel.”, is quite weak.",
            "11": "Missing References:\nZhang, X., & Li, H. (2020).",
            "12": "AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization.",
            "13": "arXiv preprint arXiv:2008.11869.",
            "14": "Zhang et al, (2019).",
            "15": "\"Effective subword segmentation for text comprehension.\"",
            "16": "IEEE/ACM Transactions on Audio, Speech, and Language Processing 27.11 (2019): 1664-1674.",
            "17": "Liu, Z., Xu, Y., Winata, G. I., & Fung, P. (2019).",
            "18": "Incorporating word and subword units in unsupervised machine translation using language model rescoring.",
            "19": "arXiv preprint arXiv:1908.05925.",
            "20": "2.",
            "21": "In seg vocab development, the choice of 99.95% is interesting but can you please tell what is the lowest word frequency in this vocab?",
            "22": "how many words with a frequency of less than 40 are present?",
            "23": "if criteria is changed from 99.95% coverage to the selection of only frequent words (maybe >40), would it had any effect on speed?",
            "24": "or performance?",
            "25": "3.",
            "26": "This paper is poorly written.",
            "27": "Thorough proofreading is required.",
            "28": "There are too many typos and grammar errors.",
            "29": "e.g.,\nIn the abstract, performances -> performance; remain -> remains; does not only improves -> does not only improve;\nIn the second paragraph of page 2, incorporate -> incorporates; combine -> combinesl\nIn Footnote 5, avalable -> available.",
            "30": "4.",
            "31": "There is no comparison on public tasks or datasets.",
            "32": "Other comments:\n1.",
            "33": "For the multi-vocabulary pre-training, the vocabulary size, i.e., the size of the seg_toks, would be an important influence factor for the downstream task performance.",
            "34": "2.",
            "35": "Only ALBERT small model is evaluated.",
            "36": "It is not quite sure if the method can further enhance the state-of-the-art language models.",
            "37": "3.",
            "38": "The citation format is in chaos, check the usage of \\citep{} and \\citet{}.",
            "39": "The citation of ALBERT is missed in the last paragraph of page 2.",
            "40": "4.",
            "41": "Check the second line in page 5."
        },
        "4bvZLS7MDM": {
            "0": "The paper focuses on pretrain techniques for Chinese vocabulary.",
            "1": "The paper proposes several interesting and novel ways to improve the pretrained model in Chinese.",
            "2": "These ideas include new ways of combining character, segment and word level tokens and new MLM tasks with different granularity of Chinese vocab.",
            "3": "For example MVP_hier explicitly combines the chars into words, which doesn't give the best results.",
            "4": "MPV_obj outperforms MPV_hier by teaching the transformer to learn how to aggregate meaning of chars with an additional loss with a coarse-grained vocab.",
            "5": "The results show that MVP pretrain can improve the performance on both sentence level tasks and sequence labeling tasks.",
            "6": "Improvements are consistent over all testing tasks although the improvements appear to be marginal, especially in comparison to the reported standard deviation.",
            "7": "For example, MVP_obj outperforms the best baseline fin by 0.52 while the std is 0.58 for MVP_obj and 0.61 for the best baseline.",
            "8": "The best results are given by MVP_pair ensemble methods.",
            "9": "Usually, ensemble methods can boost the performance of many ML tasks.",
            "10": "It is a bit surprising that the seg_tok is worse than char on sequence labeling tasks.",
            "11": "It will be useful to give some intuition or examples to illustrate this.",
            "12": "The author mentioned that the CWS attributes very little to the performance loss.",
            "13": "It will be helpful if more details are given.",
            "14": "The whole technique proposed by the paper also heavily relies on the performance of CWS.",
            "15": "Because the paper proposes a novel technique to work with Chinese characters, one more thing that the paper lacks is to mention and compare with recent works on Chinese language model pretrain and techniques to deal with Chinese vocab.",
            "16": "The baseline method is vanilla MLM which was developed for English.",
            "17": "One editing issue: page 5 the second line."
        },
        "QCKeuao6Aw": {
            "0": "    This work attempt to handle the vocabulary problem in Chinese pre-training, which is indeed an unsolved problem (for comparison, byte-pair encoding has been dominant in English pre-training models).",
            "1": "Recently, there are some work trying to combine char-based vocab and word-based vocab for Chinese pre-training, such as AMBERT (Zhang et al) and CharBERT (Ma et al).",
            "2": "Compared with these works, this paper is somewhat incremental and lacks novelty.",
            "3": "In particular, this paper proposed the multi-vocab pretraining (MVP) and designed three versions of MVP: MVP_hier, MVP_obj, and MVP_pair (which is similar to AMBERT).",
            "4": "The authors conducted solid experiments for multiple levels of vocabularies and the several MVP variants on eight Chinese benchmark datasets, which induced some useful take-away findings.",
            "5": "Overall, the paper focused on the problem of vocabulary in Chinese pre-training, and experimented several MVP methods.",
            "6": "Considering the contributions of this paper, it is more suitable as a technical report than a paper to be published.",
            "7": "Pros:\n\n    1.",
            "8": "The overall experiments are solid and convincing.",
            "9": "The implementation details including resources and libraries are sufficient, the experiments are repeated for multiple times for reproducibility.",
            "10": "2.",
            "11": "To handle the problem of Chinese vocabulary, this paper has tested three single-vocab models (char, seg, seg_tok) and several multi-vocab models (MVP).",
            "12": "The results and findings demonstrated in this paper should be trusted and also instructive to peers who is interested in the Chinese pre-training.",
            "13": "Cons:\n\n    1.",
            "14": "The methods are somewhat lack of novelty.",
            "15": "Few insightful conclusions are revealed either theoretically or experimentally.",
            "16": "2.",
            "17": "The effect of \\lambda is not a good point to be analyzed.",
            "18": "Instead, there are several interesting points that are more worth to be studied such as the interaction/compositional relationships between multiple vocabs.",
            "19": "3.",
            "20": "The writing should be improved.",
            "21": "Too many errors in the current version of paper.",
            "22": "For example:\n       1.",
            "23": "The form of citation should be corrected, e.g., \"*The pretrained language models (PLMs) including BERT Devlin et al.",
            "24": "(2018)*\" should be \"*The pretrained language models (PLMs) including BERT (Devlin et al., 2018)*\"\n       2.",
            "25": "In Page 2, the last sentence of the paragraph above Figure 1, \"We will denote this strategy as MVP_pair\", I think the \"MVP_pair\" should be \"MVP_obj\" .",
            "26": "3.",
            "27": "The second row in page 5.",
            "28": "4. seg_tok is not a good name.",
            "29": "The authors should take more time to completing this paper."
        }
    },
    "de11dbHzAMF": {
        "3GUZBRSG95": {
            "0": "**Summary and Key Contributions:**\n\nThis paper proposes the use of task embeddings for a novel conditional attention mechanism and various task-conditioned modules.",
            "1": "The paper also proposes the use of an entropy-based multi-task uncertainty sampling to automatically determine how to sample the data for training the MT architecture.",
            "2": "The paper demonstrates the effectiveness of the proposed architecture and sampling through multiple comprehensive experiments and ablations/analyses.",
            "3": "**Positives:**\n\n* The paper proposes several modifications to the transformer architecture that enable leveraging a learned task-specific embedding, namely conditional attention, conditional alignment, conditional layer norm and conditional adapter.",
            "4": "* The paper proposes the use of the entropy-based MT-Uncertainty Sampling to choose which training examples to use, in a manner that accounts for some of the subtleties (that were perhaps not immediately obvious, but clearly visible in hindsight), such as accounting for the differing numbers of classes in each task.",
            "5": "* Most of the modifications proposed are clean, well motivated and make a lot of intuitive sense.",
            "6": "The overall approach also seems to work well without the need for extensive hyperparameter tuning.",
            "7": "* Strong, thorough experimental validation of the proposed techniques.",
            "8": "Evaluations on GLUE, Super-GLUE show the proposed method often outperforming (or at least performing comparably to) several strong baselines, both MT-based and single model/fine-tuned, and with fewer parameters.",
            "9": "The paper's proposed approach sets a new state-of-the-art performance on 3 tasks (WNUT-2017, SciTail, SNLI).",
            "10": "* Good ablations of the utility of the different task embedding-grounded modules, on the performance of MT-Uncertainty sampling compared to other methods, how MT-Uncertainty sampling tends to choose most difficult tasks first while also avoiding catastrophic forgetting, and of transferring to new tasks or progressively adding in additional tasks.",
            "11": "**Suggestions:**\n* The paper claims to attain 2.2% higher performance compared to a full fine-tuned BERT large model (abstract line 16).",
            "12": "However, it seems like the paper compares a CA-MTL-RoBERTa-Base model to a BERT-base baseline.",
            "13": "This is a somewhat unfair comparison, and it might be better to either clarify or to compare the baseline to the CA-MTL-BERT-Base model (which based on Table 2 would still be a rather impressive 1.3% better while making the comparison fairer).",
            "14": "* While I understand that space is limited, Section 2.1 might have benefited from explaining FiLM, its use here and some intuition, to help keep the paper more self-contained.",
            "15": "**Questions/Clarifications:**\n* (A5 Table 11) On SciTail, it appears that Random Init performs almost as well as STS-B in the zero-shot setting, which is rather surprising.",
            "16": "Do you have any intuition/analysis as to why this might be the case?",
            "17": "* (line 104-105) How exactly was the block matrix A_n obtained?",
            "18": "And does $\\bigoplus$ represent the diag operator?",
            "19": "* (line 95) Does $p_i(\\mathbf{y_i}|\\mathbf{x_i},\\mathbf{z_i})$ represent the model prediction score, i.e., $f_{\\phi(\\mathbf{z_i}), \\theta_i}(\\mathbf{x_i})$?",
            "20": "* In Table 5, the total data used was 66.3% (or 64.6%).",
            "21": "How was the amount of data to be used determined?",
            "22": "Was a stopping point determined based on the GLUE dev set?",
            "23": "* The paper refers to \"assemble methods\" (line 267, Table 4 left header, line 657).",
            "24": "Was it perhaps meant to say \"ensemble\"?",
            "25": "=================================\n\n**Update:**\n\nThank you for you responses and clarifications to mine and the other reviewers' comments.",
            "26": "Thank you also for baking so much of the reviewers' feedback into your latest draft, in particular by improving how the comparisons are presented in the abstract and the rest of the paper, by updating Table 2 to make it clearer and cleaner, and by adding experiments and analyses in Section 4.",
            "27": "As a suggestion pertaining to the latest draft, I agree with R5 that a consequence of the additions to Section 4 have made it a little crammed, although I certainly understand that the constrained spaced forced the authors to make this trade-off.",
            "28": "Overall, I continue to maintain that this is a good paper with novel ideas that are well-justified by experimental results and analyses, and would like to reiterate that I believe that this work is a clear accept."
        },
        "lqqHbSYexrB": {
            "0": "Update: After seeing the changes the authors have made, I have slightly increased my score.",
            "1": "Thanks!",
            "2": "The authors investigate how to use multi-task training on top of a pretrained model for a variety of unrelated tasks from the GLUE benchmark.",
            "3": "Their goal is to reduce the amount of parameters and data needed, while improving (or maintaining) state-of-the-art performance.",
            "4": "The main contribution are the following innovations: a special attention mechanism called block-diagonal conditional attention, a set of modules for adaptation of a pretrained model, and an uncertainty-based multi-task data sampling method.",
            "5": "The latter aims at avoiding catastrophic forgetting, while the former avoid having to share all (or no) parameters.",
            "6": "Experiment are performed on the GLUE benchmark.",
            "7": "The authors already published their code in an anonymous GitHub repository (that's great!).",
            "8": "Overall, I like the idea of the paper: it is important to reduce the parameters needed for sets of tasks, to enable NLP models to be deployed in a larger variety of settings, and reducing the amount of data needed.",
            "9": "Also, the authors did a great job in terms of conducting evaluations from different angles.",
            "10": "For instance, they ask how different amounts of data influence model behavior or if their data sampling method can react to task difficulty.",
            "11": "Unfortunately, I think this particular paper might need some more work before being ready for publication.",
            "12": "I will discuss my concerns one-by-one in detail:\n- Most importantly, the evaluation is confusing.",
            "13": "The authors switch between using BERT_base and RoBERTa_base without being very clear about when and why, e.g., in Table 2 they have both models, but only in different sections.",
            "14": "This makes a direct comparison impossible.",
            "15": "Also in Table 2, they claim that they outperform other approaches.",
            "16": "However, they compare to BERT models and build themselves on RoBERTa_base; how are the results meaningful if they use a stronger model to start with?",
            "17": "(And the comparison to their BERT_base-based model yields roughly equal scores.)",
            "18": "I compared their numbers explicitly to Liu et al.",
            "19": "(2019), and RoBERTa_base outperforms their approach on nearly all tasks (and on average).",
            "20": "I would argue that this is not a fair comparison: did the authors intentionally choose weak baselines?",
            "21": "The same applies to Table 3: it is unclear to me why or how the baseline T5 model has been chosen.",
            "22": "I do understand that the main point is the reduction of the amount of parameters (per task), but this doesn't mean that the evaluation should paint a wrong picture.",
            "23": "- Additionally, in my opinion, the authors are misrepresenting prior work when saying in line 163 that the \"MTL approach has not yet been successful in NLP\".",
            "24": "The authors refer to the fact that MTL can (and often does!)",
            "25": "lead to performance decrease for individual tasks.",
            "26": "However, this is not the case, for instance, for massively multilingual MT (see the reference below for an example).",
            "27": "Granted, the final effect of MTL depends on task similarities, but that's probably the same for the proposed approach.",
            "28": "- Major contributions of the work should be described in the main paper.",
            "29": "Therefore, it's not ideal that the uncertainty sampling algorithm has been moved to the appendix.",
            "30": "Additionally, there are some minor things I would add or improve:\n- I would add references to multi-task training on different languages (e.g., Task 1 is translation from EN to FR and Task 2 is translation from EN to DE).",
            "31": "For instance: Roee Aharoni, Melvin Johnson, and Orhan Firat.",
            "32": "2019.",
            "33": "Massively multilingual neural machine translation.",
            "34": "NAACL.",
            "35": "(And the references therein.)",
            "36": "- The paper would benefit from more proofreading.",
            "37": "For instance, line 94: \"task latent\" -> \"latent task\"."
        },
        "lneOlScwTCc": {
            "0": "This paper proposed a new transformer model framework for multitask learning on NLP tasks.",
            "1": "To deal with challenges in multitask learning/co-training, the authors proposed five improvements, including modifications on the transformer layers with task conditioning and uncertainty sampling.",
            "2": "In the experiments, the authors showed that the proposed model can outperform full fine-tuned BERT large model with less parameters (adding some parameters on a single co-trained model), and with less training data.",
            "3": "This paper is very well written and easy to follow.",
            "4": "It is also a very meaningful application as pretrain-finetuning based approach could be both sub-optimal and ineffiicent.",
            "5": "The proposed model structure and sampling techniques suits well for multitask learning based solutions.",
            "6": "Having soft-parameter sharing conditioned by task embeddings while freezing part of pre-trianed parameters seems reasonable and efficient.",
            "7": "The uncertainty based sampling approach is also very interesting and could potentially benefit other multitask learning problems in general.",
            "8": "The authors also conducted experiments on various types of NLP tasks including GLUE and Super-GLUE, compared with baseline methods such as pretrain finetuning BERT model, adapter based approach, and T5.",
            "9": "However, I think the paper could improve in the following aspects.",
            "10": "A1.",
            "11": "In-depth connection to different multitask learning challenges.",
            "12": "This paper proposed 5 modeling components.",
            "13": "Intuitively, they all target at solving multitask learning challenges mentioned earlier in this paper, e.g., capacity balancing on multiple tasks, catastrophic forgetting, negative task transfer and interference.",
            "14": "I think more in-depth discussion on how each of the 5 modeling improvements solve what aspects of those challenges can singficanlty improve the impact of this paper.",
            "15": "For example, the conditional alignment part, inspired by Wu et al.",
            "16": "(2020), could have some similar discussion related to the covariate shifts among different tasks.",
            "17": "Uniquely, it is less clear such alignment matrix can learn to capture covariate distribution difference with task embeddings co-learned from other model improvement components introduced in this paper.",
            "18": "Similarly, it would be interesting to see discussions on how the uncertainty sampling can improve what aspects in multitask learning.",
            "19": "Comparing to many sampling tricks (e.g., discussed in T5), would uncertainty sampling dynamically improve task capacity balancing?",
            "20": "Either empirical analysis or theoretical justification could improve this subsection.",
            "21": "It would also be helpful to understand how these 5 model improvements interact with each other, both intuitively and empirically.",
            "22": "A2.",
            "23": "Might need to compare and/or include some existing work.",
            "24": "I see two lines of related work are missing.",
            "25": "(1) There are similar research in NLP mutltiask learning that adopts mixture-of-expert based model architecture to do soft-parameter sharing, for example:\n[1] https://arxiv.org/pdf/2006.16668.pdf\n[2] https://arxiv.org/pdf/2007.05891.pdf\nAnd routing/mixute-of-expert based task conditioning is somewhat popular in other domains.",
            "26": "It might be interesting to compare, or to point out the modeling/motivation differences of the proposed work compared to those approaches.",
            "27": "(2) Dynamic reweighting based optimization algorithms, for example:\n[1] https://arxiv.org/abs/1711.02257\n[2] https://arxiv.org/abs/1705.07115\n[3] https://arxiv.org/abs/1810.04650\nI think this line of work might be less popular in NLP based tasks but overall would be meaningful to be included in a discussion.",
            "28": "a minor comments:\nIt might be good to explicitly mention each of the proposed improvements in figure 1, for example, conditional attention, conditional layer norm, .."
        },
        "ZLlS_WgGyHH": {
            "0": "Update: see comment below for rationale behind change from 5 to 7.",
            "1": "## Claimed Contribution:\n\nThis paper seeks to develop a multi-task learning (MTL) strategy that performs competitively with single-task (ST) fine-tuning despite having fewer parameters per task.",
            "2": "To this end, this work introduces several task-specific modules components and an uncertainty-weighted sampling strategy.",
            "3": "Some of the task-specific module components are similar to the Adapters line of work, with a key difference being that the adapters are trained in a MTL setting versus in ST finetuning.",
            "4": "The authors also introduce latent representations of tasks that feed into these different modules.",
            "5": "The sampling strategy is well motivated and credibly improves results.",
            "6": "Overall, this MTL approach can(*) match the average performance of single-task finetuning without having to keep a separate copy of the parameters for each task.",
            "7": "(*) see caveats in Cons.",
            "8": "### Pros \n\n* Paper is well-written, concepts are introduced clearly, related work section is well-done.",
            "9": "* Comparisons are made to relevant related work, including relatively recent papers.",
            "10": "* Ablations in Table 5 are very useful and convincing.",
            "11": "* Method perform well compared to alternatives in the Adapters world.",
            "12": "* Figure 5 is also helpful to understand the benefits of incremental tasks.",
            "13": "* The sampling method is interesting and compelling, and appendix A2 gives additional interesting results.",
            "14": "This strikes me as an important contribution that deserves to be explored more widely, including outside of the Adapters context.",
            "15": "### Cons\n\n* Paper tends to overclaim / have some comparisons that don’t have the necessary context.",
            "16": "When correcting for those claims, the results are awash with its ST-tuned alternatives and worse than the MTL-then-ST counterparts.",
            "17": "In particular:\n  * The abstract claim “With our base model, we attain 2.2% higher performance compared to a full fine-tuned BERT large model on the GLUE benchmark, adding only 5.6% more trained parameters per task” has a  typo that makes it less impressive, as shown in Table 2.",
            "18": "This gap is for a full fine-tuned BERT base model, not large.",
            "19": "The large BERT model outperforms the CA-MTL base ones by 0.3.",
            "20": "In addition, the authors compare a BERT checkpoint to a Roberta one, which feels like something that should be mentioned upfront.",
            "21": "* I am not a fan of only exposing the #params per task in Table 3 and 4.",
            "22": "I would like to also see the row on #parameters of the model on each task (~220M for T5, ~370 for CA-MTL).",
            "23": "It is worth mentioning in Table 3 that you are comparing a BERT-large encoder only model to a ~T5-base encoder decoder.",
            "24": "The choice of SpanBERT in Table 4 is also suboptimal, as you are comparing a BERT checkpoint (albeit with an additional QA focused pre-training step) to a Roberta one.",
            "25": "* Table 4 compares models derived from BERT checkpoints to yours that is derived from a Roberta one, this should be emphasized more clearly as it explains some of the score discrepancy.",
            "26": "* There are no comparisons to single-task finetuned Roberta models.",
            "27": "On the Dev set, these outperform by 0.7 on Glue for the Base models (see https://arxiv.org/pdf/1907.11692.pdf Table 8), though the story varies considerably by task.",
            "28": "* There is little mention made of the discrepancies across tasks.",
            "29": "For the same base model, CA-MTL underperforms on many tasks, recovering most of the points on RTE, a task that is known for performing well when doing MTL with MNLI (see for instance STILTs, Phang et al, 2018, who find a double digit point gain on Dev when using a BERT model finetuned on MNLi before finetuning on RTE).",
            "30": "I think wider discussion of the discrepancy across task is warranted.",
            "31": "Overall, it seems like CA-MTL can capture part of the gains of MTL+ST tuning but still underperform ST tuning on tasks where little is gained by doing MTL.",
            "32": "* The STILTs paper ( https://arxiv.org/abs/1811.01088 ) seems relevant at least in the related work, esp.",
            "33": "considering you are citing some of its follow-up papers.",
            "34": "It might also be worth comparing to models that do intermediate fine-tuning.",
            "35": "* There is little discussion of when this would be advantageous.",
            "36": "Since the results are often lower than the ST and MTL-then-ST counterparts, it would be worth emphasizing that the lower disk space requirements are key in mobile/on-device contexts.",
            "37": "* One redeeming remark for some of those points: the papers this method compares to often explore a wider hyperparam grid for each task, while this method does not.",
            "38": "### Recommendation\n\nOverall, I think this is an interesting approach.",
            "39": "However, some of the claimed results are not as strong when put in the proper context, which the authors sometime fail to do.",
            "40": "As such, I think this paper is a slight reject in its current state.",
            "41": "With a fairer presentation of the comparisons it might be a slight accept."
        },
        "iGoWRlRYTXQ": {
            "0": "The paper explores a collection of strategies to improve multitask learning and bring performance on par with single task training.",
            "1": "The strategies build on and show good awareness of existing work and achieve positive overall results on GLUE, SuperGLUE, and MRQA tasks.",
            "2": "Abolation experiments demonstrate the value of the individual components being proposed.",
            "3": "The results are particularly impressive given the small number of additional parameters introduced into the model and in the context of prior work that has shown mixed results from multitask training.",
            "4": "However, despite the strong results, I still have mixed feelings about this paper.",
            "5": "The presentation follows the format of introducing a fairly large number of new methods followed by experiments that directly attempt to exceed the current SoTA.",
            "6": "The analysis of the contribution of the individual methods, while greatly appreciated, still almost seems like an after thought.",
            "7": "To be a strong accept for ICLR or another similar venue, it think the techniques explored in this paper deserve an alternative treatment that focus on the contribution of the individual methods first and then concludes with their combination.",
            "8": "With this presentation, it would be good to include results for any non-standard design choices.",
            "9": "For example, on line 110, you have \"We also experimented with full-block 110\nConditional Attention ∈ R L×L, Not only did it have N2 more parameters compared to the block-diagonal variant\".",
            "10": "I'd like to see the numbers that support this claim.",
            "11": "I also found some of the comparisons in the paper somewhat confusing.",
            "12": "For example, Table 2 compares a BERT baseline to the authors proposed bag of methods, dubbed CA-MTL, combined with a RoBERTa model.",
            "13": "If these numbers are to be meaningful, it would be good to include a RoBERTa baseline in the tables.",
            "14": "Finally, one of the original motivations for multitask training was that it might allow us to make use of less data for each individual task.",
            "15": "While it's great that the paper shows good performance on GLUE using a fraction of the available training data, and good domain adaption with varying amounts of training data, it would be desirable to dig a little deeper.",
            "16": "It would be interesting to see something like Table 1 for other tasks being explore in the paper of perhaps using the whole GLUE benchmark (e.g., using less than 64% of the training data and finding the knee where performance drops off)."
        }
    },
    "aRTRjVPkm-": {
        "Kmx2AiDbp0I": {
            "0": "Update:  Thanks for the detailed response.",
            "1": "I appreciate the additional figures and other results that you have provided.",
            "2": "However, it seems like there's still some open questions about the types of improvements being made and what this implies about the LM's attention mechanism.",
            "3": "It seems like more quantitative analysis would be needed to determine how much the LM's attention is correlating empirically to factual knowledge or if there are other factors that are affecting the downstream improvements.",
            "4": "Additionally, there's some limitations to the way the language model is being leveraged and the types of knowledge it can extract.",
            "5": "I also wanted to mention that I appreciate the addition of the suggested related work, but I would still suggest that the authors consider looking into more detailed means of comparison in the future (especially to the Petroni work), since this seemed to be a concern in multiple reviews.",
            "6": "----------------------------------------------\nOriginal:\nThis paper is aimed at using pre-trained language models to create open-ended knowledge graphs.",
            "7": "They introduce an unsupervised approach (MAMA) to construct a knowledge graph in two phases in which they take a target corpus and output a knowledge graph.",
            "8": "In the first phase (match), they extract knowledge tuples from each sentence in a corpus using a beam search over the self-attention within a pre-trained language model.",
            "9": "In the second phase (match), they ground facts to a knowledge graph schema by using combinations of entity linking and relation matching techniques from previous work.",
            "10": "While this is a promising direction for future research, I tend to lean towards rejection for a few reasons.",
            "11": "The main limitations seem to be: (1) the proposed method is a bit limited in that it can only be used with a corpus in which the target head, relation, and tail spans need to be directly mentioned in a single sentence (2) it’s not clear whether the quantitative improvements are due to factual knowledge in the pretrained model or the syntactic/semantic relationships encoded in the self-attention.",
            "12": "There's also some missing related work in extracting knowledge from pretrained models that should probably be discussed.",
            "13": "Contributions:\n- A new algorithm for unsupervised knowledge graph creation from a target corpus\n- Demonstrating the utility of large pre-trained language models towards knowledge graph creation (though, there are other works in this area that should probably be discussed more.",
            "14": "I will list a few in the limitations section.)",
            "15": "- Improvement over previous state-of-the-art models.",
            "16": "Strengths:\n- The algorithms presented here are relatively straightforward but surprisingly effective.",
            "17": "They improve computationally over previous work.",
            "18": "- Most of the paper is clearly written.",
            "19": "The experiments seem easily reproduceable.",
            "20": "- The algorithm presented here is able to be used in an unsupervised way and can work with both open-ended and more structured knowledge graph schema.",
            "21": "Limitations:\n- There are some limitations to the type of knowledge that can be extracted with the proposed approach: the only tuples that can be selected must be described within the target corpus in which the head and tail must appear within the same sentence.",
            "22": "- There is some missing prior work in creating knowledge graphs from pre-trained language models.",
            "23": "To name a few:\n  -“Language Models as Knowledge Bases?” EMNLP 2019\n  -“Commonsense Knowledge Mining from Pretrained Models.” EMNLP 2019\n  -“Comet: Commonsense Transformers for Automatic Knowledge Graph Construction.” ACL 2019\n- Although the proposed model achieves quantitative improvements over Angeli et al.",
            "24": "(2015), it seems unclear whether these improvements are due to the factual knowledge encoded in the pre-trained LM, as claimed.",
            "25": "It seems possible that these improvements could instead be due to the high quality semantic/syntactic relations encoded in the attention mechanism.",
            "26": "- The title of the paper is a bit strongly worded and may be over-claiming what is shown quantitatively in this paper.",
            "27": "Questions for the authors:\n- How the parameter study was conducted?",
            "28": "Was this using the development set, and if so for which dataset?"
        },
        "5jRB1jNUxv": {
            "0": "This paper presents an unsupervised approach for extracting OpenIE style triples from a corpus.",
            "1": "The approach leverages the internal attention maps of pretrained transformers to identify paths which correspond to relations between a head entity and a tail entity.",
            "2": "The extracted open triples are then mapped, wherever possible, to an existing KG to create what is referred to as an Open KG.",
            "3": "Strengths:\n- The unsupervised matching approach for extracting triples is quite novel and, to my knowledge, the first evidence that the attention maps of pretrained transformers contain paths which capture relational knowledge.",
            "4": "Weaknesses:\n- The main evaluation in the paper is done on the grounded facts after the mapping stage.",
            "5": "While the details are hazy, based on Section 2.2.1, it seems the mapping stage relies on a pre-existing KB aligned with a corpus to do the entity linking and relation linking steps.",
            "6": "In this case another strong baseline would be to learn distantly supervised entity and relation extraction methods.",
            "7": "Of course, this approach would not be able to produce the ungrounded facts, but when evaluating only the grounded facts it would be nice to see how the proposed approach compares to this more traditional setting.",
            "8": "- Related to the above, important details are missing about the interaction between the KG used for the entity and relation linkers and the KG used for evaluation.",
            "9": "Specifically, what is the size of the KG available for entity / relation linkers, and were there any overlapping facts between this and the facts used for evaluation?",
            "10": "- Several recent papers have looked at probing LMs for knowledge facts (e.g.",
            "11": "\"Language Models as Knowledge Bases\" Petroni et al, EMNLP 2019, and follow up papers).",
            "12": "These are very relevant, as another approach for constructing KGs from pretrained LMs is to use natural language templates.",
            "13": "But there is no discussion of these works in the paper.",
            "14": "- Selecting the threshold for the matching degree seems to be an important step influencing the quality of the final KG, but it seems to have been done in an ad-hoc manner here.",
            "15": "Other comments:\n- Section 2.1 states that \"self-attention weight matrix ... is the main container of the knowledge information in pre-trained LMs\".",
            "16": "This is clearly not true as a large amount of knowledge is also stored in the word embedding table.",
            "17": "- Section 3.2 has some confusing terminology where the facts supported by the KG schema are also referred to as \"ungrounded\" which contradicts the definitions in section 2.2.2."
        },
        "R1cI1D8r2Iv": {
            "0": "This paper targets an ambitious question -- constructing a knowledge base from scratch using pre-trained language models.",
            "1": "The proposed approach deals two problems: (a) constructing facts from raw corpora and (b) disambiguating the entities in the fact by linking them to WikiData entities.",
            "2": "First, the insight that knowledge base is embedded in the world knowledge, which is captured by the pretrained embeddings, is not new.",
            "3": "[1] studies the joint embedding of knowledge base and text and thus performs knowledge base construction in a semi-supervised fashion, which seems to be more promising than reconstructing the knowledge base from scratch in an unsupervised fashion.",
            "4": "I'm curious to see the performance of the power of existing knowledge bases, the signals there can be used to initiate the training.",
            "5": "Second, baseline comparisons seem to be missing in the experiment section.",
            "6": "Even though the task sounds difficult, the numerical performance on recall and F1 are not satisfactory.",
            "7": "It's hard to understand the current performance without a reasonable baseline.",
            "8": "Apart from that, it would be nice to see the error analysis -- what's the common pattern where MaMa fails?",
            "9": "In addition to the above major comments, I have the following questions:\n(a) In the map phase where MaMa converts a sentence into facts, do we have an underlying assumption that the input sentence is a fact passage.",
            "10": "What will happen if not?",
            "11": "(b) What if the input passage contains more than one fact?",
            "12": "E.g., \"Bob Dylan (born Robert Allen Zimmerman; May 24, 1941) is an American singer-songwriter, author, and visual artist.\"",
            "13": "(the first sentence from Wikipedia) contains facts: (bob dylan, is, song writer), (bob dylan, is author), (bob dylan, is visual artist), (bob dylan, also named, Robert Allen Zimmerman), (bob dylan, date of birth, May 24, 1941).",
            "14": "[1] Wang, Zhen, et al.",
            "15": "\"Knowledge graph and text jointly embedding.\"",
            "16": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP).",
            "17": "2014."
        },
        "4s8A2Gfswf2": {
            "0": "Paper summary: The paper introduces an unsupervised method that utilizes an off-the-shelf BERT (without any fine-tuning) to create an information extraction system without any training data.",
            "1": "It first creates ungrounded triples (a.k.a.",
            "2": "OpenIE) from raw text by looking into the attention weights between words and finding a sequence of words through a beam search that has high attention weights between every consecutive words.",
            "3": "When such sequence is created, the first and the last word become the head and the tail entities and the words between them becomes the relation.",
            "4": "The next step is grounding each triplet to a known Knowledge Graph by utilizing off-the-shelf entity and relation grounding mechanisms.",
            "5": "The proposed method shows 1-2% F1 score advantage over Stanford OpenIE on TAC KBP and Wikidata.",
            "6": "Strengths:\n- The proposed method is unsupervised (doesn't require any training process or data).",
            "7": "- The proposed method only requires a single application of BERT per paragraph (instead of, for instance, applying BET for every possible triplet), which is relatively efficient.",
            "8": "- The idea of looking into the pooled attention weights for linking entities and relations is novel.",
            "9": "Weaknesses:\n- The head and the tail entities are always single words, whereas many entities such as names have two or more words.",
            "10": "- The comparison is made only against Stanford OpenIE, which was proposed 5 years ago.",
            "11": "- The paper highly depends on the grounding techniques from many years go, namely Spitkovsky & Chang (2012) and Stanford OpenIE (2015).",
            "12": "Given that the proposed model and Stanford OpenIE have similar performance, I think it is necessary to perform ablation study and show how much is the dependency.",
            "13": "In the worst case, I am worried that most of the work is being done in this grounding stage (i.e.",
            "14": "ungrounded triplets are not so good)\n- If all possible head-tail pairs are enumerated in a sentence, my expectation is that there will be a lot of garbage triplets.",
            "15": "It is amazing yet hard to believe that such simple thresholding techniques make it work and get rid of all of the bad triplets.",
            "16": "More analysis will be helpful.",
            "17": "Overall: The approach is interesting in that it is completely unsupervised, and is able to achieve a meaningful performance (compared to Stanford OpenIE).",
            "18": "However, I am not 100% convinced how this could work so more analysis would be helpful, and given that Stanford OpenIE is pre-BERT from 5 years ago, the gain seems to be not so exciting."
        }
    },
    "SUyxNGzUsH": {
        "gIovmw9vbz": {
            "0": "Summary:\n\nThe paper studies the application of neural module network to video-grounded language tasks.",
            "1": "They propose a method dubbed Visio-Linguistic Neural Module Network (VilNMN) to retrieve spatio-temporal information in a video through a linguistic-based parsed program.",
            "2": "In particular, VilNMN first extracts entity references and their corresponding actions in linguistic cues.",
            "3": "This information is then being used to locate relevant information in the visual cue to arrive at the correct answer.",
            "4": "The proposed method is evaluated on two large scales benchmarks AVSD and TGIF-QA, demonstrating competitive performance with state-of-the-art methods.",
            "5": "Comments (Technical, Major Flaws of this paper): \n\n(1) Overall, it is interesting to see neural module network works in such a complex setting as in video-grounded language tasks and I appreciate the efforts of the authors trying to explain the method as detailed as possible.",
            "6": "(2) The use of the mathematical symbols in the paper is very confusing.",
            "7": "Normally, upper cases (capital letters) are used to denote matrices/sets of vectors while lower cases (non-capital letters) are often used to denote vectors.",
            "8": "(3) How did the authors subsample F frames/clips from a video?",
            "9": "In my understanding, there are a lot of frames in a video are blurry or distorted so if you sample them in a random manner, it would greatly affect the performance of object detection (Faster RCNN in this case).",
            "10": "Please elaborate more on this.",
            "11": "(4) Subsection 3.1, in the description of \"when\" module: A_when,i is a vector, not a matrix.",
            "12": "Same in the supplementary document.",
            "13": "In addition, I am wondering since there are many objects/entities in a video, how would the \"when\" module be able to localize the same object/entity over time given no additional supervision?",
            "14": "I think object tracking would greatly be beneficial in this case.",
            "15": "(5) In the experiments on TGIF-QA:\n- Why the authors only used ResNet features instead of using similar features as in AVSD?",
            "16": "Spatial-based level features are fine in terms of computations but are less intuitive as I expect the object-based features counterpart and the linguistic entity references represent things at the same level of abstraction.",
            "17": "- Honestly, I am skeptical about the results on the TGIF-QA datasets as the gap between VilNMN and the existing methods is very significant.",
            "18": "From Table 4, it looks like motion features (optical flow, C3D, and the likes) play a role in tasks containing repetition of actions (such as count and action).",
            "19": "I am not sure the reason why VilNMN without the use of any of those motion features could manage to outperform the existing methods with large margins?",
            "20": "- Please provide more analysis on the results on the TGIF-QA as in my understanding even when a model correctly links entities in a question with their visual representations, it does not guarantee that it can arrive at correct answers.",
            "21": "For example, in counting task, I would say most of the questions have the same parsed program.",
            "22": "How does your model work in this scenario?",
            "23": "Some other concerns:\n\n(6) At the end of the related works, the authors wrote \"In video represented as sequence of images,...., e.g though average pooling, resulting in potential loss of information\".",
            "24": "This is a big assumption as using average pooling over object proposals is a bad idea as object appearance may vary very little over time in a video and the average pooling would smash the temporal information in the video.",
            "25": "If one can properly model object tubelets via object tracking, what written in the paper wouldn't make sense.",
            "26": "(7) Section 3.2: The sentence \"...calculated as softmax scores between an entity P_i and each token in dialogue history\" is mathematically incorrect.",
            "27": "What drives the attention weights?",
            "28": "Softmax is a function applying over a set of elements.",
            "29": "I would be okay to raise the rating if the authors sufficiently address my concerns during the rebuttal phase."
        },
        "D_mav22ZWd-": {
            "0": "Description:\n\nThis paper introduces the Visio-Linguistic Neural Module Network (VilNMN) consisting of a pipeline of dialogue and video understanding neural modules.",
            "1": "Motivated by Hu et al.",
            "2": "(2017), Kottur et al (2017), this paper extends the NMNs on video tasks for interpretable neural models.",
            "3": "The model explicitly resolves entity references (dialog understanding) and detects actions from videos (video understanding) for response generation.",
            "4": "Experiments show that NMNs achieve competitive results on AVSD (video-dialog) and TGIF-QA (video-QA) benchmarks.",
            "5": "Strengths:\n- New modules for video understanding (“where”, “when”) have been proposed.",
            "6": "A step towards interpretability of compositional neural networks \n- Ablation studies have been provided to understand the importance of each module in the VILNMN model.",
            "7": "- The breakdown of relative CIDEr/BLEU (Figure 7 in supplementary) for different context and video length is interesting.",
            "8": "- SOTA results on TGIF-QA (Video QA) while competitive results on AVSD (dialog task).",
            "9": "Weaknesses:\n- Availability of code is not discussed which is essential for reproducibility.",
            "10": "- It would help to specify whether sentence vs corpus level BLEU was used for evaluation\n- Human evaluation is not provided.",
            "11": "Limitations of automatic metrics and their reliability in language generation have been discussed repeatedly.",
            "12": "See (Reiter and Belz, 2009; Novikova et al., 2017; Reiter, 2018).",
            "13": "- Table 6 denotes that the evaluation results decrease with longer video or larger context modeling which is the main focus of the paper.",
            "14": "- It might be argued that this approach would not generalize.",
            "15": "How would this model scale when the dialog becomes challenging (in terms of disfluencies, ellipses or alignment, topic switch, etc apart from co-reference; see Haber et al 2019 Photobook dataset for brief summary of dialog phenomena)?",
            "16": "Similarly when the videos become more complex, would action recognition suffice?",
            "17": "- CorefNMN (Kottur et al 2017) was designed specifically for co-reference resolution in the dialog.",
            "18": "The paper could be similarly improved by explicitly motivating the specific utility of the NMN modules compared to high-level description- eg.",
            "19": "to capture co-reference (dialog) and action recognition (video).",
            "20": "- An analysis of the AVSD dataset would help understand the importance of the dialog context in the dataset - focus of one of the modules in this paper.",
            "21": "(See Agarwal et al 2020 study for Visual dialog)\n\nQuestions:\n- Since AVSD is also posed as a retrieval task (Alamri et al.",
            "22": "2019), have the authors evaluated the system on ranking based metrics?",
            "23": "- Could the authors clarify why accuracy is also not reported for CountQA in Table 4?",
            "24": "- It would help to explicitly mention the neural modules previously defined and the novel modules, eg how the “find” module differs from Kottur et al.",
            "25": "2018.",
            "26": "- Have the authors experimented with the dialog-based modules from Kottur et al.",
            "27": "2018 - eg.",
            "28": "“refer” module?",
            "29": "- In Fig 2 (as well as the main text), it would help to clarify if the underlying text encoder is shared for the dialog history, question, and caption.",
            "30": "- How are the audio signals incorporated in the VilNMN mentioned in Table 2?",
            "31": "Pardon if I missed this.",
            "32": "- Have the authors experimented with pre-trained weights (decoders)?",
            "33": "Suggestions/Comments:\n- Previously Johnson et al.",
            "34": "(2017a); Hu et al.",
            "35": "(2017), Kottur et al.",
            "36": "(2018) have all explored NMN for visio-linguistic tasks (such as VQA, Visual dialog), the nomenclature “Visio-Linguistic Neural Module Network (VLNMN)” seems too broad.",
            "37": "Something on the grounds of “ActionNMN/ActNMN” would do justice to the work.",
            "38": "- Model descriptions of ablations in Table 3 could be improved for clarity.",
            "39": "- Implementation details could be further specified - the framework, all other hyperparameters to ease reproducibility.",
            "40": "--------------------------------------------------------------------------------------------------------------------------------------------------------\nPost Rebuttal update:\n\nI would like to thank the authors for answering the questions.",
            "41": "I believe that an updated version addressing all the concerns in detail will find its place in other future conferences.",
            "42": "Original rating is maintained."
        },
        "HkG_DfE0r24": {
            "0": "The author present a novel neural moudalar network  for video grounding tasks, which can provide interpretable intermediate reasoing outcomes and show the model robustness.",
            "1": "This model achieves competitive results on AVSD datasets and state-of-the-art performance on TGIF-QA datasets, which demonstrates the effectiveness of the model design.",
            "2": "Detailed comments are listed in the following\n• The novelity of the NMN is limited in this paper.",
            "3": "The similari idea have been used in many previous literatures.",
            "4": "I am wondering that how you define the modular space?",
            "5": "Is there any prinpicle guidelines to design module like \"find, summarize, when, describe\"?",
            "6": "• The reasoning struture in this papar is simple.",
            "7": "The module \"find, when, where\" are more like signal detectors.",
            "8": "There is no reasoning structure for how to get the final response.",
            "9": "(in this paper, just fuse the detected information to get the final answer by a response decoder).",
            "10": "So this methods cannot reveal the inner correlation between final response and detected visual/language entities.",
            "11": "• [Question] How do you train the program generation tasks from language?",
            "12": "Is there any groundtruth program structure annotation to supervise this?",
            "13": "How do you determine the hyper-parameters \\alpha and \\beta?",
            "14": "• The paper is written pooly and some expressions are confusing, like \"Different from..., our model are trained to fully genrate the parameters of components in the text\".",
            "15": "The parameter here refer the input of each module, which is different from model parameters."
        },
        "MuS5HYVnPwx": {
            "0": "This paper studies the language grounding aspect of video-language problems.",
            "1": "It proposes a Neural Module Network (NMN) for explicit reasoning of visually-grounded object/action entities and their relationships.",
            "2": "The proposed method is demonstrated to be somewhat effective in the audio-visual dialogue task and has been shown superior to existing works on video QA.",
            "3": "Overall, the paper is motivated clearly and is delivered with good clarity.",
            "4": "The followings need to be clarified.",
            "5": "i) The proposed model demonstrates impressive results on TGIF-QA but without any insightful justification.",
            "6": "Since the questions in TGIF-QA are short and usually do not involve complicated reasoning, intuitively, a heavy reasoning scheme might not necessarily pay off.",
            "7": "Please clarify the performance gain and possible reasons.",
            "8": "Also, \"soft label programs\" lack the necessary context (and should be in bold instead in Tab.",
            "9": "4).",
            "10": "ii) Including intense model variants in the main result table (Tab.",
            "11": "2) gives this paper a somewhat unfair advantage, especially when the best performing method on each metric comes from different model variants.",
            "12": "The validation set (from both AVSD and TGIF-QA) is supposed to serve the purpose of model architecture search and ablation studies.",
            "13": "Besides, the underlines in the lower part of Tab.",
            "14": "2 should go to method VGD-GPT2.",
            "15": "========== Post-Rebuttal ==========\n\nConcerns on paper/results clarity still persist.",
            "16": "Lowering my rating to 5."
        }
    },
    "whAxkamuuCU": {
        "5Y8sB-Z26HA": {
            "0": "**Summary**.",
            "1": "This paper proposes a new type of models that are equivariant to entity permutations, which is an important criterion to build language models that can easily generalize to new entities.",
            "2": "The authors modified a Memory-Network and a Third-order tensor product RNN to make them symbolic-shit invariant.",
            "3": "The new models were evaluated and compared on the 20 bAbi tasks.",
            "4": "Results show that the symbolic versions of the models yield better performance than the original ones.",
            "5": "**Positives**.",
            "6": "The topic is of great interest and it is indeed crutial that neural language models become symbol-shift invariant to allow them to better generalize.",
            "7": "This work is clearly motivated.",
            "8": "**Confusions**.",
            "9": "The beginning of Section4 mentions that the main idea of this work is to concatenate a regular \"semantic\" word vector with a \"symbolic\" representation essentially corresponding to a one-hot vector of the token order of appearance.",
            "10": "In the following paragraphs, the work presented lacks clarity and seems to over-complicate concepts with hard-to-follow math notations.",
            "11": "For instance, the “*Mapping words into and from symbolic representations*” paragraph introduces tedious math notations to describes something simple that was clear before, namely, the mapping from tokens to their respective symbolic vector, which is simply defined as the one-hot vector position appearance of this token in the context.",
            "12": "Similarly, the \"*Hybrid semantic-symbolic embeddings*\" paragraph uses again tedious math notations to describe how semantic and symbolic embedding are concatenated.",
            "13": "Given the confusion presented in Section4, it is currently not clear how adding a one-hot vector to the input embedding can make a neural model symbol-shift equivariant.",
            "14": "In particular, below are the two things I could not understand:\n1) The paper mentions that \"*all parameters are differentiable*\".",
            "15": "It is not clear if that also includes the symbolic representation or not?",
            "16": "If so, then the initial one-hot vector may not be a one-hot vector after the gradient updates performed during training, which would result in a non-symbolic representation?",
            "17": "if it is kept fix during training, then it is not clear how it is used by the network.",
            "18": "2) In addition, assuming that the symbolic representation of all tokens stays the same during training, I don't see how \"_permuted symbols share the same latent representations_\" if the latent representations are made of both on-hot vectors **and** regular word vectors.",
            "19": "I understand that the symbolic representation does not change for a permuted word since it will appear at the same place as the original word.",
            "20": "But the semantic representation will be different.",
            "21": "For instance, the semantic word vector of “banana” is similar but still different than the word vector of “apple”.",
            "22": "**Conclusion**.",
            "23": "I would suggest the authors to simplify their mathematical notation and make their paper easier to read.",
            "24": "As of now, I could not fully understand the paper and unfortunately for that reason could only put a score of 4 with a low confidence of 2."
        },
        "vR4_t_lmP5S": {
            "0": "This work proposes to improve the generalizability of bAbi models through [entity permutations].",
            "1": "More specifically the approach assumes domain knowledge of word/entity type equivalences, which helps restricting possible permutations between word POS (e.g., “John” vs “why”) or gender (e.g., “John” vs “Mary”).",
            "2": "Each word type has its own embedding param and is concatenated with normal word embeddings to form the final word representation.",
            "3": "Experiment with memory networks and Third-order Tensor Product RNN shows that the proposed approach indeed enables the models (especially TPR) to handle artificial data sets with large number of entity names.",
            "4": "Overall I find the proposed research not very well motivated.",
            "5": "Leveraging word type knowledge to improve the generalizability of NLP models has been a popular and effective approach.",
            "6": "Commonly used strategy is to replace named entities in sentences with their word type tokens .",
            "7": "e.g., from [how old is Obama] to [how old is PERSON]\nhttps://arxiv.org/abs/1601.01280\nhttps://arxiv.org/abs/1611.00020\nThe proposed approach seems to achieve a similar effect, but is a lot more complex."
        },
        "nJ1dB0KSpcK": {
            "0": "The authors propose a network that is equivariant to entity permutations without requiring the pre-specification of the set of entities.",
            "1": "To this end, the authors propose a hybrid semantic-symbolic embedding which they integrate into two QA models.",
            "2": "Finally, the authors show significant gains on the bAbi tasks, with especially impressive gains in the 1K setting.",
            "3": "The problem is quite interesting and challenging in the setting where entities are not prespecified.",
            "4": "However, given the model description it is not clear at all how the model is able to learn a symbol-shift equivariant embedding.",
            "5": "I don't understand how the model is able to determine that \"apple\" and \"orange\" have the same embedding while \"apple\" and \"John\" have different embeddings.",
            "6": "What is the loss/model architecture/data augmentation guiding this?",
            "7": "How is the model able to figure out that \"John\" and \"Sasha\" share embedding?",
            "8": "Apart from the high level details, I don't understand the following notations and operations:\n* In Section 4, what is $n$?",
            "9": "Is it total number of words in the sequence?",
            "10": "* If $B_\\varphi \\in R^{m \\times n}$  and $e^m_j \\in R^m$, the multiplication $B_\\varphi e^m_j$ doesn't make sense.",
            "11": "* How exactly is $\\alpha_x e_{\\varphi(x)} \\in R^m$?",
            "12": "What exactly is $\\alpha_x$ and what is it's shape?",
            "13": "The notation and the working of the model is not clear to me, hence, I am giving a low rating for now.",
            "14": "Apart from this I also doubt the proposed method's generalizability beyond toy settings."
        }
    },
    "cu7IUiOhujH": {
        "w1QOCSeoQ1t": {
            "0": "The paper proposes a new training objective for fine-tuning pre-trained models: a weighted sum of the classical cross-entropy (CE) and a new supervised contrastive learning term (SCP).",
            "1": "The latter uses the (negated) softmax over the embedding distances (i.e.",
            "2": "dot products) between a training instance and all other instances in the batch with the same label.",
            "3": "In contrast to the more traditional self-supervised contrastive learning (where positive pairs are obtained by applying transformations to the original data instance), there is no data augmentation; two examples with the same label constitute a positive pair.",
            "4": "Experiments on the GLUE benchmark compare the baseline (RoBERTa-Large with CE loss) against the proposed objective (RoBERTa-Large with CE+SCP loss).",
            "5": "There are 4 sets of experiments:\n1) When training on the full datasets, results are quite modest (+0.4 increase in accuracy on average over 6 GLUE tasks).",
            "6": "2) In the few-shot setting, CE+SCP does meaningfully better than the baseline (for instance, when fine-tuning on only 20 data points, CE+SCP improves accuracy by more than 10%); these gains decrease as the dataset size increases.",
            "7": "3) When the datasets are noisy (effect obtained via back-translation), CE+SCP shines again (for instance, when the degree of corruption is very high, MNLI accuracy goes from ~47% up to ~53%).",
            "8": "4) Finally, the authors look at domain shift; they fine-tune a model on SST-2, then apply few-shot learning on other sentiment classification datasets.",
            "9": "This set of experiments has quite high error margins, so I didn't find it as convincing as 2) and 3).",
            "10": "Here are some questions/suggestions for the authors regarding their experiments:\n\na) \"In all our experiments, [...] we sample half of the original validation set of GLUE benchmark and use it as our test set, and sample ~500 examples for our validation set from the original validation set [...]\" -- Evaluating the models on a *subset* of the validation set makes it harder to compare it against other papers that fine-tune RoBERTa-Large.",
            "11": "I think that, at least for Table 2, it would be useful for posterity if you could either i) get the true test scores from the GLUE server, or ii) use part of the training set for validation, and then test on the full dev set, which is more standard practice.",
            "12": "b) \"We run each experiment with 10 different seeds, and pick the top model out of 10 seeds based on\nvalidation accuracy and report its corresponding test accuracy\" -- I am assuming this statement describes how evaluation numbers are reported for a fixed set of hyperparameters.",
            "13": "Why do you choose to pick the *top* model as opposed to reporting the *average* accuracy across the 10 runs?",
            "14": "c) \"we observe that our proposed method does not lead to improvement on MNLI [...].",
            "15": "We believe\nthis is due to the fact that number of positive example pairs are quite sparse [...] with batch size 16 [...].",
            "16": "We show evidence for this hypothesis in our ablation studies that we show in Table 3\" -- Then why doesn't Table 3 include MNLI?",
            "17": "Am I missing something?",
            "18": "d) This method excels in the few-shot setting, at least compared to the CE baseline.",
            "19": "So I think it would be a lot more impactful to focus on this particular use case and convince the reader that CE+SCP is better than some other standard few-shot learning baselines (e.g.",
            "20": "meta-learning objectives).",
            "21": "I do appreciate that the current message of the paper is crystal-clear (adding a SCP term to the loss leads to better fine-tuning), but I also think that the results in Table 2 are too weak for this somewhat general statement.",
            "22": "There is quite a bit of real-estate in the paper that could be re-allocated to something more substantive (e.g.",
            "23": "Table 1).",
            "24": "Strengths:\n- The presentation of the paper is extremely clean, and the goal is clear.",
            "25": "- In the few-shot learning scenario, CE+SCP performs meaningfully better than the CE baseline.",
            "26": "Weaknesses:\n- The main weakness is related to my suggestion d) above.",
            "27": "I believe marketing CE+SCP as a general fine-tuning solution with somewhat underwhelming results in Table 2 is a missed opportunity to lead with potentially strong results on few-shot learning.",
            "28": "I'm calling the results \"underwhelming\" because there is evidence that a thorough hyperparameter sweep can boost fine-tuning accuracy on GLUE by quite a bit.",
            "29": "For instance, Dodge et al.",
            "30": "[1] show that fine-tuning BERT carefully can increase SST-2 accuracy by ~2% without any changes in the pre-trained model or fine-tuning objective.",
            "31": "[1] Dodge et al., Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"
        },
        "hb3eRpkLnrO": {
            "0": "This work adds a SCL (supervised contrastive learning) loss term during the fine-tuning stage of RoBERTa.",
            "1": "The results show that the model with the SCL term and cross-entropy (CE) achieve better GLUE scores than the classic baseline that only uses CE loss, especially when the numbers of supervised training data are small and the data is noisy.",
            "2": "Pros:\nThe method is simple and the improvement looks significant under various settings\n\nCons:\nIt is not very clear whether and why SCL loss improves the results (see the detailed comments below).",
            "3": "Clarity: \nThe text is fluent and the paper cites lots of related work, but the paper does not well explain why the method performs well.",
            "4": "Originality: \nThe SCL is not novel because it comes from computer vision but this is the first paper I have seen that successfully applies SCL in NLP tasks.",
            "5": "Significance of this work:\nIf the authors can really show that the improvement comes from SCL, it may become a popular tool in the fine-tuning stage.",
            "6": "It is possible that the source of improvement comes from the temperature tau and l2 normalization instead of SCL loss itself.",
            "7": "Both of the tricks could be also applied to CE loss.",
            "8": "Thus, could you do control experiments that replace the SCL loss with the CE loss but keeping l2 normalization and retune the tau and lambda.",
            "9": "You can report it as CE+CE.",
            "10": "As shown in Figure 1, the authors suggest that the main reason that SCL loss is better because SCL loss tends to encourage the samples belonging to the same class.",
            "11": "However, I believe that CE loss could achieve the same goal and maybe more aggressively than SCL loss.",
            "12": "My understanding is that while minimizing CE loss, we encourage each class embedding close to all its samples, so the class embedding tends to become the cluster center of all the points in the class.",
            "13": "In the meanwhile, we encourage the samples in each class close to its class embedding, so the samples within the same class will also become closer together, right (e.g., in Figure 3, CE loss could also separate the two classes with a large margin)?",
            "14": "The difference is that CE loss encourages the samples close to the average of the samples in the same class, but SCL loss encourages the samples close to each sample in the same class.",
            "15": "From this perspective, could you tell us why SCL loss is better (or tell me why this perspective is wrong)?",
            "16": "In the introduction, you cite several studies that mention the limitations of cross-entropy loss.",
            "17": "I think the motivation of the paper will be much stronger if the authors could briefly and intuitively introduce why CE loss is worse (e.g., what does it mean poor margins and why CE leads to them) and why SCL loss could fix that issue.",
            "18": "It would be even better if you can design experiments to demonstrate that (e.g., measure and compare the margins of CE+CE with CE+SCL).",
            "19": "The effect of hyperparameters on performances is not clear.",
            "20": "You mention that lambda is tuned in each downstream task.",
            "21": "It is possible that in many applications, lambda is 0.",
            "22": "Could you show the lambda value for each downstream application?",
            "23": "In addition, could you show the final tau value(s) as well?",
            "24": "In the experiment setup, you mention that you report the best model out of 10 seeds.",
            "25": "Could you also report the average performance?",
            "26": "This will tell us more about how stable this SCL method is.",
            "27": "In Table 3, could you also report CE only and CE+CE performance?",
            "28": "I believe the issues could be resolved by conducting more controlled experiments and analyses, so I currently vote weak accept.",
            "29": "If the concerns are not addressed well during the rebuttal period, I am very likely to change my vote to rejection.",
            "30": "Minor:\n1.",
            "31": "In equation (2), I think putting CE loss for multi-class classification would be more general.",
            "32": "2.",
            "33": "Before the \"Relationship to Self-Supervised Contrastive Learning\", you mention lower temperature creates harder negatives.",
            "34": "The meaning of negative here is unclear.",
            "35": "I think you mean harder examples here."
        },
        "WEB_kY5THRD": {
            "0": "The paper proposes using a combination of two losses in the fine-tuning stage when using a pre-trained model: the standard CE one, plus a supervised contrastive loss SCL (combined linearly via a \\lambda hyperparameter).",
            "1": "The supervised contrastive loss uses a normalization summation over the batch examples, with a temperature hyperparameter \\theta.",
            "2": "The empirical results cover nicely three scenarios: (a) the impact of adding the SCL loss, in the presence of all the fine-tuning data; (b) the impact of adding the SCL loss in few-shot learning scenarios; and (c) the impact of SCL in the presence of training noise (induced via back-translation through German, using a standard WMT-trained MT model).",
            "3": "The results as presented are encouraging, and support the main hypothesis of the paper, namely that training with the added SCL loss improves performance over all three scenarios mentioned above.",
            "4": "I have a few suggestions that could be seen as minor, and a few observations that are major.",
            "5": "Minor ones:\n(i) adding the SCL loss clearly impacts the training speed, yet there is no mention of that (especially as a function of the batch size); in particular, Table 3 would offer the perfect place for mentioning how the training time (for a fixed number of training steps) is affected by the increase batch size, so that the reader can understand both the “upsize” (the improved performance)  as well as the “downsize” (training cost).",
            "6": "(ii) I could see no mention of the settings for the hyperparameters used (\\lambda and \\theta), nor any ablation experiments that would indicate how their values have been chosen; in the interest of both reproducibility and increased understanding of the value of SCL, please add a sub-section that discusses this issue.",
            "7": "Major ones:\n(i) It appears that the authors have used the Roberta_large model and run their own experiments with fine-tuning, w/o and w/ the SCL loss, with little regard for reporting against the published numbers for the GLUE tasks; for instance, in Table 2, they report with CE-only to have RTE performance at 85.0, while the Roberta paper shows 86.6 for it; in this case, the CE+SCL at 85.6 no longer looks like a convincing win; a bit different but nevertheless troublesome is the result reported for CoLA, at 86.4 (CE-only); the numbers for CoLA are normally much lower than that, eg the Roberta paper shows 68.0 (CE-only); this disparity throws a lot of doubt over the accuracy of the results reported in Table 2.",
            "8": "(ii) I commend the authors for showing the variance across their results in both Table 3, 4, and 5; however, it is unclear to me that the claims that the CE+SCL approach is better are being supported by the results.",
            "9": "It is not like the CE+SCL gives lower variance, as it is clearly the case that sometimes it is higher than the CE alone; and the fact that, under this high variance, the CE method sometimes performs better CE+SCL puts the whole conclusion under doubt from an empirical standpoint.",
            "10": "One one result that seems to hold strong is the one for few-shot learning, which appears to support the main hypothesis of the paper.",
            "11": "However, the main issues mentioned above would need to be addressed in order to have the paper reach the level of clearing the bar for ICLR publication.",
            "12": "Re: References A lot of the references use the Arxiv version for papers that have been peer-reviewed and published.",
            "13": "Please fix."
        },
        "S5vKvANpZ5h": {
            "0": "Summary\n* For the fine-tuning of pre-trained language models, the authors proposed a supervised learning method that combines cross-entropy loss and contrastive loss.",
            "1": "Experimental results show that the proposed method improves over cross-entropy loss on several classification tasks of the GLUE benchmark set.",
            "2": "The proposed method outperforms cross-entropy loss in few-shot learning tasks and noisy datasets generated by English-German and German-English translation.",
            "3": "Strong points\n\n* The proposed loss function is reasonable and the effect of supervised contrastive learning was not reported for NLP applications before, the experimental results are valuable.",
            "4": "* The paper is well-organized and well-written.",
            "5": "* Without using extra datasets for fine-tuning, the proposed method consistently improves the baseline method.",
            "6": "* The generation of noisy examples using the back-translation model in Section 5.3 is an interesting approach to analyze model robustness.",
            "7": "Weak points\n* Although the supervised contrastive learning term as previously proposed in (Khosla et al.",
            "8": "2020), it is not cited in the section.",
            "9": "* The benchmark results in Table 2 are not comparable with conventional methods since the experimental setting does not follow the finetuning procedures from prior work (Devlin et al., 2019) which reports the test set performance obtained from GLUE submissions.",
            "10": "Decision reason\n* The technical contribution of this paper is limited since the proposed method is a rather strait-forward expansion of Khosla et al.",
            "11": "2020.",
            "12": "In addition, although it is novel to apply supervised contrastive learning for NLP applications, the impact of these results is also limited because the experimental results are not directly comparable with previous work.",
            "13": "Questions\n* Why did you \"sample ... taking the label distribution of the original validation set into account\" in Section 4.1?",
            "14": "I am worried that this sampling procedure may make the few-shot task easier.",
            "15": "Additional Feedback\n* Since the subtraction between two values in percentage is not a ratio,    the percentage is not an appropriate unit for it.",
            "16": "For example \"1.2% improvement on SST-2\" in Section 5.1 should be \"1.2 point improvement on SST-2\".",
            "17": "* Since Khosla et al.",
            "18": "2020 proposed a two-stage training procedure, supervised contrastive learning at the first stage and the learning of the output layer at the second stage, I would like to see the qualitative comparison with the proposed joint training procedure.",
            "19": "* Instead of using different original sentences for each T values, it is clear and compact to use the same original sentence for each T values."
        }
    },
    "US-TP-xnXI": {
        "hRjddVT9Ca3": {
            "0": "This paper proposed TANL, a novel approach by using generative models to solve structured prediction tasks in NLP.",
            "1": "The key idea is that we can formulate this as a translation from natural language input to the augmented natural language with the structure of the input, and we can leverage the label semantics of the label in the augmented natural language output.",
            "2": "In this way, it enables transfer learning from large pretrained generative language models such as T5.",
            "3": "This augmented natural language unifies the input/output format for many structured prediction tasks in NLP and thus can facilitate multi-task learning.",
            "4": "The experiments cover a dozen datasets on seven different structured prediction tasks (CoNLL04, ADE, NYT, ACE2005 for Entity Relation Extraction, CoNLL03, OntoNotes, GENIA, ACE2005 for NER, TACRED, FewRel 1.0 for Relation Classification, CoNLL05 WSJ, CoNLL05 Brown, CoNLL2012 for SRL, ACE2005 for Event Extraction, CoNLL2012 for Coreference Resolution, MultiWOZ for DST).",
            "5": "The result demonstrates that the proposed approach can achieve SOTA on several of them for Entity Relation Extraction, Relation Classification, SRL.",
            "6": "Strengths:\n- The idea of using the generative approach to solve structured prediction tasks in NLP is very novel to me, and the author shows there is a possible output format unification for a wide range of tasks and datasets, and thus we can study them all together using a single model even with the same set of hyperparameters.",
            "7": "- The format of output augmented natural language is simple while being able to encode different structures even with nested entities.",
            "8": "- The experimental results show that this generative approach even has superior performance while being much simpler than other task-specific classification models which require careful model architecture design for different tasks.",
            "9": "- The author also shows that the proposed approach is data-efficient and has an advantage in low-resource settings.",
            "10": "Weakness:\n- The experiments show that the proposed approach does not perform very well on two tasks: CoNLL-2012 for coreference resolution and MultiWOZ 2.1 for Dialog State Tracking.",
            "11": "For CoNLL-2012, TANL has Avg.F1 72.8 while the SOTA CorefQA has 79.9.",
            "12": "The author mentioned that CorefQA uses additional question answering pretraining, but TANL is based on T5 which also uses large-scale pretraining on question answering, translation, and summarization.",
            "13": "On MultiWOZ 2.1, TANL has 50.5 while the SOTA has 55.7\n \n- The advantage of multi-dataset or multi-task learning is not obvious.",
            "14": "The only major improvement from this is on CoNLL04, ADE, CoNLL05 Brown, possibly due to the fact that they are small datasets.",
            "15": "- There is no significance test for the results in Table 1.",
            "16": "Questions:\n- Have you tried with other T5 models such as T5-large?",
            "17": "or other generative models such as BART/GPT-2?",
            "18": "- I didn't understand your explanation for why the multi-task model has lower scores on coreference resolution.",
            "19": "Do you mean you only use smaller training data?",
            "20": "- Why there is no multi-dataset for Relation Classification?",
            "21": "- Why there is no multi-task or multi-dataset for DST?",
            "22": "- How do you compare your methods with Athiwaratkun et al., 2020"
        },
        "2yVvAEz2Lck": {
            "0": "This paper presents a text-to-text translation approach to a variety of structured prediction problems.",
            "1": "The authors explore several ways to represent each structured prediction problem as a text-to-text translation task and finetune T5 (Raffel et al., 2020) to perform each task.",
            "2": "The resulting model gives better results than existing models in the tasks of joint entity relation extraction, relation classification, and semantics role labeling.",
            "3": "Although this work is a rather straight-forward application of T5 to structured prediction problems, the reported experimental results and lessons learned regarding good text-to-text representations on the extensive set of structured prediction tasks should be useful to the community.",
            "4": "The experimental results in the multi-dataset and multi-task settings are also interesting (although not much analysis is given in the paper).",
            "5": "I was wondering why the authors did not apply their approach to the task of syntactic parsing, which is probably the most well-studied structured prediction task in NLP.",
            "6": "Is there any difficulty in applying the same technique to dependency or phrase-structure parsing?",
            "7": "I think the authors should also discuss the limitations of their approach.",
            "8": "Are there any structured prediction tasks in NLP that are difficult to tackle with their approach?",
            "9": "In section 5.2, it is a bit surprising that the model was able to learn the correct output format with only 9 sentences.",
            "10": "Were there no invalid outputs?",
            "11": "Minor comments:\np.3: the current state-of-the-art -> the current state of the art?",
            "12": "p.4: dynamic-programming -> dynamic programming (DP)?",
            "13": "p.5: don’t -> do not?",
            "14": "p.5: previous state-of-the-art -> previous state of the art?"
        },
        "nuEfqjWBMn": {
            "0": "# Summary\nIn this paper, the authors proposed a unified seq2seq model for structured prediction tasks in NLP.",
            "1": "They let the seq2seq model produce mixed outputs of special tokens and the original sentence.",
            "2": "Different NLP tasks, including relation classification, entity relation extraction, NER, etc.",
            "3": "can be converted into this seq2seq problem by adding special tokens.",
            "4": "The experiments show that the proposed model does better than the previous state-of-the-art, albeit with the help of multi-task and multi-dataset learning, on some of the tasks/datasets.",
            "5": "## Pros:\n1.",
            "6": "A unified framework that allows for multi-task and multi-dataset learning.",
            "7": "Their experiments also show that their model could benefit from multi-task, multi-dataset learning.",
            "8": "The experiments on few-shot relation extraction show that their model could transfer knowledge from high-resource tasks to low-resource tasks.",
            "9": "2.",
            "10": "The formulation is neat and extensible.",
            "11": "More difficult structured predictions tasks (in terms of structure), e.g.",
            "12": "dependency parsing, are in principle convertible to this format, although the authors didn't try it on parsing.",
            "13": "## Questions:\nFor structured prediction tasks, searching for the best output is a crucial part.",
            "14": "However, this paper doesn't explore search strategies too much.",
            "15": "Only in the appendix, beam search is mentioned.",
            "16": "More concretely, I would suggest the authors try to answer the following questions:\n1.",
            "17": "How much could we improve the current model purely by using better the searching strategy (the headroom)?",
            "18": "Different from CRF, the structured prediction model is not markovian, which means we have to brute-force the best output.",
            "19": "Is it possible to calculate such an upper-bound performance of the current model?",
            "20": "If so, what would be the upper-bound for each task?",
            "21": "2.",
            "22": "In this paper, the DP alignment method is a post hoc method.",
            "23": "What if we add such a monotonic alignment to the decoding process?",
            "24": "To summarize, I think this is a good paper in terms of extensive experiments and convincing results, but the search strategy still needs to be explored and justified for structured prediction tasks."
        },
        "-ieQZWWChE": {
            "0": "Recently, multiple research papers focus on task transformations by bridging the gap between different tasks[1,2,3,4].",
            "1": "The original idea may go back to [5].",
            "2": "This paper follows this line of research ideas by reducing a structured prediction problem to a translation problem.",
            "3": "The general idea is novel and very interesting.",
            "4": "By defining several manually-designed rules, multiple structured outputs are transformed into the output of the translation model.",
            "5": "The writing is clear and well-structured.",
            "6": "Pros:\n - A novel and interesting idea for formulating structured prediction tasks to translation problems.",
            "7": "This idea is well-motivated in low-resource scenarios and multi-task learning settings.",
            "8": "- The general framework is easy to implement (only requiring some scripts).",
            "9": "Cons:\n - My main concern with the proposed approach is the decoding process.",
            "10": "If the translated sequence has a nice structure, the transformation process is well performed.",
            "11": "However, the translated results might be invalid for a specific task.",
            "12": "For example, in CoNLL NER, a nested or overlapping structure might be generated.",
            "13": "It may need specially designed rules to filter them out.",
            "14": "However, this paper does not have many discussions on this point.",
            "15": "I would like to know more about this part.",
            "16": "- I also would like to know the effectiveness of different pre-trained language models.",
            "17": "In this paper, a T5-base model is utilized.",
            "18": "It might be beneficial to know the empirical effectiveness of different kinds of language models.",
            "19": "- Some words are not precise.",
            "20": "For example, the phrase \"generative models\" are frequently used to illustrate the translation model.",
            "21": "However, in the ML field, generative models may indicate the models that have a generative process of data and model the joint distribution of observed samples.",
            "22": "I am willing to increase my score if some of the questions are well clarified by the authors.",
            "23": "[1] Strzyz et al.",
            "24": "Viable Dependency Parsing as Sequence Labeling, NAACL 2019\n\n[2] Yu et al.",
            "25": "Named entity recognition as dependency parsing, ACL 2020\n\n[3] Gómez-Rodríguez et al.",
            "26": "Constituency parsing as sequence labeling, EMNLP 2018\n\n[4] Li et al.",
            "27": "A Unified MRC Framework for Named Entity Recognition.",
            "28": "ACL 2020\n\n[5] Vinyals et al.",
            "29": "Grammar as a Foreign Language."
        }
    },
    "Ozk9MrX1hvA": {
        "scZsGOYdKGd": {
            "0": "The paper proposes a novel data augmentation framework, which explores different combinations of isolated label-preserving transformations to improve the diversity of augmented samples.",
            "1": "The authors find that stacking distinct label-preserving transformations produces particularly informative samples.",
            "2": "The paper also introduces a contrastive learning objective to capture the global relationship among the data points in representation space.",
            "3": "In my opinion, the exploration of different combinations of isolated label-preserving transformations is the major contribution of this paper, which may inspire future works for data augmentation.",
            "4": "However, the contrastive regularization object is a bit incremental, and I cannot see a big difference compared with Moco or SupCon.",
            "5": "Strength:\n\n+ The idea of stacking distinct label-preserving transformations is intuitive.",
            "6": "+ The integration of the consistency training objective and the contrastive regularization objective is interesting.",
            "7": "Weakness:\n\n- Lack of novelty, the contrastive regularization object is a bit incremental, and this object is very similar to MoCo or SupCon.",
            "8": "- The model has first to obtain the augmented samples, which is computation expensive for large-scale datasets and may hinder the practical application of the model.",
            "9": "Moreover, the overall improvements are relatively small compared with R3F, and there is a lack of variance analysis.",
            "10": "Questions:\n\nWhat is the computational complexity of CoDA?",
            "11": "Why using MMD distance in section 3.1?",
            "12": "Is stacking distinct label-preserving transformations the default setting for CoDA in your GLUE experiments?",
            "13": "What if other strategies (mix, random) work better in datasets like QNLI, RTE.",
            "14": "MRPC, and so on.",
            "15": "Why not report results on those datasets?",
            "16": "What is the major difference between your contrastive regularization and MoCo or SupCon?",
            "17": "As the improvements are relatively small, could you please provide the test of statistical significance？\n\nWhat if you stack cut first and then back?",
            "18": "Does the order affect the performance?"
        },
        "_7lW_Y77tZ": {
            "0": "Paper proposes a contrastive learning-based approach to combine different data augmentation techniques for NLP tasks.",
            "1": "While the widely used consistency loss focuses on a single example, the proposed contrastive objective allows capturing the relationships among all data samples which helps in producing diverse and informative examples.",
            "2": "For experiments, the paper explores 5 data augmentation approaches with Roberta-large as the classification model.",
            "3": "Empirical results on the standard GLUE benchmark leads to an impressive 2.2% average improvement.",
            "4": "Authors also found that back-translation and adversarial training combination leads to better performance than other DA combinations.",
            "5": "Strengths:\n1.",
            "6": "The proposed framework can be applied with any text data augmentation methods.",
            "7": "It's a solid work that will help the NLP community in developing better DA techniques.",
            "8": "For example, [Kumar et al.",
            "9": "2020] shows that any pre-trained model can be used for data augmentation.",
            "10": "I believe seq2seq model like T5, BART based augmentation combined with CoDA, will further push the state of the art for text DA.",
            "11": "2.",
            "12": "Paper provides clear motivations and describes their methods, experiments in detail.",
            "13": "Authors study DA in both low-resource and rich-resource setting.",
            "14": "Ablation studies have been conducted to investigate gains from different components.",
            "15": "3.",
            "16": "Authors plan to release their code which is good for reproducibility.",
            "17": "Weakness: \nMy understanding is that all numbers reported in the paper are from a single experiment.",
            "18": "As a reader, I would like to see some variance with the results.",
            "19": "Apart from this, I don't see any major issues with the paper.",
            "20": "Questions:\n1.",
            "21": "Since one of your goals is to improve the diversity of the augmented data, have you tried replacing more words in the c-bert model?",
            "22": "By nature, c-bert is bound to replace max 15% of the tokens while maintaining the sentence length.",
            "23": "Methods such as back-translation or seq2seq models do not have such restrictions.",
            "24": "Also, have you considered using a pre-trained seq2seq model for DA as in [Kumar et al.",
            "25": "2020]\n2.",
            "26": "Fig 5, back-translation, and adversarial training have similar performance.",
            "27": "This result is intriguing.",
            "28": "Do you have some further insights into it?",
            "29": "Typos:\n- Sec2.2.",
            "30": "\"the first term correspond\" -> corresponds \n- Sec 4, Contrastive Learning para, \"ontrastive learning\" -> \"Contrastive learning\"\n\nReferences (additional DA citations):\n1.",
            "31": "Kumar, V., Choudhary, A., & Cho, E. (2020).",
            "32": "Data Augmentation using Pre-trained Transformer Models.",
            "33": "ArXiv, abs/2003.02245."
        },
        "uslwszAU-EW": {
            "0": "Summary:\n\nThe augmentation of NLP samples is an important task with no clear \"applicable to all\" mechanism.",
            "1": "This is in sharp contrast to computer vision where techniques like rotation, modification of hue, saturation as well as umpteen other techniques exist.",
            "2": "This work tries to address the issue by proposing a technique that carefully amalgamates multiple previously known approaches to generate diverse label preserving examples.",
            "3": "The experimental results on RoBERTa highlight the applicability and importance of this data augmentation approach on the downstream task of text classification (GLUE).",
            "4": "Strengths:\n\n1.",
            "5": "Empirical results.",
            "6": "Performance better than previous approaches (although minor).",
            "7": "2.",
            "8": "Paper Clarity\n3.",
            "9": "Each formulation is backed by a strong intuitive understanding.",
            "10": "4.",
            "11": "Contrastive training (negative sampling) is one of the crucial contributions of this work.",
            "12": "It seems to be making every previously known augmentation approach better.",
            "13": "Please feel free to highlight other major contributions.",
            "14": "Weaknesses (Minor):\n\n1.",
            "15": "Ad-hoc regularization parameter selection is necessary for getting performance gains.",
            "16": "This makes it difficult to conclusively prove that this is an \"applicable to all\" data augmentation scheme.",
            "17": "2.",
            "18": "It would have been better to see the performance gains on more difficult text-classification tasks (non-GLUE), or underperforming models (non-BERT based).",
            "19": "Since the gains are not much.",
            "20": "It becomes difficult to fathom if the gains are actually due to good objective function or a case of chance for choosing better examples.",
            "21": "Comments/Questions:\n\n1.",
            "22": "What is the augmentation size being used in the setup?",
            "23": "I suspect the size plays an important role in such setups and this hasn't been discussed much in the paper.",
            "24": "Also, please show the performance trends based on different augmentation sizes.",
            "25": "2.",
            "26": "How do you measure the diversity (as mentioned in the paper title) in the generated samples?",
            "27": "3.",
            "28": "Rather than using the ad-hoc approach for selecting which augmentation \"stacking\" scheme is helpful, it would have been better to compare/use an approach highlighted in \"Learning to Compose Domain-Specific Transformations for Data Augmentation\" [NeuRIPS 2017].",
            "29": "Correction:\n\n1.",
            "30": "Related Work: Contrastive learning - Under an unsupervised setting, ontrastive -> contrastive\n\nOverall:\n\nThis work highlights the importance of incorporating contrastive training for data augmentation.",
            "31": "Please let me know if I have misunderstood something(s)"
        }
    },
    "JE7a-YejzfN": {
        "95hQmZUky3L": {
            "0": "This paper proposes an analysis technique for studying the 'difficulty' of a pair of test dataset examples in NLP.",
            "1": "The setup proposed by past work on Contrast sets and Counterfactual Examples (Gardner et al, 2020 and Kaushik et al 2020 respectively) is to manually construct two dataset examples (x,y) with different labels y, while the inputs x differ only minimally.",
            "2": "This paper argues to compute the measure of a contrast / counterfactual example pair by extracting the largest Eigenvalue of a matrix (defined in part using the Fisher Information Matrix).",
            "3": "Strengths: \n* the idea of using the Fisher Information Matrix to categorize the difficulty of (contrast) examples in NLP seems new to this reviewer and well motivated.",
            "4": "* The paper argues that many of the existed (hand-crafted) perturbations are not difficult enough for models because they lie far away from the decision boundary, which could possibly lead to insights about how to build better contrast sets later on.",
            "5": "Weaknesses and questions:\n* To this reviewer, it is unclear why this paper leads to a different conclusion than that of Gardner et al, 2020 or Kaushik et al 2020.",
            "6": "It seems like those authors were able to mine 'difficult' examples where 'difficult' could be defined as resulting in a flipped prediction -- at least in their experimental setup.",
            "7": "It is unclear why if that is the case from the original papers, why the FIM eigenvalue doesn't increase -- but perhaps I'm missing something :) It would make this paper much stronger if the FIM metric was also compared with metrics proposed by those other papers, such as difference in accuracy.",
            "8": "* How do you define $\\eta$ for BERT-style models?",
            "9": "Particularly since the representations that these models learn about what a sentence is change quite a bit during finetuning.",
            "10": "* Perhaps this is more for future work, but to this reviewer, it is not clear what we learn about the insights of fragility of NLP models from this direction.",
            "11": "The methodology would be a lot stronger if we could use this technique to sample examples that are quantifiably difficult for existing models, e.g.",
            "12": "* Nie et al 2019 (Adversarial NLI)\n* Zellers et al 2019, HellaSwag: Can a Machine Really Finish Your Sentence?",
            "13": "Overall: at least to this reviewer, this work seems promising but there are a few open questions about how it works and how insightful it is.",
            "14": "I would be happy to raise my score if those questions were answered appropriately.",
            "15": "----\n\nUpdate: thanks for the response!",
            "16": "I read over the updated draft also but I'm still not sure what insights we learn about the fragility of NLP models under this evaluation paradigm.",
            "17": "For that reason I'm still confused as to whether this paradigm is better or worse than the original approaches of Gardner et al / Kaushik et al 2020, and so I'd like to stick to my score."
        },
        "AWNYTcwd9WE": {
            "0": "**Update after author response:**\nI went over the author response and have had a chance to carefully evaluate the updated draft.",
            "1": "I appreciate that the authors have taken the time to address two of my comments but I believe major concerns still remain unaddressed, so my evaluation remains unchanged.",
            "2": "1.",
            "3": "I appreciate the time taken to conduct the fastText experiments, would be helpful to show this on BERT (which is arguably your strongest model) as well.",
            "4": "2.",
            "5": "Per my reading it appears that you've shown how models behave if the replacements are sampled at random.",
            "6": "I appreciate the time taken to show that.",
            "7": "I believe this could be made better with a concrete discussion of the same.",
            "8": "Cons 3, 4, 5, and Additional comments 1,2: I do not think these have been convincingly addressed.",
            "9": "You point to Dynabench while suggesting that this would eventually lead to better evaluation sets but as it is shown in [1] and [2], \"difficult\" evaluation sets that are tied to one model are not necessarily difficult for other models.",
            "10": "If the idea is to identify model's vulnerabilities and not use these as common evaluation sets, then this is more or less the same as generating adversarial examples (but called \"difficult\" examples, in which case this would be better presented as an application/extension of Zhao et al to identify adversarial examples in NLP).",
            "11": "If the idea is to only use these examples for evaluation purposes, then the comparison to counterfactually augmented data (which you show does not include \"difficult\" examples per your definition) does not make sense since that is meant solely for augmenting training sets.",
            "12": "Discussion in Section 4 and onwards is shallow and often unclear.",
            "13": "That is primarily where I believe the exposition can be improved significantly.",
            "14": "For instance, Section 4 says FIM values capture resilience to linguistic perturbations but that has not been discussed in any of the paragraphs that follow.",
            "15": "Section 5.1 ends in a sentence that says, \"By repeating this process multiple times, more robust classifiers can be created\", and that is not discussed any further as to why you think that would be the case.",
            "16": "It is also not supported by any theory or empirical results presented in your prior or updated draft.",
            "17": "[1] Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber.",
            "18": "\"Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering.\"",
            "19": "In TACL.",
            "20": "[2] Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp.",
            "21": "\"Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension.\"",
            "22": "In TACL.",
            "23": "---------------------------------------------------------\n\nSummary:\n\nThe paper proposes using the largest eigenvalue of the Fisher information metric (previously studied to identify adversarial perturbations in computer vision) as a measure to construct \"difficult examples\" for the IMDb sentiment analysis task.",
            "24": "The authors suggest that one may perform such perturbations by replacing \"important\" words in the reviews.",
            "25": "They further analyze those examples comparing them to counterfactually revised data (CRD; Kaushik et.",
            "26": "al, 2020) and contrast sets (Gardner et al., 2020) and show that based on the FIM criteria, CRD and contrast sets are not too different from original reviews in the IMDb dataset.",
            "27": "Pros:\n- This paper describes a simple method for understanding difficulty in terms of ease-of-perturbation.",
            "28": "- The authors find that, interestingly, sentiment analysis datasets generated by counterfactually revising data are not \"difficult\" for classifiers.",
            "29": "Cons:\n- While the idea of using FIM to identify examples closer to a classifier’s decision boundary is interesting, it is a new application to NLP even though the idea has been discussed extensively in Zhao et al.",
            "30": "(2019).",
            "31": "However, the paper doesn't show why these examples may be useful.",
            "32": "The paper reports no empirical results or analysis thereof to understand the practical efficacy of these \"difficult examples\" while discussing only the changes in eigenvalues.",
            "33": "- It is not clear as to how generalizable this approach is.",
            "34": "For instance, the paper uses a replacement strategy based on substituting synonyms, antonyms, and certain kinds of noun phrases (actress names etc.)",
            "35": "which may not directly translate to other tasks such as question answering, news classification, VQA, or even NLI.",
            "36": "- The paper makes many philosophical claims without theoretical or empirical justification.",
            "37": "At one point it suggests that FIM captures resilience to linguistic perturbations but provides no backing for the claim.",
            "38": "The paper further suggests that evaluation sets should increase FIM but provides no theoretical/empirical justification for the same.",
            "39": "At another instance it says, “[t]ransfer learned models like BERT capture rich semantic structure … and tend to rely on semantically relevant words for classifying movie reviews” without citing any prior work that supports this claim.",
            "40": "- The approach is also very model specific.",
            "41": "For one model some data points may turn out to be not \"difficult\" but for a different model they may, which casts further doubt on the general applicability of this method in this setting.",
            "42": "Note Zhao et al use it to identify adversarial examples, which by definition are specific to a model.",
            "43": "Furthermore, if we go by the authors' suggestion that evaluation sets must increase FIM, then by using such \"difficult\" examples for evaluation we end up discriminating (by design) against the model that was used to identify them, and unintentionally favor other models, leading to a potentially flawed comparison.",
            "44": "I don't think that it is a desirable characteristic of an evaluation set.",
            "45": "- Particularly given the lack of empirical evidence showing the benefits of \"difficult examples\" identified by this approach, the core claim of the paper is that examples generated by prior work are not \"difficult\" in terms of the discussed metric and perhaps they need to be so.",
            "46": "However, the motivation expressed in Kaushik et al.",
            "47": "is not to create \"difficult\" examples for training or evaluation, but to learn better models by intervening on causal variables of interest.",
            "48": "A good counterfactual example generated by their approach would be one that modifies only sentiment-related variables and leave others intact (intervene on causal variables to d-separate labels from the spuriously correlated variables, see [1] for a detailed explanation), and it has no relation to any particular classifier.",
            "49": "I agree that Gardner et al.",
            "50": "do discuss their motivation as to construct examples closer to the decision boundary.",
            "51": "- The writing can be significantly improved.",
            "52": "Additional comments:\n- It is also not clear to me how replacements are sampled, it would be good to provide details on that.",
            "53": "- It would be nice to see how well models perform in-sample and out of domain when trained on \"difficult examples\" and how models trained on CAD perform on previously identified \"difficult examples\" in test sets.",
            "54": "Furthermore, are there overlaps between such examples if FIM is calculated with respect to a model trained on original data alone vs. one trained on CAD?",
            "55": "Missing references:\n- Please cite the IMDb dataset: \"Maas, Andrew, et al.",
            "56": "Learning Word Vectors for Sentiment Analysis.",
            "57": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.",
            "58": "2011.\"",
            "59": "- Prior work building on Gardner et al.",
            "60": "and Kaushik et al.",
            "61": "demonstrating the efficacy of their work: \nTeney, D., Abbasnedjad, E., & Hengel, A. V. D. (2020).",
            "62": "Learning what makes a difference from counterfactual examples and gradient supervision.",
            "63": "arXiv preprint arXiv:2004.09034.",
            "64": "- There are also a lot of arxiv citations for papers that have been peer reviewed.",
            "65": "Errata:\n- Page 2, line 3: “BERTby” -> BERT by\n- Page 2, second paragraph, line 1: Fisher information metric has already been defined in the previous paragraph so you may refer to it just as FIM over here.",
            "66": "- Page 3: Glockner et al.",
            "67": "should be \\citep and not \\citet.",
            "68": "Same for Rajpurkar et al., Recht et al., and Beery et al.",
            "69": "- Page 3: “learning byMitchell et al.",
            "70": "(2018)” -> learning by Mitchell et al.",
            "71": "(2018)\n\n\n[1] Kaushik, D., Setlur, A., Hovy, E., & Lipton, Z. C. (2020).",
            "72": "Explaining The Efficacy of Counterfactually-Augmented Data.",
            "73": "arXiv preprint arXiv:2010.02114."
        },
        "oXlHSxBqM7F": {
            "0": "## Summary\nThe authors argue that we should evaluate the robustness of NLP models near their decision boundaries, and argue that contrast sets and counterfactual examples cannot fullfill this purpose.",
            "1": "The authors propose to find examples near the decision boundary using the largest eigenvalue of the Fisher information matrix, arguing that this value gives us a sense of how stable the model prediction is near the input.",
            "2": "To verify that FIM can identify examples where the prediction is unstable, and perturbation leads to larger prediction change, the authors use some heuristic adversarial attacks: first identify tokens to replace using integrated gradients, then replace the tokens with synonyms (to confirm the prediction is sensitive) or antonyms (to confirm that the prediction is insensitive).",
            "3": "## Detailed comments\nIG-based perturbation: I was initially confused by how the authors plan to use IG for perturbation, since IG gives a scalar score for each word and does not tell us a direction for the word embedding to move along.",
            "4": "Then I realized that IG is only used to select which token to perturb, and the actual perturbation is done by replacing words with their synonyms/antonyms.",
            "5": "I think it’d help to clarify this point.",
            "6": "Framing: the largest eigenvalue of FIM tells us how easy it is to change the output probability distribution of the model by perturbing the input, but we don’t know if the groundtruth label of the example will change after perturbation.",
            "7": "In other words, in the general case, whether or not the perturbation leads to adversarial examples still requires human verification.",
            "8": "In this paper the authors get around doing human verification by replacing words with only synonyms or antonyms.",
            "9": "But I think it would help reinforce the message to make this point clearer.",
            "10": "For example, instead of saying “ and these examples are most susceptible to perturbations” (from second paragraph of section 4.1), it might be better to say “the model prediction on these examples are most susceptible to perturbations to the input”.",
            "11": "I’d argue against calling examples selected by FIM “difficult” and “easy”, maybe “unstable” and “stable” is better.",
            "12": "Keep in mind that FIM is computed specifically for a model, and the largest eigenvalue of FIM describes a property of the model’s induced distribution near that example, not the example itself.",
            "13": "Why do we need FIM?",
            "14": "This paper did not introduce a new perturbation, it introduced a way to identify examples that have high potential to be perturbed into adversarial examples.",
            "15": "Under this framing, I’m not sure that the proposed method is evaluated against proper baselines.",
            "16": "Importantly, the authors motivate this work by first observing that contrast sets and counterfactual examples do not really evaluate the models near their decision boundaries.",
            "17": "That is probably a fair assessment, but those work have a different goal in mind.",
            "18": "Both contrast sets and counterfactual examples ask human annotators for small changes that significantly change the _groundtruth label_ of the example.",
            "19": "Maybe these edited examples are not near the decision boundary, that’s because they are not designed to be; being close to the decision boundary is purely incidental.",
            "20": "I think this is an important distinction between contrast set/counterfactual examples and gradient-based adversarial attacks such as [HotFlip](https://arxiv.org/abs/1712.06751) and the human-in-the-loop counterpart such as [TrickMe](https://arxiv.org/abs/1712.06751).",
            "21": "The authors verify that the examples selected by FIM are indeed “difficult” (again, this is not the best term) using several heuristic-based attack (replacement with synonym, antonym, and replacement with actress names).",
            "22": "I don’t think this is a convincing evaluation of FIM as a difficulty measure: one can simply try these attacks and find the same set of “difficult” examples.",
            "23": "A more valid baseline is perhaps selecting examples by prediction confidence.",
            "24": "I think whether “models should be evaluated near decision boundary” is debatable.",
            "25": "Yes, it’s likely easier to create adversarial examples near the decision boundary.",
            "26": "But considering that we cannot yet claim that robustness in NLP is solved for the “easier” examples, I’m not sure focusing on examples near the decision boundary is the right objective right now.",
            "27": "Figure 1(a): no details were provided as to how the figure is generated for IMDb examples.",
            "28": "How did you reduce the dimensionality of the decision boundary to 2D?",
            "29": "How did you pick the examples?",
            "30": "What does the green band signify?",
            "31": "Are these real examples & real models or is this figure illustrative?",
            "32": "Figure 1(c): are colors (blue vs red) indicative of the sign of the eigenvalue?",
            "33": "It looks like the direction of the corresponding eigenvectors are only visualized for red points, how are they selected?"
        },
        "HohbPHXW1Zv": {
            "0": "Summary:\nThis paper defines a new metric to quantify the difficulty of an example in language classifiers: the largest eigenvalue of the Fisher Information Metric (FIM).",
            "1": "They show a geometric interpretation of the FIM, the greater the largest eigenvalues of the FIM, the closer the example is to the decision boundary.",
            "2": "The authors illustrate this with a toy example.",
            "3": "They then show how perturbations of examples with high eigenvalues for FIM, e.g., substitutions of one or more words, can flip the output of a classifier.",
            "4": "Whereas, examples with lower eigenvalues are not as vulnerable to substitutions.",
            "5": "The paper also argues that previous work on adversarial examples using contrast sets and counterfactual examples, are in fact not as vulnerable to substitutions, measured by the FIM.",
            "6": "Strengths:\n- Development of a quantifiable metric to identify difficult examples.",
            "7": "It would indeed be great to have such a metric, and help model developers to identify examples that are especially vulnerable to small perturbations.",
            "8": "This in turn can help in developing more robust models.",
            "9": "Therefore, I think there is potential for broad impact of this work.",
            "10": "- As far as I know, the idea of using eigendecomposition of FIM to differentiate between easy and difficult examples for NLP applications is novel.",
            "11": "It has been used to generate adversarial examples in computer vision.",
            "12": "Though I think the application to NLP is a strong independent contribution.",
            "13": "Assuming the claim can be established more definitively, the community can report results on thus identified difficult examples thus allowing us to inspect the vulnerabilities of any model.",
            "14": "- The potential geometric interpretation of FIM is appealing.",
            "15": "If this can be shown to hold given the nonlinearities in deep learning models this would be a very impactful result.",
            "16": "Weaknesses:\n1.",
            "17": "More empirical results are needed.",
            "18": "Alternately, theoretical proofs may be used to substantiate the claims.",
            "19": "a.",
            "20": "The idea that examples associated with large eigenvalues for FIM are more sensitive to perturbation is illustrated on a few select examples.",
            "21": "However, it would be great to see this hold across the dataset.",
            "22": "One way to show this might be to perturb all or some subset of examples in the test set, and plot the number of perturbations needed to flip the classifier output against largest eigenvalue of FIM.",
            "23": "b.",
            "24": "The reasoning that high eigenvalues correspond to points near the decision boundary is made based on an illustration on a linearly separable toy example.",
            "25": "It is not clear this will extend to deep learning models with more complex decision boundaries.",
            "26": "c. The only case against contrastive sets and counterfactual examples is that they dont have a high eigenvalue for the FIM.",
            "27": "However, as this paper does not definitively establish yet that high FIM values is always correlated with difficulty in classification/sensitivity to perturbation, the argument against competing methods is insufficient.",
            "28": "d. Since the use of eigenvalues as FIM as a measure of example difficult is generally applicable across several NLP tasks, the authors can make a stronger case by adding more datasets and tasks to the experimental results.",
            "29": "2.",
            "30": "Related work should be explained in more detail.",
            "31": "A case is made comparing the proposed metric against contrast set and counterfactual examples (recent works).",
            "32": "These should be described in more detail so paper is self-contained.",
            "33": "Similarly, prior work around eigendecomposition of FIM should be given due credit.",
            "34": "Zhao (2019) is mentioned in the introduction.",
            "35": "However, the derivation in the methods section should be duly attributed as well.",
            "36": "Recommendation:\nMy recommendation is to reject the paper at this time.",
            "37": "While I think there is great potential in the idea of using eigenvalues of the FIM to identify difficult examples, I think the experimental results are not sufficient to support the case as described above.",
            "38": "I encourage authors to make a stronger case in a future revision, by addressing the following points:\n- On any existing task/model, can it be shown that examples that are erroneously classified typically have high eigenvalues for FIM.",
            "39": "- Can the result be demonstrated on more than one dataset/task?",
            "40": "- Can the result be demonstrated on more than a handful of examples for a task.",
            "41": "e.g., by reporting average eigenvalues vs number of perturbations described above.",
            "42": "- Can the geometric interpretation be illustrated on more complex decision boundaries?"
        }
    },
    "0IOX0YcCdTn": {
        "m1BjkYpKect": {
            "0": "This work suggests an approach to improve the embodied agents interacting in an environment solving language-specified tasks.",
            "1": "The approach introduces a parallel text-based (minus any physical interaction or visual input) environment which can be used to pretrain the agent by learning language-only policies.",
            "2": "This text-based environment is an extension of the TextWorld framework.",
            "3": "Strengths:\n\n* The paper is clearly written.",
            "4": "* The premise of the paper that such a parallel environment allows agents to explore/learn in an abstract language-only environment while also allowing transfer of the learned knowledge to physical embodied environments is well motivated.",
            "5": "Weaknesses:\n\n* The main premise is that such a text-based environment can be used to learn abstract (high-level) policies that can then be transferred to an embodied agent to solve language-specified tasks in a physically simulated (with visual input) environment.",
            "6": "However, the main experiment to prove this claim in Table 4 falls short of proving this.",
            "7": "The results in that table show agents learned with an oracle state estimator which means there is no visual input processing during this mode.",
            "8": "It can also be noted that the Controller is also a heuristic module with no learning.",
            "9": "Which means under this setting, learning/evaluating in embodied environment is pure text-driven which is same as TextWorld, hence it is not surprising that TW-Only performs better.",
            "10": "* The performance of BUTLER-ORACLE in Table 3 is similar to the performance of BUTLER on TextWorld (on All Tasks) which further proves that using oracle state estimator is essentially reducing the embodied environment tasks to TextWorld task.",
            "11": "* Furthermore, given that this is the main premise of the paper, Table 4 needs to be complete with all the tasks.",
            "12": "* It is unclear why the components in BUTLER Agent interact using discretized constructs (of natural language) instead of continuous vector spaces.",
            "13": "* There seem to be several hacks adopted to obtain good results (such as using beam search during evaluation, using rule-based expert for supervision) which make this approach highly specific to the given dataset/environment limiting its general applicability.",
            "14": "Overall: The strong motivation of this work is not supported by empirical results.",
            "15": "The results presented in the paper use a number of hacks, are incomplete and lack insights.",
            "16": "Update after author response:\n\nThe author response is much appreciated.",
            "17": "However, my two main concerns remain unaddressed.",
            "18": "The authors may add these additional experiments/results to Table 3 and 4 in further revisions for a stronger submission.",
            "19": "* Table 3 is the main result of the paper which claims policies learned in TextWorld (TW) environment can be transferred over to ALFRED (ALF) environment under zero-shot setting.",
            "20": "This result alone has several weaknesses -- (1) Evaluation done on non-human goals in ALF seem to use same template as that used in TW, so it is not surprising that agents will have non-zero success rates on similar language specifications.",
            "21": "(2) When evaluation is done on human goals (which seems to be the real test), the agent's performance is very low.",
            "22": "Also, there are no baselines (e.g., random) provided to compare those scores against.",
            "23": "(3) Why are the experiments only conducted in zero-shot setting?",
            "24": "This actually brings me to the second weakness.",
            "25": "* Since the transfer learning is happening from a pure-text TW environment to a physically simulated (with visual input) ALF environment, it is more interesting/relevant to see how the language module pre-trained on text-only TW adapts to multimodal setup in ALF.",
            "26": "This adaptation will require further training/fine-tuning on ALF so that visual/control modules can adapt to this pre-trained language module.",
            "27": "This experiment was attempted in Table 4, however, as pointed in my initial review, this falls short of proving any claims made in the paper because the agents in Table 4 are learned with an oracle state estimator which means there is no visual input processing during this mode.",
            "28": "It can also be noted that the Controller is also a heuristic module with no learning.",
            "29": "Which means the setting used in Table 4 reduces learning/evaluating in embodied ALF environment to a pure text-driven environment."
        },
        "Ws-cdBeCf0b": {
            "0": "Summary:\n\nThis paper describes an approach for training an agent which completes goals specified via a language instruction in the ALFRED environment.",
            "1": "The paper proposes a model and training scheme.",
            "2": "The model decomposes the problem into three steps: (1) a perceptual module which takes as input an environment observation and generates a textual description of it including the objects and their spatial relations, (2) a goal-planning module which takes as input the high-level goal (which may require completing multiple subgoals), the textual description generated from module 1 and generates a textual description of a subgoal, such as an action the agent should take with arguments, and (3) a controller module which takes as input the state of the environment and the subgoal description generated from module 2, and generates a sequence of actions which execute this subgoal.",
            "3": "The entire model is ran each time the agent completes a subgoal, using the current environment observation as input to module 1.",
            "4": "The proposed training scheme focuses on pre-training the second module.",
            "5": "This module is pre-trained using a proposed environment, ALFWorld, which is modeled after text adventure games like TextWorld (Côté et al.",
            "6": "2018), and is intended to abstract away the perception and control problems from the reasoning module.",
            "7": "Training environments in ALFRED are converted into ALFWorld environments, and for each training goal, an oracle is constructed (which is a function that maps from any ALFWorld environment state to an optimal subsequent action that leads to the goal).",
            "8": "This facilitates pre-training the second module using DAgger through interactions with ALFWorld.",
            "9": "----------------------------------\n\nReasons for score:\n\nI vote for accepting the paper.",
            "10": "The idea of explicitly training the model to reason about subgoals is intriguing.",
            "11": "However, there are several assumptions and limitations of the proposed approach.",
            "12": "----------------------------------\n\nStrengths:\n\n- A model which decomposes high-level goals into low-level action sequences is very valuable and interesting.",
            "13": "- Comparison of single-goal vs. multiple-goal models.",
            "14": "- Good set of ablations, experiments, and comparisons, although I am not sure why Section 4.3 only looked at a single task.",
            "15": "----------------------------------\n\nWeaknesses:\n\n- Because ALFWorld is an abstraction of the problem, it must make assumptions about perception and/or physical control.",
            "16": "The paper describes several simplifications it makes: e.g., as far as I can tell, it disregards physical attributes of objects, and only retains the existence of particular object types when BUTLER::Vision processes an observation.",
            "17": "Also, ALFWorld does not implement physical/semantic constraints (e.g., what can fit in a microwave).",
            "18": "The setup of BUTLER means that losing perceived attributes in the output of the first module may result in the reasoning module being unable to reason about these attributes, and it is up to the designers of ALFWorld to decide which attributes are important to retain.",
            "19": "- As above, ALFWorld is manually designed and requires making decisions about what aspects of the problem to model and which aspects should be abstracted away.",
            "20": "This also includes requiring design of an oracle function, or alternatively an effective reward function (i.e., a denser reward function than binary task completion), which was not explored.",
            "21": "----------------------------------\n\nQuestions for the authors:\n\n- Can you elaborate more on what it means for a sequence to be prototypical (in the abstract)?",
            "22": "- What is the difference between the high-level goals and low-level step-by-step language in ALFRED?",
            "23": "Is this similar to the goal \"put a pan on the dining table\" vs. \"go to the cabinet\", \"open the cabinet\"?",
            "24": "- What exactly are the generalization dimensions for unseen rooms?",
            "25": "Novel combinations of object instances that existed during training, novel instances (e.g., color) of object types that existed during training, or completely novel object types?",
            "26": "Or something else?",
            "27": "- In the last paragraph of section 3.1, does this refer to playing a game at inference time using ALFRED, or during training with ALFWorld?",
            "28": "- Beam search is only done on the subgoal descriptions output by BUTLER::Brain, not the action actions taken in the ALFRED environment by the controller?",
            "29": "- If the Human Goals setting is using human-written (natural language) goal specifications, then what are the goals in the other evaluation settings?",
            "30": "Auto-generated?",
            "31": "----------------------------------\n\nAdditional feedback:\n\n- The color coding that begins on the third page is confusing, and the colors are very hard to tell apart (at least for me).",
            "32": "- In the related work on embodied language learning, the last sentence is a bit confusing as ALFRED is a fully interactive modality, right?",
            "33": "If not, what is considered fully-interactive?",
            "34": "----------------------------------\n\nRating after discussion: lowering to a 6, as I share concerns with R1 about experiments and generalizability of the proposed approach."
        },
        "yGRFjffJL3M": {
            "0": "The paper presents a new interactive environment which is both text-based and contains visual simulation which are aligned.",
            "1": "The authors also propose a first agent architecture which uses the visual observations as well as the text-based (named BUTLER).",
            "2": "The authors tested the generalization capabilities of the proposed BUTLER architecture compared to a seq2seq transformer model.",
            "3": "Strong points:\n- novel environment for text-based and aligned visual content (could potentially lead to follow up research) - a significant contribution to the community.",
            "4": "- have demonstrated that visual representation helps to generalize in these kind of environments (text + visual)\n- the paper is nicely written and easy to follow\n- the figures plots and tables are clear and help to understand the research\n\nWeak points:\n- the complex system: e.g.,  a pre-trained M-RCNN, a pre-trained text-agent, and training the text-agent using imitation learning (DAgger) biases the experimentation (makes the results less convincing).",
            "5": "Maybe an intermediate naive baseline should have been considered."
        }
    },
    "ey1XXNzcIZS": {
        "FVahvqP-LrJ": {
            "0": "In this paper, the authors explore alternatives to the standard token-based routing in sparsely-gated MoE models for multilingual NMT.",
            "1": "This exploration is motivated by the need for efficient inference in MoE models, for which token-based routing is a limitation.",
            "2": "The alternative is task-based routing, where examples for a task are assigned to the same experts.",
            "3": "This allows efficient device placement and request dispatch at inference time.",
            "4": "The paper compares with the approaches as well as hybrid approaches where different parts of the network use different routing mechanisms.",
            "5": "The results show that task level routing is comparable to token-level routing with the added benefit of inference efficiency.",
            "6": "Performing task-based routing on the decoder side only gives better better translation quality, at the cost of inference efficiently.",
            "7": "An analysis of routing decision in token-based routing justifies the design choices.",
            "8": "Overall, the paper takes a focused problem and experimentally shows an solution that improves inference efficiency.",
            "9": "The experiments are well-described and the paper is well-written.",
            "10": "While, the result is interesting it is only a marginal contribution with little novelty in my opinion.",
            "11": "It would also be interesting to see how this approaches compares with simpler approaches that deterministically allot parameters to different languages (Wang etal 2018, Bapna etal 2019, Zhang etal 2020)  or language groups (Fan etal 2020).",
            "12": "- Yining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu, and Chengqing Zong.",
            "13": "Three strategies to improve one-to-many multilingual translation.",
            "14": "EMNLP.",
            "15": "2018.",
            "16": "- Ankur Bapna, Naveen Arivazhagan, and Orhan Firat.",
            "17": "Simple, scalable adaptation for neural machine translation.",
            "18": "EMNLP.",
            "19": "2019.",
            "20": "- Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich.",
            "21": "Improving massively multilingual neural machine translation and zero-shot translation.",
            "22": "ACL.",
            "23": "2020.",
            "24": "- Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and Goyal, Naman and Birch, Tom and Liptchinsky, Vitaliy and Edunov, Sergey and Grave, Edouard and Auli, Michael and Joulin, Armand.",
            "25": "Beyond English-Centric Multilingual Machine Translation.",
            "26": "arXiv:2010.11125 preprint.",
            "27": "2020"
        },
        "jkaxuPMhD0c": {
            "0": "This paper compares different routing strategies in Mixture-of-Experts for multilingual NMT, and proposes to route by tasks instead of token, which can achieve better or comparable translation accuracy measured by BLEU and also enable separation of network structures at decoding time with affordable serving costs.",
            "1": "The paper claims that with task-level routing, the server only needs to pre-load K experts (assuming top-K routing for MoE layers) during inference, instead of loading all experts as in token/sentence level routing.",
            "2": "Overall, I appreciate the analyses and comprehensive experiment studies in this paper, and also the large-scale in-house datasets used in experiments.",
            "3": "The experiment findings about the different routing strategies in the encoder and decoder in en-xx and xx-en settings are also interesting.",
            "4": "The connections between the gating distribution and the similar of languages in Figure 3 also make sense.",
            "5": "However, I doubt the novelty and machine learning contribution of this paper: 1) The different routing strategies are natural and seems to have already been proposed by previous works.",
            "6": "e.g., for task-level routing, [1] used similar kind of mixture of experts in the language level.",
            "7": "2) This paper simply studies different routing strategies, which is more like empirical analyses.",
            "8": "Although the results are somewhat interesting, they are not surprising and most findings are in expectation.",
            "9": "[1] Universal Neural Machine Translation for Extremely Low Resource Languages.",
            "10": "https://arxiv.org/pdf/1802.05368.pdf\n\nMeanwhile, I have some questions on the experiment settings: when comparing with the single multilingual base model in Table 1 and Figure 2, the parameters of the MoE model are larger than the single multilingual model (e.g., 533M vs 142M in Table 1).",
            "11": "Therefore, it is obvious that MoE model achieves better accuracy than smaller single multilingual model.",
            "12": "The MoE models should compare with a single multilingual model with the same amount of parameters.",
            "13": "Another important thing I need to point out is that this paper seems to violate the anonymous policy of ICLR 2021.",
            "14": "In Section 4.3.1, the paper says \"We use an in-house training corpus (Arivazhagan et al., 2019)\".",
            "15": "However, Arivazhagan et al., 2019 shows the authors from Google, which reveals the organization of authors in this paper.",
            "16": "Meta reviewer can further double confirm if this violates the policy.",
            "17": "Besides, this paper is not carefully written and there are many typos which affect the reading.",
            "18": "e.g., 1) Two \"wo_e\" in the line below the equation in Section 2; 2) \"to route the token to a select few experts\", there is an additional \"a\"; 3) \"a learning rate of a learning rate of 3.0\" in Section 4.1."
        },
        "VjqPw6bnR-0": {
            "0": "This paper introduces several routing strategies for multilingual neural machine translation.",
            "1": "The motivation is to train a single mixture model that can serve the training and prediction of multiple models.",
            "2": "Specifically, several strategies are proposed: token-level, sentence-level and task-level.",
            "3": "Experiments on WMT and massive multilingual NMT dataset show that the proposed approach outperforms the vanilla unified multilingual model.",
            "4": "While this approach is simple and straightforward, I have some concerns.",
            "5": "Pros:\n- A mixture model is proposed for multilingual NMT, forming a hierarchical structure:  token-level, sentence-level, and task-level.",
            "6": "- The general framework is reasonable, straightforward, and easy to implement.",
            "7": "- Experiments are extensive (both the WMT data and massive NMT data).",
            "8": "Cons:\n- The general idea is not novel.",
            "9": "Building a mixture model for multi-task learning has been well studied in the literature [1,2] (not cited).",
            "10": "The relation may need to be clarified.",
            "11": "- In the experiment part, the results of single models (not unified multilingual model) need to be reported.",
            "12": "- Comparisons with other unified multilingual approaches are required [3,4],\n\n\n\n[1] Ma et al.",
            "13": "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts, KDD 2020\n\n[2] Ma et al.",
            "14": "Snr: Sub-network routing for flexible parameter sharing in multi-task learning.",
            "15": "[3] Tan et al.",
            "16": "Multilingual Neural Machine Translation with Language Clustering.",
            "17": "[4] Multilingual Neural Machine Translation with Knowledge Distillation."
        }
    },
    "Wj4ODo0uyCF": {
        "lYGH7w5rKp_": {
            "0": "In this paper, the authors present a study of different aspects of language-specific model capacity for massively multilingual machine translation.",
            "1": "To this end, language-specific behaviour is achieved via a combination of conditional computation to decide whether to use language-specific parameters or not and statically assigning experts for each languages.",
            "2": "The language specific sub-layers are incorporated throughout the network.",
            "3": "The training objective allow budgetary constraints on the amount of language-specific parameters.",
            "4": "The paper does a systematic analysis on the role of language specific parameters using the proposed architecture.",
            "5": "Based on the analysis, recommendations on design of multilingual NMT architectures are proposed and their efficacy validated experimentally.",
            "6": "The study sheds light on the amount of language specific parameter sharing, their distribution in the network, impact of language, etc.",
            "7": "I find the analysis presented in the paper very interesting and insightful - and distinguishes it from previous work in this area.",
            "8": "The hypothesis are clearly stated and the experiments are well designed.",
            "9": "The findings from the analysis are an important addition to the understanding of the role of language specific parameters in multilingual NMT.",
            "10": "In terms of modelling, the work follows in the line of recent work on language-specific parameters for multilingual NMT.",
            "11": "The deviation from existing work is mixing elements of conditional computation with language specific computation.",
            "12": "I see this work more as an analysis on language-specific parameters for a particular LS-model rather than a novel architecture.",
            "13": "It is not clear how this model would compare to other models using language specific parameters (sparsely gated mixture of experts (Lepikhin et al 2020), light-weight adapters (Bapna et al 2019)  ).",
            "14": "Questions: \n\n- Previous work has tried to combine both language-specific and shared parameters (Wang et al 2018), rather than making a binary choice between these.",
            "15": "Did the authors compare with such an approach?",
            "16": "- Since a major part of the model contains shared parameters, was there a need for new set of shared parameters along with the language-specific parameters.",
            "17": "The gating decision could have been to bypass the language-specific sublayer or not.",
            "18": "References\nYining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu, and Chengqing Zong.",
            "19": "Three strategies to improve one-to-many multilingual translation.",
            "20": "EMNLP.",
            "21": "2018."
        },
        "UUMF0tx7frV": {
            "0": "Manual parameter sharing schemes are generally costly to come up with and when they are obtained for certain language pairs they do not necessarily generalize well to arbitrary language pairs in multilingual NMT.",
            "1": "The idea of learning which parameters to share across languages in multilingual transformer models is original and potentially useful for designing and analyzing multilingual models in the context of NMT.",
            "2": "**Strengths**:\n\nThe paper is well-written and easy to follow.",
            "3": "The idea was (reasonably) well-positioned with respect to prior work and clearly presented.",
            "4": "The technical merit is essentially in coming up with the budget constraint term in the loss function that forces the multilingual encoder-decoder \"super-network\" to use the desired percentage of language-specific computation using gating.",
            "5": "A significant part of the contribution was in the analysis of the results, obtained by this learning-based parameter sharing approach, which was quite informative and revealed some interesting insights about where and when a language-specific computation is required.",
            "6": "The takeaways should be of interest to researchers and practitioners interested in designing and analyzing multilingual NMT systems.",
            "7": "**Weaknesses**: \n\n(1) Even though it is the first time such a method is applied in the context of NMT, the idea is not as much novel in the broader context of deep learning.",
            "8": "Prior work has explored \"learning-to-share\"  strategies for parameter sharing in multi-task learning (see Ruder et al., AAAI 2018), and using gating/masking to control computational paths in a differentiable way (see Fan et al., ICLR 2019, Sukhbaatar et al., ACL 2019); it is clear that the focus is NMT but it should be worth mentioning/discussing such studies to better situate the work and to help the reader assess the actual contributions.",
            "9": "(2) Another weakness is that the comparison with the vanilla and LS baselines does not seem to be properly controlled in terms of parameters.",
            "10": "I appreciate that the authors do not read too much into it and focus more on the analysis of the results, but one thing that remains unanswered in this paper is how the proposed method fairs against multilingual baselines that utilize (roughly) the same number of parameters; currently, the best models outperform the LS baseline by ~28M and ~10M parameters on OPUS-100 and WMT-14 respectively.",
            "11": "How important is this difference?",
            "12": "(3) In the experiment about linguistic similarity, it appears that the capacity schedule is the same across languages and the authors conclude from this that the schedule has little to do with linguistic characteristics.",
            "13": "However, the main driving force in the choice of the language-specific computation is currently a single hyper-parameter p which is the same across languages; so, this will lead to choices that are good on average for all language pairs involved for a given *universal* budget.",
            "14": "Do you think the conclusion would be still the same if a language-specific hyper-parameter p_l was used instead?"
        },
        "l_Gg-p4Mt2r": {
            "0": "The work proposes a hybrid architecture that has: (1) language-specific (LS) components; (2) as well as the components that are shared across all the languages -- a trade-off between specificity and generality.",
            "1": "A key conclusion of the work is that the best architectures typically are.",
            "2": "the ones that have ~10-30% language-specific capacity.",
            "3": "In terms of experimental work, the work uses WMT-14 and OPUS-100 datasets to show the proposed trade-off.",
            "4": "In terms of exposition of the ideas, it's a well-written paper for the most part.",
            "5": "One issue that the authors could improve on is clarifying how \"the amount of LS computation\" is measured.",
            "6": "You have mentioned it several times in the abstract/intro and it's neither clear nor referenced (it could be the number of parameters, it could be the number of basic computations, etc).",
            "7": "For a new reader, it takes quite a while to find that $p$ is defined in eq.",
            "8": "6 and defined as a budget contains.",
            "9": "One other quibble is that all the trade-off figures are shown based BLEU/automatic metrics, which are known to be inaccurate.",
            "10": "It would be nice to repeat one of the included evaluation with human judgments.",
            "11": "Overall, I view this as a good contribution to pave the way towards stronger, but reasonably-sized multilingual models.",
            "12": "This is partially assuming that the authors will stay true to their promise that \"Source code and models will be released.\""
        },
        "zvfRbxLUcGq": {
            "0": "In this work, the authors present a conditional language-specific routing (CLSR) scheme for transformer-based multilingual NMT systems.",
            "1": "They introduce a CLSR layer after every transformer encoder and decoder layer; each such layer is made up of hard gating functions conditioned on token representations that will either select a language-specific projection layer or a shared projection layer.",
            "2": "Further, a budget is imposed on the language-specific capacity measured by aggregating the number of gates that allow for language-specific computations; this budget constraint forces the network to identify the sub-layers that will benefit most from being language-specific.",
            "3": "This is nice work.",
            "4": "The proposed technique has been described clearly, the idea is intuitive and the experiments are pretty compelling.",
            "5": "I have a couple of minor comments/suggestions for the authors.",
            "6": "* The authors show heat-maps of LSScore distribution in Figure 6 (Appendix B) which suggest that the LS capacity schedule might have little to do with linguistic characteristics.",
            "7": "However, this might have to do with the multilingual model being trained on as many as 94 different languages.",
            "8": "It seems plausible that linguistic similarities might govern LS capacity scheduling when there are fewer training languages to learn from.",
            "9": "To check for this, it might be interesting to redo this experiment with the medium resource and low resource buckets containing 26-28 languages each.",
            "10": "* There are two (among many other) interesting things that stand out from the results in Tables 1 and 2.",
            "11": "(1) From Table 1, the only setting where CLSR* (as well as \"Top-Bottom\" and \"Dedicated\") underperforms compared to the baseline is M2O for low-resource languages.",
            "12": "It seems like the use of language-specific layers here has a strong adverse effect on performance (-4.56 with CLSR-L) which is largely offset by CLSR*.",
            "13": "Some more insights based on the individual BLEU scores for each test language in the \"Low\" bin and whether there were certain languages that were largely responsible for the drop in performance would be interesting to the reader.",
            "14": "(2) From M2O in Table 2, the win ratios of Top-Bottom are much lower when compared with Dedicated and CLSR* (61.54 vs. 84.62 vs. 84.62; 30.77 vs. 84.62 vs. 100).",
            "15": "Could the authors share their thoughts on why this drop might be appearing?"
        }
    },
    "J3OUycKwz-": {
        "QxnC9gJQbz2": {
            "0": "_**Update after author response**_: I think this is a very promising paper, and I am really excited about seeing techniques from neuroscience employed to answer questions about neural network models.",
            "1": "The authors have further conducted several additional experiments after reviewer comments, which I appreciate.",
            "2": "However, my most fundamental concern -- the mismatch between the method and the way that it is validated -- unfortunately still stands, which is why I would encourage the authors to further pursue this line of work, but recommend to reject it for ICLR.",
            "3": "**Summary**\n\nThis paper proposes to apply time-scale methods from neuroscience to investigate the timescale organisation in neural language models.",
            "4": "More specifically, the authors test the timescale of individual units in a word- and character-level LSTM by comparing the units' activations values on the same sentence, but with different contexts.",
            "5": "Using this method, the authors first  show that the higher layers on average have longer timescales.",
            "6": "They then, for all units, they fit a logistic function to the \"recovery\" curves and use the half-times of this curves as an indication of the time scale of these units.",
            "7": "They test the syntax unit and two long-distance units found by Lakretz et al and show that the number units have similar time-scales, while the syntax unit have a longer time scale.",
            "8": "Lastly, the authors analyse the connectivity between the longer time scale units and find that the units with longer processing timescales make a larger number of strong projections.",
            "9": "Within these units, the authors identify two sets of units in the word-level LSTM: \"controller units\", that play a role in how the connectivity of the network is updated, and \"integrator units\", that instead integrate information.",
            "10": "**Strong points**\n- Neuroscience has long been asking questions about the brain that are very similar to the questions we now ask about neural networks, cross-pollination between these fields is extremely important, and this paper contributes to this\n- Aside from the main technique, the paper introduces some interesting and useful methods, such as projectivity analysis and k-core analysis.",
            "11": "I think these methods can be useful for other researchers as well\n- Time scale analysis of LSTMs is a very relevant and interesting topic, that deserves more attention than it is currently getting\n\n*Concerns*\n- My main concern is that there seems to be a mismatch between the \"language time scales\" on which the authors operate: their experiment is designed to investigate the impact of extra-sentential context, but the Lakretz et al results they keep coming back to concern syntactic phenomena that are only relevant *within* a sentence, which is a different scale.",
            "12": "In other words, the units found by the authors of this paper are long-distance when it comes to integrating context, but the syntax and number units found by Lakretz et al are not really related to that: they model relationships *within* sentences.",
            "13": "Theoretically speaking, they should be reset at the beginning of every new sentence and they should thus be completely independent from the content.",
            "14": "That the authors find this to be untrue is interesting, but inconsistent with what Lakretz et al describe these unit do.",
            "15": "Since this is not addressed at all in the paper, it makes the results in general a bit difficult to interpret.",
            "16": "_**Update after author response**: In their response the authors clarified that the they have only analysed single sentences, where two distinct subsentences are combined with a conjunction.",
            "17": "This, unfortunately, does not make a difference for the argument: whether two sentences are split by a full stop or instead concatenated with \"and\" does not make any difference for the argument above, since the subject-verb agreement relationships that the units the authors look at model do not cross these boundaries either.",
            "18": "Furthermore, in their response the authors state that the find that the context representations of units was 'reset' at sentence boundaries, as I asked before.",
            "19": "I appreciate that the authors did these additional experiments, but I find the result somewhat worrisome: since the units they are looking at are syntactic units that encode number across long distance subject verb relationships, they should be reset both when a new sentence starts, as well as when a new conjunct with a new relationship starts.",
            "20": "In terms of SV relationships, there should be no difference between \"The boy kicked the ball and the girl caught it\" and \"The boy kicked the ball.",
            "21": "The girl caught it.\"",
            "22": "That the authors do find a difference points to a potential flaw in methodology._\n\n- Relatedly, the authors say that  their result that the syntax unit is a long distance unit, while the number units are not.",
            "23": "This is not consistent with what they say in the related work of the section, but also not with the results reported by Lakretz et al, who hypothesise that the syntax units represent the depth of the syntactic dependency.",
            "24": "This is something that changes with every new incoming word, whereas the number units are the ones that have to keep their activation constant across time.",
            "25": "- While, as I said before, I think it is great that the authors try to use methods from neuroscience into the field, I do think that in this case  the main method they propose is only very marginally different from earlier work (in particular Khandelwal et al.",
            "26": "Perhaps it would make more sense to put a bit more stress on the rest of the methods as well (btw, also Lakretz et al do connectivity analysis).",
            "27": "- The results are a bit underexplained, and understanding them requires many back and forths to the appendix.",
            "28": "I would have appreciated a bit more motivated interpretation of several aspects.",
            "29": "For instance: why is there such a large difference in activation differences in different units in the \"pre-shared segment\" part, and is this related to the half-time (it seems so from the plots)?",
            "30": "What is the difference between character and word-level models in terms of expectations (we'd expect there to be an additional level of time-hierarchy, perhaps?)",
            "31": "How do assessing activation differences and correlations differ in terms of conclusions?",
            "32": "These things should, in my opinion, all be worked out a bit better.",
            "33": "- Lastly, there are a few unsupported claims, the most important of which that their method recovers the previously discovered units of Lakretz et al, while (as far as I understand), they actually only *use* their method to analyse those neurons, but did not find them independently.",
            "34": "(for other suggestions and comments, see below).",
            "35": "To summarise, while I think the idea is very nice and definitely worth working out further, I do think that some work is needed to make this a publishable paper.",
            "36": "*Suggestions/comments for authors*\n_Typographic_:\n- If you use quotes in latex, you should use different ones for left (`) and right ('), for them to appear correctly (check for instance line three in the introduction)\n- To prevent additional spaces after abbreviations like e.g.",
            "37": "and i.e., put a backslash: \"e.g.\\ \"\n- Lerner et al --> put all references within parenthesis\n- Introduction switches from present tense to paste tense in the last paragraph\n- \"we measure the time-taken for the effect of this prior context to ”decay” (see Methods)\" --> I don't really understand what this means, you measure how long it takes for these changes to not be measurable anymore?",
            "38": "- Try to avoid double parethesis with abbreviations, e.g.",
            "39": ": (WLSTM Gulordava et al.",
            "40": "(2018)) should be: (WLSTM, Gulordava et al; 2018).",
            "41": "You can do this with \\citep[text before][text after]{citation}.",
            "42": "- \"has an 650-dimensional\" --> \"has a 650-dimensional\"\n- \"without fine-tuning to the novel\" --> I first thought this sentence was unfinished until I read back and realised that \"the novel\" is your corpus.",
            "43": "This is a bit confusing perhaps you could rephrase.",
            "44": "- \"how the cell state activation differ\" --> \"how the cell state activations differ\"\n- \"we will see that the activation difference drop quickly' --> drops quickly / see the activation difference drop quickly\n- There are several references that were published at ACL* conferences that are listed as arxiv papers in the reference list (Lakretz et al, Gulordava et al, Khandelwal et al)\n\n_Content_\n- I would say that the conclusion that \"Overall, prior works suggests that a small subset of units track long-range dependencies\" is rather overstated: Lakretz et al found that the units representing long distance number information were sparse, but this does not imply that long range information in general is represented sparsely.",
            "45": "Their method also focusses quite exclusively on finding sparsely distributed properties, as more distributed properties cannot be found with ablation.",
            "46": "Furthermore, this is just one study, focusing on one syntactic aspect.",
            "47": "I would suggest to rephrase this a bit.",
            "48": "- Lakretz at all actually identified several syntax units, but only one of them was interpretable.",
            "49": "- I find it a bit confusing that in 3.2, second paragraph, you first talk about comparing cell state activation, then say that you compare hidden state activations and then talk again about the cell state activation\n\n- Figure 1 C & D: I don't think these figures add much to the paper, for the following reasons i) They show only individual units and no average, making it difficult to interpret the values ii) while, as pointed out in 5.1, the *rate* of decay is the most important, the cut-off point is not indicated in the figure, which puts a stress on irrelevant aspects: the actual difference between the two lines.",
            "50": "- I would appreciate to have Figure A.1 in the main text, it is important for the story."
        },
        "PQyeL7IFvVL": {
            "0": "This paper explores the application of innovative methods to track the flow of linguistic information in LSTM language models.",
            "1": "In particular, the overarching question is how contextual information might be encoded in the network at the level of single units, and how context disruption might alter the LSTM dynamics and thus impact its predictive ability.",
            "2": "The paper is clear and it tackles an interesting question.",
            "3": "The approach is well motivated, and the authors give a brief survey of the most recent applications of this kind of methodology in linguistics and cognitive neuroscience studies.",
            "4": "The methodology is generally appropriate, though some details and parameters (e.g., numerical thresholds) seem to be chosen arbitrarily.",
            "5": "Also, the analysis could be improved by applying statistical testing in order to better quantify the strength of the observed effects.",
            "6": "Overall, I think this is a nice paper, though it might be especially relevant to the linguistics community rather than to the ICLR community.",
            "7": "Moreover, I think that further analyses are required in order to better clarify some important aspects.",
            "8": "In particular, I think that ablation studies should be performed in order to better identify the functional role of the “controller” and “integrator” units, whose actual functional role remains a bit speculative (and mostly based on structural / connectivity information).",
            "9": "It would also strengthen the paper to have some more controlled simulations, where the contextual information is defined according to specific linguistic constraints, in order to better characterize what the target units are actually encoding.",
            "10": "Indeed, as also noted by the authors almost “all the long timescale units are of unknown function”.",
            "11": "Finally, I think that it would be important to establish whether these findings are generally applicable to LSTM models, regardless of the specific architecture under investigation (e.g., What happens if we force the LSTM to rely on fewer units?",
            "12": "Does the hierarchical organization of the context improve by adding more layers?).",
            "13": "Other comments:\n- Why did the author choose to test the model on a different corpus (Anna Karenina novel) rather than considering a test set from the same corpus from which the training set was derived?",
            "14": "The Tolstoy book might have a quite different linguistic structure from that of the corpora used to train the LSTMs.",
            "15": "- It might be informative to also include a third condition in-between “Intact” and “Random” context, where the same context words are maintained with scrambled order.",
            "16": "This would allow to better understand the role of individual words in shaping context representation and activating the LSTM units.",
            "17": "- In Fig.",
            "18": "1D, it is interesting to note that the Unit 823 (green line) actually exhibits a sharp increase in difference after the shared segment starts.",
            "19": "Do the authors have a possible explanation for this kind of phenomena?",
            "20": "Was it observed systematically in other units?",
            "21": "- In relation to the results shown in Fig.",
            "22": "3A, I did not understand how the thresholds and parameters for the k-core analysis were chosen.",
            "23": "- Pg.",
            "24": "3: there is a typo regarding the size of the output layer (5,0000)\n- In Fig.",
            "25": "A1, error bars would help in better understanding the actual difference between the curves.",
            "26": "- In order to improve reproducibility, it would be very helpful to share the source code used for these analyses."
        },
        "YsssyVYMlpr": {
            "0": "This paper applies tools from neuroscience to understand how language models integrate across time.",
            "1": "The basic approach is to present a phrase, preceded by two different context phrases: one that is natural (i.e.",
            "2": "the phrase that actually preceded it in the corpus) and one that is randomly selected.",
            "3": "The authors then measure how long it takes for the unit activations to become similar for the two different contexts, which provides a measure for how long the context impacts the representation.",
            "4": "They find that (1) timescales increase at later layers of the language model (2) that only a small fraction of units exhibit long timescales (3) that long/medium-timescale units appear to come in two forms which they try and characterize using graph-style analyses.",
            "5": "--\n\nPros:\n\nHow language models integrate across time is clearly important, and this paper describes interesting first steps in characterizing the analysis of time using relevant tools from the neuroscience literature.",
            "6": "The method presented is simple and broadly applicable.",
            "7": "The graph-style results seem intriguing if a little hard to make sense of.",
            "8": "I also think that the sparsity of the long-timescale units is cool and interesting.",
            "9": "--\n\nLimitations and questions:\n\n1.",
            "10": "It’s not clear to me if the notion of time is a meaningful one in a language model.",
            "11": "For example, the duration of contextual effects on a unit that codes syntactic number will presumably be highly variable and depend upon the details of the particular sentence being encoded.",
            "12": "Thus a natural question is how variable are these timescales from moment-to-moment?",
            "13": "What’s being plotted is the average across a bunch of sentences, segmented at a particular moment (a conjunction).",
            "14": "How robust are these results if one examines a different point in a sentence?",
            "15": "Are the timescales of some units more variable than others?",
            "16": "-- Update: the authors have repeated their analysis for a different sentence point (after the 10th word) and report similar results.",
            "17": "This analysis is helpful, though of course the 10th word is not a very principled break point, and there presumably is a lot of variation in timescales that are being averaged across.",
            "18": "I continue to wonder how meaningful the notion of an absolute timescale is.",
            "19": "-- \n\n2.",
            "20": "None of the steps in the graph analyses seemed particularly natural or well-motivated to me.",
            "21": "Why were the graph edges thresholded at z>5 and why was k-core analysis performed?",
            "22": "I find it hard to make sense of what this analysis tells us about how language information is processed.",
            "23": "Is there some reason why medium timescale “controller” units and long-timescale “integrator” units should help with language processing?",
            "24": "If these results are purely exploratory and lack a clear interpretation, then perhaps the authors could help the reader by explaining the thought process behind the exploration.",
            "25": "Perhaps starting with the MDS plot would be useful rather than the k-core analysis, because the MDS plot clearly shows some interesting structure.",
            "26": "-- The authors have motivated some of their analyses by discussing brain research reporting that longer-timescale regions are more densely connected.",
            "27": "Of course, the relationship between connectivity between large-scale brain regions and the units in a LSTM remains highly speculative.",
            "28": "But having some motivation is helpful.",
            "29": "--\n\n3.",
            "30": "It would be interesting to know how dependent these findings are on the model’s architecture.",
            "31": "Would similar results be found for a Transformer or a simpler GRU-style RNN?",
            "32": "-- The authors have attempted to address this point, but with limited time were not able to train a network to a high level of performance.",
            "33": "--\n\n--\n\nMinor points:\n\nIn Figure 4, it would be helpful if the absolute timescale was labeled in all plots rather than the rank of the unit or the “normalized timescale”.",
            "34": "The absolute timescale seems much more meaningful to me (and the units can of course still be ranked, just the axis labels changed or augmented).",
            "35": "The legend for Figure 4c is incorrect."
        },
        "g6fkLwE-cm": {
            "0": "This paper looks at LSTMs with the intention of understanding their functional connectivity.",
            "1": "I am not sure exactly what the relationship between the brain and LSTMs is being assumed or proposed herein — however I understand the need to understand complex neural networks regardless of their relationship to biological systems.",
            "2": "I would have liked to have a discussion with respect to what the hierarchical organisation is due to.",
            "3": "Is this merely a repercussion of the connectivity, for example?",
            "4": "What do the authors think?",
            "5": "In terms of work that looks at ablation (i.e., damage), it might be useful to bear in mind limitations of such work if various (seemingly, perhaps) extraneous factors are not taken into account, see: https://doi.org/10.1007/s42113-020-00081-z\n\nI think this paper can be polished to the level of a solidly good paper if the authors can sketch out a bit more their rationale and syllogisms with respect to my above questions.",
            "6": "Minor:\n* Figures are very hard to read, is it possible to redesign them slightly to make the text bigger?",
            "7": "* In LaTeX to open double quotes you need to use two backticks.",
            "8": "Also the \\cite and \\citep commands should be used appropriately in terms of places where \\citep is needed as well as use of optional arguments to avoid double parentheses."
        }
    },
    "2LiGI26kRdt": {
        "uoyYDZhX3Zo": {
            "0": "### Summary\nThis paper proposes a simple method, ie multi-stage layerwise training (MSLT), to speedup BERT training.",
            "1": "Specifically, the authors progressively stack layers.",
            "2": "The bottom layers are fixed and only the new added top layers are trained.",
            "3": "The proposed method can achieve more than 110% training speedup and achieve comparable (slightly worse) performance.",
            "4": "Although compared with other speedup training methods like ELECTRA [1], the idea of this paper is not novel.",
            "5": "However, the proposed method is simple and effective to some extend.",
            "6": "Thus, I give a marginal score to this paper.",
            "7": "### Strengths\n* The proposed training method is simple and easy to implement.",
            "8": "* It can achieve 110% training speedup without significant performance degradation.",
            "9": "### Weaknesses and Questions\n* The most related method to this paper is [2].",
            "10": "Thus, it is better to give more comparisons and discussions in the Experiment, so that the author can know the advantages of MSLT compared with [2].",
            "11": "* From Figure 4, it seems that BERT-base (baseline) needs to train around 80 hours.",
            "12": "If training with MSLT using the same time as baselines (like train 2M steps), how about the performance compared with baseline?",
            "13": "[3] shows that training with more steps can help improve performance.",
            "14": "Does this phenomenon still remain in your method?",
            "15": "[1] Clark, Kevin, et al.",
            "16": "\"Electra: Pre-training text encoders as discriminators rather than generators.\"",
            "17": "ICLR.",
            "18": "2020.",
            "19": "[2] Gong, Linyuan, et al.",
            "20": "\"Efficient training of bert by progressively stacking.\"",
            "21": "ICML.",
            "22": "2019.",
            "23": "[3] Liu, Yinhan, et al.",
            "24": "\"Roberta: A robustly optimized bert pretraining approach.\"",
            "25": "arXiv preprint arXiv:1907.11692 (2019)."
        },
        "SzJ5b--Jbyy": {
            "0": "This paper presents a training strategy to progressively adding top transformer layers, which results in training time speedup.",
            "1": "Usually when training transformers, all layers are updated simultaneously.",
            "2": "The author found out doing a multi-stage layer-wise schedule helps convergence speed without significant performance degradation.",
            "3": "The paper is written very clearly.",
            "4": "The idea is conveyed with sufficient background and related work.",
            "5": "My biggest concern is from the originality and significance.",
            "6": "It looks to me the approach that the paper proposed is a very straightforward extension from the work of (Gong et al., 2019).",
            "7": "Instead of updating all parameters in all transformer layers, this paper freezes bottom layers and only update top transformer layer (newly added).",
            "8": "With experiments performed on base and large BERT models on GLUE dataset, this training strategy is approved to have slightly drop of quality but faster convergence speed.",
            "9": "I would argue that this is a great investigation and experimentation but it does not meet the criteria of acceptance.",
            "10": "In section 3.1, the author has used the attention distribution (before and after) to motivate the work in this paper.",
            "11": "However, if the author could discuss/explains why this change affects the convergence speed in analytical/mathematical solution, that would definitely gives more credit to originality and significance.",
            "12": "I would be more than willing to re-evaluate my ratings if that happens.",
            "13": "Minor comments/typos:\n\n* abstract: \"have has achieved\" -> \"have achieved\"."
        },
        "tMQs-arME6V": {
            "0": "The authors propose a multi-stage layerwise training (MSLT) approach to reduce the training time of BERT.",
            "1": "Experimental results show that the proposed method can achieve  110%+ training speedup without significant performance degradation.",
            "2": "Overall the idea of multi-stage layerwise training is reasonable and the results look promising.",
            "3": "My major concern about the work is the empirical comparisons.",
            "4": "ALBERT, a closely related work, is not compared in this work.",
            "5": "This paper claims that \"ALBERT has almost the same computational complexity as BERT, training an ALBERT model is still very time-consuming.\"",
            "6": "I checked the ALBERT paper, which claims 1.7x training speedup.",
            "7": "\"An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster.",
            "8": "The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization.\"",
            "9": "Seems the proposed method in this work (with 1.1x speedup) is not as good as ALBERT.",
            "10": "I suggest to take ALBERT as backbone and check whether the MSLT can speed up the training of ALBERT.",
            "11": "Besides, it is better to also test on more complex downstream tasks, such as SQuAD1.1/2.0 and RACE."
        },
        "QaRiexqwvbT": {
            "0": "The work proposes a simple enough idea to speed up the training of BERT by progressively stacking new layers while fixing older layers.",
            "1": "Empirically, with the same number of training steps (and less time), the proposed method can achieve a comparable performance to the original BERT.",
            "2": "When the same amount of running time (more steps) is used, the proposed strategy can further improve the performance.",
            "3": "One problem with the current paper is the empirical evaluation is only conducted on the GLUE benchmark, which is sequence-level and relatively simple.",
            "4": "I think an experience on a slightly more difficult task such as SQuAD, which also requires token-level prediction, would be necessary to test the capacity boundary of the proposed approach.",
            "5": "Another question is what would happen or what the performance would be if the entire model is not jointly trained for the last 20% steps.",
            "6": "This information will help to better understand this method.",
            "7": "In addition, the original motivation of the work comes from the fact that the attention patterns in the bottom layers do not change much after jointly trained with more higher layers.",
            "8": "However, this does not mean the lower-layer attention patterns don't change much if the entire network is jointly trained from scratch.",
            "9": "To truly establish the validity of motivation, it would be good to monitor and evaluate how much the lower-layer attention patterns change when jointly trained."
        }
    },
    "T4gXBOXoIUr": {
        "NQlsOFVSZfR": {
            "0": "Summary:\n\n1.",
            "1": "This paper tackles the medical image understanding problem.",
            "2": "The aim of this paper is to learn a generic feature representation for medical image that could benefits downstream tasks like medical image classification, zero-shot classification.",
            "3": "The main contribution of this paper is proposing a contrastive loss that the matched pair of image and text should have a higher corresponding score than the mis-matched pairs.",
            "4": "Pros:\n1.",
            "5": "The problem of learning a generic representation from limited training data is important.",
            "6": "2.",
            "7": "Comparing with the baseline, the performance of the proposed approach looks good.",
            "8": "3.",
            "9": "The paper is clearly written.",
            "10": "Weakness:\n1.",
            "11": "My first concern is the novelty.",
            "12": "If I understand correctly, the main novelty of this paper is proposing the losses (Eq.",
            "13": "2 and Eq.",
            "14": "3) in the section of contrastive visual representation learning from text.",
            "15": "However those losses are well-known in the domain of image-text retrieval.",
            "16": "Current vision-language BERT (VLBERT) approaches use similar loss to optimize their model [Lu et.",
            "17": "al., 2019].",
            "18": "The difference is that, in VLBERT papers, the model is trying to contrast the positive pair of image and text against all negative pair of image and texts that sampled from a batch: $$\\ell = - \\log \\frac{\\exp(<v_i, u_i> / \\gamma)}{\\sum_{k=1, k\\neq i}^N\\sum_{j=1, j\\neq i}^N \\exp(<v_j, u_k>/\\gamma)}.$$ While the proposed approach uses Eq.",
            "19": "2 and Eq.",
            "20": "3 to optimize text retrieval and image retrieval, respectively.",
            "21": "However, the motivation of separating the loss into Eq.",
            "22": "2 and Eq.",
            "23": "3 in unclear.",
            "24": "2.",
            "25": "In the related work, recent advance in vision-language BERT has been mentioned (Last paragraph of Sect.",
            "26": "6 Related work).",
            "27": "The paper stated that (in point 2) 'existing work has focused on visual-linguistic tasks such as visual question answering.",
            "28": "', which might not be accurate.",
            "29": "As current vision-language BERT models are trying to learn a generic representation for both image and text, where downstream tasks are ways to evaluate the representation.",
            "30": "The aim of vision-language BERT is aligned with the aim of this paper.",
            "31": "3.",
            "32": "The paper mentioned multiple times that the proposed approach is specific for medical images (Last paragraph of Related work).",
            "33": "However this is unclear to me why the proposed approach is specific for medical images?",
            "34": "I would be happy to increase my rating, if the author could differentiate the proposed approach (losses) with the one used in current visual-language BERT.",
            "35": "It would be the best if the author could state clearly why the proposed approach would be beneficial for medical images (Otherwise, 'medical' in the title of this paper might need to be removed).",
            "36": "The author might also need to re-state the novelty contribution of this paper."
        },
        "triDpDQjme7": {
            "0": "#####################   Summary   ####################\n\nThis paper presents the Contrastive VIsual Representation Learning from Text (ConVIRT) pretraining strategy to learn fine-grained medical visual representations of medical images by pretraining on large-scale image-report pairs.",
            "1": "As a result, ConVIRT improves medical visual representations by maximizing the agreement between true image-text pairs versus random pairs via a bidirectional contrastive objective between the image and text modalities.",
            "2": "The authors demonstrate that ConVIRT can help improve performance on down-streaming tasks like image classification and image retrieval tasks.",
            "3": "#####################   Strengths   #################### \n\n(1) This paper is really clearly written.",
            "4": "The paper is easy to follow and understand.",
            "5": "(2) The paper explores an interesting direction of learning fine-grained medical visual representations for medical image understanding.",
            "6": "(3) The proposed ConVIRT is well motivated.",
            "7": "The experimental results are very solid.",
            "8": "#####################   Weakensses   #################### \n\n(1) The paper is limited in its novelty borrowing ideas from some previous works: (i) contrastive learning [1][2] and (ii) image-text representations pretraining [3].",
            "9": "However, it is an interesting idea of applying contrastive learning to medical image and text.",
            "10": "So, I think this paper is novel to some extent, but not that novel, since it mainly makes some incremental contribution by combining ideas from existing work.",
            "11": "(2) The analysis is not convinced (see below).",
            "12": "#####################   Questions   #################### \n\nI have some questions for the authors:\n\n(1) What the medical visual representations have learned?",
            "13": "(2) Compared with the baseline methods (section 3.3), why the proposed ConVIRT can achieve the best results?",
            "14": "How these results can be achieved?",
            "15": "(3) Why the proposed ConVIRT can learn better medical visual representations than the SimCLR [1] and MoCo v2 [2] under the setting of image-only contrastive learning (section 5)?",
            "16": "How they differ from representations learned by existing methods, e.g., SimCLR [1] and MoCo v2 [2]?",
            "17": "(4) Can the medical visual representations pretrained on the chest image be further finetuned on the bony image?",
            "18": "Overall, although the paper doesn't provide insights around what the representations have learned and how they differ from representations learned/used by existing methods, they have provided substantial evidence to suggest that pretraining helps in a lot of downstream tasks.",
            "19": "In other words, the proposed ConVIRT seems to be useful for the researchers in the medical AI field.",
            "20": "[1] A simple framework for contrastive learning of visual representations.",
            "21": "In ICML 2020.",
            "22": "[2] Improved baselines with momentum contrastive learning.",
            "23": "arXiv preprint arXiv:2003.04297.",
            "24": "[3] ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.",
            "25": "In NeurIPS 2019.",
            "26": "####################  After Rebuttal  #################### \n\nI thank the authors for responding to the comments and have read them carefully.",
            "27": "The authors have addressed my concerns in the rebuttal."
        },
        "2THIW-CiMnP": {
            "0": "In this work, the authors propose a new model, named ConVIRT to learn the medical visual representation from paired image and textual data in an unsupervised strategy.",
            "1": "In ConVIRT, they mainly use a contrastive loss with two modalities (images and texts) as inputs to learn the representation.",
            "2": "The experimental results show their proposed model achieve higher performance than other methods in image classification and zero-shot retrieval tasks.",
            "3": "Strength.",
            "4": "In this work, the authors propose a simple and straightforward method to learn the image visual representation using modified contrastive loss with two modalities as inputs.",
            "5": "Besides, the experimental comparisons show their model outperforms other baselines and methods.",
            "6": "Weakness.",
            "7": "1.",
            "8": "Novelty.",
            "9": "In this work, the main contribution is the proposed modified contrastive loss with two modalities as inputs.",
            "10": "It is an interesting and challenging problem that how to extract the visual representation in an unsupervised strategy.",
            "11": "However, the proposed modified contrastive loss is below the standard of top-tier conference ICLR.",
            "12": "2.",
            "13": "Experiment.",
            "14": "This work evaluates their model in many tasks and datasets.",
            "15": "But, it might miss some baselines or other state-of-the-art methods.",
            "16": "(1) Baselines.",
            "17": "I suggest the authors could add the baseline using triplet loss.",
            "18": "Both contrastive loss and triplet loss could enlarge the distances between different classes.",
            "19": "(2) Baselines.",
            "20": "I also suggest the authors could add the baseline only using the images as inputs, just like Chen et al.",
            "21": "2020a.",
            "22": "and maybe only using the text data as inputs.",
            "23": "3.",
            "24": "Implementation details.",
            "25": "(1) BERT encoder.",
            "26": "In section 2.3, they use a BERT encoder to extract the textural features.",
            "27": "I suggest the authors could give more details about it, such as how to initialize the parameters?",
            "28": "As we known, there exists a huge gap between contexts form website and that from the medical report, since there are so many specific nouns.",
            "29": "(2) Text transformation function.",
            "30": "I might not agree with the text transformation function used in this work as the following reasons.",
            "31": "There might exist a big gap and different meanings between two sentences even in one document.",
            "32": "As the goal is to maximize the agreement between the true image-text representation pairs, there might occur a conflict between one image and two different meaning sentences sampled from the same documents.",
            "33": "4.",
            "34": "Visualization.",
            "35": "(1) I suggest the authors could give more visualization examples about the learned visual and textual representation, such as t-SNE.",
            "36": "Such visualization might also address my above the concerns about the potential conflict during training.",
            "37": "In these t-SNE examples, according to the proposed pipeline, the distance between the learned visual representation and the learned textual representation of the corresponding sentences in the same document should be smaller than the distance between the visual representation and some similar sentences from other documents.",
            "38": "5.",
            "39": "New type of baselines.",
            "40": "This work aims to learn the useful visual representation from both images and texts.",
            "41": "However, I suggest that they should report another kind of baselines, only using images to learn the visual representation in an unsurprised way, such as Chen et al.",
            "42": "2020a and [1] (Unsupervised Representation Learning by Predicting Image Rotations, ICLR 2018).",
            "43": "The results of the proposed method should outperform this kind of baseline."
        }
    },
    "jP1vTH3inC": {
        "WUbsJYBCKBh": {
            "0": "This paper proposes to model the generation order as latent variables for sequence generation tasks, by optimizing the ELBO involving a proposed process of Variational Order Inference (VOI).",
            "1": "To alleviate the difficulty of optimizing discrete latent variables, the authors propose to cast it as a one-step Markov Decision problem and optimize it using the policy gradient.",
            "2": "The authors also introduce the recent developed Gumbel-matching techniques to derive the close-form of the posterior distribution.",
            "3": "Pros:\n1.",
            "4": "Overall, I think the research problem, i.e., explicit modeling the generation order,  in this work is interesting and worthy of discovering\n2.",
            "5": "Casting the optimization of discrete latent variables as a one-step MDP is interesting\n3.",
            "6": "Experiments show that the induced \"best-first\" order outperforms fixed orders, which verifies the motivation of the paper`\n4.",
            "7": "Extensive and inspiring analysis \n\nCons:\n1.",
            "8": "Explicit modeling the generation order is not a very novel idea that there have been many works on this topic.",
            "9": "2.",
            "10": "For checking the generalization of the method and better comparison w/ InDIGO (though InDIGO also conducted on MSCOCO, Django and the current comparison is sufficiently fair), I would like to increase my rating if seeing more experiments on large scale machine translation benchmarks as those in InDIGO.",
            "11": "This would also further support your claim of a general-purpose approach w/ little domain knowledge (if any).",
            "12": "-------\nMinors:\n- figure 1: could consider adding x, which would better match the descriptions of the paper (modeling p(y|x) instead of p(y))\n\nMissing references: \n\n[1] Chan, W., Kitaev, N., Guu, K., Stern, M. and Uszkoreit, J., 2019.",
            "13": "KERMIT: Generative insertion-based modeling for sequences.",
            "14": "arXiv preprint arXiv:1906.01604.",
            "15": "[2] Gu, J., Wang, C. and Zhao, J., 2019.",
            "16": "Levenshtein transformer.",
            "17": "In Advances in Neural Information Processing Systems (pp.",
            "18": "11181-11191).",
            "19": "[3] Bao, Y., Zhou, H., Feng, J., Wang, M., Huang, S., Chen, J. and Li, L., 2019.",
            "20": "Non-autoregressive transformer by position learning.",
            "21": "arXiv preprint arXiv:1911.10677."
        },
        "SkUEed-plB": {
            "0": "The authors  propose the first domain-independent unsupervised learner that discovers high-quality autoregressive orders through fully-parallelizable end-to-end training without domain specific tuning.",
            "1": "Inspired by the variational auto-encoder, they propose an\nencoder architecture to infer autoregressive orders.",
            "2": "To solve  the non-differentiable ELBO ( discrete latent variables), they further construct a  practical algorithm with the help of policy gradients.",
            "3": "The experiment results, such as the  global and local statistics for learned orders, are convincing.",
            "4": "Some problems.",
            "5": "1: The authors should clearly define the two similarity metrics between autoregressive orders in the appendix.",
            "6": "2: Please check up the X_axis in the Fig 3.",
            "7": "3: It would be better if the authors provide more results in the appendix,  such as the ablation studies about the 'K' and visualizations of sequences generated by the baselines."
        },
        "yMUyFJg8Wwy": {
            "0": "- Summary\nThis paper aims to decode both content and ordering of language models and proposes Variational Order Inference (VOI).",
            "1": "The authors introduce a latent sequence variable z = (z_1, .. ,z_n) in which z_t is defined as the absolute position of the value generated.",
            "2": "The authors model the posterior distribution of z as a Gumbel-Matching distribution which is relaxed as a Gumbel-Sinkorn distribution.",
            "3": "To training the encoder and decoder networks, the ELBO is maximized using the policy gradient with baseline.",
            "4": "The experimental results on Django and MS-COCO 2017 dataset show the proposed VOI outperforms the Transformer-InDIGO, as well as suggests that learned orders depend on content and best-first generation order.",
            "5": "- Strong points\n\t1.",
            "6": "The research on non-autoregressive orders to generate language is interesting, and the proposed method using Gumbel-Sinkorn distribution is mathematically well sound and novel.",
            "7": "2.",
            "8": "The proposed method outperforms the previous Transformer-InDIGO and other baselines (Random, L2R, Common, Rare).",
            "9": "This paper analyzed the learned orders globally and locally, and conducted ablation studies.",
            "10": "3.",
            "11": "The paper is well-written and the authors also provide source codes for reproducibility.",
            "12": "- Weak points\n\t1.",
            "13": "The results are not compared with other SOTA auto-regressive algorithms.",
            "14": "- Questions\n\t- How do you think about transferring knowledge from auto-regressively trained such as GPT-2 to such non-autoregressive models?",
            "15": "Using the pertained model for the encoder and decoder would improve the results?",
            "16": "Have you tried this strategy?"
        },
        "OXn2q76q7nJ": {
            "0": "This paper designed a new generative model by capturing the auto-regressive order as latent variables for sequence generation task.",
            "1": "Based on combinatorical optimization techniques, the authors derived an policy gradient algorithm to optimize the variational lower bound.",
            "2": "Empirical results on image caption and code generation showed that this method is superior than both fixed-order generation and previous adaptive-order method transformer-InDIGO.",
            "3": "The authors further analyzed the learned orders on global and local level on COCO2017 dataset, demonstrating that the arrangement tend to follows the best-first strategy.",
            "4": "Concerns:\n1. effect of sample size K: In section 5 training part, the paper claimed \"For our model trained with Variational Order Inference , we sample K = 4 latents for each\ntraining sample.\".",
            "5": "The sample size K is used to approximate the gradient in variational order inference and it also affects the training efficiency i.e.",
            "6": "$O(NKdl^2)$.",
            "7": "It's not clear how the author choose the appropriate sample size K. Some analysis or experiemnt reults on the sensitivity of sample size K will help clarify this concern.",
            "8": "2. experiments on nmt & running time: The papers didn't report any results on machine translation, an important task on conditional sequence generation.",
            "9": "Since previous work(e.g.",
            "10": "transformer-InDIGO) demonstrated superior results on several translation datasets, it's recommended that the authors also showed results on these datasets.",
            "11": "Also one strength of the approach is its potential of fully parallelizing.",
            "12": "A running time comparison will provide more convincing evidence to this claim.",
            "13": "3. figure: Figure 1. in section 4 showed the structure of variational order inference.",
            "14": "Considering the paper is mainly focused on conditional generation, an conditional generation version will be better by incorporating x sequence into the figure."
        }
    },
    "tyd9yxioXgO": {
        "TBsQHrJEGcw": {
            "0": "This paper proposes a generative method (AG2Vid) that generates video conditioned by the first frame, first layout and an action graph.",
            "1": "An action graph is defined such that nodes represent objects in the scene and edges represent actions.",
            "2": "To capture the temporal dynamics, each pairwise connection is enriched with a time interval to indicate the temporal segment when the action happens.",
            "3": "For each time step, the method consists of several stages: First, it creates the layout corresponding to the current time step based on the current graph and previous layout.",
            "4": "Then it extracts the optical flow based on the last two layouts and the previous generated frame and finally, it generates the current frame (at pixel level) based on the predicted optical flow and the previous frame.",
            "5": "Several metrics, including human evaluation, indicates that the method outperforms powerful baselines on two datasets: CATER and Something-Something v2.",
            "6": "Pro:\n- Generating video content is a difficult task and the idea of generating frames based on action graphs to more explicitly focus on the activity class is interesting and naturally integrated into the proposed architecture.",
            "7": "- The method clearly outperforms the baselines and produces high-quality videos.",
            "8": "- The experiments regarding the generalization to novel compositions of actions are interesting and show promising results for generating videos beyond the training domain.",
            "9": "- The paper is generally well written, with clear explanations on the main aspects and a good balance between quantitative evaluation and qualitative examples.",
            "10": "Cons:\n- Since Action Genome [1] dataset provides more complex scene-graph annotations for videos in Charades, quite similar to the one required in this work (especially the contact subset of Action Genome), why do the authors choose Something-Something dataset instead of Charades?",
            "11": "- From the paper, it seems to me that the subset of videos picked from Smt-Smt dataset only contains 2 objects (nodes), thus the action graph is very simple and the temporal segment covers the entire video.",
            "12": "This aspect is only briefly mentioned in the main paper.",
            "13": "Moreover, I think the ability of the method would be more clearly demonstrated in classes that have more than 2 objects if the extraction of the AG would be possible in that case.",
            "14": "- The RNN ablation is interesting, showing the necessity of GNN processing.",
            "15": "However, more details about the motivation and intuition behind these experiments should be added in section 5.",
            "16": "Minor:\n- I think there is a typo in Eq (4).",
            "17": "Shouldn’t VGG be applied also on the predicted v_t?",
            "18": "- In the same manner, as the compositional experiment, it would be interesting to test the model using the same first frame from training videos, but changing the action labels from the Action Graphs (on Smt-Smt).",
            "19": "In this way, it would be clearer that the model doesn’t use any kind of biases in the dataset.",
            "20": "Since video generation could be a sensitive task, especially when conditioned on a set of actions, the ethical aspect of that work should be taken into consideration and discussed.",
            "21": "[1] Action Genome: Actions as Compositions of Spatio-temporal Scene Graphs, Ji et.",
            "22": "al, CVPR 2020\n\nI found the proposed method interesting and suitable for the video generation tasks.",
            "23": "Moreover, both the ablation study and the quantitative evaluation show good performance, so I recommend the acceptance.",
            "24": "########### UPDATE #########\n\nI thank the authors for their responses and for updating the paper.",
            "25": "I think this work introduces some new and valuable ideas for generating videos conditioned by an action graph and I recommend the acceptance."
        },
        "0KS4fwOThqi": {
            "0": "This paper proposes a model for video generation which disentangles the object layout prediction, frame-by-frame, from the actual pixelwise frame generation.",
            "1": "A so-called Action Graph (AG) is used as specification of the video to be generated, rather than a sentence.",
            "2": "Action graphs model objects as nodes and actions as clocked edges.",
            "3": "This way action graphs are \"clocked\" so to take into account the current progress of each action.",
            "4": "A Graph Convolutional Network (GCN) is used to process the action graph and predict the next layout.",
            "5": "The GCN is fed with the previous layout and the current AG.",
            "6": "The final frame is generated by warping the previous frame and predicting an additive signal to the warped output.",
            "7": "The network is trained similarly to a GAN.",
            "8": "Experiments on two datasets are provided human and quantitative evaluation show superior results wrt to the baseline\n\nStrengths\n- well motivated approach for video generation.",
            "9": "- experimental results show better performance with respect to segmentation based video generation \n\n\nWeaknesses\n\n- one main limitation of the method is the lack of a specific procedure to obtain action graphs.",
            "10": "This reflects both at training time and at inference time but it is more critical at training time.",
            "11": "The level of detail of annotation required is high and fine grained.",
            "12": "How would this work to obtain action annotations for a dataset such as Kinetics, AVA or EPIC-KITCHENS?",
            "13": "-unclear use of GAN training.",
            "14": "GAN-like training is exploited but it is not clear why this is necessary.",
            "15": "Moreover, apparently there is  no noise injection allowing to generate multiple videos from the same action graph.",
            "16": "What would happen if the loss in Eq (1) is dropped in favor of only the perceptual losses?",
            "17": "Is the GAN loss required to impose \"veracity\" on the layout generation?",
            "18": "Given that GT layout are available why not adding some layout consistency loss?",
            "19": "This could be done by optimizing the IoUs of objects [a].",
            "20": "- How do you manage similar AG corresponding to different output frames?",
            "21": "Is the GAN loss used for this?",
            "22": "What happens if you remove it?",
            "23": "Novelty and related work.",
            "24": "Why Nawhal et al., ECCV 2020 is not discussed in the related work section?",
            "25": "The proposed approach seems largely inspired by it and HOI-GAN is even used as a baseline, a fair writing of the related work and introduction should mention and discuss in detail how this work improves over it (are the timed edges the main addition?).",
            "26": "The concept of action progress has been used before with a very similar definition to the one used here.",
            "27": "Regarding the model:\nLayout are sets of bounding box coordinages + \"features\".",
            "28": "The model demands to the \"feature\" to encode all information which is not included in the box coordinates such as pose, lighting, texture, color etc.",
            "29": "Some of these variables could be modelled explicitely such as 3D or 2D pose.",
            "30": "Why not encoding the object pose in the layout?",
            "31": "The pose of the object could be modelled as a latent variable if not available as annotation and yet influence the optical flow generation.",
            "32": "Regarding the experiments:\nHOI-GAN, Nawhal et al.",
            "33": "2020, has been tested on EPIC-KITCHENS which is a more complex real-world setting wrt to Something-Something and CATER can AG2Vid be applied in such context?",
            "34": "if not why?",
            "35": "A major issue of this work lies in the novelty with respect to HOI-GAN, which is used as a baseline but not discussed in the related work section, plus the comments above make the paper a nice contribution but just marginally above the threshold.",
            "36": "References\n\n[a] PolarMask: Single Shot Instance Segmentation with Polar Representation, 2020\n[b] UnitBox: An Advanced Object Detection Network,2016\n[c]  Am I Done?",
            "37": "Predicting Action Progress in Videos, 2017\n[d] Temporal Cycle-Consistency Learning, 2019"
        },
        "ENi1PfOPg69": {
            "0": "This paper describes a method to generate videos from an initial image and a graph based description of how the scene should evolve.",
            "1": "The description is called \"Action Graphs\" and consists of a nodes representing objects, along with edges that describe actions between objects that should occur over given time intervals.",
            "2": "The method for producing a video is split into 4 parts:\n1) An initial image frame and set of objects with bonding boxes are provided.",
            "3": "2) The action graph is \"unrolled\", and a graph is instantiated for each time step.",
            "4": "The time intervals for actions are interpolated into 0->1 ranges and this is stored in the state of the edges for each time step.",
            "5": "This part is algorithmic with no training.",
            "6": "The task of generating the video is then as follows..\n3) The layout generating Function.",
            "7": "This is a Graph Convolutional network on a graph consisting of the \"unrolled\" action graph at time t (and the graph at t-1?)",
            "8": "and the previous positions of the objects.",
            "9": "It's task is to predict positions of objects and their bounding boxes for the next frame.",
            "10": "4) The Frame generating Function.",
            "11": "This takes the predicted layout, and the pixels from the previously generated frame and generates the next frame pixels.",
            "12": "This uses previous techniques including a \"flow prediction network\".",
            "13": "Reason For Score:\n\nThis is a weak reject.",
            "14": "The action graphs are a succinct way of declaring an evolving scene with multiple objects.",
            "15": "And the graph network used to interpolate object positions across a sequence of unrolled AGs is good.",
            "16": "My main concern is for the utility of the methods outside the synthetic CATER dataset.",
            "17": "And the contribution of the Action Graph to the \"something something\" dataset results is not clear.",
            "18": "Pros\n\nThe videos produced for the CATER dataset are nice, with objects in general retaining their boundary shapes and color/texture characteristics as they move.",
            "19": "The movements themselves appear accurate and are well timed.",
            "20": "Most impressive are the compositional actions \"eg.",
            "21": "huddle and swap\" in Fig6 which show ability to produce videos of unseen group actions (that are compositions of previously seen individual actions).",
            "22": "The work is quite thorough.",
            "23": "They carry out visual quality assessments and ablation experiments.",
            "24": "It is possible that others in the community can rally around and develop the action graph description.",
            "25": "Scenario/Storyboard descriptions of evolving scenes are necessary for work on scene understanding, and there aren't any clear candidates to rally around.",
            "26": "Cons\n\nWhat is the actual definition of the Action Graphs used?",
            "27": "For CATER it is stated that the target positions are provided for some objects, and elsewhere that an angle can be included for rotate.",
            "28": "Can we see the \"formal\" descriptions of nodes, edge types and edge attributes used?",
            "29": "Is it possible to have more than one action per object per time step?",
            "30": "This seems possible in the novel \"something something\" examples, but I can't see how this would be implemented in the Action Graph for one object without 2 edges to the same object.",
            "31": "In the something-something dataset, are all the actions in this dataset single actions for one object with an edge from the single node to itself?",
            "32": "If so, what is the contribution of the action graph?",
            "33": "What are the object(s) in \"Putting (something) on a surface\"?",
            "34": "The methods employed: graph neural network and optical flow based video generation not all that original, the contributions are in the Action Graph and the combination and application of known techniques (albeit seemingly well chosen for the CATER dataset).",
            "35": "The utility of Action Graphs is not clear beyond synthetic data, particularly the CATER dataset which has discrete action/movement time segments with well defined actions to occur within the segments.",
            "36": "In the case of this paper, all the object identities and actions over discrete time steps are given directly to the graph model to do \"scene graph interpolation\".",
            "37": "Another application would need a similar ground truth dataset for training, and it is not clear one can be made from real data."
        },
        "e3JmR7Z3hLD": {
            "0": "Overview:\n\nThe paper proposes a hierarchical approach to video synthesis based on Action Graph.",
            "1": "Action Graph is a graph representation to describe the dynamics of individual objects.",
            "2": "Based on this, the authors proposes an action scheduling mechanism to track the progress of action and then generate the scene layout at each timestamp.",
            "3": "Finally, the pixels are generated based on the predicted scene layout.",
            "4": "Experiments show that such AG2Vid paradigm can generate images on CATER and Something-Something dataset with a better quality compared to the baselines.",
            "5": "It can also generate novel actions, treated by a composition of seen actions.",
            "6": "Strengths:\n\n++ The idea of three-level abstraction (action schedule + scene layout generation + pixel generation) is sound.",
            "7": "The formulation based on the idea is neat and straightforward.",
            "8": "++ The experiments on generating multiple actions, single action, as well as novel actions indicate that the method is capable of disentangling and composing atomic action for individual object.",
            "9": "Weaknesses:\n\n-- In the stage of pixel generation, it seems that the frame generation function cannot handle the overlap case very well.",
            "10": "For example, in the \"pick place\" video in Figure 4, when the green cone is picked up and moved towards, it should have occluded the yellow cone at certain steps.",
            "11": "However, it is occluded **by** the yellow one instead in the generated video.",
            "12": "But since the masks \n$ M_{t-1}, M_t $ are given individually, I personally think it should not be the case: since there is only one object moving (i.e.",
            "13": "green cone), you can warp that specific mask and simply stick that on top of the original frame.",
            "14": "Can the authors explain why does it fail?",
            "15": "-- The results on Something-Something indicate that flow-warping method might not be a good way to preserve the structure of the object/hand.",
            "16": "I think the authors could spend some space on this limitation and discuss possible ways for improvement."
        }
    },
    "cotg54BSX8": {
        "Es9JrCKoPTw": {
            "0": "Summary: This paper is an interesting study of algebraic model extraction attacks on modern NLP models based on BERT.",
            "1": "Model extraction is the setting where a malicious attacker tries to reconstruct a copy of a black-box inference API without access to the original training data.",
            "2": "Prior work [1] showed these attacks are possible on BERT models using a distillation-like learning method, using gibberish sequences of words as queries to the API.",
            "3": "However, these attacks needed large number of queries for success.",
            "4": "This work adopts a different strategy --- equation solving the parameters of the neural network using least square linear algebra methods.",
            "5": "This not only allows extraction with lesser queries, but also ensures greater similarity between the API and extracted model (\"high fidelity\", [2]).",
            "6": "The attacks in this paper work perfectly in settings where BERT is frozen and a single classification layer is fine-tuned.",
            "7": "However, the attacks are not as effective in the more practical setting where BERT is fine-tuned, and the authors perform a thorough analysis varying critical hyperparameters.",
            "8": "-----------------------------------\n\nStrengths of the Paper:\n\n1.",
            "9": "This is a new attack setup (especially in the BERT fine-tuning setup), algebraic attacks have only been attempted on very shallow neural networks with ReLU activations.",
            "10": "Algebraic attacks have several advantages for the attacker like high fidelity and small query budgets.",
            "11": "In the frozen BERT, single layer setting this works perfectly with a very small query budget (however, see my Weakness #1).",
            "12": "2.",
            "13": "The paper is well written and easy to understand, and authors do a very good analysis of their attacks varying important hyperparameters like learning rate, number of queries, type of queries.",
            "14": "-----------------------------------\n\nWeaknesses of the Paper:\n\n1.",
            "15": "I don't think the setting where the attack works perfectly (frozen BERT with a single classification layer) is practical.",
            "16": "Theoretically it's fairly obvious this should work, and I think the main contribution here is a empirical confirmation that it works with real data.",
            "17": "There are a number of reasons why this is not practical --- (1) there are actually 2 layers between the sequence_output and the final logits, with a tanh activation (see https://github.com/google-research/bert/blob/master/modeling.py#L219-L232), or look for `BertPooler` in the HuggingFace code.",
            "18": "These two layers are needed to separate the MLM representation from the logits.",
            "19": "Even in the frozen setting, I anticipate fine-tuning atleast these two classification layers; (2) target accuracies are quite poor without fine-tuning.",
            "20": "75% on SST2 (Target Acc from Table 1) is quite poor, even a 1-layer CNN does much better and gets 83-88% accuracy (https://arxiv.org/abs/1408.5882).",
            "21": "Similarly, the Target Acc.",
            "22": "for MNLI is close to 33%.",
            "23": "Without fine-tuning and just a single classifier layer, I don't expect people to use BERT; (3) Finally access to probability distributions / logits might be a strong assumption in structured prediction NLP tasks like question answering or NER.",
            "24": "2.",
            "25": "In the more practical setting of finetuning the model, the attacks are not effective.",
            "26": "While I like the overall idea of leveraging the BERT pretrained checkpoint to do algebraic attacks, the authors' results show that this by itself is not sufficient to make an effective attack.",
            "27": "The authors statement \"For the fine-tuned models, agreement drops to 10% below the learning-based state-of-the-art\" is not entirely correct.",
            "28": "It is only true on the simpler SST-2 task, where even 1-layer CNNs perform exceedingly well.",
            "29": "In the harder MNLI task, agreement is far lower than state-of-the-art [1], with a gap of 44% vs 82.2%.",
            "30": "Performance of the extracted models on MNLI are quite low, about 40-45% in Figure 1 which is quite close to random guessing (BERT-base gets 84-85% accuracy).",
            "31": "-----------------------------------\n\nOverall Recommendation:\n\nThe authors did a good job with presentation and studied an interesting algebraic attack.",
            "32": "However, the attack only works in an impractical setting of a frozen BERT, and is ineffective in the more practical setting of finetuning BERT.",
            "33": "Intuitively, it's fairly obvious this attack should work in the frozen BERT, single layer setting.",
            "34": "This result by itself is not sufficient for acceptance to ICLR.",
            "35": "While I'm leaning reject, I encourage the authors to explore the BERT fine-tuning setting more.",
            "36": "For instance, can a hybrid attack be constructed which uses the best of both worlds?",
            "37": "Since queries do not seem to cost much [1], can the attacks be stronger in this hybrid setting with more liberal query budgets?",
            "38": "-----------------------------------\n\nMinor Issues:\n\nIn Proposition 1, uniformly sampling from a n-d cube is not entirely correct.",
            "39": "BERT has a fixed discrete input space, since you only feed text as input to BERT.",
            "40": "You are going to have a maximum of V^L unique points in the support of the [CLS] vector space (where V is the vocab and L is the maximum sequence length).",
            "41": "Since V ~ 30k and L = 512, I guess it's not a problem practically.",
            "42": "392.702 ---> 392,702\n\n-----------------------------------\n\nReferences:\n\n[1] - https://arxiv.org/abs/1910.12366  \n[2] - https://arxiv.org/abs/1909.01838"
        },
        "Tl4eK89Ywur": {
            "0": "##########################################################################\nSummary:\n\nThe paper considers the reconstruction of the last layer for NLP data processing models.",
            "1": "This problem is equivalent to the parameter estimation for logistic regression in the first of the paper and quite close to it in the second part when we purposely change the encoder via transfer learning.",
            "2": "No surprise, that the reconstruction in this setting works well.",
            "3": "This is what we already know from linear algebra and Gauss-Markov [1], Bernstein-von-Mises like theorems in statistics [2, chapter 10].",
            "4": "More interesting is the part about what is happening, when we deal with reconstruction under a transfer learning setting.",
            "5": "In this case, we observe a predictable degradation of the quality of the models, but nothing more specific\n\n##########################################################################\nReasons for score: \n\nI vote for rejection, as this paper doesn't contribute to our understanding of what is happening in real-world NLP models with many layers, rather focusing on the last layer fine-tuning.",
            "6": "##########################################################################\nMore detailed review:\n\n################\nTheoretical results\n\nAll proposition in the paper are obvious and also equivalent to the recovery procedure for the coefficients of a multiclass logistic regression:\n1.",
            "7": "Proposition 1 is obvious\n2.",
            "8": "Proposition 2 is obvious\n3.",
            "9": "Proposition 3 is obvious\n4.",
            "10": "Proposition 3 is obvious\n\nThe general statement that concludes this section and leads to further experiments should be compared to theoretical results for softmax (or multinominal) regression, see e.g.",
            "11": "[3] for some details on the quality of the estimates in this setting.",
            "12": "Also, see similar results for logistic regression in [4].",
            "13": "Both these papers present result on the quality of parameters' estimates in a more advanced subsampling setting, and even in this case, they provide the speed of converges for the error of parameter estimates.",
            "14": "So for the benefit of the quality of the paper, I suggest dropping all theoretical results as they are not new.",
            "15": "################\nPractical results\n\n1.",
            "16": "Due to the reasons similar to that mentioned above the experiments for $\\eta = 0$ can be dropped to avoid confusion from the reader\n2.",
            "17": "For the setting with the fine-tuning of the models, we can see from experiments that after learning emerges a disagreement between the parameters estimates via the proposed procedure and the initial values of parameters.",
            "18": "In particular, how can we measure the distance between two models even if they are one-layer logistic regression models, and can we do something if there is one layer in a setting closer to the white box problem.",
            "19": "[1] Henderson, C. R. (1975).",
            "20": "Best linear unbiased estimation and prediction under a selection model.",
            "21": "Biometrics, 423-447.",
            "22": "[2] Van der Vaart, A. W. (2000).",
            "23": "Asymptotic statistics (Vol.",
            "24": "3).",
            "25": "Cambridge university press.",
            "26": "[3] Yao, Y., & Wang, H. (2019).",
            "27": "Optimal subsampling for softmax regression.",
            "28": "Statistical Papers, 60(2), 235-249.",
            "29": "[4] Wang H, Zhu R, Ma P (2018b) Optimal subsampling for large sample logistic regression.",
            "30": "J Am Stat Assoc 113(522):829–844"
        },
        "CZMk-m1WvFw": {
            "0": "The paper proposes an algebraic attack for extracting the parameters\nof a semi-private language model that consists of a pre-trained\nencoder and a privately trained classification layer.",
            "1": "The method is to first sample from the input space, compute their\nembeddings using the known encoder, and then use the embeddings and\nthe queried classifier softmax output to solve for the classifier weights.",
            "2": "It overcomes the obstacles encountered by former such attempts due to \nthe requirements of known embeddings and raw logits.",
            "3": "The paper provides support for the method in arguing that a random\nbasis (like an arbitrary set of embeddings obtained from encoding a\nset of arbitrary, distinct input) is sufficient to serve as a basis\nthat spans the classifier layer's input space, and that using the\nsoftmax output instead of raw logits can lead to equivalent solutions\nup to a translation invariance.",
            "4": "Experiments on two public datasets and two versions of the BERT model\nshow the effectiveness of the method, and demonstrate that\nthe number of queries needed is relatively small,\nthe probes can be drawn from the distribution of legitimate input,\nand that fine tuning the encoder makes the attack less effective as the true\nembeddings deviate from those computed from the publicly known\nencoder.",
            "5": "The paper is well written and the method is sound and practical.",
            "6": "Suggestions on defenses against such attacks are of good reference\nvalue.",
            "7": "One question is whether the proposed approach could be put to some\npositive use, such as learning about a model's potential weakness in\nthe input space?"
        },
        "DUpl0854on-": {
            "0": "Summary:\n\nThis paper proposes a range of algebraic model extraction attacks (different from the prevalent learning-based approaches) for transformer models trained for NLP tasks in a grey-box setting i.e., an existing, public, usually pretrained encoder, with a private classification layer.",
            "1": "Through attacks on different sizes of models and a range of downstream tasks, they observe that only a portion of the embedding space forms a basis of the tuned classification layer’s input space, and using a grey-box method, this can be algebraically computed.",
            "2": "The pretraining-finetuning experiments on different tasks also show the smallest number of dimensions needed for high-fidelity extraction, and also that the model extraction attacks effectiveness decreases with fine-tuning the larger models base layers---which is an insight that is very useful for a lot of interpretability/probing work.",
            "3": "Reason for score:\n\nI think this paper is very well-formulated---both theoretically and empirically with promising results that will be useful not just for grey-box adversarial attacks, but also for works interesting in the effects of pretraining-finetuning (which at this point encompasses nearly all NLP tasks).",
            "4": "The empirical results look promising---however I would like to see this demonstrated on more than just 2 datasets (and maybe even a GPT-like model, instead of just BERT) to see if (1) the results hold empirically and (2) if there any insights to be gleaned about adversarial attacks from different task structures and model types.",
            "5": "Positive points + questions:\n\n1.",
            "6": "The transformation of the raw logits for recovering information is really interesting.",
            "7": "In the experiments for the random set of n embeddings chosen to form a basis of the last layer’s input space---are there any insights on what those embeddings amount to semantically; and also what a ground truth selection of embeddings (e.g., that an oracle adversary would compute) should be?",
            "8": "It would be helpful to have a discussion and examples of those.",
            "9": "2.",
            "10": "Is there a difference in extraction results when using in-distribution queries vs. random?",
            "11": "Most of the results say “extraction is possible with both” which is good to see, but a more finer-grained analysis/explanation of benefits/pitfalls of each would really help clarity.",
            "12": "3.",
            "13": "It’s nice that both a single-sentence and pairwise-sentence (SST-2 vs. MNLI) task are used to evaluate effects for the fine-tuning experiments in big transformer models.",
            "14": "4.",
            "15": "The results look very promising and these insights are extremely helpful even for general probing/interpretability works (especially the learning rate finetuning effects) and also hold up to existing BERT-finetuned results.",
            "16": "5.",
            "17": "Unlike previous work, this algebraic model extraction words even with non-linear activation layers---and this is helpful given the current standard of fine-tuning large transformer models e.g., with simple MLP/softmax classifiers.",
            "18": "6.",
            "19": "Slightly different from previous work, not only can this work when attacks require embeddings to be chosen, but also when selecting (e.g., random/or from a distribution) needs to be done as well.",
            "20": "Negative points + questions:\n\n1.",
            "21": "For the fine-tuning/learning rate experiments it would be good to evaluate this on more than just 2 tasks (e.g., maybe a range of different tasks in GLUE) not only to see if the trend still holds, but also to see if task “type” or characteristics of the task/fine-tuning affect the extraction fidelity.",
            "22": "2.",
            "23": "The *extracted model accuracy of BERT-base with MNLI seems to be quite static (almost no effect on increasing or decreasing learning rate)---and it would be really helpful to see how statistically significant those results are and what they look like over different seeds.",
            "24": "3.",
            "25": "Is there a comparison between the algebraic approach and a learning-based approach for the same tasks?",
            "26": "(I think the paper is novel and useful enough in itself, but it would be helpful to see a side-by-side comparison).",
            "27": "4.",
            "28": "Is there a comparison between extracting only a single layer or going beyond to having multiple layers of target/finetuned classifiers?",
            "29": "Is this approach feasible and similarly beneficial as a grey-box attack in that scenario?",
            "30": "It would be really helpful to have a discussion on what that would require for future work.",
            "31": "Additional minor comments:\n\nThis is really well written and placed in literature, no minor nitpicks re: writing!"
        }
    },
    "taQNxF9Sj6": {
        "qQNEkzJ2OvR": {
            "0": "The goal of this work is to enable existing pre-trained transformers (e.g.",
            "1": "GPT-2) to operate over long input contexts.",
            "2": "This is achieved by breaking the input sequence into segments and processing each segment through the transformers while allowing tokens in the current segment to attend over a summary vector of the tokens in the previous segment.",
            "3": "The summary vector is created as a weighted combination of the tokens in the summarized segment.",
            "4": "Thus the summary vector introduces recurrence where each segment can use information from the previous segment.",
            "5": "These modifications yield a better language model for long input texts.",
            "6": "The main benefits of this approach are as follows: (1) The modifications yield a better language model for long input texts, especially when compared to a tiling based approach (2) Potential for reducing memory footprint in these models by shrinking the amount of text that is to be processed in one-go.",
            "7": "My main concern with the paper is that unfortunately the evaluation is only limited to perplexity numbers on a couple of datasets.",
            "8": "While this is a useful metric for evaluation, this alone is inadequate to demonstrate the quality of the model as a text generating system or as a language model that will be fine-tuned for target tasks or to understand how much impact the model will have in these applications.",
            "9": "-- For the model to be considered as a text generation system, there needs to be some human evaluation of the generated outputs.",
            "10": "There are a small number of examples in the paper but that is not enough for a quantitative assessment.",
            "11": "To clearly establish the benefits of the proposed modification it would be even better to consider generation tasks where conditioning on long inputs is essential.",
            "12": "-- How will model fare when used in a target task defined over long input contexts?",
            "13": "The related work section includes some papers that evaluate on such tasks.",
            "14": "For example on target task could be HotpotQA, which requires QA over ten paragraphs which easily exceed the 512 token limits.",
            "15": "It is important to know how the proposed recurrence model compares to tiling GPT (disjoint version) or other simpler approaches on these long input tasks.",
            "16": "-- Another key strength of the model is that it potentially allows for processing the input in smaller segments.",
            "17": "While perplexity gains are helpful, here again there is a missed opportunity in terms of human evaluation of the generated outputs over shorter segments, and the impact of these choices in different applications.",
            "18": "To reiterate, this paper presents a very nice idea to a well-motivated problem.",
            "19": "The executed experiments show that this idea is likely to work but the gaps in experimentation leave much room for speculation about the potential impact of this approach in end applications."
        },
        "9bPwIXKKFYE": {
            "0": "Summary:\n\nThe paper proposed to add a recurrent component to pretrained transformers.",
            "1": "The component pools the hidden states of a context window and passes it to the next context window as an additional input to the self-attention layer.",
            "2": "The component reduces the memory usage at both training and inference time, and enables the Transformer model to work on a longer sequence.",
            "3": "The component is evaluated on two language modeling datasets and outperforms baseline models.",
            "4": "Reasons for score: \n\nI vote for accepting the paper.",
            "5": "The paper proposed a nice and simple way to make use of the existing pretrained Transformers with reduced memory usage and extended sequence length.",
            "6": "This should benefit practitioners who want to apply these language models on a more diverse set of downstream tasks where the sequence length doesn’t fit the one from the original pretrained model.",
            "7": "The results presented in the paper are significant.",
            "8": "The paper is well-written and easy to follow.",
            "9": "Comments:\n\n1.",
            "10": "It would be better to include the failure results stated in the end of section 3.1.",
            "11": "I’m surprised that a key-value pair can boost the performance that much.",
            "12": "2.",
            "13": "The paper should add more content on differentiating with Transformer-XL.",
            "14": "I believe the difference is more than relative embeddings.",
            "15": "For example, each Transformer-XL layer attends to an earlier layer of previous timestep, this convolutional operation making the structure no longer “recurrent”.",
            "16": "Typos: \n- Third line in section 3.1: “at position t” -> “at position i”"
        },
        "Nd8-ViHCEzW": {
            "0": "The paper proposes recurrent connections between two adjacent Transformers, which transfers the previous context to the next step.",
            "1": "This is a practically useful technique, improving the performance (perplexity in the experiments), and worth publishing.",
            "2": "However, I have some comments and questions about the article.",
            "3": "Section 5.3 is an interesting question.",
            "4": "The authors argue that topical information or so between adjacent windows is propagated.",
            "5": "Although it is a plausible argument, it seems like it is hardly supported by table 3.",
            "6": "The authors said more complex recurrence modules do not make any significant difference.",
            "7": "Then, the authors need to explain why the variations do not matter.",
            "8": "For example, the authors fixed l_ins to be 2, without an explanation.",
            "9": "It is interesting to see the relationship between the overlap length and improvement using the recurrent connection.",
            "10": "It would be better to have further discussion about the relation and different roles.",
            "11": "The Transformer model is also fine-tuned with the recurrent connection.",
            "12": "So, I was wondering if the fine-tuning improves the Transformer model too.",
            "13": "It would be interesting to compare the updated Transformer to the previous one.",
            "14": "In Eq.",
            "15": "(1), is there 1/T?"
        }
    },
    "7_G8JySGecm": {
        "8gOMLCJbqFm": {
            "0": "This paper presents a method for combining planning an learning in text-based games.",
            "1": "In particular it augments Monte-Carlo Tree Search to include a language-similarity bonus to encourage exploration of similar actions.",
            "2": "This bonus works by computing a Language Action Value Estimate - which is based on increasing the score of an action by an amount corresponding to the Q-Values of similar actions the agent has experienced.",
            "3": "Similarity here is defined by cosine distance in action-embedding space.",
            "4": "Using this augmented MCTS algorithm the authors introduce their MC-LAVE agent which alternates between MCTS planning and policy training via supervised learning from the planned trajectories.",
            "5": "Experiments across nine IF games show consistent improvement relative to prior RL and planning-based agents.",
            "6": "Additional analysis is performed to show how MC-LAVE uses the language action value estimates to learn how to overcome a notable bottleneck in the game of Zork.",
            "7": "I found this paper is well-motivated and principled in its application of Monte-Carlo methods to text-based games.",
            "8": "The results show clear improvement over previous agents on a large majority of games and the utility of the Language Value Estimator is clearly shown by the comparison against PUCT-RL.",
            "9": "I have read the author rebuttal and appreciate the changes made to clarify the distinctions and handicaps used by each of the algorithms.",
            "10": "Additionally the exploration into ranges of the delta parameter was appreciated.",
            "11": "My largest issue with the paper is that it does not make a clear distinction between pure-RL methods (DRRN/TDQN/KG-A2C) and methods that leverage additional functionality/handicaps to make planning possible (MC!Q*Bert/PUCT-RL/MC-LAVE-RL).",
            "12": "Reinforcement learning and planning are different paradigms and even though MC-LAVE-RL does both planning and policy learning - its use of the MCTS planner makes it not an apples-to-apples comparison with the pure RL methods.",
            "13": "To this end, I'd strongly encourage the authors to include a discussion of which Jericho handicaps are used by MC-LAVE and make a clear distinction in the presented results between the planning and the pure-RL agents.",
            "14": "The delta hyperparameter for neighborhood size is not given in Table 4 - it would be good to understand what value was used in practice and how large was the effective neighborhood.",
            "15": "For example, it would be interesting to understand the effective edit distance in terms of the number of words that could be substituted while remaining in the same neighborhood - e.g.",
            "16": "are \"take lantern\" and \"take egg\" in the same neighborhood?",
            "17": "Taking this idea further - I would be interested to understand how algorithmic performance changes as a function of the neighborhood size - I imagine there may be a sweet spot in terms of this hyperparameter."
        },
        "tZAqxg5rBSJ": {
            "0": "This paper introduces Monte-Carlo planning with language action value estimates to guide exploration.",
            "1": "The method builds on top of MCTS w/ PUCT, where a policy distribution over actions is introduced to estimate Q for actions not seen during sampling.",
            "2": "The modification proposed here is an additional term to the Q estimate, which is a weighted average of Q values of similar actions, where similarity is computed using word embeddings of words that make up an action.",
            "3": "The authors show gains over MCTS  w/ PUCT on 8/9 real text games.",
            "4": "The modification is small, but it is intuitive and shows consistent gains over the baseline without this modification.",
            "5": "Some concerns I have are:\n\n1. what is the variance for the experiments in Table 1?",
            "6": "2. sample size of 3 is very small, can you increase this and report the mean/variance?",
            "7": "3. it seems like a single bottle-neck state is crucial for achieving good Zork1 performance.",
            "8": "Is this the case with the other two \"difficult games\" DeepHome and Ludicorp?",
            "9": "Do the authors have qualitative observations as to why this method helps on these two games?",
            "10": "4. this paper is missing what I think is a very relevant citation in Branavan 2012 (https://arxiv.org/abs/1401.5390), which uses language from a game manual to guide MCTS."
        },
        "zUmR2mUr-3": {
            "0": "Summary: This paper presents a method of incorporating prior knowledge into MCTS via language, using interactive fiction games as a test bed.",
            "1": "Their method MC-LAVE used word embeddings on the language action space to help induce a non-uniform distribution over the action space.",
            "2": "Pros:\n1.",
            "3": "The motivation is very clear, MCTS is generally action agnostic and using language to provide additional semantic information to it can prove to be very effective.",
            "4": "The search + RL paradigm has already been shown to work well in cases like Go.",
            "5": "The idea of using cosine similarity in word embeddings is a simple but effective way of biasing the MCTS in the right directions.",
            "6": "2.",
            "7": "The paper in general is well-written and easy to follow, the qualitative analysis and the additional diagrams in the appendix illustrating the variations in policies are appreciated.",
            "8": "Cons:\nSome of the claims are not quite accurate even when compared to the works already cited here -\n1.",
            "9": "PUCT-RL is the only directly comparable baseline given action space and other handicap differences.",
            "10": "(i) It appears that MC-LAVE is using the valid action handicap in Jericho as a *hard constraint* (Eq.",
            "11": "6 and Algorithm 1) - this means that the MC-LAVE only has a search space of on average < 100 actions per step.",
            "12": "The other baselines all use the full template-based action space (except the DRRN) of size 10^8 - a auxiliary entropy loss is used there derived from the valid actions but it is not a hard constraint.",
            "13": "As noted in contemporary works such assumptions dramatically reduce the difficulty and language understanding capabilities of text games (Yao et al.",
            "14": "2020 https://arxiv.org/abs/2010.02903).",
            "15": "(ii) The second issue is that MC-LAVE assumes that the simulator is deterministic and can conduct rollouts and reset within the span of an episode - standard planning assumptions but incompatible with all other baselines (except for MC!Q\\*BERT) which do not use this handicap.",
            "16": "(iii) Some suggested baselines that make these assumptions would be a heuristic A\\* search, or modifying any of the existing algorithms to use smaller action spaces and/or apply alternative exploration strategies seen in previous works such as modular policy chaining (that MC!Q*BERT uses) or Go-Explore (Madotto et al.",
            "17": "https://arxiv.org/abs/2001.08868)\n2.",
            "18": "Even with the results that already exist, it is claimed (for example in section 5.1) that MC-LAVE-RL is the only algorithm to pass bottlenecks such as getting the action \"take lantern\" right.",
            "19": "But the diagrams in the appendix for the policy the MC!Q*BERT agent learns as well as the original paper for that agent show otherwise?",
            "20": "3.",
            "21": "Other concerns along these lines: all of this paper's results are averaged over 3 runs while the other baselines are over 5 runs - an indication of variance would be useful to assess whether the differences are significant, especially since some of the margins quite small (23 on MC-LAVE-RL vs 22.8 for the next best on Ludicorp) added to the fact that hyperparameters are different for each game - does that imply that the authors tuned the hyperparameters for each game?",
            "22": "The analysis shows that it outperforms only all the other baselines only on 5/9 games and matches on 3.",
            "23": "The abstract and intro claim state of the art across all games.",
            "24": "I think this means that some amount of claim rewriting is required in addition to the changed baselines.",
            "25": "Overall, I think this paper has a clear motivation and some interesting ideas on how to incorporate semantic language information into planning algorithms.",
            "26": "However, in its current state - the comparisons made are not meaningful which makes the claim of state of the art tenuous (state of the art does not matter so much as showing that you make progress in line with the motivation).",
            "27": "Making these comparisons would require a heavy rewrite starting from the abstract to the analysis and so I would recommend reject right now but look forward to seeing an updated version of the paper in the future with some of these changes.",
            "28": "After author response:\nSee reply comment in the thread below for further score justification."
        },
        "LDtsxV76aoa": {
            "0": "This paper presents a novel MCTS-based policy improvement operator called MC-LAVE designed specifically for environments with text-based action spaces.",
            "1": "MC-LAVE adds an additional term to PUCT that shares information across semantically similar actions.",
            "2": "This additional term for a given action $a$ is set to the soft maximum over the Q function evaluated at all $(o, \\bar{a})$ pairs of transitions in the replay buffers whose transition action $\\bar{a}$ is within some cosine distance $\\delta$ of the action $a$.",
            "3": "As shown in Appendix C, addition of this term preserves the expected regret bounds of PUCB.",
            "4": "The paper reports empirical improvements over existing methods on various interactive fiction (IF) games in the Jericho suite.",
            "5": "The results of MC-LAVE are promising, however, the paper leaves me with a few lingering questions that I feel should be resolved before I would feel this paper were ready for conference publication:\n\n1.",
            "6": "It is not clear to me that the improvements are, as repeatedly emphasized by the authors throughout the paper, due specifically to MC-LAVE leveraging semantic similarity of useful actions to focus on the most promising actions: It is possible that the additional term is simply injecting additional randomness to the action selection rule, which benefits exploration.",
            "7": "To control for this possibility, the authors should include a baseline that performs the action selection rule of MC-LAVE with the semantic neighborhood N(a) of action $a$ set to a uniformly randomly sampled set of valid actions in each corresponding state of the replay buffer.",
            "8": "2.",
            "9": "Related to the above point, the hyperparameter $\\delta$ seems to be quite important, as it effectively defines the size of the neighborhood.",
            "10": "However, neither the exact setting of $\\delta$ used in the experiments nor the effect of varying $\\delta$ is presented in the paper.",
            "11": "Relatedly, another important baseline similar to the random neighborhood baseline mentioned above is setting $\\delta$ to a very large number.",
            "12": "3.",
            "13": "I am not entirely convinced that the action-spaces of the IF games present a suitable testbed for the semantics-based information sharing that MC-LAVE attempts to achieve, as the action space vocabulary for valid actions in each state seems quite limited and tending to repeat the same key verbs, based on the presented examples in Table 3.",
            "14": "Perhaps some comment about the diversity of these games could be made to defend against this criticism, or perhaps the action space of the games could be edited to exhibit a more diverse vocabulary to allow better empirical demonstration of the effectiveness of MC-LAVE in performing semantics-based info sharing across related actions.",
            "15": "4.",
            "16": "How important are the embeddings?",
            "17": "Based on the presented examples, it is not clear to me how the semantics-based info sharing defined in the MC-LAVE action selection rule should benefit the agent in choosing \"take lantern\" over \"open trap door\" in a meaningful way beyond just recognizing that \"take\" is often useful in other, albeit, completely different contexts.",
            "18": "In that case, perhaps word embeddings are not even necessary, but rather maintaining a table of Q-values for $(o, a_i)$ for each action word token $a_i$ and taking the soft maximum over the corresponding Q-values of action tokens might suffice.",
            "19": "5.",
            "20": "I would love to see some examples of how the language-driven exploration term in the MC-LAVE action-selection rule weighs the various action tokens in a fully-trained policy, and which words from other states in the replay buffer the words are being linked to.",
            "21": "I think such a qualitative analysis would be useful in better understanding how the \"language-driven exploration\" term enables the agent to attain the empirical gains reported.",
            "22": "6.",
            "23": "It's not clear how MC-LAVE, even if effectively sharing info between semantically-related words at each node of the MCTS tree, is improving \"non-myopic exploration\" over alternative methods, which point the authors seem to emphasize throughout the preliminary sections of the paper.",
            "24": "7.",
            "25": "Some discussion around why the authors think MC-LAVE underperforms w.r.t.",
            "26": "existing methods on ZTUU would improve the discussion of the experimental results."
        }
    },
    "9y4qOAIfA9r": {
        "XGYA484ZAxt": {
            "0": "Summary of paper: the authors explore adding a soft structural attention constraint to BERT, by penalizing attention weights that are substantially different from a head–dependent \"adjacency\" matrix derived from dependency parses.",
            "1": "BERT is then fine-tuned with and without (\"domain-finetuned\") this constraint on corpus data for which fMRI recordings from participants during reading are available.",
            "2": "A linear classifier from the final layer of BERT's embedding (mean-pooled) is then learned to the fMRI data.",
            "3": "Within this pipeline, domain-finetuned models are not an improvement over unfinetuned BERT, but fine-tuning with the structural attention constraint improves decoding to fMRI data, especially for word-level data (the Wehbe2014 dataset).",
            "4": "Assessment: this is a nice paper that investigates an intuitive method of incorporating syntax-based, structural soft attention constraints into Transformer encoder models for language.",
            "5": "What makes the contribution fairly distinctive is evaluation on alignment with human fMRI recordings during comprehension of the texts.",
            "6": "The results show improvements in decoding relative to baseline models that involve no fine-tuning and/or domain-adaptation fine-tuning alone (no structural attention constraints), especially for fMRI data that are recorded below the sentence level.",
            "7": "The authors also evaluate the effect of fine-tuning on targeted syntactic evaluations from Marvin & Linzen; the results here are not particularly conclusive.",
            "8": "Overall, this is a potentially solid, if not ground-breaking, contribution.",
            "9": "However, there are a number of technical questions that are left unclear in the submission, and some of the results are cause for some concern.",
            "10": "These concerns need to be addresed in order for the submission to be fully satisfactory.",
            "11": "The single biggest concern is the extraordinarily high word perplexity scores in Table 2 for Wehbe2014 -- which get much, much worse after fine-tuning.",
            "12": "It is important to understand what's going on here in order to make sense of the core potential contribution of the paper, because it's only in the Wehbe2014 dataset where there seem to be appreciable improvements in decoding performance.",
            "13": "I would guess that the high perplexity comes from poor prediction of the proper nouns in the Harry Potter book chapter.",
            "14": "Maybe there needs to be some amount of fine-tuning of the models to the domain of the test-set corpus.",
            "15": "Overall, the paper needs more clarity on why it is only the Wehbe2014 dataset where the perplexity is so high and the fine tuning affects decoding performance so much.",
            "16": "Additional technical questions:\n\n1) How is the split of a word into word pieces handled in the adjacency matrix representing word–word dependencies?",
            "17": "2) How are the adjacency matrix and each head's attention weight matrix converted into a distribution for computing cross-entropy loss?",
            "18": "Are the entries normalized globally?",
            "19": "By row?",
            "20": "By column?",
            "21": "3) What are the perplexities like for domain-finetuned (no structural attention constraint) BERT?",
            "22": "These are missing from Table 2 (Appendix B), but are potentially important in interpreting your results.",
            "23": "4) What words are pooled over for the Wehbe2014 analyses -- the four words in the 2-second window?",
            "24": "5) Section 4.1 reports that UD and DM finetuned models are significantly better in brain decoding than the un-finetuned baseline, at p<0.0001, but the 95% confidence intervals for subject scores look very different.",
            "25": "And the difference in mean decoding performance for DM finetuning is barely visible.",
            "26": "How are you computing your confidence intervals and your p-values?",
            "27": "Why are they so different, and how are you getting such high confidence in improvements over the unfinetuned baseline here?",
            "28": "6) How do your results compare to those using the best fine-tuning methods from Gauthier & Levy (2019), which involve scrambling the input sentences?",
            "29": "7) Given that in Wehbe2014 each fMRI image corresponds to four words, most of which probably contain both function and content words, how is the content/function word analysis defined and performed?",
            "30": "Additional comments:\n\n* the authors write that \"increase in perplexity roughly corresponds to lower brain decoding scores\", but this doesn't look consistent with Table 2 and Figure 3.",
            "31": "For Wehbe2014, UCCA data yield the worst decoding accuracy but yield better perplexity than DM data, which yield decoding accuracy only slightly worse than the UD data.",
            "32": "The monotonicity is cleaner for Pereira2018 but the overall differences in decoding performance are much smaller."
        },
        "IKPgvd-G2u": {
            "0": "This paper tests whether fine-tuning large pre-trained language models\n(LMs) with structural information can increase the correlation between\nthese representations and the representations of brain activity\nmeasured while processing the same stimuli.",
            "1": "The injection of the\nstructural information is done through fine-tuning of the pre-trained\nmodel by \"guided attention\", which makes use of binary relations\nbetween the words according to three different syntactic or semantic\nformalisms.",
            "2": "The authors map the brain activity to each of the\nalternative LM representations via a regression model, and measure the\nalignment by using correlation between the predicted (from brain\nactivity) and actual output of the alternative models.",
            "3": "The results\nshow that under certain conditions representations learned through\nguided attention aligns better with the representations of brain\nactivity.",
            "4": "In general the investigates an interesting question which may be\n(eventually) relevant to both understanding the way humans process\nlanguage, and possibly building better computational models.",
            "5": "The\nmethod followed in the study is (mostly) sound, and the paper is\nwritten well.",
            "6": "A potential issue with the method is the direction of the prediction\nin \"brain decoding\" regression (section 3.5).",
            "7": "Authors predict the\nmodel representations from the \"brain representation\" (this seems to\nbe based on earlier studies, but I did not verify).",
            "8": "In my opinion the\nreverse is more meaningful.",
            "9": "Since the invariant quantity in this study\nis the representations coming from the brain imaging.",
            "10": "This is\nimportant, because the success of the regression is not only about the\namount of information in the predictors, but also simplicity of the\ntask.",
            "11": "Hence, an alteration of the model representations that\nsimplifies them may result in better predictions, and hence, higher\ncorrelations\n\nExcept the above, I have some (mostly minor) comments:\n\n- I would be happier with a bit more explicit discussion of the main\n  results.",
            "12": "After reading the articles, I am still not sure what to\n  take away from the main experiments.",
            "13": "The effect on two different\n  data sets (also means representations at different levels/units) are\n  quite different - not allowing a clear conclusion.",
            "14": "Side issues\n  discussed (the effects of the use of different formalisms, the\n  effect of domain, particular syntactic patterns, content vs.\n  function words are also relatively brief and far from being\n  conclusive).",
            "15": "I think a clearer discussion of the main results, and\n  investigation of reasons for the discrepancy between the data sets\n  would make the paper stronger.",
            "16": "- It would help if the data is explained slightly better.",
            "17": "Particularly, it would make the paper more self contained if the\n  authors specify whether any of the data sets (section 3.3) had\n  automatic annotation.",
            "18": "On a somewhat related note, comparisons\n  between the formalisms seem to correlate with the data sizes, which\n  is not pointed out in the paper.",
            "19": "- A few language/typography issues/suggestions: \n\n    - I am not sure about the ICLR guidelines, but avoiding citations\n      in the abstract is a good idea (abstracts should stand alone).",
            "20": "- Footnote marks should go after punctuation (footnote mark 8)\n    - Conclusions line 3: \"attention guided\" -> \"guided attention\" ?",
            "21": "- There are case (normalization) issues in the references:\n      \"groningen\", \"erp\", \"bert\" (not exhaustive, a through check is\n      recommended)."
        },
        "ErZ-t3zCjqu": {
            "0": "An interesting paper that discusses whether injecting three types of syntactic and semantic formalisms lead to better alignment with how language is processed in the brain.",
            "1": "The authors conduct experiments with the BERT model and two fMRI datasets and show that including linguistic structure through fine-tuning can improve brain decoding performance.",
            "2": "The paper would be improved by experimenting with language models other than BERT, as it is not clear at the moment whether the produced results are generalizable to different language models or are BERT-specific.",
            "3": "For example, additional experiments with AlBert, distilBert and RoBerta would provide additional insights on the effect of size of the model, in terms of the number of parameters.",
            "4": "Comparison of Bert to GPT and XLNet would emphasize the advantages/disadvantages of autoencoder-based vs autoregressive models and could potentially provide additional insight on how attention is represented in human brain.",
            "5": "It would also be interesting to read a discussion of semantic analysis, as currently the paper concentrates the most on syntactic formalism as represented in both BERT and fMRI data.",
            "6": "Specifically, it would be interesting to know if the injection of syntax impacts the semantic representations.",
            "7": "One of the possible methods to measure that would be probing for semantic classes (as in Yaghoobzadeh et al., 2019.",
            "8": "Probing for Semantic Classes)"
        },
        "Dp9vC7JXi1Y": {
            "0": "This paper describes experiments that inject linguistic information (for example dependency structures) into BERT, then measure improvements in correlation with FMRI measurements of humans reading an underlying sentence (which is also analyzed by BERT).",
            "1": "Linguistic information is incorporated by biasing attention heads to line up with dependency (or other) structures.",
            "2": "Positives about the paper: it's an interesting experiment to try, and an important direction of work.",
            "3": "Negatives:\n\n* It's a somewhat small increment over previous work, not sure it merits a full conference paper.",
            "4": "As it stands the paper presents the approach and results, with little inspection of why improvements are seen.",
            "5": "I would like the authors to go much deeper with the analysis.",
            "6": "Are there particular syntactic constructions that are being better modeled?",
            "7": "Is the new model much more sensitive to long range dependencies, as found in syntactic structures?",
            "8": "Are particular classes of words effected more than others?",
            "9": "Answering these questions will be challenging but would add a lot to the paper.",
            "10": "* Most importantly, the evaluation metrics are unclear.",
            "11": "The critical sentence in the paper is \"To evaluate the regression models, Pearson’s correlation coefficient between the predicted and the corresponding heldout true sentence or word representations is computed\".",
            "12": "This is a terse description of a critical part of the approach, and I can't make sense of it.",
            "13": "Part of my unease about the evaluation is the following.",
            "14": "The matrix $D_{fr}$ is the output from BERT.",
            "15": "Importantly in The definition of L_{ifr} this matrix is predicted from the \"brain\" matrix B_i.",
            "16": "If $D_{fr}$ was all zeros it would be trivially predictable (and hence gameable).",
            "17": "In the original Gauthier and Levy paper they appear to use metrics in addition to MSE.",
            "18": "In this paper some variant of Pearson's correlation coefficient is used - but I can't understand what exactly this is, and my worry is that it is trivially gameable in the same way as MSE."
        }
    },
    "punMXQEsPr0": {
        "O5NhgmcwWw9": {
            "0": "Summary:\n\nThe paper provides a novel pretrained language model for document understanding named BROS, which adds spatial layout information and new area-masking strategy.",
            "1": "The authors do some experiments on four public datasets to illustrate the effectiveness of BROS.",
            "2": "The new architecture is well-suited for understanding texts in document, which is valuable.",
            "3": "##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for accepting.",
            "4": "I deem that the pre-trained language model based on BERT that encodes spatial information is useful for 2D document.",
            "5": "Hopefully the authors can address my concern in the rebuttal period (see cons below).",
            "6": "##########################################################################Pros: \n\n \n1.",
            "7": "The paper addresses siome limitations which are very important for document understanding: spatial information, spatial\nrelation, and the information of text blocks.",
            "8": "2.",
            "9": "This paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed model.",
            "10": "The entire structure is organized well and the formulas are very detailed.",
            "11": "##########################################################################\n\nCons: \n\n1.",
            "12": "What are the advantages of BROS in terms of speed and resource consumption?",
            "13": "It would be more convincing if the authors can provide more cases in the rebuttal period.",
            "14": "2.",
            "15": "For the Figure 1(b), it would be better to provide more details about it, which seems not very clear to me.",
            "16": "Like how to\nmask in red area?",
            "17": "##########################################################################\n\nQuestions during rebuttal period: \n\n \nPlease address and clarify the cons above \n\n \n#########################################################################"
        },
        "fm02Jtlc8rU": {
            "0": "### Overall\n\nAuthors used BERT alongside to a 2D-position embedding based on a sinusoidal function and a graph-based decoder to improve performance on document information extraction tasks.",
            "1": "They do pre-train their model (BROS) on a large dataset with 11M documents, and then used such models to perform downstream tasks in four smaller datasets.",
            "2": "Their models achieve better quantitative results when compared to the provided baselines.",
            "3": "### Positive aspects \n\n* Positional encoder based on sinusoidal function seems to be effective.",
            "4": "* Authors perform experimentally sound experiments, following closely LayoutLM.",
            "5": "* Pre-trained models could be useful.",
            "6": "* Authors reproduced results from their strongest baseline.",
            "7": "* Better results in all downstream tasks.",
            "8": "### Cons and aspects to improve\n\nMy main concern is that the overall contribution is seems to be limited.In fact, the original paper of the Transformer approach, already proposed such kind of embedding.",
            "9": "It is good to know that it works for 2D-coordinates for the task at hand, though it seems to be more a marginal improvement on existing work rather than a standalone contribution.",
            "10": "It is hard to tell what are the standalone contributions of the paper, and what is coming from other works.",
            "11": "Authors could have provided more in-depth details (visualizations, analysis, examples) to show main differences between the proposed approach and baselines (specially LayoutLM).",
            "12": "Also they could visually demonstrate the advantages of their approach.",
            "13": "Authors could have plugged their embedding strategy in LayoutLM to understand the impact of that particular component.",
            "14": "I would like to have seen qualitative examples of model predictions, and more examples from the dataset.",
            "15": "A figure containing the whole process could be helpful to better understand the processing required to train / test such models.",
            "16": "Figure from LayoutLM is a good example of that, it comprises the entire process and makes it easier to understand the whole architecture.",
            "17": "* In the abstract, authors say \"BROS utilizes a powerful graph-based decoder that can capture the relation between text segment\"* Though in the text such a component (that is from other work) is only mentioned twice without further detail.",
            "18": "It is unclear to me:\n\n* How regions of interest are detected in this work?",
            "19": "(I assumed authors used the same strategy as LayoutLM).",
            "20": "* OCR seems to be an extra-step in the preprocessing stage.",
            "21": "What to do if the user does not have the same OCR.",
            "22": "What is the impact of a good OCR for training and testing (prediction of new, unseen documents)?",
            "23": "* \"In-house OCR engine was applied\" can authors provide more details on that?",
            "24": "This line of work could be much stronger if the models comprised the whole process (detection, text extraction, recognition) in an end-to-end manner.",
            "25": "### Notes on text and style\n\nThere are parts of the manuscript that felt somewhat informal and confusing to me.",
            "26": "I will provide some details as follows.",
            "27": "* Set a default format for numbers in tables.",
            "28": "Table 1 has two distinct decimal number formats.",
            "29": "* Personally, I think it is better to write $5 \\times 10^{-5}$ rather than 5e-5.",
            "30": "* In the results section there is a typo: *\"performances with a large margins of 2.32pp in\"*.",
            "31": "Also, text could be more formal.",
            "32": "I would avoid using the use of the *pp* abbreviation.",
            "33": "* \"By achieving the best, these results prove that BROS\" this sentence can be improved.",
            "34": "* \"Moreover, it should be noted that BROS achieves higher f1 score than 79.27 of LayoutLM using visual features\".",
            "35": "I think authors wanted to say that even though BROS does not rely on visual features, it does outperform LayoutLM which, in turn, uses visual features."
        },
        "cLvMtFUhH1l": {
            "0": "The paper proposes the pre-trained language model BROS which aims to leverage both text and spatial information to improve information extraction on documents.",
            "1": "Using the graph-based decoder from SPADE, BROS achieves the SOTA performance on some entity extraction and entity linking downstream tasks.",
            "2": "However, the area-masking strategy does not show significant improvement over the LayoutLM and the graph decoder is proposed in SPADE which is not new.",
            "3": "In addition, as most commercial OCR tools have already got very good reading order information, the benefit from the graph decoder might be marginal.",
            "4": "Pros\n-\tThe paper introduces the area-masking pre-training strategy that can be seen as a natural generalization of masking language model in the 2D plane.",
            "5": "-\tThe authors integrate spatial information into the attention mechanism as a pair-wise bias term, which is reasonable.",
            "6": "-\tBROS utilizes the graph-based decoder from SPADE and improves performance on downstream tasks.",
            "7": "Cons\n-\tThe area-masking strategy is to mask small area centered at some tokens, which is actually similar to masking the center token only.",
            "8": "Also, given that the FUNSD dataset is small, the area-masking strategy does not show significant improvement over vanilla MLM.",
            "9": "-\tThis paper shows that sinusoid & linear functions can encode 2D position efficiently.",
            "10": "However, it is not reasonable to compare sinusoid & linear and learnable embeddings on small data, since learnable embeddings could leverage large amount of data and get more gains.",
            "11": "-\tThe graph-based decoder part is identical to which in SPADE so it is not suitable to appear as the contributions of this paper.",
            "12": "In summary, this paper largely overlaps with the previous research work.",
            "13": "I do not think it is qualified for the ICLR conference."
        },
        "J_RcevSrOQj": {
            "0": "> Summary: \n\nThe paper studies the problem of large-scale pre-training for semi-structured documents.",
            "1": "It proposes a new pre-training strategy called BERT relying on Spatiality (BROS) with area-masking and utilizes a graph-based decoder to capture the semantic relation between text blocks to alleviate the serialization problem of LayoutLM.",
            "2": "It points out that LayoutLM fails to fully utilize spatial information of text blocks and will face difficulties when text blocks cannot be easily serialized.",
            "3": "The three drawbacks of LayoutLM are listed:\n* X-axis and Y-axis are treated individually with point-specific embedding\n* Pre-training is identical to BERT so does not consider spatial relations between text blocks\n* Suffer from the serialization problem\n\nThe proposed three corresponding methods of BROS are:\n* Continuous 2D positional encoding \n* Area-masking pre-training on 2D language modeling\n* Graph-based decoder for solving EE & EL tasks\n\n> Strength:\n\n* The paper makes incremental advances over past work (LayoutLM) and the proposed BROS models achieves SOTA performance on four EE/EL datasets (i.e., FUNSD, SORIE*, CORD, and SciTSR)\n\n* The paper is generally easy to follow and could be better if provide more important details in Section 3.2 & 3.3\n\n* The experiment and discussion for Section 5.3 are quite convincing.",
            "4": "BROS could achieve robust and consistent performances across all the four permuted version datasets, which demonstrates that BROS is adaptive to documents from the practical scenarios.",
            "5": "> Major concerns:\n\n- For Section 3.2, the author didn’t even provide the pre-training objective for the area-masked language model.",
            "6": "I think the author should include the objective and define the exponential distribution explicitly.",
            "7": "* I’m confused about why the performance difference in Table 4 between original LayoutLM and BROS is larger than that in Table 1.",
            "8": "In the original LayoutLM, the 2D position encoding method is tied with token embedding.",
            "9": "This applies to both Table 1 and Table 4.",
            "10": "However, in Table 4 the performance difference on FUNSD EE is 42.46 vs 74.44, while in Table 1 the performance comparison is 78.89 vs 81.21.",
            "11": "Could the author give some explanations on this?",
            "12": "- In Table 4, it is better for the author to clearly indicate each ablation’s components.",
            "13": "The author should also give the performance of the original LayoutLM and performances after gradually adding each new component to the original LayoutLM: such as original LayoutLM + Sinusoid & Linear, original LayoutLM + Sinusoid & Linear + untied with token embedding, etc.",
            "14": "* For encoder design in Eq.",
            "15": "(2), the second term is used to model the spatial dependency given the source semantic representation.",
            "16": "How about adding a symmetric term to model the semantic dependency given the source spatial representation.",
            "17": "My question is simply that why not adopt a symmetric design for Eq.",
            "18": "(2)?",
            "19": "* Can the author give the reason behind the design of $\\mathbf{t}^{ntc}$ in Eq.(4)?",
            "20": "I’m not so clear about the function of $\\mathbf{t}^{ntc}$.",
            "21": "Does the $\\mathbf{f}_{ntc}$ output a distribution of the probability to be the next token over all N tokens?",
            "22": "* Could the author give a detailed analysis on which strategy contributes the most to BROS’ robustness against permuted order information?",
            "23": "From the results of Table 4, it is not the SPADE decoder and the most important factor seems to be calculating semantic and position attentions separately, i.e., untied with token embedding and explicitly modeling semantic/position relations between text blocks.",
            "24": "Do the authors agree with my conjecture?",
            "25": "> Minor concerns:\n\n* Although SPADE is not the most important component of BROS, I believe including details of the grade-based decoder will help the readers to understand the model much better.",
            "26": "* I’m curious about the performance of SpanBERT on the four datasets since the author said that area-masking of BROS is inspired by SpanBERT.",
            "27": "* In Table 3, the value of LayoutLM - FUNSD should be 78.89 since all other p-FUNSD & FUNSD related values are consistent with Table 1 & 2."
        }
    },
    "Q5ZxoD2LqcI": {
        "oqnbZgzfteY": {
            "0": "This paper explores the problem of machine translation for African languages.",
            "1": "The authors show that trilingual models outperform bilingual models for a set of languages spoken in Cameroon, and explore two strategies to find the best auxiliary language for a given translation pair: a manual one based on their linguistic relationship, and an automatic one based on language modelling.",
            "2": "I agree with the authors that African languages have been underrepresented in NLP research in general and machine translation in particular.",
            "3": "In that regard, I think that their motivation is truly admirable, and I would like to encourage them to keep working on this direction.",
            "4": "Unfortunately, I think that the work itself does not meet the standards of the conference in its current form.",
            "5": "While the focus on African languages is new and interesting, I find that the paper has little substance beyond that.",
            "6": "According to the authors the paper makes 3 main contributions, which I feel are too narrow or otherwise questionable as discussed next:\n\n1) \"Our first contribution is the creation and public release of a new dataset.\"",
            "7": "The dataset in question is a parallel version of the Bible.",
            "8": "There have already been some efforts to extract parallel corpora from the Bible, which are not even discussed in the paper.",
            "9": "For instance, Mayer & Cysouw (2014) [1] report covering 830 languages, which are likely to include the ones explored in the paper.",
            "10": "Moreover, the authors extracted the corpus from an existing website with Bible versions in >1000 languages, so this was just a scraping effort.",
            "11": "In connection to that, I am not sure if the authors considered potential copyright restrictions when releasing their corpus.",
            "12": "2) \"Our second contribution is the proposal of a new metric to evaluate similarity between languages\".",
            "13": "The proposed method measures the similarity between two languages, L1 and L2, by applying a language model trained on L1 to L2.",
            "14": "This looks like a rather simplistic approach and, in the lack of any other baseline, it is hard to get a sense of how good it is.",
            "15": "Also, I am not familiar with this topic, but there is certainly some work on automatically identifying similar languages.",
            "16": "For instance, [2] shows that the degree of overlap in the BPE vocabulary of different languages is already a good indicator of the their linguistic proximity.",
            "17": "3) \"Our third contribution is the set of empirical evidences that doing multi-task learning on multiple similar languages generally improves the performances on individual pairs.\"",
            "18": "There were already in-depth studies showing that multilingual training is helpful for low-resource machine translation, see for instance [3].",
            "19": "Unfortunately, this is not even cited in the paper.",
            "20": "In connection to the last point, the authors limit themselves to trilingual models and the main focus of their work is on identifying the most suitable auxiliary language.",
            "21": "An obvious baseline that is missing in the paper is to train a multilingual model on the combination of all languages.",
            "22": "In addition, the models used by the author are usually trained at a large scale, and might not work well with very small datasets like the Bible.",
            "23": "Even a phrase-based statistical machine translation system might work better in this setup, and I overall miss better baselines.",
            "24": "[1] http://www.lrec-conf.org/proceedings/lrec2014/pdf/220_Paper.pdf\n[2] https://aiide.org/ojs/index.php/AAAI/article/view/4677\n[3] https://arxiv.org/abs/1907.05019"
        },
        "qB9zWsnF5pP": {
            "0": "This paper motivates clearly the need for research in machine translation of underresourced (and thus underresearched) African languages, and proposes ways to aid the training of MT systems using data from related languages.",
            "1": "The main contribution is that two ways (and a random baseline) to select a langauge to add to the training of an MT model are evaluated (one based on linguists' clustering, one based on probabilities assigned through a language model of the target langauge) on pairs drawn from a set of three langauges, choosing new langauges from a set of 13.",
            "2": "#### Strengths/what I loved:\n- The paper performs a very clearly motivated and well set-up experiment with appropriate analysis.",
            "3": "- I loved to see a focus on datasets for underresourced langauges and the paper's repeated focus on whether or not previous work was reproducible and how this paper tries to be as reproducible as possible (even if that is not possible with Bible data for legal reasons).",
            "4": "- I always like to see attention to detail and explanation as i the \"Due to the multi-task nature...\"-paragraph on page 7!",
            "5": "- The observation that adding weakly related languages *hurt* was very interesting to see, thank you for including it!",
            "6": "#### Criticism/weaknesses:\n- While the overall setup (choose a language to add using either linguistic knowledge or automatic metrics) is solid and interesting, the setup and contributions promised on pg.",
            "7": "2 make it sound especially like you are proposing a way to infer linguistic relations between languages (\"a method for aggregating languages based on [a number of features]\").",
            "8": "- The related work section is rather unfocused: it might help to make clear at the beginning of each paragraph how all the work cited in that paragraph relates to the present paper (e.g., \"There are a number of papers on translation of African langauges, covering English to Setswana (Abbott & Martinus (2018); Transformers on the Autshumato dataset (Groenewald & Fourie (2009) reaching BLEU 33.53), English to Afrikaans...\" or something along these lines.",
            "9": "The division between the first and the second paragraph isn't quite clear to me and I am not sure what the third paragraph is on: transfer in MT?",
            "10": "- Not only training, but unfortunately also validation is performed only on Bible data and only on a cluster of 3 languages.",
            "11": "The first is an issue because while I understand that Bibles are the easiest data to come by and that for training one takes what one can get, any results that stay within this Bible domain are unlikely to transfer well to real-world settings, making me doubt the practical use of the paper as-is.",
            "12": "The latter exacerbates the issue, because essentially as a paper motivated by poor MT performance or nonexistence on many African languages it fails to convince me that it is indeed solving that task and not one of Bible translation, and as a paper that talks about linguistic features in general, the extremely narrow focus on a cluster of literally only 3 languages seems unnecessarily restrictive if we are already settling for Bible data.",
            "13": "(Note that I think it is fair to say that these limitations were necessary because individual experiments are too costly to run more than this very barebones proof-of-concept, but that claim should be made and substantiated.)",
            "14": "- Experiments are not tested for statistical significance or otherwise qualified through a sense of the variance that's inherent in these results.",
            "15": "This is especially unfortunate for the \"Random\" baseline, which given the small set of candidates will produce rather unreliable estimates when only tested on 1 sample (please let me know if I misunderstood!)",
            "16": "#### Questions:\n- Point 2 of the Motivation section claims that language barriers aid the spread of misinformation... that's not quite clear to me: shouldn't any barrier impede information flow and in fact easy machine translation help the spread?",
            "17": "Unless it is correcting counter-information that you want to advocate for...\n- Page 5 says you used an English tokenizer for all languages.",
            "18": "Just to clarify: this is tokenizing before applying BPE, right?",
            "19": "Did you try to go without tokenization at all, i.e., using BPE as a standalone tokenizer?",
            "20": "- What is the point of the last two paragraphs of section 5, i.e., talking about clustering?",
            "21": "From my understanding you don't use that for your experiments, as there you only choose the 1 nearest neighbor---and if you did want to build larger clusters, the greedy approach sketched in here would be a needlessly restricted choice: there are a number of better agglomerative clustering algorithms out there!",
            "22": "- The paper does cite the Masakhane project preprint from March 2020 (though with a broken arXiv identifier in the references!",
            "23": "), but I would be very curious to see how it relates to the newer October 2020 preprint that contains a lot more content.",
            "24": "(Of course, I know that this paper is far too recent to expect or even ask you to refer to it in the preprint!",
            "25": "I would just personally be curious and think it might be a nice thing to add for a camera-ready.)",
            "26": "#### Typos and other small things:\n- Probably due to my unfamiliarity, I wasn't 100% clear on what you meant by ``vernacular'' throughout, it might be nice to give a quick definition.",
            "27": "- Some small grammar errors throughout, for example \"as follow\" instead of \"as follows\" in a few places.",
            "28": "- Citations should be in parentheses when they are not constituents in a sentence and bare when they are (as in the beginning of the second paragraph on page 3).",
            "29": "The citation of Lample and Conneau on page 2 somehow ended up with double parentheses...\n- I would have liked to see references for section 2, but I suppose that is not really the scope or point of the papers.",
            "30": "I also guess that the third point implies that colonial languages are learned instead of vernaculars?",
            "31": "That might get expanded a little, too.",
            "32": "- The JW300 dataset should be properly cited (Agić and Vulić, 2019: https://www.aclweb.org/anthology/P19-1310/ ).",
            "33": "- The description of MLM on page 4 says \"we keep them unchanged 10%\"---this sounds like this is a deviation you are introducing.",
            "34": "- The entire first half of page 6 could be condensed into a much smaller and more concise paragraph, namely a sentence like the one that then starts with \"At a paragraph level\", or to more clearly separate motivation from actual definition (which is nice and simple)\n- I would separate the top and bottom half of Table 1 more clearly, or better yet, make them separate tables altogether, given how they don't even both have the same set of columns.",
            "35": "- I fear the example given in the first paragraph of page 7 is a poor example: I see no reason a priori why learning a different unrelated language should *hurt* other languages I know.",
            "36": "Then again, as a non-native speaker of English, my own native langauge certainly has taken the back seat in my head... either way it might be good to elaborate a little on this idea or just leave it out.",
            "37": "Looking forward to discuss!"
        },
        "NWyBOq57d6U": {
            "0": "This paper considers translation between African languages.",
            "1": "Overall, I think this is a great effort, I think it's great that the authors are tackling this important problem.",
            "2": "However, the field of multilingual machine translation is a very well-researched field, and it seems that the authors have developed their methodology largely independent of the literature in this field.",
            "3": "In fact, there are already existing well-researched methods on many of the topics presented in this paper.",
            "4": "To give just a few examples:\n\n* *Pre-training for low-resource translation:* Liu, Yinhan, et al.",
            "5": "\"Multilingual denoising pre-training for neural machine translation.\"",
            "6": "arXiv preprint arXiv:2001.08210 (2020).",
            "7": "* *Leveraging linguistic similarities:* Lin, Yu-Hsiang, et al.",
            "8": "\"Choosing transfer languages for cross-lingual learning.\"",
            "9": "arXiv preprint arXiv:1905.12688 (2019).",
            "10": "* *Translation between low-resource language pairs:* Chen, Yun, et al.",
            "11": "\"A teacher-student framework for zero-resource neural machine translation.\"",
            "12": "arXiv preprint arXiv:1705.00753 (2017).",
            "13": "I would suggest that the authors read these papers, and maybe other papers that cite them.",
            "14": "Also, perhaps read papers on the ACL Anthology (https://www.aclweb.org/anthology/) from prominent conferences such as ACL, EMNLP, NAACL that contain the keywords \"multilingual\" and \"translation\" to get a better idea of the state of the art in the field.",
            "15": "There are lots of methods that people have developed, and I think that they could be effectively applied to the very important problem at hand here!"
        },
        "g7IV5s8uQAO": {
            "0": "This paper describes three contributions: (1) a new multilingual parallel corpus that covers 1,477 languages, with text from the Bible -- for the region of interest in this paper (Cameroon) this means 22 languages instead of 18 with JW300; (2) a method for determining similarity of languages based on LM scores; (3) a series of experiments that evaluate the utility of adding a third language to a pair of interest based on either the similarity metric proposed by (2) or based on historical data, for use in a multilingual MT system.",
            "1": "The authors show large improvements by adding a third language based on historical language similarity (always outperforms random) or their language-model-derived similarity (often outperforms random).",
            "2": "Working on MT for African languages is important, and I’d definitely like to see more of it.",
            "3": "Unfortunately, I think this paper suffers substantially from insufficient replication details.",
            "4": "Even if those details were present, I don’t think it is necessarily sufficiently novel for ICLR.",
            "5": "Regarding replication, the paper relies on three technologies: language modeling for the language similarity calculation, CLM and MLM pretraining, and multilingual MT.",
            "6": "Crucial details are missing for all three of these.",
            "7": "For the language modeling and for CLM and MLM pretraining, it isn’t clear what the monolingual data source is.",
            "8": "Without being specified, it’s implied that it’s just one side of the parallel corpora - but why would pretraining help in that setting if it’s not adding data?",
            "9": "In general, the data should be discussed with more precision.",
            "10": "It would be really useful to talk about sizes of datasets (if the different languages have differently sized datasets).",
            "11": "For multilingual MT, it isn’t clear what flavor of multilingual MT is being used: is it a tagged model?",
            "12": "Where are the tags (source versus target versus embedding)?",
            "13": "I understand that the authors have provided code, but the paper should stand alone, and I wasn’t able to easily answer these questions after spending 15 minutes with the code.",
            "14": "Regarding novelty, I think the paper would need to flesh out either contributions (2) or (3) to really find a home at a venue like ICLR.",
            "15": "(2) is an interesting idea, but right now we have one extrinsic test of the idea - I think a fully fleshed out version would need to compare to baselines and also provide intrinsic evaluations that compare to other “ground truth” definitions of similarity, such as the historical definition used in this paper.",
            "16": "(3) would require looking into why we’re seeing such huge jumps just from adding another language.",
            "17": "Are there disparities in dataset sizes that aren’t mentioned in the paper?",
            "18": "If adding another language at most doubles the amount of parallel data, I don’t see why that should reflect a boost from 12.6 to 28.8 BLEU.",
            "19": "There might be something really interesting going on here, but it would need to be explored much more carefully.",
            "20": "Smaller points:\n\nWhen citing Eberhard, I would include “M.” with the first name, not the last name.",
            "21": "The proposed similarity metric is based on an equation normalizing a perplexity difference according to a base perplexity.",
            "22": "But I don’t think this can be treated as a similarity, as lower is better in perplexity.",
            "23": "So if I have language A that receives a perplexity of 10 according to LM0, and another language B that receives a perplexity of 3 (and thus much higher average probability per token), and LM0 assigns its own language a perplexity of 2, then we’ll get scores of (10-2)/2 = 4 for A and (3-2)/2 = 0.5 for B, making A look more similar.",
            "24": "I think you probably intended to use average probability or average log probability here, or have the equation calculate a difference."
        }
    },
    "WlT94P_zuHF": {
        "hbKcZiXjhad": {
            "0": "Summary\n-------\nUsing multi-scale hierarchical and compressive techniques, this paper examines a way to increase the context length of transformers.",
            "1": "Although the individual components are similar to previous work, they are combined in a novel way that shows a path toward longer and more efficient context lengths.",
            "2": "Positives\n---------\nIncreasing the context length of transformers is an interesting and relevant topic, and the proposed solution can have real impact in moving the state of the art forward.",
            "3": "The results demonstrated in the experiments show an improvement over previous models.",
            "4": "The paper is clear and detailed, and well situated in the literature.",
            "5": "The algorithms were clear and the comments were useful for understanding the proposed idea.",
            "6": "Negatives\n---------\nThe experiments do not compare to many other approaches, even though those approaches are cited throughout the paper.",
            "7": "In the conclusion, the paper mentions comparison with Multi-Scale approaches, but that is not present in the experiments.",
            "8": "Since the architecture (ignoring the compression) is similar to multi-scale approaches, it would be good to compare against empirically.",
            "9": "Although the complexity analysis is thorough, I'd like to see empirical results of memory/compute requirements as a function of the context length.",
            "10": "If I understand correctly, the sub-linear results depend on particular settings of the memory length and compression rate.",
            "11": "How do the settings used in the experiments compare to those used for the analysis?",
            "12": "Is there a perplexity/efficiency tradeoff, and can you characterize that experimentally?",
            "13": "Reasons for Score\n-----------------\nThe idea proposed in the paper is novel and exciting, but I have some concerns about whether the gains promised by the theoretical analysis can be realized while maintaining modeling quality.",
            "14": "Minor issues that did not affect score\n------------------\nFigure 1 has some scaling/resolution issues that make it hard to read.",
            "15": "It's also sideways, which is inconvenient."
        },
        "GGMOZ9g1qog": {
            "0": "The work introduces the Transformer-QL, a transformer-based model that aims to capture long distance dependencies in the input.",
            "1": "The network processes the information defining multiple temporal scales, with finer scales for nearby elements, and coarser scales for distant information.",
            "2": "It also includes the recurrent memory extension from Transformer-XL from Dai et al.",
            "3": "The model is tested in a long range language modeling task.",
            "4": "==================\n\nThe presented Transformer-QL model improves over similar previous methods on language modeling.",
            "5": "A thorough complexity analysis is included.",
            "6": "The presentation can be improved, all the definitions are hard to follow.",
            "7": "The layers are described in a textual fashion, barely any math (and extended in the pseudo-code).",
            "8": "Further, the performance improvements are nice, though not impressive.",
            "9": "==================\n\nThe paper is hard to follow for moments.",
            "10": "For example, a Figure to define what are $n_s, n_c, n_m$ would help to understand the paper with a nice visual benefit.",
            "11": "Introducing the layer and networks in a simple way would help clarify the implementation and other notation.",
            "12": "The paper describes the idea of multiple temporal scales.",
            "13": "In the experimental sections, it seems that only two scales are used.",
            "14": "What would happen if the number of scales is increased to capture  longer contexts?",
            "15": "Would the method gain in performance?",
            "16": "Further, how does the network perform when a longer context is obtained *maintaining the same number of parameters* as a network with less temporal scales?",
            "17": "The motivation behind Transformer-QL is to increase the context length processed beyond what other methods can.",
            "18": "I agree with the authors that extending the context is important.",
            "19": "How effective is the method to capture farther long-term dependencies compared to previous methods?",
            "20": "The experimental results mainly address similar networks with similar context lengths.",
            "21": "This could be tested by ablating elements from the input or ablating the recurrent memory vectors (setting them to zero during inference).",
            "22": "What would happen to the network if compression is removed?",
            "23": "==================\n\nMy score is based on the clarity improvements needed and the lack of evaluation of the effectiveness on the main objective of the method (longer contexts).",
            "24": "I would appreciate if the authors can reply to my questions above.",
            "25": "==================\n\nMinor issues:\n\n-Section 2.1: “input sequence are” -> “input sequences are”"
        },
        "-4DW1m-CoGz": {
            "0": "The authors propose Transformer-QL to capture the contextual information in multiple temporal scales - finer scales to capture recent past information and coarser scales to capture distance past information.",
            "1": "The results show significant improvement in the perplexity score over Transformer-XL and Compressive Transformer.",
            "2": "I agree that making use of multiple temporal scales can capturing more context information.",
            "3": "However, the proposed method is like to merge the key ideas from Transformer-XL and Compressive Transformer.",
            "4": "And my major concern regards the experiments.",
            "5": "The performance of Transformer-QL is still far from the work \"ADAPTIVE INPUT REPRESENTATIONS FOR NEURAL LANGUAGE MODELING\" which does not target capturing long context.",
            "6": "Then why do we need Transformer-QL?",
            "7": "The authors should have done more experiments on other datasets, maybe refer to the paper, Big Bird: Transformers for Longer Sequences.",
            "8": "Pros:\n1.",
            "9": "The idea of using multiple temporal scales to capture more context information is convincing.",
            "10": "2.",
            "11": "The experiments show that Transformer-QL can outperformance Transformer-XL and Compressive Transformer.",
            "12": "Cons:\n1.",
            "13": "The authors only test the model on one language modeling task and far from SOTA model on this task.",
            "14": "More datasets should be used to evaluate the model.",
            "15": "2.",
            "16": "The authors should have some comparison to some recent models on processing long sequences, such as the models from \"Efficient Transformers: A Survey\"\n\n#######\nAs no author response, I will keep my rating."
        },
        "Tkjgb9P_xsx": {
            "0": "This paper proposes Transformer-QL, an improvement over Transformer-XL architecture which allows to use longer context at reduced cost.",
            "1": "The paper is well-written and its good experimental results are strengthened by theoretical complexity estimation.",
            "2": "The text of the paper, though well-structured and concise, requires some polishing and misspellings correction (for example, I am doubtful that the proposed architecture is endowed with the ability to bite while section D of the appendix claims that \"Multi-scale Transformer has been widely bitten by Transformer-QL\")."
        }
    },
    "7K0UUL9y9lE": {
        "GdNUZpUxH9L": {
            "0": "This paper presents a linear-time attention model based on importance sampling and locality sensitive hashing.",
            "1": "The idea is to use Bernoulli sampling to approximate the self-attention - which is quadratic in complexity.",
            "2": "Honestly, this is a very crowded space and already many models exist (https://arxiv.org/abs/2009.06732).",
            "3": "The authors are aware of these works, cite them and yet there is no comparison.",
            "4": "This method seems to be rooted in LSH and a very natural question is how does compare to Reformers.",
            "5": "There is not even a sparse transformer or local attention baseline in the experiments.",
            "6": "This raises questions about whether this paper will even make any impact at all.",
            "7": "(comparisons with longformer is done only on speed/memory but not qualitatively).",
            "8": "why?",
            "9": "I also find other flaws with the model.",
            "10": "If sampling is used, this essentially makes the model stochastic (correct me if Im wrong here).",
            "11": "but there are undesirable properties of this such as having non-deterministic inference.",
            "12": "Another flaw is that method potentially introduces a lot of instability in training.",
            "13": "I think the authors could comment a little on this.",
            "14": "Transformers are already notoriously difficult to train and I figure that this method would probably make it way harder for practitioners to get the hyperparameters correct.",
            "15": "I think playing with the scaling and normalization of the self-attention weights is something non-ideal, and unless the authors can show this is reasonable stable i am not convinced.",
            "16": "I also find it difficult to understand the choices of tasks.",
            "17": "It seems like GLUE benchmark is used, yet most of the tasks (like SST) are relatively shorter sequences.",
            "18": "I think the authors need to explore datasets that showcase the model's ability on longer sequences.",
            "19": "Artificially raising sequence len during pretraining is not really sufficient to be convincing that the model is doing something useful for longer sequences (since the masked out tokens really depend on local context).",
            "20": "My constructive feedback to the authors to improve the paper is to have reasonable baselines for comparison.",
            "21": "The datasets are also not appropriate.",
            "22": "I would suggest some actually long-range tasks in order to showcase the model's capabilities.",
            "23": "At the rate of the number of new models that tackle this problem, I suspect it would be wise to wrap up your sleeves and add actual efficient transformer baselines."
        },
        "eC-sAowKE_": {
            "0": "### Summary\n\nThe paper proposes to replace the weighted average of the values in standard self-attention with the average of values sampled in a way that the expectation is close to the result of self-attention.",
            "1": "In particular, the authors associate with each query-key pair, a Bernoulli random variable with expected value close to the exponential of the dot-product.",
            "2": "Sampling these variables and averaging the values per query is formulated in an efficient way using locality-sensitive hashing.",
            "3": "### Strengths\n\n- Using sampling to approximate self-attention is novel and promising.",
            "4": "- The LSH formulation where the values are averaged in the bucket is a clever way to avoid the pairwise interactions between queries and keys.",
            "5": "- Training with hashing and evaluating using the expectation is interesting and provides evidence for the approximation quality of YOSO.",
            "6": "### Weaknesses\n\n1.",
            "7": "In the evaluation there is never an explicit comparison with respect to both time and performance.",
            "8": "I appreciate that given a large enough sequence length YOSO will always be faster but will it be good enough?",
            "9": "2.",
            "10": "One of the most important parts of the methodology, the gradient computation, is the least clearly written.",
            "11": "For instance, equation 11 contains $\\nabla_{Attn}$ which is never defined and subsequent equations contain $\\nabla_{YOSO}$ which seems to contradict equation 11.",
            "12": "Moreover, how is equation 11 derived?",
            "13": "Is it the gradient of the expectation?",
            "14": "3.",
            "15": "In table 3, the performance is only measured with respect to inference.",
            "16": "Given that the most computationally intensive part of the method is the backward pass, a comparison with respect to wall-clock time per epoch, as well as total training time would be very informative.",
            "17": "### Reasons for recommendation\n\nI find the idea very elegant and interesting however, the experimental section is somewhat lacking.",
            "18": "There is no clear evaluation of the trade-off between speed and performance.",
            "19": "The MLM task, although significant and demanding, contains sequences of small length, otherwise why not show a graph of performance vs inference-time."
        },
        "VTl7JKi4-75": {
            "0": "This paper tries to improve the efficiency of (multi-head) self-attention by reducing the computational complexity from a quadratic one to a linear one.",
            "1": "The authors propose to use Bernoulli sampling to approximate the self-attention's softmax distribution through importance sampling via LSH, which makes a linear cost self-attention possible.",
            "2": "Pros:\n1.",
            "3": "This paper provides a thorough solution of how to accelerate self-attention via an approximation by Bernoulli sampling and LSH.",
            "4": "2.",
            "5": "The experiments show that the proposed approach could achieve considerable speedup while preserving model performance.",
            "6": "Cons:\n1.",
            "7": "The authors should give a more direct comparison with an important related work the reformer, which also uses LSH to speedup the computation of attention.",
            "8": "2.",
            "9": "The experiments were mainly conducted on MLM on GLUE, which lacks generalization among tasks.",
            "10": "Adding more tasks such as MT or autoregressive/causal LM would make the experimental part more solid and convincing.",
            "11": "---------\nMinors:\n- Fig 2, 3 and Tab 1 are not cross-refed in the main body\n- Format of citation: seems all of the citations are of this format - authors (year), which is not correct when citations do not act as subjective of objective in the sentence.",
            "12": "Please check the format guideline."
        },
        "0WJ9QAJTEUT": {
            "0": "This article presents YOSO, an locality sensitive sampling based attention mechanism for large scale language modeling.",
            "1": "Strength:\nA new idea of applying  locality sensitive sampling to approximate attention matrix in the transformer\n\n\nWeakness:\n1.",
            "2": "Comparison of Complexity: [1] presents the complexity of different efficient transformers.",
            "3": "For linformer[2], the time and memory complexity is O(nk).",
            "4": "Is there any justification of LSH sampling equipped YOSO with complexity more than O(nm\\tau log(d)+nmd)?",
            "5": "2.Experiments: YOSO takes linformer as baselines.",
            "6": "However, the pre-training experiment part does not provide steps vs ppl of linformer with YOSO in Figure 4.",
            "7": "What is the comparison result of YOSO with linformer on iteration wise convergence?",
            "8": "Also, linformer demonstrates better accuracy in downstream tasks such as SST-2.",
            "9": "Is there any comparison to an explanation that can analyze this difference in performance?",
            "10": "3.Efficiency: YOSO demonstrates an advantage over linformer and longformer in memory and runtime.",
            "11": "However, is there any analysis on why YOSO achieves this superiority with higher complexities?",
            "12": "Are there any system-level advantages that YOSO can show?",
            "13": "Some discussions: \nReformer[3] design an attention mechanism that computations are held in the neighbor tokens inside the hash buckets.",
            "14": "YOSO also uses hash based sampling to compute attention via neighbor tokens that have high collision probability.",
            "15": "On the other hand, linformer introduces a more global view for attention by the low rank projection.",
            "16": "Is there any analysis of the local vs global intuition?",
            "17": "[1]Efficient Transformers: A Survey https://arxiv.org/pdf/2009.06732.pdf\n\n[2]Linformer: Self-Attention with Linear Complexity https://arxiv.org/abs/2006.04768\n\n[3] Reformer: The Efficient Transformer https://arxiv.org/abs/2001.04451"
        }
    },
    "Qpik5XBv_1-": {
        "bxS1-55Cv35": {
            "0": "This paper presents a model for image segmentation from referring expressions which integrates linguistic representations of the referring expressions both at low-level and high-level stages of visual processing.",
            "1": "They argue that this model is both more cognitively plausible and more successful than models which only use linguistic representations to modulate attention over high-level visual features.",
            "2": "I vote for rejection, mainly on grounds of significance and quality, expanded below.",
            "3": "To change my vote I would require a substantial improvement on one or both of the quality/significance issues listed below, either presenting the model with a clear conceptual motivation, or doing satisfactory model analysis to understand the contribution of the paper.",
            "4": "Pros: The presented model shows some moderate quantitative improvement over other recent work.",
            "5": "Cons: Poor conceptual motivation and error analysis, with little evident understanding of the actual effect of the linguistic representations within the model.",
            "6": "Quality/Significance\n\n1.",
            "7": "The paper does not provide a clear motivation for their model.",
            "8": "Here are some arguments I looked for but did not find:\n  1.",
            "9": "Cognitively: There are some references to relevant cognitive science papers, but there seems to be little concrete inspiration taken from this or other cognitive work in the particular model design.",
            "10": "2.",
            "11": "A priori based on the task: What would we expect to gain from using language in early-stage visual representations?",
            "12": "What sort of correlations might exist between particular types of linguistic input and low-level visual representations?",
            "13": "This might be another way to motivate the model, but I can't find any such discussion in the introduction or anywhere else.",
            "14": "2.",
            "15": "The evaluations don't convince me that this paper has made a significant conceptual contribution.",
            "16": "1.",
            "17": "The quantitative results don't seem to constitute an enormous improvement over past work.",
            "18": "The variability in table 2 across evaluation sets makes me doubt the statistical significance of the claims.",
            "19": "No statistical significance tests (across resamples of test data or across random training restarts) are provided.",
            "20": "2.",
            "21": "There is no satisfactory analysis of the actual cause of the model's success.",
            "22": "What are the contents of the linguistic representations, and how exactly do they modulate low-level visual features?",
            "23": "For reference, Hu et al.",
            "24": "(2020, Figure 4) [1] and Hui et al.",
            "25": "(2020, Figure 5) [2] both do some of what I'm looking for here, showing the influence of language on the behavior of the model.",
            "26": "While the more complex representations used in this model make it more difficult to provide e.g.",
            "27": "an easy heatmap, we absolutely need to see an error analysis that helps us believe your claim that language ought to play a role in low-level visual processing.",
            "28": "Originality\n\nI don't closely follow the relevant literature and can't speak confidently on the originality of the model.",
            "29": "I did have trouble understanding the innovation over Step-ConvRNN, however -- these models seemed within tweaking-distance of one another based on the presentation in this paper.",
            "30": "[1]: https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Bi-Directional_Relationship_Inferring_Network_for_Referring_Image_Segmentation_CVPR_2020_paper.pdf#page=5\n[2]: https://arxiv.org/pdf/2010.00515.pdf#page=14\n\n## Post-rebuttal update\n\nI have read the other reviews and the authors' rebuttals, and do not wish to change my review.",
            "31": "I strongly believe that numerical task improvements are not in themselves a conceptual contribution.",
            "32": "I look forward to the results of the analyses the authors mention in response to R3-Q2, to better understand what exact interaction between language and low-level visual input is being modeled.",
            "33": "Along with R4 I remain unconvinced of the strength of the empirical results.",
            "34": "The authors' response is not helpful here.",
            "35": "I can't understand where the numbers (mean 60.74 IoU, std 0.06) come from -- taking stats across table 2 and table 1, I get very different results, so I must be misunderstanding where they come from.",
            "36": "Significance tests would not take too much time -- it's not absolutely critical that you retrain the models for this.",
            "37": "You can use data resampling methods instead.",
            "38": "For example, on each individual dataset, run bootstrap tests comparing the predictions of your model and others on random resamples of the evaluation data and corresponding predictions.",
            "39": "Pooling IoU results across datasets within model and then comparing between models can yield misleading results and should be avoided."
        },
        "CiQc9XzOPRR": {
            "0": "This article proposes a novel approach integrating language throughout the visual pathway for segmenting objects according to referring expressions.",
            "1": "The article is well written, and poses an important question about how best to integrate linguistic and visual information.",
            "2": "The limitations of the currently dominant top-down approach are well argued.",
            "3": "The answer proposed by the authors is to integrate linguistic information throughout the visual hierarchy.",
            "4": "The task of segmenting by referring expression is important and well chosen.",
            "5": "The proposed model is sound, and well described in the article, and the experimental results demonstrate that the model outperforms clearly the state-of-the art in all metrics.",
            "6": "The qualitative examples provided are quite impressive and demonstrate the success of the approach.",
            "7": "In sum, I feel this is a well written paper addressing a very timely and important problem in computer vision and AI research and should be of broad interest in the community."
        },
        "YpGeltReJ7D": {
            "0": "This paper concerns the problem of image segmentation from referring expressions.",
            "1": "Given an image and a query phrase about a particular object in the image, the goal is to locate the target object as a mask at the pixel level.",
            "2": "The basic framework is U-Net, which consists of two branches: an image encoder and segmentation map decoder (connected at the bottom in a U-shape).",
            "3": "The paper proposes to use language to modulate the image encoding and decoding process intensively, by applying auxiliary convolutional connections between the two branches and further condition the convolution kernel on the language embedding.",
            "4": "Overall, the paper is easy to follow and has done a good literature review.",
            "5": "The major concern of the paper lies in the empirical results.",
            "6": "Despite that the introduction of top-down and bottom-up language modulation significantly boost the baseline performance (Tab.",
            "7": "1), the full model struggles to match existing works on certain metrics such as UNC testA/testB, UNC+ testB, and ReferIt, which put a question mark on the effectiveness of the work.",
            "8": "The results on the validation set are promising but not as good on the test set, which indicates a possible over-tuning of the model.",
            "9": "A minor comment on the model part.",
            "10": "In the text above Eq.",
            "11": "1, the paper mentions \"[...] ,we split the textual representation [...]\".",
            "12": "However, what is the rationale for splitting the representation since each split does not attach to any particular abstract of the image feature (low-level, mid-level, and high-level)?",
            "13": "Besides, some numbers from Tab.",
            "14": "1 do not match those from Tab.",
            "15": "2.",
            "16": "For instance, the IoU on LSCM and Step-ConvRNN.",
            "17": "Please double check.",
            "18": "============== Post-Rebuttal ==============\n\nThe authors' responses to point 1 & 2 do not sound (reflecting a question to another paper does not solve the problem).",
            "19": "The authors mentioned \"We made this decision based on Mei et al (2018) which proposed our baseline model (the top down approach)\", where the reference of Mei et al (2018) cannot be found in the paper, as a critical baseline.",
            "20": "This raises a flag on the novelty of the work and completeness of the related work.",
            "21": "Therefore, I am lowering my rating to 4."
        },
        "vr4cjmbkui": {
            "0": "This paper proposes to integrate visual and linguistic features in both top-down and bottom-up modulation of the visual input.",
            "1": "This is done by fusing two modalities while doing convolution and deconvolution operations over the visual input.",
            "2": "Experiments on image segmentation from referring expressions in standard datasets show that the proposed approach achieves state-of-the-art or competitive results.",
            "3": "Ablation studies show that both top-down and bottom-up is essential.",
            "4": "I believe the novelty and contribution are rather thin because many ways of the modeling language are not explored at all.",
            "5": "Below I list suggestions (S) and questions (q) for authors:\n\nS1 Second paragraph of introductions: please add a figure to explain the concepts of top-down, bottom-up processing, high-level, low-level effects etc.",
            "6": "S2 Section 2.2.: Please cite the below papers [1,2] for referring expression comprehension.",
            "7": "For Section 2.4 please add [3]\n\nS3 Figure1: following this figure is not intuitive.",
            "8": "I recommend adding two arrows for top-down and bottom-up processing and adding more space between two branches.",
            "9": "S4 Section 4.2: It is not clear how each of these ablations was performed.",
            "10": "For instance, I'm not 100% sure whether two modalities are fused at different levels of top-down or bottom-up processing.",
            "11": "[1] Nagaraja et.",
            "12": "al.",
            "13": "\"Modeling context between objects for referring expression understanding.\"",
            "14": "[2] Cirik et.",
            "15": "al \"Using syntax to ground referring expressions in natural images.\"",
            "16": "[3] Chen et.",
            "17": "al.",
            "18": "\"Touchdown: Natural language navigation and spatial reasoning in visual street environments.\"",
            "19": "S4: Section 4.3: the claim of modeling the long-range dependencies is a bit speculative.",
            "20": "I would rephrase that.",
            "21": "S5: Figure2: Failure cases are more informative than successful ones.",
            "22": "Please either bring the figure from A.1 or add a comparison with a model from the literature where the other model is successful where yours is not to do a contrastive analysis on how your model can be improved.",
            "23": "Q1: Section3: What's the effect of the number of layers for the model?",
            "24": "Why stop at 3?",
            "25": "Do you have results for the number of layers 0,1,2?",
            "26": "Q2: Is there a way to interpret the interaction between language and visual input?",
            "27": "Q3: Have you experimented with different ways of fusing or processing language input?",
            "28": "Examples: gating the language representation, attention over tokens, using different fusion methods, bi-directional LSTM, BERT-like contextual representations, adding inductive bias with parse tree for referring expressions, alignment between feature maps and word tokens or phrases?"
        }
    },
    "cYr2OPNyTz7": {
        "etuEUUNuMQr": {
            "0": "To reduce the variance due to the sampling of masks, the authors propose a fully-explored masking strategy, where a text sequence is divided into a certain number of non-overlapping segments.",
            "1": "And they show this technique improves accuracy in downstream tasks.",
            "2": "This idea is novel and interesting to me, and the derivation and experiment results look encouraging.",
            "3": "However, I feel that experiments can be strengthened, and notations can be improved.",
            "4": "Below are my major concerns:\n\nIf the major motivation is to reduce gradient variance, can we just use larger mini-batch size?",
            "5": "In the experiments, it is not reported that the learning rate or the mini-batch size is well tuned for the baseline.",
            "6": "I'd like some confirmation that larger batch size won't get much improvement for the baseline model.",
            "7": "For example, in the continual learning, you said \"choose a batch size of 48\", that seems to be small to me?",
            "8": "In algorithm 1, in each iteration, only data sample (S_i) is used, how is this choice motivated?",
            "9": "It'll be good to have some ablation study of the combined effect of using only one data sample in a mini-batch, and the full-explored masking.",
            "10": "For example, if for the baseline model, we also only use one data sample and apply different masks, will there be improvement?",
            "11": "In experiments, mainly accuracy is shown, but since the major motivation to reduce gradient variance, why not show some comparison of gradient variance of MLM and MLM-FE?",
            "12": "In Sec2, you said \"yet is outperformed by the proposed fully-explored masking (see Table 2).",
            "13": "\", do you mean Table 3?",
            "14": "In addition, I have several notation confusions:\n\nAssumption1: What is the hamming distance m_1 and m_2, when m_i are random variables?",
            "15": "Do you mean the expected hamming distance?",
            "16": "Or we are assuming x is fixed?",
            "17": "Please be more clear.",
            "18": "In sec3.1, you used S_t for minibatch, but in sec3.4, you use S_i for \"a text sequence\", which is confusing.",
            "19": "In sec4.1, the authors said \"A batch size of 256 is employed, \", does that mean K=256 in Algorithm 1?"
        },
        "b-kmDuN1cRk": {
            "0": "Summary:\n\nThis work proposes a fully-explored masking strategy, which maximizes the Hamming distance between any of the two sampled masks on a fixed text sequence.",
            "1": "The motivation is to reduce the undesirable large variance of MLM objective, based on the hypothesis that randomly sampled masks in MLM would lead to undesirably large gradient variance, which as a result typically hurts the training efficiency with stochastic gradient optimization algorithms.",
            "2": "---------------------------------------\nStrength:\n\nThe hypothesis from the variance reduction is interesting.",
            "3": "The method is sound.",
            "4": "Theoretical discussion proves that the gradients derived from the new masking schema have a smaller variance and can lead to more efficient self-supervised training.",
            "5": "Experiments on both continual pre-training and general pre-training from scratch show the effectiveness of the proposed method.",
            "6": "Case studies show that the method can help improve training efficiency.",
            "7": "---------------------------------------\n\nConcerns & Questions:\n\n1.",
            "8": "Regarding the proof, the notations in Section 3 are quite loose, especially for Section 3.1.",
            "9": "It might be better to tighten them and put the proofs in the Appendix into the main body of this paper.",
            "10": "2.",
            "11": "The authors claim the variance of the K-masks gradient can be reduced by decreasing the gradient co-variance between different masks, but this is just for a given sentence and the problem still exists considering different sentences.",
            "12": "3.",
            "13": "The proof in A.2 is not persuasive, I cannot agree that equation 14 can be concluded through previous statements.",
            "14": "4.",
            "15": "In terms of the experiments, there are only dev results reported on GLUE (Table 2).",
            "16": "It is hard to infer the test gains, given the possibly significant hyperparameter optimization on the dev set.",
            "17": "It seems that the authors have the infrastructure for computing single-model test-set results.",
            "18": "So why not report the test results?",
            "19": "5.",
            "20": "Why are the backbone models (RoBERTa and BERT, respectively) different in Table 1 and Table 2?",
            "21": "I'm concerned whether these improvements will hold after optimizing BERT carefully like RoBERTa, or using more advanced backbone methods like ALBERT.",
            "22": "Why not using a larger model (e.g., Roberta-large)?",
            "23": "6.",
            "24": "There is no comparison with other public masking methods in Table 2, such as whole-word-masking, span masking, etc.",
            "25": "Did you use dynamic masking as that was previous used in RoBERTa?",
            "26": "How about the benefits compared with the proposed one?",
            "27": "7.",
            "28": "Concerning the claim, “the proposed fully-explored masking strategies lead to pre-trained models with stronger generalization ability.”, it is not clear how the proposed method yields stronger generalization ability.",
            "29": "8.",
            "30": "The ablations cannot serve the topic of this paper well.",
            "31": "For the first ablation, it just gives out the performance of intermediate models on a single task.",
            "32": "It might be better to give out the trend of training loss and validation loss.",
            "33": "For the second ablation, why do all the larger splits lead to similar performance?",
            "34": "Intuitively, doing more mask-then-predict procedures is better for learning language representations.",
            "35": "The explanation at the end of Section 4 is not persuasive.",
            "36": "---------------------------------------\nMinor issue: the citation format is not consistent, please check the usage of \\citep{} and \\citet{}."
        },
        "Uee9uPi_YZU": {
            "0": "This paper try to improve Masked Language Model pre-training by reducing the variance using a new masking scheme.",
            "1": "Pros:\n1.",
            "2": "It is very novel and interesting that the authors try to understand the training of MLM by analyzing the relation gradient variance and masking scheme.",
            "3": "2.",
            "4": "They propose a new masking scheme that reduces the gradient variance.",
            "5": "3.",
            "6": "This paper shows the gradient obtained by the new masking is not biased.",
            "7": "(Lemma 2)\n4.",
            "8": "The performance improvement under continual pretraining and BERT pretraining looks good to me.",
            "9": "5.",
            "10": "The convergence is faster than the traditional MLM scheme.",
            "11": "Cons/Questions:\n1.",
            "12": "One of my concerns is that in the usual MLM training, each sentence is observed very limited times (e.g., $\\leq 3$).",
            "13": "Under such a setting, the overlapping between different maskings won't have too many overlapping.",
            "14": "2.",
            "15": "It would be better to validate the assumption by comparing the gradient variance between MLM training with and w/o the new masking scheme.",
            "16": "3.",
            "17": "It is not clear to me how Theorem 1/Assumption 1 motivates the new masking scheme.",
            "18": "What does it mean by \" The covariance $Cov(g(m1),g(m2))$ is monotone decreasing in terms of the Hamming distance\"?",
            "19": "Do you mean \"$Cov(g(m1),g(m2) | H(m1,m2)=\\tau)$ is monotone decreasing in term of $\\tau$\"\n4.",
            "20": "In Figure 4 (left), it would be better to run the experiments multiple times and plot mean and variance.",
            "21": "5.",
            "22": "If we compare the result of 6-split masking of ACL-ARC (Figure 4 right) and the baseline (DAPT) in Table 1, there is actually no performance gain.",
            "23": "I would say selecting an appropriate splitting number is tricky.",
            "24": "It would be helpful to present the 1-split performance (i.e., DAPT) for better understanding of the results.",
            "25": "Minor:\n1.",
            "26": "The text in Figure 2 can be larger.",
            "27": "2.",
            "28": "Last paragraph on page 4: in terms od ==> of"
        },
        "28jIK8rqr9": {
            "0": "The paper argues that randomly sampled masks in masked language model can lead to large gradient variance, and hence it proposes a new masking strategy called fully-explored masking with theoretical support to reduce the variance.",
            "1": "Experimental results show that fully-explored masking outperforms random masking in most cases.",
            "2": "Strengths:\n1.",
            "3": "The proposed method seems theoretically sound.",
            "4": "2.",
            "5": "The new masking strategy is simple and easy to implement.",
            "6": "Weaknesses:\n1.",
            "7": "The authors claim that the proposed masking strategy can reduce the gradient variance.",
            "8": "However, there are no empirical results to *directly* support the claim.",
            "9": "2.",
            "10": "The proposed masking strategy is empirically compared with random masking only.",
            "11": "However, there are existing papers which try to improve random masking, e.g.",
            "12": "\"ELECTRA\" and \"Ernie: Enhanced representation through knowledge integration\".",
            "13": "The authors may need to compare with these papers."
        },
        "A-9B93rYcJT": {
            "0": "\n###  Summary\nThis paper theoretically shows that the gradient variance of the standard MLM (masked language modeling) task in BERT-style training depends on the covariance of the gradient covariance between different masks within the mini-batch.",
            "1": "This paper then empirically shows that the covariance can be reduced by making the masks less overlapped.",
            "2": "A modified version of MLM is proposed, which has been shown with a smaller gradient variance than the standard MLM.",
            "3": "The experimental results show that the new masking strategy does lead to some gains on several benchmarks.",
            "4": "### Strong Points\n- This paper rigorously shows that the gradient of MLM can be reduced by decreasing the covariance of the gradient covariance between different masks within the mini-batch.",
            "5": "This is a very interesting observation and it is nice to see that this can be proved mathematically.",
            "6": "- This paper is well-written and very easy to follow.",
            "7": "- The experiment is also well done.",
            "8": "### Weak Points\n- This paper is based on several important assumptions, none of which has been proved mathematically (although there are some empirical evidence)\n   + Assumption #1: the model quality can be improved by reducing the variance of the gradient.",
            "9": "+ Assumption #2: the covariance of gradients can be reduced by increasing the Hamming distance between two different masks.",
            "10": "- The empirical gain of the proposed method seems to be quite small.",
            "11": "Given that the variance of the reported result is so large, it is unclear to me whether those gains are real.",
            "12": "### Other Comments\n- Honestly, I don’t quite see why the proposed method is related to continual pretraining, and why should it be included in the experiment section.",
            "13": "Is it because you want to show that under the situation that “in-domain data are usually much more limited” the proposed method is still effective?"
        }
    },
    "onxoVA9FxMw": {
        "7OEdSRfr94P": {
            "0": "This paper studies the position embeddings of transform-based models, and proposed a unified position embedding evaluation method in three aspects, i.e., Monotonicity, Translation invariance and Symmetry, which can well summarise the properties of the existing position embedding methods.",
            "1": "### Strengths of the paper\n\n1.",
            "2": "It is good that the authors summarise three property for position embedding models, and discuss four related position embedding models under the three properties.",
            "3": "2.",
            "4": "The three properties proposed by this paper are suitable for most position embedding models.",
            "5": "3.",
            "6": "The experimental details are complete and easy to reproduce.",
            "7": "Extensive experiment details are provided in the appendix.",
            "8": "### Weaknesses of the paper\n\n1.",
            "9": "The presentation and organization of this paper should be improved.",
            "10": "Typos and language issues can be easily found, see the minor comments below.",
            "11": "The contributions are not well highlighted in both the abstract and introduction section.",
            "12": "2.",
            "13": "The authors take many efforts to conduct experiments for the position embedding of BERTs, and provide an empirical study of the four existing position embedding models (fully learnable APEs (Gehring et al., 2017), (2) fixed sinusoidal APEs (Vaswani et al., 2017), (3) fully learnable RPEs (Shaw et al., 2018), and (4) fixed sinusoidal RPEs (Wei et al., 2019).).",
            "14": "The author tested these models and their combinations over three benchmarking datasets, however, as an empirical study paper, the analysis is too weak.",
            "15": "(1) For the qualitative result shown in Table 2 and Table 3, neither insight nor connection to three properties is provided for the result.",
            "16": "In fact, the result shown in Section 4 is intuitive and not something new, since these are the basic motivations of the RPEs and learnable PEs.",
            "17": "Moreover, the effectiveness of using both RPE and APE has already been validated in \"Self-Attention with Structural Position Representations, arXiv:1909.00383.",
            "18": "2019\".",
            "19": "(2)  I would like to see, in the experiment part, some new experimental results and conclusions in terms of the four summarised properties, which are key contributions the authors claimed.",
            "20": "However, what I see is just some position vector embedding similarities (i.e.",
            "21": "Fig.",
            "22": "2 and Fig.3) in terms of the four position embedding models, where the results are also somehow expected and intuitive.",
            "23": "The conclusions of the experimental results are too subjective.",
            "24": "For example, from the analysis of Fig (2), the conclusion that : “Lastly, note that fully-learnable RPEs also do not significantly distinguish far-distant RPEs (from -64 to -20 and from 20 to 64), suggesting that truncating RPEs into a distance of 64, like (Shaw et al., 2018), is reasonable.”  is a bit farfetched.",
            "25": "In this figure the white part is as narrow as (-5,+5),  a further quantitative evidence in other forms for this conclusion will be more preferable.",
            "26": "The same issue also exists in the conclusion of Figure (3): “This may be an advantage of RPEs over APEs to perceive forward and backward words, especially in span prediction tasks where capturing this matters.”\n\n3.",
            "27": "This paper lacks the discussion with the latest position embedding models such as,\n\n   [1] \"Learning to Encode Position for Transformer with Continuous Dynamical Model\".",
            "28": "ICML.",
            "29": "2020\n\n   [2] \"Encoding Word Order in Complex Embeddings\",  ICLR.",
            "30": "2019, where the 'Translation' property is also discussed.",
            "31": "Other minor comments:\n\n1.",
            "32": "In the introduction section, \"distances in $\\mathbb{N}$ and $\\mathbb{R}^{D}$ \", $\\mathbb{N}$ and $\\mathbb{R}^{D}$ should be explained when they appear in the first place.",
            "33": "2.",
            "34": "The references should be formatted in a unified manner.",
            "35": "3.",
            "36": "Table 2, the bold indicators for the best performances are put on the wrong numbers, e.g.",
            "37": "in QNLI, the bold should be 89.5,  in WNLI task it should be 51.3, and in STS-B it should be 87.5.",
            "38": "Overall, I think this paper indeed shows some interesting empirical results of position embedding models for BERT.",
            "39": "But, the analysis is monotonous and too subjective, lacking the necessary mathematical quantitative indicators, which prevents it from being a general way to verify the conclusions in this paper.",
            "40": "------\n### Comments after the discussion\n\nThank you for your detailed response.",
            "41": "I think most of my concerns were addressed so I updated the score to 6."
        },
        "8ki42ZkgicZ": {
            "0": "Updates after author responses and revisions:\n\nI was positive about this paper previously and am glad to see that the authors have done a great deal to try to respond to our concerns and strengthen the paper.",
            "1": "I am more positive about the paper now and have increased my score to an 8.",
            "2": "I think this paper is going to be useful for the community and I know I will reference it later and direct others to it who are interested in learning more about position embeddings in transformers (whether or not it actually gets published).",
            "3": "--------------\n\nThis paper studies position embeddings (PEs) in transformers, suggesting a few reasonable formal properties of PEs and determining whether these properties are captured by various choices for defining PEs.",
            "4": "The paper considers both absolute and relative PEs, and both fully-learnable and sinusoidal.",
            "5": "A new variation is to learn frequencies in the sinusoidal PEs.",
            "6": "Experiments are conducted by training BERT with various choices for PEs and evaluating on GLUE tasks and SQuAD.",
            "7": "Fully-learnable absolute PEs, the default in BERT, works quite well overall, but adding learnable sinusoidal relative PEs as well can improve performance on average.",
            "8": "There are also visualizations of PE dot products for the various methods, showing that learning PEs in the context of BERT training can yield PEs that satisfy the formal properties (for the most part) laid out by the authors.",
            "9": "I really enjoyed reading this paper.",
            "10": "PEs in BERT have been the subject of a lot of informal discussion over the past couple of years, but I don't think I've read a paper that studies the topic with such breadth and depth.",
            "11": "With some doable improvements and clarifications, I think the paper can become an excellent resource for others interested in representing position and distance in transformer-like models.",
            "12": "I like the idea of learned sinusoidal embeddings and think that idea can be potentially useful for other researchers.",
            "13": "The paper notes that with the fixed sinusoidal embeddings, distances beyond about 50 are not distinguished, but with learning they can be.",
            "14": "The experiments show that learning frequencies in sinusoidal PEs works better than using fixed sinusoidal PEs.",
            "15": "I'm curious what the learned frequencies look like and how similar they are to the original frequencies chosen by the transformer authors.",
            "16": "Some other thoughts I had while reading: \n\nAt least for language tasks with a given window, could we just re-use the learned frequencies and use fixed sinusoidal PEs in the future with those same learned frequencies?",
            "17": "I guess a similar choice can be made with fully learned PEs.",
            "18": "Do we really need to learn PEs from scratch every time?",
            "19": "What aspects of the data or task would influence this?",
            "20": "Maybe given their experience, the authors could hand-design useful general-purpose PEs?",
            "21": "Based on the discussion in Sec.",
            "22": "5.3, maybe a good recommendation for BERT-like models would be to use a single learnable PE only for [CLS] and something else (e.g., learned sinusoidal APEs or RPEs) for other positions?",
            "23": "This seems easy enough to do in practice and may bring the best of both worlds.",
            "24": "Below are some questions and points of confusion I had:\n\nIn the fully-learnable APE experiments described in Sec 4.2, for the first 10 epochs, were the unseen PEs randomly initialized and finetuned in the downstream SQuAD experiments?",
            "25": "I'm confused as to why some of the visualizations in Fig 3 show white bands along the diagonal (d, f, and g).",
            "26": "I would have expected all to have dark bands along the diagonal (as in b, c, d, and h).",
            "27": "I was super confused by the identical word probing parts of the paper.",
            "28": "Sec.",
            "29": "3.1 includes a sentence beginning with \"This shows that the selection of words\".",
            "30": "The sentence is awkward and confusing to me.",
            "31": "I don't know what the \"This\" is referring to.",
            "32": "It seems like there should have been a result reported there for the \"This\" to refer to.",
            "33": "The following sentence, starting with \"Namely\", is also confusing to me.",
            "34": "The description of identical word probing then points to Sec 5.2, and the Sec 5.2 section title is \"IDENTICAL WORD PROBING\", but it seems to be focused on describing Fig 3, which has as its caption \"Dot products between relative position vectors\".",
            "35": "Where are the identical word probing results actually reported?",
            "36": "In the \"Pre-training\" paragraph of Section 4, it is mentioned that a pretrained BERT checkpoint is used, but also that other BERT models with other position embeddings were trained.",
            "37": "How was the pretrained model actually used?",
            "38": "Was it used to initialize the other models or were all models trained from scratch?",
            "39": "Minor things: \n\nIn the first equation in Sec 3.2, there are a couple of instances of K/2 which I think should be D/2 instead.",
            "40": "Also, the vectors start with \\omega_1 but the summation at the end of the equation starts with \\omega_0.",
            "41": "I'm confused by the bolding in Table 2.",
            "42": "The best number in each column is not always in boldface (QNLI, WNLI, etc.).",
            "43": "Also, sometimes when there is a tie for the best result, multiple numbers are in bold, while other times only one result is in bold.",
            "44": "Typos:\n\np. 1: \"constrains\" --> \"constraints\"\n\nfootnote 4: \"seeing\" --> \"See\"\n\nSec.",
            "45": "4.1: \"outperform notably\" --> \"notably outperform\"\n\nSec.",
            "46": "5.3: \"PEs o\" --> \"PEs to\""
        },
        "12pkZxA4ho5": {
            "0": "The paper presents a systematic analysis of approaches used to encode position information in transformers and in particular BERT-based models.",
            "1": "The paper investigates absolute and relative position embedding strategies that use either fixed/learnable sinusoidal or fully learnable position embeddings.",
            "2": "These embeddings are characterized based on different properties that are either inherent from their formulation or observed empirically such as monotonicity, translation invariance, and symmetry.",
            "3": "Interestingly these properties appear to emerge naturally when having learnable parameters in APEs and RPEs.",
            "4": "Different PE strategies are empirically validated by pre-training BERT with different PEs (including a combination of absolute and relative position representations) and fine-tuning on GLUE and SQuAD (1.1 and 2.0).",
            "5": "Visualizations of the dot products between position vectors for different PE strategies are presented as well that demonstrate the monotonicity (or local monotonicity), symmetry, and translation invariance.",
            "6": "Overall, the paper is well written, motivated, and systematically studies an important design decision in transformers.",
            "7": "The overall methodology is sound and should prove useful to the community when studying position embeddings in transformers.",
            "8": "Strengths\n\nThe paper is well written, well-motivated, and is systematic in its claims, experiments, and methodology.",
            "9": "It takes a good step forward in characterizing desirable properties of position embeddings and studying if they emerge directly from their parameterization or via training.",
            "10": "It studies a variety of position embedding strategies and their conjunction and experiments with fairly realistic models and benchmarks.",
            "11": "Weaknesses\n\nWhile the analyses are well carried out, they are still specific to BERT-like masked language modeling training and it isn’t clear if PEs behave similarly in encoder-decoder like models for summarization/translation or decoder only language models.",
            "12": "It does not control for the choice of (pre)training objective, for example, if trained from scratch, purely as a retrieval model or on supervised text classification, would different learned PE patterns emerge?",
            "13": "It provides certain properties one can hope PEs to exhibit but doesn’t talk too much about circumstances under which these would be good inductive biases to have.",
            "14": "Questions & Comments:\n\nUnder what circumstances do properties like monotonicity in learnable PEs arise?",
            "15": "Would PEs for an autoregressive variant also display similar properties?",
            "16": "(would be interesting to see what happens with say a transformer LM trained from scratch on wikitext-103)\n\nIt isn’t quite clear to me at first glance why the preservation of the order of distances is necessary for position embeddings when used with models that build nonlinear functions of these embeddings.",
            "17": "Would absolute position embeddings that are randomly initialized or all initialized orthogonal to one another (something like 1-hot vectors of dimension L) *and fixed*  work?",
            "18": "This would be interesting to see (especially the latter) because it does not satisfy monotonicity or translation invariance and violates the desiderata in eq (1)\n\nIs Figure 7 based on BERT fine-tuned for NER on the CONLL/Ontonotes dataset?"
        },
        "lQsimEZqETL": {
            "0": "This paper proposes a formal framework to compare position embeddings (PEs) and presents an empirical study comparing variants of absolute position embeddings (APEs) and relative position embeddings (RPEs) on three properties: 1) monotonicity, 2) translation invariance, and 3) symmetry and evaluates their performance on classification (GLUE) and span prediction tasks (SQUAD).",
            "1": "The authors also report results on learnable sinusoidal APEs and learnable sinusoidal RPEs, PE variants which had not been previously proposed.",
            "2": "The first three properties seem well-motivated (monotonicity and translation invariance), but it is not obvious that symmetry should be a property of an ideal PE, or at least the paper is not convincing on this front.",
            "3": "In a sentence (ABCD), doesn’t the word A typically have a different relationship to B than B does to A?",
            "4": "The identical word probing test was a clever way to disentangle the impact of the word from that of the PEs.",
            "5": "While it does seem valuable to more rigorously compare PEs as they are critical components of SOTA language models, the experimental results were not particularly convincing (e.g.",
            "6": "although it’s a very appealing story, it didn’t seem so clear from the tables that APEs did better at classification and that RPEs did better at span prediction.)",
            "7": "The writing quality was borderline and there were a number of small errors:\n- fully-learnable APEs nearly meet all properties even under no constrains” -> “constraints”\n- Under the equation at the beginning of Section 3.1, “word-word correspondence” is repeated four times, which I am sure was not the intention.",
            "8": "- nit: “since relative distance with the same offset will be embedded as a same embedding.” -> “the same embedding”\n- “compared to far-way” -> “faraway”\n- “attends more on forwarding tokens than backward tokens” -> “forward tokens”?",
            "9": "- nit: “In Transformer, where attention calculation does not…” -> “where the attention calculation”\n- “allows PEs o better perceive word order” -> “to”"
        }
    },
    "6GkL6qM3LV": {
        "ezglM0J2B77": {
            "0": "\nSummary:\n\nThe paper designs a neural architecture (called N-bref) for code decompiling, i.e., translating binary code to high-level code (e.g., C/C++).",
            "1": "Experiments show that a high token-level accuracy on some LeetCode dataset, compared with a few baseline methods including Seq2Seq and Transformer.",
            "2": "Major concerns: \n\n1.",
            "3": "The paper does not appear to be novel.",
            "4": "The paper identifies 4 challenges for decompiling, but some of them are generic, such as long-dependency problems and data augmentation.",
            "5": "For the other two challenges (datatype and control/dataflow), the paper proposes to decompose the generation into two subtasks: source code generator (SC-Gen) and data type solver (DT-Solver).",
            "6": "Technically, both SC-Gen and DT-Solver are modeled by the same neural architecture (but differently parametrized): a memory-augmented structural transformer.",
            "7": "This appears to be a ragbag of existing and known models: Transformer [Vaswani et al., 2017], Tree Transformer [Sun et al., 2020], and Memory Augmentation [Cornia et al., 2020].",
            "8": "I do not feel this paper very exciting.",
            "9": "2.",
            "10": "The evaluation metric is problematic (or at least unclear).",
            "11": "The performance of a model is measured by \"token accuracy\" when the authors \"expand the decompiled AST from the root into the same structure as the golden AST (AST_G).\"",
            "12": "If this is what the authors meant, a real program is never generated.",
            "13": "The authors assume a correct program is there, and predict the next AST token/rule assuming previous partial AST is the same as AST.",
            "14": "Such measure of success could be drastically different from the real performance of generation.",
            "15": "The authors should consider string match of the generated program compared with reference, or the functional accuracy of the generated program.",
            "16": "With token accuracy given the correct partial AST, I would not agree with the claim that \"N-Bref successfully reverts human-written\" programs.",
            "17": "Minor:\n\nFor Ins2AST [Fu et al., 2019], did you use Ins2AST+Attention?",
            "18": "That seems to be better than Ins2AST (w/o attention).",
            "19": "Decompilation tasks is --> Decompilation task is"
        },
        "1x4ihnzcUWY": {
            "0": "The paper is interesting and addresses an interesting topic.",
            "1": "The results seem promising, though the evaluation is done on relatively small test cases.",
            "2": "The proposal to divide the problem into two sub-tasks, i.e., data type solver and source code generation, is promising and can probably have impact on future decompiler proposals.",
            "3": "The main problem with the paper is that it is hard to read and understand.",
            "4": "I'm aware of the page limitations, but the authors have crammed so much inte these pages that is hard to follow.",
            "5": "Further, there are also a number of inconsistencies and unclear stuff (see below).",
            "6": "For example, the authors claim that one thing their proposed method extends beyond earlier methods is to handle pointer references.",
            "7": "However, I can't find in the paper how that is done.",
            "8": "The usefulness may be limited of this work, since they only work on unoptimized code (see e.g., page 4, 2nd paragraph).",
            "9": "However, in reality, most code have went though substantial optimization during the compile phase.",
            "10": "Some other comments / questions:\n* I lack information about the execution time of the training and inference.",
            "11": "* Page 2, 2nd paragraph.",
            "12": "You claim that your code generator produces similar code styles as human programmers.",
            "13": "What do you mean by that, and how du you support that claim?",
            "14": "* Fig 1b.",
            "15": "Here are a number of strange / confusing things:\n  - Why are not all asm instructions shown in the data flow graph?",
            "16": "For example movl (line 2), call (line 4), testq (line 5), etc.",
            "17": "are missing.",
            "18": "- Why do you show one movss (line 9) and not the other movss (line 7)?",
            "19": "- You have mixed up lines 7 and 8 in the graph (show it as mulss -36(rbp),xmm0), which is confusing."
        },
        "_H6wrgteIwo": {
            "0": "The authors present a neural-based decompilation framework.",
            "1": "They generate synthetic input data in order to make sure source code examples have a consistent code style that can be more easily learned.",
            "2": "They predict types of variables and the actual code in two separate steps.",
            "3": "For both steps, they employ a custom transformer architecture where evey second layer of the encoder is a graph neural network.",
            "4": "There are two separate encoders of this kind, one conditions on a CFG obtained from static analysis of the input assembly, while the other one conditions on the partial output AST that has been generated thus far.",
            "5": "Strengths:\n- The authors propose an end-to-end system for neural decompilation\n- Interesting use of graph neural networks to increase sensitivity to structure in a transformer.",
            "6": "- Favourable comparison to multiple baselines.",
            "7": "- Evaluation also considers a human-written dataset.",
            "8": "Weaknesses:\n- It is not so easy to fully understand the approach end-to-end.",
            "9": "Perhaps the presentation can be improved.",
            "10": "(Though I understand that it can be challenging to fit an explanation of an ambitious approach with multiple novel components into 8 pages.)",
            "11": "When reading the paper, it happened to me a couple times that I tried to go back to some piece of information and I did not find it at the location where I would expect to find it.",
            "12": "Some details are discussed in the introduction, but apparently nowhere else, for example how the output of the DT-Solver is used.",
            "13": "It would help to reorganize the paper a bit so that the exposition follows the order of operations when running the approach and to discuss which data goes where in the technical sections.",
            "14": "The paper says that DT-Solver and SC-Gen are both based on the same architecture, but DT-Solver is not really discussed in detail.",
            "15": "As an example, it is not stated if the types of variables are chosen from some fixed set (which one?)",
            "16": "or if the DT-Solver generates a type AST, but Figure 1 suggests a fixed set.",
            "17": "Figure 3 is helpful, but it seems it does not show the full story for either DT-Solver or SC-Gen.",
            "18": "Figure 1 is a bit confusing, as the shown AST does not appear to match the given source code.",
            "19": "(E.g., there is no variable of type `int *`), and variable declarations are shown as part of a single AST of the program even though later they are treated separately.",
            "20": "- The evaluation metric is explained rather vaguely, so I am not sure if I fully understand what is meant, but this is crucial to interpret the results.",
            "21": "How do you \"expand the decompiled AST from the root into the same structure as the golden AST\"?",
            "22": "What happens during expansion if a token does not match?",
            "23": "Is the subtree removed?",
            "24": "I guess after the expansion step, you compare AST nodes that end up at the same position in the tree?",
            "25": "Further questions:\n\nAccuracy based on syntax comparison to synthetically-generated input examples is not necessarily what an end user cares about.",
            "26": "How well do your decompilation results preserve semantics?",
            "27": "I.e., if token accuracy is imperfect, what kinds of mismatches do you typically get?",
            "28": "It would also be interesting to understand a bit better the distribution of the results, e.g., how do the results change if you count the fraction of results with perfect token accuracy instead of computing averages over token accuracy?",
            "29": "As far as I understand, the positional encoding for ASTs drops a lot of structure information, which is then recovered by the GNN layers.",
            "30": "Have you considered using richer positional encodings along the lines of [i]?",
            "31": "[i] https://www.microsoft.com/en-us/research/publication/novel-positional-encodings-to-enable-tree-based-transformers/\n\n\nMinor:\n\nPage 1: \"learns to decompile the source code to assembly\".",
            "32": "That seems backwards.",
            "33": "Consider using \\citep and \\citet.",
            "34": "Please review your paper with a focus on grammar as well as whitespace and other formatting issues.",
            "35": "(For example, you should use ``$\\mathit{xmm0}$`` instead of ``$xmm0$`` ,`` `control flow'`` instead of ``'control flow'``, etc.)",
            "36": "Page 14: There is wrong indentation or missing curly braces next to the \"continue\" statement in Figure 3."
        },
        "XF8c0MoC-1": {
            "0": "High-level view:\n\nI don’t think this is necessarily a bad paper, but I think it’s unacceptable for ICLR in its current form.",
            "1": "I currently lean heavily toward rejection.",
            "2": "After thinking over the concepts in the paper more, I might lean more strongly toward rejection or toward acceptance (if the authors can address the issues I raise below).",
            "3": "I provide details below examination of how I’ve come to my evaluation rating below.",
            "4": "Summary:\n\nThis paper principally focuses on the idea of decompilation.",
            "5": "Decompilation can mean many things, but the general idea as I understand it, is to take a representation of a software program from one level (e.g., program binary) and then “lift it” to a level that is higher in abstraction (e.g., from binary to assembly, from assembly to C, from C to a lambda calculus, etc.).",
            "6": "As I understand it, it’s called decompilation because it tends to do the opposite of what a compiler does.",
            "7": "Compilers tend to lower a representation of a software program into something that is closer to the hardware and therefore potentially more efficient.",
            "8": "The benefits of decompilation are numerous.",
            "9": "One major benefit is in the ability to perform programming language – to – programming language transformation.",
            "10": "Another, which is the focus of this paper, is for reverse engineering purposes of a binary.",
            "11": "There are many others.",
            "12": "As such, in my opinion this is unquestionably an important subtopic for the field of machine programming and the authors approach also seems satisfactory to me for ICLR (described below).",
            "13": "The authors present a new approach called: neural-based binary reverse engineering framework (N-Bref).",
            "14": "N-Bref has a number of components that it relies on to perform its decompilation.",
            "15": "They consist mostly of components from the programming languages community (e.g., assembly code, abstract syntax trees for encoding and decoding, etc.)",
            "16": "and the machine learning community (e.g., deep neural networks for learning structural transformations, etc.).",
            "17": "The authors empirically evaluate their N-Bref’s accuracy on a number of problems from the open source LeetCode problem set and generate 25,000 pairs of high-level source and low-level source which are broken into training (60%), validation (20%), and testing (20%).",
            "18": "LeetCode problems tend to be fairly simple, self-contained, and, to my knowledge, are coding problems that are meant to help train new programmers or prepare software developers for coding interviews, amongst other things.",
            "19": "An emerging use of LeetCode is to use it as a baseline for machine programming (MP) in a variety of different ways.",
            "20": "In this case, the authors are using LeetCode coded solutions in MP to compiled the source code into a lower level form (assembly I believe) and then see if N-Bref can return the assembly back to the original form or some semantically equivalent form.",
            "21": "Their empirical approach seems sound to me.",
            "22": "Overall, the authors show better accuracy for their tested problem set against REWARD, a baseline system (a transformer), lang2logic, and Ins2AST across two dimensions: data type recovery and abstract syntax tree (AST) generation.",
            "23": "High-level concerns:\n\nThere are several reasons I’m not positive about this paper.",
            "24": "Perhaps the biggest reason is I can’t seem to understand what is novel about the system.",
            "25": "That is, unless I’ve just missed something, it seems that all of the core components of N-Bref are lifted from prior work with perhaps some minor augmentation.",
            "26": "This feels largely incremental to me.",
            "27": "On the other hand, one could argue that N-Bref is novel because it combines a number of existing components in a unique way to achieve better performance that prior work.",
            "28": "I can see this perspective.",
            "29": "However, if we considered this view, it seems like the problem they are solving should be more impactful than type recovery and AST generation.",
            "30": "I’m not saying these problems aren’t important – especially type recovery (I think this problem is deeply important) – but that it should go further to demonstrate more dimensions of decompilation.",
            "31": "The second major concern I have with this paper is the small dataset they are using.",
            "32": "Consider, for a moment, that they are using only 25,000 input/output pairs for their training/validation/testing.",
            "33": "Now consider a prior accepted ICLR 2020 paper, Hoppity (Dinella et al.",
            "34": "), which trained on nearly 300k code change commits in GitHub.",
            "35": "This looks like an order of magnitude difference in dataset empirical evaluation to me.",
            "36": "On top of that, the only data is coming from LeetCode.",
            "37": "We have no empirical demonstration that this approach will work on other datasets outside of LeetCode.",
            "38": "If the authors can address these two primary concerns by the time of decisions, I will likely slant toward the positive.",
            "39": "If they do not (or will not), I will likely champion this paper’s rejection, as I do not believe in its current form it’s up to ICLR standards.",
            "40": "Low-level concerns:\n\nThe language in the paper seems to use many strong and ambiguous claims: “N-Bref outperforms previous neural-based decompilers by a large margin.” First of all, what is a “large margin”?",
            "41": "There’s not quantitative measurement in the word “large”.",
            "42": "Large could mean 1%, 10%, 100,000%.",
            "43": "This kind of language is not what I expect from tier-1 publications.",
            "44": "Another example is: “However, key challenges remain:” where they then summarize two problems.",
            "45": "I agree that the two problems they highlight are important.",
            "46": "But I absolutely do not agree that those are the *only* two problems that stand in the way of decompilation.",
            "47": "Also, there seems to be some lack of understanding of the field of machine programming, from my perspective.",
            "48": "For example in the abstract the authors claim “decompilation aims to reverse engineer binary executables.” I 100% disagree with this definition.",
            "49": "As I stated above, I believe, the more general space of decompilation is actually the idea of lifting a software program representation from one format to a higher-level format that increases the level of abstraction from the hardware.",
            "50": "Moreover, I know of many decompilation systems (e.g., verified lifting is one), that has an entirely different goal than reverse engineering.",
            "51": "Verified lifting is principally focused, as I understand it, is focused on language to language translation.",
            "52": "Perhaps the grossest overclaim the authors make is in the introduction \n\n“Our work pushes the performance of neural-based decompiler to a new level and presents a new baseline and stand dataset for future developers.” \n\nI find that sentence simply unacceptable.",
            "53": "I could never give an accept rating to a paper that makes such an outlandish claim with such a small body of evidence.",
            "54": "Moreover, other people have used LeetCode as a baseline, so it’s not the first time people have done this.",
            "55": "So it seems wrong to me on many levels.",
            "56": "This continues throughout the paper …\n\nThat said, these are minor nits that the authors, if they so choose, could probably fix with little effort.",
            "57": "I would hope that in a later version of the paper the authors would tone the language down, move away from the number of strong claims they make in the paper, and provide measurable data points when making claims about performance: “Our system is more accurate than <list the systems you’re comparing against> from X% to Y%.” Right now the only way to figure that out seems to be to deeply study the experimental evaluation, which is a bit inappropriate in my opinion.",
            "58": "I believe it could (and should) be listed directly in the abstract and in the introduction.",
            "59": "By hiding these details, it creates a perception of overclaiming – at least it did for me."
        }
    },
    "F438zjb-XaM": {
        "TtTVGfJl-6G": {
            "0": "Edit after seeing others reviews -- I think I gave this paper a MUCH higher score than the other reviewers, simply because it is very novel with Fon language.",
            "1": "I agree with all of your points about what is lacking, but in my mind, the novelty was enough to still give a 7.",
            "2": "Now I definitely think that is too high.",
            "3": "I think this paper can reasonably be rejected, but I'd like to give actionable of constructive criticism, since I do think the work on this low resource language is important for the NLP community.",
            "4": "With such low resources, we cannot expect the same type of work as we would for other languages.",
            "5": "Overview: This paper discusses the problems of common tokenization strategies for low resource african languages, and proposes a new tokenization method to overcome these problems.",
            "6": "They train low resource NMTs using 4 different tokenization strategies, to show that their proposed tokenization method leads to the best NMT results by several metrics.",
            "7": "Contribution: The authors contribute a new tokenization method, code, and a dataset.",
            "8": "The good: Very interesting and important work!",
            "9": "Many people will be excited to use this data.",
            "10": "Paper is mostly clearly written, and easy to read.",
            "11": "The paper flows well.",
            "12": "Someone with this paper could reproduce the work, more or less.",
            "13": "The bad:\n* Figure 1 is difficult to read and messy.",
            "14": "First, by \"Input\" you actually mean \"Source\".",
            "15": "The input would be the source sentence with its appropriate tokenization, no?",
            "16": "Also, I think putting the english translation in a different font or color would be greatly helpful to our eyes.",
            "17": "I really think this must be fixed!",
            "18": "Figure 1 is presently not pleasant to look at, even though it has interesting results`!",
            "19": "* Section 4 - I think you really need to re-state that the algorithm has a human-in-the-loop for clarity.",
            "20": "Before describing your algorithm, humans are only mentioned once in the algorithm.",
            "21": "Indeed, at first, the words \"The following algorithm\" confused me, because I thought it was more a \"methodology\", since Step 2 is where the humans are in the loop, unless you have a Fon POS tagger and I am misunderstanding?",
            "22": "But then at the end, I saw you include Encode as step 4, so it is the machine...",
            "23": "The fact that I flustered a bit with my understanding here, was confused, and had to spend a few minutes thinking about it, means it needs a bit of tweaking.",
            "24": "Maybe add a comment saying Step 2 is the human-in-the-loop step of the algorithm?",
            "25": "Suggested additions:\n* I think more specific linguistic details about Fon are missing.",
            "26": "For example, if you could give us one or two sentences of Fon in the beginning of the paper, that demonstrate some of the difficulties of the language, I think this would greatly strengthen the motivation.",
            "27": "You *tell* us that Fon is \"a language with special tokenization needs\" and that \"standard tokenization methods do not alwaysadequately deal with the grammatical, diacritical, and tonal properties of some African language\", and you cite the relevant papers.",
            "28": "But I would still like to be *shown*.",
            "29": "I think just including two sentences that have some of these features, and that gets the point accross of \"how would we tokenize this?\"",
            "30": "would really help the motivation.",
            "31": "Its not that I/readers dont believe you when we are *told*, but being *shown* makes it much more interesting and give people an appreciation for Fon tokenization challenges!",
            "32": "* Can we get any information about how the annotators were trained?",
            "33": "I think this is standard for such papers.",
            "34": "Other smaller suggested fixes: \n* Section 5, near the end - Little grammatical mistake.",
            "35": "\"... bunch of those errors has\" should be \"errors have\".",
            "36": "* Section 6.3 - Please change \"The results from Table 2 and Table 1\" to say \"Table 1 and Table 2\".",
            "37": "It does not make sense to list them out of order.",
            "38": "I also think it makes sense to switch Figure 1 and Figure 2 entirely.",
            "39": "I.e., Figure 1 should be your results table, and figure 2 should be the examples for us to see.",
            "40": "* Section 6.3 - Slightly confusing wording.",
            "41": "The second sentence is confusing to me, and I am a native English speaker.",
            "42": "\"It is important to note that while BLEU of other methods reduced on the Fr→Fon task, WB improved on it.\"",
            "43": "To me saying \"BLUE reduced for the other methods\" means that you have some other baseline you are comparing to.",
            "44": "Am I missing something?",
            "45": "Are you comparing against Fon--> Fr?",
            "46": "Questions:\n* Section 6.2 - Does it really take all 500 epochs to run, or do you have early stopping at some point when the loss flatlines?",
            "47": "* Because BPE is such a standard baseline, why do you not include it as a baseline?",
            "48": "I know you cite the Abbott & Martinus, 2018 paper, stating that BPE is bad for analytical languages, but I still think it would prove a point to show BPE performing badly for your data.",
            "49": "Overall: Very interesting work, and can't wait to see this data be used :-) I think the paper could be greatly strengthened by taking some time to include an example that demonstrates the linguistic and typological features of Fon that makes it difficult."
        },
        "KjKVwahcz-": {
            "0": "The authors investigate different tokenization methods for the translation between French and Fon (an African low-resource language).",
            "1": "This means that they compare different ways to construct the input and output vocabularies of a neural machine translation (NMT) system.",
            "2": "They further propose their own way to create those units, based on phrases, which is called WEB.",
            "3": "The NMT system the authors use follows Bahdanau et al.",
            "4": "(2015): it is a GRU sequence-to-sequence model with attention.",
            "5": "The dataset they use has been created and cleaned by bilingual speakers and consists of roughly 25k examples (this is a really small dataset for NMT, so the authors are taking on a really hard task!).",
            "6": "WEB works in the following way: after phrases have been found automatically, bilingual speakers analyze what the longest phrases which correspond to translated phrases in the other language are.",
            "7": "Only the longest phrases for each example are kept for the final vocabulary.",
            "8": "The authors show that WEB improves the performance in both translation directions by a lot on all metrics, clearly showing that the work they invest into creating the vocabulary pays out.",
            "9": "Thus, I think this work is important to be able to provide speakers of Fon with a functioning translation system.",
            "10": "However, I am unsure if this work is suitable for a machine learning conference.",
            "11": "While the overall goal of this work is to create an NMT system, the main contribution is the manual cleaning of the dataset and semi-manual creation of the vocabularies.",
            "12": "I would recommend to the authors to submit this paper to a conference with a stronger focus on NLP and NLP resources (maybe LREC)?",
            "13": "I further want to emphasize that I think work like this paper is incredibly important and the authors shouldn't feel discouraged.",
            "14": "Importantly, the manual labor needed for WEB has been a lot and it's obvious that it helps for NMT.",
            "15": "I just don't think that this paper is a good fit for ICLR.",
            "16": "Minor point: has the creation of WEB access to the test data?",
            "17": "If so, the authors should change that (or collect new test data?)",
            "18": "to ensure a fair evaluation."
        },
        "XjVDidBdZoL": {
            "0": "This paper proposes another variant of phrase-based MT for African languages,\ninvolving native speakers for manual annotations.",
            "1": "Instead of just using subwords or statistical phrase identification, the \nauthors propose to use the intuition of native speakers for translating African\nFon languages into French (and vice versa).",
            "2": "According to their experiments, BLEU and other indexes significantly improved\nover standard IBM-1 phrase-based machine translation.",
            "3": "However, from the description and examples in this paper, I have a little doubt\nfor this improvement:\n\n- For creating the aligned corpus, the authors say that they chose only short\nexpressions, namely 1-6 words.",
            "4": "According to the results shown in Table 1,\nthis essentially amounts to simply memorizing frequent idiomatic phrases.",
            "5": "Therefore, improvements with this kind of human intervention over such an\neasy sentences is basically trivial.",
            "6": "Of course, the paper says that the test\ndata comprises of long and complex sentences; but the examples are not, thus\nI cannot identify whether the problem is really difficult or not.",
            "7": "- Even if the proposed human annotation is effective, that does not seem to\nleverage characteristic property of African languages.",
            "8": "In Section 3, \"un d'o\nganji\" has an ambiguities about \"un\", but this kind of ambiguity of a word is \nshared by almost all the other languages (imagine translating \"given\" in a\nconditional proposition).",
            "9": "The property of African Fon languages, such as\ndiacritics and affixation, are not used here.",
            "10": "Finally, the proposed annotation algorithm in page 3 seems to quite vague to\nme.",
            "11": "Where v came from?",
            "12": "If w is a word, what is the meaning of \"w \\subseteq v\"?",
            "13": "Also, this algorithm seems to use a simple longest match: however, in many\ncases the usage of a word is only clear using succeeding words; i.e.",
            "14": "some\nforward-backward algorithm is necessary for correct identification of a phrase.",
            "15": "That being said, I strongly agree with the authors that neural machine\ntranslation of African low-resourced language is important.",
            "16": "I hope that the\nauthors would add more persuasive results and analysis to realize a practical\ntranslation of Fon languages."
        }
    },
    "QtTKTdVrFBB": {
        "7dbbnudfpV": {
            "0": "##### Summary:\nThis paper presents Random Kernel Attention which is based on replacing the kernel function in the Linear Attention with random projection kernels.",
            "1": "In general, I think the method is novel and quite impactful.",
            "2": "Nowadays, some people are still staying away from attention because of its quadratic time and space complexity.",
            "3": "To the best of my knowledge, it is the first attention method with linear complexity that can match or even outperforms the conventional attention.",
            "4": "##### Strengths:\n- The method is intuitive and interesting to me.",
            "5": "- The results are strong.",
            "6": "Unlike Linear Attention, the proposed RFA outperforms the original multi-head attention baseline on both LM and MT tasks.",
            "7": "This is quite impressive.",
            "8": "Based on my experience the Linear Attention with ELU non-linearity can bearly match the performance of the original attention mechanism.",
            "9": "- The authors provide several in-depth analyses in the appendix.",
            "10": "I like the experiments in C.2.",
            "11": "- It is nice to see that the authors confess that the training is actually increased when using the RFA compared to the original Transformer.",
            "12": "Usually, the inference time and memory usage are more important in practice.",
            "13": "- It is great to see that the authors compare with the baselines that cache the query/key/value representations.",
            "14": "Nowadays, some papers avoid it make their speedup look better.",
            "15": "##### Weaknesses & suggestions:\n- It is not clear what D is used in the experiments.",
            "16": "The authors just vaguely say that they don't observe the improvement by setting it great than 2d.",
            "17": "However, it would be better to see plots at least in the appendix.",
            "18": "Also, I wonder if it would behave differently with different d. Also, it would be great to see what exactly the number is in the experimental setup to make this paper more reproducible.",
            "19": "- The arccos feature maps have only D-dimensional features, unlike the Gaussian feature maps which have 2D.",
            "20": "It is not clear whether the authors use the same D for both variants or double the D of arccos to keep the feature dimensions the same.",
            "21": "- After introducing the random projection weights, the number of parameters would increase.",
            "22": "It would be better if the number of parameters and the inference speed are both provided in Tables 1 & 2.",
            "23": "- The authors should clarify that the time complexity in Table 3 is based on the assumption that we have infinite number of threads or GPT/TPU cores that can be scaled up when M is increased.",
            "24": "Otherwise, the time complexity of training the softmax model is still O(M^2) because there is a matrix multiplication between matrices of sizes M-by-M and M-by-d. \n\n##### Questions:\n- Based on the experiments, it seems that the Gaussian random feature maps don't really try to approximate \n- How would the gating mechanism perform on the encoder side?",
            "25": "Similar to BiLSTM, half of the dimensions can be applied in a backward manner to make it bidirectional.",
            "26": "- Do you resample the random weights during the time?",
            "27": "- I wonder if the authors will release their implementation.",
            "28": "Based on my quick re-implementation, the proposed RFA doesn't really converge on some other dataset.",
            "29": "I believe there might be some differences in how the parameters are initialized which is not clearly described in the paper.",
            "30": "Admittedly, there is a chance that I have a bug in the code.",
            "31": "- Based on the conclusion in C.2 that RFA is not approximating the softmax kernel, would it be better if we just trained those projection matrices instead of fixing them as random matrices?"
        },
        "F1vLmmoggBw": {
            "0": "The paper presents a linear time and space attention mechanism based on random features to approximate the softmax.",
            "1": "The paper is clearly written and easy to follow.",
            "2": "The results are convincing: not chasing SOTA, but comparing to sensible baselines, namely [Baevski & Auli 2019] for language modeling on Wikitext-103, and [Vaswani et al.",
            "3": "2017] for machine translation on WMT14 EN-DE/EN-FR and IWSLT14 DE-EN.",
            "4": "The difference between theoretical speed-up and experimental speed-up is honestly discussed, and the overhead of the random features is not swept under the rug.",
            "5": "However, having an experimental study on the random features dimensions' impact on empirical compute time vs. approximation performance vs. end-task performance would have been a plus.",
            "6": "I had read \"Rethinking Attention with Performers\" when I reviewed this paper, and I originally thought it was the same paper, but (along with notation being different) they start to differ on page 3.",
            "7": "Where \"Performers\" goes with positive orthogonal random features (to improve over vanilla RFA), this paper adds a gating mechanism: this adds the possibility to learn some monotically decaying attention over older context, similar to learned receptive fields of attention (as in e.g.",
            "8": "[Sukhbaatar et al.",
            "9": "2019]).",
            "10": "Overall, this is a good paper, and I don't see why we should downplay it in light of simultaneous (\"Performers\" got on ArXiV on September 30th, the ICLR deadline was October 2nd) quite similar contribution that the authors took the time to discuss.",
            "11": "(It would be even better if they could compare to it in a future version.)"
        },
        "Db7xKJiZVR": {
            "0": "1.",
            "1": "The logic in the introduction is a bit contradictive to me:\n\nSome are able to achieve better asymptotic complexity (citations).",
            "2": "while it\nis more challenging to improve on shorter sequences: the additional computation steps required by\nsome approaches can overshadow the time and memory they save.",
            "3": "Doesn't this simply mean that for short sequences there is no such computational burden?",
            "4": "I think the story starts with pointing out the importance for long-sequence but turns to the topic on short sequence\nwhich is confusing.",
            "5": "The need for short sequence acceleration needs to be justified IMO.",
            "6": "2.",
            "7": "Following 1, the baseline should be added.",
            "8": "For a fair comparison, I think the baseline should add those methods as claimed in the introduction\n(Lee et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Beltagy et al., 2020, inter alia), (Kitaev et al., 2020; Wang et al.,\n2020; Roy et al., 2020, inter alia) and let us know how badly they performed under the short sequence.",
            "9": "In particular we don't know\nif the sacrifice of short sequence time would benefit a lot in long sequences for existing methods.",
            "10": "The current experimental baseline can't reflect this.",
            "11": "3.",
            "12": "Is the speedup over total computational time or just the attention part?",
            "13": "To best of my knowledge, under many circumstances in particular for short sequence, attention alone might not be the \nmost time-consuming part of the model.",
            "14": "I think it will be helpful for authors to have a complete graph of the computational model used instead of only figure 1 concept graph.",
            "15": "Specifically, is there\nany feed-forward computation involved and how many layers of the models used in comparison.",
            "16": "4.",
            "17": "Introduction of Eq 6,7 is confusing.",
            "18": "Up to eq 5 it's clear whatr's going, but it comes from nowhere to intorduce these 2 modules in eq 6,7.",
            "19": "So my understanding is that\nRFA simply refers to the approximation of computing the softmax.",
            "20": "So the statement:\n\nfor softmax-attention.",
            "21": "The latter is typically used in two different ways in the transformer architecture, each resulting in a different computation for RF\n\nis confusing as the RFA is now redefined.",
            "22": "I believe RFA should only refer one thing and I don't think eq(6) and eq(5) leads to the\nsame result.",
            "23": "On the other hand, eq 7 should be the same as eq 5.",
            "24": "Is this correct?",
            "25": "In addition, the notation in (6) looks wrong to me.",
            "26": "\\phi(x) as introduced in eq 2 is in R^{2D} but S_{t-1} is in R{D}, not sure what does + mean in this context.",
            "27": "I couldn't find out where you properly define the meaning of D either.",
            "28": "5.",
            "29": "Clarification of contribution\n\nEq 6,7 reads like RNN style update but the intuition is lacking.",
            "30": "Do you want to claim that this structure design is inspired by RNN and it leads to a better result?",
            "31": "Put in another way, using RFA in transformer is from Rawat et al., 19 so do you think your major contribution is to design such \na gated usage of RFA?",
            "32": "5.",
            "33": "Discussion of D\n\nSince RF is not the major contribution, you summarize existing results of FA in sec2.2.",
            "34": "I think I'd like to see a discussion of\nsufficient number D analytically or empirically.",
            "35": "Could you also cite the convergence bound on this approximation?",
            "36": "To me, D looks to be an important efficiency tradeoff.",
            "37": "Say sequence length is M\nand feature in d dimension.",
            "38": "Original Attention is O(M^2 d).",
            "39": "The computation of RFA\nrequires outer product, which is O(D^2d) so overall it's O(M D^2 d), if M is around 64 or 128 (common usage) and D is 64, I actually\ndon't see why RFA could improve 2x.",
            "40": "Do you pre-compute and pre-store anything?",
            "41": "6.",
            "42": "Time analysis on language modeling is not presented.",
            "43": "Since it's a efficiency paper, I think it should be complete.",
            "44": "Overall, I think the paper provides an interesting view of discussion, but there are many flaws in the current version which needs to be corrected before a more serious consideration.",
            "45": "Especially, in terms novelty, the paper is relatively limited as the RF is explored in Rawat et al., 19.",
            "46": "So my point 5 is important to answer and I would like to see all the details are clarified in order to make the contribution stronger."
        },
        "UsRcQtQCt_F": {
            "0": "Summary\n* This paper proposes a linear time and space attention variant that matches (or exceeds) the accuracy of standard attention while maintaining the speedup of prior work in linear time/space attention.",
            "1": "* The approach is centered around a linear approximation of softmax attention, and is extended with a gating mechanism similar to a GRU.",
            "2": "* The accuracy improvements from the gated extension are demonstrated via language modeling on WikiText-103, while the speed-ups are demonstrated on translation.",
            "3": "Contributions\n* Demonstrates the importance of the choice of kernel in RFA, as prior results from Katharopoulos et.",
            "4": "al [1] underperforms softmax attention, while this work gets comparable performance by changing the kernel.",
            "5": "* Proposes an extension of RFA with gating that improves accuracy on language modeling, relative to softmax attention.",
            "6": "Strengths\n* The RFA-gated formulation is both more accurate and potentially faster (at least for decoding) than softmax attention, as demonstrated on language modeling.",
            "7": "* The writing was clear and thorough.",
            "8": "* The experiments support the claims of improved accuracy via choice of kernel and gating, and preservation of speedups inherited from the linear attention formulation.",
            "9": "Weaknesses\n*  The improved accuracy and speed over the baseline transformer are great, and the experiments serve to nicely illustrate those independently.",
            "10": "I would like to see a third application where these two qualities are demonstrated together.",
            "11": "* The gating mechanism could also be applied to the output of softmax attention, but that comparison is not included.",
            "12": "Please correct me if this is incorrect.",
            "13": "Recommendation: Weak Accept\n* Well-written and timely exploration of linear attention.",
            "14": "* Empirical results demonstrate accuracy improvement over softmax attention, while preserving the linear time complexity.",
            "15": "* Extension of RFA with a gating mechanism appears to be effective, but I do not believe the claim that it is hard to apply this to softmax attention is valid, leaving the main contribution an exploration of a couple different kernels for linear attention.",
            "16": "Questions\n* Can you clarify how the RFA defined in section 3.1 differs from the linear attention in Katharopoulos et.",
            "17": "al.",
            "18": "[1]?",
            "19": "What is different other than the choice of phi?",
            "20": "* Associative reductions such as those described in sections 3.1 and 3.2 can be computed in time logarithmic in sequence length (at the cost of n log n memory consumption) on a parallel device using a binary reduction or the prefix sum trick.",
            "21": "Would this result in speed gains, or is the performance of attention in the parallelizable softmax setting already saturated?",
            "22": "* Are there experiments with a bidirectional gated RFA in the conditional setting, i.e.",
            "23": "in the encoder for translation?",
            "24": "Nits\n* The formatting of equation 5 is a bit strange, as the start of the equation is inline while the rest is not.",
            "25": "[1] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret.",
            "26": "Transformers are rnns: Fast autoregressive transformers with linear attention.",
            "27": "In Proc.",
            "28": "of ICML, 2020.",
            "29": "Edit: I have updated my rating based on author response."
        }
    },
    "dKwmCtp6YI": {
        "ZTDrA_qwKLm": {
            "0": "The paper provides an empirical investigation of an important problem: the transferability of language modeling signals across languages in the transformer model.",
            "1": "This is an important question because it can teach us both on the relations between languages and the properties of the transformer model (although it is not easy to tease the two effects apart).",
            "2": "This is a thorough paper with a large number of experiments and with interesting conclusions that nicely generalize the low level patterns observed in the experiments.",
            "3": "These conclusions are likely to be useful for the research community as part of its on going investigation of language transfer and the transformer model.",
            "4": "I have several comments though:\n\n1.",
            "5": "The language of the paper is often very complicated.",
            "6": "Just as a couple of examples: It was very hard for me to follow the abstract, the first paragraph, the (very long) sentence that start with \"in order to eliminate\" (1.1), item 3 in the list of contributions and this is just a partial list.",
            "7": "I ask that if the paper is accepted the authors will try to improve this aspect.",
            "8": "2.",
            "9": "The writing is often over pedagogical and I often got the feeling that the authors try to educate their readers (but not in the positive sense of the word).",
            "10": "I would try to avoid this style.",
            "11": "3.",
            "12": "This work seems highly relevant to the following paper:\n\n\"Towards Zero-shot Language Modeling.\"",
            "13": "Edoardo Maria Ponti, Ivan Vulic,Ryan Cotterell, Roi Reichart and Anna Korhonen .",
            "14": "EMNLP 2019\n\nI think the discussion parts can gain form comparing the conclusions of the two papers, when relevant."
        },
        "6vp_hHxTzOu": {
            "0": "The paper investigates whether languages are equally hard to Conditional-Language-Model (CLM).",
            "1": "To do this, the authors perform controlled experiments by modeling text from parallel data from 6 typologically diverse languages.",
            "2": "They pair the languages and perform experiments in 30 directions with Transformers, and compare 3 different unit representations: characters, bytes, and word-level (BPE).",
            "3": "I appreciate the authors' effort for their systematic controlled experiments.",
            "4": "However, I'm leaning towards rejecting this paper since I think some of the claims made in the paper are too strong and not really backed up by their experiments.",
            "5": "Some comments:\n* The term \"Conditional-Language-Model\" can be misleading, since this paper model a target language conditioned on a source language, so more like in a machine translation setting rather than standard language modeling setting where you can also condition on the previous history.",
            "6": "* I'm also not sure if comparing perplexity by conditioning on another different language (source language) is correct.",
            "7": "The experiments would be clearer if done with standard LM with Transformers encoder model like BERT for example.",
            "8": "* At the end of Section 2, the authors mention about \"generalizations\", but I couldn't really find any discussion about this in the paper.",
            "9": "Maybe this can be clarified?",
            "10": "* I found that claiming script bias in character models is too strong, if the experiment only shows bias in ZH (and this is expected since its character-level has different notion with languages with Latin script).",
            "11": "* Byte-level: I found that there is still some \"erraticity\" in Figure 3(b) especially when the data size increases (which is more practical in real world application), so this is not entirely resolved?",
            "12": "* I also think the summary in Section 1.2 stating that linguistic typological information is not necessary given \"statistical properties concerning sequence length and vocabulary\" is not necessarily valid since these two properties are the results from linguistic typology information.",
            "13": "Missing references:\n1.",
            "14": "From characters to words to in between: Do we capture morphology?",
            "15": "Clara Vania and Adam Lopez.",
            "16": "ACL 2017.",
            "17": "2.",
            "18": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss.",
            "19": "Barbara Plank, Anders Søgaard and Yoav Goldberg.",
            "20": "ACL 2016."
        },
        "XFM-CP46DgZ": {
            "0": "This paper is trying to answer an important question: How does representation play a role in carrying meanings?",
            "1": "In doing so, the authors experimented with 6 languages in  3 + 5 kinds of representations.",
            "2": "The authors concluded that the different performances among language pairs maybe be the result of word segmentation in different ways.",
            "3": "This is an interesting step towards understanding meaning representations, especially in languages that do not have an alphabet.",
            "4": "However,  as much as I agree with some of the final conclusions, the soundness of the experiments appears to be in question.",
            "5": "My main concerns are about the additional experiments with Chinese.",
            "6": "The authors claimed that \"On the character level, target language ZH (ZHtrg) shows a different learning pattern throughout.\"",
            "7": "There are two types of character-level representations used: Wubi and Pinyin.",
            "8": "Wubi was originally invented for professional typesetters so that they can type fast.",
            "9": "The segmentation may not be correlated with the meaning of the word at all, as claimed by the papers cited by the authors.",
            "10": "Pinyin, on the other hand, is highly ambiguous.",
            "11": "One pinyin may representation dozens of words and the authors did not take tones into considerations at all.",
            "12": "The author also mentioned that \"After filtering length to 300 characters maximum per line in parallel for the 6 languages, we made 3 subsets of the data with 1 million lines each\".",
            "13": "Each language carries meaning differently and the information density is drastically different.",
            "14": "300 characters in Chinese carry much more information than 300 characters in English.",
            "15": "This is an unfair comparison.",
            "16": "It would make a lot more sense if the authors treat each language differently because of their orthographic differences."
        },
        "zh4Xv5Ynj_D": {
            "0": "Summary: The authors attempt to investigate to what extent languages are hard to conditionally language-model.",
            "1": "They do this by using some information theoretic measures.",
            "2": "Claims:\n- There are no statistically significant differences between source language representations, but there are significant difference between pairs of target language representations.",
            "3": "- There is no complexity that intrinsic to a language except its statistical properties concerning sequence length and vocabulary (unless word-based methods are used).",
            "4": "- They also observe phenomena such as Double Descent and erraticity.",
            "5": "----\nStrengths:\n- The Experiments are extensive.",
            "6": "- The relative similarity of source language representations is interesting and worth exploring further.",
            "7": "Weaknesses: \n- The diagrams are difficult to read\n- The paper is hard to follow and would benefit from a clearer focus rather than the broad range of topics covered here.",
            "8": "For example:\n    - It is difficult to understand what the methods/terms (the information theoretic measure used, double descent) are - little time is spent explaining these.",
            "9": "- Double descent is discussed in the paper but it is still made not clear why this is relevant in the paper.",
            "10": "- Several portions of text are repeated - with some editing, space can be made to discuss concepts important to the paper\n- The authors make recommendations for modeling (Eg.",
            "11": "using char level or byte level models for certain models - which have been extensively studied for this): this is not followed up with any concrete results on translation/downstream tasks or pointing out relevant work."
        }
    },
    "kLbhLJ8OT12": {
        "ZOwNZ0CqOuV": {
            "0": "Summary: \nThis paper proposes modeling the hierarchical structure between dialog policy and natural language generator with option network and train it with HRL.",
            "1": "It also introduces a discriminator modeled with language models as an additional reward, which further improves the learning procedure's comprehensibility.",
            "2": "Besides, this paper has demonstrated the interpretability of the latent dialog act via clustering methods.",
            "3": "Pros: \n1.",
            "4": "This paper proposed formulating dialog policy as a high-level policy over dialog act and NLG as a low-level policy and train the word policy module using HRL.",
            "5": "It is another move towards improving policy learning via HRL.",
            "6": "2.",
            "7": "The proposed model achieves SOTA performance in the policy optimization leaderboard on the MultiWOZ dataset.",
            "8": "3.",
            "9": "It is interesting to see that when formulated as latent factors, the distribution of dialog acts still shows a clustering pattern, which manifests the model's interpretability.",
            "10": "4.",
            "11": "This paper introduced an additional reward using a discriminator of language models.",
            "12": "The ablation study shows that this reward can be useful (though not significant).",
            "13": "Cons:\n\nThe proposed HRL approach is a direct application of the option framework on a task-oriented dialog system.",
            "14": "It is similar to the hierarchical structure in the open-domain dialog system.",
            "15": "Minor issues:\n1.",
            "16": "Introduction paragraph 2: When introducing end2end models, there is a lack of citations of recent e2e models including DAMD, SimpleTOD, SOLOIST, etc.",
            "17": "2.",
            "18": "Abstract Line 9:  o gur -> our"
        },
        "FyZ6NxC2cC4": {
            "0": "This paper attempts to model task-oriented dialogue system using hierarchical reinforcement learning between the actions policy and natural language generation system.",
            "1": "The utterances are encoded with GRU cells, the action policy is one-layer linear model and outputs the mean and variance of a multivariate Gaussian distribution, and the NLG is an LSTM decoder.",
            "2": "The paper is well written, easy to follow, and adequately motivated.",
            "3": "Pros:\n\nHierarchical RL formulation with options for joint action policy and natural language generation\n\nThe paper proposes asynchronous updates between dialogue policy and NLG to theoretically guarantee their convergence to a local maximizer.",
            "4": "It also proposes using a language models as a discriminator model for reward assignment to further improve the comprehensibility of generated responses.",
            "5": "Pre-training of task-oriented dialog models with  variational bayes similar to VHRED (Serban et al., 2017).",
            "6": "Experiments show significant performance improvement over the baselines with both automatic and human evaluations.",
            "7": "Also, the results show that continuous representation of the action policy in HDNO performs better than the discrete representation in the LaRL, which is very interesting and informative.",
            "8": "Cons:\nThe use of an oracle dialogue state and an oracle database search result.",
            "9": "This is a major drawback of this work.",
            "10": "Recent work in this space are now considering imperfect dialogue state and the corresponding database search results.",
            "11": "There is also an increasing need to consider non-trivial database interactions such as booking for more practical applications.",
            "12": "The evaluation/comparison using updated official evaluator may be missing for some of the more recent work e.g..,\n\nGPT-2 (Budzianowski and Vulic 2019)\nStructured Fusion (Mehri, Srinivasan, and Eskenazi 2019) SOLOIST (Peng et al.",
            "13": "2020)\nDSTC8 Track 1 Winner (Ham et al.",
            "14": "2020)\nSimpleTOD (Hosseini-Asl et al.",
            "15": "2020)\n\n\nQuestions:\n\nWhat do you mean by “Distinguished from a conventional modular system, we additionally give a context to NLG to satisfy the option framework.”?",
            "16": "You mean the dialogue context is equivalent to the state space in HDNO formulation?",
            "17": "How are the oracle dialogue state and an oracle database search results encoded?",
            "18": "What kind of model is used for the language model discriminator?",
            "19": "LSTM?",
            "20": "How did you handle the action policy toward the database for booking, which can cause database mutation?",
            "21": "Are those ignored?",
            "22": "Are the results in Table 5 based on the updated official evaluator similar to Table 1?",
            "23": "If not, then the comparison in Table 5 is not apples to apples."
        },
        "OIrZP6msnCS": {
            "0": "Summary\nThe paper looks the problem of lack of comprehensibility that arises when we use RL to train a E2E dialog system to maximise a given reward function.",
            "1": "The paper proposes a  HRL/options framework based method to learn a dialog policy over learned latent dialog acts which can then guide the lower level NLG.",
            "2": "This along with a regularization reward using language model the paper aims to improve comprehensibility.",
            "3": "The show improved performance in MultiWoz dataset.",
            "4": "Strengths\n1.",
            "5": "The paper looks at a very relevant problem in dialog research.",
            "6": "The ability to use RL along with SL and being able to use RL without compromising on comprehensibility.",
            "7": "2.",
            "8": "The paper proposes Options framework based method for using the hierarchical structure in dialog to learn the dialog policy and NLG in a hierarchical fashion.",
            "9": "3.",
            "10": "They provide a training that guarantees convergence to local maxima.",
            "11": "4.",
            "12": "They show improved total performance in MultiWoz dataset compared to recent, relevant baselines.",
            "13": "Weakness/Comments/Questions\n1.",
            "14": "The paper starts with the motivation of handling comprehensibility.",
            "15": "More discussion is required on a) what are the reasons of loss in comprehensibility in this case (it is briefly mentioned in the intro) b) why their individual design choices and how they handles the different reasons c) some evaluation to verify this\n2.",
            "16": "It would helpful to place it more clearly where the contribution of the paper lies in the related work.",
            "17": "My understanding is that contribution of the paper is in exploring using options framework to goal-oriented dialog to handle the issue in question.",
            "18": "HRL in general has been used previously for goal-oriented dialog, using language models to regularize RL models has been used and pertaining using SL is widely used.",
            "19": "If there are particular differences in the above, it would nice to clearly state them and also say why the different choices and verify if the different choices are beneficial compared to the previous ones.",
            "20": "3.",
            "21": "Relevant to some of the above points.",
            "22": "It is not clear to me why we cannot use HDSA+R or LARL + NLG + language model reward.",
            "23": "Not necessarily these particular combinations.",
            "24": "Some discussion on the current design choices and why making the proposed methods features to some of the other baselines is not a way to achieve some benefits is not the right way to do it.",
            "25": "It's ok for the proposed method to be one particular way, but that discussion would be useful.",
            "26": "4.",
            "27": "Clarification on the task setting: Is it the case that the agent's current utterance does not decide what the next user utterance is?",
            "28": "i,.e the agent is given the ground truth context every time and asked to predict the correct next utterance.",
            "29": "That prediction does not affect the way in which the overall dialog goes?",
            "30": "If that is the case, that should be made more explicit.",
            "31": "In that case, the dialog policy learned is more of a contextual bandit setting.",
            "32": "The complexity of learning options would be way different in the two different settings.",
            "33": "Just wanted to understand it.",
            "34": "5.",
            "35": "In the 6.2.3 visualization of clusters, it would be very useful to have a visualization of clusters from some baselines on other ways of learning.",
            "36": "To see if this is due to some new addition by the paper or is generally present.",
            "37": "6.",
            "38": "There are several parts to the method and there are I assume several differences in the architecture etc with baselines etc.",
            "39": "It would be really useful to have an ablation study to disentangle which piece of the training or method is contributing to what and how in the performance measure.",
            "40": "Otherwise for example it is not clear to me if the improvement in Blue compared to LaRL comes from the extra reward using the language model or from the options framework.",
            "41": "What happens when the extra reward for using the language model is added to LaRL (that might be tough if you have to modify others code).",
            "42": "what part of the performance is coming from pretraining (especially if using VAE type is novel, then quantifying that is important with and without VAE type SL), etc.",
            "43": "Questions to authors\n1.",
            "44": "It would be great if you could respond to some of the above comments.",
            "45": "Thanks.",
            "46": "Minor\n1.",
            "47": "Typo in the abstract: *In our work, we\n2.",
            "48": "Move to E2E system can be motivated a bit more (allows end-user feedback to be passed through all modules easily and don't have to worry about how a change in one module affects all other modules explicitly etc)\n3.",
            "49": "Might be useful to define what is exactly meant by 'comprehensibility'\n4.",
            "50": "The intro says \"the goal is absolutely clear\".",
            "51": "Not sure if that is really true.",
            "52": "For example, it's not clear what the correct reward function to provide to the RL agent is.",
            "53": "As the paper points out, the success rate alone is not enough.",
            "54": "The dialog needs to be polite, follow natural language, short, etc which are hard to automatically measure.",
            "55": "Human evaluation is costly and could also have bias like the paper points out.",
            "56": "Just trying to say that automatic evaluation of dialog systems is a hard problem.",
            "57": "5.",
            "58": "I am just curious why you use GRU for the encoder and LSTM for the decoder?",
            "59": "After author response\nThe authors have responded to most of my questions/concerns satisfactorily.",
            "60": "Changing my score from 4 to 6"
        },
        "NhXcWZ-3hkb": {
            "0": "Summary\n=========\nAuthors applied reinforcement learning framework to the problem of task-oriented dialog.",
            "1": "In particular, they used the option framework to represent the connection between the dialog policy and the natural language generation.",
            "2": "Theoretically, they showed that synchronized updates to the low-level and high-level policy may never converge, yet asynchronized updates guarantees convergence.",
            "3": "Authors also used a discriminator reward signal to cope with sparse reward (dialog success rate) and better representation of the human evaluation.",
            "4": "Empirical results on MultiWoz 2 and 2.1 shows improvement over other state-of-the-art techniques.",
            "5": "Summary:\n+ Appealing theoretical contributions\n+ Empirical results are encouraging\n+ The use of discriminator for reward shaping in addition to task success rate is interesting \n- Writing and explanation can be improved.",
            "6": "There are few places (see details) that authors have assumptions in mind but do not provide those assumptions until later.",
            "7": "Would be great to state them upfront to avoid confusion.",
            "8": "- The original option framework assumes given options.",
            "9": "Given the ending of the paper, I interpreted that the set of dialog acts (i.e.",
            "10": "options) are learnt automatically but I could not find this to be communicated explicitly.",
            "11": "Questions:\nDid you look into the quality of dialogs specifically?",
            "12": "In some of our recent experiments we found out that those automatic metrics do not necessarily correspond to great user experience.",
            "13": "Details\n=========\nAbstract: In o gur work => In our work\nP5: \"oracle dialogue state\": What is the oracle dialog state and how is it calculated?",
            "14": "The world oracle conveys the meaning of being absolute truth which sounds a bit unexpected.",
            "15": "The same comment is applicable to oracle database (DB) results.",
            "16": "Is it possible in your system you queried the DB with wrong parameter?",
            "17": "If yes, do you still name the output of the DB as oracle results?",
            "18": "=> Ah.",
            "19": "you explained this in page 6, in Task Description.",
            "20": "I highly recommend bringing this assumption earlier to avoid readers confusion.",
            "21": "P5: Section 4.4: I am still eager to know how you select your dialog actions.",
            "22": "Would be great to tell your reader earlier."
        }
    },
    "PS3IMnScugk": {
        "AWIkIvYdf15": {
            "0": "####Summary: \nTo tackle situations where compositionality is mostly required at inference time, the paper proposes a novel data augmentation method with an RNN based generator (recombination); to make the generator generate highly compositional patterns, the paper proposes a resampling method.",
            "1": "The methods have been tested on two benchmarks focusing on the issue, SCAN and morphological analysis.",
            "2": "The system performs on par with recently proposed GECA for SCAN and favorably to GECA on morphological analysis.",
            "3": "The two datasets have some “toy flavor”, while SCAN great favors example combination (with recomb-2 performs much better than recomb-1), recomb-1 seem to perform better for morphological analysis dataset, leaving questions about how to choose the exact models in general.",
            "4": "####Pros: \nThe paper proposes the first RNN based neural generator to perform data augmentation for “extreme” compositionality inference.",
            "5": "The paper has explained and empirically showed that this learned generator needs a resampler.",
            "6": "With these two elements, the approach performs on par with recently proposed GECA (where the data is not augmented via a neural generator) on two datasets.",
            "7": "####Question: \n1.",
            "8": "Comparison with GECA: I can read from the paper that the performance is on par with GECA.",
            "9": "However, I am unable to grasp nuances, leaving important questions untouched such as: In what scenarios do we expect the model to perform better than GECA?",
            "10": "In experimental details, the slightly better performance in Table 2, can it be attributed to finer generation powered by the RNN generator?",
            "11": "Why does Recomb-2 perform less well than GECA in SCAN?",
            "12": "2.",
            "13": "A uniform framework for resampling\nDifferent recombinations perform more or less favorably across different datasets.",
            "14": "While the exact choice depends on the dataset characteristics, a framework will be more attractive if it can perform well on different scenarios.",
            "15": "Could the authors list some possible approaches to automatically choose this hyper-parameter please?",
            "16": "####Minor Comments: \nThe paper mentions in several places symbolic scaffolding without citations, literature is certainly rich here, e.g.",
            "17": "[1,2] are papers integrating symbolic constraints for semantic parsing.",
            "18": "There are also neural architectures that particularly target to ensure some symbolic famous properties such as [3].",
            "19": "The authors say in the introduction that the approach (Andreas, 2020) is task specific which seems not correct.",
            "20": "In fact, it can be applied to a large range of NLP problems (e.g.",
            "21": "all the experiments in this paper compares to the approach GECA))\n\nIn section 3, it says that “the use of a continuous latent variable appears to make no difference”, I would suggest to precise “make no difference” as “make no difference in prediction performance“ as the latent variable can facilitate some generation control shown in (Guu et al.",
            "22": "2018).",
            "23": "In this paper, GECA is first introduced in section 5.",
            "24": "I would recommend to put the citation around it (Andreas, 2000) although previously cited.",
            "25": "[1] Sequence-based structured prediction for semantic parsing, Xiao et al.",
            "26": "2016\n[2] A syntactic neural model for general-purpose code generation, Yin and Neubig 2017\n[3] Making Neural Programming Architectures Generalize via Recursion, Cai et al.",
            "27": "2017\n\n####Authors have engaged in the discussion, clarified questions about the paper and addressed comments in its newest revision.",
            "28": "I have consequently revised my score from 5 to 6."
        },
        "VNmLOclumTt": {
            "0": "Summary:\n* Motivated by the fact that certain datasets require modeling compositional phenomena, the lack of flexibility of highly structured models, and the strong performance of large unstructured models on unstructured data, this paper approaches the problem of getting unstructured models to generalize on compositional data.",
            "1": "* Prior work showed that a simple rule-based data augmentation approach could allow unstructured models to generalize on compositional data.",
            "2": "This paper demonstrates that a learned data augmentation strategy can be as effective at encouraging generalization as a rule-based one.",
            "3": "Contributions:\n* Extends the prototype+edit model (Guu et al 2018) to a recombinator model with multi-source copy attention.",
            "4": "* Proposes a resampling scheme for upweighting rare examples.",
            "5": "* Obtains results comparable to strong rule-based data augmentation baseline GECA on two datasets, demonstrating that the combination of both resampling and recombination is effective.",
            "6": "Strengths:\n* Clearly written.",
            "7": "The method is simple and is broken down cleanly into recombination and resampling.",
            "8": "* Ablation studies support the need for both recombination and resampling.",
            "9": "* Sufficient performance to support the claim.",
            "10": "The approach manages to match the performance of GECA with a learned method, while remaining more flexible.",
            "11": "Weaknesses:\n* Neighbourhood heuristics are one of the last remaining applications of manual rules in the method, but seem necessary for computationally feasible training.",
            "12": "Decision: Accept\n* Problem is important: Whether unstructured models can generalize on structured data has implications for whether or not to move towards more structured models.",
            "13": "This paper provides experimental evidence that unstructured models can generalize on structured data with a data augmentation procedure that uses fewer manually specified rules than previously shown.",
            "14": "This provides a path forward by continuing to iterate on the augmentation procedure.",
            "15": "Questions:\n* Given that GECA is feasible on both datasets, would there be benefit to combining the examples from the learned augmentation strategy with those from GECA?",
            "16": "In other words, do the different augmentation strategies result in orthogonal improvements?",
            "17": "* Why does resampling hurt GECA on SCAN?",
            "18": "Suggestions:\n* Both datasets are quite small.",
            "19": "The story could be strengthened by demonstrating the method scales better than GECA by applying it to a larger dataset as well, such as a translation dataset.",
            "20": "Nit:\n* w is overloaded to both be a value of d (eqns 12, 13, 17) as well as the weighting function (eqns 1, 18)"
        },
        "5nE_CBF5ZLh": {
            "0": "\nSummary:\n\n \nThe paper proposes an interesting approach to systematically generate new examples and augment the training data with these examples.",
            "1": "The goal is to target rare and unseen sequences of text or instructions with this augmentation.",
            "2": "In particular, it proposes learning to copy parts of the reference examples.",
            "3": "The approach is based on prototype-based models where every training example can be explained by at least one other example and a parametric rewriting operation.",
            "4": "They show that these models do not perform well when facing complex (and rare) composition events and propose a recombination addition to address this issue.",
            "5": "I like the idea of systematically augmenting training data targeting rare and unseen subsequences.",
            "6": "I give an \"accept\" to this work because of its novelty and contribution (see pros below).",
            "7": "My minor concern is about the impact and/or usefulness of this work when dealing with real-world datasets and more complicated tasks as well as some clarity issues (see cons below).",
            "8": "Hopefully, the authors can address my concern in the rebuttal period.",
            "9": "##########################################################################\n\nPros:\n\n1.",
            "10": "The paper addresses one of the interesting and important shortcomings of current neural models: the ability to generalize to rare and unseen sequences.",
            "11": "I find this problem important to investigate and applicable to many areas of research.",
            "12": "2.",
            "13": "The proposed approach is flexible and practical to use.",
            "14": "The design of the prototype-based data augmentation method is reasonable and interesting.",
            "15": "3.",
            "16": "This paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed framework.",
            "17": "##########################################################################\n\nCons: \n\n \n1.",
            "18": "In Table 2, we see F1 score for morphological analysis.",
            "19": "It is not entirely clear to me why the results in the NOVEL section of the table is very close to ALL section.",
            "20": "NOVEL shows model accuracy on examples whose exact tag set never appeared in the training data and I expected a bigger gap in the performance.",
            "21": "2.",
            "22": "There is a big gap between the performance of 1- and 2-prototype models.",
            "23": "Do the authors know what explains this gap?",
            "24": "Have the authors explored the higher order of prototype recombinations?",
            "25": "3.",
            "26": "What are the challenges of applying this approach to real-world data sets for instance in machine translation?",
            "27": "I will suggest the authors discuss the implications and possible shortcomings of such an approach when dealing with natural (and potentially long) sequences of text.",
            "28": "The definition of unseen subsequences and compositional learning will be more complex there.",
            "29": "4.",
            "30": "It is not clear to me what the authors mean by \"hints\".",
            "31": "Is the complete sequence counts as a hint?",
            "32": "Or only the subsequence that was identified as rare?",
            "33": "#########################################################################\n\nSome typos: \n\n\n(1) Relevant literature that was not mentioned in this paper:\n         https://arxiv.org/abs/1705.00440\n         https://arxiv.org/abs/1801.02929\n\n(2) Equation (31): tex formatting issue.",
            "34": "(3) Typo in page 15 section G1: Fig.",
            "35": "Table 2 shows -> Table 2 shows"
        },
        "TqhM7h1Ob7f": {
            "0": "This paper presents a prototype-based method for data augmentation based on a generative model without rule/template based requirements.",
            "1": "The generative model creates new input-output pairs from training fragments (recombination: rewrite model conditioned on multiple examples), and samples in low-density places (rare words) of the training data (resampling).",
            "2": "Empirical results show that the in combination recombination and resampling perform on par with a recently introduced rule-based method, GECA.",
            "3": "Experiments are conducted on two compositional generalization tasks: SCAN and sigmorphon.",
            "4": "The paper is very clearly written and motivated, doing a good job in presenting the recent and past pertinent literature.",
            "5": "The problem addressed is of great interest, and the two proposed contributions of resampling and recombination are likely to be useful to further research in data augmentation.",
            "6": "The extension of prototype models to multiple examples is a promising step, but depending on the task leaves open questions.",
            "7": "The empirical results on SCAN are strong.",
            "8": "The results on Sigmorphon are strong in the sense of obtaining comparable accuracy to simple rule-based approach, which itself is very simple and has many incorrect examples it constructs, but do not clearly outperform it.",
            "9": "Granting resampling is a contribution, wouldn’t the proper comparison be GECA resampling with the recomb-1 or -2, since those include resampling as well?",
            "10": "In that case the Sigmorphon performance is much closer to each other for the two methods.",
            "11": "Taking the performance jump from 1 to 2 prototypes on SCAN as potential for further jumps, why restrict to n=2?",
            "12": "And related, why do you think -2 outperforms -1 in some instances on Sigmorphon?"
        }
    },
    "xYJpCgSZff": {
        "CpXDU7BvLs": {
            "0": "This paper propose a counterfactual approach to improve the performance in information extraction tasks and in particular on the rarer classes.",
            "1": "While this is an important research problem, I think the paper has the following issues:\n\n- The overall  approach is best described as a form of a data augmentation based on the model predictions.",
            "2": "Steps 1 to 3 is essentially finding out which part of the input had the most impact on the model prediction by comparing the impact of a token being masked at random, and then steps 4 and 5 use this info to improve the prediction.",
            "3": "This is quite commonly done in the context of model interpretation, e.g.",
            "4": "finding out which words matter the most in a prediction was done in the work on LIME: http://sameersingh.org/files/papers/lime-kdd16.pdf , and by many others that followed this up, e.g.",
            "5": "finding out which words are the most important via removal: https://arxiv.org/abs/1804.07781.",
            "6": "The latter is actually doing what the first three steps do of the proposed approach.",
            "7": "Steps 4 and 5 essentially use this output which presumably captures the contextual information and combine it with the contextual word representation, which is a pretty standard thing to do.",
            "8": "However this existing work in NLP has been ignored.",
            "9": "- The causal framing in Figure 1 appears flawed.",
            "10": "There is no good reason to assume that the NER tag \"causes\" the trigger representation and not the other way around.",
            "11": "However this is a fundamental assumption here.",
            "12": "Also, why is not the contextual word representation \"causing\" the NER tag?",
            "13": "Furthermore, there is no causality inferred here; the model output is post-processed, and the causality is not assessed in any way.",
            "14": "- Few-shot learning has been investigated in the context of information extraction.",
            "15": "Here are some papers: https://www.aclweb.org/anthology/D19-1649/ https://www.aclweb.org/anthology/P19-1589/ .",
            "16": "- The paper is hard to follow.",
            "17": "What is v_r in equation 3?",
            "18": "In section 2.2, it should be explicit that the tokens are not replaced, but removed.",
            "19": "Note that replacement with semantically equivalent words has been explored: http://sameersingh.org/files/papers/sears-acl18.pdf\n\n- There are some vague statements about the novelty vs previous work by Tang et al, but no explicit statement made in the model description.",
            "20": "Eventually it seems to be about considering the syntax (page 7), but this is only used to just select which words to remove as far as I can tell and the GNN over dependency trees which is based on previous work.",
            "21": "- While it is stated that the results of previous work are reproduced, then it is stated that they were not really.",
            "22": "While this is not necessarily the authors' fault (reproducibility can be difficult sometimes), combined with the uncommon choice of metrics and the lack of any comparison to previously reported results in the literature, means that it is impossible to understand where the presented method stands."
        },
        "fnOc03ACQbH": {
            "0": "This paper proposes a novel model integrating both causal inference and structure-aware counterfactual training to enhance the long-tail performances of information extraction.",
            "1": "The causal mechanism considers a structured causal model that takes into account all possible cause-effect relations for the final predictions, including contexts, target representations, POS tags, NERs, etc.",
            "2": "They also implement counterfactual training strategy by selecting the most important factors and wipe off the side effects to enhance the long-tail situations.",
            "3": "The strengths of the paper includes:\n1.",
            "4": "In general, this paper is well-written and easy to follow.",
            "5": "The motivation and the structure are clear.",
            "6": "2.",
            "7": "The ideas of both structured causal model and structure-aware counterfactual training are interesting.",
            "8": "3.",
            "9": "Extensive experiments are conducted to demonstrate the effect of the whole model and each component.",
            "10": "It is interesting to see how different generation of counterfactual examples using dependency structure affect the final performance.",
            "11": "Some improvements could be made:\n1.",
            "12": "If structure is considered, why not try to mask on some dependency relations?",
            "13": "It will be interesting to see the difference between masking words and relations.",
            "14": "2.",
            "15": "What is the effect of using (5) instead of (4) in terms of the experimental result?",
            "16": "Have you conduct such comparison experiment?",
            "17": "And how sensitive is $\\alpha$ to the final performance?",
            "18": "3.",
            "19": "It is also better to demonstrate some qualitative examples on which factors are most important for NER, RE and ED."
        },
        "248WSsjNHfT": {
            "0": "#### Update after author response and other reviewers comments:\nI think after the addition of new evaluation (Appendix A.5) on the aspect of counter-factual and other comments made by reviewers I'll stick to my score.",
            "1": "Also, I liked the idea of applying counter-factual to long-tailed distribution IE problem.",
            "2": "### Summary\nModel performance suffers because of spurious relations present in the data, this is particularly true for long-tailed scenarios.",
            "3": "To overcome this challenge authors introduces counterfactual thinking to IE.",
            "4": "To learn the main effect and ignore the spurious relations (side effects) the paper proposes structured causal model (SCM) with syntax structures using GNN on sentence dependency tree.",
            "5": "Extensive experiments on multiple tasks with different datasets shows significant performance improvement.",
            "6": "### Strong Points\n 1.",
            "7": "Application of counterfactual along with utilization of syntax structures for long-tailed IE is novel and aptly motivating.",
            "8": "2.",
            "9": "I found the idea of measuring the main effect and a way to ignore the \"side effects\" very interesting.",
            "10": "3.",
            "11": "Extensive experiments provides ample evidence that reducing spurious relations leads to improved performance.",
            "12": "### Weak Points\n 1.",
            "13": "How incorporation of the 3rd component (W_{x}X^∗) in Eqn.",
            "14": "5 measures the effects of the entity is not clear to me.",
            "15": "X^* is already taken into account when calculating Y_{x^∗}(S), is it not?",
            "16": "### Other comments\n 1.",
            "17": "The phrase \"**replace** the tokens along the shortest path of the two entities of the relations and generate a new sequence S^∗\" was not really clear to me from the paper.",
            "18": "Could you please explain this part such as replaced with what?",
            "19": "2.",
            "20": "What prompted the authors to consider ≤ 4000 as few scenario for \"OntoNotes5.0\" dataset whereas for all other scenarios it's either 30 or 100?",
            "21": "I feel **4000** is too large for it to be considered \"Few\" for a class.",
            "22": "3.",
            "23": "It's curious to find that GloVe based approaches outperformed BERT based approaches by a large margin in ATIS dataset in NER task (Table 1) and MAVEN dataset for ED task."
        },
        "yKyaqEqPCI8": {
            "0": "The novelty of the paper seems to be in application of the counterfactual analysis to address the long-tailed IE issues, which might be interesting to the IE researchers.",
            "1": "Overall, more theory about the counterfactual generation for IE task should be added, for this is what the novelty of the paper; also, for the rebalancing learning for slide effect and counterfactual, the theory appears to be not enough.",
            "2": "The weak of this work is the theoretical and conceptual underpinnings of the proposed methodology.",
            "3": "Here are my major concerns for  the paper:\nQ1.",
            "4": "The main investigation of the paper is the unstructured text; however, I can see no discussion in the methodology part about how author represent unstructured text in SCM model (representation learning).",
            "5": "Is it unimportant to mention or the SCM model definition is always the same no matter what sources of the data?",
            "6": "Q2.",
            "7": "Section2.2.",
            "8": "Counterfactual Generation, as this forms the main contribution of this work, still no useful information is given in this section, but only a do-calculus is given.",
            "9": "Since this remains at the basic concept of causal inference, how can the paper combine this with IE task?",
            "10": "Q3.",
            "11": "Equation (3) u_i appears to be the representation of the i_th position of a sentence, which I think is important notion across the whole paper, however, in the later illustration after equation(3), u_i didn’t appears again even once.",
            "12": "Why?",
            "13": "I am doubtful about the reproducibility of the paper.",
            "14": "Q5.",
            "15": "How author design the optimization function?",
            "16": "How equation (5) (3) been used?",
            "17": "Q4.",
            "18": "As for the experiment, author use MR and MF1 as the evaluation metrics, since this work can measure causal effect.",
            "19": "More evaluation regarding the causal effect should be added."
        }
    },
    "3k20LAiHYL2": {
        "ePJMLI-Q-Bn": {
            "0": "#### Summary\nThis paper addresses the issue of incorporating commonsense into large pretrained language models.",
            "1": "They propose a pretraining method that does not leverage knowledge graphs but rather use approaches which involve corrupting the input sentence in various ways to recover the original sentence.",
            "2": "This methodology is used to try to bake in commonsense into models.",
            "3": "Results are shown on both discriminative and generative common sense datasets\n\n#### Novelty and clarity\nThe training procedure of corrupting inputs to retrieve outputs is not new but the use on commonsense tasks does seem novel and also is an interesting approach.",
            "4": "The paper was very clear to read and the technical aspects were well described.",
            "5": "#### Strengths\n(1) The use of a self-supervised approach is great because it requires no annotation and the training procedure is simple.",
            "6": "It is also described well\n(2) The variety of baselines used is good and comparison against models larger than the proposed model is interesting to see.",
            "7": "(3) The use of generated sentences to improve language models on hard areas like commonsense and offensiveness is a great idea as it can help make the model more robust\n\n\n#### Weaknesses\n(1) There should be a more comprehensive set of results completed to see how much improvement this model has.",
            "8": "Mainly on the CommonGen class there should be some manual evaluation done similar in the original paper to see if the outputted sentences make sense and that the improvement in automatic metrics is carried over into human evaluation.",
            "9": "(2) A key aspect to look into is the robustness of this model.",
            "10": "In the C2S approach the concepts were shuffled to generate the correct sentence.",
            "11": "During inference time if the concepts were shuffled in a different manner would the model still be able to generate the correct sentences?",
            "12": "There was three random seeds used but as was said \"the performance is sensitive to different random seeds.\"",
            "13": "which seems that the model isn't as robust to newly seen inputs\n\n#### Detailed Comments\nIf spacy was also used for POS tagging along with tokenization this should be made clear.",
            "14": "Also for every sentence was there 3 nouns/verbs extracted?",
            "15": "One thing I'm unclear about is in Table 2 why is the \"Our T5-Base\" better than the \"T5-Base\" above?",
            "16": "Is this T5 with additional epochs?",
            "17": "I think this should be made clear.",
            "18": "Additionally I wouldn't say and \"is only slightly worse than KG-BART.\"",
            "19": "It seems a lot worse especially on BLEU and CIDER.",
            "20": "It is nice to see a smaller model is beating a larger model on some metrics\n\"The difference between CALM and CALM (Joint) is that the former is initialized by the CALM(Mix).\"",
            "21": "Did you mean to say latter instead of former?",
            "22": "Also I don't see CALM (Join) in the table.",
            "23": "I'm assuming this is CALM without Mix warmup\n\n\n#### Questions for the Authors\n1) How did you ensure shuffling the sentences still has grammatical correctness?",
            "24": "A sentence like \"Running I am\" is not grammatically correct.",
            "25": "2) Instead of a POS tagger why did you not use an NER extractor?",
            "26": "Also wouldn't swapping different fruits into sentences like replacing \"Apples grow on trees\" with \"Watermelons grow on trees\" help with robustness\n3) And what point is the generative model good enough that it doesn't help to create distractor sentences"
        },
        "GFaBPjbgCWz": {
            "0": "Summary:\nThis paper proposes the “Concept Aware Language Model” (CALM) --- a pre-trained T5 Transformer is trained on self-supervised intermediate tasks to learn relational commonsense knowledge, before fine-tuning on downstream tasks.",
            "1": "The intermediate tasks include (1) concept to sentence (c2s) generation - given a list of permuted concepts (verbs and nouns), generate the target sequence, (2) concept order recovery (cor) - given a sequence with the order of concepts (verbs and nouns) shuffled, generate the original sequence, (3) given a sequence and it’s perturbed version (concepts shuffled), generate the sequence (classification but as a generation task).",
            "2": "Training is first done in a multi-task fashion, followed by a stage of training that uses generated outputs from (1) and (2) for (3).",
            "3": "The goal of the paper is to show that carefully designed objectives for self-supervised intermediate task training add relational commonsense knowledge which helps improve model performance on downstream commonsense reasoning tasks.",
            "4": "Strengths:\n- S1 - The idea of self-supervised intermediate task training is an exciting one especially in the form of adding reasoning capabilities that BERT, GPT, T5 etc.",
            "5": "might not be acquiring during the large-scale pre-training phase.",
            "6": "- S2 -  As shown by results in Table 1 on 5 commonsense reasoning tasks, the intermediate task training proposed in this work improves performance over and above the T5 model (and variants including with salient span masking for concepts).",
            "7": "- S3 - The experiments involved averaging across 3 random seeds and the authors have reported confidence intervals.",
            "8": "Weaknesses and questions:\n- W1 - One missing baseline is T5 trained to unshuffle entire sequences - given an input with the tokens shuffled, generate the original sequence.",
            "9": "This would show how much value c2s and cor are really adding.",
            "10": "The current T5 baselines are all trained purely for infilling which seems a bit unfair compared to CALM which is generating entire sequences.",
            "11": "- W2 - Given a T5 model that can score sequences (maybe after training on the autoencoding objective), would it score “apples grow on trees” higher than “trees grow on apples”?",
            "12": "If yes, then the model seems to already exhibit this style of reasoning.",
            "13": "Would it score “apples grow on trees” and “apples grow in the ground” similarly?",
            "14": "The distinction here is between sequences that are non-grammatical or unlikely to ever appear versus sequences that may have appeared (eg: “potatoes grow in the ground”).",
            "15": "Presently, (a) it’s unclear if the designed objectives are providing commonsense reasoning above something the model can know from autoregressive language model scoring, and (b) it appears that the objectives are not designed to add relational commonsense knowledge of the sort where we know apples don’t grow in the ground.",
            "16": "- W3 - c2s is designed specifically to do well on CommonGen.",
            "17": "Are the gains on this task smaller than what you might have expected?",
            "18": "If yes, why isn’t it helping more?",
            "19": "- W4 - Figure 4 needs error bars, the dev sets are really small and it’s hard to interpret which differences are significant.",
            "20": "__UPDATE__\n\nThanks for the incredibly detailed response!",
            "21": "I've raised my score to a 8.",
            "22": "I do in general quite like the paper, and the responses here are thought-provoking.",
            "23": "I'm not sure I'm totally convinced by the WSC results comparing CALM the classifier to T5 the sequence scorer.",
            "24": "Not sure if it's an apples to apples comparison...but I'm not sure there's a straightforward setup for this, and perhaps it starts to get beyond the scope of what's being presented here."
        },
        "HD2ykaLN-zh": {
            "0": "This paper suggestsan intermediate training regime that can be used between pretraining and the end-task finetuning.",
            "1": "The authors suggest that their method captures more commonsense knowledge by being focused on capturing knowledge about “concepts”.",
            "2": "Four different denoising objectives—two generative and two discriminative—are proposed and described in detail, with various possible ways of optimizing for all four.",
            "3": "Experimental results show improvements over both the base T5 model and the large T5 model.",
            "4": "The proposed method achieves SoTA results on CommonGen with slightly more than half the parameters of the current SoTA model.",
            "5": "Ablations show the necessity of applying a 2-step intermediate training scheme with mixed training followed by joint training.",
            "6": "CALM shows better results with less data than the base model.",
            "7": "Strengths:\n- Unifying generative and contrastive training is an important and interesting goal.",
            "8": "- The objectives suggested are cheap to compute and seem to increase the signal available in the data.",
            "9": "- Extensive results show improvements over a base model and a larger model across a range of tasks.",
            "10": "- Performance with relatively little finetuning data are encouraging.",
            "11": "Weaknesses:\n- Somewhat weaker results on some CommonGen metrics are disappointing.",
            "12": "- Using “concept” to stand in for verbs and nouns is somewhat confusing.",
            "13": "After reading other reviews and the authors’ responses to all of the reviewers, I recommend this paper by accepted—extensive results show that the CALM objectives offer more signal from data than current pretraining methods.",
            "14": "This paper suggests a number of cheap-to-compute corruptions of the input data that, when used to reconstruct the input, enrich the underlying model.",
            "15": "These objectives certainly improve over the original T5 base _and_ larges models that are used as initializations, and especially outperform the base model in the low-data regime.",
            "16": "The authors use objectives which capture both generative and discriminative information, which some have suggested contain mutually beneficial signal but have not been unified in a single training method.",
            "17": "Below are two paragraphs from my original review.",
            "18": "The authors have done further experiments and show that there are still gains on these tasks when model sized is increased significantly.",
            "19": "Furthermore, they have clarified that on the key metric of CommonGen they achieved SoTA with only slightly more than half the parameters of the current SoTA model.",
            "20": "I therefore believe that these results show merit.",
            "21": "> However, in every task except CommonGEN the authors do not discuss any methods that are even close to the state of the art.",
            "22": "For CSQA, the best number in this paper is 63.32 vs. 79.5 on the current leaderboard.",
            "23": "Similar numbers are true for the rest of the tasks: 60.90 vs. 87 for OBQA, 71.01 vs. 90 for PIQA, and 63.20 vs.  89.70 for aNLI.",
            "24": "I do not believe that SoTA results are necessary to write a good paper, and indeed the obsession our field has with SoTA is unhealthy.",
            "25": "Yet, it is difficult for me to trust that the effects in this paper will generalize to better performing models without further evidence: what if the CALM intermediate objectives only help with mistakes that larger models do not make in the first place?",
            "26": "> On the generative task CALM performs closer to SOTA, but it improves only slightly on T5.",
            "27": "This is especially disappointing as the objectives introduced _directly_ match the task in CommonGEN, making this intermediate training a form of noisy training data rather than pretraining.",
            "28": "I still feel that the authors’ use of “concept” and “commonsense” is vague, when their method can be defined more clearly with more mundane terminology.",
            "29": "In practice, the authors use nouns and verbs as their concepts, which is fine in terms of pretraining objectives, but surely does not capture the generality of concepts.",
            "30": "The authors have somewhat clarified in this in their updated version.",
            "31": "Finally, the CALM intermediate objectives share many properties with all of the datasets tested on and are likely calibrating the model to the kind of correlations they should expect to predict in advance of finetuning.",
            "32": "One way this can be seen is  that the slopes of the T5 and CALM lines are very similar after an initial “bump” which T5 likely needs to calibrate to the new distribution.",
            "33": "This makes claims of learning “commonsense” hard to verify, though I do agree that _something_ relevant to solving these problems is clearly being learned.",
            "34": "Altogether, I think this paper makes an interesting contribution to the question of: How can we get the most pretraining signal from unstructured data using off-the-shelf tools?",
            "35": "I recommend this paper for acceptance, though I encourage the authors to revise their paper to make this the focus of the story, rather than the vaguely defined notion of “concept”."
        },
        "0AAGc4nty-t": {
            "0": "This paper proposes two self-supervised pre-training tasks to further pre-train a pre-trained language model for commonsense reasoning.",
            "1": "The first task is called concept-to-sentence generation, which reconstructs input sentences from noun/verb phrases extracted from the input.",
            "2": "The second task is called concept order recovering, which predicts original sentences after shuffling the order of noun/verb phrases in input sentences.",
            "3": "Experimental results show that the pre-trained language models fine-tuned with the two proposed tasks can lead to improvement on five commonsense reasoning benchmark datasets.",
            "4": "Strengths: \n\n+ The idea of teaching language models through self-supervised learning tasks is neat.",
            "5": "+ The performance of the proposed methods on few training examples looks great.",
            "6": "+ The results section is well structured.",
            "7": "There are ablation studies on the training objectives of each proposed task as well as a comparison of generated sentences.",
            "8": "Concerns: \n\n- The key concern about the paper is the lack of rigorous experimentation to study the effectiveness of the two self-supervised learning tasks.",
            "9": "First, the methods are only compared with T5-base related methods on the four commonsense classification tasks.",
            "10": "The leaderboard of commonsense QA shows that more than 20 systems report an accuracy higher than 63.32, which is the best configuration of the proposed method.",
            "11": "Second, the proposed tasks are applied only to T5.",
            "12": "I am wondering if it is effective on the other pre-trained language models.",
            "13": "Third, the performance improvement on the classification tasks appears marginal.",
            "14": "Statistical tests are desirable to show if such improvements are significant.",
            "15": "- Noun and verb phrases extracted from sentences are not always concepts.",
            "16": "Masking-out certain words in the input is similar to the idea of removing non-content words from input.",
            "17": "A deeper analysis of the proposed method would have been nice to understand which part is effective in the new task, keeping content words or not using mask-out, what if only concepts from a knowledge base is kept instead of content words?",
            "18": "-  The CALM model proposed in this work performs worse than the SOTA models in three out of four metrics on CommonGen, despite it uses less model parameters.",
            "19": "What if the proposed tasks are applied on T5-Large?",
            "20": "Minor comments: \n\n* I am not convinced that the generated corrupted sentences are in general grammatically correct, as stated in Sec.",
            "21": "2.1, \n\n* I do not see a strong connection between completing COR and compositional reasoning, as stated in Sec.",
            "22": "2.1.",
            "23": "* The way of getting distractor sentences appears ad-hoc, may need further justification.",
            "24": "* Y in equation (5) and (6) needs explanation."
        }
    },
    "dzZaIeG9-fW": {
        "gU4Epd_Vxg": {
            "0": "### Summary ###\nThe paper presents a technique for inference of certain kinds of program invariants directly from the program’s source code.",
            "1": "The basic idea is to treat conditional statements as hints for facts about the program state that should hold at a given program point.",
            "2": "### Strengths ###\n\n* This is a challenging problem and the paper shows some successful examples.",
            "3": "The idea that some useful invariants can be inferred based on local information, while not new, is interesting and can lead to follow up work of practical value.",
            "4": "* The contrastive hinge loss of syntactically close but semantically opposite statements is interesting.",
            "5": "### Weaknesses ###\n\n* The paper falls short on the framing of the invariant inference problem, and on the technical details of what does it mean to infer a meaningful local invariant.",
            "6": "Starting from trivialities like the fact that the problem is generally undecidable (and not as stated in Section 2), through the use of incorrect terminology for invariants, guards, pre/post conditions, etc.",
            "7": "This just makes the paper hard to follow.",
            "8": "* Fundamentally, beyond simple invariants (array bounds, nullness checks) it is not clear why program invariants would generalize well across different programs.",
            "9": "The exception is of course the use of libraries and invariants in library contracts (as learned in [PLDI19a, PLDI19b]).",
            "10": "For nullness guards, you should take a look at [https://arxiv.org/pdf/1902.06111.pdf].",
            "11": "I think it would improve the paper if you could focus on a certain kind of invariants, and show that these invariants can in fact generalize across programs.",
            "12": "* As a concrete example, take your own Figure 1.",
            "13": "Assuming that these are two different programs, there is no reason to assume that the contract of `calculateTime()` remains the same.",
            "14": "Had `calculateTime()` been part of some standard library shared between programs, the case for generalization would have been much stronger.",
            "15": "* There has been so much work on static inference of invariants that it is impossible to list even all the closely related work.",
            "16": "Some things that are worth looking into are the work on Scalable static analysis [Scaling], the inference of necessary preconditions [Logozzo], and bug detection that is based on \"belief\" [deviant, belief], which is closely related to your intuition about naturalness and human-written invariants.",
            "17": "Also helpful to look at [loopInvariant] and the related work mentioned there.",
            "18": "* Comparison to Daikon.",
            "19": "As you correctly point out, Daikon infers likely pre/postconditions.",
            "20": "The description of how you compare your invariants to those inferred by Daikon is not clear unless all relevant cases related to (pre)conditions on method parameters.",
            "21": "### Questions for Authors ###\n\n* It would be helpful to see more characteristics of the real missing if conditions that you have collected.",
            "22": "I am wondering if these are simple conditions of the kind of missing nullness checks or missing array-bound checks.",
            "23": "The way in which you have collected these samples is likely to create a bias towards simple missing conditions.",
            "24": "How many terms are in these conditions?",
            "25": "How many of them are nullness checks?",
            "26": "How many are array-bound checks?",
            "27": "How many include simple string operations and/or other simple method calls as implied by Table 2?",
            "28": "### Improving the Paper ###\n\n1.",
            "29": "I liked the idea of removing conditionals to infer likely necessary preconditions.",
            "30": "It would help to clarify when what you predict is a guard, a precondition, an invariant, or something else.",
            "31": "2.",
            "32": "You are clearly not trying to infer any loop invariants, and it would help clarify that upfront.",
            "33": "### References ###\n\n[PLDI19a] Scalable taint specification inference with big code\nhttps://dl.acm.org/doi/10.1145/3314221.3314648\n\n[PLDI19b] Unsupervised learning of API aliasing specifications\nhttps://dl.acm.org/doi/10.1145/3314221.3314640\n\n[Scaling] Scaling static analyses at Facebook\nhttps://dl.acm.org/doi/10.1145/3338112\n\n[Logozzo] Automatic inference of necessary preconditions\nhttps://link.springer.com/chapter/10.1007/978-3-642-35873-9_10\n\n[deviant] Bugs as deviant behavior: a general approach to inferring errors in systems code\nhttps://dl.acm.org/doi/10.1145/502034.502041\n\n[belief] Static error detection using semantic inconsistency inference\nhttps://dl.acm.org/doi/abs/10.1145/1250734.1250784\n\n[loopInvariants] Learning Loop Invariants for Program Verification\nhttp://papers.nips.cc/paper/8001-learning-loop-invariants-forprogram-verification"
        },
        "o0bFk1Jdus-": {
            "0": "Summary: This paper proposes a novel approach for training a Transformer model to predict program invariant.",
            "1": "The model is trained using training data that are synthesized from explicit conditional checks in functions and is used to predict invariants of unguarded blocks in similar functions.",
            "2": "Strength\n1.",
            "3": "The paper addresses the important and challenging problem of program invariant generation from static code in a scalable way.",
            "4": "2.",
            "5": "Real-world “missing if-guard” bugs are detected using the proposed model.",
            "6": "Weakness\n1.",
            "7": "The idea of synthesizing training data by automatically converting explicitly guarded code to its implicitly guarded counterpart is interesting.",
            "8": "However, the effectiveness of the trained model to infer program invariants in a general way is not clear from the experimental results.",
            "9": "The evaluation with real-world bugs focuses on “missing if-guard” bugs.",
            "10": "The difficulty of detecting this bug cannot be understood as there is no accuracy results from an existing tool (for example, Daikon) in detecting this real-world \n2.",
            "11": "Although a comparative analysis with Daikon is presented, the presented approach focuses on a narrower class of invariants compared to Daikon.",
            "12": "Moreover, Daikon relies on execution traces.",
            "13": "A comparison with an existing ML based approach using static code (e.g., [1]) would provide more interesting insights about the model’s accuracy.",
            "14": "3.",
            "15": "A contrastive hinge loss is introduced to address the “syntactically close” but “semantically opposite” cases.",
            "16": "However, from Figure-4, it seems the performance of the model is not impacted in a significant way by the loss function.",
            "17": "[1] P. Garg, D. Neider, P. Madhusudan, and D. Roth.",
            "18": "Learning Invariants using Decision Trees and Implication Counterexamples.",
            "19": "Question to author:\nPlease address and clarify the cons above."
        },
        "w9OcVw1PQR": {
            "0": "The paper proposes to discover likely invariants for code by observing snippets that check for the given conditions and assuming these conditions encode invariants for the code executing before and after the condition check was checked to hold (respectively not hold for negated invariant).",
            "1": "This is a novel idea that uses code with correct if conditions to guess the invariants for code that has the conditions missing.",
            "2": "My main criticism for the paper is that it does not give a compelling reason why one would want to apply this technique.",
            "3": "While this is a smart way to obtain the invariants, the paper does not give too much intuition why it could be useful in practice.",
            "4": "Even on the examples in the paper, the machine learning algorithm probably learns invariants from identifier names and not from the semantics of  the code around.",
            "5": "The authors can relate the work to a large corpus of learning invariants for functions based on things like usages of functions, e.g.",
            "6": "like done in [1] or [2] and the techniques there find actual bugs in code.",
            "7": "For example, if the invariant is non-nullness of x, this may be because x comes from a function that sometimes returns null or because it comes from a function that does not accept null.",
            "8": "If I would want to do for example bugfinding, I would want to know contradicting invariants coming from the two functions.",
            "9": "In terms of execution, the paper is well written and the techniques look state-of-the-art from a machine learning perspective (although there are no baselines given).",
            "10": "However, the experiments are insufficient for showing usefulness of the idea.",
            "11": "With Daikon overlap in the 70% range and precision also in the same range, it is not clear that the tool gives any new valid invariants on top of Daikon.",
            "12": "In terms of bugfinding, the results are also inconclusive that any bugs can be found.",
            "13": "If I would put the tool to test 100 methods, where normally less than 10 of them are buggy, I can expect 20 false positives.",
            "14": "Minor:\ntheorem proofers -> theorem provers\nFigure 5 a talks about overlap, but axis says precision.",
            "15": "[1] Ted Kremenek, Paul Twohey, Godmar Back, Andrew Y. Ng, Dawson R. Engler:\nFrom Uncertainty to Belief: Inferring the Specification Within.",
            "16": "OSDI 2006\n[2] Insu Yun, Changwoo Min, Xujie Si, Yeongjin Jang, Taesoo Kim, Mayur Naik: APISan: Sanitizing API Usages through Semantic Cross-Checking"
        },
        "KotmMUQKhyV": {
            "0": "The paper presents a method for statically learning code invariants from source code using a variant of transformers.",
            "1": "Strengths\n----\n- The paper demonstrates that on the synthetic dataset the proposed approach can infer many invariants.",
            "2": "Weaknesses\n----\n- The evaluation with a synthetic dataset seems very weak.",
            "3": "“If” checks are not good proxies for useful invariants as in most programs there are many if checks that are simply unreachable or redundant.",
            "4": "In practice,  not all invariants are useful for the downstream tasks (code fixing, bug finding, etc.)",
            "5": "mentioned by the authors.",
            "6": "Without a more direct evaluation, it is very hard to tell how useful the learned invariants actually are for these tasks.",
            "7": "The paper will be significantly stronger if the authors can evaluate their tool against existing loop invariant inference datasets with ground truth data like those used in Si et al.",
            "8": "(NeuRIPS 2018).",
            "9": "- The transformer-based model seems to be directly reused from Hallendoorn et al.",
            "10": "(ICLR 2020).",
            "11": "Thus the contribution in terms of model design is limited.",
            "12": "- The authors also did not cite/compare against the current state-of-the-art loop invariant learning work: CLN2INV: Learning Loop Invariants with Continuous Logic Networks.",
            "13": "Ryan et al.",
            "14": "ICLR 2020"
        }
    }
}