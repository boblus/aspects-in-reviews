{
    "0tEed0ZiFX": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenge of learning Semantic Role Labeling (SRL) from multiple disjoint label sets, specifically VerbNet and PropBank.",
            "1": "- It proposes a novel framework that jointly models these label sets as one sequence, enforcing SEMLINK constraints during decoding to improve overall F1 scores.",
            "2": "- The approach includes a constrained marginal model that leverages large amounts of PropBank-only data, achieving state-of-the-art results on the CoNLL05 benchmark.",
            "3": "Potential reasons for acceptance\n   - The proposed joint modeling framework effectively addresses the issue of generating structurally inconsistent label sequences in multitask setups.",
            "4": "- The paper demonstrates significant improvements in SRL performance, both in-domain and out-of-domain, compared to previous state-of-the-art models.",
            "5": "- The use of SEMLINK constraints during decoding and learning is a novel contribution that enhances the model's ability to handle disjoint label sets.",
            "6": "- The experimental results are robust, with detailed comparisons and statistical significance tests supporting the claims.",
            "7": "Potential reasons for rejection\n   - **Complexity and clarity of the proposed method:**\n     - The paper introduces several technical components (e.g., joint CRF, marginal CRF, SEMLINK constraints) that may be challenging for readers to fully grasp without extensive background knowledge.",
            "8": "- The explanation of the model architecture and training process could be more concise and clearer, as the current presentation is dense and may confuse readers.",
            "9": "- **Evaluation and generalization:**\n     - While the paper shows strong performance on the CoNLL05 benchmark, it would benefit from additional evaluations on other datasets to demonstrate the generalizability of the proposed method.",
            "10": "- The reliance on gold predicate labels for evaluation may limit the practical applicability of the model in real-world scenarios where such labels are not available.",
            "11": "- **Impact of semi-supervised learning:**\n     - The paper claims that semi-supervised learning with PropBank-only data improves performance, but the results show mixed outcomes, particularly for out-of-domain generalization.",
            "12": "- The analysis of why semi-supervised learning does not consistently improve performance could be more thorough, providing insights into potential limitations and areas for improvement.",
            "13": "Suggestions for improvement\n   - **Simplify and clarify the presentation:**\n     - Streamline the explanation of the model architecture and training process, using diagrams or flowcharts to illustrate key components and their interactions.",
            "14": "- Provide more intuitive explanations and examples to help readers understand the benefits and implementation of SEMLINK constraints.",
            "15": "- **Expand evaluation:**\n     - Evaluate the proposed method on additional datasets beyond CoNLL05 to demonstrate its generalizability and robustness across different domains and languages.",
            "16": "- Include experiments that do not rely on gold predicate labels, simulating more realistic scenarios and assessing the model's performance in such settings.",
            "17": "- **Enhance analysis of semi-supervised learning:**\n     - Conduct a more detailed analysis of the impact of semi-supervised learning, exploring why it does not consistently improve performance and identifying potential factors that influence its effectiveness.",
            "18": "- Investigate alternative approaches to leveraging PropBank-only data, such as different semi-supervised learning techniques or data augmentation strategies.",
            "19": "- **Broader impact and future work:**\n     - Discuss the broader impact of the proposed method, including potential applications in other NLP tasks and integration with large language models.",
            "20": "- Outline future research directions, such as developing fully end-to-end SRL parsers or incorporating document-level context to further enhance performance."
        },
        "ufHVWDcPw4": {
            "0": "- I think the direction explored in this work is interesting and important for the future development of unifying semantic resources.",
            "1": "- The paper is well-written and easy to follow, and the experiments are well-conducted.",
            "2": "- The experiments only involve joint prediction of PB and VN, where there are abundant joint data and high-coverage mapping resources.",
            "3": "The paper could be much stronger and more interesting if it could be extended to scenarios where there are less such joint resources (like PropBank/VerbNet & FrameNet).",
            "4": "- The performance gaps between different methods seem small.",
            "5": "Though this is not quite surprising giving strong neural models, it would be more interesting to explore scenarios where the proposed methods could be more helpful (such as low-resource cases and less-frequent predicates)."
        },
        "s1sxbnlOPT": {
            "0": "The results are state of the art, the methods sensible and the discussion useful.",
            "1": "The existence of compatible verbnet and PropBank labels is an accident of the history of the field There are not many situations parallel to this, so the result is probably of interest only to a small subset of ACL researchers.",
            "2": "The authors make little attempt to draw general lessons from their work."
        },
        "gpvduTkDqT": {
            "0": "Experiments conducted within this study are comprehensive, accompanied by a thorough analysis.",
            "1": "The proposed approach can be employed to facilitate mutual transformation between SRL data annotated under different standards, which holds significance for the automated construction of annotated datasets.",
            "2": "The writing of this paper requires a little improvement, as certain sections are difficult to comprehend.",
            "3": "The foundational aspect of SEMLINK is inadequately introduced.",
            "4": "For instance, in section 5, readers less familiar with SEMLINK might struggle to grasp concepts like \"SEML(u)\" and the associated inferences.",
            "5": "I believe the authors could enhance understanding by incorporating more illustrative examples.",
            "6": "Additionally, as mentioned by the authors in the Limitation section, the model in this study relies on gold predicate positions and predicate attributes.",
            "7": "This limitation diminishes the practicality of the model in real-world scenarios."
        }
    },
    "dbRZyDxYlL": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the integration of speech and text modalities to improve speech translation, which is a significant and relevant problem in the field of natural language processing and machine translation.",
            "1": "- The proposed FuseST model introduces novel techniques for cross-modal alignment and fusion, which are innovative and contribute to the advancement of speech translation technology.",
            "2": "Potential reasons for acceptance\n   - The paper presents a comprehensive analysis of the modality gap between speech and text and proposes targeted improvements to bridge this gap.",
            "3": "- The experimental results demonstrate that the FuseST model outperforms state-of-the-art models on multiple benchmarks, showing significant improvements in BLEU scores.",
            "4": "- The paper provides a detailed description of the model architecture, training objectives, and experimental setups, making it reproducible and transparent.",
            "5": "- The authors conduct extensive ablation studies and analyses to validate the effectiveness of their proposed methods and provide insights into the model's performance under different conditions.",
            "6": "Potential reasons for rejection\n   - **Limited novelty in the fusion approach**\n     - The concept of fusing speech and text modalities is not entirely new, and similar approaches have been explored in previous works.",
            "7": "- The paper could benefit from a more in-depth comparison with existing fusion methods to highlight the unique contributions of the proposed FuseST model.",
            "8": "- **Dependence on ASR quality**\n     - The effectiveness of the FuseST model heavily relies on the quality of the ASR transcripts, which may vary significantly across different datasets and real-world scenarios.",
            "9": "- The paper does not address the potential limitations and challenges of using ASR systems with high error rates, which could impact the overall performance of the model.",
            "10": "- **Scalability and generalization**\n     - The paper primarily focuses on specific language pairs and datasets, raising questions about the scalability and generalization of the proposed model to other languages and domains.",
            "11": "- Additional experiments on a wider range of languages and more diverse datasets would strengthen the claims of the paper and demonstrate the robustness of the FuseST model.",
            "12": "Suggestions for improvement\n   - **Enhance the novelty and contributions**\n     - Provide a more detailed comparison with existing fusion methods and highlight the unique aspects and advantages of the proposed FuseST model.",
            "13": "- Discuss the limitations and challenges of the current approach and suggest potential directions for future research to address these issues.",
            "14": "- **Address ASR quality dependence**\n     - Include experiments and analyses that explore the impact of varying ASR quality on the performance of the FuseST model.",
            "15": "- Propose methods to mitigate the negative effects of high ASR error rates and improve the robustness of the model in real-world scenarios.",
            "16": "- **Expand the scope of experiments**\n     - Conduct additional experiments on a broader range of languages and datasets to demonstrate the scalability and generalization of the FuseST model.",
            "17": "- Include more diverse and challenging test cases to evaluate the model's performance under different conditions and highlight its strengths and weaknesses.",
            "18": "- **Improve clarity and presentation**\n     - Ensure that all figures, tables, and descriptions are clear, well-organized, and easy to understand.",
            "19": "- Provide more detailed explanations of the key concepts, methods, and results to enhance the readability and accessibility of the paper for a wider audience."
        },
        "8RCgMDKDnh": {
            "0": "The motivation is clear.",
            "1": "For speech translation, speech contains some signal to help translation but if ASR doesn't capture it then error would propagate to the final translation quality.",
            "2": "(1) The proposed method is complicated.",
            "3": "It combines pretrained audio, text modular with 8 different losses.",
            "4": "This make it hard to conclude the improvement is came from proposed model or just introduce many hyper-parameters to tune.",
            "5": "(2) The comparison missed one direction of fuse speech and text representation for speech translation.",
            "6": "For example, MAESTRO: Matched Speech Text Representations through Modality Matching (https://arxiv.org/abs/2204.03409).",
            "7": "That basically learn a joint encoder can encode both audio and text.",
            "8": "Can the author comment the pros and cons of this direction versus proposed method?",
            "9": "(3) Presentation is hard to follow.",
            "10": "For example, it mentioned all encoder decoder share the same param in section 3.2, but why figure 2 mark some of param shared?",
            "11": "There also lots of acronym in the paper, that make it hard to follow.",
            "12": "(4) It's hard to follow the comparison, since external modular been used.",
            "13": "In table 2, when mark as external speech/text data been used, does it comparable with other cited paper?",
            "14": "Maybe to make at least one data point that comparable, can the author provide their in-house cascade model, instead of cite other paper number?"
        },
        "VKyTgnA6ZG": {
            "0": "A) Experiments and Code: The experimental section is thorough and the paper achieves good results with improvements on machine translation as well.",
            "1": "The paper includes several baselines to compare against.",
            "2": "The code is also released, good job with this!",
            "3": "B) The figures in the paper are intuitive, color-coded and easy-to-understand.",
            "4": "C) The model framework section (3.2) is clearly explained, and nicely builds up to Equation 13 which has 8 loss terms.",
            "5": "D) The paper is also well-scoped in that the aim is clearly defined as speech translation, and the way to improve it is by improving cross-modal alignment.",
            "6": "The paper does not have more or less to offer than what it claims which is a great thing.",
            "7": "A) Writing: The paper might benefit with revisions in writing.",
            "8": "For example, L377 mentions that the \"only difference between our approach and others is the specific method for fusing speech and text\".",
            "9": "Accordingly, the related work section should talk about these works and discuss how the paper is different from them.",
            "10": "In its current state, its mostly a list of papers and their citations.",
            "11": "L096-101 are also unclear, since prompt tags are not introduced before this.",
            "12": "B) The overall framework optimizes for 8 objectives and it is unclear how each help and why they are included.",
            "13": "Although the paper has a lot of content to offer, a more intuitive approach for the inclusion of each regularization term even at the cost of a few experiments, would be beneficial, since a major contribution of the paper is the model.",
            "14": "For example, stating something like: \"analysing previous model outputs or systems reveal certain limitations which can be overcome by these additional objectives ...\".",
            "15": "The complexity of the framework and non-clarity on each objective makes me hesitant on the excitement front, but I find the study to be thorough."
        },
        "eeiprbWMlK": {
            "0": "The strategy is simple and straightforward.",
            "1": "The numerical results show significant improvements over the baseline.",
            "2": "My major concern is the novelty of paper.",
            "3": "Similar ideas on architecture, contrastive learning and cross attention regularization have been proposed by several other papers, e.g.",
            "4": "[1][2].",
            "5": "The design of the model is quite similar to [1] and [2] expect that the shared encoder takes concatenate input instead of separated input.",
            "6": "[1] Unified Speech-Text Pre-training for Speech Translation and Recognition\n[2] WACO: Word-Aligned Contrastive Learning for Speech Translation"
        }
    },
    "F1G7y94K02": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical aspect of understanding how factual associations are stored and retrieved in auto-regressive language models (LMs), which is a significant step towards improving model transparency and interpretability.",
            "1": "- The study introduces a novel approach to dissecting the internal mechanisms of LMs through the lens of information flow, providing insights into the stages of attribute extraction and the roles of different sublayers.",
            "2": "Potential reasons for acceptance\n   - The paper provides a comprehensive analysis of the internal workings of LMs, which is valuable for the research community focused on model interpretability and knowledge localization.",
            "3": "- The methodology, including attention knockout and gradient-based analysis, is well-executed and offers a clear understanding of how information propagates within the model.",
            "4": "- The findings have practical implications for future research on model editing and improving the accuracy of factual predictions in LMs.",
            "5": "Potential reasons for rejection\n   - **Complexity of Methodology**\n     - The attention knockout and gradient-based analysis methods, while effective, are complex and may be challenging for other researchers to replicate without detailed guidance.",
            "6": "- The reliance on multiple interventions and projections to the vocabulary space might introduce approximations that could affect the accuracy of the findings.",
            "7": "- **Generalizability of Results**\n     - The study focuses on specific models (GPT-2 and GPT-J) and a particular dataset (COUNTER FACT), which may limit the generalizability of the findings to other models and datasets.",
            "8": "- The paper does not extensively discuss how the findings might apply to other types of LMs or different architectures, which could be a limitation for broader applicability.",
            "9": "- **Interpretation of Results**\n     - The interpretation of the results, especially the identification of \"knowledge hubs\" in attention heads, relies on manual inspection and qualitative analysis, which may introduce subjectivity.",
            "10": "- The paper could benefit from more quantitative metrics to support the claims about the roles of different sublayers and attention heads in attribute extraction.",
            "11": "Suggestions for improvement\n   - **Methodological Clarity**\n     - Provide more detailed explanations and step-by-step guidance on the attention knockout and gradient-based analysis methods to facilitate replication by other researchers.",
            "12": "- Include additional visualizations and examples to illustrate the methodology and findings more clearly.",
            "13": "- **Broader Evaluation**\n     - Extend the analysis to include a wider range of models and datasets to assess the generalizability of the findings.",
            "14": "- Discuss potential variations in the internal mechanisms of different LMs and how the proposed methods can be adapted to study them.",
            "15": "- **Quantitative Analysis**\n     - Incorporate more quantitative metrics to evaluate the significance of the identified \"knowledge hubs\" and the roles of different sublayers in attribute extraction.",
            "16": "- Use statistical tests to validate the findings and reduce the reliance on manual inspection and qualitative analysis.",
            "17": "- **Practical Implications**\n     - Explore the practical implications of the findings for model editing and improving factual predictions, providing concrete examples and potential applications.",
            "18": "- Discuss how the insights gained from this study can inform the development of more interpretable and accurate LMs in the future."
        },
        "PD7VubKiWD": {
            "0": "Overall the paper is well written and easy to understand.",
            "1": "It presents interesting and useful findings for better understanding how a transformer-based LM predicts for subject-relation query.",
            "2": "For example, they found that early layers are important in propagating information for predicting the attribute.",
            "3": "Another example is the attribute information is aggregated gradually through layers.",
            "4": "These findings could not only help understand the models but also inspire new methods that better aggregate information for subject-relation queries.",
            "5": "The experiments are comprehensive and provide answers for questions at different levels.",
            "6": "The study is only on subject-relation query.",
            "7": "It might not be easy to apply the proposed methods to understand the information flow in other research tasks, where important words in query are not known beforehand (just as the subject and relation word in the subject-relation query task), such as reading comprehension."
        },
        "7wNpNpGd6M": {
            "0": "With the rise of the desire for explainability of LMs works which identify (or go some way towards identifying) important locations are of great potential value.",
            "1": "The paper uses figures to advantage, illustrating complex concepts and results.",
            "2": "This is a fairly technical paper, presumably a stepping stone on an incremental path."
        },
        "Qu2g97TNk9": {
            "0": "The study of internal representations of Transformer-based models is still an evolving domain, which needs more contributions.",
            "1": "Particularly, little is known about how factual predictions are built.",
            "2": "This paper proposes an automatic approximation of the subject-attribute relatedness, namely the attribute rate.",
            "3": "It is a reasonable and novel idea.",
            "4": "The experiments are well designed and the results are well presented.",
            "5": "There are some interesting findings.",
            "6": "Although we should start with simple experiments in order to study a complex system, it is often unclear whether conclusions drawn from simple experiments can extend to general, complicated cases.",
            "7": "It is difficult to tell whether other NLP tasks follow the same pattern.",
            "8": "The authors suggest that \"these findings open new research directions for knowledge localization and model editing\", but it is unclear whether the localization depends on training tasks, or model size, among other things."
        }
    },
    "ytQFU2XsBR": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenge of combining two distinct reasoning methods, Chain-of-Thought (CoT) and Program-Aided Language Models (PAL), to leverage their respective strengths.",
            "1": "- The proposed method uses a large language model (LLM) to dynamically select between CoT and PAL, aiming to improve performance on reasoning tasks.",
            "2": "- The paper presents both theoretical analysis and empirical results to validate the effectiveness of the proposed model selection method.",
            "3": "Potential reasons for acceptance\n   - The paper introduces a novel approach to model selection that combines the strengths of CoT and PAL, which could be beneficial for various reasoning tasks.",
            "4": "- The theoretical analysis provides a solid foundation for the proposed method, explaining the conditions under which it can improve performance.",
            "5": "- The empirical results demonstrate significant performance improvements across multiple datasets and LLMs, including new state-of-the-art results on GSM8K and SVAMP.",
            "6": "- The method shows potential for reducing computation costs while maintaining or improving performance, which is valuable for practical applications.",
            "7": "Potential reasons for rejection\n   - **Limited scope of evaluation:**\n     - The experiments are primarily focused on arithmetic and symbolic reasoning tasks, which may not fully represent the diversity of reasoning challenges in real-world applications.",
            "8": "- The paper does not explore the applicability of the proposed method to other domains or types of reasoning tasks, limiting its generalizability.",
            "9": "- **Inherent bias in LLMs:**\n     - The paper acknowledges the inherent bias in LLMs, which affects the success rate of model selection based on the order of options presented.",
            "10": "- The proposed method does not address or mitigate this bias, which could impact the reliability and fairness of the model selection process.",
            "11": "- **Dependency on specific LLMs:**\n     - The method relies on the in-context learning capabilities of specific LLMs (e.g., Codex, ChatGPT, GPT-4), which may not be available or perform similarly across different LLMs.",
            "12": "- The paper does not provide a detailed analysis of how the method would perform with other LLMs or in scenarios where LLMs have limited in-context learning capabilities.",
            "13": "Suggestions for improvement\n   - **Expand the scope of evaluation:**\n     - Conduct experiments on a wider range of reasoning tasks, including those from different domains, to demonstrate the generalizability and robustness of the proposed method.",
            "14": "- Include more diverse datasets and tasks to provide a comprehensive evaluation of the method's performance.",
            "15": "- **Address inherent bias in LLMs:**\n     - Investigate techniques to mitigate the inherent bias in LLMs, such as probability calibration or alternative selection strategies, to improve the fairness and reliability of the model selection process.",
            "16": "- Provide a detailed analysis of the impact of bias on the success rate and overall performance of the proposed method.",
            "17": "- **Explore dependency on specific LLMs:**\n     - Evaluate the proposed method with a broader range of LLMs, including those with varying in-context learning capabilities, to assess its applicability and performance across different models.",
            "18": "- Analyze the method's performance in scenarios where LLMs have limited in-context learning capabilities and propose potential solutions to address these limitations.",
            "19": "- **Enhance theoretical analysis:**\n     - Provide a more detailed theoretical analysis of the conditions under which the proposed method can achieve significant performance improvements, including potential failure cases and limitations.",
            "20": "- Explore the theoretical implications of combining more than two base models with distinct strengths to further enhance the performance and robustness of the method."
        },
        "WVb7J9LT1Q": {
            "0": "- Proposed a simple, intuitive, and effective prompting method.",
            "1": "- The proposed method is complementary to the self-consistency (a prompting technique) and can be combined with it to further improve the performance.",
            "2": "- Achieved new state-of-the-art performance on GSM8K, an arithmetic reasoning task\n- Provided interesting insights:\n    - Even though CoT and PAL have similar performance, the performance difference in each sample is actually high, and there is room for performance improvement through combining them with the model selection.",
            "3": "- The combination of CoT and PAL is a more successful in the model selection than other combinations.",
            "4": "- The three LLMs used tended to prefer the first option in the model selection - Seems to be a lack of explanation about the experimental setup (Question A-C)"
        },
        "8YPqgAftnO": {
            "0": "+ The notion of utilizing large models for selection is innovative and effectively leverages the comprehension capabilities of these models.",
            "1": "+ Thorough theoretical analysis substantiates the method's advantages.",
            "2": "+ The paper is well-written and easily comprehensible.",
            "3": "- The improvements observed on GPT-4 are marginal, with most gains being minimal; it remains to be seen whether the slight benefits from the multiple reasoning in this approach justify the additional complexity compared to traditional single-pass methods(CoT/PAL).",
            "4": "- The authors have not conducted experiments or comprehensive analyses on the extra time and cost overhead introduced by their method.",
            "5": "- While experiments with proficient closed-source models like ChatGPT and CodeX are conducted, it is suggested to supplement the analysis with experiments on open-source models like Llama to assess the method's generalizability."
        },
        "nJY4c13dhB": {
            "0": "+ The paper provides atheoretical analysis to support the feasibility of the proposed model combination.",
            "1": "+ The empirical results are robust, showcasing performance improvements across multiple datasets and with various LLM backbones.",
            "2": "+ The paper is well written and easy to follow.",
            "3": "- The paper primarily utilizes existing models such as CoT and PAL without introducing notable innovations.",
            "4": "Its contribution appears to be an application of known methods rather than presenting a transformative approach or fresh insights.",
            "5": "- The paper heavily leans on existing models without introducing significant novel techniques or modifications.",
            "6": "This might position the paper as offering incremental improvements rather than groundbreaking contributions.",
            "7": "- The paper's reliance on in-context learning and self-evaluation abilities of LLMs might make the selector sensitive to prompts, potentially limiting its broader applicability.",
            "8": "- While the paper acknowledges LLM biases, it doesn't seem to propose any concrete solutions or mitigation strategies"
        },
        "m87rKxgoev": {
            "0": "- The paper presents a comprehensive theoretical analysis that quantifies the error rates of base methods and delineates the conditions under which the proposed methodology enhances performance effectively.",
            "1": "- The paper is articulately written, ensuring clarity and ease of understanding.",
            "2": "Furthermore, the empirical evidence robustly supports the theoretical claims, achieving state-of-the-art results on GSM8K and SVAMP.",
            "3": "- While the paper emphasizes \"model selection\", the experimental design primarily addresses \"prompting method selection\", with both base methods and the selection model leveraging the same LLM backbone.",
            "4": "Consequently, the proposed approach can be perceived as a novel prompting strategy, sequentially generating CoT and PAL results using the backbone LLM.",
            "5": "It would be beneficial to conduct further experiments with genuinely distinct models to truly harness their respective strengths, such as integrating CodeX and ChatGPT with GPT-4.",
            "6": "- The proposed method demonstrates effectiveness, but it necessitates thrice the computational and inferential resources for a single problem.",
            "7": "The \"model selection\" issue as addressed in this paper seems to be a variant of the self-consistency methods, a topic that has been extensively explored.",
            "8": "Prioritizing the selection of prompt strategies based on a specific problem might present a more significant research avenue.",
            "9": "- The motivation behind the paper stems from the notion that CoT and PAL excel in different task types.",
            "10": "However, a thorough analysis and explicit examples highlighting the disparities between these two methods are conspicuously absent.",
            "11": "Furthermore, there's a need for elucidation on why these methods showcase different correct distributions within the experimental section."
        }
    },
    "sCu26OfxxZ": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a novel negotiation dialogue agent designed for the online marketplace, which is integrative in nature, allowing negotiation on price as well as other factors such as the addition or removal of items from a deal bundle.",
            "1": "- The creation of a new dataset called Integrative Negotiation Dataset (IND) and the introduction of a semi-automated data creation method are significant contributions.",
            "2": "- The use of a reward-based dialogue system to train the negotiation agent, leveraging a set of novel rewards tailored for the negotiation task, is a novel approach.",
            "3": "Potential reasons for acceptance\n   - The integrative approach to negotiation, which goes beyond price negotiations, offers a more flexible and comprehensive negotiation experience.",
            "4": "- The creation of the IND dataset and the semi-automated data creation method address the challenge of data scarcity in certain domains.",
            "5": "- The combination of supervised learning and reinforcement learning, along with the novel reward function, enhances the agent’s negotiation capabilities.",
            "6": "- The experimental results demonstrate the effectiveness of the proposed approach and reward system in improving the agent’s negotiation performance.",
            "7": "Potential reasons for rejection\n   - **Limited scope of product categories in the dataset**\n     - The dataset focuses on only 10 different electronic items, which may limit the generalizability of the findings to other product categories.",
            "8": "- The integrative negotiation approach may need to be tested on a wider range of products to validate its effectiveness across different domains.",
            "9": "- **Dependence on GPT-J for data generation**\n     - The use of GPT-J for data generation requires a large GPU memory size (40 GB), which may not be accessible to all researchers.",
            "10": "- The context window limitation of GPT-J (2,048 tokens) constrains the prompting mechanism, potentially leading to some hallucinations in the generated data.",
            "11": "- **Manual intervention in data correction**\n     - The need for human experts to make minor edits and filter the automatically generated dialogues to ensure quality introduces a level of subjectivity and potential bias.",
            "12": "- The manual correction process may not be scalable for larger datasets or more complex negotiation scenarios.",
            "13": "- **Evaluation metrics and human evaluation**\n     - The evaluation metrics used (e.g., METEOR, BERT Score, Word Mover distance) primarily focus on surface similarity and semantic similarity, which may not fully capture the effectiveness of the negotiation strategies.",
            "14": "- The human evaluation scores are based on a small number of interactions (15 times per evaluator), which may not provide a comprehensive assessment of the agent’s performance.",
            "15": "Suggestions for improvement\n   - **Expand the scope of the dataset**\n     - Include a wider range of product categories beyond electronic items to test the generalizability of the integrative negotiation approach.",
            "16": "- Consider incorporating more diverse negotiation scenarios, such as services or bundled offers, to enhance the dataset’s applicability.",
            "17": "- **Address the limitations of GPT-J**\n     - Explore alternative language models with larger context windows or more efficient memory usage to reduce the dependency on high GPU memory.",
            "18": "- Implement techniques to mitigate hallucinations in the generated data, such as incorporating additional validation steps or using more robust prompting mechanisms.",
            "19": "- **Reduce manual intervention in data correction**\n     - Develop automated methods for data correction and quality assurance to minimize the need for human intervention and reduce potential bias.",
            "20": "- Implement more rigorous validation processes to ensure the accuracy and consistency of the generated dialogues.",
            "21": "- **Enhance evaluation metrics and human evaluation**\n     - Incorporate additional evaluation metrics that capture the effectiveness of negotiation strategies, such as success rate, negotiation duration, and user satisfaction.",
            "22": "- Increase the number of human evaluators and interactions to provide a more comprehensive assessment of the agent’s performance and reduce potential variability in the scores."
        },
        "povrqjyeVo": {
            "0": "1) The semi-automatic data collection technique is an interesting step and potentially useful in this space where collecting quality data is challenging.",
            "1": "This naturally has pros and cons (I discuss some of them below).",
            "2": "The process followed was also rigorous, involving post-processing and human evaluation to ensure quality.",
            "3": "2) In their approach, the authors defined useful rewards that are all relevant to negotiation and can potentially help future work.",
            "4": "3) The paper consists of a comprehensive evaluation, showing the superiority of the proposed approach on a variety of metrics based on both automatic and human evaluation.",
            "5": "1) The proposed approach is unclear.",
            "6": "RL formulation has not been stated properly.",
            "7": "It is unclear what the state and actions are and whether the rewards are defined at an utterance level or a dialogue level.",
            "8": "I suggest the authors to describe the approach in a more thorough manner.",
            "9": "2) The paper argues for win-win negotiations that “see” a product as a bundle rather than negotiating for a single issue of price.",
            "10": "However, neither the proposed approach nor the evaluation focuses on the other items in the bundle.",
            "11": "The rewards are based only on the price.",
            "12": "In other words, the paper set out with the motivation of handling a scenario with multiple items in a bundle, but the spirit of the approach seems no different than prior work by He et al 2018.",
            "13": "What am I missing here?",
            "14": "3) In addition, the pitch of the paper ignores a body of prior work on multi-issue negotiations that do capture multiple items rather than just one.",
            "15": "These datasets already involve integrative negotiations (along with distributive negotiations).",
            "16": "For instance, work on DealOrNoDeal task (Lewis et al.",
            "17": "2017) and more recent datasets like CaSiNo (Chawla et al.",
            "18": "2021) and JobInterview (Yamaguchi et al.",
            "19": "2021).",
            "20": "These should be discussed in related work.",
            "21": "One difference is that in this paper, the set of issues varies depending on the given context.",
            "22": "Thus, the claims in Lines 126-128 and 189-191 are misleading.",
            "23": "To fix these, I believe either the pitch of the paper should be changed or the experiments should be better designed in a way that captures the multiple items in a bundle (just like modeling work on multi-issue negotiation tasks).",
            "24": "One way to do this can be by defining a reward that captures the other items in the bundle.",
            "25": "4) The paper lacks additional critical analysis, such as comparing the intent accuracy of the final models and analyzing how the model deals with the other items in the bundle - this is important since this relates to how the paper has been motivated.",
            "26": "Hence, I suggest the authors to include this in the paper."
        },
        "XTiwyxsqhb": {
            "0": "This work provides substantial contents including:\n1.",
            "1": "A concrete pathway to collect data\n2.",
            "2": "A detailed training framework containing SFT and RL, as well as the delicate reward design.",
            "3": "Relatively positive experimental results both on automatic and human evaluation.",
            "4": "Figures in this paper should be polished up.",
            "5": "For example, several lines in Figure 2 are actually crooked."
        },
        "tIkVZRTB6b": {
            "0": "Authors present a novel new dataset of reasonable size, containing discussion on integrative negotiation dialogues.",
            "1": "The dialogue contain clarification intents, which is frankly useful in itself as a totally different task worth studying.",
            "2": "3,300 training dialogues is a respectable amount.",
            "3": "For quantitative metrics (METEOR, BERT-Score, WMD, Perplexity, and Length), the proposed INA model outperforms all baselines.",
            "4": "In human evaluation across five qualitative metrics, the INA model also does better than all ARDM and NegTOD baselines.",
            "5": "The quality of the dialogues is still fairly unrealistic.",
            "6": "To highlight a couple of areas:\na) The users don't really dive deep into the description or product details.",
            "7": "(ie.",
            "8": "\"The SnapDragon processor is 2 generations old now, you should lower the price.\")",
            "9": "b) There are no emotional appeals that would occur (ie.",
            "10": "\"Oh, please!",
            "11": "I really need it for my daughter's birthday gift!\")",
            "12": "c) Conversations are all roughly the same length, real life chats can go on for 100+ turns easily (or conversely, will end abruptly).",
            "13": "d) The prices are off by a factor of 100x?",
            "14": "(ie.",
            "15": "Tablet costs $92,800, rather than $928)\n\nThere are no ablations or extra analysis on the rewards used for RL-training.",
            "16": "In general, the modeling is not too novel, relying largely on pre-trained SL followed by RL, which is fairly common for dialogue system training.",
            "17": "This is not a huge deal though if we view this as resource paper, rather than a modeling paper."
        }
    },
    "mTiHLHu3sP": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the limitations of in-context learning (ICL) for relation extraction (RE) using large language models (LLMs) like GPT-3.",
            "1": "- It proposes a novel approach, GPT-RE, which incorporates task-aware representations and gold label-induced reasoning to improve the performance of ICL for RE.",
            "2": "- The paper demonstrates significant improvements over existing GPT-3 baselines and even fully-supervised baselines on multiple RE datasets.",
            "3": "Potential reasons for acceptance\n   - The proposed GPT-RE method shows substantial improvements in performance on several RE datasets, achieving state-of-the-art results on some.",
            "4": "- The paper provides a thorough empirical evaluation and analysis, demonstrating the effectiveness of the proposed approach.",
            "5": "- The methodology is well-explained, and the paper includes detailed ablation studies to validate the contributions of different components of GPT-RE.",
            "6": "- The work addresses a critical issue in the field of NLP, specifically the limitations of ICL for RE, and provides a practical solution.",
            "7": "Potential reasons for rejection\n   - **Limited generalizability of results**\n     - The experiments are conducted on a limited number of datasets, which may not fully represent the diversity of RE tasks.",
            "8": "- The performance improvements may not generalize to other domains or types of RE tasks not covered in the paper.",
            "9": "- **Over-reliance on specific retrieval methods**\n     - The success of GPT-RE heavily depends on the quality of the task-aware retrieval methods, which may not always be feasible or effective in different settings.",
            "10": "- The fine-tuned relation representation approach requires additional training and resources, which may limit its applicability in resource-constrained environments.",
            "11": "- **Incomplete resolution of overpredicting NULL issue**\n     - While the paper claims to alleviate the issue of overpredicting NULL examples, it acknowledges that the problem is not completely solved.",
            "12": "- The proposed solution may still underperform in scenarios with a high proportion of NULL examples, as seen in datasets like ACE05.",
            "13": "Suggestions for improvement\n   - **Expand the evaluation to more diverse datasets**\n     - Include additional RE datasets from different domains and with varying characteristics to better assess the generalizability of the proposed approach.",
            "14": "- Consider evaluating GPT-RE on open RE tasks to explore its performance in more flexible and less constrained settings.",
            "15": "- **Enhance the retrieval methods**\n     - Investigate alternative retrieval methods that do not rely on fine-tuning smaller PLMs, potentially exploring ways to leverage representations generated by GPT-3 itself.",
            "16": "- Explore hybrid retrieval approaches that combine multiple retrieval strategies to improve the robustness and effectiveness of demonstration selection.",
            "17": "- **Address the NULL prediction issue more comprehensively**\n     - Develop additional techniques to further mitigate the overpredicting NULL issue, possibly by incorporating more sophisticated handling of NULL examples during training and retrieval.",
            "18": "- Conduct a more detailed analysis of the NULL prediction problem to identify specific patterns and causes, which could inform the development of targeted solutions.",
            "19": "- **Provide more detailed implementation details and code**\n     - Ensure that all implementation details are thoroughly documented, and consider releasing the code and datasets used in the experiments to facilitate reproducibility and further research.",
            "20": "- Include more examples and case studies to illustrate the practical application and benefits of GPT-RE in real-world RE tasks."
        },
        "ZHgq3xoJuR": {
            "0": "- The paper is well organized and mostly easy to follow.",
            "1": "- The study addresses a performance gap between fine-tuned models and in-context learning for relation extraction, and it proposes a solution which achieve SoTA or at least competitive results on multiple, widely used RE datasets.",
            "2": "- In additional ablation studies a deeper analysis of the performance demonstrates the potential of the approach in different settings.",
            "3": "- Overall the study is an interesting contribution to the relation extraction task Regarding the experiments done on TACRED and ACE05: due to the cost of running the model the authors sampled test subsets from that data, which is larger then SEMEVAL and SciERC.",
            "4": "The approach is understandable.",
            "5": "However, strangely the sampled subsets are smaller than the whole test data of the other two datasets; I would expect the subsets of TACRED and ACE05 to have at least the same size or even a bigger size as the other two datasets in order to reflect the whole test data better.",
            "6": "It does hamper the comparison between the models.",
            "7": "This is especially to relevant wrt TACRED, which contains a much larger number of relations than other datasets, and  many of them are probably not well represented in the sampled subset.",
            "8": "The authors plan to release the subsets, however the paper contains no information about the distribution of the relations in the test subsets."
        },
        "2jyjIez6vi": {
            "0": "The paper is well written with good survey of related works.",
            "1": "The authors showed that with task-aware representations, all the retrieval-based models have higher performance than the GPT-Random, while the gold label-induced reasoning helps to improve the performance.",
            "2": "The approach can be a good resource for future research of LLMs to resolve the RE problems.",
            "3": "In section 2.4.2, we define the relation representation as Rel = hi + hj.",
            "4": "Does the relation representation include the context in between, or does it include only the representations of two entities?",
            "5": "If it is the former, it is not surprising that the GPT-RE_FT model achieves good performance, the way the relation representation is obtained is similar to previous works, but instead of putting through a neural network, now it is put into a retriever.",
            "6": "The GPT-RE_SimCSE has the novelty in the entity prompts, which is still limited.",
            "7": "However, if it is the latter, as stated in section 2.4.2 \"representation Rel is naturally enriched with the entity information.",
            "8": "\", it is kind of weird that the GPT-RE_FT can achieves state-of-the-art with only entity information and without context."
        },
        "JRebPsRo6Q": {
            "0": "The paper adeptly highlighted a potential concern regarding GPT-3's utilization of in-context learning (ICL) and effectively provided a resolution.",
            "1": "The observed performance enhancements are indeed promising, as they notably elevate the performance beyond that of GPT-3 alone.",
            "2": "This achievement suggests that the strategies introduced in the paper hold the potential to offer benefits to a wide array of large language models (LLMs).",
            "3": "I think the paper could further strengthen its argument by providing additional evidence to substantiate the identified shortcomings of in-context learning (ICL).",
            "4": "By delving deeper into the reasons behind these shortcomings, the paper could enhance its overall credibility and contribute to a more comprehensive understanding of the challenges associated with ICL."
        }
    },
    "R4yb4m7Nus": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in NLP, which is the brittleness of models to adversarial attacks.",
            "1": "- It introduces a novel approach, Model-tuning Via Prompts (MVP), which shows significant improvements in adversarial robustness.",
            "2": "- The work is comprehensive, covering multiple datasets, models, and attack strategies, providing a thorough evaluation of the proposed method.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and effective method for improving adversarial robustness in NLP models.",
            "4": "- The experimental results are robust, showing consistent improvements across various datasets and models.",
            "5": "- The paper provides a detailed analysis and ablation studies, enhancing the understanding of why MVP improves robustness.",
            "6": "- The work is well-motivated and addresses a significant problem in the field of NLP.",
            "7": "Potential reasons for rejection\n   - **Limited scope of models and tasks:**\n     - The experiments are primarily conducted on models with less than 1B parameters, which may not generalize to larger models.",
            "8": "- The tasks considered are limited to classification, boolean question answering, and paraphrase detection, excluding other NLP tasks like sequence labeling or machine translation.",
            "9": "- **Evaluation of real-world applicability:**\n     - The paper does not provide a detailed discussion on the practical implications and deployment challenges of MVP in real-world applications.",
            "10": "- The latency and computational overhead of using multiple prompt templates are mentioned but not thoroughly analyzed in terms of real-world feasibility.",
            "11": "- **Human study limitations:**\n     - The human study is conducted with a small and specific group (machine learning graduate students), which may not represent the general population.",
            "12": "- The study focuses on a single dataset (BoolQ), limiting the generalizability of the findings regarding human perception of adversarial examples.",
            "13": "Suggestions for improvement\n   - **Expand the scope of models and tasks:**\n     - Include experiments with larger models (e.g., GPT-3) to evaluate the scalability of MVP.",
            "14": "- Test MVP on a broader range of NLP tasks, such as sequence labeling, machine translation, and summarization, to demonstrate its versatility.",
            "15": "- **Detailed analysis of real-world applicability:**\n     - Provide a more comprehensive analysis of the computational overhead and latency associated with using multiple prompt templates.",
            "16": "- Discuss potential deployment challenges and solutions for integrating MVP into real-world NLP systems.",
            "17": "- **Enhance the human study:**\n     - Conduct the human study with a more diverse group of participants to improve the generalizability of the findings.",
            "18": "- Include multiple datasets in the human study to assess the robustness of human perception across different types of adversarial examples.",
            "19": "- **Additional ablation studies:**\n     - Investigate the impact of different types of prompts and candidate answers in more detail to provide deeper insights into the robustness improvements.",
            "20": "- Explore the combination of MVP with other state-of-the-art adversarial training methods to further enhance robustness."
        },
        "WTPIMut1Me": {
            "0": "- The idea of prompt tuning for robustness is interesting.",
            "1": "The author states the motivation from the point of view of random parameter vulnerability.",
            "2": "- The explanation makes sense and the experimental results are strong.",
            "3": "- A key motivation for the paper is that vanilla fine tuning strategy by adding some random parameters can harm the model's performance and robustness .",
            "4": "However, some experimental results of the proposed method contradict motivation, making the idea less convincing.",
            "5": "By replacing candidates with dummy words such as Jack, John, Ann, the model is still robust.",
            "6": "In my opinion, such labels are no better than a random projection layer.",
            "7": "How can this avoid hurting the parameters of the model?",
            "8": "In pretrained models, many word embeddings are undertuned or even untuned.",
            "9": "For example, some dummy tokens are left for further use.",
            "10": "What if you use these words?",
            "11": "- Since the paper highlights the usefulness of prompts for robustness, further experimental study is needed.",
            "12": "For example, how do you choose the prompts?",
            "13": "How do different prompts affect the robustness of the model?",
            "14": "As a baseline, what if you use some dummy prompts, such as \"foo bar (mask)\"?",
            "15": "- It remains unclear whether robustness arises from the prompt-tuning strategy itself or from some tricky ensemble-style strategy, i.e.",
            "16": "by using different models (prompts) with different class logits whose signal may fool the attacker.",
            "17": "In Table 2, what is the adversarial accuracy of each prompt?",
            "18": "What is the result of \"1 prompt + 4 candidates\"?"
        },
        "wZGI5Py8Ob": {
            "0": "- Strong experiment results, especially wrt adversarial robustness.",
            "1": "- Human annotators validated efficacy of adversarial example set\n- Sound ablation studies and clear analysis that explains the results\n- MVP + Adv outperforms MLP-FT + Adv handily - Only tested on datasets that are simple classification tasks (BoolQ is 2 classes, AGNews contains 4 classes).",
            "2": "- No evaluation for datasets with larger number of classes, no evaluation for non-classification tasks."
        },
        "JNK2ekiizK": {
            "0": "Using prompts to avoid adding random parameters for adversarial robustness is a nice approach and it works on the datasets presented in the paper.",
            "1": "MVP shows better adversarial robustness compared to fine-tuning without sacrificing on accuracy.",
            "2": "The experiments confirm the issue with adding random features in the fine-tuning stage as raised by Kumar e. al 2022 etc.",
            "3": "fine-tuning using prompts is an alternate to the LPFT.",
            "4": "Lack of discussion and analysis on the inference latency as more templates are added for MVP.",
            "5": "This paper would have benefited from a direct comparison between LPFT and MVP on both OOD generalization and Adversarial robustness.",
            "6": "Both these approaches combat the \"brittleness\" of the fine-tuning process.",
            "7": "It would have been interesting to see LPFT vs MVP using different number of prompts.",
            "8": "A discussion on the relative advantages of each approach would have added to this work."
        }
    },
    "IksoHnq4rC": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant challenge in neural NLP models, which is the generation of adversarial samples for data augmentation to enhance model robustness.",
            "1": "- The proposed method, A3, combines adversarial training and data augmentation in a novel way by introducing an end-to-end adversarial sample generation model.",
            "2": "- The approach is efficient as it generates reusable adversarial samples independent of the target model, which is a novel contribution compared to existing methods.",
            "3": "Potential reasons for acceptance\n   - The method shows significant improvements in model performance and robustness against multiple attacking techniques across various NLP tasks.",
            "4": "- The proposed approach is computationally efficient, reducing the time required for data augmentation compared to existing methods.",
            "5": "- The paper provides a comprehensive evaluation of the method on multiple datasets and tasks, demonstrating its effectiveness and generalizability.",
            "6": "Potential reasons for rejection\n   - **Lack of diversity in conditions:**\n     - The current form of condition limits the diversity and fine-grained control over the generation process.",
            "7": "- The paper does not explore alternative forms of conditions that could potentially improve the quality and diversity of generated adversarial samples.",
            "8": "- **Limited comparison with state-of-the-art methods:**\n     - The paper compares the proposed method with only a few existing methods (SSMBA and A2T), which may not provide a comprehensive evaluation of its effectiveness.",
            "9": "- There is a lack of comparison with other recent and relevant adversarial training and data augmentation techniques in NLP.",
            "10": "- **Insufficient analysis of parameter settings:**\n     - The paper provides limited analysis on the impact of different parameter settings, such as the ratio of perturbed words (ρ), on the performance and robustness of the model.",
            "11": "- More detailed experiments and discussions on the sensitivity of the method to various hyperparameters would strengthen the evaluation.",
            "12": "Suggestions for improvement\n   - **Enhance condition diversity:**\n     - Explore and incorporate more diverse and fine-grained conditions, such as syntactic structures or semantic roles, to guide the adversarial sample generation process.",
            "13": "- Investigate the impact of different types of conditions on the quality and effectiveness of generated adversarial samples.",
            "14": "- **Expand comparison with state-of-the-art methods:**\n     - Include a broader range of recent adversarial training and data augmentation methods in the comparison to provide a more comprehensive evaluation of the proposed method.",
            "15": "- Conduct experiments on additional datasets and tasks to further validate the generalizability and effectiveness of the approach.",
            "16": "- **Detailed parameter analysis:**\n     - Perform a more thorough analysis of the impact of various hyperparameters, such as the ratio of perturbed words (ρ), on the model's performance and robustness.",
            "17": "- Provide detailed discussions and visualizations of how different parameter settings affect the generated adversarial samples and the overall training process.",
            "18": "- **Improve clarity and organization:**\n     - Enhance the clarity and organization of the paper by providing more detailed explanations of the proposed method and its components.",
            "19": "- Include more visual aids, such as diagrams and flowcharts, to illustrate the architecture and training process of the end-to-end adversarial sample generation model."
        },
        "c7x2v9RTL4": {
            "0": "The results show some improvements.",
            "1": "The motivation of adding each loss and module is clear.",
            "2": "TL;DR I appreciate the efforts and observations / merits found by the authors.",
            "3": "However, this paper poorly presents the methodology (both details and its key advantage), and it’s hard to validate the conclusion with such little hyperparameter analysis.",
            "4": "I would love to see more detailed results, but I could not accept this version to be accepted as an EMNLP paper.",
            "5": "1 There are too many missing details when presenting the methodology: e.g., what will be the effect if I remove one or two losses presented by the authors?",
            "6": "Though their motivations are clear, they do not validate the hypothesis clearly.",
            "7": "2 A lot of equations look like placeholders, such as equations (1, 2, 3, 5, 6).",
            "8": "3 Some of the pieces are simply using existing methods, such as  equation (12), the presentation of these methods are also vague (can only be understood after checking the original paper).",
            "9": "4 The pipeline misses a lot of details.",
            "10": "For example, how long does it take to pre-train each module?",
            "11": "How adding pre-training will benefit the performance?",
            "12": "How to schedule the training of the discriminator and the main module?",
            "13": "Not mentioning the detail design of the RNN network used.",
            "14": "5 Why do we need to focus on the four aspects?",
            "15": "They are just listed there.",
            "16": "Also, some of the results presentation does not seem to be thorough and valid.",
            "17": "For example, in table 2, the Quora datasets have the highest perturbation ratio, but the downgraded performance is the least among the three.",
            "18": "Is it really because the adversarial samples are effective instead of the task variance or dataset variance?",
            "19": "Also, we didn’t see the attack performance of other comparison methods.",
            "20": "And how is the test set generated?",
            "21": "What is the size of the adversarial test set and why is that a good benchmark?",
            "22": "6 In table 4, it’s actually hard to say which is better, A^3 or A^2T, if you count the number of winners for each row and column.",
            "23": "7 In table 5, is the computation time also considered the pre-training stage?",
            "24": "If not, why?",
            "25": "Does the pre-training stage can serve as a unified step which is agnostic to the dataset and tasks?",
            "26": "8 I don’t quite understand the point of section 4.6, and its relationship to the effectiveness of A^3.",
            "27": "This influence of /rho seems to be really obvious.",
            "28": "I would rather be more interested in changing the six hyperparameters mentioned in line 444 and test their effectiveness.",
            "29": "9 The related work section is also not very well-written.",
            "30": "I couldn’t understand what is the key difference and key advantage of A^3 compared to the other methods."
        },
        "IrEWaGcAup": {
            "0": "Outside of some rough grammar in places I found this paper to be very easy to follow and the motivations to create the end-to-end system well founded.",
            "1": "The adversarial + augmentation approach appears to greatly outperform the comparison methods in Table 4, of which only one other was also an adversarial + augmentation method (and it performed significantly slower).",
            "2": "The efficiency of the proposed method was significantly improved over the comparison methods, allowing for time and cost reduction over prior approaches in generating large datasets.",
            "3": "In the reported results tables there is no mention if the data is from a single run or best of many.",
            "4": "There is no given standard deviation or CI to frame the variation of quality in the generation between the compared methods.",
            "5": "Reproducibility would be hard if the authors will not release code, incomplete specifics on the BiRNNs used were given in the paper (were the RNNs identical for all stages?",
            "6": "optimizer?",
            "7": "learning rate?",
            "8": "etc).",
            "9": "If no code is to be released, an appendix would be welcome with further details on complete model construction."
        },
        "mW8IbzQaPT": {
            "0": "- Generate the conditions for paraphrasing model to make a suitable adversarial sample.",
            "1": "- Test on three different tasks to reveal the novelty of works.",
            "2": "- Simple but powerful performances regarding the experiments done by authors.",
            "3": "- Idea is very similar to Generative Adversarial Network.",
            "4": "The author needs to point out the difference to show the novelty of work in the Related work sections better.",
            "5": "Please see \"Missing References\" part.",
            "6": "- The algorithm is not clear, especially for discriminator since author sometimes argues that it is pre-trained model (line 82-85) but sometimes not (line 5 in Algorithm 1).",
            "7": "Please develop the algorithm part better to make it more understandable.",
            "8": "- The insights of authors choices (e.g.",
            "9": "model selection) and the investigation of experimental results are missing.",
            "10": "For example, why A2T in SNLI+DistillBERT is better than your approach (Table 3)?",
            "11": "- Please include the example of adversarial examples in each approach to understand better.",
            "12": "- Lack of ablation study regarding the selection of hyperparameters (e.g.",
            "13": "Section 4.1.3).",
            "14": "- Hard to reproduce the results since there is no offered source code.",
            "15": "- Please double-check whether authors follow the ACL format since the labelling of Table/Figure/Algorithm does not work."
        }
    },
    "ljsGKc8cVR": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the significant challenge of long document retrieval, which is crucial for applications like web search and question answering.",
            "1": "- It introduces a novel model, Longtriever, which efficiently handles long documents by splitting them into blocks and modeling both local and global semantics.",
            "2": "- The proposed pre-training phase with a novel task, Local Masked Autoencoder (LMAE), enhances the model's understanding of long documents, addressing the issue of scarce annotations.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-motivated and novel approach to long document retrieval, which is a challenging and important problem.",
            "4": "- The experimental results demonstrate that Longtriever outperforms state-of-the-art models on multiple benchmark datasets, showing its effectiveness.",
            "5": "- The proposed methodology, including the hierarchical model structure and the LMAE pre-training task, is innovative and well-explained.",
            "6": "- The paper provides a thorough evaluation, including ablation studies and hyperparameter analysis, which strengthens the validity of the results.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability**\n     - Some sections of the paper, particularly the methodology, are dense and may be difficult for readers to follow.",
            "8": "- The explanation of the LMAE task and its integration into Longtriever could be clearer and more detailed.",
            "9": "- **Comparative analysis**\n     - While the paper compares Longtriever with several baselines, it lacks a detailed analysis of why Longtriever performs better than specific models.",
            "10": "- The paper could benefit from a more in-depth discussion of the limitations and potential weaknesses of Longtriever compared to other models.",
            "11": "- **Generalization and applicability**\n     - The paper focuses on specific datasets for evaluation.",
            "12": "It would be beneficial to test Longtriever on a wider range of datasets to demonstrate its generalizability.",
            "13": "- The practical implications and potential limitations of deploying Longtriever in real-world applications are not thoroughly discussed.",
            "14": "Suggestions for improvement\n   - **Improve clarity and readability**\n     - Simplify and clarify the explanation of the methodology, particularly the LMAE task and its integration into Longtriever.",
            "15": "- Use more diagrams and visual aids to illustrate the model architecture and the flow of information within Longtriever.",
            "16": "- **Enhance comparative analysis**\n     - Provide a more detailed analysis of the performance differences between Longtriever and other models, highlighting specific strengths and weaknesses.",
            "17": "- Include a discussion on the potential limitations of Longtriever and how it compares to other models in different scenarios.",
            "18": "- **Expand evaluation**\n     - Test Longtriever on additional datasets to demonstrate its generalizability and robustness across different types of long documents.",
            "19": "- Discuss the practical implications of using Longtriever in real-world applications, including potential challenges and solutions for deployment.",
            "20": "- **Address scalability and efficiency**\n     - Provide more details on the scalability and efficiency of Longtriever, particularly in terms of computational resources and time required for training and inference.",
            "21": "- Explore potential optimizations to further improve the efficiency of Longtriever, especially for very large-scale datasets."
        },
        "wHTFPFeSEu": {
            "0": "The proposed Longtriever architecture is intuitive and the paper is well written and easy to follow.",
            "1": "The fine-tuned Longtriever outperforms other existing retrievers on MS MARCO document ranking dataset.",
            "2": "It seems to me that the main contribution of the paper comes from LMAE pre-training, which is a revised version of MAE pre-training specific for modelling long document rather than the proposed architecture.",
            "3": "The second issue comes from the evaluation.",
            "4": "From the experiments, it seems the main factor for Longtriever to outperform other models comes from LMAE pre-training, which the other models do not experience, rather than the proposed Longtriever architecture (by comparing III in Table 4 with other long-document model in Table 2).",
            "5": "It is possible that if we apply the same LMAE pre-training strategy to other models, we can get similar ranking effectiveness.",
            "6": "The evaluation is only conducted on MS MARCO document ranking dataset.",
            "7": "However, there are many other document ranking datasets, such as Robust04, ClueWeb or Gov2."
        },
        "Y4U3y7LSiv": {
            "0": "- The work proposes a novel pre-training task, the Local Masked Autoencoder (LMAE), which enhances the model's ability to capture the underlying meaning of long texts by leveraging both local and global representations.",
            "1": "- The writing and organization of the paper are well, making it easy to follow and understand.",
            "2": "- The proposed LMAE approach bears a strong resemblance to LexMAE [1], yet the paper fails to provide a comparative analysis or discussion of the two methods.",
            "3": "- The experimental results presented in the paper do not include comparisons with other state-of-the-art models, such as LexMAE [1] and FDG [2].",
            "4": "- The description of the experimental setup is unclear, leaving readers uncertain about key experimental details.",
            "5": "For instance, it is not specified in Table 1 whether all methods, including the proposed approach, use a sequence length of 512 as input.",
            "6": "[1] Lexicon-Bottlenecked Pretraining for Large-Scale Retrieval.",
            "7": "ICLR 2023.\\\n[2] Fine-Grained Distillation for Long Document Retrieval."
        },
        "DmRhA3Zspm": {
            "0": "The paper is well written, providing a clear motivation and view of the comparative landscape.",
            "1": "The goals/challenges of document retrieval (e.g., computational cost, document understanding, scare annotations) are very valuable for readers.",
            "2": "It is noteworthy that the authors have effectively tackled each of these aspects within their work.",
            "3": "Longtriever introduces a novel framework and architectural design for dense retrieval.",
            "4": "While the concept of integrating document knowledge into dense retrieval is not new, the authors innovate by presenting a fresh paradigm for aggregating representations through the utilization of inter-intra mechanisms.",
            "5": "The authors provide a (very) comprehensive experimental setting, showing their model is competitive with SOTA methods.",
            "6": "The ablation studies shed some light on the importance of the author's choices along the way but do not provide any deep insight into the method's mechanism.",
            "7": "I encourage the authors to provide such analysis to help readers to understand the proposed method.",
            "8": "While Longtriever presents a novel contribution, it's important to note that the LMAE method may not be entirely novel.",
            "9": "The fundamental concept behind LMAE resembles a pre-training task found in autoencoders, such as BART, which involves sentence reconstruction through masking.",
            "10": "You don't state you will provide the code for your paper upon acceptance (if you had, and I missed it, please let me know), I believe one could reproduce your results without it, but I (strongly) encourage you to provide it for future researchers.",
            "11": "The organization of Section 5 is very hard to follow.",
            "12": "Notably, Tables 1 and 2 are positioned within Section 3, making it difficult for readers to follow the flow of experimental results seamlessly."
        }
    },
    "9r8WwpJv7M": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper presents a novel approach to understanding the linguistic knowledge that NLP models learn by incorporating linguistic complexity indices into the training process.",
            "1": "- The development of data-driven linguistic curricula based on psycholinguistic and language acquisition research is a significant contribution to the field of NLP.",
            "2": "- The approach aims to improve the learning process of NLP models by scheduling data samples in a meaningful order, which is a novel application of curriculum learning in NLP.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a relevant and timely problem in NLP, focusing on the linguistic knowledge that models learn during training.",
            "4": "- The proposed approach is well-motivated and grounded in existing research on linguistic complexity and curriculum learning.",
            "5": "- The experimental results demonstrate the effectiveness of the proposed approach, showing consistent performance gains over competing models.",
            "6": "- The paper provides a comprehensive evaluation of the approach on several benchmark NLP datasets, highlighting its generalizability and robustness.",
            "7": "- The authors make their source code and data available, promoting transparency and reproducibility in research.",
            "8": "Potential reasons for rejection\n   - **Lack of clarity in methodology**:\n     - The paper could benefit from a more detailed explanation of the correlation and optimization approaches used to estimate the importance of linguistic indices.",
            "9": "- The process of aggregating linguistic indices into a single difficulty score could be more clearly described, including the rationale behind the chosen methods.",
            "10": "- **Limited analysis of results**:\n     - The paper provides limited analysis of the experimental results, particularly in terms of why certain linguistic indices are more important for specific tasks.",
            "11": "- The discussion on the learning dynamics of NLP tasks could be expanded to provide deeper insights into the observed trends and patterns.",
            "12": "- **Potential computational overhead**:\n     - The incorporation of linguistic complexity indices and the generation of data-driven curricula may introduce additional computational overhead during training, which is not thoroughly addressed in the paper.",
            "13": "- The scalability of the proposed approach to larger datasets and more complex models is not discussed, which could be a concern for practical applications.",
            "14": "Suggestions for improvement\n   - **Enhance clarity in methodology**:\n     - Provide a more detailed explanation of the correlation and optimization approaches used to estimate the importance of linguistic indices, including mathematical formulations and examples.",
            "15": "- Clarify the process of aggregating linguistic indices into a single difficulty score, and discuss the rationale behind the chosen methods.",
            "16": "- **Expand analysis of results**:\n     - Conduct a more thorough analysis of the experimental results, particularly focusing on why certain linguistic indices are more important for specific tasks.",
            "17": "- Expand the discussion on the learning dynamics of NLP tasks, providing deeper insights into the observed trends and patterns.",
            "18": "- **Address computational overhead**:\n     - Discuss the potential computational overhead introduced by the incorporation of linguistic complexity indices and the generation of data-driven curricula.",
            "19": "- Provide an analysis of the scalability of the proposed approach to larger datasets and more complex models, including potential optimizations to mitigate computational costs.",
            "20": "- **Improve readability and structure**:\n     - Ensure that the paper is well-organized and clearly written, with a logical flow of ideas and arguments.",
            "21": "- Use visual aids, such as diagrams and tables, to illustrate key concepts and results, making the paper more accessible to readers."
        },
        "ao6ORw67Dj": {
            "0": "The paper presents a novel approach to curriculum learned based on linguistic difficulty that goes beyond previous approaches with regard to the features used, and the way of modelling the curricula.",
            "1": "Furthermore, the paper provides an insightful analysis on the importance of different linguistic indices during different training stages.",
            "2": "Overall, the paper presents a very interesting idea, in combination with a meaningful evaluation.",
            "3": "There is not much information on the model and the experimental settings, only in the appendix.",
            "4": "I think it would be better to have some more information also in the main part of the paper.",
            "5": "A general downside of the approach is the dependency on linguistic knowledge generated from human experts, which will limit the amount of languages that can be modelled with this approach."
        },
        "eoee8dpdnS": {
            "0": "Several innovative methodologies are introduced in this work:\n(1) The incorporation of correlation and optimization methods to gauge the importance of diverse linguistic indices.",
            "1": "(2) Utilization of either the maximum or weighted average for aggregation.",
            "2": "(3) Introduction of curricula encompassing time-varying sigmoid, moving negative-sigmoid, time-varying Gaussian function, etc.",
            "3": "The paper demonstrates adept composition and a well-structured layout.",
            "4": "Readers can readily discern the research inquiries pursued and the corresponding actions taken by the authors.",
            "5": "The authors undertake a substantial array of experiments, bolstering the paper's foundation and substantiating its claims convincingly.",
            "6": "There are a series of methods proposed as mentioned before.",
            "7": "However, it's worth noting that not all of these methods have been examined within the conducted experiments.",
            "8": "For instance, the distinction between the correlation and optimization methods, as well as their respective impacts on system performance, is notably absent.",
            "9": "This paper could potentially benefit from additional pages to accommodate a more extensive array of experiments, thereby offering a more comprehensive and in-depth understanding.",
            "10": "This concern extends to the treatment of linguistic indices.",
            "11": "While multiple indices collaboratively contribute, the experimental focus narrows primarily to word rarity when evaluating different models.",
            "12": "Yet, word rarity is not the top three most important linguistic indices in each dataset, which casts a certain degree of uncertainty upon the conclusions drawn.",
            "13": "Furthermore, certain expressions within the paper lack clarity.",
            "14": "For instance, the term \"competence-based\" in line 350 and \"indices filtering\" in line 353 could benefit from a more explicit explanation.",
            "15": "While the intended meanings can be inferred to some extent, providing a clear and concise elucidation would enhance the overall understanding of the content."
        },
        "Xa1B7lMkCm": {
            "0": "- The paper nicely delineates how individual complexity measures contribute to performance on each task differently, and at what stage of the model training process; this may help others identify what impacts their model performance given what task they're working on.",
            "1": "- Well motivated approach of classifying data samples, then sequencing and weighting them during training - the insights how to use the data could be generalized to other methods \n- Appendix provides helpful context of what indices were used and follow up experiments \n - As the authors point out, linguistic complexity measures require expert knowledge to define and are not language agnostic.",
            "2": "This method then may not generate over to other languages until similar measures of complexity are defined.",
            "3": "- It was not all together clear if the described experiments should be carried out by others in their own work such as identifying complexity of data to better understand their model performance, or if this is a more self contained experiment"
        }
    },
    "J5FFUHZjNx": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces STEER LM, a novel supervised fine-tuning (SFT) method that allows end-users to control responses during inference by conditioning responses on a multi-dimensional set of attributes.",
            "1": "- This approach addresses the limitations of reinforcement learning from human feedback (RLHF), such as complex training setups and the inability to control model alignment with implicit values at run-time.",
            "2": "- The proposed method demonstrates the potential to generate high-quality, customizable responses, making it a significant contribution to the field of language model alignment.",
            "3": "Potential reasons for acceptance\n   - The paper presents a clear and well-motivated problem statement, addressing the limitations of existing RLHF methods.",
            "4": "- The proposed STEER LM method is innovative and offers a practical alternative to RLHF, with the potential for easier implementation and training.",
            "5": "- The experimental results show that STEER LM outperforms state-of-the-art baselines, including RLHF models, on the Vicuna benchmark.",
            "6": "- The paper provides a comprehensive evaluation, including both automatic and human evaluations, to validate the effectiveness of the proposed method.",
            "7": "- The open-sourcing of the code and model on the NVIDIA NeMo toolkit enhances the reproducibility and accessibility of the research.",
            "8": "Potential reasons for rejection\n   - **Lack of detailed comparison with other SFT methods:**\n     - The paper primarily focuses on comparing STEER LM with RLHF models but does not provide an in-depth comparison with other SFT methods.",
            "9": "- A more detailed analysis of how STEER LM compares to other SFT approaches in terms of performance and training complexity would strengthen the paper.",
            "10": "- **Limited evaluation on diverse benchmarks:**\n     - The evaluation is primarily conducted on the Vicuna benchmark, which may not fully capture the versatility and robustness of the proposed method.",
            "11": "- Including additional benchmarks, especially those covering different languages and domains, would provide a more comprehensive assessment of STEER LM's capabilities.",
            "12": "- **Potential bias in automatic evaluation:**\n     - The paper acknowledges that GPT-4, used for automatic evaluation, may have a bias towards longer and more informative responses.",
            "13": "- This bias could potentially inflate the performance scores of STEER LM, and additional measures to mitigate this bias should be considered.",
            "14": "- **Scalability and efficiency concerns:**\n     - While the paper claims that STEER LM is easier to train compared to RLHF, it does not provide detailed information on the computational resources and time required for training.",
            "15": "- A thorough analysis of the scalability and efficiency of the proposed method, including comparisons with RLHF and other SFT methods, would be beneficial.",
            "16": "Suggestions for improvement\n   - **Provide a detailed comparison with other SFT methods:**\n     - Include a comprehensive analysis of how STEER LM compares to other SFT approaches in terms of performance, training complexity, and computational resources.",
            "17": "- Highlight the specific advantages and potential trade-offs of using STEER LM over other SFT methods.",
            "18": "- **Expand the evaluation to include diverse benchmarks:**\n     - Evaluate STEER LM on additional benchmarks that cover different languages, domains, and tasks to demonstrate its versatility and robustness.",
            "19": "- Consider including benchmarks that specifically test the model's ability to handle diverse and challenging scenarios.",
            "20": "- **Address potential bias in automatic evaluation:**\n     - Implement measures to mitigate the potential bias of GPT-4 towards longer and more informative responses in the automatic evaluation.",
            "21": "- Consider using multiple evaluation models or human evaluators to provide a more balanced assessment of the model's performance.",
            "22": "- **Analyze scalability and efficiency:**\n     - Provide detailed information on the computational resources, time, and cost required for training STEER LM.",
            "23": "- Compare the scalability and efficiency of STEER LM with RLHF and other SFT methods, highlighting any potential advantages or limitations.",
            "24": "- **Include more detailed ablation studies:**\n     - Conduct additional ablation studies to isolate the impact of different components and design choices in STEER LM.",
            "25": "- Provide insights into how each component contributes to the overall performance and effectiveness of the proposed method."
        },
        "IYy4GSbR3c": {
            "0": "Retraining using bootstrap high-quality data is sound and it makes user can decide the attributes like humor or toxicity\nIt outperforms ChatGPT 3.5 in auto evaluation and human evaluation on Vicuna test.",
            "1": "Detailed ablation studies proved its effective design.",
            "2": "See questions."
        },
        "LkoTQhlLH6": {
            "0": "(1) The paper proposes a novel approach, SteerLM, which offers a simpler and more user-controlled alternative to RLHF for aligning language models with human preferences.",
            "1": "(2) The methodology is well-structured and easy to understand.",
            "2": "The authors provide clear explanations of the attribute prediction model, attribute-conditioned SFT, and bootstrapping with high-quality samples.",
            "3": "(3) The paper includes comprehensive experimental evaluations, including automatic evaluation with GPT-4, human evaluation, and an ablation study, to demonstrate the effectiveness of STEER LM.",
            "4": "(1) More details are needed for the base model.",
            "5": "What is the base performance?",
            "6": "How much compute does it costs?",
            "7": "Are there any public access?",
            "8": "Because it is unclear about the base model, we do not know how much improvement has been made by the proposed method.",
            "9": "(2) There are 13 attributes in OASST, why do the authors choose 7 out of 13?",
            "10": "(3) The method is not new.",
            "11": "It is needed to discuss the differentiation on the method with RAFT [1] and RRHF [2].",
            "12": "Especially, an important and similar work is omitted [1].",
            "13": "[1] RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment https://arxiv.org/abs/2304.06767\n\n[2] Rrhf: Rank responses to align language models with human feed- back without tears https://arxiv.org/abs/2304.05302"
        },
        "abfvIbOmfP": {
            "0": "The paper proposes an alternative method for model alignment, providing a different approach to RLHF.",
            "1": "The proposed SteerLM approach demonstrates strong performance, surpassing existing models on the Vicuna benchmark.",
            "2": "The proposed method may introduce challenges in collecting alignment data, potentially increasing the difficulty of the alignment process.",
            "3": "It is important to verify the proposed method on other benchmarks or approaches to determine its generalizability.",
            "4": "It remains unclear whether the superior performance is primarily attributed to the 43B pretrained model or the attribute conditioned SFT.",
            "5": "The paper would benefit from additional evaluation results.",
            "6": "Existing benchmarks lack evaluations specifically related to toxicity and hallucinations, which are crucial aspects for assessing alignment effectiveness."
        }
    },
    "ouiQX2XWYc": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the deployment of large language models (LLMs) by proposing a novel watermarking strategy.",
            "1": "- The approach leverages the quantization process to embed watermarks, which is a unique and innovative method compared to traditional watermarking techniques.",
            "2": "- The significance lies in its potential to protect model weights from malicious usage and license violations, which is increasingly important as LLMs become more prevalent.",
            "3": "Potential reasons for acceptance\n   - The proposed method introduces a new dimension to watermarking by utilizing the quantization gap, which is a novel contribution to the field.",
            "4": "- The experimental results demonstrate the effectiveness of the method in various scenarios, including text-agnostic and text-related watermarking.",
            "5": "- The paper provides a comprehensive evaluation of the method, including comparisons with baseline approaches and detailed analysis of the results.",
            "6": "- The approach is practical and can be easily integrated into existing LLM deployment pipelines, making it highly relevant for real-world applications.",
            "7": "Potential reasons for rejection\n   - **Watermark Erasing Vulnerability**\n     - The paper acknowledges that the watermarks can be easily erased by further pre-training, which undermines the robustness of the proposed method.",
            "8": "- The effectiveness of the watermarking strategy in preventing unauthorized usage is limited if the watermarks can be removed with relative ease.",
            "9": "- **Limited Generalization of Text-Related Watermarks**\n     - The results show that the text-related watermarks do not generalize well to unseen data, which could limit the applicability of the method in diverse real-world scenarios.",
            "10": "- The reliance on specific triggers for watermark activation may not be practical for all use cases, especially when dealing with varied and unpredictable inputs.",
            "11": "- **Complexity and Practicality of Implementation**\n     - The interval optimization strategy, while effective, may introduce additional complexity in the training and deployment process.",
            "12": "- The need for multiple-random-test strategies to confirm watermarking success adds to the operational overhead, which could be a barrier to adoption.",
            "13": "Suggestions for improvement\n   - **Enhance Watermark Robustness**\n     - Investigate methods to make the watermarks more resistant to erasure through further pre-training, such as combining quantization watermarks with other watermarking techniques.",
            "14": "- Explore adaptive watermarking strategies that can dynamically adjust to different training and usage scenarios to maintain robustness.",
            "15": "- **Improve Generalization of Text-Related Watermarks**\n     - Develop techniques to enhance the generalization of text-related watermarks, ensuring they can be reliably activated across a wider range of inputs.",
            "16": "- Consider incorporating machine learning techniques to identify and generate more effective triggers that are less likely to be bypassed.",
            "17": "- **Simplify Implementation and Verification**\n     - Streamline the interval optimization process to reduce complexity and make it more accessible for practical implementation.",
            "18": "- Develop automated tools and frameworks to facilitate the integration of the proposed watermarking strategy into existing LLM deployment workflows.",
            "19": "- Provide detailed guidelines and best practices for practitioners to effectively implement and verify the watermarks in their models."
        },
        "vZyf1oeeyl": {
            "0": "This paper works on a timely and important problem.",
            "1": "The watermarking could help protect intellectual property and prevent misuse of open-sourced large language models.",
            "2": "This paper proposes a watermarking technique that utilizes model quantization gaps to embed watermarks, avoiding the need for predefined triggers.",
            "3": "The proposed method is novel.",
            "4": "Experiments demonstrates that the proposed approach effectively work on major open-source models like GPT-Neo and LLaMA.",
            "5": "The paper has limited discussion of watermark robustness under finetuning.",
            "6": "Open-source LLMs like LLaMA-1 and LLaMA-2 are often finetuned for real-world usage.",
            "7": "The impact of finetuning on retaining the quantization watermarks is unclear.",
            "8": "This is a major concern for real usage.",
            "9": "There is limited analysis of the tradeoff between watermark capacity and model performance.",
            "10": "It would be great if the authors add more discussion of the evaluation of the watermarked models."
        },
        "5y4YD1PqAN": {
            "0": "- Very interesting connection with model watermark and quantization\n- Success demonstration that such method is applicable to LLaMA and GPT-Neo - I am highly interested in the intersection of quantization and model watermarking.",
            "1": "However, I am not convinced that the issue of watermark erasure should be disregarded.",
            "2": "A possible real-world scenario of model watermarking is when a user obtains a released model and fine-tunes on personalized/custom data.",
            "3": "Verifying the ownership of the finetuned model is very crucial to protect model IP (consider company x uses company y's model without following y's model license).",
            "4": "- Quantized model often results in performance degradation.",
            "5": "Since your work involved customized int8 quantization, it would be interesting to see if the performance of the quantized model is close to the original full-precision model.",
            "6": "I'd suggest testing the quantized model using LLM Harness benchmark [1].",
            "7": "- Other than these two issues I am fine with other experiments -- well-executed.",
            "8": "[1]: https://github.com/EleutherAI/lm-evaluation-harness"
        },
        "Of0x08fUb3": {
            "0": "- The idea of planting watermarks during the quantization process is quite novel and interesting\n- The proposed three testing scenarios and three metrics are good experimental setup designs and can be used by similar works along this line (though there are some experimental setup concerns mentioned in reject reasons) - Important comparison missing: there are only comparisons among different optimization methods or the watermarking based on the quantization idea in the paper, what would be the comparison result with traditional watermarking text-related watermarking?",
            "1": "I assume it’s feasible to obtain WPR, TMR and SR of the traditional trigger-based watermarking techniques.",
            "2": "- Important details missing: 1) what are concrete watermarks used, are they natural language sentences or something else?",
            "3": "2) How many watermarks are used in the experiments, when talking about WPR/SR, what is the total test case number (as it’s important for us to know whether the method is robust or not)?",
            "4": "- The application scenarios of the proposed watermarking technique are limited.",
            "5": "We have to assume the downstream model provides access to both quantized and full-precision copies of the new models to be able to check whether there is watermarking left.",
            "6": "If the downstream developer only provides API access to the model, we cannot detect the origin of the model.",
            "7": "- The experimental setting is relatively constrained.",
            "8": "The same set of datasets (around Line 354) is used to train the model with watermarks and to test the task success rate.",
            "9": "It’s hard to see whether the proposed method can yield similar performance when the training/testing data is changed."
        }
    },
    "ZVy8L79f5f": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenge of linking surface facts extracted through Open Information Extraction (OIE) to large-scale Knowledge Graphs (KGs), which is a significant problem in the field of natural language processing and knowledge representation.",
            "1": "- The proposed benchmark, FaLB, introduces novel evaluation protocols that cover various aspects of OIE-to-KG linking, including transductive, inductive, polysemous, and out-of-KG detection, which are not comprehensively addressed in previous works.",
            "2": "Potential reasons for acceptance\n   - The paper presents a comprehensive and multifaceted benchmark for evaluating OIE-to-KG linking, which is a valuable contribution to the research community.",
            "3": "- The authors provide a detailed analysis of the performance of several baseline models on the proposed benchmark, highlighting the challenges and areas that require further research.",
            "4": "- The release of all resources (data, benchmark, and code) to the public promotes transparency and reproducibility, encouraging further research and development in this area.",
            "5": "Potential reasons for rejection\n   - **Limited novelty in methodology**:\n     - The paper primarily focuses on the creation of a benchmark and evaluation protocols rather than proposing novel algorithms or models for OIE-to-KG linking.",
            "6": "- The baseline models used in the experiments are adaptations of existing methods, and there is no significant innovation in the model architecture or training techniques.",
            "7": "- **Insufficient evaluation on diverse datasets**:\n     - The evaluation is conducted on two datasets, REBEL and SynthIE, which may not fully represent the diversity of real-world data.",
            "8": "- The reliance on synthetic data (SynthIE) for training and evaluation might not accurately reflect the performance of the models on natural, noisy text data.",
            "9": "- **Lack of detailed error analysis**:\n     - The paper provides a high-level overview of the performance of the models but lacks a thorough error analysis to understand the specific challenges and failure modes of the models.",
            "10": "- Detailed case studies or qualitative analysis of the errors could provide deeper insights into the limitations of the current approaches and guide future improvements.",
            "11": "Suggestions for improvement\n   - **Introduce novel algorithms or enhancements**:\n     - Propose new algorithms or enhancements to existing models specifically designed to address the challenges of OIE-to-KG linking, such as handling polysemous entities or out-of-KG detection.",
            "12": "- Explore the use of advanced techniques like graph neural networks or transformer-based models to improve the linking performance.",
            "13": "- **Expand evaluation to more diverse datasets**:\n     - Include additional datasets from different domains and with varying levels of noise to evaluate the robustness and generalizability of the models.",
            "14": "- Consider using real-world datasets with human-annotated ground truth to complement the synthetic data and provide a more comprehensive evaluation.",
            "15": "- **Conduct detailed error analysis**:\n     - Perform a thorough error analysis to identify common failure modes and specific challenges faced by the models.",
            "16": "- Provide qualitative case studies to illustrate the types of errors and their potential causes, which can inform future research directions.",
            "17": "- **Enhance the benchmark with more challenging tasks**:\n     - Introduce more challenging tasks or scenarios in the benchmark, such as linking facts with incomplete or noisy information, to push the boundaries of current models.",
            "18": "- Consider incorporating additional evaluation metrics that capture different aspects of linking performance, such as precision, recall, and F1-score for specific types of entities or relations."
        },
        "qok8Wd2eIb": {
            "0": "- A novel benchmark for OIE-to-KG linking that covers multiple aspects of OIE to KG linking, as well as Out-of-KG detection.",
            "1": "- Evaluation with multiple baselines.",
            "2": "- Clear presentation.",
            "3": "- An artificial setup where only triple slots (subj, obj, pred) are used for linking without the use of the sentence in the context.",
            "4": "- Weak baselines (that don't use the context sentence)\n - A data augmentation (paraphrasing / using different aliases for entities) that is applied on the triple level and not on the sentence level, which wouldn't allow to use source sentence (because the model could use the mismatch information between the surface form in the triple and the sentence)"
        },
        "9sRDQSbxJS": {
            "0": "The paper gave a valuable dataset for OIE-to-KG linking.",
            "1": "It would also be beneficial for those who considering the OIE-to-KG linking problem.",
            "2": "The novelty is somewhat limited, making the model is a combination of existing works."
        },
        "02xR4U0RMm": {
            "0": "The proposed data set can be used as a new benchmark for KG fact extraction from text.",
            "1": "The dataset curation process considers a wide variety of settings which can serve a test bed for existing and future models for this task.",
            "2": "The paper proposes a solution that works well on the proposed data set.",
            "3": "No clear comparison is made between the proposed data set and other existing large scale datasets (e.g., T-Rex, ReVerb45K, etc.).",
            "4": "Such comparisons would make it clear why the community should adopt this data set instead of existing ones.",
            "5": "Lack of strong baselines.",
            "6": "The proposed method is overwhelmingly better than any of the baselines.",
            "7": "There are existing works of fact extraction from text (e.g., KBPearl, CESI) that could serve as a strong baseline."
        }
    },
    "4AiERjB5JD": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper presents a novel method for unsupervised text style transfer using prefix-tuning and pre-trained language models.",
            "1": "- The approach introduces three types of prefixes (shared, style, and content) to encode task-specific information, target style, and content information, respectively.",
            "2": "- The recursive use of language models in the style transfer process is a unique strategy that enhances the interaction between the input sentence and the language model, leading to improved performance.",
            "3": "Potential reasons for acceptance\n   - The proposed method demonstrates significant improvements in content preservation and fluency compared to state-of-the-art baselines.",
            "4": "- The use of prefix-tuning reduces the number of trainable parameters to only 2% of the pre-trained language model, making the approach efficient.",
            "5": "- The paper provides comprehensive evaluations, including automatic metrics and human evaluations, to validate the effectiveness of the proposed method.",
            "6": "- The ablation studies offer insights into the contributions of different components of the model, highlighting the importance of each prefix type.",
            "7": "Potential reasons for rejection\n   - **Limited scope of datasets:**\n     - The experiments are conducted on only two datasets (Yelp and IMDb), which may not be sufficient to generalize the findings to other text style transfer tasks.",
            "8": "- The datasets used are sentiment-based, and the method's performance on other types of style transfer (e.g., formality, politeness) is not explored.",
            "9": "- **Complexity of the model:**\n     - The introduction of multiple prefixes and the recursive use of the language model add complexity to the model, which may make it harder to implement and understand.",
            "10": "- The paper does not provide a detailed analysis of the computational cost and training time associated with the proposed method.",
            "11": "- **Lack of comparison with more recent models:**\n     - The paper compares the proposed method with a limited set of baseline models, and some recent advancements in text style transfer are not considered.",
            "12": "- The comparison with zero-shot methods like LaMDA is not thoroughly discussed, especially in terms of content preservation and style control.",
            "13": "Suggestions for improvement\n   - **Expand the scope of datasets:**\n     - Include additional datasets with different types of style transfer tasks (e.g., formality, politeness) to demonstrate the generalizability of the proposed method.",
            "14": "- Provide a more diverse set of examples in the case study to showcase the versatility of the model.",
            "15": "- **Simplify the model architecture:**\n     - Consider simplifying the model by reducing the number of prefixes or exploring alternative ways to encode style and content information.",
            "16": "- Provide a detailed analysis of the computational cost and training time to help readers understand the trade-offs involved.",
            "17": "- **Include more recent baselines:**\n     - Compare the proposed method with more recent and advanced text style transfer models to provide a comprehensive evaluation.",
            "18": "- Discuss the strengths and weaknesses of the proposed method in comparison to zero-shot methods like LaMDA, especially in terms of content preservation and style control.",
            "19": "- **Improve clarity and readability:**\n     - Provide more detailed explanations and visualizations of the model architecture and the prefix-tuning process to enhance understanding.",
            "20": "- Include a more thorough discussion of the limitations and potential future work to address the identified challenges."
        },
        "wHvluZ2pQW": {
            "0": "- Proposing a new method based on GPT-2 for unsupervised text style transfer.",
            "1": "- Introducing three different types of prefixes to encode task-specific information, target style, and content details, thereby providing more comprehensive information to the model compared to the embeddings used in previous works.",
            "2": "- Adopting a recursive approach to improve the interactions between the input sentence and GPT-2.",
            "3": "- Providing comprehensive results, conducting ablation studies, and including subjective evaluations from human assessors to gain a deeper understanding of the proposed approach.",
            "4": "- This paper presents incremental work.",
            "5": "The proposed method is highly similar to the framework of (Dai et al., 2019) and incorporates prompt learning techniques that have been previously proposed.",
            "6": "- The evaluation of the proposed method is limited to sentiment transfer tasks.",
            "7": "It would be beneficial to explore the performance of the proposed method on other tasks (such as formality transfer) and datasets.",
            "8": "- The experiments in this paper only compare a small number of baselines.",
            "9": "It would be advantageous to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019]."
        },
        "URwVSgR3D7": {
            "0": "This work is clearly motivated to do text style transfer and proposes a quite new and interesting.",
            "1": "The adversarial framework of the methodology is reasonable.",
            "2": "The prefixes are quite significant according to the experimental results.",
            "3": "Moreover, the losses designed for the generator are quite thoroughly considered.",
            "4": "The statement of the methodology and the structure of the paper are good.",
            "5": "The implementation is comprehensive.",
            "6": "The results are solid.",
            "7": "Baselines in this paper are limited.",
            "8": "There are no baselines from 2022.",
            "9": "There a also no baselines of editing-based [1] or prompt-based methods [2].",
            "10": "There is no analysis of the three losses to train the generator.",
            "11": "[1] Machel Reid and Victor Zhong.",
            "12": "2021.",
            "13": "LEWIS: Leven- shtein editing for unsupervised text style transfer.",
            "14": "In Findings of ACL-IJCNLP, pages 3932–3944.",
            "15": "[2] Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky.",
            "16": "2022.",
            "17": "Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models.",
            "18": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2195–2222."
        },
        "UDFpZ39upl": {
            "0": "* The method provided improvements compared to three different models.",
            "1": "Compared to LaMDA, the improvement is mainly in BLEU score.",
            "2": "* The paper is written clearly (but there are parts that need to be explained better (e.g., why do we need the recursive exactly and what happens if we  do not do that?)",
            "3": "* The paper is only evaluated on two datasets that target the same task (positive to negative sentences or vice versa).",
            "4": "The approach would be more powerful if different types of styles have been considered.",
            "5": "The provided examples focus on changes for the language (e.g.",
            "6": "but) or antonym changes (e.g.",
            "7": "love vs. hate) but not clear how does it perform in more complex scenarios.",
            "8": "* The paper has focused on transferring only two styles to each other.",
            "9": "Not clear what happens with scaling the number of styles."
        }
    },
    "fONyQKyvsY": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a new dataset of 14,053 articles from two state-backed disinformation websites, Reliable Recent News (RRN) and WarOnFakes (WoF), which is significant for the study of disinformation networks.",
            "1": "- The paper performs a comprehensive linguistic and temporal analysis of the dataset, which is novel in the context of multilingual disinformation studies.",
            "2": "- The dataset is made publicly available, which can facilitate further research in the field of disinformation detection and analysis.",
            "3": "Potential reasons for acceptance\n   - The creation and public release of a novel dataset that includes multilingual disinformation articles is a valuable contribution to the NLP community.",
            "4": "- The paper provides a detailed methodology for data collection, topic analysis, and linguistic analysis, which can be replicated and built upon by other researchers.",
            "5": "- The analysis of linguistic properties and temporal patterns of disinformation articles offers new insights into the behavior and strategies of state-backed propaganda websites.",
            "6": "- The paper addresses the ethical considerations of working with disinformation data, which is important for responsible research practices.",
            "7": "Potential reasons for rejection\n   - **Lack of validation for topic analysis model**\n     - The topic analysis model has not been formally validated against human or expert annotations, which raises concerns about the accuracy and reliability of the identified topics.",
            "8": "- The paper mentions some small-scale manual validation, but this is not sufficient to establish the robustness of the topic analysis results.",
            "9": "- **Limited representativeness of the dataset**\n     - The dataset is limited to two specific disinformation websites, which may not fully represent the diversity of state-backed disinformation efforts.",
            "10": "- The paper acknowledges that the dataset is complementary to other sources like Sputnik and Russia Today, but the inability to include these sources due to access restrictions limits the comprehensiveness of the study.",
            "11": "- **Potential for reinforcing disinformation narratives**\n     - The process of creating and distributing a disinformation dataset could inadvertently increase the spread and prominence of harmful narratives, despite the authors' efforts to mitigate this risk.",
            "12": "- The paper should provide more detailed guidelines for responsible use of the dataset to ensure that it is not misused for generating new disinformation.",
            "13": "Suggestions for improvement\n   - **Validation of topic analysis model**\n     - Conduct a formal validation of the topic analysis model by comparing the identified topics with those assigned by human or expert annotators.",
            "14": "- Include a detailed discussion of the validation results and any adjustments made to improve the model's accuracy.",
            "15": "- **Expand the dataset to include more sources**\n     - Explore ways to include additional state-backed disinformation sources, such as Sputnik and Russia Today, to enhance the representativeness of the dataset.",
            "16": "- Consider collaborating with researchers or institutions that have access to these sources to supplement the dataset.",
            "17": "- **Provide detailed guidelines for responsible use**\n     - Develop and include comprehensive guidelines for the responsible use of the dataset, emphasizing the importance of presenting disinformation narratives alongside authoritative evidence of their falsehood.",
            "18": "- Highlight the ethical considerations and potential risks associated with using the dataset, and provide recommendations for mitigating these risks.",
            "19": "- **Enhance the linguistic analysis**\n     - Perform a more in-depth linguistic analysis, including a comparison with other reputable news sources beyond the New York Times, to provide a broader context for the findings.",
            "20": "- Investigate the use of specific linguistic features, such as sentiment and stance, to gain further insights into the strategies used by disinformation websites."
        },
        "YD3fDMVeKn": {
            "0": "This is an interesting paper.",
            "1": "I think the dataset can be very valuable not only for the NLP community, but also for analysis from other disciplines, such as discourse analysis, sociology, politics or journalism.",
            "2": "I found the analysis interesting, although more from a sociological, cultural perspective than from an NLP point of view.",
            "3": "There is no background analysis.",
            "4": "There is much work done in propaganda detection, both in the publication of linguistic resources, such as datasets, and in the development of models to detect it.",
            "5": "In addition, propaganda has been studied for many years from other disciplines.",
            "6": "All this background needs to be accounted for in a paper that presents a new resource for automatic propaganda detection.",
            "7": "Examples of the dataset would be very helpful in a paper that introduces a new dataset, as the reader learns about your work, but is not able to see it."
        },
        "03YFWWAWuf": {
            "0": "Scientifically sound article, relevant cues on disinformation, interesting discussion on the methods and their limitations, as well as on ethical issues.",
            "1": "Decisions made on the tools are clearly described and quite valuable.",
            "2": "Part of content analysis is specific to the websites of interest and cannot be generalized easily.",
            "3": "Beyond topic analysis which involves a great deal of fine-tuning and may not be easy to reproduce on other datasets, the article lacks more refined textual analysis, i.e.",
            "4": "proper sentiment detection other than word lists (section 3.5) and better word grouping or filtering methods than n-gram analysis (section 3.7)."
        },
        "9BGdyTKRqf": {
            "0": "This paper presents an interesting corpus and analysis of articles crawled from propaganda websites.",
            "1": "Upon publication, the presented corpus would offer a wide range of research possibilities.",
            "2": "Some manual annotations (for instance, on the factuality of the articles) would have been interesting; however, the presented work fits the short paper track very well."
        }
    },
    "XJRNw74kXK": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of verifying the real-world understanding of Large Language Models (LLMs) through a novel dataset, POSQA, which focuses on physical object size comparisons.",
            "1": "- The study is inspired by cognitive theories and aims to probe the embodied comprehension of LLMs, which is a relatively unexplored area in the context of LLM evaluation.",
            "2": "- The research provides insights into the limitations of LLMs in understanding real-world physical properties, which is significant for the development of more robust and reliable AI systems.",
            "3": "Potential reasons for acceptance\n   - The introduction of the POSQA dataset is a valuable contribution to the field, providing a new benchmark for evaluating the real-world understanding of LLMs.",
            "4": "- The paper presents a comprehensive analysis of LLM performance using various prompting techniques and external knowledge augmentation, offering detailed insights into the strengths and weaknesses of current models.",
            "5": "- The study's findings on the vulnerability of LLMs to prompt surface forms and the reliance on contextual information over internal weights are important for future research and development in AI.",
            "6": "- The methodology is well-documented, and the experiments are thorough, making the results reproducible and the conclusions credible.",
            "7": "Potential reasons for rejection\n   - **Limited scope of the dataset:**\n     - The POSQA dataset includes only 92 objects, which may not be sufficient to generalize the findings to a broader range of real-world scenarios.",
            "8": "- The dataset focuses solely on size comparisons, which might limit its applicability in evaluating other aspects of physical world understanding.",
            "9": "- **Sensitivity to prompt formats:**\n     - The study highlights that LLMs are sensitive to the format of the queries, which raises concerns about the robustness of the evaluation method.",
            "10": "- The performance differences observed with different prompt templates suggest that the results might be influenced by the specific phrasing used, rather than the underlying model capabilities.",
            "11": "- **Reliance on external knowledge:**\n     - The paper shows that LLMs perform significantly better when provided with external knowledge, which might indicate that the models lack inherent understanding and rely heavily on context.",
            "12": "- This reliance on external information could limit the practical applicability of LLMs in scenarios where such information is not readily available.",
            "13": "Suggestions for improvement\n   - **Expand the dataset:**\n     - Increase the number of objects in the POSQA dataset to cover a wider range of physical entities and scenarios, enhancing the generalizability of the findings.",
            "14": "- Include additional types of physical reasoning tasks beyond size comparisons to provide a more comprehensive evaluation of LLMs' real-world understanding.",
            "15": "- **Address prompt sensitivity:**\n     - Investigate methods to reduce the sensitivity of LLMs to prompt formats, such as using more standardized or controlled prompt templates.",
            "16": "- Explore the impact of different prompt phrasing on model performance in greater detail to identify best practices for prompt design.",
            "17": "- **Enhance model robustness:**\n     - Develop techniques to improve the inherent understanding of LLMs, reducing their reliance on external knowledge and making them more robust in real-world applications.",
            "18": "- Consider incorporating multimodal data (e.g., visual information) to provide LLMs with a more holistic understanding of physical properties and interactions.",
            "19": "- **Conduct human comparison studies:**\n     - Perform more extensive human comparison studies to better understand the alignment between LLMs and human cognition, particularly in terms of physical world understanding.",
            "20": "- Use these studies to identify specific areas where LLMs fall short and guide future model improvements."
        },
        "rq93pabyw8": {
            "0": "The paper presents a novel dataset aimed at evaluating the parametric knowledge of Large Language Models on a specific type of questions that has not been extensively explored in previous research.",
            "1": "The probing setup utilized in the study is well-structured and methodologically sound.",
            "2": "Furthermore, the conclusions drawn from the experimental results are well-supported and credible.",
            "3": "The paper mentions the study of the alignment of LLM knowledge with human comprehension of the world as one of its primary contributions.",
            "4": "However, this aspect is only briefly addressed in the discussion section, where a small-scale experiment is conducted.",
            "5": "Unfortunately, the analysis of this experiment is limited, as it solely compares ChatGPT's performance with that of human annotators.",
            "6": "Furthermore, the paper contains several grammatical errors that may impede comprehension, particularly for non-native English speakers.",
            "7": "To enhance the paper's clarity, I suggest addressing the listed grammatical errors in the Presentation Improvements field and conducting a more thorough analysis of the experiment comparing LLMs' knowledge to human understanding.",
            "8": "This could provide deeper insights into the alignment or disparities between the two."
        },
        "rGWB7fkvvT": {
            "0": "The dataset is interesting, new, and well-designed.",
            "1": "On the last point, its unique topic fills an apparent gap in the literature, allowing easily interpretable experimental design for the paper's main research questions on in-context learning.",
            "2": "The takeaways are mostly clear, not well-known, and interesting.",
            "3": "Writing is mostly clear.",
            "4": "**Main Point**\nSensitivity analysis on prompts and parameters is not provided.",
            "5": "For instance, temperature can be an important factor in generating factual responses (as in the studied case).",
            "6": "Similarly, LLMs can be sensitive to prompts in a way that is decidedly not human-like.",
            "7": "Experimenting with a variety of different prompt formats / parameters and testing for consistency is an important, missing analysis.",
            "8": "Doing so on the entire dataset, with all LLMs, and all prompts would be challenging - but it is reasonable to check these things on a smaller subset.",
            "9": "**Similar Minor Points**\n - Similarly to the above, but a more minor point, some sensitivity analysis on the answer mapping would be nice.",
            "10": "For instance, your error analysis in the appendix indicates trouble with the \"AIDS virus\" - this may be caused by AI safety features.",
            "11": "- There appears to be a general lack of analysis on statistical significance, which can be important when interpreting many results.",
            "12": "This may cause the authors/readers to draw false positive inferences.",
            "13": "I do consider this a minor point, since most takeaways are not inferred by \"close calls\" - most discussed differences in accuracy, for instance, are substantial.",
            "14": "- Some results may be \"over stated\", for instance, the gap between ChatGPT performance and average human performance is < 20%.",
            "15": "Arguably, this is the type of variability one might expect from humans of varying experiences/education levels.",
            "16": "*Post-rebuttal Revisions*\nThe main points are addressed by authors during the rebuttal, and authors have agreed to include these in the paper (Appendix with pointers in the main text is fine)."
        },
        "GzvOg1UIiz": {
            "0": "Interesting, simple, and focused (size comparison) approach to probe real-world understanding in LLMs.",
            "1": "The POSQA dataset is the main contribution of the paper.",
            "2": "The paper lays out the methodology and evaluation metrics in detail to systematically test the LLMs.",
            "3": "The results are relevant and interesting.",
            "4": "They show that LLMs are still far from humans in real-world understanding.",
            "5": "Moreover, model’s preference between context and weights is an important finding and can potentially lead to interesting future investigations.",
            "6": "The results might not be very general for actual “real-world understanding” of LLMs.",
            "7": "Size comparison is still just one part of the bigger picture about real-world understanding.",
            "8": "The link to the dataset is not active.",
            "9": "So, the claims made about the dataset can’t be verified.",
            "10": "I would request the authors to look into it.",
            "11": "No details about the human annotation process used for comparison with ChatGPT."
        }
    },
    "Bou2YHsRvG": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the significant challenge of lexical ambiguity in Neural Machine Translation (NMT), particularly focusing on polysemous words.",
            "1": "- It introduces a novel approach called Word Sense Pretraining for Neural Machine Translation (WSP-NMT), which leverages word sense-specific information from Knowledge Bases to improve translation quality.",
            "2": "- The approach is evaluated on various challenging data and resource-scarce scenarios, demonstrating its robustness and effectiveness.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and well-motivated approach to addressing lexical ambiguity in NMT, a known challenge in the field.",
            "4": "- The experimental results show significant improvements in translation quality, particularly in low-resource and zero-shot translation scenarios.",
            "5": "- The integration of word sense information and structured knowledge in multilingual pretraining is a valuable contribution to the field.",
            "6": "- The paper provides a thorough evaluation, including fine-grained accuracy improvements on the DiBiMT disambiguation benchmark.",
            "7": "Potential reasons for rejection\n   - **Limited evaluation on under-represented languages:**\n     - The paper acknowledges that WSP-NMT may not be as effective for under-represented languages due to the lack of high-quality disambiguation resources and Knowledge Bases.",
            "8": "- The experiments on Indo-Iranian languages show that the approach does not outperform the baseline, highlighting the limitations in low-resource language settings.",
            "9": "- **Dependency on external resources:**\n     - The approach relies heavily on the availability and quality of external resources such as BabelNet and WSD systems.",
            "10": "- Any biases or inaccuracies in these resources could propagate into the pretraining data and affect the final translation quality.",
            "11": "- **Complexity and computational cost:**\n     - The proposed approach involves multiple stages, including word sense disambiguation, extraction of translations from BabelNet, and generation of morphological inflections.",
            "12": "- This added complexity and computational cost may limit the practical applicability of the approach, especially for large-scale multilingual NMT systems.",
            "13": "Suggestions for improvement\n   - **Expand evaluation to more under-represented languages:**\n     - Conduct experiments on a wider range of under-represented languages to better understand the limitations and potential improvements of WSP-NMT in these settings.",
            "14": "- Explore the creation of small sense-annotated datasets for these languages to enhance the effectiveness of the approach.",
            "15": "- **Address dependency on external resources:**\n     - Investigate methods to mitigate the impact of biases and inaccuracies in external resources such as BabelNet and WSD systems.",
            "16": "- Consider incorporating additional sources of knowledge or alternative disambiguation techniques to improve robustness.",
            "17": "- **Optimize computational efficiency:**\n     - Explore ways to streamline the multiple stages of the approach to reduce complexity and computational cost.",
            "18": "- Investigate the use of more efficient WSD systems or alternative methods for generating morphological inflections to enhance scalability.",
            "19": "- **Provide more detailed analysis of error cases:**\n     - Include a more comprehensive analysis of the types of errors made by the baseline and WSP-NMT models, particularly in challenging disambiguation scenarios.",
            "20": "- Highlight specific examples where WSP-NMT outperforms the baseline to provide deeper insights into the strengths and weaknesses of the approach."
        },
        "drqrL0wzP7": {
            "0": "The quality of writing is high and reader-friendly.",
            "1": "The paper is well-organized.",
            "2": "The idea presented is logical and easy to understand.",
            "3": "Comprehensive experiments are conducted to verify the effectiveness of the proposed method in different scenarios, such as different language pairs across high, medium, and low resources.",
            "4": "For some experimental results, there is a lack of reasonable and sufficient explanation.",
            "5": "For instance, in figure 3, the authors' method underperforms the baseline in the en-fr and fr-en settings.",
            "6": "The reason and analysis for this are missing.",
            "7": "A majority of the experiments focus on the presentation of results.",
            "8": "The analyses of the method itself and the experimental outcomes are not comprehensive enough.",
            "9": "Given that the authors' method underperforms the baseline in some instances, one might question to what extent the performance improvement brought by this pretraining method can be attributed to the authors' claim of \"moving code-switched pretraining from the word level to the sense level, by leveraging word sense-specific information from Knowledge Bases\"."
        },
        "rY9db7Mnsq": {
            "0": "See the main contributions.",
            "1": "See the weaknesses in Paper Topic And Main Contributions."
        },
        "dva6eNI0YY": {
            "0": "The motivation of this paper is clear.",
            "1": "- The authors target the core weakness of the existing code-switched pre-training strategy, which struggles to address lexical ambiguity by generating noise with sense-agnostic lexicons.",
            "2": "The authors show their achievements in resolving lexical ambiguity effectively.",
            "3": "- It's especially noteworthy for the performance gains on verb inflections, which are difficult to disambiguate due to the plethora of polysemous words.",
            "4": "The experimental configuration in terms of high, medium, and low resource settings slightly departs from other studies.",
            "5": "- The authors set 100\\~200K, 500K\\~1M, 1.5M\\~3M as low, medium, and high resource settings based on parallel corpus size.",
            "6": "However, in the PC32 dataset paper [2] used by the previous work [1], which the authors follow the experimental setting, much larger datasets were used as rich-source.",
            "7": "Considering Figure 3 and 5 (if x-axis is scaled to a huge size of dataset), it might be unable to beat AA [1] in high-resource setting.",
            "8": "The applicability of WSP-NMT is questionable.",
            "9": "- The authors claim that WSP-NMT outperforms on low-resource language pairs.",
            "10": "Since many data-scarce language pairs are under-represented in WSD models and BabelNet, there might be several situations where it is difficult to utilize WSP-NMT.",
            "11": "[2] Lin et al., 2020, Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information"
        }
    },
    "I13VHLJjLO": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Reward-Augmented Decoding (RAD), a novel text generation procedure that uses a unidirectional reward model to encourage a language model to generate text with certain desired properties.",
            "1": "- RAD is significant because it addresses the challenge of controlling text generation without the need for additional training, which can be computationally expensive and may degrade performance on text different from the training data.",
            "2": "- The novelty lies in the use of a unidirectional reward model that caches activations from prior generation steps, reducing computational overhead and making the method efficient and scalable.",
            "3": "Potential reasons for acceptance\n   - The method is innovative and addresses a critical problem in the field of natural language processing: controlled text generation.",
            "4": "- The paper provides a thorough evaluation of RAD, demonstrating its effectiveness in generating non-toxic and sentiment-controlled text.",
            "5": "- RAD outperforms other weighted decoding methods and matches the performance of state-of-the-art methods that involve additional training, showing its practical utility.",
            "6": "- The approach is computationally efficient, making it suitable for use with very large language models, as evidenced by its application to the LLaMA family of models.",
            "7": "Potential reasons for rejection\n   - **Limited scope of experiments**\n     - The experiments focus primarily on detoxification and sentiment control, which may not fully capture the potential of RAD in other text generation tasks.",
            "8": "- Additional experiments on a wider range of tasks could strengthen the paper's claims about the generalizability of RAD.",
            "9": "- **Dependence on specific reward models**\n     - The effectiveness of RAD relies on the quality of the reward model, which may vary depending on the task and the data used for training the reward model.",
            "10": "- The paper does not explore the impact of different reward models on the performance of RAD, which could be a limitation.",
            "11": "- **Potential computational overhead**\n     - While the paper claims that RAD incurs minimal computational overhead, the actual overhead may vary depending on the size of the language model and the reward model.",
            "12": "- The paper could provide more detailed analysis and comparisons of computational costs across different model sizes and configurations.",
            "13": "Suggestions for improvement\n   - **Expand the range of experiments**\n     - Conduct experiments on additional text generation tasks, such as style transfer, question answering, and dialogue generation, to demonstrate the versatility and robustness of RAD.",
            "14": "- Include more diverse datasets to evaluate the performance of RAD in different contexts and with different types of text.",
            "15": "- **Analyze the impact of different reward models**\n     - Investigate how the choice of reward model affects the performance of RAD, including experiments with different types of reward models and training data.",
            "16": "- Provide insights into how to select or design effective reward models for various text generation tasks.",
            "17": "- **Detailed computational cost analysis**\n     - Offer a more comprehensive analysis of the computational costs associated with RAD, including comparisons with other methods across different model sizes and configurations.",
            "18": "- Include metrics such as memory usage, inference time, and scalability to provide a clearer picture of the practical implications of using RAD.",
            "19": "- **Address potential limitations**\n     - Discuss potential limitations of RAD, such as its dependence on the quality of the reward model and the trade-offs between fluency, diversity, and alignment with desired attributes.",
            "20": "- Provide guidelines or best practices for practitioners to mitigate these limitations when applying RAD to real-world text generation tasks."
        },
        "i3u19r36zk": {
            "0": "- The proposed method is superior both in performance and decoding\n  speed in comparison with other weighted decoding methods and\n  re-training methods.",
            "1": "- The paper is relatively easy to follow.",
            "2": "- It is not clear why the proposed method achieves better performance\n  (e.g., average max toxicity) in comparison with other weighted\n  decoding methods and achieves results comparable to re-training\n  methods.",
            "3": "Discussion is required for this point."
        },
        "l19ZKksWUM": {
            "0": "* The proposed method is intuitive and sound.",
            "1": "Moreover, it is easy to implement and does not cost excessive computation resources.",
            "2": "* The experiments show that the method is effective.",
            "3": "* The description of the method is not clear.",
            "4": "Specifically, the term $\\hat{r}$ in Section 2.2 is not clearly explained.",
            "5": "The source of this label is questionable.",
            "6": "* The contribution of the paper is thin.",
            "7": "Essentially, this is a combination of the well-known re-ranking technique and the now popular reward models.",
            "8": "Therefore, it does not provide interesting technical insights."
        },
        "A11XlBZPCH": {
            "0": "The proposed method is simple yet effective and thus should influence subsequent research in controlled text generation.",
            "1": "The proposed method achieves a good trade-off between efficiency and effectiveness compared to previous weighted decoding and fine-tuning methods.",
            "2": "The results of the experiment are reliable because of the extensive comparison with previous models and the recalculation of the scores of previous models based on nice attention to the differences in API.",
            "3": "I found no major reasons for rejection, but it would be more insightful to explain why the proposed method performs better than other weighted decoding methods."
        }
    },
    "ZSHcpMXWxX": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces DUMB, a new benchmark for evaluating Dutch language models, which fills a gap in the current landscape of language model evaluation for Dutch.",
            "1": "- The benchmark includes a diverse set of nine tasks, four of which were previously unavailable in Dutch, enhancing the comprehensiveness of the evaluation.",
            "2": "- The introduction of the Relative Error Reduction (RER) metric offers a novel approach to comparing model performance across tasks, potentially providing more meaningful insights than traditional mean score metrics.",
            "3": "Potential reasons for acceptance\n   - The creation of a new benchmark specifically for Dutch language models addresses a significant need in the NLP community, promoting further research and development in this area.",
            "4": "- The inclusion of previously unavailable tasks in Dutch broadens the scope of evaluation and provides a more robust assessment of model capabilities.",
            "5": "- The proposed RER metric offers a potentially more accurate and insightful method for comparing model performance, which could be adopted by other benchmarks and studies.",
            "6": "- The paper provides a thorough evaluation of 14 pre-trained language models, offering valuable insights into the current state of Dutch language modeling and identifying areas for improvement.",
            "7": "Potential reasons for rejection\n   - **Limited scope of tasks**:\n     - The benchmark does not include every possible NLP task type, such as parsing, which could limit its comprehensiveness.",
            "8": "- The focus on classification tasks may overlook other important aspects of language understanding and generation.",
            "9": "- **Potential bias in task selection**:\n     - The selection of tasks and datasets may introduce biases that could affect the generalizability of the benchmark results.",
            "10": "- The reliance on existing datasets with varying licenses and availability could limit the reproducibility and accessibility of the benchmark.",
            "11": "- **Evaluation methodology**:\n     - The use of the RER metric, while novel, may not be directly comparable to aggregate scores from other benchmarks, potentially limiting its adoption and impact.",
            "12": "- The evaluation focuses solely on Transformer-encoder models, excluding generative models, which could provide a more comprehensive assessment of language model capabilities.",
            "13": "- **Model comparison limitations**:\n     - The comparison includes more English models than Dutch models, which may skew the results and conclusions.",
            "14": "- The absence of large-sized Dutch models limits the ability to fully assess the potential of Dutch language models compared to their multilingual and English counterparts.",
            "15": "Suggestions for improvement\n   - **Expand the scope of tasks**:\n     - Include additional NLP task types, such as parsing and coreference resolution, to provide a more comprehensive evaluation of language models.",
            "16": "- Consider incorporating tasks that require generative capabilities to assess a broader range of model functionalities.",
            "17": "- **Address potential biases**:\n     - Ensure a balanced selection of tasks and datasets to minimize biases and improve the generalizability of the benchmark results.",
            "18": "- Provide clear guidelines and documentation for dataset usage and licensing to enhance reproducibility and accessibility.",
            "19": "- **Enhance evaluation methodology**:\n     - Compare the RER metric with traditional aggregate scores to demonstrate its advantages and encourage its adoption by other benchmarks.",
            "20": "- Include evaluations of generative models and explore prompt engineering approaches to provide a more holistic assessment of language model capabilities.",
            "21": "- **Broaden model comparison**:\n     - Include more Dutch models, particularly large-sized variants, to provide a more balanced and comprehensive comparison.",
            "22": "- Explore the development and evaluation of Dutch DeBERTaV3 models to validate the potential improvements suggested by the regression model analysis."
        },
        "SRTuh5IrR8": {
            "0": "The paper presents a thorough comparison of monolingual and multilingual Dutch pre-trained and non-Dutch pre-trained models across 9 different tasks.",
            "1": "The paper presents a new metric for performance comparison across different tasks.",
            "2": "An analysis based on current results is presented to support the nature of pre-training that could improve performance on this task for future work.",
            "3": "L535-536: It is unclear why RER correlation is a more intuitive way to compare performance across tasks and what it actually measures (missing an intuitive explanation rather than just comparing the numbers).",
            "4": "Also to clarify, it is clear what RER captures but not what the correlation does.",
            "5": "A regression analysis is presented to identify and reason why non-dutch models outperform dutch-pre-trained models, however while the analysis presents some insights on the nature of pretraining and model size, it is unclear why and how the choice of pretraining language would play a role."
        },
        "cxBZ2ms5fP": {
            "0": "The work presented in this paper is relevant to the further development of Dutch and multi-lingual LMs.",
            "1": "The benchmark is well constructed, tested quite extensibly with existing LMs and the results reveal directions for further research.",
            "2": "The paper is well written and has a logical structure.",
            "3": "I currently do not see any reasons to reject this paper."
        },
        "OVOUZPRAZW": {
            "0": "The benchmark datasets are an interesting contribution to the analysis of the Dutch language.",
            "1": "The paper includes extensive experimentation and meaningful analysis of existing models according to the tasks.",
            "2": "The inclusion of the Abusive Language Detection is of particular importance, as it is not included in GLUE or similar benchmarks.",
            "3": "The work might be significant only to researchers working on Dutch, but even then other researchers might find interesting ideas in this work to apply to other languages."
        }
    },
    "I5hTganf3z": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces VECHR, a novel expert-annotated multi-label dataset for vulnerability type classification and explanation rationale in the European Court of Human Rights (ECtHR) cases.",
            "1": "- It addresses a significant gap in NLP research by focusing on the concept of vulnerability, which is crucial for effective human rights protection but remains elusive within the ECtHR.",
            "2": "- The dataset and associated tasks offer a challenging and useful resource for Legal NLP researchers, with potential applications in improving litigation strategy and legal policy.",
            "3": "Potential reasons for acceptance\n   - The introduction of a novel dataset (VECHR) that fills a critical gap in the legal NLP domain, specifically focusing on vulnerability classification in ECtHR cases.",
            "4": "- Comprehensive benchmark results using state-of-the-art models, highlighting the challenging nature of the task and providing a baseline for future research.",
            "5": "- The inclusion of a token-level explanation dataset (VECHR explain) annotated by domain experts, which enhances the explainability and trustworthiness of the models.",
            "6": "- The paper's thorough analysis of model robustness to distributional shifts, which is crucial for real-world applications in the legal domain.",
            "7": "Potential reasons for rejection\n   - **Limited dataset size and annotation scope:**\n     - The dataset is limited to 1,070 cases, which may not be sufficient for training robust models, especially given the complexity of legal texts.",
            "8": "- The annotation process is restricted to a single expert per case, which may introduce bias and limit the reliability of the annotations.",
            "9": "- **Low overall model performance:**\n     - The benchmark results indicate low overall performance across models, with limited agreement between models and experts, highlighting the challenging nature of the task.",
            "10": "- The models exhibit limited robustness to distributional shifts, as evidenced by the drop in performance on the VECHR challenge dataset.",
            "11": "- **Potential ethical and legal concerns:**\n     - The dataset includes non-anonymized documents with real names of individuals involved, which may raise privacy and ethical concerns.",
            "12": "- The potential for misuse of the vulnerability classification models, especially in high-stakes legal contexts, needs to be carefully considered and addressed.",
            "13": "Suggestions for improvement\n   - **Expand the dataset size and scope:**\n     - Increase the number of annotated cases to provide a more comprehensive dataset for training and evaluation.",
            "14": "- Include multiple annotations per case to capture a more nuanced and multi-faceted understanding of vulnerability.",
            "15": "- **Enhance model performance and robustness:**\n     - Explore advanced model architectures and training techniques to improve classification accuracy and explainability.",
            "16": "- Investigate methods to better handle distributional shifts, such as domain adaptation or transfer learning approaches.",
            "17": "- **Address ethical and legal concerns:**\n     - Anonymize the dataset to protect the privacy of individuals involved in the cases.",
            "18": "- Develop guidelines and safeguards to prevent the misuse of the models, ensuring they are used responsibly and ethically in legal contexts.",
            "19": "- **Improve the clarity and presentation of the paper:**\n     - Provide more detailed explanations of the annotation process and the criteria used for labeling vulnerability types.",
            "20": "- Include additional visualizations and examples to illustrate the dataset characteristics and model performance more effectively."
        },
        "iqRUNYla4e": {
            "0": "R1: It constructs a dataset for an important task.",
            "1": "R2: The dataset is constructed very well, with details provided on annotation agreement, annotators’ backgrounds, etc.",
            "2": "R3: The paper is very well written.",
            "3": "I learned some things about how to write a good appendix for a data resource paper.",
            "4": "Even though many pages of the appendix are used, it is written very cleverly.",
            "5": "Readers who are not reviewers or want to reproduce don’t need to read the appendix.",
            "6": "This is particularly hard to do for a short paper.",
            "7": "Kudos for that.",
            "8": "R4: Good well-thought effort is put into writing the limitations and ethics statement section.",
            "9": "It would be sad to reject such a good paper for just this reason but I will state it so they can improve it.",
            "10": "Hyperparameter information for fine-tuning like batch size, and learning rate is missing.",
            "11": "Also, what version of BERT is used is missing.",
            "12": "It is better to specify a base or large, cased or uncased, and other details for each model."
        },
        "VQX5bql8EG": {
            "0": "**Novel Dataset Creation**: The introduction of the VECHR dataset is the main contribution.",
            "1": "It fills a research gap by providing an expert-annotated multi-label dataset for vulnerability type classification, a previously unexplored area in NLP.",
            "2": "**Explainability Focus**: By including explanation rationale in the dataset, the paper aligns with the growing interest in explainable AI.",
            "3": "This aspect allows for more interpretable models, crucial for applications in legal and social contexts.",
            "4": "**Interdisciplinary Relevance**: The paper's focus on human rights and legal applications showcases the breadth of NLP applicability, bridging the gap between technology and social sciences.",
            "5": "**Limited Generalizability**: The focus on a very specific legal context (ECtHR) may limit the generalizability of the findings and the applicability of the dataset to other legal systems or vulnerability assessments.",
            "6": "A broader analysis or comparison with other contexts might have enhanced the paper's value.",
            "7": "**Distribution of content:** For a work on a dataset, specific definitions of data types, the data collection process, and detailed distribution of the dataset are crucial.",
            "8": "However, these contents are placed in the appendix, and there is a lack of necessary analysis in the main body."
        },
        "aTp91IkKP8": {
            "0": "This paper provides the dataset VECHR, which can bridge the NLP tools with experts in efficiently classifying and analyzing vulnerability, where vulnerability is important in ECtHR.",
            "1": "This dataset is small, especially the annotation scale of explanation rationale.",
            "2": "Due to the small size of the evaluation data, the evaluation results may be unstable.",
            "3": "This paper lacks the performance of LLM (such as LLaMA 13B, chatGPT) on this dataset."
        },
        "N3KSgD4g5A": {
            "0": "First of all, the paper is very well motivated, presented and written.",
            "1": "It was worth a read.",
            "2": "It is true that datasets are scarce in the legal domain.",
            "3": "Given the challenges and intricacies of the domain, NLP models are still far away from producing trustworthy explainable results which could be directly consumed without human intervention.",
            "4": "In this regard, the proposed dataset is a valuable contribution.",
            "5": "It is additionally novel given the task of vulnerability type analysis from European legal cases.",
            "6": "The paper proposes multiple test sets to evaluate various facets of compared models.",
            "7": "In this regard, the thought process of the authors and their efforts towards getting multiple test sets annotated is noteworthy.",
            "8": "It adds value to the paper.",
            "9": "Data collection and annotation steps are thoroughly explained with appropriate details.",
            "10": "Although I found certain discrepancies.",
            "11": "Please refer to my questions below.",
            "12": "Despite being a short paper, the authors have made sure to include necessary details in the appendix as well.",
            "13": "While more recent large language models could have been tried out, given that this is primarily a dataset and benchmarking paper, I am satisfied with the experiments.",
            "14": "All compared models have fairly low scores, which gives ample scope to future researchers to come up with better NLP models over time.",
            "15": "Finally, limitations of the work are clearly mentioned.",
            "16": "Although I am inclined towards accepting the paper, I have a few concerns:\n1.",
            "17": "Some of the dataset statistics reported in Section 4 and appendix I do not seem to correlate.",
            "18": "The dataset size is small with close to 50% of the documents not labelled with any vulnerability label.",
            "19": "Additionally, only 2 labels dominate the overall label distribution.",
            "20": "This raises questions on the dataset quality as well the scope for future models to perform well on this dataset.",
            "21": "At least one generative model could have been tried out leveraging the descriptions of vulnerability types to improve the classification performance.",
            "22": "Description of the proposed \"Concept-aware Hierarchical\" model is not clear."
        }
    },
    "D4Cb4gAWro": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces TextReact, a novel method that augments predictive chemistry models with text retrieval from scientific literature.",
            "1": "- It addresses the limitations of conventional chemoinformatics models that rely solely on structured molecular data by incorporating unstructured textual information.",
            "2": "- The approach is validated on two significant chemistry tasks: reaction condition recommendation and one-step retrosynthesis, demonstrating substantial performance improvements.",
            "3": "Potential reasons for acceptance\n   - The method shows a significant improvement over state-of-the-art chemoinformatics models, with a 58.4% increase in top-1 prediction accuracy for reaction condition recommendation and a 13.6-15.7% improvement for one-step retrosynthesis.",
            "4": "- The paper provides a comprehensive evaluation, including both random and time-based splits, which validates the generalization capability of the proposed method.",
            "5": "- The integration of text retrieval with molecular representations is a novel approach that can be extended to other domains and tasks, showcasing the potential for broader impact.",
            "6": "- The authors provide a detailed description of the methodology, including the training process, model architecture, and evaluation metrics, ensuring reproducibility.",
            "7": "Potential reasons for rejection\n   - **Limited scope of tasks evaluated**:\n     - The paper focuses on only two chemistry tasks, which may not fully demonstrate the versatility and applicability of the proposed method across different domains.",
            "8": "- Additional tasks or applications could strengthen the argument for the method's generalizability and broader impact.",
            "9": "- **Dependence on the quality of retrieved texts**:\n     - The performance of TextReact heavily relies on the quality and relevance of the retrieved texts, which may not always be guaranteed.",
            "10": "- The paper does not thoroughly address scenarios where the retrieved texts are of low quality or irrelevant, which could impact the model's performance.",
            "11": "- **Complexity and computational cost**:\n     - The method involves multiple components, including a SMILES-to-text retriever and a text-augmented predictor, which may increase the complexity and computational cost of the approach.",
            "12": "- The paper does not provide a detailed analysis of the computational resources required, which could be a concern for practical implementation.",
            "13": "Suggestions for improvement\n   - **Expand the scope of tasks**:\n     - Evaluate the method on additional chemistry tasks or other domains to demonstrate its versatility and broader applicability.",
            "14": "- Include tasks that involve different types of chemical reactions or properties to showcase the method's robustness.",
            "15": "- **Address the quality of retrieved texts**:\n     - Provide a more detailed analysis of how the quality and relevance of retrieved texts impact the model's performance.",
            "16": "- Explore methods to filter or rank the retrieved texts to ensure higher quality inputs for the predictor.",
            "17": "- **Optimize computational efficiency**:\n     - Include a detailed analysis of the computational resources required for training and inference, and explore ways to optimize the efficiency of the method.",
            "18": "- Consider joint training of the retriever and predictor to potentially reduce the overall computational cost and improve performance.",
            "19": "- **Enhance interpretability**:\n     - Provide more insights into how the retrieved texts contribute to the model's predictions, potentially through case studies or qualitative analysis.",
            "20": "- Include visualizations or examples that illustrate the alignment between molecular representations and textual information, enhancing the interpretability of the method."
        },
        "wSedisjIVh": {
            "0": "The paper is easy to follow and is nicely read in most parts.",
            "1": "The proposed retriever may benefit other practitioners and researchers in the field of chemistry if it is open-sourced.",
            "2": "The effectiveness of the proposed model has not been sufficiently demonstrated because state-of-the-art models were not compared in the experiments.",
            "3": "In the one-step retrosynthesis task, while several previous models exhibit superior performance than the performance of the proposed model, they were not considered in the conducted experiments (Table 4).",
            "4": "Please review previous studies [1,2] and this link [3] to ensure that essential baselines have not been overlooked.",
            "5": "If the authors feel that it is unfair to compare the previous best models I have mentioned with the proposed model (i.e., TextReact), please reply during the author response period.",
            "6": "I will change the initial score if the responses resolve my concerns.",
            "7": "[1] Zhu et al., O -GNN: incorporating ring priors into molecular modeling, ICLR 2023\n\n[2] Seidl et al., Improving few-and zero-shot reaction template prediction using modern hopfield networks, Journal of chemical information and modeling, 2022\n\n[3] https://paperswithcode.com/sota/single-step-retrosynthesis-on-uspto-50k"
        },
        "8wFPGhHLj7": {
            "0": "TextReact allows for accurate prediction of reaction properties with minimal overhead.",
            "1": "This work will likely inspire novel research into retrieval augmented reaction prediction.",
            "2": "The paper does not study any additional methods but bi-encoders.",
            "3": "The contribution would significantly be improved by integrating a cross-encoder or sparse retriever like BM25.",
            "4": "It is unclear how the model performs in more difficult molecular reactions where there can be one, many or no catalysts or solvents\n3.",
            "5": "Retrieval metrics are missing such as recall@100"
        },
        "aUpPgoL8XW": {
            "0": "The experiments provide strong support for their claim, achieving more than twice as high accuracy compared to the best baseline ChemBERTa.",
            "1": "The paper further considers two different types of train/test splits (random and time-based).",
            "2": "The advantage of the proposed method is confirmed in both scenarios.",
            "3": "Claims are further supported through careful ablations, such as testing a setting where the gold texts are removed and a setting where only the historical corpus is used in the time split setting.",
            "4": "The method still shows a solid advantage over a Transformer baseline (which was ~on par with ChemBERTa in other experiments).",
            "5": "The paper could be further improved by analyzing the influence of SMILES input vs. retrieved text input.",
            "6": "Given the comparison with the literature that only uses SMILES input, it seems that retrieved text provides an extremely rich source of information.",
            "7": "It leads to the question how a model would perform only based on retrieved text, if feasible.",
            "8": "The paper would benefit from a discussion of how useful such models are in practice (see Question)."
        }
    },
    "9K1urVN7ti": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces DueT, a novel transfer learning method for vision and language models using contrastive learning.",
            "1": "- DueT incorporates adapters with a gating mechanism into image and text encoders, which are initialized from pre-trained uni-modal models and then frozen.",
            "2": "- The method aims to achieve efficient learning with fewer trainable parameters while preventing catastrophic forgetting.",
            "3": "Potential reasons for acceptance\n   - The proposed method demonstrates significant improvements in accuracy and parameter efficiency over existing methods like simple fine-tuning, LiT, and LoRA.",
            "4": "- DueT's approach of using gated adapters is innovative and addresses the challenge of catastrophic forgetting effectively.",
            "5": "- The experimental results are comprehensive, covering both English and Japanese domains, and show consistent performance gains across various datasets and tasks.",
            "6": "- The paper provides detailed ablation studies and analysis, which strengthen the validity of the proposed method.",
            "7": "Potential reasons for rejection\n   - **Limited evaluation scope**\n     - The study focuses primarily on classification and retrieval tasks, leaving out other potential zero-shot transfer tasks like detection, segmentation, visual question answering, and image captioning.",
            "8": "- The effectiveness of DueT in LM-based VLMs and other tasks remains unexplored.",
            "9": "- **Dependency on pre-trained models**\n     - The method relies heavily on the quality and domain coverage of the pre-trained uni-modal models, which may limit its applicability in scenarios where such models are not available or are of lower quality.",
            "10": "- The paper does not thoroughly investigate the impact of different pre-trained models on the performance of DueT.",
            "11": "- **Complexity and implementation details**\n     - The introduction of gated adapters adds complexity to the model, which may pose challenges in implementation and tuning for practitioners.",
            "12": "- The paper lacks a detailed discussion on the computational overhead introduced by the gating mechanism and how it compares to other parameter-efficient methods.",
            "13": "Suggestions for improvement\n   - **Expand evaluation scope**\n     - Include additional zero-shot transfer tasks such as detection, segmentation, visual question answering, and image captioning to demonstrate the broader applicability of DueT.",
            "14": "- Evaluate the method on LM-based VLMs to provide insights into its effectiveness in different model architectures.",
            "15": "- **Investigate dependency on pre-trained models**\n     - Conduct experiments with a wider range of pre-trained models to understand the impact of their quality and domain coverage on DueT's performance.",
            "16": "- Provide a detailed analysis of how different pre-trained models affect the transfer learning process and the final results.",
            "17": "- **Simplify and clarify implementation**\n     - Offer a more detailed explanation of the gating mechanism, including its computational overhead and how it compares to other parameter-efficient methods.",
            "18": "- Provide practical guidelines and best practices for implementing and tuning DueT, making it more accessible to practitioners.",
            "19": "- **Address potential limitations**\n     - Discuss potential limitations of DueT, such as its reliance on pre-trained models and the complexity introduced by the gating mechanism, and suggest possible solutions or future research directions to address these issues."
        },
        "F83HmFp575": {
            "0": "- The proposed method is simple and effective.",
            "1": "It is technically valid and can enhance the image-text contrast learning framework.",
            "2": "- The authors describe their network design in detail, including the architecture of the proposed gated adapter unit.",
            "3": "- The authors considered an array of design choices for the baselines, and provided clear comparisons to demonstrate the effectiveness of DueT.",
            "4": "- Large-scale pre-training is conducted to validate the idea, including pre-training on public (YFCC) and private (JWeb) dataset, respectively.",
            "5": "- The paper is well-written and easy to understand.",
            "6": "I don't find serious concerns.",
            "7": "The only minor concern is that adding GAUs would also increase training parameters, from 1% to 58%, depending on the hyper-parameter.",
            "8": "But considering that all other layers are frozen, the increased num parameters are acceptable."
        },
        "ZhYTsthpTl": {
            "0": "- The paper does a good job of applying ideas from parameter-efficient fine-tuning tuning (PEFT) methods to the problem of image-text contrastive learning.",
            "1": "The results demonstrate the effectiveness of the proposed solution, evidenced by the strong results on zero-shot retrieval and image classification.",
            "2": "- Experiments are conducted not only on the standard English benchmarks, but also on Japanese datasets.",
            "3": "It is rare to come across a multimodal method that shows experiments on languages beyond English, so it's great to see a non-English language represented in the experiments.",
            "4": "- The methodology section is extremely well structured: 3.1 explains all the background well, 3.2 explains the GAU method well, and 3.3 motivates all the design choices very well, effectively explaining how different design choices correspond to different learning settings (L347-L359).",
            "5": "Everything was well-written and extremely easy to follow.",
            "6": "- The various ablation studies in Section 4.4 are also very well-motivated, and experiments are designed accordingly -- specifically, how the size of the bottleneck layer affects the model's learning ability, the utility of GAUs in different layers, the analysis of gate values at different layers, the effect of gate initialization, and effect of amount of pre-training data.",
            "7": "All of these are very interesting questions that have been addressed in the ablation study.",
            "8": "There are no real reasons to reject this work -- some additional details would be useful (mentioned in the Questions for Authors section), but these are not grounds for rejecting the work in my opinion."
        },
        "nyXfKxVH0u": {
            "0": "1: The author suggested a method to use adapters with pre-trained image and text encoders.",
            "1": "This reduced the number of parameters that needed to be trained.",
            "2": "The adapters had gates that allowed effective transfer and connection of knowledge from the pre-trained encoders for each modality.",
            "3": "2: The author conducted experiments with various text and image encoders, adapter units, parameter initialization methods, and efficiency measures, following the proposed approach.",
            "4": "The results demonstrate the effectiveness of the proposed method.",
            "5": "3: The analysis of solid ablation covered various aspects, such as parameter efficiency, the effectiveness of GAUs on different layers, the performance of gates, and so on 1: Using pre-trained image and text encoders for image-text contrastive learning is a common practice, lacking originality.",
            "6": "There are many similar works in the literature.",
            "7": "2: Testing on a limited number of languages or training sets is not sufficient to support the claims"
        }
    },
    "FKNtgr0qQy": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the emergence of abstract state representations in embodied sequence modeling, a topic that is crucial for understanding the internal mechanisms of decision-making models.",
            "1": "- The study uses the BabyAI environment to design a \"blindfolded\" navigation task, which is a novel approach to investigate the internal representations of environmental states in sequence models.",
            "2": "- The findings suggest that key features of state representations can emerge via embodied sequence modeling, which has significant implications for the application of sequence modeling objectives to more complex decision-making domains.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-defined research question and addresses it with a novel experimental setup.",
            "4": "- The use of the BabyAI environment and the \"blindfolded\" navigation task provides a clear and controlled setting to investigate the emergence of state representations.",
            "5": "- The results are robust and show that abstract state representations can emerge in sequence models, even when intermediate states are not explicitly provided.",
            "6": "- The paper includes thorough experiments and analyses, including probing experiments and evaluations on different BabyAI levels, which strengthen the validity of the findings.",
            "7": "- The study contributes to the understanding of how sequence models can learn internal representations of states, which is valuable for the development of more effective decision-making models.",
            "8": "Potential reasons for rejection\n   - **Limited generalizability of results**\n     - The experiments are conducted in the BabyAI environment, which is a synthetic and relatively simple setting.",
            "9": "The findings may not generalize to more complex real-world environments.",
            "10": "- The study does not include experiments on other environments or tasks, which limits the scope of the conclusions.",
            "11": "- **Insufficient comparison with other methods**\n     - The paper focuses on the comparison between Complete-State and Missing-State models but does not compare the proposed approach with other state-of-the-art methods in decision-making or reinforcement learning.",
            "12": "- Including comparisons with other models or approaches could provide a better understanding of the advantages and limitations of the proposed method.",
            "13": "- **Lack of detailed analysis on failure cases**\n     - The paper does not provide a detailed analysis of the failure cases or the limitations of the proposed models.",
            "14": "Understanding the scenarios where the models fail could provide insights for further improvements.",
            "15": "- Including a discussion on the limitations and potential failure modes of the models would strengthen the paper.",
            "16": "Suggestions for improvement\n   - **Expand experiments to more complex environments**\n     - Conduct experiments in more complex and realistic environments to test the generalizability of the findings.",
            "17": "This could include environments with more diverse tasks and more complex state spaces.",
            "18": "- Include additional benchmarks and datasets to provide a broader evaluation of the proposed method.",
            "19": "- **Include comparisons with other methods**\n     - Compare the proposed models with other state-of-the-art methods in decision-making and reinforcement learning to highlight the strengths and weaknesses of the approach.",
            "20": "- Provide a detailed analysis of how the proposed method performs relative to other approaches in terms of accuracy, efficiency, and generalization.",
            "21": "- **Analyze failure cases and limitations**\n     - Provide a detailed analysis of the failure cases and limitations of the proposed models.",
            "22": "This could include identifying specific scenarios where the models struggle and discussing potential reasons for these failures.",
            "23": "- Include a discussion on the limitations of the current approach and suggest directions for future research to address these limitations.",
            "24": "- **Improve clarity and presentation**\n     - Ensure that the figures and tables are clearly labeled and easy to interpret.",
            "25": "This includes providing detailed captions and explanations for all visual elements.",
            "26": "- Improve the overall readability of the paper by refining the writing and ensuring that the key points are clearly communicated.",
            "27": "This could involve reorganizing sections for better flow and coherence."
        },
        "LdFpfK6uRS": {
            "0": "- A novel \"blindfolded\" navigation task that directly aims to test for an agents ability to learn useful intermediate representations of the world (in the minigrid domain).",
            "1": "- A throughout introspection of probing performances in different layers of the Transformer (Figure 4).",
            "2": "- A nice ablation that shows that language instructions impact to probe performances (Table 2).",
            "3": "- The contribution of this work would be stronger when the evaluation had been done for more than a single environment in the minigrid environment.",
            "4": "That sequence models are able to learn strong world representations has been already convincingly shown in other work, most famously in the World Models paper of Hu and Schmidhuber (2018) for the Doom and CarRacing environments.",
            "5": "- I find the results (in Table 1) contradict the conclusion that \"the models can still learn to infer the intermediate states\" (L554).",
            "6": "Actually, the results show that the model seems rather sensitive to the state information that is directly passed as an input or not.",
            "7": "The Missing-State model achieves higher (non-optimal) scores on \"Board\" information than on the \"Neighbor\" information because this information is given initially and carried over to further timesteps.",
            "8": "Similarly, the Complete-State model achieves lower scores at global information than on \"Neighbor\" information because it is highly influenced by the given state at a certain timestep.",
            "9": "The more interesting baseline would have been the CNN representations.",
            "10": "- The paper could be more precise in what it actually targets to measure.",
            "11": "Of course the model learns \"some\" useful internal state representation.",
            "12": "Are these internal representations the ground truth states?",
            "13": "The \"internal representation of a state\" is only measured via the proxy of probes.",
            "14": "Are these internal representations properly grounded?",
            "15": "The reported results are not specifically related to the given instruction (Can the agent identify the targeted object?"
        },
        "uuAshh4tsn": {
            "0": "The paper is well-written and straightforward.",
            "1": "The authors found meaningful differences in terms of grounded representations depending on whether they used instructions or missing/complete state model.",
            "2": "The evaluations are relatively limited and restricted to a single environment.",
            "3": "While the authors justified the probing technique used for the evaluation, it would be interesting to have additional ways to explore the grounding (perhaps some evaluation of transfer learning or visualization of the attention layer)."
        },
        "vHnLaSYlhC": {
            "0": "The paper is timely and sheds light on how a relatively new, and potentially widely used, new RL paradigm works.",
            "1": "The contribution is easy to understand, the paper very readable, and experiments thorough (e.g., the ablation on language instructions), and the results convincing.",
            "2": "Overall, I find that the contribution of this paper is clear, straightforward, and technically sound.",
            "3": "Overall, I found this to be a strong paper with a straightforward contribution.",
            "4": "However, it is missing some references to past work and its underlying motivation can be made more convincing.",
            "5": "The underlying motivation can be clarified.",
            "6": "The section \"A debate about sequence modeling\" can be strengthened.",
            "7": "In particular, it's not obvious that the view in Bender and Koller (2020) applies to embodied sequence modeling for decision making.",
            "8": "Bender and Koller argue that, in the context of __language modeling__, form prediction without external grounding does not lead to meaning.",
            "9": "In the language modeling case, however, there is a crucial separation between what the models are trained on (text corpus; form prediction) and how language is __used__ by humans (in the real world; grounded meaning).",
            "10": "In the case of embodied sequence modeling, there is __no clear separation__ between the (pre-)training data and the use; I've understood that the sequence modeling pre-training and actual deployment of the model occur in the same environment.",
            "11": "Therefore, I would argue that, intuitively, pre-training RL agents using sequence modeling _on the same environment_ does allow them to capture meaning.",
            "12": "In addition, the Bender and Koller (2020) argument has since been refuted; meaning doesn't need grounded referents (Piantadosi & Hill, 2023), and language model representations are able to predict external, visual representations (Ilharco et al., 2021).",
            "13": "I think that by saying, e.g., \"we take Bender and Koller's view for language/pixel modeling, but test this view empirically for embodied sequence modelling\", the motivation can be cleaned up.",
            "14": "Missing some key references \n\n- [Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?",
            "15": "](https://aclanthology.org/2021.emnlp-main.166) (Qiu et al., EMNLP 2021)\n\n- [Probing Contextual Language Models for Common Ground with Visual Representations](https://aclanthology.org/2021.naacl-main.422) (Ilharco et al., NAACL 2021)\n\n- [A Benchmark for Systematic Generalization in Grounded Language Understanding](https://arxiv.org/abs/2003.05161) (Ruis et al., NeurIPS 2020)\n\n- [Representation Learning for Grounded Spatial Reasoning](https://aclanthology.org/Q18-1004) (Janner et al., TACL 2018)"
        }
    },
    "cMMxJxzYkZ": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the important task of empathetic response generation, which is crucial for building harmonious social relationships and developing helpful AI systems.",
            "1": "- The novelty lies in the empirical investigation of large language models (LLMs) like ChatGPT for this task, and the proposal of three improvement methods: semantically similar in-context learning, two-stage interactive generation, and combination with a knowledge base.",
            "2": "Potential reasons for acceptance\n   - The paper provides a comprehensive empirical study on the performance of LLMs in empathetic response generation, filling a gap in the current research.",
            "3": "- The proposed improvement methods are well-motivated and show significant performance gains in both automatic and human evaluations.",
            "4": "- The exploration of using GPT-4 to simulate human evaluators is innovative and could have practical implications for reducing evaluation costs and time.",
            "5": "Potential reasons for rejection\n   - **Lack of detailed analysis on the limitations of the proposed methods:**\n     - The paper does not thoroughly discuss the potential drawbacks or limitations of the proposed improvement methods.",
            "6": "- There is no analysis of the scenarios where the proposed methods might fail or perform suboptimally.",
            "7": "- **Insufficient comparison with other state-of-the-art models:**\n     - The paper compares LLMs with a limited set of baseline models, and it is unclear if the selected baselines are the most relevant or competitive.",
            "8": "- There is a lack of discussion on how the proposed methods compare with other recent advancements in empathetic response generation.",
            "9": "- **Limited exploration of diverse datasets:**\n     - The study is primarily based on the EMPATHETIC DIALOGUES dataset, and there is no exploration of the performance of the proposed methods on other datasets.",
            "10": "- The generalizability of the findings to other domains or languages is not addressed.",
            "11": "Suggestions for improvement\n   - **Provide a detailed analysis of the limitations of the proposed methods:**\n     - Discuss potential scenarios where the proposed methods might not perform well.",
            "12": "- Analyze the trade-offs involved in using each of the improvement methods.",
            "13": "- **Expand the comparison with other state-of-the-art models:**\n     - Include a broader range of baseline models for comparison to provide a more comprehensive evaluation.",
            "14": "- Discuss how the proposed methods compare with other recent advancements in empathetic response generation.",
            "15": "- **Explore the performance on diverse datasets:**\n     - Evaluate the proposed methods on additional datasets to demonstrate their generalizability.",
            "16": "- Consider including datasets from different domains or languages to assess the robustness of the methods.",
            "17": "- **Enhance the discussion on the practical implications of using GPT-4 as a human evaluator:**\n     - Provide more detailed analysis on the cost and time savings achieved by using GPT-4 for evaluation.",
            "18": "- Discuss the potential limitations and ethical considerations of replacing human evaluators with LLMs."
        },
        "qiFEdpfLws": {
            "0": "(1) The experiments are thorough and demonstrate the effectiveness of the authors’ proposed ideas.",
            "1": "(2) The clarity of the paper is good.",
            "2": "The contribution is incremental.",
            "3": "The proposed ideas to augment ChatGPT have been extensively studied in prior works for other tasks.",
            "4": "For example, using semantically similar exemplars for in-context learning [2,3], CoT prompting [4,5], and using external commonsense knowledge [1].",
            "5": "The authors should highlight the motivation of applying such ideas in the context of empathetic dialogue generation, the unique challenges of empathetic dialogue generation, and why applying such ideas can help address the challenges.",
            "6": "[1] Sabour, Sahand, Chujie Zheng, and Minlie Huang.",
            "7": "\"Cem: Commonsense-aware empathetic response generation.\"",
            "8": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "9": "Vol.",
            "10": "36.",
            "11": "No.",
            "12": "10.",
            "13": "2022.",
            "14": "[2] Rubin, Ohad, Jonathan Herzig, and Jonathan Berant.",
            "15": "\"Learning To Retrieve Prompts for In-Context Learning.\"",
            "16": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.",
            "17": "2022.",
            "18": "[3] Liu, Jiachang, et al.",
            "19": "\"What Makes Good In-Context Examples for GPT-3?.\"",
            "20": "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures.",
            "21": "2022.",
            "22": "[4] Kojima, Takeshi, et al.",
            "23": "\"Large language models are zero-shot reasoners.\"",
            "24": "Advances in neural information processing systems 35 (2022): 22199-22213.",
            "25": "[5] Wei, Jason, et al.",
            "26": "\"Chain-of-thought prompting elicits reasoning in large language models.\"",
            "27": "Advances in Neural Information Processing Systems 35 (2022): 24824-24837."
        },
        "olCDJOvggz": {
            "0": "Clear discussion of problem statement and proposal of three fundamental techniques for improved empathetic response generation.",
            "1": "Improved results over several baseline methods in both automatic and human eval metrics.",
            "2": "The examples shown in paper indicate that the proposed method is powerful and could be explored further for the problem of empathetic response generation.",
            "3": "A weakness is that the evaluation could have been more through.",
            "4": "Although expected, it is unclear which components bring the improvement in performance.",
            "5": "Are the in-context example responses very similar to the target test references?",
            "6": "What happens when you try to use all three of the proposed techniques together (ChatGPT + SS ICL + Two-stage + Knowledge)?",
            "7": "Some important details about the proposed components could have been added to the paper.",
            "8": "For example, how does the generated knowledge snippets from COMET in section 3.3.3 look like?"
        },
        "O2uBOaZpt7": {
            "0": "- Show improvement over existing sota on this benchmark\n- Introduce three methods of improving chatgpt for this task which seem to be mostly successful in improving performance\n- Comprehensive automatic and human evaluations.",
            "1": "Experiments and metrics seem reasonable\n- Generated output in Table 5 looks very impressive - Some of the results tables claim that statistical significance testing has taken place, but it’s unclear which things are being compared in the statistical tests.",
            "2": "This seems important to clarify since some of the differences are a bit smaller.",
            "3": "- This paper is empirically significant in establishing a new SOTA on this benchmark, but the technical novelty of the approach is more limited.",
            "4": "It’s not proposing a new modelling approach, and the ideas for improving over chatgpt (e.g.",
            "5": "two stage reasoning with emotion prediction, using a knowledge base, etc) are similar to prior approaches that have been successful in improving smaller models on this dataset."
        },
        "7mlXKA9gb7": {
            "0": "Overall, the paper is well-organized, well-written, and the evaluation is quite comprehensive.",
            "1": "Empathetic chat is a task that has garnered growing interest in recent years, and, as such, this work is valuable in serving as an evaluation benchmark on empathic quality of LLM-generated text.",
            "2": "In addition, the three proposed improvements to prompt-based empathetic dialogue generation are well-validated, and both the human and automatic evaluation are well-designed and straightforward.",
            "3": "The paper provides clear qualitative examples of the effects the author's proposed improvements have on empathetic dialogue.",
            "4": "The main limitation of this work, as the authors mention, is the focus on evaluation with only the EmpatheticDialogues dataset.",
            "5": "Conducting similar evaluations across more diverse datasets would be needed for more generalizable results.",
            "6": "The human evaluation has few annotators, which can bias the evaluation towards a narrow definition of empathy.",
            "7": "Within the human evaluation, the provided instructions on ranking empathy are not clear, and it appears that sub-dimensions of empathy are not included in the evaluation, which could improve interpretability of the results.",
            "8": "Discussion of the analysis on using LLMs to simulate human evaluators is cursory, and would warrant more analysis beyond the scope of the contributions of this work.",
            "9": "While this evaluation is timely, correlation alone cannot demonstrate that LLMs effectively simulate empathy reasoning, and the results of this analysis do not clearly bolster the contributions of this work."
        },
        "e2P95gh0pf": {
            "0": "* The paper offers a detailed examination of LLMs' performance in the domain of empathetic dialogue, providing valuable insights into their capabilities.",
            "1": "* The methods and templates introduced by the authors can serve as valuable tools for researchers and practitioners in the NLP community.",
            "2": "* The motivation behind the study is well-articulated, emphasizing the real-world relevance of the task.",
            "3": "Additionally, the related work section effectively lays down foundational knowledge *  Some of the methodologies in the paper, like the \"tricky\" choice of exemplars for ICL that matches most closely with the test set, raise concerns.",
            "4": "This potentially makes the improvements and all subsequent results unreliable.",
            "5": "* The paper lacks crucial details about the human evaluation process.",
            "6": "Information regarding the hiring process, demographic details of the evaluators, measures taken to ensure unbiased evaluations, and procedures to validate the accuracy of annotations is missing.",
            "7": "* The paper does not specify if the proposed prompt template was compared against other prompt types or variations."
        }
    },
    "Wom397PB55": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems.",
            "1": "- The dataset is curated by domain experts and contains 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance.",
            "2": "- The paper evaluates a wide spectrum of 16 large language and code models with different prompting strategies, providing a comprehensive benchmark for LLMs' capabilities in solving complex science problems.",
            "3": "Potential reasons for acceptance\n   - The introduction of a novel dataset that fills a gap in the current evaluation of AI models' capabilities to apply theorems in problem-solving.",
            "4": "- Comprehensive evaluation of multiple state-of-the-art models, including GPT-4, ChatGPT, and various open-source models, providing valuable insights into their performance.",
            "5": "- Detailed analysis of different prompting strategies, such as Chain-of-Thoughts and Program-of-Thoughts, and their impact on model performance.",
            "6": "- The paper's findings highlight the significant performance gap between closed-source models like GPT-4 and open-source models, suggesting areas for future research and improvement.",
            "7": "Potential reasons for rejection\n   - **Limited generalizability of the dataset:**\n     - The dataset focuses on university-level theorems, which may not be representative of all types of theorem-driven problems encountered in real-world applications.",
            "8": "- The dataset's questions are curated and adjusted by domain experts, which may introduce biases and limit the diversity of problem types.",
            "9": "- **Evaluation methodology:**\n     - The paper relies heavily on the accuracy metric, which may not fully capture the models' reasoning capabilities and understanding of theorems.",
            "10": "- The evaluation of multimodal questions is limited, and the performance of visual LLMs on these questions is not thoroughly explored.",
            "11": "- **Theorem augmentation and multimodal input:**\n     - The paper's exploration of theorem augmentation and multimodal input is limited, and the proposed methods do not yield significant improvements.",
            "12": "- The paper does not provide a detailed analysis of why these methods fail to improve performance, leaving room for further investigation.",
            "13": "Suggestions for improvement\n   - **Expand the dataset:**\n     - Include a wider range of theorem-driven problems from different domains and difficulty levels to improve the generalizability of the dataset.",
            "14": "- Incorporate more diverse question types, such as those requiring symbolic, matrix, or figure-based answers, to better evaluate models' capabilities.",
            "15": "- **Enhance evaluation methodology:**\n     - Use additional metrics, such as reasoning accuracy and error analysis, to provide a more comprehensive evaluation of models' performance.",
            "16": "- Conduct a more thorough evaluation of multimodal questions and explore alternative methods for integrating visual information with language models.",
            "17": "- **Improve theorem augmentation and multimodal input methods:**\n     - Investigate more sophisticated strategies for theorem augmentation, such as integrating theorem descriptions into the models' reasoning process.",
            "18": "- Develop specialized visual encoding models that can better represent diagrams and other visual inputs in science questions.",
            "19": "- **Provide more detailed analysis:**\n     - Include a more in-depth analysis of the reasons behind the performance gap between closed-source and open-source models.",
            "20": "- Explore the impact of different pre-training and fine-tuning strategies on models' performance in solving theorem-driven problems."
        },
        "Hx9Qngoyte": {
            "0": "1.The proposed dataset, TheoremQA, is the first of its kind to focus on theorem-driven question-answering.",
            "1": "It covers a wide range of theorems from various fields such as Math, Physics, Electrical Engineering & Computer Science, and Finance.",
            "2": "This dataset can serve as a valuable resource for researchers in the field.",
            "3": "The authors have conducted a thorough evaluation of a wide spectrum of 16 LLMs on TheoremQA.",
            "4": "Their comprehensive analysis provides valuable insights into the performance of these models and their ability to integrate theorems and understand multimodal inputs.",
            "5": "The methodology adopted by the authors is sound and well-justified.",
            "6": "They have used innovative prompting strategies, including Chain-of-Thoughts (CoT) and Program-of-Thoughts (PoT), to guide the LLMs.",
            "7": "The paper does not provide detailed examples of the prompts used in the experiments.",
            "8": "Including such examples, particularly in the appendix, would have been beneficial for readers to better evaluate the appropriateness and effectiveness of the prompting strategies employed by the authors.",
            "9": "Without these examples, it is challenging to fully assess the validity of the authors' claims."
        },
        "akwS4mnXN0": {
            "0": "Motivation:\n\n- Addresses an important open problem - evaluating scientific reasoning capabilities of AI systems, which requires applying knowledge.",
            "1": "- Proposes a new benchmark dataset to advance progress in this direction.",
            "2": "Experiments:\n\n- Thorough comparative evaluation of 16 state-of-the-art models with different prompting strategies.",
            "3": "- Rigorous analysis of results reveals capabilities and limitations of current models.",
            "4": "Writing:\n\n- Clearly presented.",
            "5": "Does a good job explaining the dataset, experiments, results and analyses.",
            "6": "- Provides useful insights that can guide future research to overcome limitations.",
            "7": "- Shares data to enable further analysis.",
            "8": "This is a resource paper, and it excels in all aspects.",
            "9": "I believe it deserves acceptance."
        },
        "HwbpzpDy1A": {
            "0": "(1) The paper contributes a new dataset for the QA research community, which consists of university-level questions backed with corresponding Theorems\n(2) It performs rigorous experiments with current SoTA LLMs with this dataset to check the performance measures of those LLMs\n(3) Also, it gives insights into why LLMs fail in some particular cases to perform QA and shows a detailed error analysis and its limitations.",
            "1": "Although this paper proposed quality dataset questions along with the theorems, the distribution of the questions is somewhat imbalanced.",
            "2": "Out of 800 questions, more than half of the questions are from Mathematics only.",
            "3": "So, distribution is not even."
        }
    },
    "tSfZo6nSN1": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of automating radiology report generation, which can significantly reduce radiologists' workloads.",
            "1": "- It introduces a novel approach, RECAP, which incorporates dynamic disease progression reasoning to generate precise and accurate radiology reports.",
            "2": "- The method captures both spatial and temporal information, which is crucial for evaluating a patient's current condition and generating accurate reports.",
            "3": "Potential reasons for acceptance\n   - The proposed method demonstrates significant improvements in generating precise and accurate radiology reports compared to state-of-the-art baselines.",
            "4": "- The incorporation of both spatial and temporal information is a novel approach that addresses the limitations of previous methods.",
            "5": "- Extensive experiments on two publicly available datasets show the effectiveness of the proposed model.",
            "6": "- The paper provides a detailed methodology, including the construction of a disease progression graph and a dynamic progression reasoning mechanism, which can be valuable for future research in this area.",
            "7": "Potential reasons for rejection\n   - **Limited generalizability:**\n     - The proposed framework requires predefined observations and progressions for training, which may not be available for other types of radiographs.",
            "8": "- The reliance on historical records, which are not always available, may limit the applicability of the method in real-world scenarios.",
            "9": "- **Error propagation:**\n     - The outputs of Stage 1 are prerequisites for Stage 2, which may lead to error propagation and affect the overall performance of the model.",
            "10": "- The paper does not provide a detailed analysis of how error propagation impacts the final report generation.",
            "11": "- **Evaluation metrics:**\n     - The paper primarily uses BLEU, METEOR, and ROUGE scores for evaluation, which may not fully capture the clinical accuracy and relevance of the generated reports.",
            "12": "- The reliance on these metrics may not provide a comprehensive assessment of the model's performance in a clinical setting.",
            "13": "Suggestions for improvement\n   - **Enhance generalizability:**\n     - Explore methods to reduce the dependency on predefined observations and progressions, making the framework more adaptable to different types of radiographs.",
            "14": "- Investigate alternative approaches to handle cases where historical records are not available, ensuring the model can still generate accurate reports.",
            "15": "- **Address error propagation:**\n     - Provide a detailed analysis of the impact of error propagation from Stage 1 to Stage 2 and propose potential solutions to mitigate this issue.",
            "16": "- Consider incorporating mechanisms to correct or compensate for errors introduced in Stage 1 during the report generation process.",
            "17": "- **Improve evaluation metrics:**\n     - Include additional evaluation metrics that better capture the clinical accuracy and relevance of the generated reports, such as expert evaluations or clinical efficacy scores.",
            "18": "- Conduct a more comprehensive evaluation of the model's performance in a clinical setting, including case studies and qualitative analyses.",
            "19": "- **Expand experimental analysis:**\n     - Provide more detailed experimental results, including the performance of the model on different types of observations and progressions.",
            "20": "- Include a broader range of baselines for comparison to demonstrate the robustness and effectiveness of the proposed method."
        },
        "1ac474Yj6F": {
            "0": "Medical report generation is a critical clinical task which can relieve radiologists from the heavy workload.",
            "1": "Using historical reports and mining dissease progresses are useful for report generation.",
            "2": "The proposed approach fails to outperform existing works.",
            "3": "For example, in Table 1, the B-4 of proposed approach is lower than the basic baseline ViT-transformer on MIMIC-ABN.",
            "4": "Why the ViT-transformer is not evaluated on MIMIC-CXR data set.",
            "5": "What if the patients are the first time visitors without historical reports.",
            "6": "The authors need to evaluate the proposed approach on new patients and old patients respectively.",
            "7": "The experiment setting is not fair.",
            "8": "For the proposed approach, the historical reports of patients are used to generate reports for current input.",
            "9": "While these data are unseen to baseline works.",
            "10": "These historical reports should be added into the training data set of baselines.",
            "11": "One existing work which also includes historical reports in modeling should be referenced and discussed.",
            "12": "DeltaNet: Conditional medical report generation for COVID-19 diagnosis, Coling 2022.",
            "13": "Since the proposed approach targets to mine the progress of diseases to generate better results.",
            "14": "Such intermediate results (the disease progress) should be evaluated in experiments.",
            "15": "The IU data set should be included in experiments."
        },
        "KKRUhSyin1": {
            "0": "- The Spatial/Temporal component of radiology reports has long been overlooked.",
            "1": "This work is a great step toward this direction.",
            "2": "- Extensive comparison to prior work\n- Use of RadGraph to compute a semantic graph and progression\n- Substantial improvements on CE Metrics.",
            "3": "- Results are reported in a wide range of metrics\n\n - The model is quiet complex and difficult to understand\n- It seems some information from the ground-truth reports are used as input to the model (at test-time).",
            "4": "I might be wrong about this one (see questions for the authors)"
        },
        "XDIqz0LEfQ": {
            "0": "Good motivation for using longitudinal data with disease progression information to generate radiology reports.",
            "1": "This can potentially be used to generate other types of reports or other clinical NLP/NLG tasks.",
            "2": "The comparison experiments and ablation studies show promising results of the proposed method, especially the benefit of considering information from the last visit of a patient.",
            "3": "The examples are also insightful.",
            "4": "The paper is overall well-written.",
            "5": "The experiments are restricted to the MIMIC dataset only.",
            "6": "It is unclear how this method may perform on a different dataset or different types of notes.",
            "7": "The model only considers the most recent last visit of the patient.",
            "8": "What if more historical visits are considered?"
        }
    },
    "wcgfB88Slx": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the optimization of explanation-infused prompts for large language models (LLMs) in a black-box fashion, which is a significant contribution to the field of natural language processing (NLP).",
            "1": "- The proposed method leverages unlabeled data and proxy metrics to improve the performance of LLMs on textual reasoning tasks, which is a novel approach.",
            "2": "Potential reasons for acceptance\n   - The paper presents a well-defined problem and a clear methodology for optimizing explanation-infused prompts.",
            "3": "- The experimental results demonstrate significant improvements in performance across multiple datasets, showcasing the effectiveness of the proposed approach.",
            "4": "- The use of proxy metrics to efficiently search for better-performing explanations is a practical and innovative solution to the problem of high computational costs.",
            "5": "Potential reasons for rejection\n   - **Limited generalizability of the approach:**\n     - The paper primarily focuses on a specific set of reasoning tasks and datasets, which may limit the generalizability of the proposed method to other types of tasks or datasets.",
            "6": "- The approach relies heavily on the capabilities of the LLMs used, which may not be applicable to less capable models or different languages.",
            "7": "- **High computational overhead:**\n     - The optimization process involves significant computational overhead, including pseudo-labeling the development set and scoring combinations using silver accuracy.",
            "8": "- The approach may not be feasible for real-time applications or scenarios with limited computational resources.",
            "9": "- **Dependency on proxy metrics:**\n     - The effectiveness of the proposed method is highly dependent on the accuracy of the proxy metrics used to rank candidate explanations.",
            "10": "- The paper does not provide a thorough analysis of the potential limitations or failure cases of the proxy metrics, which could impact the reliability of the approach.",
            "11": "Suggestions for improvement\n   - **Expand the scope of experiments:**\n     - Include additional datasets and tasks to demonstrate the generalizability of the proposed method.",
            "12": "- Test the approach on LLMs with varying capabilities and in different languages to assess its robustness.",
            "13": "- **Reduce computational overhead:**\n     - Explore ways to reduce the computational cost of the optimization process, such as more efficient sampling techniques or parallel processing.",
            "14": "- Investigate the potential for incremental or online optimization methods that can adapt to new data without requiring a full re-optimization.",
            "15": "- **Enhance the analysis of proxy metrics:**\n     - Provide a more detailed analysis of the proxy metrics used, including their limitations and potential failure cases.",
            "16": "- Consider developing additional or alternative proxy metrics that could improve the reliability and accuracy of the optimization process.",
            "17": "- **Improve clarity and presentation:**\n     - Ensure that the methodology and experimental setup are clearly explained, with sufficient detail for reproducibility.",
            "18": "- Include more visual aids, such as diagrams or flowcharts, to illustrate the optimization process and the role of proxy metrics."
        },
        "CAoakTfjkG": {
            "0": "Authors propose a new framework to find the best in-context explanations for CoT, and use two proxy metrics to approximate downstream performance and enable efficient search.",
            "1": "The metrics generally correlate positively with final accuracy.",
            "2": "The experiments covered different kinds of reasoning tasks, and the authors provided sufficient details of the statistics.",
            "3": "Moreover, the overall presentation of this paper is good.",
            "4": "The idea of selecting demonstrations from unlabeled data can be potentially applied to many scenarios.",
            "5": "Although the authors use an additional constraint, searching over possible combinations of candidate explanations to find the one with the best performance on dev set might still be very expensive, e.g., paper [1] uses a very similar strategy to (use brute force to) find best in-context demonstrations.",
            "6": "The effectiveness of selected explanations is largely dependent on the specific dataset, for instance, the ones for GSM8K dataset can hardly be applied to other benchmarks like Multiarith, SingleEQ/OP, AddSub, GSM-Hard, although they are very similar in topics and have identical input/output formats.",
            "7": "[1] Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives https://arxiv.org/abs/2305.08088"
        },
        "mJOMkuQZzP": {
            "0": "- The experiment design is solid and there are strong results that demonstrate the effectiveness of proposed metrics and framework.",
            "1": "- It is interesting to see that both proposed metrics $\\mathcal{S}_\\text{OSAcc}$ and  $\\mathcal{S}_\\text{OSLL}$ select better explanations than random chance.",
            "2": "Figure 3 is informative for understanding why these metrics help.",
            "3": "- The authors provide comprehensive analyses of the framework.",
            "4": "The analyses demonstrate that the proposed framework is compatible with a common prompting technique (self-consistency), and works well under a reduced computation budget.",
            "5": "- The proposed method makes use of a silver set of pseudo-labeled examples, and it seems fair to further consider a best-of-$k$ baseline, which samples $k$ candidate explanation sets and then chooses the best set based on its accuracy on the pseudo-labeled examples.",
            "6": "This baseline is known to be effective in https://arxiv.org/abs/2112.09332.",
            "7": "- To make searching over candidate explanations tractable, the framework uses a greedy proxy metrics that assume combining the best individual explanations lead to the best set of explanations.",
            "8": "This assumption limits the performance of the method (i.e., the optimal set of explanations could be unattainable).",
            "9": "- There are a few moving parts in the proposed framework (e.g., a pseudo-labeled silver set, ensemble of proxy metrics and candidate explanation sets).",
            "10": "This complexity could make it difficult to apply this framework in a real-world setting."
        },
        "HW6veT1NI1": {
            "0": "- interesting and timely problem statement.",
            "1": "- reasonable solution (I like the emphasis and discussion on computational budget)\n- well written and interesting analysis.",
            "2": "- by products like proxy metrics, pseudo labelling of unlabeled data can be useful for other applications.",
            "3": "The following are the questions I have and these are not necessarily 'reasons to reject'.",
            "4": "- I was looking for a comparison with the zero-shot chain of thought baseline which authors refer as ZOT (Kojima et al., 2022).",
            "5": "The example selection method has a cost.",
            "6": "Also, few shot experiments involve extra token usage cost than zero shot.",
            "7": "- Some of the numbers while comparing proposed method vs baselines seem to be pretty close.",
            "8": "Wondering, if authors did any statistical significance test?",
            "9": "- A parallel field to explanation selection is prompt/instruction engineering, where we often change the zeroshot instruction.",
            "10": "Another alternative is prompt-tuning via gradient descent.",
            "11": "Wondering if authors have any thoughts regarding the tradeoff.",
            "12": "- Few shot examples has various types of example biases such as majority bias, recency bias etc.",
            "13": "(http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf, https://aclanthology.org/2023.eacl-main.130/, https://aclanthology.org/2022.acl-long.556.pdf).",
            "14": "Wondering if authors have any thought on how the robustness look like with the application of their method?",
            "15": "I am looking forward to hear answers to these questions from the authors."
        }
    },
    "edwSiVzFpU": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper presents the first comprehensive survey on end-to-end task-oriented dialogue (EToD) systems, which is a significant contribution to the field.",
            "1": "- It introduces a new taxonomy for EToD, categorizing approaches into modularly EToD and fully EToD, providing a clear and structured overview of the research landscape.",
            "2": "- The paper discusses new frontiers and challenges in EToD, offering valuable insights and directions for future research.",
            "3": "Potential reasons for acceptance\n   - The paper fills a gap in the literature by providing a thorough review of EToD systems, which has not been done before.",
            "4": "- The new taxonomy and categorization of EToD approaches help in understanding the progress and trends in the field.",
            "5": "- The discussion on future directions and challenges is comprehensive and can guide future research efforts.",
            "6": "- The paper includes a public website with abundant resources, which can be a valuable tool for researchers in the EToD community.",
            "7": "Potential reasons for rejection\n   - **Lack of detailed comparative analysis:**\n     - The paper primarily focuses on high-level comparisons of different approaches, such as overall system performance, rather than a fine-grained analysis.",
            "8": "- It does not provide in-depth comparative analyses to understand the advantages and disadvantages of various models, such as comparing KB retrieval results and performance across different domains.",
            "9": "- **Limited coverage of recent advancements:**\n     - The paper may not cover the very latest advancements in the field, given the rapid pace of research in EToD.",
            "10": "- Some recent works and emerging trends might be missing, which could limit the comprehensiveness of the survey.",
            "11": "- **Insufficient discussion on practical applications:**\n     - The paper focuses more on theoretical aspects and research challenges, with limited discussion on practical applications and real-world deployment of EToD systems.",
            "12": "- It could benefit from more examples and case studies of successful EToD implementations in industry.",
            "13": "Suggestions for improvement\n   - **Enhance comparative analysis:**\n     - Include more detailed comparative analyses of different EToD approaches, focusing on specific aspects such as KB retrieval accuracy, response generation quality, and domain adaptability.",
            "14": "- Provide case studies or examples to illustrate the strengths and weaknesses of various models in practical scenarios.",
            "15": "- **Update with recent advancements:**\n     - Ensure the survey includes the latest research developments and emerging trends in EToD, possibly by conducting a follow-up review or an addendum.",
            "16": "- Highlight any groundbreaking works or novel approaches that have been published since the initial survey.",
            "17": "- **Expand discussion on practical applications:**\n     - Include more discussion on the practical applications and real-world deployment of EToD systems, providing examples from industry and case studies of successful implementations.",
            "18": "- Discuss the challenges and solutions related to deploying EToD systems in real-world settings, such as scalability, user interaction, and integration with existing systems.",
            "19": "- **Improve resource accessibility:**\n     - Ensure the public website with EToD resources is regularly updated and maintained, providing the latest papers, datasets, and tools for researchers.",
            "20": "- Consider adding tutorials, guidelines, and best practices for using the resources, making it easier for new researchers to get started in the field."
        },
        "c29IgPlAnU": {
            "0": "The paper provides a thorough overview of the progress made in the field of task-oriented dialogue and examines emerging frontier areas while summarizing the associated challenges.",
            "1": "The writing is accessible and straightforward, making the content easily understandable to readers.",
            "2": "Lack of Discussion on Integration with Large Pre-trained Models: In the era of large pre-trained models, the paper fails to address the integration of task-oriented dialogue with these models, which could have significant implications for EToD research.",
            "3": "Omissions of Recent Progress: Some noteworthy recent developments in EToD, such as QToD (Q-TOD: A Query-driven Task-oriented Dialogue System) and Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog, are not included in the paper."
        },
        "LTDSZcggyy": {
            "0": "Please refer to the “Paper Topic And Main Contributions” part.",
            "1": "Please refer to the “Paper Topic And Main Contributions” part."
        },
        "OHPCFD1mc7": {
            "0": "This work is important as it is the first survey of the field.",
            "1": "The paper has clear taxonomy, good coverage, leaderboards and good discussion on future directions.",
            "2": "All the relevant data is made publicly available.",
            "3": "Overall, this work can provide visibility, transparency and traction to the field.",
            "4": "The paper lacks detailed discussion on the evaluation protocols for end-to-end TOD.",
            "5": "This includes automatic metrics, human evaluation and recent LLM based metrics like BERTScore and its variant."
        }
    },
    "9F6h0oIYsP": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of rumor detection on social media, which has significant societal implications.",
            "1": "- The proposed framework, CLKD-IMRD, introduces a novel combination of Contrastive Learning and Knowledge Distillation to handle incomplete modalities in multimodal posts.",
            "2": "- The approach aims to capture semantic consistency between text and image pairs and enhance model generalization to incomplete modalities, which is a novel contribution to the field.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-motivated and timely problem, given the increasing prevalence of misinformation on social media.",
            "4": "- The proposed method demonstrates significant improvements over state-of-the-art methods on multiple benchmark datasets, showcasing its effectiveness.",
            "5": "- The integration of contrastive learning and knowledge distillation is innovative and addresses the challenge of incomplete modalities, which is often overlooked in existing models.",
            "6": "- The experimental results are comprehensive, including ablation studies and analysis of the impact of different components, providing strong evidence for the effectiveness of the proposed approach.",
            "7": "Potential reasons for rejection\n   - **Limited generalizability:**\n     - The experiments are conducted on specific datasets (Chinese Weibo and English Pheme/Twitter), which may not fully represent other rumor detection scenarios or platforms.",
            "8": "- The model's performance on other languages and datasets is not explored, raising questions about its broader applicability.",
            "9": "- **Absence of real-time evaluation:**\n     - The evaluation primarily focuses on offline performance measures, without considering real-time or dynamic evaluation scenarios.",
            "10": "- The model's performance in real-time rumor detection settings is not investigated, which is crucial for practical applications.",
            "11": "- **Complexity and deployment:**\n     - The proposed framework involves multiple components, including contrastive learning, knowledge distillation, and co-attention mechanisms, which may increase the complexity of implementation and deployment.",
            "12": "- The paper does not provide a detailed discussion on the computational requirements and scalability of the proposed approach.",
            "13": "Suggestions for improvement\n   - **Expand the evaluation to more diverse datasets:**\n     - Conduct experiments on additional datasets from different languages and social media platforms to demonstrate the generalizability of the proposed approach.",
            "14": "- Include datasets with varying characteristics, such as different types of rumors and varying levels of multimodal content.",
            "15": "- **Investigate real-time performance:**\n     - Evaluate the model's performance in real-time or dynamic settings to assess its practical applicability in real-world scenarios.",
            "16": "- Consider the latency and computational efficiency of the model in real-time rumor detection tasks.",
            "17": "- **Simplify the model and discuss deployment:**\n     - Explore ways to simplify the model without compromising its performance, making it more feasible for deployment in practical applications.",
            "18": "- Provide a detailed discussion on the computational requirements, scalability, and potential deployment strategies for the proposed approach.",
            "19": "- **Enhance the interpretability of the model:**\n     - Include more detailed visualizations and explanations of how the model captures semantic consistency between text and image pairs.",
            "20": "- Provide case studies or examples to illustrate the model's decision-making process and its effectiveness in debunking rumors."
        },
        "45K55GZTVE": {
            "0": "Proposes a multi-modal model as a teacher model to guide the single-modal model.",
            "1": "The experimental results are very comprehensive.",
            "2": "It does not explain why supervised contrastive learning is used.",
            "3": "The most recent multi-modal rumor detection model compared is from 2020.",
            "4": "The effectiveness of the student model has not been compared with other models, making it difficult to prove the effectiveness of distillation.",
            "5": "Main experiment focused on multi-modal scenarios; more experiments should be conducted starting from the perspective of missing modalities, beacuse the main motivation is incomplete modalities."
        },
        "ycghNWLBzC": {
            "0": "- Good motivation and well-conducted experiments.",
            "1": "They compare their results to a range of baseline models.",
            "2": "- They tackle the problem of missing modality.",
            "3": "This is not only valuable for rumor detection but for multimodal classification of social media posts in general.",
            "4": "- The paper present different types of analysis to shed light on the benefits of their approach.",
            "5": "For instance, they perform an ablation study, a study using different versions of the student model, and a study on the effect of using different number of comments.",
            "6": "- Interesting findings, for instance, they show that using more comments does not necessarily improve performance.",
            "7": "This is interesting given the idea of \"more data is always better\".",
            "8": "This is also opposite to findings in multimodal social media classification: Xu, Chunpu, and Jing Li.",
            "9": "\"Borrowing Human Senses: Comment-Aware Self-Training for Social Media Multimodal Classification.\"",
            "10": "arXiv preprint arXiv:2303.15016 (2023).",
            "11": "- Experiments are only done on two datasets for binary classification.",
            "12": "Since you are presenting a new method it is worth applying the models to a range of benchmark datasets.",
            "13": "- Some choices are not justified/not clear.",
            "14": "See questions for the authors."
        },
        "odJR0vJ18R": {
            "0": "1) The work presents a rumor detection framework that relies on the supervised contrastive learning and teacher network.",
            "1": "The framework can capture semantic interactions among source texts, images, and user comments.",
            "2": "2) This paper presents a knowledge distillation driven rumor detection model that can probably handle incomplete modalities (i.e., lack of images or texts).",
            "3": "1) Multimodal rumor detection is not new, and this work focuses on incomplete modality case, where it may be lack of image or text information in given posts.",
            "4": "Thus it may lead to incremental contribution.",
            "5": "2) The proposed method was tested on only 2 small datasets, and it's not clear if it can be generalizable to different runor detection tasks."
        }
    },
    "U6SEUS76IE": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in federated learning (FL) for natural language processing (NLP) by proposing a novel Federated Interactive Distillation (FedID) framework.",
            "1": "- The approach aims to mitigate the limitations of existing FL and federated distillation (FD) methods, such as communication overhead, handling heterogeneity, and confirmation bias.",
            "2": "- The introduction of a benchmarking framework based on the GLUE benchmark for evaluating FD methods in NLP is a significant contribution to the research community.",
            "3": "Potential reasons for acceptance\n   - The proposed FedID framework demonstrates superior performance in both homogeneous and heterogeneous federated scenarios, as evidenced by the experimental results.",
            "4": "- The paper provides a comprehensive evaluation of FedID against various baselines, including FL and FD methods, showing its effectiveness in mitigating confirmation bias.",
            "5": "- The introduction of a unified benchmarking framework for FD in NLP tasks is a valuable contribution that can facilitate future research and comparisons in this area.",
            "6": "- The paper is well-structured, with clear explanations of the methodology, experimental setup, and results, making it accessible to readers.",
            "7": "Potential reasons for rejection\n   - **Limited novelty in methodology**:\n     - The core idea of using labeled data to rectify local models during knowledge transfer is not entirely new and has been explored in other contexts.",
            "8": "- The paper could benefit from a more detailed comparison with existing methods that use similar approaches to highlight the unique contributions of FedID.",
            "9": "- **Assumption of labeled data availability**:\n     - The assumption that a small amount of labeled data is retained by the server may not always be practical in real-world scenarios.",
            "10": "- The paper should discuss potential limitations and alternative strategies for scenarios where labeled data is not available.",
            "11": "- **Communication cost concerns**:\n     - While the paper claims that FedID reduces communication costs, the frequent communication between the server and clients for interactive distillation may still be a concern.",
            "12": "- A more detailed analysis of the communication costs and potential optimizations could strengthen the paper's contributions.",
            "13": "- **Evaluation on limited datasets**:\n     - The evaluation is primarily based on the GLUE benchmark, which, while comprehensive, may not cover all possible scenarios and data distributions in real-world applications.",
            "14": "- Including additional datasets or real-world case studies could provide a more robust validation of the proposed method.",
            "15": "Suggestions for improvement\n   - **Expand the discussion on related work**:\n     - Provide a more detailed comparison with existing methods that use labeled data for rectifying local models to highlight the unique contributions of FedID.",
            "16": "- **Address the assumption of labeled data availability**:\n     - Discuss potential limitations and alternative strategies for scenarios where labeled data is not available, such as semi-supervised or unsupervised approaches.",
            "17": "- **Optimize communication costs**:\n     - Provide a more detailed analysis of the communication costs associated with FedID and explore potential optimizations to reduce the frequency and volume of communication.",
            "18": "- **Broaden the evaluation**:\n     - Include additional datasets or real-world case studies to provide a more comprehensive validation of the proposed method.",
            "19": "- Consider evaluating the method on tasks beyond the GLUE benchmark to demonstrate its generalizability and robustness.",
            "20": "- **Clarify the novelty and contributions**:\n     - Emphasize the unique aspects of FedID compared to existing methods and provide a clearer articulation of its contributions to the field of federated learning and NLP."
        },
        "MExaeE6LHA": {
            "0": "Useful privacy-preserving training method with good performance\n2.",
            "1": "Reasonable and effective algorithm design\n3.",
            "2": "Comprehensive experiment to show the effectiveness of the method\n 1.",
            "3": "Lack case-level comparison to analyze/justify in detail why the proposed method alleviates confirmation bias\n2.",
            "4": "Lack of comparison with alternative designs (e.g.",
            "5": "for server-to-client interaction, can we also use the proxy dataset instead of the gradient?)",
            "6": "Lack real-world use case analysis for the proposed training scenario (in the limitation section the author explains a constructed scenario which is reasonable, but still it would be informative to find concrete use case)"
        },
        "4r60k5CkoC": {
            "0": "The FedID approach is generic and can potentially be applied to train non-NLP models as well in a Federated Learning setting.",
            "1": "In this paper, the authors specifically apply it to PLMs and show their efficiency and performance benefits for NLP tasks - making it relevant for EMNLP.",
            "2": "The experimental results are comprehensive providing interesting insights around public dataset distribution between clients and communication aspects.",
            "3": "The heterogenous setting is of course more interesting from a 'confirmation bias' perspective, where the authors show FedID's robustness when only a small amount of training data is available that are insufficient to train the local models.",
            "4": "FedID is more of an incremental contribution over existing Federated Distillation (FD) approaches, with a different distribution mechanism.",
            "5": "So the strength of paper is primarily on the applied part.",
            "6": "For Federated Learning contributions, it is always good to assess how resilient it is to malicious clients/server.",
            "7": "So I would encourage the authors to add this discussion."
        },
        "m8V8O9eD9m": {
            "0": "It is a good idea to solve the homogeneous and heterogeneous in federated distillation through some labeled data of the server\n2.",
            "1": "The author has done sufficient experiments to verify the effectiveness of the method 1.",
            "2": "The homogeneous and heterogeneous of PLM and server are the homogeneous and heterogeneous faced by other models.",
            "3": "It seems that there is not much difference.",
            "4": "As the author said, when the server model is PLM, the data can be reasonably divided into public data on the server and private data on the client, but it can also be used in other FDs in visual scenes, recommendation scenes, and audio scenes.",
            "5": "Therefore, the first exploration of the server model is that the homogeneous and heterogeneous problems in FD of PLM do not seem to be a contribution, so the contribution seems to be a bit weak.",
            "6": "Although many published federated learning papers lack theories, if there are some more important analyzes such as errors, it will make the papers more solid\n3.",
            "7": "Lack of any reproducible content, including code, experiment/hyperparameter settings,"
        }
    },
    "uPz5a2NvrG": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical task of automatic medical report generation, which is significant for clinical automation and reducing the workload of radiologists.",
            "1": "- The proposed method introduces a novel normal-abnormal semantic decoupling network that optimizes visual extraction by focusing on abnormal semantics, which is a unique approach compared to existing methods.",
            "2": "Potential reasons for acceptance\n   - The method demonstrates superior performance on the benchmark MIMIC-CXR dataset, surpassing state-of-the-art methods.",
            "3": "- The approach effectively mitigates the impact of noisy normal semantics and reports, which is a common issue in medical data.",
            "4": "- The paper provides a comprehensive evaluation, including both natural language generation metrics and clinical efficacy metrics, which strengthens the validity of the results.",
            "5": "Potential reasons for rejection\n   - **Complexity and clarity of the method:**\n     - The paper introduces several novel components, such as the abnormal pattern memory encoder and the decoupling of normal and abnormal semantics, which may be difficult for readers to fully understand without additional clarification.",
            "6": "- The detailed descriptions of the various components and their interactions could be overwhelming and may require further simplification or visual aids to enhance comprehension.",
            "7": "- **Generalizability of the approach:**\n     - The method is evaluated primarily on the MIMIC-CXR dataset, and while it shows promising results, it is unclear how well the approach would generalize to other medical imaging datasets or different types of medical reports.",
            "8": "- The paper does not provide a thorough discussion on the potential limitations and challenges of applying the proposed method to other domains or datasets.",
            "9": "- **Handling of rare diseases and specific descriptions:**\n     - The paper acknowledges that the method may struggle with rare diseases and specific descriptions that are not commonly mentioned in the training data, but it does not provide a concrete solution or future direction to address this issue.",
            "10": "- The reliance on keyword-based detection for normal/abnormal semantics may lead to misclassifications, and the paper does not explore alternative approaches to improve the accuracy of semantic extraction.",
            "11": "Suggestions for improvement\n   - **Enhance clarity and readability:**\n     - Simplify the descriptions of the various components and their interactions, possibly by using more visual aids such as diagrams and flowcharts to illustrate the overall architecture and key processes.",
            "12": "- Provide a more detailed explanation of the novel components, such as the abnormal pattern memory encoder and the decoupling of normal and abnormal semantics, to ensure that readers can fully grasp the concepts.",
            "13": "- **Expand evaluation and generalizability:**\n     - Evaluate the proposed method on additional medical imaging datasets and different types of medical reports to demonstrate its generalizability and robustness.",
            "14": "- Include a discussion on the potential limitations and challenges of applying the method to other domains or datasets, and suggest possible solutions or future research directions to address these challenges.",
            "15": "- **Address handling of rare diseases and specific descriptions:**\n     - Explore alternative approaches to improve the accuracy of semantic extraction, such as incorporating more advanced natural language processing techniques or leveraging external medical knowledge bases.",
            "16": "- Provide a more detailed analysis of the method's performance on rare diseases and specific descriptions, and suggest potential improvements or future research directions to enhance the model's ability to handle these cases."
        },
        "34uvjucb98": {
            "0": "It is interesting to separately model normal and abnormal reports.",
            "1": "The proposed approach could outperform baselines on the benchmark MIMIC-CXR dataset.",
            "2": "The ablation study shows that each of the component could help improve the performance.",
            "3": "The abnormal mode memory part is not quite clear, which is difficult to interpret.",
            "4": "The proposed method is somewhat incremental.",
            "5": "This paper only includes results of one dataset in the main text."
        },
        "RHLG5RNoTb": {
            "0": "The idea of using the codebook in VQVAE to store various anomaly information seems to be a right way to go.",
            "1": "The memory loss in the objective functions is novel and makes sense.",
            "2": "The experimental results seem to be very promising.",
            "3": "And the baselines are comprehensive.",
            "4": "I didn't find any obvous reason to reject the paper."
        },
        "0NlaBNIzU4": {
            "0": "The methodology achieves higher than state-of-the-art for MIMIC CXR report generation.",
            "1": "The methodology is novel, and involves an understanding the structure of radiology report findings beyond just caption generation problem for radiographic images.",
            "2": "In radiology reports, the radiologists tend to write more about the abnormalities than focus on the normal perceptions in the report, and this architecture utilizes that aspect in their modeling quite well.",
            "3": "The analysis of the model is rigorous and gives us a clear understanding of the types of errors to expect.",
            "4": "Though one of the core concepts of the paper is normal-abnormal semantics in reports, the classification from the reports is largely dependent on keyword-based method, which, the authors accept is largely noisy.",
            "5": "The definition of what is an abnormal semantics in a report varies depending on application, and there is no specification of any such definition followed in this paper.",
            "6": "This makes the normal/abnormal semantic section a little weak compared to the other sections.",
            "7": "-- After rebuttal: Even with the rebuttal, I think this portion could have been handled much better."
        }
    },
    "i0vMIpaEn4": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a novel divergence-based adaptive policy (DaP) for simultaneous machine translation (SiMT).",
            "1": "- The proposed method decouples the adaptive policy model from the translation model, offering increased flexibility and potential for improved performance.",
            "2": "- The approach is both memory and computation efficient, extending a frozen wait-k model with lightweight parameters.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a significant challenge in SiMT by proposing a flexible and efficient adaptive policy.",
            "4": "- Experimental results demonstrate that the proposed method outperforms strong baselines across various benchmarks.",
            "5": "- The approach offers an improved trade-off between translation accuracy and latency, which is crucial for real-time applications.",
            "6": "- The paper provides a thorough analysis and ablation studies, supporting the validity and robustness of the proposed method.",
            "7": "Potential reasons for rejection\n   - **Lack of intrinsic evaluation of the policy model:**\n     - The paper primarily focuses on BLEU vs. AL and NLL vs. AL curves for evaluation.",
            "8": "- There is a need for more intrinsic evaluations of the policy model itself to guide further improvements.",
            "9": "- **Limited exploration of modeling variations:**\n     - The paper provides only a limited exploration of different modeling variations for the policy model.",
            "10": "- More in-depth analysis and enhancements could be beneficial to fully understand the potential of the proposed approach.",
            "11": "- **Threshold parameter sensitivity:**\n     - The threshold parameter λ controls latency but does not have a direct one-to-one relationship with latency, unlike the fixed wait-k policy.",
            "12": "- This nuanced aspect requires careful consideration and might complicate the practical application of the method.",
            "13": "Suggestions for improvement\n   - **Conduct intrinsic evaluations of the policy model:**\n     - Include more intrinsic evaluations of the policy model to better understand its performance and guide further improvements.",
            "14": "- **Explore additional modeling variations:**\n     - Investigate different modeling variations for the policy model to identify potential enhancements and fully understand the approach's capabilities.",
            "15": "- **Clarify the relationship between threshold parameter and latency:**\n     - Provide a more detailed analysis of the relationship between the threshold parameter λ and latency to help practitioners better understand and apply the method.",
            "16": "- **Expand on the potential for further improvement:**\n     - Discuss potential areas for further improvement, such as better modeling of divergence supervision signals, to inspire future research and development."
        },
        "aqMh3QxdpG": {
            "0": "-\tSiMT has received more and more attention due to its low latency characteristics.",
            "1": "The proposed method achieves promising results, surpassing the current state-of-the-art ITST.",
            "2": "-\tThe proposed divergence-based policy is interesting and has certain interpretability.",
            "3": "-\tThe proposed method adopts a frozen multi-path wait-k translation model and only needs to train the policy module.",
            "4": "-\tThe article is well written and easy to follow.",
            "5": "-\tThe article has no obvious weaknesses to reject.",
            "6": "-\tA potential issue is that divergence-based policies may be affected by language pairs.",
            "7": "For example, the word order difference in De-En may make the model READ more words, and the author added Max-Continuous-READ to alleviate this issue."
        },
        "LZ6DUU7eAP": {
            "0": "The approach to independently train the adaptive policy based on the idea of divergence is original and the results support the benefits of this simpler approach.",
            "1": "The presentation of the paper is excellent and easy to understand the motivation behind the authors' decisions.",
            "2": "The translation quality improvement over the SoTA ITST model is not significant, but systematic across tasks and latency values."
        },
        "1e09TMAfJ4": {
            "0": "The main contribution of the paper is the proposal of a novel method to construct read/write supervision signals from a parallel training corpus based on statistical divergence.",
            "1": "The paper also presents a lightweight policy model that enables adaptive read/write decision-making for a well-trained multi-path wait-k translation model.",
            "2": "The proposed approach is both memory and computation efficient and offers an improved trade-off between translation accuracy and latency, outperforming strong baselines.",
            "3": "The paper makes contributions to the field of NLP engineering experiment and provides a new approach for SiMT that can be applied to various languages.",
            "4": "There are two papers which are very related to this paper:\n1.",
            "5": "Zheng et al.",
            "6": "Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation.",
            "7": "EMNLP, November 2019.",
            "8": "Zheng et al.",
            "9": "Simultaneous translation policies: from fixed to adaptive.",
            "10": "ACL, 2020, arXiv\n\nThe authors should compare their work with these two lines of work."
        }
    },
    "okV4KG4kMg": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenge of understanding humor in short-form videos, a growing trend on social media platforms.",
            "1": "- It introduces a novel dataset, ExFunTube, which is unique in its multimodal nature, covering a wide range of humor types and domains.",
            "2": "- The study proposes a zero-shot video-to-text prompting method to enhance large language models' (LLMs) ability to explain video humor.",
            "3": "Potential reasons for acceptance\n   - The creation of the ExFunTube dataset fills a gap in existing humor datasets by including both verbal and visual elements.",
            "4": "- The proposed zero-shot video-to-text prompting method shows significant improvements in LLMs' humor explanation capabilities.",
            "5": "- The paper provides comprehensive evaluations using model-based automatic scores, rationale quality experiments, and human evaluations, demonstrating the effectiveness of the proposed approach.",
            "6": "Potential reasons for rejection\n   - **Dataset limitations**\n     - The dataset relies on user-generated content from a specific subreddit, which may introduce biases in the types of humor represented.",
            "7": "- The manual filtering process, although thorough, may still miss some inappropriate or culturally insensitive content.",
            "8": "- **Evaluation methods**\n     - The reliance on model-based automatic scores and rationale quality experiments may not fully capture the nuances of humor understanding.",
            "9": "- Human evaluations, while valuable, are limited to a small sample size and may not be representative of broader audience perceptions.",
            "10": "- **Generalizability of the approach**\n     - The zero-shot video-to-text prompting method's effectiveness may vary significantly across different types of humor and video content not represented in the dataset.",
            "11": "- The approach heavily depends on the performance of existing state-of-the-art models, which may not generalize well to other contexts or future advancements in AI.",
            "12": "Suggestions for improvement\n   - **Dataset enhancement**\n     - Expand the dataset to include a more diverse range of sources beyond the \"r/youtubehaiku\" subreddit to capture a broader spectrum of humor.",
            "13": "- Implement additional automated filtering techniques to further reduce the risk of inappropriate or culturally insensitive content.",
            "14": "- **Evaluation methods**\n     - Increase the sample size for human evaluations to obtain more robust and representative results.",
            "15": "- Consider incorporating additional evaluation metrics that capture the subtleties of humor understanding, such as context-specific humor appreciation.",
            "16": "- **Generalizability and robustness**\n     - Test the zero-shot video-to-text prompting method on a wider variety of humor types and video content to assess its generalizability.",
            "17": "- Explore the integration of newer or alternative state-of-the-art models to ensure the approach remains effective as AI technology evolves.",
            "18": "- **Detailed analysis and case studies**\n     - Provide more detailed analysis and case studies to illustrate the strengths and limitations of the proposed method in different scenarios.",
            "19": "- Include qualitative feedback from human evaluators to gain deeper insights into the model's performance and areas for improvement."
        },
        "fOyPWR3L2v": {
            "0": "This paper has many strengths that make it worthy of acceptance, including the following:\n\n1.",
            "1": "The authors have devised an innovative and inspiring method for dataset creation, seamlessly capturing both textual and visual nuances.",
            "2": "By leveraging current state-of-the-art (SOTA) models and techniques, the paper showcases its relevance and modern approach.",
            "3": "The team has conducted thorough experiments and evaluations.",
            "4": "The research's modular framework stands out, designed to easily integrate or replace components with upcoming SOTA models, ensuring long-term adaptability.",
            "5": "The paper exhibits promising strengths; however, there are certain aspects that could be improved:\n\n1.",
            "6": "Audio information is not fully utilized.",
            "7": "A more extensive exploitation of this modality could enhance the system's comprehension of the data and potentially improve the overall results.",
            "8": "The choice of selected models and methods in the study could use more detailed justification.",
            "9": "Understanding why these specific models were chosen over others and their relative advantages would be beneficial.",
            "10": "Additionally, the potential pitfalls or errors introduced by these models are not sufficiently addressed."
        },
        "AVwfpIHeHc": {
            "0": "The article released a large-scale multimodal humor video dataset that provides novel multi-humor interval annotations, which can provide new data support for community understanding of videos.",
            "1": "Although there are numerous similar frameworks, the article provides a simple and feasible method for converting videos to text.",
            "2": "It can capture videos according to their plot/shot.",
            "3": "The article provides a feasible method for understanding humorous videos, which can achieve the SOTA understanding performance.",
            "4": "The article provides humor interval annotations, but the distribution of the number of humor points is not very balanced, and I believe that videos without humor intervals should be added (negative samples can assist the model in understanding the true meaning of humor).",
            "5": "The annotation of the released dataset was not fully evaluated for quality (or not displayed in the article), so the quality of the dataset cannot be guaranteed.",
            "6": "There is uncomfortable content in the released dataset.",
            "7": "For example, the second example in the attachment not only has formatting issues, but its content also makes me feel very uncomfortable while watching."
        },
        "BNQcQ512yc": {
            "0": "Construct a multi-modal humor video dataset ExFunTube from YouTube with high quality control and support from large language model(LLM), the dataset cover a wide range of domains with various types of humor, and contains annotate start and end timestamps of funny moments and provide text explanations for each moment.",
            "1": "This seems the only contribution.",
            "2": "The author believes that a flaw of the dataset in MAF is the reliance on visual information (at line 79).",
            "3": "However, my opinion is exactly the opposite.",
            "4": "Since it is a multimodal data set, it is reasonable to rely on any modality, and each modality has its importance.",
            "5": "Due to the conversion of visual and acoustic modalities into text descriptions, the final explanation of humor is entirely dependent on text modality, which leads to the loss of some important modal-specific information in the intermediate steps.",
            "6": "More appropriate feature encoding and modal fusion methods could be improved.",
            "7": "In section 4.1, I didn’t see any prompt demonstration.",
            "8": "And I didn’t understand why you emphasize the performance improvement (Line 334) in the section 4.1, which is the part of approach, rather than experimentation.",
            "9": "The proposed approach relies on tedious preprocessing steps for three modalities.",
            "10": "For example, BLIP-2 needs much computing resource and time.",
            "11": "And simultaneously with GPT3.5, the application of this approach is very unrealistic.",
            "12": "Some comparisons with various prompt baselines (same LLM) are missing.",
            "13": "Missing the experiments and analysis about the importance of visual, speech and sound in your task.",
            "14": "With LLMs, it seems natural for your proposed approach performing better than MAF with only traditional Transformer.",
            "15": "This paper is hard to follow and the experiments are difficult to re-produce."
        }
    },
    "FGBWDf7Z19": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenging task of unsupervised speech segmentation into words, which is crucial for modeling infant language acquisition and improving speech-based language models.",
            "1": "- The proposed method leverages recent advancements in self-supervised learning (SSL) models, specifically fine-tuning the XLS-R model, to predict word boundaries from noisy inputs generated by state-of-the-art segmentation systems.",
            "2": "- The approach demonstrates significant improvements in segmentation performance, setting a new state-of-the-art and showing potential for zero-shot segmentation in unseen languages.",
            "3": "Potential reasons for acceptance\n   - The method achieves a substantial improvement in segmentation performance, with an average increase of 130% in token-F1 scores compared to previous state-of-the-art systems.",
            "4": "- The approach is innovative, combining SSL models with semi-supervised learning techniques to iteratively refine word boundary predictions.",
            "5": "- The paper provides comprehensive experimental results across multiple languages and datasets, demonstrating the robustness and generalizability of the proposed method.",
            "6": "- The zero-shot segmentation results highlight the potential for the method to generalize to languages not seen during fine-tuning, which is a significant advancement in the field.",
            "7": "Potential reasons for rejection\n   - **Limited evaluation on diverse datasets:**\n     - The method is primarily tested on the ZeroSpeech corpora, which are pre-segmented and studio-recorded, potentially limiting the generalizability to more diverse and noisy real-world datasets.",
            "8": "- The performance of the model in noisier recording conditions or with different types of speech data (e.g., conversational speech, spontaneous speech) is not evaluated.",
            "9": "- **Dependency on initial segmentation quality:**\n     - The success of the method heavily relies on the quality of the initial word boundaries provided by the off-the-shelf segmentation systems.",
            "10": "- If the initial boundaries are of low quality, the iterative fine-tuning process may not yield significant improvements, as seen with the VAD and random segmentation baselines.",
            "11": "- **Lack of detailed analysis on hyperparameter tuning:**\n     - The paper mentions that hyperparameters were tuned to maximize performance on DP-Parse boundaries, but it does not provide a detailed analysis of the impact of different hyperparameters on the performance of other segmentation systems.",
            "12": "- The choice of hyperparameters and their influence on the final results should be more thoroughly discussed to understand the robustness of the method.",
            "13": "Suggestions for improvement\n   - **Expand evaluation to more diverse datasets:**\n     - Test the method on a wider range of datasets, including those with noisier recording conditions, different types of speech (e.g., conversational, spontaneous), and various levels of background noise to assess the robustness of the approach.",
            "14": "- Include datasets from different domains (e.g., medical, legal) to evaluate the method's applicability in various real-world scenarios.",
            "15": "- **Provide a detailed analysis of hyperparameter tuning:**\n     - Include a comprehensive analysis of the impact of different hyperparameters on the performance of the method, especially for segmentation systems other than DP-Parse.",
            "16": "- Discuss the sensitivity of the method to hyperparameter changes and provide guidelines for selecting optimal hyperparameters for different datasets and segmentation systems.",
            "17": "- **Investigate the impact of initial segmentation quality:**\n     - Conduct experiments to analyze the effect of varying the quality of initial word boundaries on the final segmentation performance.",
            "18": "- Explore techniques to improve the robustness of the method when starting with lower-quality initial boundaries, such as incorporating additional data augmentation or using ensemble methods.",
            "19": "- **Enhance the interpretability of results:**\n     - Provide more detailed explanations and visualizations of why certain segmentation systems (e.g., DP-Parse) perform better with the proposed fine-tuning method compared to others (e.g., DPDP, VG-HuBERT).",
            "20": "- Include qualitative analyses of the segmented outputs to illustrate the improvements and potential limitations of the method in different scenarios."
        },
        "QPRPCShLCt": {
            "0": "1. the fact that simple finetuning on pseudo labels via the general cross-entropy loss can bring such significant gain is very interestingly, which reveals interesting properties of the pretrained self-supervised speech models.",
            "1": "2. this work examined using pseudo labels generated by a range of very different state-of-the-art unsupervised systems, namely DPDP, VG-HuBERT, and DP-Parse, and observed universal improvement.",
            "2": "This indicate the robustness of the proposed approach.",
            "3": "Training one model on all languages leads to better performance than training language specific models.",
            "4": "Although this has been observed in large scale speech models (e.g.",
            "5": "OpenAI's Whisper), it's the first time I observe this phenomenon in small scale studies (the total amount of data is below 80 hours.)",
            "6": "4. a few tricks has being proposed to (potentially) make their approach works better, namely augmentation, smoothing, loss selection, peak detection.",
            "7": "These tricks are not novel, but the usage is new and they make sense intuitively.",
            "8": "These tricks could be valuable for researchers working on speech segmentation\n Although significant improvements are shown, more explanations are desired: \n\n1. with regard to the impressive performance zero-shot DP-Parse in Table 1, why is zero-shot working so well?",
            "9": "What kind of words are being predicted?",
            "10": "Or is there a pattern?",
            "11": "; \n2. what leads to the discrepancy between using pseudo-labels from DPDP, VG-HuBERT, and DP-Parse"
        },
        "2d9hcEjXjY": {
            "0": "Exploring unsupervised ways to perform word segmentation in speech is an interesting direction 1.",
            "1": "I wonder if this is unsupervised, as it still receives supervision signals from a trained system.",
            "2": "Training that system requires supervised labels, and it is uncertain where we can get those labels if the experiments are unsupervised.",
            "3": "This paper does not compare to any force-alignment systems.",
            "4": "The task, in my understanding, can be achieved reasonably easily using force alignment with a GMM-based system.",
            "5": "Why is the system trained on Gold reference achieved 100.0 token-F1?",
            "6": "So is that system already perfect?",
            "7": "What on earth is the evaluation metric?",
            "8": "How is it computed?"
        },
        "60X75uO91q": {
            "0": "The idea of the proposed method is simple yet yields good results.",
            "1": "By fine-tuning the added random feed-forward layer, the paper demonstrates that the pre-trained speech SSL system can be effective for the downstream task of speech segmentation.",
            "2": "Additionally, the paper utilizes speech from different languages, which provides a valuable exploration into the understanding of SSL speech models on multilingual speech segmentation tasks.",
            "3": "The proposed method does not convincingly demonstrate its generalization and novelty, mostly showing the effectiveness of fine-tuning a certain speech SSL model on speech segmentation tasks.",
            "4": "In addition to the main approach, there are many extra tricks used during training and evaluation, such as post-processing, loss sample selection, and data augmentation steps.",
            "5": "It would be clearer to add some analysis or ablation study to convince the readers and show the distinct contributions of these different steps."
        }
    },
    "7MmYaN93lb": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of robustness in multilingual neural machine translation (MNMT), which is essential for developing reliable NLP systems.",
            "1": "- It explores the transferability of robustness across different languages in MNMT, a relatively under-explored area, providing new insights into the robustness of multilingual models.",
            "2": "- The proposed robustness transfer analysis protocol and the empirical findings contribute to the understanding of how robustness can be transferred across languages, which is valuable for improving MNMT systems.",
            "3": "Potential reasons for acceptance\n   - The paper tackles an important and timely problem in the field of NLP, specifically in the context of MNMT.",
            "4": "- The proposed robustness transfer analysis protocol is a novel approach that can be useful for future research in this area.",
            "5": "- The empirical findings are well-supported by extensive experiments, providing strong evidence for the transferability of robustness across languages.",
            "6": "- The paper includes a thorough analysis and visualization of the results, enhancing the understanding of the underlying mechanisms of robustness transfer.",
            "7": "Potential reasons for rejection\n   - **Limited exploration of underlying reasons for findings**\n     - The paper primarily focuses on empirical results without delving deeply into the theoretical explanations for the observed robustness transfer.",
            "8": "- Further investigation into the linguistic and model-specific factors contributing to robustness transfer would strengthen the paper.",
            "9": "- **Synthetic noise vs. natural noise**\n     - The study uses synthetic noise for the experiments, which may not fully capture the complexity and variability of naturally occurring noise.",
            "10": "- The transferability of robustness to real-world scenarios with natural noise remains uncertain and requires further validation.",
            "11": "- **Generalizability of findings**\n     - The experiments are conducted on specific datasets (TED TALKS and News Commentary), which may limit the generalizability of the findings to other datasets or domains.",
            "12": "- Additional experiments on a wider range of datasets and domains would provide more comprehensive evidence for the robustness transfer phenomenon.",
            "13": "Suggestions for improvement\n   - **Investigate underlying reasons for robustness transfer**\n     - Conduct a more in-depth analysis of the linguistic and model-specific factors that contribute to the transferability of robustness across languages.",
            "14": "- Explore the role of shared vocabulary, grammar structures, and language features in facilitating robustness transfer.",
            "15": "- **Incorporate natural noise in experiments**\n     - Include experiments with naturally occurring noise to better understand the transferability of robustness in real-world scenarios.",
            "16": "- Compare the results with those obtained using synthetic noise to identify any discrepancies and refine the robustness transfer analysis protocol accordingly.",
            "17": "- **Expand the range of datasets and domains**\n     - Conduct experiments on a broader range of datasets and domains to validate the generalizability of the findings.",
            "18": "- Include low-resource languages and diverse language families to assess the robustness transfer in different linguistic contexts.",
            "19": "- **Enhance theoretical explanations**\n     - Provide more theoretical insights into the mechanisms of robustness transfer, supported by empirical evidence.",
            "20": "- Discuss potential implications for the design and training of MNMT models to improve their robustness across languages."
        },
        "fOpWWQsTbr": {
            "0": "* The investigation of robustness transferability has not been explored before.",
            "1": "* The results clearly indicate that robustness is transferable.",
            "2": "This finding can be used by follow-up work that aims to improve MNMT.",
            "3": "* The connection to related work is insufficient.",
            "4": "Apart from the observation that the authors investigate robustness in a multilingual setting, and previous work considered a bilingual setting, it does not become clear how this work fits in with the broader field.",
            "5": "Black-box methods and white-box methods are mentioned, but not explained.",
            "6": "No attempt is made to put the experimental results in context with earlier findings of the field.",
            "7": "The paper of the quality would improve with some more discussion: which findings are in agreement with earlier work?",
            "8": "Which disagree, and what are the possible reasons?",
            "9": "From the paper, it's also not clear how the types of noise that you consider are (dis)similar to earlier work.",
            "10": "* The findings lack depth\n\nMost of the findings are unsurprising, which is of course not necessarily an issue.",
            "11": "Unfortunately, for the surprising findings, there is no further investigation.",
            "12": "For instance, comparing Table 2 and Table 3, the authors find that the degree of robustness transfer does not seem to rely on source language similarity, which is a highly surprising finding.",
            "13": "I think the paper would benefit a lot from digging into this deeper.",
            "14": "* Important information is missing.",
            "15": "Presentation could use improvement.",
            "16": "It is not clear what data is used exactly.",
            "17": "Authors mention TED Talks and News Commentary, but it is not clear which experiments use which dataset.",
            "18": "Maybe they are combined?",
            "19": "It is also not clear which test set is used.",
            "20": "TED Talks, News Commentary, or the combination thereof?",
            "21": "It is unclear how to interpret \"bold\" in the Table; this is not mentioned anywhere and it's not as simple as \"best score\".",
            "22": "The differences in Figure 4 and Figure 5 are very subtle, and based on these plots I would not conclude that representations learned using noisy data is more compact compared to representations trained on clean data.",
            "23": "It would be easy to quantify this claim by calculating representational differences, which could be used as additional evidence.",
            "24": "The BLEU signature is missing."
        },
        "MVgXDzRDUa": {
            "0": "- The authors conduct a series of analyses on the core research question, from shallow to deep, providing sufficient (some even redundant) evidence for their claim.",
            "1": "- The findings may motivate more future work on the robustness of the multilingual encoders (known to be vulnerable as QE models).",
            "2": "- Many figures in the paper lead to similar conclusions (e.g., Figure 2 vs.",
            "3": "Figure 3, Figure 4 vs.",
            "4": "Figure 5), which take up lots of space in the paper and make the paper look verbose.",
            "5": "It would be better to move some of them to the appendix.",
            "6": "- Figure 4,5 look a little confusing and need more detailed descriptions.",
            "7": "- For most translation directions, the robustness transferred from EN-FR is not significant enough (still lags behind the results on clean corpus by ≈10 BLEU scores).",
            "8": "Although the authors use growth rates to make the benefits look more obvious in Tables 1,2,3, the transfer effects are still limited.",
            "9": "- I would like to see more results based on multilingual pre-trained models, e.g., mBART (Liu et al., 2020), mRASP2 (Pan et al., 2021).",
            "10": "Liu et al.",
            "11": "Multilingual Denoising Pre-training for Neural Machine Translation.",
            "12": "TACL 2020.",
            "13": "Pan et al.",
            "14": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation.",
            "15": "ACL 2021."
        },
        "bQzh03o4Kf": {
            "0": "The motivation of this paper is clear.",
            "1": "The proposed experiments illustrate some insightful findings.",
            "2": "Although adversarial attacks enhance the robustness of MNMT models, the sacrifice on clean test set seems not trivial (e.g., 1~2 BLEU score on the en-fr translation task in Table1).",
            "3": "It may be helpful to also investigate the trade-off between robustness and clean-performances of MNMT models.",
            "4": "Some cases would be helpful when explaining the findings in section 4.3 and 4.4, i.e., why character-level attack is more useful for enhancing MNMT model robustness in related languages?"
        }
    },
    "XMpzcC9L5z": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the predictability of large language model (LLM) capabilities, which is a significant and timely topic given the rapid advancements in LLMs.",
            "1": "- The study uses the BIG-bench dataset, a comprehensive evaluation suite, to investigate the predictability of LLM performance across various configurations.",
            "2": "- The paper introduces the concept of \"small-bench,\" a subset of tasks that can effectively predict performance on a larger set, which has practical implications for efficient LLM evaluation.",
            "3": "Potential reasons for acceptance\n   - The paper tackles an important problem in the field of NLP, providing insights into the predictability of LLM capabilities.",
            "4": "- The methodology is robust, employing various machine learning models and data splitting strategies to ensure comprehensive analysis.",
            "5": "- The findings are significant, showing that LLM performance can be predicted with high accuracy in certain settings and identifying the importance of task diversity and value in constructing \"small-bench.\"",
            "6": "- The paper is well-structured and clearly presents the research questions, methodology, results, and implications.",
            "7": "Potential reasons for rejection\n   - **Limited to BIG-bench results**\n     - The study's conclusions are based solely on the BIG-bench dataset, which may not fully represent real-world task distributions.",
            "8": "- The generalizability of the findings to other datasets or real-world applications is not thoroughly discussed.",
            "9": "- **Limited to publicly-available LLM meta-data**\n     - The study relies on limited meta-data (model family and number of parameters) and does not account for other important factors like pre-training stability, convergence, and corpus composition.",
            "10": "- The assumption that the input features can implicitly capture this information may not hold in all cases.",
            "11": "- **Limited to interpolation settings**\n     - The experiments focus mainly on interpolation settings, where the test configurations are new combinations of seen elements.",
            "12": "- The study does not extensively explore extrapolation settings, which are crucial for predicting the performance of larger, unseen models.",
            "13": "- **Evaluation metrics limitations**\n     - The primary evaluation metrics (RMSE and R2) have limitations, especially in group-wise comparisons.",
            "14": "- Alternative metrics like task-average Pearson Correlation and Kendall Rank Correlation are introduced but not deeply analyzed.",
            "15": "Suggestions for improvement\n   - **Expand the scope beyond BIG-bench**\n     - Include additional datasets or real-world tasks to validate the generalizability of the findings.",
            "16": "- Discuss the potential limitations and applicability of the results to other contexts.",
            "17": "- **Incorporate more comprehensive LLM meta-data**\n     - Consider including additional meta-data such as pre-training stability, convergence, and corpus composition if available.",
            "18": "- Discuss the potential impact of these factors on the predictability of LLM performance.",
            "19": "- **Explore extrapolation settings in more depth**\n     - Conduct more extensive experiments on extrapolation settings to understand the limitations and capabilities of the prediction models.",
            "20": "- Provide a detailed analysis of the challenges and potential solutions for predicting performance in these settings.",
            "21": "- **Enhance the evaluation metrics analysis**\n     - Provide a more in-depth analysis of the alternative evaluation metrics introduced.",
            "22": "- Discuss the implications of using different metrics and how they affect the interpretation of the results.",
            "23": "- **Improve the clarity and presentation of results**\n     - Include more visualizations and examples to illustrate the findings and their practical implications.",
            "24": "- Provide a clearer comparison of the different methods and their performance across various settings."
        },
        "wXsywxorgC": {
            "0": "The paper introduces a novel method for predicting the performance of Large Language Models (LLMs), which has significant practical implications.",
            "1": "This method could aid developers and researchers in conserving resources when training models, given the vast array of possible combinations of models, hyperparameters, evaluation tasks, and in-context examples.",
            "2": "The results of the paper are notable.",
            "3": "The high R2  scores indicate that the BIG-bench dataset contains highly predictable patterns, thus validating the authors' approach to performance prediction.",
            "4": "In addition, the authors present the idea of “small-bench,” which could reduce computational barriers for models with limited training computation at their disposal.",
            "5": "They provide an understanding of how to construct “small-bench” effectively, suggesting potential applications of these methods to optimize other benchmarks in different contexts Only RMSE and R2 scores are reported.",
            "6": "While this limitation is addressed in the paper, they do not capture the complete picture of a model’s accuracy.",
            "7": "There are a relatively small number of model families in BIG-bench.",
            "8": "This could lead to results, even in the challenging Train-Test split, being due to intrinsic correlation between these model families that might not translate to other models.",
            "9": "There is little discussion of the variety of models and tasks in BIG-bench.",
            "10": "It’s unclear whether the model families present in BIG-bench are significantly different from each other (at least as much as another model with a different pre-training pipeline might be)."
        },
        "Q3AvcVvi5X": {
            "0": "- The problem the authors are trying to solve is real and very relevant to the needs of LLM evaluation: (1) how to predict an LLM's performance without actually running the experiment, and (2) how to quantitatively select the best subset of comprehensive benchmark for quick evaluation.",
            "1": "- The proposed approach is effective even for challenging scenarios.",
            "2": "- The presentation of the main idea is easy to follow, with comprehensive experimental results to back up.",
            "3": "- Choosing explanatory variables in your design matrix does not have a sufficient explanation.",
            "4": "For example, two models with the same architecture could perform widely differently if training one converges while the other fails.",
            "5": "- The authors do not discuss how the emergent abilities of LLMs could influence the regression models' performance.",
            "6": "For example, how do the LLMs' emergent abilities contribute to the RMSE of your regression models?",
            "7": "- The authors need to discuss what conditions should be met to apply their proposed approach.",
            "8": "For example, the authors do not analyze T0/T5 models (L967-969) because of the unavailability of sufficient data to train the model.",
            "9": "However, the authors do not discuss the minimal data requirement to develop a valid analysis with the proposed approach.",
            "10": "- The authors claim that there are limitations of BIG-Bench Hard (BBH) and BIG-Bench Lite (BBL) based on the proposed metrics.",
            "11": "However, to establish the discovered BIG-Bench subset's advantage over BBH and BBL, the authors should use an independent source of information.",
            "12": "For example, a previously ignored LLM due to low ranking on BBH and HHL is found to rank high on the discovered benchmark, and this model is able to do something that high-ranking models on BBH and BBL could not do well."
        },
        "KyZTVwOc5a": {
            "0": "The experiments in this article demonstrate that some capabilities of LLMs are predictable.",
            "1": "This method might serve as a reference metric for future LLM development.",
            "2": "Furthermore, the article emphasizes the significant impact of data on LLMs and suggests a vast space for further research.",
            "3": "This article relies solely on superficial information to assess the predictability of LLM capabilities, which weakens its persuasiveness.",
            "4": "The experiments in this article are quite limited, as all tests were conducted on just one dataset.",
            "5": "This might introduce potential biases and uncertainties regarding its generalization capabilities."
        }
    },
    "rJhk7Fpnvh": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the performance of Large Language Models (LLMs) on Natural Language Inference (NLI) tasks, specifically focusing on the sources of hallucination.",
            "1": "- It introduces two biases, memorization at the sentence level and statistical patterns at the corpus level, as significant contributors to hallucination in LLMs.",
            "2": "- The study is novel in its approach to systematically investigate these biases through controlled experiments across multiple LLM families (LLaMA, GPT-3.5, and PaLM).",
            "3": "Potential reasons for acceptance\n   - The paper provides a thorough and systematic investigation into the sources of hallucination in LLMs, which is a critical issue for the reliability of these models in real-world applications.",
            "4": "- The use of controlled experiments to isolate and identify specific biases is a strong methodological approach that adds rigor to the findings.",
            "5": "- The findings have significant implications for the development and evaluation of LLMs, suggesting that current models rely heavily on memorization and corpus statistics rather than robust reasoning.",
            "6": "- The paper offers practical suggestions for controlling these biases in future evaluations, which can guide future research and development in the field.",
            "7": "Potential reasons for rejection\n   - **Limited scope of biases investigated**\n     - The paper focuses on two specific biases (memorization and corpus statistics) but does not explore other potential sources of hallucination, which may limit the comprehensiveness of the findings.",
            "8": "- The study does not address how these biases interact with other known issues in LLMs, such as contextual understanding or handling of ambiguous language.",
            "9": "- **Dependence on specific datasets**\n     - The experiments are conducted on a limited set of NLI datasets (Levy/Holt and RTE-1), which may not fully represent the diversity of real-world NLI tasks.",
            "10": "- The findings may not generalize to other types of NLI datasets or tasks that involve different linguistic structures or domains.",
            "11": "- **Lack of exploration of mitigation strategies**\n     - While the paper identifies significant biases, it does not provide in-depth exploration or testing of potential strategies to mitigate these biases in LLMs.",
            "12": "- The suggestions for future evaluation controls are valuable, but practical implementation and effectiveness of these controls are not demonstrated.",
            "13": "Suggestions for improvement\n   - **Expand the scope of biases investigated**\n     - Consider exploring additional sources of hallucination in LLMs, such as contextual understanding, handling of ambiguous language, or interaction with other known issues in LLMs.",
            "14": "- Investigate how the identified biases interact with these other factors to provide a more comprehensive understanding of hallucination in LLMs.",
            "15": "- **Diversify the datasets used in experiments**\n     - Conduct experiments on a broader range of NLI datasets that cover different linguistic structures, domains, and real-world applications to ensure the findings are generalizable.",
            "16": "- Include datasets that involve more complex or varied NLI tasks to test the robustness of the identified biases across different scenarios.",
            "17": "- **Explore and test mitigation strategies**\n     - Develop and test practical strategies to mitigate the identified biases in LLMs, such as modifications to the pretraining process, fine-tuning techniques, or architectural changes.",
            "18": "- Provide empirical evidence on the effectiveness of these strategies in reducing hallucination and improving the robustness of LLMs in NLI tasks.",
            "19": "- **Provide more detailed analysis and discussion**\n     - Include a more detailed analysis of the implications of the findings for real-world applications of LLMs, such as question answering, summarization, and fact verification.",
            "20": "- Discuss potential ethical considerations and risks associated with the identified biases and their impact on the reliability and trustworthiness of LLMs in practical use cases."
        },
        "ocpSwMg3Tn": {
            "0": "The theory proposed in this paper is of important value in the task of characterization and mitigation of hallucinatory behavior in LLMs.",
            "1": "It was quite unclear how the experiments performed in the work to corroborate the authors’ theory did that.",
            "2": "In the random premise task – how could the authors ensure that the random predicate indeed resulted in NO-ENTAIL?",
            "3": "I understand that such random sampling has a very small probability of resulting in something that is not NO-ENTAIL, but given that many predicates have synonyms, and other predicates whom they entail, and hence by proxy the current hypothesis might also entail, it feels like it is crucial to ensure that indeed NO-ENTAIL was the case for all the instances (as this is not the train set, but rather the evaluation set).",
            "4": "Additionally, it was not clear how the generic argument task and the random argument task proved what the authors claimed.",
            "5": "All in all, the whole dataset transformation and the ensuing experimental setup felt very cumbersome, and not very clear."
        },
        "SUxmLppr6M": {
            "0": "- The paper shows an important and easy-to-miss limitation of current LMs, highlighting the need to disentangle performance on specific datasets form performance on the general task of interest.",
            "1": "- Very elegant experimental design, both showing evidence for the two new heuristics, and isolating the mechanism by which the model memorizes.",
            "2": "I don't see any reason to reject the paper.",
            "3": "One technical limitation is the use of proxies for occurrence in the training data.",
            "4": "Hopefully indexing the pretraining data of large LMs would enable measuring to what extent these proxies actually reflect occurrence in the training data."
        },
        "aHtYsPHtWX": {
            "0": "Overall I think the the paper presents a nice and creative methodology for showing that biases may lead LLMs to hallucinate.",
            "1": "At the beginning I had a hard time understanding the main contribution of the paper; however, once understood, it believe the conclusions made by this work are significant to the NLP field.",
            "2": "While the contribution may be considered as significant, I believe that it might be a bit limited (only NLI, only 3 LLMs) to be considered for the main EMNLP stage.",
            "3": "Also I find the paper a bit hard to follow.",
            "4": "There are not enough examples, and it took me a lot of time to understand the methodology while I believe it could be described in a better way (for example, relative frequency bias should be better explained).",
            "5": "Other than that I have no major concerns."
        }
    },
    "3AxESAk0Re": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a fundamental task in the vision and language domain, specifically image and text retrieval, which has numerous real-world applications.",
            "1": "- The proposed STAIR model introduces a novel approach by using sparse semantic representations instead of dense embeddings, which are more interpretable and potentially more accurate.",
            "2": "- The model extends the CLIP architecture to map images and texts to a sparse token space, making it easier to integrate with existing information retrieval systems.",
            "3": "Potential reasons for acceptance\n   - The STAIR model demonstrates significant improvements over the state-of-the-art CLIP model, with notable increases in Recall@1 for both text-to-image and image-to-text retrieval tasks.",
            "4": "- The model also shows better performance in zero-shot classification and linear probing tasks compared to CLIP.",
            "5": "- The paper provides a comprehensive analysis of the interpretability of the sparse embeddings, showing that they are substantially more interpretable than dense embeddings.",
            "6": "- The multi-stage training approach proposed in the paper is innovative and crucial for grounding the sparse embeddings in meaningful tokens.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in some sections:**\n     - The explanation of the multi-stage training process could be more detailed to ensure that readers fully understand the necessity and implementation of each stage.",
            "8": "- The description of the FLOPs regularization and its impact on sparsity and performance could be expanded for better clarity.",
            "9": "- **Limited comparison with other models:**\n     - The paper primarily compares the STAIR model with CLIP.",
            "10": "Including comparisons with other state-of-the-art models like ALIGN or GLIP would strengthen the evaluation.",
            "11": "- The paper could benefit from a more detailed discussion on how STAIR performs relative to other sparse representation models in the information retrieval field.",
            "12": "- **Potential biases in the training data:**\n     - The paper acknowledges the potential biases in the training data, particularly from web-mined content, but does not provide a detailed analysis or mitigation strategies.",
            "13": "- The impact of these biases on the model's performance and interpretability should be more thoroughly examined.",
            "14": "- **Computational overhead of multi-stage training:**\n     - The multi-stage training process requires more iterations than the baseline CLIP model, leading to increased computational costs.",
            "15": "This aspect should be discussed in more detail, including potential ways to optimize the training process.",
            "16": "Suggestions for improvement\n   - **Enhance clarity and detail in explanations:**\n     - Provide a more detailed explanation of the multi-stage training process, including the specific roles and benefits of each stage.",
            "17": "- Expand the discussion on FLOPs regularization, including its impact on sparsity and performance, and provide more empirical evidence to support the claims.",
            "18": "- **Broaden the comparison with other models:**\n     - Include comparisons with other state-of-the-art models like ALIGN and GLIP to provide a more comprehensive evaluation of the STAIR model's performance.",
            "19": "- Discuss how STAIR performs relative to other sparse representation models in the information retrieval field, highlighting its unique advantages and potential limitations.",
            "20": "- **Address potential biases in the training data:**\n     - Conduct a more detailed analysis of the potential biases in the training data and their impact on the model's performance and interpretability.",
            "21": "- Propose and implement strategies to mitigate these biases, and evaluate their effectiveness in improving the model's fairness and robustness.",
            "22": "- **Optimize the multi-stage training process:**\n     - Explore ways to optimize the multi-stage training process to reduce computational costs, such as by fine-tuning the number of iterations or using more efficient training techniques.",
            "23": "- Provide a detailed comparison of the computational costs and benefits of the multi-stage training process, including potential trade-offs between performance and efficiency."
        },
        "9xEdZdNOJL": {
            "0": "* The problem addressed by this paper, i.e., learning interpretable image and text representations, is important, which can facilitate the transparency of large VLMs.",
            "1": "* The proposed methods, i.e., the sparse embedding space and the multi-stage training strategy, are reasonable and effective.",
            "2": "* The experiments are well-designed and the results are convincing, which validate the advantage of STAIR in terms of both interpretability and effectiveness across various downstream vision-language tasks.",
            "3": "* The limitations of this work are properly recognized in the paper, which is accompanied by discussions on potential solutions and future directions.",
            "4": "* The paper is well-written and easy to follow.",
            "5": "* No quantitative evaluation supports the claim that STAIR is more efficient than CLIP in text-image retrieval.",
            "6": "* It is unclear whether STAIR can still enhance the interpretability and downstream performance, when combined with more advanced VLMs, e.g., BLIP[1].",
            "7": "[1] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"
        },
        "vb5Qn02eHP": {
            "0": "The paper explores the possibility of employing sparse and grounded token space in image-text retrieval to make embeddings interpretable, with performance improvement on CLIP.",
            "1": "1.Although I agree that the sparse embedding mentioned is more interpretable, this interpretability seems unnecessary in image-text retrieval.",
            "2": "Dense embeddings that machines can understand are enough.",
            "3": "On the other hand, the dense embeddings maybe achieve better approximations to natural semantic (manifold) spaces.",
            "4": "2.The paper claims the proposed STAIR can tackle “…are built on a fixed vocabulary, which cannot handle out-of-vocabulary concepts” in line 87-90.",
            "5": "However, the sparse token space adopts the vocabulary as the basis of embedding space.",
            "6": "How does STAIR deal with words out of the vocabulary?",
            "7": "It seems that there is no essential difference from the dense situation.",
            "8": "3.It is not clear whether the performance improvement comes from the sparse and grounded embedding or the designed training recipe.",
            "9": "How will performance change when porting the training recipe to models with dense embedding?",
            "10": "There seems to be a lack of ablation experiments in this regard.",
            "11": "4.In addition, STAIR needs to be applied to other CLIP or ALIGN based models to verify its effectiveness."
        },
        "VBix0ymHeJ": {
            "0": "A novel representation method for vision and language balancing performance, interpretability and efficiency.",
            "1": "Sufficient experiments verify the effectiveness of this method.",
            "2": "One major part of the training data of this model is 1B internal image-text pairs, which may not be released and cause difficulty to reproduce the experimental results.",
            "3": "As mentioned in Section 6.1, multi-stage training plays a crucial role in ensuring STAIR embedding grounded to meaningful tokens.",
            "4": "But there is no clear explanation about this or detailed discussion about the contribution of each training stage.",
            "5": "As claimed in lines 228-230. all parameters including BERT and MLM head are trained at the stage 1, the only difference between stage 1 and STAIR_{SINGLE-STAGE} is the text masking strategy.",
            "6": "But this masking strategy doesn't seem to resolve the redefining problem ( As declared in lines 218-220, this masking strategy is designed to avoid model learning a shortcut by relying on less common tokens to bridge the modality gap)."
        }
    },
    "DmrIEHJxN5": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the significant challenge of improving reasoning capabilities in small language models, which is crucial for advancing AI's ability to solve complex problems.",
            "1": "- The proposed Cognitive Tree (CogTree) framework is novel in its approach, leveraging dual process theory from cognitive science to enhance reasoning abilities in language models.",
            "2": "Potential reasons for acceptance\n   - The paper presents a well-structured and innovative framework that combines intuitive and reflective systems to improve reasoning in small language models.",
            "3": "- The experimental results demonstrate that the proposed method can achieve performance comparable to much larger models like GPT-3.5, which is a significant achievement.",
            "4": "- The paper provides a thorough evaluation on two challenging reasoning tasks, showcasing the effectiveness and efficiency of the CogTree framework.",
            "5": "Potential reasons for rejection\n   - **Lack of clarity in some sections**\n     - The explanation of the Reflective System's scoring mechanism could be more detailed to ensure readers fully understand how it operates.",
            "6": "- The description of the training process for both systems might benefit from additional clarity and examples to illustrate the steps involved.",
            "7": "- **Limited comparison with other state-of-the-art methods**\n     - While the paper compares CogTree with GPT-3.5, it would be beneficial to include more comparisons with other recent methods in the field to provide a broader context of its performance.",
            "8": "- The paper could discuss more about the limitations and potential weaknesses of the CogTree framework compared to other approaches.",
            "9": "- **Insufficient analysis of failure cases**\n     - The paper includes some error analysis, but it could be more comprehensive in identifying and discussing the reasons behind the failures in both EB and GSM8K tasks.",
            "10": "- Providing more detailed examples of failed cases and potential solutions or improvements could strengthen the paper.",
            "11": "Suggestions for improvement\n   - **Enhance clarity and detail in explanations**\n     - Provide more detailed explanations and examples for the Reflective System's scoring mechanism and the training process for both systems.",
            "12": "- Include diagrams or flowcharts to visually represent the processes and interactions between the Intuitive and Reflective Systems.",
            "13": "- **Expand comparison with other methods**\n     - Include comparisons with a wider range of state-of-the-art methods to provide a more comprehensive evaluation of the CogTree framework's performance.",
            "14": "- Discuss the limitations and potential weaknesses of the CogTree framework in comparison to other approaches.",
            "15": "- **Improve error analysis**\n     - Conduct a more thorough error analysis, identifying specific reasons for failures and discussing potential improvements or solutions.",
            "16": "- Provide detailed examples of failed cases and analyze what went wrong and how the framework could be adjusted to handle such cases better.",
            "17": "- **Explore scalability and generalization**\n     - Investigate the scalability of the CogTree framework by testing it on larger models and more diverse datasets.",
            "18": "- Discuss the generalization capabilities of the framework and how it can be adapted or extended to other types of reasoning tasks or domains."
        },
        "MJPUc8mPLh": {
            "0": "- The paper addresses two relevant issues: the low performance of LLMs in reasoning problems and their computational costs.",
            "1": "- It draws inspiration from cognitive science, making it easier to understand.",
            "2": "Additionally, it applies contrastive learning, where reasoning steps and negative results play a major role which makes the system more understandable.",
            "3": "- The experiments seem to be well-designed and robust.",
            "4": "- The results enhance the accuracy of the systems and, more importantly, significantly reduce the number of parameters required for the calculation, although it requires more intermediate steps.",
            "5": "- A key point of the method, the decomposition of the queries, is not explained in detail, making very difficult to understand how the sub-questions are generated.",
            "6": "In section 3.3 is mentioned \"by leveraging in-context examples\", but it's not clear to me what this means."
        },
        "424YY1bGnZ": {
            "0": "The question whether LMs are able to gain any sort of semantic knowledge and logical reasoning ability is a relevant one, so that paper is fully appropriate for the Audience of this Conference.",
            "1": "- It seems that LMs are not fully adequate to deal with higher-order tasks, such as complex logical reasoning.",
            "2": "Their usage should thus be supported by some sort of either theoretical or practical motivation."
        },
        "La4cpiOiUn": {
            "0": "A thorough experiment about the CogTree method.",
            "1": "Prove the effectiveness of finetuning and adding a reflective system.",
            "2": "Although the story of the intuitive and reflective system sounds fancy, essentially, it's a traditional system with a generator along with a scoring mechanism (discriminator).",
            "3": "The difference lies in the decomposition part, which was discussed widely in previous work like Least-to-Most prompting, Self-Ask,  Decomposition Distillation, DecompT5, etc.",
            "4": "Expected to see the performance gain compared with other SOTA finetune-based methods.",
            "5": "Expected to see a general method to adapt to datasets that do not have intermediate subquestions and answers, which is common in most situations."
        }
    },
    "YokfK5VOoz": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the field of natural language processing (NLP) concerning the potential for large language models (LLMs) to memorize and redistribute copyrighted material.",
            "1": "- It provides empirical evidence on the extent of verbatim memorization in LLMs, which is a relatively underexplored area, especially in the context of copyrighted texts.",
            "2": "- The study's findings have significant implications for the development and deployment of LLMs, particularly in ensuring compliance with copyright laws and protecting intellectual property rights.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a highly relevant and timely issue, given the increasing use of LLMs in various applications.",
            "4": "- It employs a rigorous experimental methodology, using multiple models and datasets to provide a comprehensive analysis of verbatim memorization.",
            "5": "- The findings are well-supported by empirical data, and the paper provides clear and actionable insights for both researchers and practitioners in the field.",
            "6": "- The discussion on the legal and ethical implications of the findings adds depth to the study and highlights the broader impact of the research.",
            "7": "Potential reasons for rejection\n   - **Limited scope of datasets:**\n     - The study focuses on a specific range of best-selling books and LeetCode problems, which may not fully represent the broader landscape of copyrighted materials.",
            "8": "- The selection of datasets might introduce bias, as the chosen books and problems are highly popular and widely available, potentially skewing the results.",
            "9": "- **Lack of diversity in language models:**\n     - The experiments are conducted on a limited set of language models, primarily focusing on well-known models like GPT-3.5 and Claude.",
            "10": "This may not capture the behavior of other models with different architectures and training methods.",
            "11": "- The study does not include newer or less mainstream models, which could provide additional insights into the memorization capabilities of LLMs.",
            "12": "- **Absence of cloze probing experiments:**\n     - The paper does not include cloze probing (i.e., asking models to predict masked tokens) as an additional experiment, which could provide a more nuanced understanding of memorization and generalization in LLMs.",
            "13": "- Including cloze probing could help differentiate between models' ability to generate coherent text and their tendency to memorize and reproduce verbatim content.",
            "14": "- **Insufficient legal analysis:**\n     - While the paper discusses potential copyright violations, it does not provide a detailed legal analysis or draw concrete legal conclusions.",
            "15": "- The study could benefit from collaboration with legal experts to better understand the implications of the findings and provide more robust recommendations.",
            "16": "Suggestions for improvement\n   - **Expand the scope of datasets:**\n     - Include a more diverse range of copyrighted materials, such as academic papers, news articles, and other forms of creative content, to provide a more comprehensive analysis of verbatim memorization.",
            "17": "- Consider using less popular or niche datasets to examine whether the findings hold across different types of content.",
            "18": "- **Increase the diversity of language models:**\n     - Conduct experiments on a wider variety of language models, including newer models and those with different architectures and training methods, to capture a broader range of behaviors.",
            "19": "- Include models from different organizations and research groups to ensure a more representative analysis.",
            "20": "- **Incorporate cloze probing experiments:**\n     - Add cloze probing experiments to the study to provide a more nuanced understanding of memorization and generalization in LLMs.",
            "21": "- Compare the results of cloze probing with direct and prefix probing to identify any differences in the models' behavior.",
            "22": "- **Enhance the legal analysis:**\n     - Collaborate with legal experts to provide a more detailed analysis of the copyright implications of the findings.",
            "23": "- Include a discussion on potential legal frameworks and guidelines for the use of LLMs in generating and distributing content.",
            "24": "- **Improve the clarity and presentation of results:**\n     - Provide more detailed explanations of the experimental setup and the metrics used to evaluate memorization.",
            "25": "- Include additional visualizations and tables to better illustrate the findings and make the results more accessible to readers."
        },
        "9TaJAFEz5x": {
            "0": "- The problem that the authors tackle is an important and practical one -- it is essential to shed light on the legal implications of using LLMs in the wild, and this line of research could hopefully contribute to informing policy in the future.",
            "1": "- The authors analyze many recent state of the art LLMs, and therefore these results could immediately be of interest to researchers and LLM trainers who could test and mitigate copyright infringements in their usage of these models.",
            "2": "- I am unsure if the setting and findings are non-trivially different from previous work on memorization.",
            "3": "For instance, the method used is the exact same as Carlini et al., 2023 (the authors do declare this) -- in that both papers use 50 tokens to prompt the open source models and measure sequence overlaps with the generated token sequences.",
            "4": "Furthermore, the results that models are indeed memorizing copyrighted text is also something that can be predicted by Carlini et al.’s paper.",
            "5": "- There also seems to be some discrepancy about what counts as a copyright violation.",
            "6": "The abstract suggests a limit of 50-1000 words but the results are interpreted only using the 50-word limit.",
            "7": "- Due to the complexities surrounding the topic of copyright violation, this study essentially reduces to one involving long-text memorization (albeit within a focused domain: books and coding problems), which has already been shown for LLMs by Carlini et al., 2023 (though for different LLMs)."
        },
        "bJZaPgivLs": {
            "0": "The paper raises a very crucial question about legality of LLMs and provides some quantitative as well as qualitative evidence for copyright infringement by LLMs.",
            "1": "Update post rebuttal:\n\nI agree with their discerning rebuttal that the experiments around copyright violation specifically deserve to be considered, and in that their methods seem apt and sound.",
            "2": "Hence, I am updating both my soundness and excitement scores.",
            "3": "I would still like to see a more thorough discussion or some other way to assure readers of a legally sound narrative, for example, by listing lawyers who may have been consulted for the paper in the camera ready.",
            "4": "It is hard for Computer Science or Linguistics educated reviewers (like myself) to judge the legal arguments in the paper.",
            "5": "Despite the authors not explicitly making big legal claims, the major contribution of the paper is apparently best left to be judged in a court of law.",
            "6": "The other quantitative insights, e.g., that larger LLMs quote verbatim more, have already been established in literature, as cited by the authors themselves.",
            "7": "Therefore, this paper adds only a few qualitative examples and a few quantitative statistics on fraction of times the copyrighted source was quoted verbatim.",
            "8": "A larger study concerning more research questions and analysis will be better appreciated, e.g., what kind of material is more likely to be quoted verbatim?",
            "9": "Are there ways to modify (at least the smaller models) either at pretraining or at inference to avoid copyright infringement?"
        },
        "VDBHuR4U41": {
            "0": "people really want to know will LLM violate copyright law.",
            "1": "the definition of copyright violation is not clear."
        }
    },
    "fhEkqMyvb0": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue of detecting political bias in news articles, which is significant for maintaining fair and unbiased reporting.",
            "1": "- The proposed approach introduces a novel multi-head hierarchical attention model that considers both sentence-level semantics and document-level rhetorical structure, which is a novel contribution to the field.",
            "2": "Potential reasons for acceptance\n   - The paper presents a well-defined problem and proposes a novel solution that is both robust and style-agnostic.",
            "3": "- The experimental results demonstrate that the proposed model outperforms previous approaches in terms of robustness and accuracy.",
            "4": "- The paper includes thorough analysis and human evaluation, which adds credibility to the proposed method.",
            "5": "Potential reasons for rejection\n   - **Lack of clarity in model explanation**\n     - The description of the multi-head hierarchical attention model could be more detailed and clearer, especially for readers who are not familiar with advanced NLP techniques.",
            "6": "- The paper could benefit from more visual aids or diagrams to explain the model architecture and its components.",
            "7": "- **Limited generalizability of the dataset**\n     - The dataset used for training and evaluation is primarily based on English news articles from Western countries, which may limit the generalizability of the findings to non-English or non-Western contexts.",
            "8": "- The paper does not address how the model would perform on news articles with different cultural or linguistic backgrounds.",
            "9": "- **Insufficient comparison with other state-of-the-art methods**\n     - While the paper compares the proposed model with a BERT baseline, it does not provide a comprehensive comparison with other state-of-the-art methods in political bias detection.",
            "10": "- Including more baselines and a detailed comparison would strengthen the validity of the proposed approach.",
            "11": "Suggestions for improvement\n   - **Enhance model explanation**\n     - Provide more detailed explanations of the multi-head hierarchical attention model, including the role of each component and how they interact.",
            "12": "- Include more visual aids, such as diagrams or flowcharts, to help readers understand the model architecture and its functioning.",
            "13": "- **Expand dataset and evaluation**\n     - Consider including news articles from non-English and non-Western sources to evaluate the model's performance in different cultural and linguistic contexts.",
            "14": "- Discuss potential challenges and solutions for adapting the model to diverse datasets.",
            "15": "- **Include more baselines for comparison**\n     - Compare the proposed model with a wider range of state-of-the-art methods in political bias detection to provide a more comprehensive evaluation.",
            "16": "- Include detailed analysis and discussion of the strengths and weaknesses of the proposed model compared to other methods.",
            "17": "- **Improve clarity and readability**\n     - Ensure that the paper is well-organized and that each section flows logically from one to the next.",
            "18": "- Use clear and concise language to explain complex concepts, making the paper accessible to a broader audience."
        },
        "hxSypqUzVV": {
            "0": "[A1] The proposed model for bias detection seems novel and easy to replicate which allows it to be used by researchers in future studies.",
            "1": "[A2] The experiments done in the paper seems sound and the qualitative experiments are interesting.",
            "2": "[R1] Bias detection is a well-studied task at this point and the authors sidelined a lot of works and models previously proposed for bias detection (refer to missing citations).",
            "3": "As a result, the authors compared their model's performance only with BERT, which is not a state-of-the-art model when it comes to political bias detection in news articles.",
            "4": "Because BERT is not a good encoder when it comes to encoding long contexts such as news articles.",
            "5": "Different approaches have been proposed to perform political bias detection in news articles that are directly applicable to the setting of this paper.",
            "6": "Without comparison with those models, it is difficult to understand the effectiveness of the proposed approach in this paper.",
            "7": "[R2] The proposed model even underperforms BERT in the low-training set and balanced test data setting (refer to Table 2).",
            "8": "However, I did not find a good justification for this low performance in the paper.",
            "9": "Does it mean the proposed model requires more training data and has some bias toward a particular class label?",
            "10": "Per-class classification scores would be really helpful to evaluate that."
        },
        "jUMyWPDTIz": {
            "0": "(1) Their approach avoids the problem of recognizing writing styles of sources instead of principled analysis of bias, which seems an interesting idea\n\n(2) The machine learning models and statistical significance analysis are sophisticated\n\n(3) there is an effort to identify distinct article structures using k-means clustering \n(1) among the multiple papers I have refereed on stance prediction in this cycle, this is the one that I somehow found most difficult to understand.",
            "1": "See my comments on writing below.",
            "2": "(2) The analysis of structure type of Section 8 was not really convincing to me.",
            "3": "The k-means analysis showed that 92% of the articles reflected the inverted pyramid.",
            "4": "with the other two clusters so small as to seem insignificant.",
            "5": "(3) There are some weird results where the baseline model out-performed their model for smaller test sizes, although I applaud the authors for the honesty of including the non-flattering data in the paper."
        },
        "3gfmTmGXXo": {
            "0": "This paper is well written and easy to follow (except for section 4; see my questions below).",
            "1": "The author also presents thorough related work.",
            "2": "The author conducts extensive experiments, and employ several statistical tools to validate their results and findings.",
            "3": "Most of their experimental results seem to be statistically sound (see my concern of section 7.3 conclusion in \"Reasons To Reject\").",
            "4": "The experimental results are insightful.",
            "5": "For example, interesting structural analysis are presented in section 8, which uncovers structural properties which follows the general practice and theoretical background in journalism.",
            "6": "The unsupervised learning approach used here is also interesting, and opens up new possible directions for detecting sentence-level bias.",
            "7": "Comparison is not comprehensive.",
            "8": "One of your cited work, POLITICS (liu et al, 2022), developed a new LM trained on news articles with a special training objective.",
            "9": "To draw a fairer comparison, models more powerful than BERT need to be included.",
            "10": "Section 7.3: I don't think rejecting the null hypothesis could validate the claim \"This shows that our model is more invariant to the data it was trained on than the BERT classifier\".",
            "11": "Instead, rejecting the null hypotheses in almost all but 1 case shows that *your model is more invariant to the random seeds*.",
            "12": "In order to show that your model is invariant to the training data, your null hypothesis should be something like \"mu_1000 = mu_5000 = mu_10000 = mu_full\", i.e., multiple mean difference comparison.",
            "13": "To this end, you may consult ANOVA.",
            "14": "Despite the interestingness of section 8, I am concerned about the validity.",
            "15": "In other words, if you run your k-means algorithm again, you may end up with a different cluster?",
            "16": "It's advised to run k-means multiple times and try to determine the cluster id of an article based on its \"aggregated score\".",
            "17": "Some writings in section 4 are not clear.",
            "18": "For example, you should bold u_i in equation 4, and you are advised to explicitly say that \"the propagation is for structual analysis\" in line 332-335, otherwise people might get confused why you want to propagate the label to sentence-level."
        }
    },
    "j9E9xLlTmB": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the cognitive plausibility of subword tokenization, a topic that has not been extensively explored in previous research.",
            "1": "- It introduces a novel evaluation paradigm that correlates tokenizer output with human performance in lexical decision tasks, providing a new perspective on the effectiveness of tokenization algorithms.",
            "2": "- The study compares three tokenization algorithms across multiple languages and vocabulary sizes, offering a comprehensive analysis of their cognitive plausibility.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and significant contribution to the field of natural language processing by focusing on the cognitive plausibility of subword tokenization.",
            "4": "- The methodology is well-designed, utilizing lexical decision tasks to evaluate the cognitive alignment of tokenization algorithms.",
            "5": "- The results provide valuable insights into the performance of different tokenization algorithms, particularly highlighting the limitations of the UnigramLM algorithm.",
            "6": "- The study's findings have practical implications for the development of more cognitively and linguistically plausible tokenizers.",
            "7": "Potential reasons for rejection\n   - **Limited language scope**\n     - The cognitive analyses are limited to only two Romance and two Germanic languages, which may not generalize to other language families.",
            "8": "- The study does not include morphologically complex languages like Turkish or Finnish, which could provide additional insights into the performance of tokenization algorithms.",
            "9": "- **Variability in data collection**\n     - The response times were collected from separate experiments with slight variations in the data collection procedure, which may affect the comparability of the results.",
            "10": "- Averaging over responses may conceal individual differences between participants, potentially impacting the accuracy of the findings.",
            "11": "- **Metric limitations**\n     - The chunkability metric, while novel, may not fully capture the complexity of human lexical processing and could benefit from further refinement.",
            "12": "- The study does not explore alternative correlation metrics in detail, which could provide a more comprehensive understanding of the relationship between tokenization and cognitive performance.",
            "13": "Suggestions for improvement\n   - **Expand language scope**\n     - Include a wider range of languages, particularly those with complex morphological structures, to provide a more comprehensive evaluation of tokenization algorithms.",
            "14": "- Consider including languages from different language families to ensure the findings are generalizable.",
            "15": "- **Standardize data collection**\n     - Ensure consistency in the data collection procedures for lexical decision tasks to improve the comparability of response times and accuracy across languages.",
            "16": "- Consider using a single experimental setup or harmonizing the procedures across different languages.",
            "17": "- **Refine chunkability metric**\n     - Explore alternative metrics or refine the chunkability metric to better capture the nuances of human lexical processing.",
            "18": "- Investigate the use of other correlation metrics, such as Spearman’s, Kendall’s, and Goodman-Kruskal, to provide a more robust analysis of the relationship between tokenization and cognitive performance.",
            "19": "- **Address individual differences**\n     - Consider analyzing individual differences in participant responses to provide a more detailed understanding of the cognitive plausibility of tokenization algorithms.",
            "20": "- Explore the use of mixed-effects models or other statistical techniques to account for variability between participants."
        },
        "P7rnDFOx9z": {
            "0": "I see this as an impactful short paper: it proposes a fundamental rethinking of how we analyze and evaluate subword tokenizers.",
            "1": "I think the push to evaluate tokenizers in terms of cognitive plausibility is the most important contribution of the paper, above and beyond the specific method proposed, though I also see that method as being promising.",
            "2": "I'd like to see a more thorough analysis of the results along the following dimensions: \n* In correlating with human metrics, the authors don't control (to my knowledge) for potential confounds such as frequency, which is known to affect lexical decision time.",
            "3": "A regression analysis including such metrics would be more compelling; I would rate this paper significantly higher were one given and I think this is the most important point for revisions if the paper is accepted.",
            "4": "* In Figures 1 and 2, I'd like to see the y-axes rescaled either to -1 to 1 or -0.5 to 0.5 for a more consistent reading of the results; more generally, I'd like to see the authors acknowledge more explicitly that none of the correlations reported are particularly high (which is really fine to acknowledge in this setting, since their contribution is the evaluation metric).",
            "5": "I'd also like to see significance testing where the authors report differences in correlation between models.",
            "6": "* The authors mention the shortcomings of Pearson's correlation in their limitations section, but I'd like to see another statistic presented because of this -- something like Goodman-Kruskall might be promising to report.",
            "7": "I think all of these can be easily addressed in the additional page, and aside from the first point, they do not detract from my overall excitement about the proposal.",
            "8": "I'd also like to see some discussion of the segmentations in Table 1: the proposed tokens for e.g.",
            "9": "outfoxed to not correspond to the constituent morphemes of this word, and I'd like to see some discussion of this mismatch.",
            "10": "If the subword tokenizer is often finding tokens that don't directly correspond to morphemes, then I think the cognitive plausibility claims are probably weaker.",
            "11": "Again, though, this does not detract from the main aim of the paper, which is to provide a novel evaluation procedure."
        },
        "6zOb7bJeI9": {
            "0": "The research rationale is well defined and rises an important question: how cognitive plausible are tokenizer algorithm results?",
            "1": "Results suggest a new evaluation paradigm that deserves an open discussion with the conference community  No reason to reject it"
        },
        "baX5Apdf7U": {
            "0": "This paper is clear and well-written.",
            "1": "The cognitive perspective introduced in the task is interesting and provides new lights on the subword tokenisation mechanism.",
            "2": "This technique is of course efficient, but nothing indicates that it is used by the human parser.",
            "3": "These results could argue in favor of the idea that input segmentation is based on low-level techniques, delaying (or even avoiding in some cases) a lexical access to the mental lexicon.",
            "4": "The methodology is not original and the confirm known results."
        }
    },
    "s7Vh8OIIm6": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a novel Hybrid Inverted Index (HI2) that combines embedding clusters and salient terms to accelerate dense retrieval.",
            "1": "- The proposed method aims to address the limitations of traditional inverted file structures by leveraging both semantic and lexical features.",
            "2": "- The paper presents comprehensive experiments to validate the effectiveness and efficiency of HI2 across various retrieval benchmarks.",
            "3": "Potential reasons for acceptance\n   - The hybrid approach of combining embedding clusters and salient terms is innovative and addresses a significant gap in the current dense retrieval methods.",
            "4": "- The experimental results demonstrate that HI2 achieves lossless retrieval quality with competitive efficiency, outperforming several strong baseline methods.",
            "5": "- The paper provides a detailed analysis of the effectiveness-efficiency trade-off, showcasing the robustness of HI2 across different index configurations and embedding models.",
            "6": "- The implementation details and the availability of the source code and model checkpoints enhance the reproducibility and practical applicability of the proposed method.",
            "7": "Potential reasons for rejection\n   - **Complexity and Hyperparameter Tuning:**\n     - The proposed method introduces more hyperparameters compared to conventional inverted file structures, which may require extensive tuning for optimal performance.",
            "8": "- The paper does not provide a detailed guideline or strategy for hyperparameter tuning, which could be challenging for practitioners to implement effectively.",
            "9": "- **Independent Searching Mechanism:**\n     - The current implementation of HI2 involves independent searching of clusters and terms, which may not fully exploit the potential synergy between the two components.",
            "10": "- The paper does not explore alternative mechanisms for more flexible and integrated searching behaviors, which could further enhance the performance of HI2.",
            "11": "- **Limited Analysis of Scalability:**\n     - The paper does not provide an in-depth analysis of the scalability of HI2, particularly in terms of handling extremely large-scale datasets and high-dimensional embeddings.",
            "12": "- The impact of the increased index size on storage and memory requirements is not thoroughly discussed, which could be a concern for practical deployment.",
            "13": "Suggestions for improvement\n   - **Provide Hyperparameter Tuning Guidelines:**\n     - Include a detailed section on hyperparameter tuning, offering practical guidelines and strategies for selecting and optimizing the hyperparameters of HI2.",
            "14": "- Conduct ablation studies to analyze the sensitivity of HI2 to different hyperparameter settings and provide insights into their impact on performance.",
            "15": "- **Explore Integrated Searching Mechanisms:**\n     - Investigate alternative mechanisms for more integrated and flexible searching behaviors that can dynamically balance the contributions of embedding clusters and salient terms.",
            "16": "- Propose and evaluate methods for joint optimization of cluster and term selection to fully exploit their complementary strengths.",
            "17": "- **Enhance Scalability Analysis:**\n     - Conduct experiments on larger-scale datasets and higher-dimensional embeddings to evaluate the scalability of HI2.",
            "18": "- Provide a detailed analysis of the storage and memory requirements of HI2, discussing potential trade-offs and optimization techniques for practical deployment.",
            "19": "- **Expand Evaluation Metrics:**\n     - Include additional evaluation metrics such as precision, F1-score, and NDCG to provide a more comprehensive assessment of the retrieval quality.",
            "20": "- Compare the performance of HI2 with other state-of-the-art retrieval methods beyond the selected baselines to further validate its effectiveness.",
            "21": "By addressing these suggestions, the paper can provide a more comprehensive and practical evaluation of the proposed Hybrid Inverted Index, enhancing its contribution to the field of dense retrieval."
        },
        "BYtwUIEEVU": {
            "0": "* The research topic is important.",
            "1": "* The paper is well written.",
            "2": "* The proposed techniques are solid.",
            "3": "* Some latest related works from the field of high dimensional similarity search are ignored such as [1].",
            "4": "They have been proved to have better performance than graph based methods such as HNSW compared here.",
            "5": "* The comparison with some inverted list based sparse retrieval methods are missing.",
            "6": "There are some works about compression over inverted lists and perfrom similarity search on them such as [2] [3].",
            "7": "They could also be extended as baseline methods.",
            "8": "Since they are optimized, they should perform better than the selected sparse method here.",
            "9": "* There is no result about space overhead."
        },
        "yH8CEQtwVL": {
            "0": "The idea to combine embedding clusters and salient terms is interesting.",
            "1": "The paper is well organized.",
            "2": "The description of the method design and implementation is clear and easy to follow.",
            "3": "The experimental results prove the effectiveness and efficiency of the proposed method on two popular benchmark datasets.",
            "4": "In the experiments, only the query latency is given.",
            "5": "The setup latency should also be given.",
            "6": "The usage of “lossless retrieval quality” is easy to be confusing."
        },
        "7szchI4gAO": {
            "0": "- The hybrid inverted index that it is fast and precise.",
            "1": "- It provides a way to combine lexical and semantic scores smoothly\n- It introduces a way to distill the data to become smaller, and faster, while maintaining precise Methodology weaknesses: It is not clear if they are seeing the same information as the hybrid index.",
            "2": "If they are seeing the same, it is fair and it is a matter of speed the comparison, if they are seeing differences it is a matter of the score but speed is not necessarily comparable, at least as how the manuscript presents results as compared with IVFPQ and HNSW."
        }
    },
    "GOBxWdRpfz": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Re-ViLM, a Retrieval-augmented Visual Language Model, which enhances the Flamingo model for zero-shot and few-shot image captioning.",
            "1": "- The novelty lies in the integration of a multimodal retriever and retrieval-augmented LM layers, which allows the model to retrieve relevant knowledge from an external database, reducing the number of model parameters and improving performance.",
            "2": "- The approach addresses the inefficiencies of existing models in incorporating new data and handling abundant visual concepts and rich textual descriptions.",
            "3": "Potential reasons for acceptance\n   - The proposed Re-ViLM model demonstrates significant improvements in zero-shot and few-shot image captioning tasks compared to baseline models, including Flamingo.",
            "4": "- The retrieval-augmented approach effectively reduces the number of model parameters while maintaining or improving performance.",
            "5": "- The paper provides extensive experimental results on various benchmarks, showcasing the robustness and effectiveness of the proposed model.",
            "6": "- The introduction of a simple filtering strategy to avoid \"copy-and-paste\" behavior during retrieval is a practical and effective solution to a common problem in retrieval-augmented models.",
            "7": "Potential reasons for rejection\n   - **Lack of official implementation and dataset details:**\n     - The paper relies on a re-implementation of the Flamingo model and manually crafted interleaved image-text datasets, which may raise concerns about reproducibility and comparability with other works.",
            "8": "- The absence of official implementation details and datasets used for training and evaluation may hinder the ability of other researchers to replicate the results.",
            "9": "- **Limited scalability and generalization:**\n     - The study does not explore the scalability of Re-ViLM to larger model sizes (e.g., 80B parameters) to fully understand the benefits of retrieval on large-scale visual language models.",
            "10": "- The paper focuses on the Flamingo framework, and it remains unclear how the proposed retrieval design would perform when applied to other image-to-text frameworks.",
            "11": "- **Potential overfitting to specific datasets:**\n     - The performance improvements observed in the paper may be partially attributed to the specific datasets used for pretraining and retrieval, raising concerns about the generalizability of the model to other datasets and tasks.",
            "12": "- The constructed interleaved image-text dataset for pretraining may not fully capture the diversity and complexity of real-world scenarios, potentially limiting the model's applicability.",
            "13": "Suggestions for improvement\n   - **Provide official implementation and dataset details:**\n     - Release the code and datasets used for training and evaluation to facilitate reproducibility and allow other researchers to build upon the work.",
            "14": "- Include detailed descriptions of the re-implementation process and any modifications made to the original Flamingo model.",
            "15": "- **Explore scalability and generalization:**\n     - Investigate the scalability of Re-ViLM to larger model sizes and provide insights into the benefits and challenges of scaling retrieval-augmented models.",
            "16": "- Apply the proposed retrieval design to other image-to-text frameworks and evaluate its performance to demonstrate the generalizability of the approach.",
            "17": "- **Enhance dataset diversity and complexity:**\n     - Incorporate more diverse and complex datasets for pretraining and evaluation to better capture real-world scenarios and improve the model's generalizability.",
            "18": "- Consider using additional benchmarks and tasks to provide a more comprehensive evaluation of the model's performance.",
            "19": "- **Address potential overfitting concerns:**\n     - Conduct ablation studies to understand the impact of different datasets and retrieval strategies on the model's performance.",
            "20": "- Provide a thorough analysis of the model's behavior on out-of-domain and unseen data to assess its robustness and generalizability."
        },
        "QR2VSH4Kue": {
            "0": "- It is interesting to have such a trial of captioning with retrieval.",
            "1": "- The benefit of incorporating retrieval into captioning is not well demonstrated."
        },
        "A5alIcceo5": {
            "0": "The advantages of this article are as follows:\n1.",
            "1": "This paper uses existing public datasets to build a pre-training and evaluation dataset consisting of interleaved image-text pairs for multi-modal pre-training, facilitating contextual learning.",
            "2": "The Re-ViLM model outperforms baselines on various downstream tasks, and results from ablation studies also support the effectiveness of the proposed improvements.",
            "3": "The weakness of this paper is listed as follows:\n1.",
            "4": "The novelty of this paper was limited.",
            "5": "The Re-ViLM model basically followed the structure of the state-of-the-art visual LM and a few minor improvements were proposed.",
            "6": "The most serious problem with this paper is that there is no section discussing limitations."
        },
        "9HAaW7LcSP": {
            "0": "A new method is proposed, by building the model based on pre-trained models and utilizing retrieval-augmented knowledge, the proposed method leads to better results across different settings including zero-shot, few-shot and fully fine-tuning.",
            "1": "Some ablation studies are also conducted.",
            "2": "One important baseline is needed, which is directly evaluating with the retrieved captions (with the proposed filtering).",
            "3": "Meanwhile, more examples of retrieved examples with corresponding generated results are suggested to be shown.",
            "4": "Results with different number of retrieved images are suggested, which can better illustrate the effectiveness of the proposed architecture and retrieval-augmentation respectively.",
            "5": "Inference time and memory comparisons are not shown, they are suggested because extra time and memory are needed to perform the retrieval."
        }
    },
    "ZgJSDBU3px": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical process in modern legal information systems: legal case retrieval.",
            "1": "- It proposes a novel pre-trained model, CaseEncoder, which leverages fine-grained legal knowledge to enhance the encoding of legal case documents.",
            "2": "- The approach includes innovative data sampling and pre-training tasks specifically designed for the legal domain, which are not commonly found in existing models.",
            "3": "Potential reasons for acceptance\n   - The proposed model, CaseEncoder, demonstrates significant improvements over existing general and legal-specific pre-trained models in zero-shot legal case retrieval tasks.",
            "4": "- The introduction of fine-grained legal knowledge in both data sampling and pre-training phases is a novel approach that enhances the model's ability to capture legal features.",
            "5": "- The experimental results on multiple benchmarks show that CaseEncoder outperforms other models, indicating its effectiveness and robustness.",
            "6": "- The paper provides a comprehensive evaluation and ablation study, which supports the validity of the proposed methods and their contributions to the field.",
            "7": "Potential reasons for rejection\n   - **Limited applicability to other legal systems:**\n     - The definition of similar legal cases is based on the assumption that \"cases committing the same crime have articles in common,\" which may not be applicable to all legal systems.",
            "8": "- The paper does not validate the effectiveness of CaseEncoder on datasets from different legal systems or languages, limiting its generalizability.",
            "9": "- **Complexity and scalability of the proposed methods:**\n     - The fine-grained sampling method and the Biased Circle Loss function introduce additional complexity to the model, which may affect its scalability and ease of implementation.",
            "10": "- The transitive rule in the Biased Circle Loss function is not consistent under all circumstances, which may lead to suboptimal performance in certain cases.",
            "11": "- **Lack of comparison with more recent models:**\n     - The paper does not compare CaseEncoder with some of the latest state-of-the-art models in the legal domain, which could provide a more comprehensive evaluation of its performance.",
            "12": "- The reliance on traditional methods like BM25 for certain tasks may limit the potential of the model compared to more advanced techniques.",
            "13": "Suggestions for improvement\n   - **Expand the evaluation to other legal systems and languages:**\n     - Validate the effectiveness of CaseEncoder on datasets from different legal systems and languages to demonstrate its generalizability and applicability beyond the Chinese legal system.",
            "14": "- **Optimize the complexity and scalability of the methods:**\n     - Simplify the fine-grained sampling method and the Biased Circle Loss function to improve the model's scalability and ease of implementation.",
            "15": "- Address the inconsistency of the transitive rule in the Biased Circle Loss function to ensure optimal performance in all cases.",
            "16": "- **Include comparisons with more recent models:**\n     - Compare CaseEncoder with the latest state-of-the-art models in the legal domain to provide a more comprehensive evaluation of its performance.",
            "17": "- Explore the use of more advanced techniques for tasks currently relying on traditional methods like BM25 to potentially enhance the model's capabilities.",
            "18": "- **Provide more detailed explanations and visualizations:**\n     - Include more detailed explanations of the pre-training tasks and the design choices behind them to help readers understand the rationale and benefits of the proposed methods.",
            "19": "- Provide additional visualizations and examples to illustrate the effectiveness of CaseEncoder in capturing legal features and improving legal case retrieval."
        },
        "aw86R2hBSv": {
            "0": "- The proposed model consistently outperforms the RoBERTa baseline as well as other reported methods.",
            "1": "- The ablations are helpful in that it shows how much impact each ingredient has on the final performance (e.g.",
            "2": "data sampling, training objective, etc.).",
            "3": "- This paper may not be suitable for the EMNLP venue for two reasons: 1) The proposed approach relies heavily on domain-specific techniques and the method is not general enough to provide much value for the wider NLP research community.",
            "4": "2) The reviewers may not have the necessary legal acumen to properly judge the claims of this paper.",
            "5": "For instance, the authors justifies the generality of its main methodology by claiming that \"the idea of annotating law articles as fine-grained legal knowledge can be applied ... even to the common law systems.",
            "6": "Therefore, the effectiveness of the proposed methodology in this paper is not limited to a specific legal system.",
            "7": "\", something EMNLP reviewers may not be able to verify.",
            "8": "This paper is probably better suited for a legal NLP conference or workshop.",
            "9": "- The paper focuses on a \"retrieval\" task but only considers embedding models among its baselines.",
            "10": "It would be great to add comparison to standard IR models.",
            "11": "For instance, the BM25 baseline, SoTA dense retrievers (e.g.",
            "12": "DRAGON https://huggingface.co/facebook/dragon-plus-query-encoder), SoTA sparse retrievers (e.g.",
            "13": "SPLADE++ https://github.com/naver/splade), or multi-vector retrievers (e.g.",
            "14": "ColBERT-v2 https://github.com/stanford-futuredata/ColBERT).",
            "15": "Complex domain-specific approaches are better justified if they're shown to significantly outperform off-the-shelf retrieval models.",
            "16": "- Some important details are missing.",
            "17": "Please correct me in the rebuttal if I missed anything, but I failed to find any mention of the \"pre-training data\" used in this paper.",
            "18": "It only mentions the evaluation datasets, but nothing is discussed on the data used to pre-train the model.",
            "19": "What's the corpus for MLM?",
            "20": "What's the case corpus used in the contrastive training?"
        },
        "UiYtTo6aKY": {
            "0": "- PLMs may not be a good fit for legal documents as it fails to capture their specificities, because their aim is to have a broad coverage.",
            "1": "This is the starting point of the paper and the identified problem it aims to solve.",
            "2": "This is very relevant to the current domain-specific NLP literature and concerns.",
            "3": "The authors seek to re-balance this by introducing a layer of legal knowledge, and aiming for a more domain-targeted pre-training.",
            "4": "Literature has indeed shown that models that are closer to a domain, eg legal, despite their smaller size, may still perform better on a range of tasks.",
            "5": "- Very precise task definition in section 3\n- Section 4.1: the fine-grain case sampling aims at addressing the usual bottleneck of explicit supervision, using \"branch level similarity\".",
            "6": "This is very relevant to the legal domain in particular that suffers from lack of resources.",
            "7": "- The idea of introducing legal pre-training is interesting, while most legal NLP work have been focusing on fine-tuning PLMs.",
            "8": "- Evaluation is made on several benchmarks of legal NLP and CaseEncoder always performs better.",
            "9": "Overall, this is a very interesting paper, well written and clear, that explores a new way of introducing knowledge and of fitting the specific needs and challenges of legal NLP.",
            "10": "- Section 4.1: because the sampling is based on law article, with the aim of retrieving similar cases, the authors make the assumption that \"cases committing the same crime have articles in common\" line 277.",
            "11": "One could argue that this may not always be the case, depending on the jurisdiction, the type of law if civil or common, and the area of law (e.g.",
            "12": "international law).",
            "13": "Another drawback is that it would not allow to differentiate among cases that include the same \"crime\".",
            "14": "This rather limited and bounded definition of case similarity is a limitation."
        },
        "EzSmnWSMPQ": {
            "0": "The idea of splitting legal articles into branches is interesting, and I think it is potentially valuable for future studies on this task.",
            "1": "The proposed branch-level similarity is a simple yet effective method to measure the relevance between cases and articles, which shows helpful to the contrastive learning process.",
            "2": "Extensive experiments demonstrate the effectiveness of the proposed method.",
            "3": "Although this paper mentioned two important kinds of concepts, i.e., key circumstances and key elements, they are indirectly modeled via two pretraining tasks.",
            "4": "In addition, this paper does not provide any case study to show that the model indeed comprehends these two kinds of concepts better.",
            "5": "Some parts of the model section are a little confusing."
        }
    },
    "adIeh9ZsfC": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in text-to-video retrieval (TVR) by focusing on frame selection methods to improve retrieval efficiency without sacrificing performance.",
            "1": "- It provides a comprehensive empirical study of existing frame selection methods and introduces two novel methods: redundancy-aware and low-quality-aware frame selection.",
            "2": "- The study's findings offer valuable insights into optimizing frame selection for TVR, which is significant for advancing the field and improving practical applications.",
            "3": "Potential reasons for acceptance\n   - The paper presents a thorough empirical analysis of frame selection methods, which is a valuable contribution to the TVR research community.",
            "4": "- The introduction of two novel frame selection methods (redundancy-aware and low-quality-aware) demonstrates innovation and potential for improving TVR performance.",
            "5": "- The experimental results are comprehensive, covering multiple benchmarks and providing detailed comparisons of different methods.",
            "6": "- The paper offers practical guidelines for selecting optimal frame selection methods, which can benefit both researchers and practitioners in the field.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper could provide more detailed explanations of the implementation of the novel frame selection methods, particularly the redundancy-aware and low-quality-aware methods.",
            "8": "- The description of the experimental setup and parameter settings could be more explicit to ensure reproducibility.",
            "9": "- **Limited evaluation metrics:**\n     - The paper primarily focuses on recall-based metrics (R@1, R@5, R@10, R@sum) and median rank (MdR).",
            "10": "Including additional metrics such as precision, F1-score, or computational cost analysis would provide a more comprehensive evaluation.",
            "11": "- The impact of frame selection on the overall computational efficiency and resource consumption during training and inference could be further elaborated.",
            "12": "- **Generalization to other datasets:**\n     - The study is conducted on three specific TVR benchmarks (MSR-VTT, DiDeMo, ActivityNet Captions).",
            "13": "Evaluating the proposed methods on additional datasets or real-world scenarios would strengthen the generalizability of the findings.",
            "14": "- The paper could discuss potential limitations or challenges in applying the proposed methods to other types of video content or retrieval tasks.",
            "15": "Suggestions for improvement\n   - **Enhance methodological clarity:**\n     - Provide more detailed descriptions of the redundancy-aware and low-quality-aware frame selection methods, including the specific algorithms and parameter settings used.",
            "16": "- Include a step-by-step explanation of the experimental setup, including data preprocessing, model training, and evaluation procedures, to ensure reproducibility.",
            "17": "- **Expand evaluation metrics:**\n     - Incorporate additional evaluation metrics such as precision, F1-score, and computational cost analysis to provide a more comprehensive assessment of the proposed methods.",
            "18": "- Include a detailed analysis of the impact of frame selection on computational efficiency and resource consumption during both training and inference phases.",
            "19": "- **Broaden the scope of evaluation:**\n     - Evaluate the proposed methods on additional datasets or real-world scenarios to demonstrate their generalizability and robustness.",
            "20": "- Discuss potential limitations or challenges in applying the proposed methods to other types of video content or retrieval tasks, and suggest possible solutions or future research directions.",
            "21": "- **Improve visualizations and examples:**\n     - Include more visualizations and qualitative examples to illustrate the effectiveness of the proposed frame selection methods in retaining relevant frames and filtering out inessential ones.",
            "22": "- Provide case studies or detailed examples of how the proposed methods improve retrieval performance in specific scenarios or applications."
        },
        "00cuwBppbl": {
            "0": "The Redundancy-Aware frame selection utilizes the k-medoids++ clustering algorithm to partition the representations of each frame into K groups.",
            "1": "Choose the representation of the center frame of each group on behalf of the whole group.",
            "2": "This eliminates redundant frames.",
            "3": "The Low Quality-Aware frame selection appends a score network after the vision encoder to assess the quality score of each frame.",
            "4": "Leverage the paired text as weak supervision to optimize the score network.",
            "5": "This eliminates the low-quality frames.",
            "6": "The six frame selection methods are verified in more detailed experiments, and the two proposed frame selection methods improve the efficiency of the TVR task without sacrificing the retrieval performance.",
            "7": "1.The evaluation indicators are not very reasonable.",
            "8": "R@sum is rarely seen in articles in TVR.",
            "9": "2.The experiment part uses too much space on the influence of different frame selections on the retrieval performance, which has little improvement.",
            "10": "However, it spends little space on the effect of different frame selections on the retrieval efficiency, whose improvement is very obvious.",
            "11": "3.In experiments, when combining different frame selections, the paper does not show how the hyper parameter K affects the results, which is similar to the analyses in Table 4.",
            "12": "Drawing the conclusion that proper frame selections improve the retrieval efficiency without sacrificing the retrieval performance of TVR,  should be more rigorous."
        },
        "thyr5WdQ8G": {
            "0": "- The authors proposed two simple yet effective text-free methods to improve the frame selection of text-to-video retrieval\n- Experimental results show that the combination of two proposed text-free methods can outperform other text-free baselines.",
            "1": "In the scenario of text-guided methods, the proposed redundant-aware method can also outperform the baselines.",
            "2": "I have some concerns about the following points:\n- in the training of the LQ-A module, is the text paired with a single frame or the whole video clip?",
            "3": "How to guarantee that the similarity can reflect the quality of a frame if the text is paired with a video clip?",
            "4": "A partially matched frame could also play an important role in a video clip.",
            "5": "How about considering the explicit lens movement, transition, and blurred frames, which are easy to detect, making the system interpretable?",
            "6": "-  How the LQ-A and Redun-A are combined?",
            "7": "The calculation of the score of Redun-A is not clear.",
            "8": "- The overall performance of text-free methods is better than the text-guided methods which involve additional query information and higher computational cost.",
            "9": "If it's properly implemented, could you please provide more analysis about it?"
        },
        "N9WedrHFP1": {
            "0": "Thorough review of the state of the art of post-deep learning key frame extraction methods.",
            "1": "Clear improvement over the state of the art.",
            "2": "Intuititively satisfying results.",
            "3": "Well written paper.",
            "4": "1, This is not a weakness peculiar to these authors.",
            "5": "In general the video summarization literature seems to show ignorance of video summarization techniques developed in the multimedia community more than 20 years ago.",
            "6": "Those were techniques that did not rely on the elaborate machine learning techniques on display at this moment but worked quite well.",
            "7": "Here are three examples of such papers:\n\nhttps://www.semanticscholar.org/paper/Motion-activity-based-extraction-of-key-frames-from-Divakaran-Peker/515f899f61c33782ec78045fa849e27e607d9985\n\nhttps://www.ee.columbia.edu/~sfchang/papers/talk-iciap-0903.pdf\n\nhttps://ieeexplore.ieee.org/document/809161\n\nAn inspection of the bibliographies of these papers will reveal that there have been a number of keyframe extraction techniques developed in the past.",
            "8": "Would be nice of the authors to address why their technique establishes novelty over those methods.",
            "9": "Note that I don't want to single these authors out since the entire vision community seems to be reinventing past techniques while also genuinely exploiting the power of deep learning techniques that did not exist in the times of the three papers cited above."
        }
    },
    "vjTnfxbkaL": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical task of Aspect-Based Argument Mining (ABAM) in computational argumentation, which is a significant area of research.",
            "1": "- The proposed Hierarchical Enhancement Framework (HEF) introduces three novel components: Semantic and Syntactic Fusion (SSF), Batch-level Heterogeneous Graph Attention Network (BHGAT), and Span Mask Interactive Attention (SMIA), which are tailored to address specific challenges in ABAM tasks.",
            "2": "- The framework aims to optimize underlying representations, detect argument unit stances, and constrain aspect term recognition boundaries, which are innovative approaches in the field.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-structured and comprehensive framework that addresses key challenges in ABAM, demonstrating significant improvements in performance.",
            "4": "- The introduction of novel components (SSF, BHGAT, SMIA) and their integration into the HEF framework showcases innovation and contributes to the advancement of ABAM research.",
            "5": "- The experimental results on multiple datasets validate the effectiveness of the proposed framework and components, providing strong empirical evidence of their utility.",
            "6": "- The paper includes detailed ablation studies and comparisons with state-of-the-art methods, highlighting the contributions and effectiveness of each component in the framework.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability:**\n     - Some sections of the paper, particularly the technical descriptions of the components, are dense and may be difficult for readers to follow.",
            "8": "- The paper could benefit from clearer explanations and more intuitive descriptions of the novel components and their interactions within the framework.",
            "9": "- **Evaluation and generalization:**\n     - The paper primarily focuses on ABAM tasks and does not extensively explore the generalization of the proposed components to other related tasks in natural language processing.",
            "10": "- The limitations section mentions that BHGAT is currently only suitable for classification tasks with topic information, which may limit the broader applicability of the framework.",
            "11": "- **Comparative analysis:**\n     - While the paper includes comparisons with state-of-the-art methods, it could provide more detailed analysis and discussion on why the proposed framework outperforms existing methods.",
            "12": "- The paper could benefit from additional experiments on more diverse datasets to further validate the robustness and generalizability of the framework.",
            "13": "Suggestions for improvement\n   - **Enhance clarity and readability:**\n     - Simplify and clarify the technical descriptions of the SSF, BHGAT, and SMIA components to make them more accessible to a broader audience.",
            "14": "- Include more illustrative examples and visualizations to help readers understand the novel components and their interactions within the framework.",
            "15": "- **Expand evaluation and generalization:**\n     - Conduct additional experiments on a wider range of datasets, including those from different domains, to demonstrate the generalizability and robustness of the proposed framework.",
            "16": "- Explore the application of the novel components (SSF, BHGAT, SMIA) to other related tasks in natural language processing, such as sentiment analysis or entity recognition, to showcase their broader utility.",
            "17": "- **Provide deeper comparative analysis:**\n     - Include more detailed analysis and discussion on the reasons behind the performance improvements of the proposed framework compared to existing methods.",
            "18": "- Highlight specific cases or examples where the proposed framework excels or fails, providing insights into its strengths and limitations.",
            "19": "- **Address limitations and future work:**\n     - Discuss potential strategies to overcome the current limitations of the BHGAT component and explore its applicability to a wider range of tasks.",
            "20": "- Outline future research directions that could build on the proposed framework, such as integrating additional features or exploring alternative architectures."
        },
        "qt6d63GnLW": {
            "0": "1、The paper propused novel module the BHGAT component.",
            "1": "1、Semantic and Syntactic Fusion component proposed in the paper are not innovative.",
            "2": "The author's explanation of why this component is used lacks clarity.",
            "3": "2、The paper does not consider many state-of-the-art ABAM methods for comparison, it has been noted that the compared baseline models do not adequately address the ABAM task but rather other tasks.",
            "4": "Therefore, the comparison between the proposed method and the baseline model fails to convincingly demonstrate its advancement.",
            "5": "3、Equations and model structure diagrams, being essential components of your research, should align and corroborate with each other.",
            "6": "However, in this paper, there are evident discrepancies, such as eq9, eq10, eq11.",
            "7": "4、In the second part of the narrative, some of the elaborations are not clear enough, for example， in the Basic Module, the meaning of the symbols W and U and Q is not given.",
            "8": "5、Lack of clarity in expression: The writing of the article could be improved, it lacks clarity and precision, and certain paragraphs are too colloquial."
        },
        "1dijR01Az6": {
            "0": "The studied problem is interesting and important.",
            "1": "In general, this paper is well-written and organized.",
            "2": "The experimental results show that the proposed method significantly outperforms state-of-the-art baselines\n 1.The Semantic and Syntactic Fusion (SSF) component aiming to bridge the gap between distant words is not novel.",
            "3": "The topic of long-distance dependencies between words has been studied in previous works [1][2].",
            "4": "It’s not clear how the proposed method advances from them.",
            "5": "2.The overall framework figure is not clear.",
            "6": "This figure does not illustrate The key component modules (Figure 2).",
            "7": "This paper does not clearly explain how the three key challenges were addressed, and each module does not correspond to a challenge.",
            "8": "[1]\tYuanhe Tian, Guimin Chen, Yan Song, and Xiang Wan.",
            "9": "2021.",
            "10": "Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks.",
            "11": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4458–4471.",
            "12": "[2]\tLi, Y., Li, Z., Zhang, M., Wang, R., Li, S., & Si, L. (2019).",
            "13": "Self-attentive Biaffine Dependency Parsing.",
            "14": "In IJCAI (pp.",
            "15": "5067-5073)."
        },
        "lVwAE2ht4U": {
            "0": "The idea of hierarchical enhancement framework for aspect-based argument mining is innovative and intuitive.",
            "1": "This paper clearly and logically describes its contribution to hierarchical enhancement framework for aspect-based argument mining.",
            "2": "From the extensive and comprehensive experimental results of experiments in this paper, the proposed framework demonstrates significant performance improvements over existing approaches..\n3.",
            "3": "The framework studied in this paper is interesting and well motivated and the paper is well written.",
            "4": "Experiments are tested on real datasets.",
            "5": "It is better to choose more baselines from different articles for comparison.",
            "6": "The paper lacks related work.",
            "7": "Experiments on the effectiveness of SMIA component could be added to make the experiments more comprehensive.",
            "8": "Compared with LSTM-CRF, although the performance of SF-CRF is slightly better, will the computational complexity increase dramatically?"
        }
    },
    "oVAod8GRI9": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces NEURO SIM, a novel neuro-symbolic approach for image manipulation guided by natural language instructions.",
            "1": "- It extends the Neuro-Symbolic Concept Learning (NSCL) framework to handle complex multi-hop reasoning tasks in image manipulation.",
            "2": "- The approach is weakly supervised, relying on VQA annotations rather than direct image manipulation supervision, which is a significant contribution to reducing annotation costs.",
            "3": "- The creation of a new dataset, CIM-NLI, and its larger variant, CIM-NLI-LARGE, for evaluating the proposed method is another notable contribution.",
            "4": "Potential reasons for acceptance\n   - The proposed method demonstrates competitive performance with state-of-the-art supervised approaches despite using weak supervision.",
            "5": "- The introduction of a new dataset specifically designed for complex image manipulation tasks fills a gap in the existing resources.",
            "6": "- The approach's interpretability through the generation of intermediate programs is a valuable feature for understanding and debugging the model's behavior.",
            "7": "- Extensive experiments, including zero-shot generalization and multi-hop reasoning, provide a thorough evaluation of the method's capabilities.",
            "8": "Potential reasons for rejection\n   - **Rendering Quality**:\n     - The generated images by NEURO SIM sometimes suffer from rendering errors, such as malformed or partially rendered objects.",
            "9": "- The overall image quality is lower compared to fully supervised models like TIM-GAN, which might affect the practical usability of the method.",
            "10": "- **Complexity of Semantic Parsing**:\n     - The semantic parser's accuracy is crucial for the method's success, and any errors in parsing can lead to incorrect manipulations.",
            "11": "- The reliance on a predefined DSL and the need for a separate training phase for the semantic parser might limit the method's adaptability to new domains.",
            "12": "- **Evaluation on Real-World Datasets**:\n     - The experiments are primarily conducted on synthetic datasets (CLEVR and Minecraft), and the method's performance on real-world images is not thoroughly evaluated.",
            "13": "- The generalization to real-world scenarios with more complex and diverse visual attributes remains uncertain.",
            "14": "Suggestions for improvement\n   - **Enhance Image Rendering**:\n     - Improve the image rendering module to address the quality issues and ensure that the generated images are visually appealing and accurate.",
            "15": "- Consider integrating more advanced generative models or fine-tuning the existing decoder with additional supervision.",
            "16": "- **Robustness of Semantic Parsing**:\n     - Explore ways to enhance the robustness of the semantic parser, possibly by incorporating more sophisticated natural language understanding techniques.",
            "17": "- Investigate the use of large pre-trained language models to improve parsing accuracy and reduce the dependency on a predefined DSL.",
            "18": "- **Evaluation on Real-World Data**:\n     - Conduct experiments on real-world datasets to demonstrate the method's applicability and robustness in practical scenarios.",
            "19": "- Include a diverse set of images with varying complexities to test the model's generalization capabilities.",
            "20": "- **Human-in-the-Loop Feedback**:\n     - Implement a human-in-the-loop feedback mechanism to iteratively improve the model's performance based on user corrections and suggestions.",
            "21": "- This could help in refining the semantic parser and the manipulation networks, leading to more accurate and reliable results."
        },
        "DVom2O5TkK": {
            "0": "The strengths of this paper and the main benefits to the NLP community are as follows:\n- Novel Model: The paper introduces NEUROSIM, the first neuro-symbolic, weakly supervised, and interpretable model for text-guided image manipulation that does not require output images for training.",
            "1": "This model combines the strengths of neural and symbolic representations, offering modularity, interpretability, and improved generalizability.",
            "2": "- New Dataset: The authors extend the CLEVR dataset to create a new dataset called Complex Image Manipulation via Natural Language Instructions (CIM-NLI) and CIM-NLI-LARGE for testing zero-shot generalization.",
            "3": "These datasets provide a benchmark for evaluating models on the task of image manipulation using natural language instructions.",
            "4": "- Competitive Performance: Despite being weakly supervised, NEUROSIM is highly competitive with or beats state-of-the-art baselines that make use of supervised data for manipulation.",
            "5": "This demonstrates the effectiveness of the proposed approach in handling complex reasoning tasks.",
            "6": "- Interpretability: The model is interpretable, as it parses instructions into a symbolic program based on a Domain Specific Language (DSL) comprising object attributes and manipulation operations.",
            "7": "This allows for better understanding and debugging of the model's behavior.",
            "8": "Limited Scope: The paper focuses on image manipulation tasks involving multiple objects and complex multi-hop natural language instructions.",
            "9": "While this is an interesting and challenging problem, the scope of the paper might be limited for researchers working on other aspects of NLP or image manipulation\n2.",
            "10": "Generalizability: Although the paper introduces the CIM-NLI-LARGE dataset to test zero-shot generalization, it is unclear how well NEUROSIM would perform on real-world images or more diverse datasets.",
            "11": "Further evaluation on a wider range of datasets would help establish the model's generalizability.",
            "12": "Comparison with Other Models: The paper compares NEUROSIM with several other models in terms of problem setting, task complexity, and approach.",
            "13": "However, it would be beneficial to see more detailed comparisons, including the performance of NEUROSIM on tasks that other models excel at or struggle with."
        },
        "kSoFPqFR96": {
            "0": "Pros:\n\n1.",
            "1": "Neuro-symbolic methods have the advantage of being completely explainable and make very little errors.",
            "2": "Given the rise of image generation methods, having a neuro-symbolic editor on top of it is an interesting setup, and thus the task is well motivated.",
            "3": "The paper also makes dataset contribution which can be useful for future work.",
            "4": "The proposed neuro-symbolic method is competitive with strong baseline like Instruct-Pix2Pix (IP2P) even though it is trained on significantly less data.",
            "5": "Cons:\n\n1.",
            "6": "My main concern is that there is limited application for CLEVER like scenes which are constrained to add/remove/change.",
            "7": "Further, ``change'' attributes are also restricted.",
            "8": "Compare this to dataset used in IP2P where there are no constraints and is free form.",
            "9": "As such, a model trained on in this dataset is likely to have limited real world use case.",
            "10": "The authors introduce multi-hop reasoning, but these are very synthetic (App B.3).",
            "11": "In real world use case, the instructions are very likely to be only zero-hop since the user knows what they want to edit.",
            "12": "That is the dataset is artificially made tougher.",
            "13": "I feel the authors missed a simpler (but potentially stronger) baseline which uses a combination of neuro-symbolic reasoning and IP2P.",
            "14": "For instance, a neuro-symbolic reasoner (language only, no vision component) could create single-step (zero-hop) instructions which could then be executed by IP2P in a sequential manner.",
            "15": "This would also remove the issues of worse FID scores.",
            "16": "(Minor) The authors remark that other datasets namely CSS, CoDraw and i-CLEVER exist.",
            "17": "Some comparison on those dataset might be helpful for future work (I don't expect the conclusions to change though).",
            "18": "How does the model handle lighting conditions?",
            "19": "In the qualitative example (fig 4, top row), it appears Neuro-SIM is failing to replicate the conditions and thus the generated image is inconsistent.",
            "20": "In Table 3., how does Neuro-SIM with 54k examples perform?",
            "21": "Is the performance saturated?"
        },
        "BMhqsNrerM": {
            "0": "- Presents a unique approach to image editing by integrating Neuro-Symbolic approaches, it seems like a pretty novel method to me.",
            "1": "- I found the idea of using RL to do image editing intriguing, especially since its very difficult to get image editing training data for complex instructions\n- Very detailed evaluation that contains lots of experiments, analysis and details about the system, including a human evaluation and studies on many different settings.",
            "2": "- Positive results over the baselines at least for low data regimens\n- Releases a dataset that can be used in future work - There is little intrinsic motivation to manipulate these kind of synthetic images, meanwhile it seems clear there are many hurdles to applying this system to natural images.",
            "3": "Additionally the image editing operations themselves are relatively basic, most of the complexity/composition in the task comes from identifying what object the instructions is referring.",
            "4": "- The system if very complex but still has mixed results compared to the end-to-end baselines, although the authors have pointed out their system only requires VQA data to train.",
            "5": "- I am struggling a bit to understand why the REINFORCE algorithm works.",
            "6": "The reward seems to be very shallow, so I am failling to see how it could train the model to select the correct manipulation operator, or to prevent the model from simply selecting a random object in the scene.",
            "7": "I think more discussion here is needed.",
            "8": "The author rebuttal had a very helpful explanation of this, I would strongly encourage the authors to include it somewhere in the paper.",
            "9": "Despite these concerns overall I think this is an interesting and novel system and should be accepted."
        }
    },
    "HhoG04UD3E": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces PMIndiaSum, a multilingual and cross-lingual summarization corpus focused on Indian languages, which is a significant contribution given the under-resourced nature of these languages in NLP research.",
            "1": "- The dataset covers 14 languages across four language families and includes 196 language pairs, making it the largest collection of Indian language pairs for summarization to date.",
            "2": "- The paper provides benchmarks for various summarization paradigms, demonstrating the utility of the dataset in aiding summarization between Indian languages.",
            "3": "Potential reasons for acceptance\n   - The creation of a large-scale, high-quality dataset for Indian languages addresses a critical gap in NLP resources, enabling further research and development in this area.",
            "4": "- The detailed methodology for data acquisition, processing, and quality assurance ensures the reliability and usability of the dataset.",
            "5": "- The inclusion of benchmarks for different summarization paradigms provides a valuable reference for future research and highlights the dataset's practical applications.",
            "6": "- The dataset's public availability under a permissive license encourages widespread use and further contributions from the research community.",
            "7": "Potential reasons for rejection\n   - **Limited domain coverage**\n     - The dataset is sourced from a single governmental website, which may introduce a domain bias towards political news.",
            "8": "- The style of the summaries is limited to headline-like summaries, which may not generalize well to other types of summarization tasks.",
            "9": "- **Language support limitations**\n     - While the dataset covers 14 languages, some languages like Manipuri are not supported by the pre-trained language models used in the benchmarks, potentially limiting the dataset's applicability.",
            "10": "- The performance of summarization models for certain languages, such as Telugu, is notably lower, which may indicate issues with data quality or model compatibility.",
            "11": "- **Evaluation and error analysis**\n     - The error analysis reveals significant issues with factuality, omission, and redundancy in the generated summaries, indicating that the models still have substantial room for improvement.",
            "12": "- The manual evaluation of headlines versus first sentences as summaries is limited to only three languages, which may not fully represent the entire dataset.",
            "13": "Suggestions for improvement\n   - **Expand domain coverage**\n     - Consider incorporating data from additional sources to diversify the domain coverage and reduce potential biases.",
            "14": "- Explore the inclusion of different types of summaries, such as abstracts or key points, to enhance the dataset's versatility.",
            "15": "- **Enhance language support**\n     - Investigate methods to improve the performance of summarization models for underperforming languages, such as Telugu, by addressing potential data quality issues or model compatibility.",
            "16": "- Provide additional support and resources for languages not currently supported by pre-trained models, such as Manipuri.",
            "17": "- **Improve evaluation and error analysis**\n     - Conduct a more comprehensive manual evaluation across all languages in the dataset to ensure the generalizability of the findings.",
            "18": "- Develop and implement strategies to address common errors identified in the error analysis, such as improving factual accuracy and reducing redundancy in generated summaries.",
            "19": "- **Continuous updates and community involvement**\n     - Establish a mechanism for continuous updates to the dataset, incorporating new articles and addressing any identified issues.",
            "20": "- Encourage community contributions and collaborations to further enhance the dataset and its applications, potentially through a dedicated platform or repository."
        },
        "zJxEIkuM9T": {
            "0": "- This paper is well-written, well-documented, and easy to follow.",
            "1": "- This paper contributes to the summarization by presenting a new dataset that focuses on Indian languages (including low-resource ones).",
            "2": "- This author conducts extensive experiments and provides several benchmark results under different settings.",
            "3": "- The dataset is domain-specific and biased toward political news.",
            "4": "Therefore it might be questionable whether it could be used as a proper dataset if one wants to evaluate the general capability of a PLM.",
            "5": "- Although the author conducts many experiments, some analyses are not detailed enough (I guess it is due to the page limit).",
            "6": "I would suggest the author go into more detail about the analysis in the camera-ready version."
        },
        "r7Fjp1Uzrv": {
            "0": "- Paper is well written and clear and easy to follow.",
            "1": "The methodology is repeatable for researchers wanting to extend this work.",
            "2": "- The research follows standard experimentation and evaluation so it is comparable with work in the field\n- The corpus itself is of sufficient size and scope to have an impact on this language pairs and the research community\n- Supporting monolingual, cross-lingual, and multi-lingual summarization for under-resourced languages is a good contribution by the authors - Some of the language pairs are covered by other corpora but I do not get a feel for how this would compare in the tasks with different data\n- As noted by the authors the use of external translation models makes isolating the impact of this methodology and dataset difficult as that model may be and is likely ingesting additional data for some language pairs"
        },
        "mtKV7YzHV2": {
            "0": "- Large multilingual dataset for under-resourced language summarization\n- Thorough set of descriptive statistics for the dataset Unclear whether this is the appropriate venue for this paper; something like LREC might be a better fit."
        }
    },
    "BdpoEj33DZ": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces MAILEX, the first dataset for event extraction from conversational email threads.",
            "1": "- It proposes a new taxonomy covering 10 event types and 76 arguments specific to the email domain.",
            "2": "- The dataset includes 1.5K email threads and ∼4K emails, annotated with ∼8K event instances.",
            "3": "- The paper highlights the unique challenges of email event extraction, such as non-continuous, shared trigger spans, non-named entity arguments, and modeling email conversational history.",
            "4": "Potential reasons for acceptance\n   - The introduction of a novel dataset and taxonomy for email event extraction fills a significant gap in the current research landscape.",
            "5": "- The paper provides a comprehensive analysis of the challenges associated with email event extraction.",
            "6": "- The experiments conducted with three different approaches (fine-tuned sequence labeling, fine-tuned generative extraction, and few-shot in-context learning) offer valuable insights into the task's complexity.",
            "7": "- The dataset and source code are made publicly available, promoting further research and development in this area.",
            "8": "Potential reasons for rejection\n   - **Limited context of the dataset**:\n     - The dataset is based on the Enron email corpus, which may not fully represent the diversity of email communications in different domains.",
            "9": "- The context-specific nature of the dataset may limit its generalizability to other email datasets or real-world applications.",
            "10": "- **Performance of the models**:\n     - The performance of the models, especially the in-context learning approach, is significantly lower than expected, indicating that the task is far from being addressed.",
            "11": "- The paper does not provide a clear path forward for improving the models' performance, leaving the reader with unresolved challenges.",
            "12": "- **Modeling email history**:\n     - The current approaches for modeling email history are simplistic, relying on concatenating previous emails, which may not effectively capture the conversational context.",
            "13": "- The paper does not explore more advanced techniques for selectively including relevant emails from the history or using dynamic memory for event extraction.",
            "14": "Suggestions for improvement\n   - **Expand the dataset**:\n     - Consider incorporating emails from other publicly available datasets or domains to increase the diversity and generalizability of MAILEX.",
            "15": "- Annotate additional email threads to cover a broader range of event types and argument roles.",
            "16": "- **Improve model performance**:\n     - Explore more advanced modeling techniques for handling non-continuous, shared triggers and long-span, non-named entity arguments.",
            "17": "- Investigate the use of attention mechanisms or dynamic memory to better model the email conversational history.",
            "18": "- Experiment with other state-of-the-art models and techniques, such as prompt-tuning or fine-tuning open-source chat language models, to improve performance.",
            "19": "- **Provide clearer guidance for future research**:\n     - Offer more detailed suggestions for addressing the challenges identified in the paper, such as specific modeling strategies or architectural improvements.",
            "20": "- Highlight potential areas for further investigation, such as the integration of email event extraction with downstream applications like task management or meeting summarization."
        },
        "ZF7OeHgpOK": {
            "0": "This paper makes several contributions towards the information extraction field.",
            "1": "First, they define a taxonomy for event extraction in corporate email threads.",
            "2": "Second, they release the MailEx dataset which would be a valuable resource for evaluating IE models.",
            "3": "Third, they contribute novel event extraction models and highlight the deficiencies of few-shot LLMs.",
            "4": "I did not find any major technical weaknesses in the paper.",
            "5": "However, the authors could improve the language of certain sections such as the end-to-end extraction model discussion to improve comprehension.",
            "6": "I would also recommend that the authors show statistical test results to support their model comparison claims."
        },
        "xoOXULs9wI": {
            "0": "A new dataset for Email domain itself is interesting and could be important for real applications.",
            "1": "The proposed baseline methods do not perform very well on this dataset, which leaves a large space for future works to further improve.",
            "2": "Though the dataset is very interesting, the granularity of proposed event type is not fine enough.",
            "3": "All the event type is about a specific task for business, like \"schedule a meeting\" and \"request data\".",
            "4": "This is more like emails in a company.",
            "5": "However, if we are talking about event in emails, emails should also cover all the event types of other texts.",
            "6": "People can talk about any events in email, but the key difference is: the context of email is more daily, which may make the task harder.",
            "7": "In my opinion, the authors should consider extend the type set to include all the event types that other datasets use.",
            "8": "The result of GPT-3.5 could be problematic, since the event template provided in the prompt is not easy to understand."
        },
        "UugkVM48jZ": {
            "0": "The dataset is novel and has addressed some of the critical characteristics of email data.",
            "1": "The proposed dataset can be a valuable study resource in this unexplored research area.",
            "2": "The authors have adequately described all the sections of the manuscript, including the taxonomy of the dataset, annotation details, and a detailed analysis of the dataset statistics and unique characteristics.",
            "3": "The authors have described three types of baselines for the proposed dataset and compared the results.",
            "4": "The authors have provided the experimental setting of the models that improve the reproducibility of the baseline models.",
            "5": "The authors also performed the qualitative analysis of the results based on the unique challenges of event and argument extraction of the proposed dataset.",
            "6": "No reason to reject the work."
        },
        "rL74iKrFVH": {
            "0": "The paper has several strengths:\n- Overall, it is well-written with a clear and well-motivated introduction.",
            "1": "- The high-quality annotated dataset will facilitate event extraction research in the email domain and prompt the study of email assistance.",
            "2": "There is a lack of clarity regarding the criteria of designing the schema.",
            "3": "For example, why differentiate the event type \"Request Meeting Data\" from \"Request Data\" as the previous event only cover 0.71% of all events?",
            "4": "The average number of words of arguments is 7.41 and some of them are non-continuous spans.",
            "5": "This will cause difficulties and ambiguities in identifying the arguments."
        }
    },
    "Z1wGHeHBrk": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical gap in the evaluation of Vision-Language Models (VLMs) by focusing on their ability to generalize \"visible\" physical knowledge from static images.",
            "1": "- The introduction of VIPHY, a dataset designed to probe VLMs on object color, size, and spatial relations, is a novel contribution that extends beyond existing benchmarks.",
            "2": "- The automated pipeline for deriving high-quality resources from images at scale is a significant methodological advancement.",
            "3": "Potential reasons for acceptance\n   - The paper presents a comprehensive and well-structured dataset (VIPHY) that covers multiple aspects of visually accessible knowledge, which is a valuable resource for the research community.",
            "4": "- The benchmarking results provide insightful comparisons between VLMs and language models (LMs), highlighting the strengths and weaknesses of current models in retaining and generalizing visual knowledge.",
            "5": "- The findings that a caption-pretrained baseline (CapBERT) outperforms VLMs on size and spatial tasks challenge existing assumptions and open new avenues for research.",
            "6": "- The paper includes detailed experimental setups, metrics, and analysis, ensuring reproducibility and transparency.",
            "7": "Potential reasons for rejection\n   - **Limited evaluation on subtype selection module:**\n     - The paper relies on a pretrained vision-language model (UniCL) for subtype selection without providing a quantitative evaluation of its performance.",
            "8": "- The potential noise introduced by this module could affect the overall quality of the dataset and the validity of the benchmarking results.",
            "9": "- **Assumptions in spatial knowledge extraction:**\n     - The method assumes that the camera's image plane is orthogonal to the ground, which may not hold true for all images, leading to inaccuracies in spatial relation extraction.",
            "10": "- The paper does not address how edge cases (e.g., top-view images) are handled, which could limit the generalizability of the findings.",
            "11": "- **Biases in the dataset:**\n     - The dataset is built from existing visual datasets that lack geographical diversity, potentially introducing biases that are not adequately addressed in the paper.",
            "12": "- The paper acknowledges these biases but does not provide a detailed analysis of their impact on model performance.",
            "13": "- **Limited multilingual considerations:**\n     - The dataset and evaluation are limited to English, excluding considerations for multilingual models and potentially limiting the applicability of the findings to non-English contexts.",
            "14": "Suggestions for improvement\n   - **Evaluate the subtype selection module:**\n     - Provide a quantitative evaluation of the subtype selection module to assess its accuracy and the potential impact of any noise introduced by this step.",
            "15": "- Consider incorporating human annotations for a subset of the data to validate the performance of the automated pipeline.",
            "16": "- **Address assumptions in spatial knowledge extraction:**\n     - Include a discussion on how edge cases (e.g., top-view images) are handled and the potential impact of the orthogonal image plane assumption on the results.",
            "17": "- Consider alternative methods for spatial relation extraction that can account for varying camera angles and perspectives.",
            "18": "- **Analyze and mitigate dataset biases:**\n     - Conduct a detailed analysis of the geographical and cultural biases present in the dataset and their impact on model performance.",
            "19": "- Explore methods to mitigate these biases, such as augmenting the dataset with more diverse images or using bias correction techniques.",
            "20": "- **Expand multilingual considerations:**\n     - Discuss the potential limitations of the dataset and evaluation being limited to English and explore ways to extend the work to multilingual contexts.",
            "21": "- Consider creating multilingual versions of the dataset or evaluating the performance of multilingual models on VIPHY.",
            "22": "By addressing these suggestions, the paper can further strengthen its contributions and provide a more robust evaluation of VLMs' ability to generalize visible physical knowledge."
        },
        "OtEuVrwFTd": {
            "0": "The problem about physical knowledge acquisition and reasoning of VLM is generally interesting.",
            "1": "The work contributed a new dataset in this line of work.",
            "2": "Latest VLMs such as InstructBLIP, LLaVA not included for experiment which makes the claim that existing VLMs struggle to effectively consolidate physical knowledge less appealing."
        },
        "gLjmaSAkQi": {
            "0": "- This work proposes a new task to estimate the attributes such as size, color and space from images, by leveraging VLM and other vision tools.",
            "1": "- The experiments are very extensive and solid.",
            "2": "- This paper is well-written and easy-to-follow.",
            "3": "- The authors provide both source code and data along with the submission, which I appreciate a lot.",
            "4": "I hope it can be made publicly avaible, once published.",
            "5": "- Some implementation details inside the proposed framework are not very informative from the submission.",
            "6": "For example, what object detectors do the authors use for cluster relation?",
            "7": "What depth estimation method does the authors use?",
            "8": "-  As I am not an expert in NLP but in Computer Vision, especially with some low-level vision background, the claim of 'Physical Commonsense Knowledge' does not seem very comfortable for me.",
            "9": "In fact, from the image preception and image formulation prespective, the physicial attributes may actually contain more attributes than color, space and size.",
            "10": "So, it would be highly appreciated if the authors can enhance this part, and provide some references to justify that, these three key factors can also align with the definition of physcial knowledge in vision community.",
            "11": "- Some other minor issues on presentation, please refer to the 'presentation improvement' section."
        },
        "Jt3PfSkiHJ": {
            "0": " - Introduces an innovative pipeline to automatically construct visual physics commonsense dataset that involves use of diverse tools: parser, depth estimation, vision language models, bounding box areas.",
            "1": "- Presents empirical findings that the vision language models show far worse performance in VIPHY than humans and calls for the need to address visible physics common understanding of such models to the NLP community.",
            "2": "- Demonstrates an in-depth evaluation of the performance of current vision language models, taking into account various factors such as types of questions, color categories, and label cardinality.",
            "3": "- No human validation of automated process which might be prone to error due to the following reasons:\n  - Colors are derived from prediction of VL model, specifically OFA that might not yield a correct answer.",
            "4": "-  Size relies on depth estimation models and bounding box areas to get the label.",
            "5": "How do you account for the  case when smaller objects are zoomed in for size questions (See the example in Question A)?",
            "6": "- Spatial: how do you validate that the objects are similar level with one another vs \"smaller/larger\".",
            "7": "What if objects are in front of / behind one another, and have different levels of bounding box coordinates?",
            "8": "- Unclear motivation of dataset collection for visible physics commonsense v.s.",
            "9": "prior work:\n  - Dataset is constructed based on VisualGenome, which already include color and spatial information (above, under, behind, in front of, etc) in their scene graphs attributes and relation labels, and size can be trivially inferred from object labels most of the times.",
            "10": "Why can we not just evaluate on a subset of VisualGenome scene graph tasks that deal with colors and spatial information?",
            "11": "- Winoground [1] was carefully hand-curated by expert annotators instead of relying on model-based predictions, and include far rich annotations testing visual-language compositionality that actually require looking at images to do well.",
            "12": "- ViComte [2] already covers similar and diverse visual commonsense knowledge, such as color, shape, material, size, and visual co-occurrence in their dataset.",
            "13": "How does the authors' work provide more benefits over this dataset?",
            "14": "- Limited coverage of \"visible commonsense\" understanding.",
            "15": "Color seems to be purely vision task, and size and spatial most of the times can be inferred from text information.",
            "16": "There are far more interesting cases that test such knowledge, such as counterfactual reasoning asking (what would happen if this action is applied to the object in this image).",
            "17": "- The findings of CapBERT is not novel as it is already explored in Zhang et.",
            "18": "al [2].",
            "19": "and the paper simply runs the same set up to validate their dataset.",
            "20": "- This in fact further highlights that this dataset is essentially prone to **severe text biases, in which visual understanding is not required to perform well for size and spatial questions**.",
            "21": "It is difficult to claim that the dataset tests *visible* physics commonsense knowledge if such information is not needed.",
            "22": "- Language model experiments are not as comprehensive and do not not cover performance of stronger LLMs such as ChatGPT.",
            "23": "It would be more interesting for the language community to understand in their physics commonsense understanding skills.",
            "24": "- Experiment results illustrate that the task does not have much too improve on from using current SoTA models.",
            "25": "Color questions are extracted from OFA models, and size + spatial questions do not require visual understanding to perform well.",
            "26": "Couldn't then the task be solved by 1) run OFA (which is not evaluated on this dataset) for color questions, 2) run LLMs such as GPT-4 for size + spatial reasoning tasks.",
            "27": "- More qualitative examples of when images are important to do well in the task.",
            "28": "[1]: Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality [Thrush et.",
            "29": "al.]",
            "30": "[2]: Visual Commonsense in Pretrained Unimodal and Multimodal Models [Zhang et.",
            "31": "al.]"
        },
        "YLJLgfUy0o": {
            "0": "The proposed dataset is relatively large in size and  might be a helpful resource to the community.",
            "1": "The writing is clear.",
            "2": "The evaluation lacks inclusion of recent models, which may limit the comprehensiveness of the assessment.",
            "3": "Among the four VLMs evaluated, only one is from 2022, while the rest are from earlier years.",
            "4": "Including more recent, potentially stronger models like OFA, Unified-IO etc., would make the analysis and conclusions more robust and up to date.",
            "5": "Despite the author's emphasis on the dataset's size as a novelty, previous benchmarks have derived similar conclusions and analyses using smaller, manually curated datasets as partially noted in the Related Works section.",
            "6": "The paper does not clarify how utilizing a larger, potentially noisier dataset adds unique value or insights to the community."
        }
    },
    "5sGLPiG1vE": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses an important issue in vision-language models, specifically the Concept Association Bias (CAB), which affects the performance of models like CLIP in tasks requiring fine-grained correspondence between vision and language.",
            "1": "- The novelty lies in identifying and systematically analyzing CAB, demonstrating its impact on zero-shot classification and visual question answering (VQA), and proposing potential mitigation strategies.",
            "2": "Potential reasons for acceptance\n   - The paper provides a thorough investigation of CAB, a previously underexplored phenomenon, contributing to the understanding of vision-language model limitations.",
            "3": "- The experiments are well-designed and cover various aspects of CAB, including its prevalence in different models and its impact on performance.",
            "4": "- The proposed mitigation strategy of deeper modality interaction and fine-tuning is a practical approach that shows promise in reducing CAB.",
            "5": "- The paper includes extensive related work and situates its contributions within the broader context of vision-language model research.",
            "6": "Potential reasons for rejection\n   - **Limited scope of attributes tested**:\n     - The paper primarily focuses on color and part-whole relationships, leaving out other important attributes like shape, texture, and material.",
            "7": "- This limited scope may not fully capture the extent of CAB across different types of object-attribute relationships.",
            "8": "- **Dependence on specific datasets**:\n     - The experiments rely heavily on the Natural-Color Dataset (NCD) and Visual Question Answering (VQA-v2) dataset, which may not generalize to other datasets or real-world scenarios.",
            "9": "- The use of synthetic datasets like UNCD and UNCD-v2 may not accurately reflect the complexities of natural images.",
            "10": "- **Mitigation strategy limitations**:\n     - The proposed fine-tuning approach may not fundamentally solve the binding problem, as indicated by the lower accuracy on UNCD after fine-tuning.",
            "11": "- The paper does not explore alternative methods for mitigating CAB, such as incorporating object-centric learning or other architectural changes.",
            "12": "- **Insufficient analysis of model behavior**:\n     - The paper does not provide a detailed analysis of why certain models exhibit more CAB than others, particularly the differences between contrastive and autoregressive losses.",
            "13": "- The impact of different training objectives and architectures on CAB is not thoroughly explored.",
            "14": "Suggestions for improvement\n   - **Expand the scope of attributes tested**:\n     - Include experiments on additional attributes like shape, texture, and material to provide a more comprehensive analysis of CAB.",
            "15": "- Test CAB on a wider range of datasets to ensure the findings are generalizable.",
            "16": "- **Explore alternative mitigation strategies**:\n     - Investigate other methods for reducing CAB, such as incorporating object-centric learning or modifying the model architecture to better handle object-attribute binding.",
            "17": "- Consider using attention mechanisms or other techniques to improve the model's ability to correctly bind objects and their attributes.",
            "18": "- **Provide a deeper analysis of model behavior**:\n     - Analyze the differences between models trained with contrastive and autoregressive losses in more detail to understand why some models exhibit more CAB.",
            "19": "- Explore the impact of different training objectives and architectures on CAB to identify potential improvements.",
            "20": "- **Improve the clarity and organization of the paper**:\n     - Ensure that the explanations of experiments and results are clear and concise, with well-labeled figures and tables.",
            "21": "- Provide a more detailed discussion of the implications of CAB for real-world applications and future research directions."
        },
        "n7x8k4D8z4": {
            "0": "Enough experiments and many inspiring findings.",
            "1": "The Concept Association Bias did not be scrutinized carefully.",
            "2": "Good writing.",
            "3": "The structure of the paper is clear and reading it is smooth and natural.",
            "4": "Some of the observations can be more solid.",
            "5": "For example, as described in line 493 - 495, the relation between the CAB Score and the accuracy can be quantified statistically whether the correlation is significant enough."
        },
        "qL0BQWLDsq": {
            "0": "The compositionality and attributed binding of VL models is an important problem.",
            "1": "This paper systematically studies this problem and gives some insights.",
            "2": "A comprehensive evaluation of different popular VL models ranging from models based on CE loss to models based on regressive loss, is performed.",
            "3": "The experimental parts are solid and CLIP with different encoders are evaluated.",
            "4": "There is no new effective method proposed to address the Concept Association Bias problem even though the work did some study of the effect of finetuning.",
            "5": "The novelty is limited and many similar works have been done previously, e.g.",
            "6": "[1][2][3][4].",
            "7": "[1] Learning to Compose Soft Prompts for Compositional Zero-Shot Learning\n\n[2] Training-Free Compositional Image and Text Matching\n\n[3] Does CLIP Bind Concepts?",
            "8": "Probing Compositionality in Large Image Models\n\n[4] Augmenting CLIP with Improved Visio-Linguistic Reasoning"
        },
        "1TxdgZCeDe": {
            "0": "I think the paper asks a focused interesting question, exhaustively investigates that, proposes a mitigation mechanism, and clearly points out the limitations of the proposed solution.",
            "1": "I like the expository style of writing.",
            "2": "I don't see many reasons."
        }
    },
    "wirDXDQwYZ": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the relatively unexplored area of quantifier semantics in foundation models, which is significant for enhancing natural language understanding and human-AI interactions.",
            "1": "- The introduction of the QuRe dataset and the PRESQUE framework represents a novel approach to quantifier reasoning by combining natural language inference with pragmatic reasoning.",
            "2": "Potential reasons for acceptance\n   - The paper presents a well-defined problem and a novel solution that leverages pragmatic reasoning to improve quantifier understanding in foundation models.",
            "3": "- The introduction of the QuRe dataset fills a gap in existing resources for evaluating quantifier semantics, providing a valuable benchmark for future research.",
            "4": "- The experimental results demonstrate a significant improvement in performance using the PRESQUE framework compared to a literal reasoning baseline, highlighting the effectiveness of the proposed approach.",
            "5": "Potential reasons for rejection\n   - **Limited scope of quantifiers:**\n     - The study focuses on a small subset of generalized quantifiers, which may not cover the entire spectrum of quantifier semantics.",
            "6": "- The limited scope may reduce the generalizability of the findings to other types of quantifiers or more complex linguistic contexts.",
            "7": "- **Dataset limitations:**\n     - The QuRe dataset is derived solely from Wikipedia, which may limit the applicability of the results to other domains or types of text.",
            "8": "- The reliance on crowd-sourced annotations and GPT-3.5-turbo assistance may introduce biases or inconsistencies in the dataset.",
            "9": "- **Evaluation metrics and analysis:**\n     - The evaluation metrics used (e.g., HIT@1, MRR, cross-entropy) may not fully capture the nuances of quantifier understanding and the effectiveness of the PRESQUE framework.",
            "10": "- The analysis of human preferences and the comparison with literal reasoning baselines could be more comprehensive to provide deeper insights into the strengths and weaknesses of the proposed approach.",
            "11": "Suggestions for improvement\n   - **Expand the scope of quantifiers:**\n     - Include a broader range of quantifiers in the study to cover more aspects of quantifier semantics and improve the generalizability of the findings.",
            "12": "- Investigate the performance of the PRESQUE framework on more complex linguistic contexts involving multiple quantifiers or nested quantifier structures.",
            "13": "- **Enhance the dataset:**\n     - Diversify the sources of the dataset beyond Wikipedia to include texts from different domains, genres, and languages to test the robustness of the approach.",
            "14": "- Implement more rigorous quality control measures for crowd-sourced annotations and explore alternative methods for obtaining high-quality annotations.",
            "15": "- **Refine evaluation metrics and analysis:**\n     - Develop additional evaluation metrics that capture the subtleties of quantifier understanding and provide a more comprehensive assessment of the PRESQUE framework.",
            "16": "- Conduct a more detailed analysis of the human preferences and the comparison with literal reasoning baselines to identify specific areas where the PRESQUE framework excels or needs improvement.",
            "17": "- Include ablation studies to understand the contribution of different components of the PRESQUE framework and identify potential areas for further optimization."
        },
        "l2WtFnw8eX": {
            "0": "- The motivation and problem statement of the paper is clear, and the theoretical background and experimental design to support the authors’ claims are clearly explained, making the text itself highly complete.",
            "1": "- The attempt to limit the range of Generalized Quantifiers using the perspective of pragmatics seems novel, and the QuRe dataset also has a versatility that can be useful in the NLI field where quantifier-related reasoning is required in the future.",
            "2": "- I have no reason to reject."
        },
        "EivO1ByeJJ": {
            "0": "The paper makes a contribution to the NLP interpretation of generalized quantifiers (GQs), which are frequent in natural language communication but have been mostly neglected in NLP.",
            "1": "GQs are notoriously vague in their meaning but for some applications  (e.g.",
            "2": "in human-robot interaction) it is necessary to interpret the quantification in percentages.",
            "3": "The paper offers an interesting pragmatically inspired algorithm to determine percentage interpretation on the basis of existing LLMs that are fine-tuned for NLI.",
            "4": "It also introduces the new dataset QuRe of sentences that feature GQs.",
            "5": "The QuRe sentences are automatically (GPT-3.5-turbo) annotated with percentage range (derived from the original sentences that included percentage information plus potential modifications such as \"about\", \"at least\")  and 3 topic labels.",
            "6": "They are also manually annotated with a 3-level “specificity” score which indicates whether the sentence without the quantifier gives partial or full information about the percentages or whether it is indeterminable from the context.",
            "7": "For the 5 quantifiers in the Herbelot-Vecchio dataset percentage interpretations are crowd-sourced from 25 human judges and visually compared to (RoBERTa-large NLI-based ) PRESQUE results.",
            "8": "A random sample of the results on the Herbelot-Vecchio dataset is evaluated by human judges.",
            "9": "The results on the QuRe dataset are evaluated by employing five evaluations measure  that emphasize different properties (HIT@1, MRR, Cross-Entropy, F1@n, and consecutiveness}.",
            "10": "Although not good, the results are better than (random seeds) baselines and the generall quantifier strengths correspond to previous hierarchies based on human judgements.",
            "11": "Origin of QuRe und its distribution of GQs: The QuRe dataset is based on Wikipedia sentences that include the target percentages which means that the LLMs have most likely seen the target interpretation in their pre-training data.",
            "12": "That decreases the usefulness of the data set.",
            "13": "In addition, the distribution of the quantifier expressions in QuRe is very skewed (26,3 % \"some\" aka about 196 instances but less than 1% aka 7 or less instances of \"likely\", \"seldom\", \"occasionally\", \"none\" - the authors only provide relative frequencies).",
            "14": "The distribution of percentage scopes is not reported, but doesn't seem to be balanced either.",
            "15": "Linguistic soundness: The set of \"quantifier words\" is very heterogenous, some are not normally seen as quantifiers: The reference to Srivastava et al.",
            "16": "(2018) accounts for the inclusion of frequency adverbs such as \"usually\", \"rarely\", but the inclusion of bare adjectives such as \"tiny\"or \"large\" without a noun such as \"part\" or \"amount\" requires further explanation (I don't think that it is a problem that they are listed in the crowd-sourcing task without explanation, though).",
            "17": "Thoroughness/Informativeness: The quantitative evaluations on QuRe report only averaged values over all quantifiers.",
            "18": "Soundness: The human evaluation on the performance of L0 vs. L1 on the 50 test sentences that were randomly chosen from the Herbelot-Vecchio dataset (10 sentences per each of the 5 quantifiers) evaluated by 40 annotators is not tested for statistical significant differences.",
            "19": "[In favour of the authors' interpretation: If I interpret the numbers in figure 7 correctly the results for \"some\", \"all\" and the overall values are statistically significant (based on a chi-square test) and do indeed prefer the output of the PRESQUE L1].",
            "20": "Clarity: The paper works with two datasets with two different sets of \"quantifiers\".",
            "21": "The paper should always indicate which dataset is referred to,  e.g.",
            "22": "in all table/figure captions (as it is the case in figure 1 and table 3)."
        },
        "MtlQh9h9oQ": {
            "0": "The paper smartly designed a new corpus with quantifiers mapped to a percentage scope which be easily evaluated for quantifier semantics inference.",
            "1": "The authors presented the annotation and data statistics in detail.",
            "2": "A suite of comprehensive evaluations is conducted including the comparison of the distributions between human and model interpretation, automatic metrics, consecutiveness, etc.",
            "3": "Overall this paper presented a thorough introduction to data collection, analysis, and experimental results.",
            "4": "I have several clarification questions regarding the framework design and annotation procedure.",
            "5": "Please check below."
        }
    },
    "MXMA6vQtSZ": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses an important and relatively unexplored area of political bias in automatic summarization models.",
            "1": "- It introduces an entity replacement method to investigate how different summarization models portray political figures, specifically Donald Trump and Joe Biden.",
            "2": "- The study provides a foundation for future research on bias in summarization and normative discussions on the ideal qualities of automatic summaries.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a significant issue in NLP, contributing to the understanding of political bias in summarization models.",
            "4": "- The methodology is innovative, using entity replacement to control for content and assess biases.",
            "5": "- The findings are consistent and robust across multiple summarization models, providing valuable insights into how these models handle political entities.",
            "6": "- The paper is well-structured, with clear explanations of the methodology, results, and implications.",
            "7": "Potential reasons for rejection\n   - **Limited scope of entities studied**\n     - The study focuses only on two US politicians, Donald Trump and Joe Biden, which may limit the generalizability of the findings.",
            "8": "- The inclusion of only two additional entities (Obama and Bush) may not be sufficient to verify the robustness of the findings across a broader range of political figures.",
            "9": "- **Exclusive focus on English news articles**\n     - The study is limited to English news articles, which may not capture biases present in other languages or political contexts.",
            "10": "- The findings may not be applicable to non-English speaking countries or different political regimes.",
            "11": "- **Lack of consideration for co-reference resolution**\n     - The entity replacement method does not account for co-reference resolution, which could affect the accuracy of the replacement and the resulting summaries.",
            "12": "- This limitation may lead to inconsistencies in the analysis and interpretation of the results.",
            "13": "- **Potential for adversarial use**\n     - The methodology could be misused to manipulate summaries by replacing entity names, contributing to misinformation and fake news.",
            "14": "- The ethical implications of this potential misuse are not thoroughly addressed in the paper.",
            "15": "Suggestions for improvement\n   - **Expand the scope of entities studied**\n     - Include a broader range of political figures from different countries and political contexts to enhance the generalizability of the findings.",
            "16": "- Consider studying less prominent or controversial politicians to see if the biases observed are consistent across different levels of prominence.",
            "17": "- **Incorporate co-reference resolution**\n     - Implement co-reference resolution in the entity replacement method to ensure more accurate replacements and improve the reliability of the analysis.",
            "18": "- Address potential challenges and limitations associated with co-reference resolution in the context of entity replacement.",
            "19": "- **Explore non-English news articles**\n     - Extend the study to include news articles in other languages to investigate whether similar biases exist in non-English summarization models.",
            "20": "- Compare the findings across different languages and political contexts to provide a more comprehensive understanding of political bias in summarization.",
            "21": "- **Address ethical implications**\n     - Discuss the potential for adversarial use of the methodology and propose safeguards to prevent misuse.",
            "22": "- Highlight the importance of responsible deployment and usage of summarization models, considering the ethical implications of the findings."
        },
        "lZB70sZrMy": {
            "0": "- methodology of keeping everything the same except the entity name is simple and strong\n- code and data will be released\n- has important consequences for example related to how quickly (or not quickly) associations change in a language model (aka president vs vice president )\n- good awareness of limitiations - by only comparing Trump, Biden, Bush, Obama with such strong presences it is limited into the generalizability, but it shows at least the potential of the method.",
            "1": "- a lot of important information for being able to assess the results yourself are now put in the appendices.",
            "2": "Particularly appendix A and B"
        },
        "GD8niu6AsU": {
            "0": "The study provides empirical evidence of substantial differences across four summarization models when analyzing the summaries of news articles involving different politicians.",
            "1": "The research lays the groundwork for further exploration of biases in summarization models, encouraging future research and discussion in this essential research area.",
            "2": "The authors acknowledge the importance of considering normative questions related to what constitutes an ideal summary.",
            "3": "They emphasize the need for responsible use of summarization models and provide a framework for assessing biases.",
            "4": "While the paper states that the term \"bias\" is used to refer to significant differences, it might be beneficial to clarify and define the specific type of bias being explored, to avoid misunderstandings or misinterpretations.",
            "5": "To strengthen the study's findings, the paper could include a comparative analysis with human-generated summaries to ascertain the extent of bias in automatic summarization models compared to human summarizers."
        },
        "470YXeQvAq": {
            "0": "The use of an entity replacement method to analyze the portrayal of politicians in automatically generated summaries is interesting and has intellectual merit.",
            "1": "The observation and conclusion it leads to, i.e.",
            "2": "\"summarization model biases towards learn different representations for different entities and provide consistently different summaries depending on the  entities involved.",
            "3": "\", presents an important and interesting research problem for the community to address.",
            "4": "In the paper's current form, there doesn't seem to be an definitive conclusion which will help future researchers understand \"what exactly we need to work on to improve summarization models/task definitions\".",
            "5": "IMO there are many open questions left hanging that could be answered with further experiments and analysis, which will greatly strengthen the overall presentation + claims made in the paper.",
            "6": "Here are some improvements that I would suggest:\n1.",
            "7": "With all summarization models, include what summarziation datasets they are trained on, as this directly governs model's behavior on what types of summary it will produce.",
            "8": "It would be much easier to read + understand your results that way.",
            "9": "Include analysis for more \"control\" variables, e.g.",
            "10": "does the source of a document matter for what kind of summaries models will produce?",
            "11": "e.g.",
            "12": "Do models tend to generate Trump less frequently than Biden in left-leaning vs. right-leaning sources?",
            "13": "Including analysis like this will strengthen your claim that the bias comes from \"summarization model\", not something else.",
            "14": "Here are some potential extensions + further questions for analysis that the authors can think about --\n1.",
            "15": "What might be the cause of the Trump vs. Biden bias from the model or summarization task perspective.",
            "16": "Again, from my intuition, the choice of training data for summarziation models will have a large impact on the results,  e.g.",
            "17": "most of the models mentioned in the paper are trained on data from more left-leaning news sources.",
            "18": "Generalize the analysis to include more entities -- This will potentially reveal what types of entities do model exhibit biases towards, and help us understand the systematic causes behind the phenomena."
        }
    },
    "RN5KLywTll": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical gap in the performance of vision-language (VL) models, specifically their struggle with spatial reasoning.",
            "1": "- The authors introduce three new benchmarks (What’sUp, COCO-spatial, and GQA-spatial) to isolate and evaluate the spatial reasoning capabilities of VL models.",
            "2": "- The study provides a comprehensive evaluation of 18 popular VL models, highlighting their poor performance on spatial reasoning tasks compared to human performance.",
            "3": "Potential reasons for acceptance\n   - The paper introduces novel benchmarks that are tightly controlled to specifically test spatial reasoning, which is a significant contribution to the field.",
            "4": "- The comprehensive evaluation of a wide range of VL models provides valuable insights into their limitations and areas for improvement.",
            "5": "- The study's findings on the inadequacies of current pre-training corpora and the ineffectiveness of basic modeling interventions are important for guiding future research.",
            "6": "- The release of the datasets and code promotes transparency and encourages further research in this area.",
            "7": "Potential reasons for rejection\n   - **Limited scale of benchmarks**:\n     - The What’sUp benchmark, while controlled, is restricted in scale compared to other benchmarks like ARO and GQA.",
            "8": "- The limited number of images (820 in What’sUp) may not be sufficient to generalize the findings across diverse real-world scenarios.",
            "9": "- **Insufficient improvement methods**:\n     - The paper's attempts to improve model performance through re-normalization of probabilities, better prompts, and finetuning did not yield significant improvements.",
            "10": "- The lack of substantial improvement methods may limit the practical impact of the study's findings.",
            "11": "- **Focus on spatial reasoning only**:\n     - The paper focuses solely on spatial reasoning, which, while important, is just one aspect of VL model capabilities.",
            "12": "- A broader study encompassing other types of reasoning could provide a more comprehensive understanding of VL model limitations.",
            "13": "Suggestions for improvement\n   - **Expand the scale of benchmarks**:\n     - Increase the number of images in the What’sUp benchmark to provide a more robust evaluation of spatial reasoning capabilities.",
            "14": "- Consider incorporating more diverse and complex scenarios to better reflect real-world applications.",
            "15": "- **Explore additional improvement methods**:\n     - Investigate more advanced modeling techniques, such as incorporating inductive biases or denser supervision, to enhance spatial reasoning capabilities.",
            "16": "- Explore the use of synthetic data generation to create more varied and challenging spatial reasoning tasks.",
            "17": "- **Broaden the scope of the study**:\n     - Extend the evaluation to include other types of reasoning, such as temporal reasoning, object-attribute association, and compositionality.",
            "18": "- Conduct a comprehensive analysis of how different types of reasoning interact and affect overall model performance.",
            "19": "- **Detailed analysis of failure cases**:\n     - Provide a more in-depth analysis of the specific failure cases observed in the models.",
            "20": "- Identify common patterns or errors that could inform targeted improvements in model training and architecture."
        },
        "KNlX0UoLDu": {
            "0": "- The proposed dataset, though relatively small, is a very useful resource to test models' ability to deal with spatial relationships.",
            "1": "I particularly value the controlled nature of the dataset, which prevents models from exploiting distribution/training biases.",
            "2": "I would just recommend the authors name it differently (see my point below).",
            "3": "- The set of models used for the experiments is very comprehensive and includes many different types of architectures.",
            "4": "- The results are extremely interesting, and highlight one key limitation of current L&V models -- spatial understanding -- that the community should take very seriously.",
            "5": "- It is not explained how human judgments on a sample of 100 data points were collected (if any): who are the annotators, how many per sample, how were they recruited, what was their native language, how much they were paid, what were the instructions, and what it means to compute a \"conservative accuracy\" on the task.",
            "6": "These are important details that should be given.",
            "7": "- Not a reason to reject, per se, but I don't think it is fair to name the new dataset RealCLEVR considering that it doesn't directly build on the actual CLEVR dataset (which contains images of geometrical objects with few basic features: color, texture, size, etc.).",
            "8": "In this sense, the name is misleading, and I would recommend the authors to consider using a different one.",
            "9": "- The paper lacks an insightful discussion on top of the various results: zero-shot, fine-tuning, and ablations.",
            "10": "What do the results implicate and what do they tell us about model abilities to deal with spatial prepositions and similar expressions?"
        },
        "S3pYvMUkxd": {
            "0": "- Interesting problem\n- Clear writing - Lack of novelty\n- Shallow literature review"
        },
        "yQXc44aw4g": {
            "0": "The contributions of this paper are substantial:\n\n* The number of tested VL models is astounding: 18!",
            "1": "* The authors have carefully introduced new data (dog on the table) to also capture rare spatial relations.",
            "2": "* Interesting analysis of LAION-2B which is used as a pretraining corpus for VLMs.",
            "3": "It is surprising that the strategies which come out of these analyses, do not work too well.",
            "4": "This is really good to know.",
            "5": "* I have to say that even after reading this paper, I now do not feel more informed about why VLMs fail at spatial reasoning than before.",
            "6": "It still seems that we have no idea of what goes on in these models and the combination of the training task and rarity of some prepositions in the training data are to blame, but the exploration of data-informed improvements did not succeed.",
            "7": "Why?",
            "8": "This makes the paper's impact onto the community less than I expected from title, abstract and introduction.",
            "9": "* The paper does not make very clear what previous work on spatial relations does and how it differs from it."
        }
    },
    "sCtJmxhvJe": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of gender bias in GPT-generated text, which is a significant concern given the increasing use of large language models (LLMs) in various applications.",
            "1": "- The creation of a novel dataset with fine-grained normative ratings of gender bias in GPT-generated text is a valuable contribution to the field.",
            "2": "- The use of Best-Worst Scaling (BWS) for annotation is innovative and provides a more nuanced understanding of gender bias compared to traditional binary classification methods.",
            "3": "Potential reasons for acceptance\n   - The paper introduces a novel dataset that can be used for further research on gender bias in LLMs.",
            "4": "- The use of BWS for annotation is a methodological advancement that can improve the reliability and granularity of bias detection.",
            "5": "- The analysis of the dataset provides valuable insights into the themes and patterns of gender bias in GPT-generated text.",
            "6": "- The benchmarking experiments with existing models and LLMs demonstrate the utility of the dataset and highlight areas for improvement in bias detection.",
            "7": "Potential reasons for rejection\n   - **Limited diversity of annotators:**\n     - The annotators were all from Microsoft Research India, which may introduce a geographical and cultural bias in the perception of gender bias.",
            "8": "- The lack of diversity in annotator demographics (e.g., gender, age, educational background) may limit the generalizability of the findings.",
            "9": "- **Potential bias in seed selection and prompting:**\n     - The seeds and in-context examples used for data generation were manually curated by the authors, which may introduce their own biases into the dataset.",
            "10": "- The paper does not provide a detailed analysis of how different seed types and prompting methods might influence the generation of biased text.",
            "11": "- **Limited scope of analysis:**\n     - The analysis focuses primarily on the distribution of scores and thematic analysis but does not delve deeply into the underlying causes of gender bias in GPT-generated text.",
            "12": "- The paper does not explore the potential impact of different training data and model architectures on the generation of biased text.",
            "13": "- **Flawed reasoning by GPT-4:**\n     - The qualitative analysis reveals that GPT-4's reasoning for its bias scores is often flawed, especially for less explicit biases.",
            "14": "- The paper does not propose concrete solutions to improve the reasoning capabilities of LLMs in detecting and explaining gender bias.",
            "15": "Suggestions for improvement\n   - **Increase diversity of annotators:**\n     - Recruit annotators from diverse geographical, cultural, and demographic backgrounds to ensure a more comprehensive understanding of gender bias.",
            "16": "- Consider including annotators with different levels of expertise in gender studies and NLP to capture a wider range of perspectives.",
            "17": "- **Enhance seed selection and prompting methods:**\n     - Provide a more detailed analysis of how different seed types and prompting methods influence the generation of biased text.",
            "18": "- Consider using a more systematic approach to seed selection, such as leveraging existing datasets or conducting a pilot study to identify effective seeds.",
            "19": "- **Expand scope of analysis:**\n     - Conduct a deeper analysis of the underlying causes of gender bias in GPT-generated text, including the impact of training data and model architectures.",
            "20": "- Explore the potential interactions between different types of biases (e.g., gender, race, age) and how they manifest in GPT-generated text.",
            "21": "- **Improve reasoning capabilities of LLMs:**\n     - Investigate methods to enhance the reasoning capabilities of LLMs in detecting and explaining gender bias, such as incorporating contextual information or using more sophisticated reasoning frameworks.",
            "22": "- Consider developing and evaluating new models or techniques specifically designed to improve the interpretability and accuracy of bias detection in LLMs."
        },
        "sM2FEOhbSv": {
            "0": "The authors highlight the importance of a nuanced approach to gender bias, noting that implicit gender bias can be subtle but is important to detect.",
            "1": "As such, the authors present a clear case for the benefit of having graded gender bias annotations.",
            "2": "They also clearly present the benefits of the BWS approach in terms of efficiency for annotating subjective judgements.",
            "3": "The authors analyse their generated data set to identify key themes using multiple methods, which allows them to draw some top-level conclusions about trends in the kind of biased content being produced.",
            "4": "Given the authors’ assertion that gender bias is highly subjective I would have liked to see more discussion on the method used to convert labels into scores.",
            "5": "Some implicit assumptions (that annotator gender has no impact on the importance of a judgement, that controversial statements with a mix of high and low ranks should receive a “middling” score) need to be made explicit and discussed.",
            "6": "On a related note, given the topic of the paper, I think it is a shame the authors did not engage with the literature on handling annotation disagreement for subjective labels - the only paper referenced that discusses how annotations are subjective is Blodgett et al.",
            "7": "2020, although the authors repeat this point in a number of places.",
            "8": "I would have liked to see the authors engage more with the limitations of using GPT models to generate biased text.",
            "9": "They acknowledge that the model is “limited by the dataset it has seen during training”, but the paper would benefit from more consideration of this point.",
            "10": "For example, it is likely that OpenAI “cleaned up” the training data seen by the model during training, or conducted alignment before release, meaning the range of explicitly gender biased sentences the model was exposed to was greatly reduced, limiting what it can produce.",
            "11": "It could have been interesting to do a qualitative analysis comparing the generated samples with sentences from a gender bias data set that was sampled from an online corpus (and in general, more qualitative analysis of the data set beyond identifying key words would have been beneficial)."
        },
        "ewsHOAACVA": {
            "0": "The authors provide a dataset for explicit rankings of the degrees of gender bias, expanding upon existing datasets for binary classification.",
            "1": "I believe that this is an important space to break into, as binary classification is very limiting and can constrain different viewpoints.",
            "2": "The dataset creation is analyzed thoroughly, from prompting methods to annotator agreement to sample bin themes.",
            "3": "The authors benchmarked the dataset across different types of models, including toxicity, gender bias, and, and offensive language.",
            "4": "One concern I have is in using a LLM to generate a new dataset.",
            "5": "This can drastically limit the ideas generated in a dataset and provide a constrained viewpoint in such a sensitive domain.",
            "6": "In addition to the sample generation, the limited diversity in annotators should be considered.",
            "7": "An ideal dataset would be diverse across various cultural opinions."
        },
        "qTU2Se7eRt": {
            "0": "LLM's gender bias responses were generated using a very carefully curated list of seeds, which the authors derived from existing related datasets (StereoSet and COPA).",
            "1": "The prompting methods are diverse and carefully chosen to ensure good coverage of different text types (conversation, completion, conversion).",
            "2": "The annotation process was carefully designed and monitored, using 20 annotators that were carefully instructed to do their job.",
            "3": "The quality and consistency of annotations is evaluated correctly.",
            "4": "The authors seem to know the related literature and resources very well, which they describe well and from which they choose the parts that best fit their research.",
            "5": "Theme identification is carried out in a rigorous manner, calculating PMI scores of unique words and sentences and an informed Dirichlet model and Convokit to identify meaningful n-grams.",
            "6": "The qualitative analysis complements well the plethora of qualitative results offered and offers further insights.",
            "7": "The authors make the annotated datasets freely available.",
            "8": "I can't think of any."
        }
    },
    "z1RYLqEpuP": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of attribution in cross-lingual question answering (QA) systems, which is essential for ensuring the trustworthiness and factual accuracy of generated answers.",
            "1": "- It introduces the XOR-AttriQA dataset, a novel resource for evaluating attribution in cross-lingual QA, covering five languages: Bengali, Finnish, Japanese, Russian, and Telugu.",
            "2": "- The study is the first to systematically evaluate and model attribution in cross-lingual QA, providing valuable insights and tools for improving the reliability of these systems.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a significant problem in the field of natural language processing, particularly in the context of cross-lingual QA, which has practical implications for users who speak low-resource languages.",
            "4": "- The introduction of the XOR-AttriQA dataset is a substantial contribution, as it provides a new benchmark for future research on attribution in cross-lingual settings.",
            "5": "- The experimental results demonstrate the effectiveness of using Natural Language Inference (NLI) models and large language models (PaLM 2) for detecting attribution, offering a promising direction for improving QA systems.",
            "6": "- The paper provides a comprehensive analysis of the shortcomings of current QA systems in terms of attribution and proposes practical solutions to address these issues.",
            "7": "Potential reasons for rejection\n   - **Limited scope of languages covered:**\n     - The study focuses on only five languages, which may not be representative of the full diversity of languages and their unique challenges in cross-lingual QA.",
            "8": "- The selection of languages might not include some of the most challenging low-resource languages, potentially limiting the generalizability of the findings.",
            "9": "- **Reliance on automatic translation:**\n     - The evaluation scenarios involve automatic translation, which can introduce errors and affect the accuracy of attribution assessments.",
            "10": "- The impact of translation quality on the results is not thoroughly analyzed, leaving questions about the robustness of the findings.",
            "11": "- **Small sample size for fine-tuning:**\n     - The fine-tuning experiments with PaLM 2 are based on a very small sample of 250 examples, which may not be sufficient to draw definitive conclusions about the model's performance.",
            "12": "- The paper does not explore the potential benefits of using larger datasets for fine-tuning, which could provide more reliable results.",
            "13": "- **Lack of comparison with other state-of-the-art models:**\n     - The paper primarily focuses on the CORA system and does not compare its findings with other state-of-the-art cross-lingual QA models.",
            "14": "- A broader comparison could provide a more comprehensive understanding of the attribution problem across different systems.",
            "15": "Suggestions for improvement\n   - **Expand the scope of languages:**\n     - Include a more diverse set of languages, particularly those that are known to be challenging for cross-lingual QA, to enhance the generalizability of the findings.",
            "16": "- Consider including languages from different language families and regions to cover a wider range of linguistic phenomena.",
            "17": "- **Analyze the impact of translation quality:**\n     - Conduct a detailed analysis of how translation errors affect attribution assessments and explore ways to mitigate these effects.",
            "18": "- Consider using human translations for a subset of the data to compare with automatic translations and assess the impact on attribution accuracy.",
            "19": "- **Increase the sample size for fine-tuning:**\n     - Use larger datasets for fine-tuning the attribution detection models to obtain more reliable and robust results.",
            "20": "- Explore the use of data augmentation techniques to create more training examples and improve model performance.",
            "21": "- **Broaden the comparison with other models:**\n     - Compare the attribution performance of the CORA system with other state-of-the-art cross-lingual QA models to provide a more comprehensive evaluation.",
            "22": "- Include a discussion of how different models handle attribution and identify common challenges and potential solutions.",
            "23": "- **Provide more detailed examples and case studies:**\n     - Include more examples and case studies to illustrate the types of attribution errors encountered and how the proposed models address these issues.",
            "24": "- Provide a qualitative analysis of the strengths and weaknesses of the attribution detection models based on real-world examples."
        },
        "hkBy9tf5QB": {
            "0": "Attribution is the most promising approach in addressing factual errors.",
            "1": "This paper finds that even the best XORQA system predictions lack attribution.",
            "2": "This paper proposed to use a large language model and natural language inference models to detect and rerank passages to improve the attribution-level of a state-of-the-art XORQA system.",
            "3": "I don't see a big reason to reject this paper."
        },
        "WqcbVJpVA6": {
            "0": "A novel dataset in a novel setup of cross-lingual question answering.",
            "1": "The evaluation of attribution has been done for only one model, although being popular but not the best in current time."
        },
        "jUo18HJPaB": {
            "0": "The paper is well written and easy to read.",
            "1": "It introduces attribution in cross-lingual QA.",
            "2": "In addition to presenting existing attribution problems in the current system, the paper also collects new annotations, proposes two methods for attribution classification and could further improve QA performance.",
            "3": "- It’s great to see the paper tries to improve explainability in QA systems by attributing answers to retrieved documents.",
            "4": "However, framing attribution detection as a binary classification problem does not provide further clarity compared to the ranking provided by the retriever.",
            "5": "- Domain is limited to wikipedia articles.",
            "6": "There’s a lack of discussion about how this work could be extended to other domains.",
            "7": "- Comparison between mT5–QA and mT5-NLI.",
            "8": "The amount of data used to finetune these two variants are significantly different.",
            "9": "I’m wondering if adding more QA data to the former, would that bring the gap closer?",
            "10": "Or training on a combination of QA and NLI data would further boost the performance?",
            "11": "- There’s no mention of the compute cost and the training details.",
            "12": "- The authors appear to have not invested much thought into the limitation section, as it does not discuss the paper's own limitations.l"
        }
    },
    "CgAfbI4kGS": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses an important and underexplored question: the impact of Knowledge Graph Completion (KGC) on downstream tasks, specifically Knowledge Graph Question Answering (KGQA).",
            "1": "- The introduction of the COMPLE QA benchmark is a novel contribution, providing a comprehensive dataset and evaluation framework to study this impact.",
            "2": "- The findings that the best-performing KGC methods do not necessarily lead to the best QA results are significant and challenge existing assumptions in the field.",
            "3": "Potential reasons for acceptance\n   - The paper introduces a novel benchmark (COMPLE QA) that fills a gap in the current research landscape by linking KGC performance to downstream KGQA performance.",
            "4": "- The experimental setup is robust, involving multiple KGC and KGQA methods, and the results are well-analyzed and presented.",
            "5": "- The findings have practical implications for the development of KGC methods, suggesting that these methods should be evaluated not just in isolation but also in terms of their impact on downstream tasks.",
            "6": "- The paper is well-written and structured, making it accessible to a broad audience in the field of knowledge graphs and natural language processing.",
            "7": "Potential reasons for rejection\n   - **Limited scope of downstream tasks:**\n     - The study focuses solely on KGQA as the downstream task, which, while important, is just one of many potential applications of KGC.",
            "8": "- The impact of KGC on other tasks such as recommendation systems, semantic search, or entity linking is not explored, which limits the generalizability of the findings.",
            "9": "- **Dependence on specific datasets and models:**\n     - The benchmark and experiments are based on a specific subset of Freebase and particular KGQA models (DecAF and Pangu), which may not represent the full diversity of real-world scenarios.",
            "10": "- The conclusions drawn might not hold for other knowledge graphs or QA models, raising questions about the broader applicability of the results.",
            "11": "- **Handling of incorrect triplets:**\n     - The paper acknowledges that incorrect triplets introduced by KGC methods can negatively impact QA performance, but the proposed solution of discarding incorrect triplets is not practical in real-world applications where ground truth is not available.",
            "12": "- More realistic strategies for mitigating the impact of incorrect triplets are not explored in depth.",
            "13": "Suggestions for improvement\n   - **Expand the scope of downstream tasks:**\n     - Include additional downstream tasks such as recommendation systems, semantic search, and entity linking in the benchmark to provide a more comprehensive evaluation of the impact of KGC.",
            "14": "- This would help in understanding whether the observed trends in KGQA hold for other applications as well.",
            "15": "- **Diversify datasets and models:**\n     - Test the benchmark on different knowledge graphs (e.g., Wikidata, DBpedia) and a wider range of KGQA models to ensure the findings are robust and generalizable.",
            "16": "- Include experiments with large language models and few-shot learning techniques to assess the impact of KGC in these contexts.",
            "17": "- **Develop practical strategies for handling incorrect triplets:**\n     - Propose and evaluate more realistic methods for mitigating the impact of incorrect triplets, such as confidence-based filtering, ensemble methods, or post-processing techniques.",
            "18": "- Investigate the trade-offs between precision and recall in KGC and their downstream effects on QA performance.",
            "19": "- **Provide more detailed analysis and insights:**\n     - Include a more detailed analysis of the types of errors made by different KGC methods and their specific impact on QA performance.",
            "20": "- Explore the reasons behind the observed discrepancies between KGC performance metrics (e.g., MRR, F1) and downstream QA performance, providing deeper insights into the underlying mechanisms."
        },
        "KOmEgITkwH": {
            "0": "The experimental results seem solid.",
            "1": "The writing is clear and easy to follow\nThe question is interesting and worth investigation 1.",
            "2": "The overall setting is weird to me.",
            "3": "The incompletion ratio is set to 20, 50, 80%.",
            "4": "however, how would one opt to such a KG system with such an extremely high incompletion ratio?",
            "5": "This is not realistic.",
            "6": "The incompletion in KG can sometimes lead to improvement in QA.",
            "7": "This is an interesting finding, but how would you support the QA system with this finding?",
            "8": "I did not see a deeper analysis of leveraging this and how to overcome the drawbacks."
        },
        "ADvxxb4nNW": {
            "0": "* The paper is well written and motivated, and it is easy to follow.",
            "1": "* The experimental design is sound and detailed.",
            "2": "* Interesting and novel task that has been overlooked in the literature.",
            "3": "Has potential on further defining the practical usefulness of KGC by influencing the way of evaluating of KGC in the context of practical applications.",
            "4": "The paper lacks analysis that would give insights on why better KGC performance does not align with better KGQA performance.",
            "5": "This includes quantitative analysis, e.g,.",
            "6": "the entity types / relations for which this phenomenon is more frequent, and qualitative analysis, e.g., questions that are anyway harder to answer even if the correct triples have been predicted by KGC.",
            "7": "Since this is a short paper, this could be included in the Appendix."
        },
        "bZhVg943r6": {
            "0": "A strength of this paper is its focus on grounding a long studied academic Graph task (KGC) in a long standing downstream NLP task (KGQA).",
            "1": "The intuition to ground a task in its benefits to other tasks gives weight to the importance of their proposal on application in this domain.",
            "2": "Additionally, the authors start from a widely cited and popular benchmark (GrailQA) that includes questions from IID, compositional, and zero-shot domains; this adds empirical weight to their findings because this is widely thought to be a high quality benchmark.",
            "3": "Lastly, the authors select a representative set of popular KGC methods.",
            "4": "In terms of writing, this paper is well written and easy to follow.",
            "5": "I think there are two reasons to consider rejecting this paper:\n1) The notion of incompleteness for their KGC task is somewhat artificial; the authors obtain relations of answer entities in dev/test set, and  \"incomplete\" them by obscuring one of the entities in the relation, and then \"randomly choose a proportion P of these triplets as the final validation and test sets\".",
            "6": "This challenge here is that not all triples of answer entities are equally meaningful for QA; i.e.",
            "7": "it is possible that some and probably likely that some, if not most, triples, are largely meaningless.",
            "8": "As a result, obscuring them would be expected to have little/no impact on QA.",
            "9": "This risks significantly diluting the correlation between KGC quality and downstream QA.",
            "10": "Specifically, a KGC model might look great because it fills in lots of relations but then look bad for QA because some large percent of those relations were meaningless.",
            "11": "This can potentially mislead the reader into thinking that a particular KGC model is strong on KGC but bad for QA, when in fact the measurement of KGC model strength on this benchmark may just be an artifact of the benchmark.",
            "12": "I think this can be improved by applying stratified sampling over predicates to ensure that we are controlling not just for the entities we obscure but also for relative importance of the information the KGC model completes.",
            "13": "Further motivating the need for a deeper look into this approach is Appendix D, which seems to indicate a weak correlation between KGC and KGQA.",
            "14": "It would be interesting to see the set of questions for which there is little to no correlation, as this is where the most opportunity is to improve this benchmark.",
            "15": "Ultimately I view this as something that dilutes, but does not invalidate, the conclusions.",
            "16": "There is a correlation, and that correlation needs to be explored further, but unfortunately the authors don't dig into the link between KGC and KGQA much to explain this correlation.",
            "17": "2) Modeling techniques studied; the authors study two KGQA techniques, Pangu and DecAF.",
            "18": "While both exhibit strong performance, these are not the most natural nor most representative baseline choices.",
            "19": "Neither of these techniques are popular or widely cited; DecAF is somewhat popular and adopts an IR approach to KGQA, in which documents of triples are retrieved as evidence to inform answer generation, while Pangu is not and employs a neurosymbolic planner/critic pipeline.",
            "20": "This paper would be made more sound by the inclusion of more popular and representative techniques, especially those of the semantic parsing variety, of which there are many popular approaches (citations below).",
            "21": "The lack of semantic parsing baseline is concerning for a few reasons, including that 1) these are the most widely studied approach to KGQA and 2) semantic parsing provides a more direct means to assess KGC.",
            "22": "W/r/t 2) specifically, these approaches would enable the authors to measure the benefit of KGC on generated parse success rate.",
            "23": "For example, using a parsing approach trained on the complete KG and evaluated on the incomplete one would provide a realistic measurement of the impact of missing data, as parses that seek to return the obscured data would no longer be executable.",
            "24": "The authors should give more justification behind why they chose DecAF and Pangu, beyond just \"here are 2 SOTA techniques\".",
            "25": "While they discuss the lack of an LLM-only baseline in the limitation, they don't include semantic parsing.",
            "26": "@article{Ye2021RNGKBQAGA,\n  title={RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering},\n  author={Xi Ye and Semih Yavuz and Kazuma Hashimoto and Yingbo Zhou and Caiming Xiong},\n  journal={ArXiv},\n  year={2021},\n  volume={abs/2109.08678},\n  url={https://api.semanticscholar.org/CorpusID:237562927}\n}\n\n@inproceedings{Abdelaziz2021ASP,\n  title={A Semantic Parsing and Reasoning-Based Approach to Knowledge Base Question Answering},\n  author={I. Abdelaziz and Srinivas Ravishankar and Pavan Kapanipathi and Salim Roukos and Alexander G. Gray},\n  booktitle={AAAI Conference on Artificial Intelligence},\n  year={2021},\n  url={https://api.semanticscholar.org/CorpusID:235363625}\n}"
        },
        "2abDelRfFD": {
            "0": "- Interesting research area\n- Combination of results from different domains (QA, embedddings)\n - No major scientific contribution\n- No surprising insights\n- Results achieved on one dataset"
        }
    },
    "lCy3RwscMn": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper presents a novel method for breaking down complex tasks into simpler binary sub-tasks, which are then used to generate interpretable features for machine learning models.",
            "1": "- The approach leverages the zero-shot capabilities of large language models (LLMs) to generate weak labels for training smaller transformer models, which can then be used for zero-shot inference on new binary questions.",
            "2": "- The method aims to combine the strengths of LLMs and interpretable machine learning models, such as decision trees, to achieve high performance and interpretability.",
            "3": "Potential reasons for acceptance\n   - The proposed method addresses a significant challenge in AI and machine learning: the need for interpretable models that can handle complex tasks.",
            "4": "- The approach is innovative in its use of LLMs to generate weak labels for training smaller models, which can then be used for zero-shot inference.",
            "5": "- The paper demonstrates the effectiveness of the method on two different tasks, showing that it can outperform state-of-the-art LLMs in some cases.",
            "6": "- The method is computationally efficient compared to other techniques like chain-of-thought prompting.",
            "7": "- The paper provides a detailed explanation of the methodology, including the generation of binary sub-tasks, training of the transformer model, and integration with interpretable models.",
            "8": "Potential reasons for rejection\n   - **Limited generalization to other tasks:**\n     - The paper demonstrates the method on only two tasks, which may not be sufficient to prove its generalizability to a wide range of applications.",
            "9": "- The tasks chosen (incoherent answer detection and scientific abstract classification) may not represent the full spectrum of complex tasks that the method could be applied to.",
            "10": "- **Performance of NLLF alone:**\n     - The performance of the NLLF-enhanced models alone was not exceptional, which may raise concerns about the robustness of the method.",
            "11": "- The reliance on expert features to achieve high performance may limit the applicability of the method in domains where expert features are not readily available.",
            "12": "- **Complexity of the method:**\n     - The method involves multiple steps, including the generation of binary sub-tasks, weak labeling, training of a transformer model, and feature selection, which may be seen as overly complex.",
            "13": "- The need for manual grouping of similar questions and feature selection using a genetic algorithm adds to the complexity and may limit the method's scalability.",
            "14": "- **Evaluation metrics and comparison:**\n     - The evaluation metrics used (precision, recall, F1-score) may not fully capture the benefits of the method, especially in terms of interpretability.",
            "15": "- The comparison with other methods, such as chain-of-thought prompting, may not be comprehensive enough to highlight the advantages of the proposed approach.",
            "16": "Suggestions for improvement\n   - **Expand the evaluation to more tasks:**\n     - Demonstrate the method on a wider range of tasks to show its generalizability and robustness.",
            "17": "- Include tasks from different domains to highlight the versatility of the approach.",
            "18": "- **Improve the performance of NLLF alone:**\n     - Investigate ways to enhance the performance of the NLLF-enhanced models without relying heavily on expert features.",
            "19": "- Explore alternative methods for generating and selecting binary sub-tasks to improve the quality of the NLLF.",
            "20": "- **Simplify the methodology:**\n     - Streamline the process of generating binary sub-tasks and weak labels to reduce the complexity of the method.",
            "21": "- Consider automating the manual grouping of similar questions and feature selection to improve scalability.",
            "22": "- **Enhance the evaluation and comparison:**\n     - Use additional evaluation metrics, such as interpretability scores, to better capture the benefits of the method.",
            "23": "- Provide a more comprehensive comparison with other state-of-the-art methods, including a detailed analysis of the trade-offs between performance and interpretability."
        },
        "qUsat9IDoL": {
            "0": "The idea is very novel.",
            "1": "The idea is very interesting in terms of decomposing tasks into sub-tasks.",
            "2": "The proposed method is somewhat complicated to apply to real-world scenarios.",
            "3": "While it presents innovative concepts and potential benefits, its implementation may pose challenges for practical use.",
            "4": "The intricate nature of the approach demands a high level of expertise and resources, which could limit its adoption in real-life applications."
        },
        "VRMtgQ5gt9": {
            "0": "It is an interesting work towards interpretable prediction.",
            "1": "The authors propose to utilize LLM to extract attributes and use the data generated by LLM to train BERT-like model to obtain Natural Language Learned Feature (NLLF).",
            "2": "NLLF and other man-crafted features are used in the decision tree which has more explainability than the black-box models.",
            "3": "The experimental results are promising which outperform the sota models (ChatGPT with/without CoT and BERT-like model).",
            "4": "- The authors only conduct experiments on 2 binary datasets: Scientific Abstract Classification (SAC) and Incoherent Answer Detection (IAD).",
            "5": "These two tasks are simple and, in my view, not high-stake enough.",
            "6": "It is still doubt that how the framework works on other difficult tasks.",
            "7": "- The paper introduces explainable AI in the introduction; however, the topic of the paper is interpretable methods.",
            "8": "The authors do not explain the definition of interpretability and do not explain the difference between these two concepts.",
            "9": "The authors also do not provide the work in this line.",
            "10": "- In the experiment, the only comparison model with explainability is ChatGPT with CoT.",
            "11": "It is better to see at least one more explainable model.",
            "12": "Also, self-ask* performs better in high-stake questions.",
            "13": "It would be better if there are some comparisons with self-ask.",
            "14": "*: Press, Ofir, et al.",
            "15": "\"Measuring and narrowing the compositionality gap in language models.\"",
            "16": "arXiv preprint arXiv:2210.03350 (2022)."
        },
        "WFVZhSYA70": {
            "0": "+ The idea of breaking the main task into intermediatory sub tasks is quite interesting and can be helpful for the further research.",
            "1": "+ The visualizations and diagrams are quite expressive.",
            "2": "- The experiments conducted didn't involve well-known baselines.",
            "3": "- The proposed algorithm also takes time and needs computational resources for huge datasets.",
            "4": "This point should be mentioned in the limitations section."
        }
    },
    "dFlGP1l65l": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the phenomenon of media storms, which are significant due to their impact on public discourse and democratic debate.",
            "1": "- The novelty lies in the development and application of a pairwise article similarity model to identify and analyze media storms at a granular level, which has not been extensively done before.",
            "2": "Potential reasons for acceptance\n   - The paper introduces a state-of-the-art article embedding model that enables fast and accurate comparison of news articles, which is a valuable contribution to the field.",
            "3": "- The creation and public release of a comprehensive dataset of media storms over a nearly two-year period provide a valuable resource for future research.",
            "4": "- The empirical validation of claims about storm evolution, topical distribution, and intermedia agenda setting adds to the existing body of knowledge in media and communications theory.",
            "5": "Potential reasons for rejection\n   - **Scalability of the clustering pipeline**\n     - The current approach considers pairwise similarity between articles with named entity pairs having under 20,000 occurrences, which might exclude some of the largest news storms.",
            "6": "- The cutoff of 20,000 occurrences for named entities might unintentionally exclude significant events, such as the death of George Floyd, which could limit the comprehensiveness of the analysis.",
            "7": "- **Reliance on existing datasets**\n     - The study relies on three existing datasets of news articles, which may not be representative of any one news ecosystem but rather capture a broad sample of U.S. national and local media outlets with some international coverage.",
            "8": "- The datasets were collected from RSS feeds, meaning there is limited knowledge on the completeness of coverage, which could affect the accuracy of the results.",
            "9": "- **Correlational nature of the analysis**\n     - The results on the development of media storms are correlational and do not assess the causal mechanisms behind media storms, which limits the ability to draw definitive conclusions about the causes of media storms.",
            "10": "- The self-referential nature of the news production process makes it difficult to locate causal mechanisms, and the study does not address this challenge adequately.",
            "11": "Suggestions for improvement\n   - **Enhance scalability and comprehensiveness**\n     - Consider alternative methods to improve the scalability of the clustering pipeline, such as more sophisticated filtering techniques or parallel processing, to ensure that significant events are not excluded.",
            "12": "- Re-evaluate the cutoff for named entity occurrences to include more comprehensive coverage of large news storms.",
            "13": "- **Address dataset limitations**\n     - Supplement the existing datasets with additional sources to ensure a more representative sample of the news ecosystem, including more international coverage.",
            "14": "- Investigate the completeness of the RSS feed data and consider alternative data collection methods to ensure more comprehensive coverage.",
            "15": "- **Incorporate causal analysis**\n     - Design and implement studies that can test the causal mechanisms behind media storms, such as matched pairs of media storms with comparable non-storms or natural experiments based on unanticipated real-world events.",
            "16": "- Explore the interactions between traditional media, social media, and political elites to understand their influence on the trajectory of media storms.",
            "17": "- **Improve the news similarity model**\n     - Develop a faster article matching approach to allow for analysis of a larger, global media ecosystem.",
            "18": "- Enhance the model's accuracy in distinguishing between articles about different events, especially those generated from templates, to improve the reliability of the analysis."
        },
        "sd6I19M3HB": {
            "0": "- This paper is an explicit study on media storms.",
            "1": "I agree with the authors that the analysis of media storms is interesting and that the community can gain lots of insight from them.",
            "2": "- While the authors didn’t use a large amount of data to train their article similarity model, they ran it on a large number of articles to create a large dataset.",
            "3": "This dataset can be useful for future work to study media storms.",
            "4": "- The analysis done by the authors confirms what readers expect should happen in media storms.",
            "5": "However, the fact that this happens over the authors large analysis confirms that the dataset the authors are proposing is solid.",
            "6": "- The paper is well written and easy to follow.",
            "7": "- The main thing I would have liked to see in this paper is some results using the media storms data for a downstream task.",
            "8": "Some of these could also be tasks the authors propose, like identifying trends in future media storms based on historical media storms.",
            "9": "- Instead, this paper mostly just focuses on analyzing the storms (and some of the results are not surprising given the definition of media storms, like the fact that topic coverage increases).",
            "10": "It would be cool if the authors also use the storms data for other tasks, or at least discuss how it can be used.",
            "11": "Here are some examples I can think of:\n        - Can we predict when a media storm is about to happen?",
            "12": "When it’s going to end?",
            "13": "How long is it going to be?",
            "14": "- Can we predict which users are likely to be involved in a media storm?",
            "15": "Sources?",
            "16": "How does their perspectives change over time?",
            "17": "- Can we predict which outlets are likely to lead the coverage?",
            "18": "During a storm, can we determine this based on past storm data?",
            "19": "- Do some media storms cover more factual information and others spread more fake news?",
            "20": "When does each tend to happen?",
            "21": "- This can be useful if we know a media storm is happening, then maybe we should analyze the data around the media storm more carefully.",
            "22": "- Apart from the above, the main contribution of this paper is a dataset and an analysis of it, which in my opinion doesn't make the contribution of the paper extremely strong, thus my ambivalent Excitement score."
        },
        "Lume5E3gtr": {
            "0": "A. In-depth qualitative and quantitative analysis of media storms, what they are, their implications and how they come into being.",
            "1": "While popularity clusters and temporal evolution have been studied in depth with respect to social media and news coverage, outbursts are still significant aspects to study for understanding significance of events.",
            "2": "This study, dataset and modeling approach have the potential to serve as good, improved baselines for research of this kind.",
            "3": "By selecting the first 288 tokens and the last 96 tokens, how are the authors ensuring the true semantic meaning of the articles is being covered rather than context and linguistic fluff use to build up a news article and conclude it respectively?",
            "4": "A lot of articles consist of a premise and conclusion which can possibly consist of other related events or anecdotal pieces etc, that might not be relevant to the exact topic at hand.",
            "5": "Whereas, some articles are to the point.",
            "6": "When trying to pair such articles with a VERY high threshold of 0.9, there is a strong possibility of missed correlations and missed article linkup, which can possibly alter results (even if slightly).",
            "7": "It'll be good to have some more depth about the data in terms of average length of articles when data is truncated etc., and how these scenarios are being handled in the study."
        },
        "mRJciXj37D": {
            "0": "There is a lot of work done on understanding media storms which agrees with existing knowledge of media storms\n2.",
            "1": "There is a detailed recipe on how to extract the storms from the news articles 1.",
            "2": "There is no novel method\n2.",
            "3": "There is no novel conclusion or observation\n3.",
            "4": "Many conclusions are just one of the possible explanations - I suggest authors to focus on finding experiments to show that conclusion they think is more likely is the right conclusion"
        }
    },
    "I8VTNsq5eB": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces CESAR, a novel framework for automatically generating compositional instructions for multi-turn dialog tasks.",
            "1": "- CESAR addresses the gap in current LLMs' performance when handling complex instructions with multiple constraints.",
            "2": "- The framework is applied to enhance the InstructDial benchmark, resulting in InstructDial++, which includes a significantly larger and more diverse set of dialog tasks.",
            "3": "Potential reasons for acceptance\n   - The introduction of CESAR provides a scalable and automated method for generating complex dialog tasks, which is a significant advancement in the field.",
            "4": "- The enhancement of the InstructDial benchmark to InstructDial++ with a larger dataset and more tasks demonstrates the practical applicability and scalability of CESAR.",
            "5": "- The experimental results show that models trained with CESAR-generated tasks outperform those trained with traditional methods, indicating the effectiveness of the framework.",
            "6": "Potential reasons for rejection\n   - **Lack of detailed comparison with other state-of-the-art methods:**\n     - The paper could benefit from a more comprehensive comparison with other recent instruction tuning benchmarks beyond the ones mentioned.",
            "7": "- It would be helpful to see a direct comparison of CESAR's performance against other frameworks in a broader range of dialog tasks.",
            "8": "- **Limited exploration of negative conditions and non-dialog data:**\n     - The paper acknowledges that it has not explored negative conditions, which could be a significant limitation in understanding the full potential of CESAR.",
            "9": "- The exclusion of non-dialog data in the multi-tasking setup might limit the generalizability of the findings.",
            "10": "- **Potential over-reliance on structured prompts:**\n     - The structured format of CESAR prompts, while beneficial for scalability, might not translate well to natural human queries.",
            "11": "- The paper should address how CESAR-trained models perform with more naturally phrased prompts and whether additional training or fine-tuning is required.",
            "12": "Suggestions for improvement\n   - **Expand the comparison with other benchmarks:**\n     - Include a more detailed comparison with other state-of-the-art instruction tuning benchmarks, highlighting CESAR's advantages and potential areas for improvement.",
            "13": "- **Explore negative conditions and non-dialog data:**\n     - Conduct experiments that incorporate negative conditions to understand their impact on CESAR's performance.",
            "14": "- Include non-dialog datasets in the multi-tasking setup to evaluate the framework's generalizability across different types of data.",
            "15": "- **Evaluate performance with natural prompts:**\n     - Test CESAR-trained models with naturally phrased prompts to assess their performance in real-world scenarios.",
            "16": "- Consider incorporating a conversion mechanism from structured to natural prompts within the framework to enhance its applicability.",
            "17": "- **Provide more qualitative examples:**\n     - Include additional qualitative examples of CESAR-generated tasks and their outputs to illustrate the framework's capabilities and limitations.",
            "18": "- Highlight specific cases where CESAR outperforms traditional methods and discuss the underlying reasons for these improvements."
        },
        "9GtGslfs8T": {
            "0": "1) The paper introduces a novel framework, CESAR, which addresses the challenge of generating complex instructions for dialog tasks.",
            "1": "2) CESAR unifies a large number of dialog tasks in the same format and allows programmatic induction of complex instructions without any manual effort.",
            "2": "3) The paper enhances the InstructDial benchmark with new datasets and tasks and demonstrates the scalability of CESAR in providing rich instructions.",
            "3": "1) The paper lacks a detailed explanation of the CESAR framework, making it difficult for readers to fully understand the methodology.",
            "4": "2) The evaluation section could benefit from more detailed analysis and comparisons with existing methods or baselines.",
            "5": "3) The paper does not provide implementation details or code availability, which could hinder reproducibility."
        },
        "TWz8GkCHPD": {
            "0": "The paper proposed a new form to represent compositional tasks.",
            "1": "The paper created a new benchmark InstructDial++ to evaluate model's capability on compositional tasks.",
            "2": "Firstly, I suspect that the assumption of this paper is wrong.",
            "3": "Are open source models really less capable of handling compositional tasks?",
            "4": "I did not find any evaluation on current state-of-the-art Llama models in the paper.",
            "5": "Secondly, I saw in this paper (Table 7) that ChatGPT does not perform well on author's generated compositional task compared to the finetuned model (Table 5).",
            "6": "It is likely that the author created an \"artful\" dataset."
        },
        "Y7mNHijUIB": {
            "0": "The paper introduces a unique framework that merges various 1-D atomic tasks, I think the motivation is clear and sound.",
            "1": "The established InstructDial benchmark is expanded with an addition of 68 novel downstream tasks.",
            "2": "While the paper introduces 68 new downstream tasks based on seven 2-D Cesar tasks, it mainly offers overall performance.",
            "3": "This obscures the individual quality and contribution of each task.",
            "4": "There's a possibility that only a few high-quality tasks drive the improvements, while the majority add little value.",
            "5": "The methodological clarity regarding CESAR's task combination into coherent instruction prompts remains ambiguous.",
            "6": "It's essential to discern the reliability of these instructions and ensure task diversity, preventing overlapping or overly similar tasks.",
            "7": "The paper doesn't sufficiently differentiate itself from FLAN-T5, which already encompasses several dialogue tasks and also detailed task design.",
            "8": "It would great if the paper could discuss the differences.",
            "9": "The generalization experiments of CESAR-FLAN-xxl, initialized from FLAN-xxl, are somewhat undermined.",
            "10": "Given that FLAN-xxl already integrates around 400 datasets and over 1.5k tasks, and InstructDial also introduces several tasks for each dataset.",
            "11": "The paper's claim of \"unseen generalization\" isn't wholly convincing unless both datasets and tasks from FLAN-xxl and InstructDial are explicitly excluded from evaluation.",
            "12": "Besides, it may also leak data information for seen domains if different tasks come from the same datasets."
        },
        "BTcyqGtLuL": {
            "0": "The proposed unified framework presents a novel idea.",
            "1": "The motivation and details of the approach have been described in great detail.",
            "2": "Abundant experiments have validated the authors' method.",
            "3": "I haven't identified any significant shortcomings of the paper."
        },
        "HcIaomlZql": {
            "0": "The paper is generally well written and well structured.",
            "1": "Although the introduction and description of CESAR were somewhat challenging to follow due to many new terms being introduced, the accompanying tables and figures helped in following along.",
            "2": "The authors have identified a critical gap between performance of closed and open-source LLMs in task-oriented dialogues and clearly delineated their motivations with preliminary experiments to support their hypothesis that compositional instructions can help improve performance on complex dialog tasks.",
            "3": "The CESAR framework is well defined and designed to be scalable across different tasks within task-oriented dialogue.",
            "4": "They have explained how n-D tasks are defined, how discriminative tasks can be incorporated and how they ensure order-invariance while linearizing their inputs\n\nThe InstructDial++ dataset is a meaningful contribution (the authors have committed to sharing the dataset on Github) and results from Table 3 - 4 indicate that fine-tuning FLAN model on InstructDial++ shows performance gains over a FLAN model trained on InstructDial\n\nThe quantitative results indicate that CESAR-FLAN performs better than existing approaches on a good number of atomic tasks and seems to out-perform other approaches in compositional tasks.",
            "5": "One of the primary motivations of the paper is to bridge the gap between closed LLMs and open LLMs, hence there should have been more comparison and evaluation between CESAR and GPT-3.5-turbo in the main paper.",
            "6": "The authors have tabulated some results in the appendix section and they have carefully analyzed why the numbers might be deceiving, however it would be better to show several qualitative examples across CESAR and GPT-3.5-turbo and analyze trends in performance, it not clear whether the gap has been bridged or not\n\nIn L213 it is stated that scalable generation of compositional tasks is one aspect that CESAR aims to solve, however it is not clear in the paper how the compositional tasks are generated in a  scalable fashion using CESAR"
        }
    },
    "cOxL1tlSQw": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the limitations of traditional stance detection by introducing a dynamic stance detection approach.",
            "1": "- It presents a new annotation scheme and creates a multilingual corpus (DySC) in Catalan and Dutch, which is significant for non-English NLP research.",
            "2": "- The study empirically demonstrates the differences between static and dynamic stance detection, highlighting the potential for better generalization across topics and languages.",
            "3": "Potential reasons for acceptance\n   - The introduction of a novel dynamic stance detection task and annotation scheme is a significant contribution to the field.",
            "4": "- The creation of a new multilingual corpus (DySC) in Catalan and Dutch adds valuable resources for NLP research in under-resourced languages.",
            "5": "- The empirical analysis and experiments provide strong evidence supporting the advantages of dynamic stance detection over static stance detection.",
            "6": "- The paper includes detailed methodology and thorough experiments, enhancing the reproducibility and reliability of the results.",
            "7": "Potential reasons for rejection\n   - **Limited scope of data collection:**\n     - The data collection is based on a manually-curated selection of keywords, which may introduce bias and limit the exhaustiveness of the dataset.",
            "8": "- The reliance on specific online platforms (Twitter and Racó Català) may not capture the full diversity of online discussions.",
            "9": "- **Annotation challenges:**\n     - The inter-annotator agreement (Cohen’s kappa) for dynamic stance detection is relatively low (0.52-0.58), indicating potential difficulties in achieving consistent annotations.",
            "10": "- The complexity of the dynamic stance annotation process may limit its scalability and applicability to larger datasets.",
            "11": "- **Model performance:**\n     - The performance of the models, especially in cross-lingual and zero-shot topic scenarios, leaves room for improvement, with macro F1 scores ranging from 0.19 to 0.65.",
            "12": "- The merging of fine-grained labels for the experiments may oversimplify the task and obscure the nuances captured by the original annotation scheme.",
            "13": "Suggestions for improvement\n   - **Expand data collection:**\n     - Consider using more diverse and automated methods for data collection, such as in-context learning approaches, to reduce potential bias and increase the coverage of relevant messages.",
            "14": "- Include additional online platforms and forums to capture a wider range of discussions and interactions.",
            "15": "- **Enhance annotation process:**\n     - Provide more detailed guidelines and training for annotators to improve inter-annotator agreement and consistency.",
            "16": "- Explore semi-automated or crowd-sourced annotation methods to scale up the annotation process while maintaining quality.",
            "17": "- **Improve model performance:**\n     - Experiment with more advanced and specialized pre-trained models, such as those fine-tuned on similar tasks or languages, to enhance performance.",
            "18": "- Investigate the use of multi-task learning or transfer learning techniques to leverage related tasks and improve generalization across topics and languages.",
            "19": "- Consider retaining the fine-grained labels in the experiments and developing models capable of handling the full complexity of the dynamic stance annotation scheme."
        },
        "Jj4OWEUPbR": {
            "0": "The paper presents a new way of modeling the tasks of stance detection dynamically instead of statically.",
            "1": "I see this as a promising new way of framing the task.",
            "2": "Together with the new task, the paper presents a new resource in Catalan and Dutch annotated for static and dynamic stance detection.",
            "3": "This is a rich new dataset that allows for analyzing static and dynamic stance annotations.",
            "4": "The paper presents initial experiments with language models that give valuable first insights into the generalizability of the two annotation frameworks.",
            "5": "I did not see real reasons to reject the paper.",
            "6": "There are, however, a couple of points that should be clarified: \n\nAnnotation analysis: I am really not so sure about the comparison of static versus dynamic stance annotations and their consequences for the data (Section 4.2).",
            "7": "The annotations resulting from both annotation frameworks are considered side by side and it is discussed what aspects of a debate they can capture.",
            "8": "My problem with this comparison is that the two annotation frameworks essentially capture a different task; one (static) assesses a text with respect to a claim or topic (predefined, explicit) while the other (dynamic) assesses a text with respect to another text.",
            "9": "In the latter, the claim in the first text remains entirely implicit.",
            "10": "While the overall topic of a discussion may be related to the topic of covid vaccines, the first text may contain a claim that is entirely unrelated.",
            "11": "This is not really an insight that arises from the data.",
            "12": "Rather, it is inherent to the design of the two different frameworks.",
            "13": "I think it is valid to show the consequences of these differences in the data, but I think it is important to be clear about the fact that the frameworks have different goals and thus capture different information.",
            "14": "Update after rebuttal: Thank you for the clarification.",
            "15": "I am confident that the authors can make the necessary changes in the final version of the paper.",
            "16": "Automatic stance detection: Please clarify whether the train and test splits for dynamic and static stance detection are the same.",
            "17": "I understand that there is a difference in the number of instances (and labels) due to the different annotation frameworks, but right now, it is not clear whether the texts are the same.",
            "18": "It would be great to have a bit more information about topic overlap between train and text in the ‘all topics experiments’.",
            "19": "Update after rebuttal: Thank you for the clarification!",
            "20": "Topic independence is a huge plus of the dynamic way of modeling stance.",
            "21": "I think it would be good to make this (and its consequence for automatic approaches) more prominent in the framing of the paper.",
            "22": "Minor remark: I don’t think that Dutch can really be called a low-resource language with respect to NLP resources.",
            "23": "‘Low resource’ may be true with respect to stance resources, but not in terms of general NLP resources (see for instance Joshi et al.",
            "24": "2020).",
            "25": "Perhaps check this.",
            "26": "Reference\nJoshi, P., Santy, S., Budhiraja, A., Bali, K. and Choudhury, M., 2020, July.",
            "27": "The State and Fate of Linguistic Diversity and Inclusion in the NLP World.",
            "28": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp.",
            "29": "6282-6293)."
        },
        "6IZtpD28UH": {
            "0": "- Useful resource: The paper presents an annotated dataset in two languages and on several topics, while ensuring cross-lingual comparability.",
            "1": "- Baseline results: The paper presents basic experiments, including on cross-topic and cross-lingual transfer, that will be useful as a reference for future work.",
            "2": "Unfortunately, the presentation of the research questions and the results is not very clear, and especially the relation to “static” stance detection remains unclear: Does this paper propose an improvement over previous approaches, or does it propose an entirely new task?",
            "3": "- Unclear presentation: The experiments with automatic stance detection (Section 5) focus on a comparison between “Static Training” and “Dynamic Training”.",
            "4": "However, the two concepts are not formally defined.",
            "5": "Is “Dynamic Training” multi-task learning on dynamic and static stance detection?",
            "6": "If so, the use of different subsets used in the experiments (L496–501) make it difficult to see whether this objective has a positive effect.",
            "7": "Or is “Dynamic Training” just learning to classify the relation to the parent text, independent of the static stance?",
            "8": "In that case, it is not clear in what respect this approach “address[es] the limitations of traditional Static Stance detection” (L569), since it would appear to be a new task.",
            "9": "- Claim not supported by example: A main claim of the paper (L420f.)",
            "10": "is that static stance and dynamic stance are not equal: Two statements can have the same stance w.r.t a topic but still disagree with each other.",
            "11": "While Table 4 provides quantitative support for this claim, Example 1 (L437–446) does not seem to be a valid illustration.",
            "12": "In fact, Reply (b) seems to be critical of the COVID-19 vaccination campaign, and not in favor.",
            "13": "I would encourage the authors to check the example and make sure it does not contain an annotation error."
        },
        "nSuGYP1bXp": {
            "0": "There is merit in the fact that the authors annotate both static and dynamic stance in order to show the relations between the two levels of analysis.",
            "1": "I also like the crosslinguistic approach in Catalan and Dutch.",
            "2": "The paper has a couple of gaps that I address below in the more detailed comments.",
            "3": "Overall I think this is rather an LREC paper, given its focus."
        }
    },
    "jQozdfjJSZ": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces MingOfficial, a large-scale multi-modal dataset focusing on officials from China's Ming Dynasty, which is a significant contribution to Chinese historical research.",
            "1": "- The integration of structured career records and textual historical data using a graph neural network (GNN) to study social structures and boost downstream tasks is novel and innovative.",
            "2": "- The dataset and the proposed framework have the potential to stimulate further research into the role of social context and representation learning in identifying individual characteristics in historical studies.",
            "3": "Potential reasons for acceptance\n   - The dataset is comprehensive, covering 13,031 officials and 69,688 paragraphs from historical texts, providing a rich resource for researchers.",
            "4": "- The use of GNN to combine multi-modal data and enhance the identification of nuanced characteristics is a strong methodological contribution.",
            "5": "- The significant improvement in performance for identifying nuanced identities, as demonstrated by the experiments, showcases the practical utility of the proposed approach.",
            "6": "- The public availability of the dataset and the interactive tool can facilitate further research and applications in various fields beyond Chinese studies.",
            "7": "Potential reasons for rejection\n   - **Limited interpretability of GNN models**\n     - The specifics of how the model draws predictions from the data remain unclear.",
            "8": "- A more transparent understanding of which career records or official relations significantly contribute to the model’s predictions would greatly aid researchers.",
            "9": "- **Dependence on the quality and scope of data**\n     - The dataset might contain inaccuracies and omissions that could affect the results.",
            "10": "- The data is confined to the Ming Dynasty, limiting the generalizability of the findings to other historical periods or cultures.",
            "11": "- **Scalability and generalizability concerns**\n     - The performance of the approach with significantly larger datasets or different time frames remains uncertain.",
            "12": "- The method has not been contrasted with other potential methods for identifying target groups, limiting the comparative analysis.",
            "13": "- **Resource-intensive annotation process**\n     - The process for making reliable annotations is time-consuming and requires significant expertise, which might limit the scalability of the approach.",
            "14": "Suggestions for improvement\n   - **Enhance model interpretability**\n     - Develop methods to provide more transparent insights into how the GNN models make predictions.",
            "15": "- Identify and highlight the specific career records or official relations that significantly contribute to the model’s predictions.",
            "16": "- **Expand the dataset and improve data quality**\n     - Include additional historical periods or cultures to enhance the generalizability of the findings.",
            "17": "- Implement methods to verify and correct inaccuracies and omissions in the dataset.",
            "18": "- **Test scalability and generalizability**\n     - Conduct experiments with larger datasets and different time frames to test the scalability of the approach.",
            "19": "- Compare the proposed method with other potential methods for identifying target groups to provide a more comprehensive analysis.",
            "20": "- **Streamline the annotation process**\n     - Develop automated or semi-automated methods to assist in the annotation process, reducing the time and expertise required.",
            "21": "- Explore ways to leverage additional data sources to enrich the dataset and improve prediction accuracy."
        },
        "2d8E8fVYAo": {
            "0": "The paper addresses a fascinating and essential topic in the field of Chinese studies.",
            "1": "This topic is of significant interest to scholars and researchers in history and linguistics.",
            "2": "The paper provides sufficient information to reproduce the experimental results.",
            "3": "The paper's reliance on only one dataset could be perceived as a potential limitation, as it may hinder the ability to establish the universality or general applicability of the proposed method.",
            "4": "The proposed method appears to be primarily built upon existing research, and it lacks sufficient innovation."
        },
        "4in4T7vcWt": {
            "0": "+ representation learning of historical people is an interesting research question\n+ the MingOfficial dataset could be a useful resource - Is the base language model used to encode historical text actually trained on ancient mandarin?",
            "1": "How did the authors evaluate this adopted language model?",
            "2": "- Why is military power selected as a special attribute for representation learning?",
            "3": "It would be nice to have an ablation study removing this learning objective or adding other attributes as part of the learning signal.",
            "4": "- There is no Figure 11 in the paper, but line 434 indicates otherwise.",
            "5": "In addition, there is a ?",
            "6": "?",
            "7": "on line 436.",
            "8": "- I suggest having an ethics statement to discuss relevant concerns if any.",
            "9": "Also, the limitations section is incomplete on line 629."
        },
        "jHqpofWNOy": {
            "0": "This paper will publish a new dataset containing Ming dynasty officials’ career records, annotated personnel types, and related historical texts, which can benefit the following research work in this field.",
            "1": "This work introduced how to process and annotate the proposed dataset, which offered a guideline to make annotations for other similar tasks.",
            "2": "Also, the exploratory analysis can help understand this dataset better.",
            "3": "The authors provided a graph construction method to present Ming dynasty’s complex political landscape, which can be also referred to in other data-driven Chinese historical research studies.",
            "4": "The authors conducted extensive experiments to evaluate the proposed GNN-based framework.",
            "5": "This manuscript needs to be carefully revised in several aspects, including completing the discussion of limitations and addressing various spelling errors (see Typos Grammar Style below).",
            "6": "From the experiment results, it can be found that incorporating coP-P and simP-P always negatively impacted the performance.",
            "7": "So, I think there is a need to find a more effective way to fuse these two views of P-P interactions.",
            "8": "The experiments were conducted on only one dataset.",
            "9": "It would be better if the evaluation of the proposed framework could be performed on more datasets.",
            "10": "The scale of the introduced dataset is relatively small, with a more limited number of annotated data available.",
            "11": "However, I understand the difficulty to build such an interesting dataset."
        }
    },
    "fL8AKDvELp": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the efficiency and performance issues in training large language models using Sparse Mixture-of-Experts (SMoE) by introducing a novel approach called HyperRouter.",
            "1": "- HyperRouter dynamically generates router parameters through a fixed hypernetwork and trainable embeddings, aiming to balance between training the routers and freezing them.",
            "2": "- The proposed method shows significant improvements in efficiency and performance across various NLP tasks compared to existing routing methods.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel approach to address the representation collapse issue in SMoE training, which is a significant problem in the field.",
            "4": "- Extensive experiments demonstrate the superior performance and efficiency gains of HyperRouter compared to state-of-the-art methods.",
            "5": "- The implementation details and results are well-documented, providing a clear understanding of the methodology and its benefits.",
            "6": "- The paper includes a thorough analysis of the representation capabilities and the impact of the proposed method on the routing policy.",
            "7": "Potential reasons for rejection\n   - **Limited scalability validation**\n     - The experiments are conducted on medium-scale datasets and small TransformerXL models due to computational constraints.",
            "8": "- Further empirical evaluations on larger datasets and recent large language models are required to validate the scalability of HyperRouter.",
            "9": "- **Lack of comparison with more diverse baselines**\n     - The paper primarily compares HyperRouter with SMoE and SMoE-Dropout, but additional comparisons with other state-of-the-art methods in different domains could strengthen the claims.",
            "10": "- **Potential overfitting concerns**\n     - The paper does not provide sufficient details on how overfitting is mitigated during training, especially given the additional trainable parameters introduced by HyperRouter.",
            "11": "- **Complexity of implementation**\n     - The introduction of a fixed hypernetwork and trainable embeddings adds complexity to the implementation, which might be a barrier for practical adoption.",
            "12": "Suggestions for improvement\n   - **Expand scalability experiments**\n     - Conduct experiments on larger datasets and with more recent large language models to validate the scalability and generalizability of HyperRouter.",
            "13": "- **Include more diverse baselines**\n     - Compare HyperRouter with a wider range of state-of-the-art methods across different domains to provide a more comprehensive evaluation of its performance.",
            "14": "- **Address overfitting concerns**\n     - Provide more details on the regularization techniques used to prevent overfitting, and consider including additional experiments to demonstrate the robustness of the method.",
            "15": "- **Simplify implementation**\n     - Explore ways to simplify the implementation of HyperRouter, such as sharing hypernetworks among layers or generating router parameters in a more efficient manner, to make it more accessible for practical use."
        },
        "MRZU3GNVAk": {
            "0": "The results based on a small scale Transformer-XL on both pretraining and finetuning tasks are very convincing.",
            "1": "It shows for pretraining tasks and during inference, HyperRouter achieves the same performance as competitors with half the number of experts, warrant its advantage for LLM deployment.",
            "2": "It also shows for finetuning tasks, there is consistent accuracy gains over competitors on all datasets.",
            "3": "The additional experiment results about the inference FLOPs, parameter comparison, and entropy analysis and routing visualization in the appendix add more strength to the analysis.",
            "4": "They provide extra angles to help understand why the proposed approach is advantageous.",
            "5": "All experiment results are based on a small-scale transformer-XL.",
            "6": "Based on the configuration in the paper, the model has about 1.8M parameters.",
            "7": "The SMoE-Droput paper reported results on BERT base (110M parameters), RoBERTa base (125M), and Transformer-XL (18M).",
            "8": "So the reader of this paper may not have a clear picture of what the performance gains and inference advantages are on large and medium scale transformers (LLMs).",
            "9": "Will the conclusions generalize to larger-scale LLMs or the gain will diminish as it scales?",
            "10": "We need more empirical evidence.",
            "11": "[Update] The authors addressed the two questions with proper empirical evidence and explanation in the rebuttal."
        },
        "PSjLbwupWX": {
            "0": "- HyperRouter introduces a novel approach that balances fixed and trainable routers in SMoE training, enhancing the routing policy and mitigating representation collapse issues, resulting in more efficient and effective large language model training.",
            "1": "- The authors conduct experiments on both pre-training and fine-tuning for various datasets and note that HyperRouter:\na) Outperforms both SMoE and SMoE-Dropout (in the pre-training regime)\nb) Significantly outperforms SMoE-Dropout when using only one expert (in the pre-training regime)\nc) Performs competitively with SMoE275 Dropout while only using half of the experts (in the pre-training regime)\nd) In the fine-tuning regime- it outperforms all the other strategies on SST-2 and IMDB datasets with only 8 experts\n\n - The experiments in the paper have a narrow scope since the authors use only one kind of transformer architecture.",
            "2": "The results are are not very conclusive for future research to build on.",
            "3": "- I'm not sure if utilizing just 4 layers (out of 18) from TransformerXL would accurately predict the model's behavior.",
            "4": "The authors might have opted for a smaller model for directly translating the results."
        },
        "xbaLgNzseo": {
            "0": "The authors propose an interesting approach that facilitates dynamic router parameter generation.",
            "1": "The evaluation demonstrates the potential and performance improvement of the proposed approach.",
            "2": "The paper needs a comprehensive analysis of sparse MoE, including the communication overhead (all to all).",
            "3": "Currently, it's not clear where the performance gain comes from, basically, different number of experts incurs different communication overhead.",
            "4": "The evaluation needs experiments on distributed deployment and a larger model.",
            "5": "For the arguments that the existing approach has two key limitations, the authors should present key experiment results for demonstration."
        }
    },
    "kgxtMJHe7w": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical bottleneck in developing extraction models for visually rich documents by proposing a novel selective labeling approach.",
            "1": "- The approach simplifies the labeling task to binary \"yes/no\" labels and combines it with a custom active learning strategy, which is a significant innovation in reducing data-labeling costs.",
            "2": "- The method demonstrates a substantial reduction in annotation costs (by 10x) with negligible loss in accuracy, which is highly relevant for practical applications in business workflows.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-defined problem and offers a clear, innovative solution that addresses a significant challenge in the field of document extraction.",
            "4": "- The experimental results are robust, showing that the proposed method can achieve nearly the same accuracy as traditional methods while dramatically reducing labeling costs.",
            "5": "- The approach is validated across multiple domains, demonstrating its generalizability and practical applicability.",
            "6": "- The paper provides a thorough analysis of different aspects of the proposed method, including selection metrics, sampling methodologies, and the impact of initial dataset size.",
            "7": "Potential reasons for rejection\n   - **Limited availability of datasets:**\n     - The datasets used in the experiments are proprietary and not publicly available, which limits the reproducibility of the results.",
            "8": "- The lack of public datasets makes it difficult for other researchers to validate and build upon the work.",
            "9": "- **Dependence on candidate generation:**\n     - The method relies on high-recall candidate generators, which may not be available or effective for all types of documents and fields.",
            "10": "- The performance of the method is constrained by the quality of the candidate generation step, which is not the focus of the paper.",
            "11": "- **Complexity of adapting to sequence labeling models:**\n     - The paper acknowledges that adapting the method to sequence labeling models is non-trivial and leaves this as future work.",
            "12": "- This limitation may hinder the adoption of the method in scenarios where sequence labeling models are preferred or required.",
            "13": "Suggestions for improvement\n   - **Provide more details on candidate generation:**\n     - Include a more comprehensive discussion on the candidate generation process, its limitations, and how it can be improved or adapted for different document types.",
            "14": "- Explore alternative candidate generation methods that could enhance the overall performance of the proposed approach.",
            "15": "- **Increase transparency and reproducibility:**\n     - Consider open-sourcing a portion of the datasets or providing detailed guidelines on how similar datasets can be created and annotated.",
            "16": "- Share the code and implementation details of the selective labeling framework to facilitate reproducibility and further research.",
            "17": "- **Address the adaptation to sequence labeling models:**\n     - Provide preliminary insights or experiments on how the selective labeling approach can be adapted to sequence labeling models.",
            "18": "- Discuss potential challenges and solutions for integrating uncertainty estimates and training with partially labeled documents in sequence labeling models.",
            "19": "- **Expand the evaluation to more diverse datasets:**\n     - Test the proposed method on a wider range of publicly available datasets to demonstrate its versatility and effectiveness across different document types and domains.",
            "20": "- Include a comparison with other state-of-the-art methods to highlight the advantages and potential trade-offs of the selective labeling approach."
        },
        "jZZCX9I7at": {
            "0": "Evaluation of two step annotation consisting of selection of candidate from extracted results and annotating labels of the selected candidates.",
            "1": "In named entity recognition, which is categorized as sequential labeling problem as in document extraction of this paper, similar problems have been studied.However, they are not mentioned.",
            "2": "Some of them are listed in the missing reference section.",
            "3": "The reviewer feels that the combination of existing methods including the missing reference ones and active learning consist of this paper's method."
        },
        "1Syk1gHgXP": {
            "0": "- **Simple and sound method:** The proposed method is straight forwarded and easily appliable.",
            "1": "- **Extensive evaluation:** The authors deeply investigated the behaviour of the proposed method under multiple circumstances, which revealed insightful results.",
            "2": "- **Felt there are details missing:** Although, the focus is on the SL method itself and not on the extractive models, it would be beneficial if the paper becomes more self-contained by briefly addressing the candidate generator and scorer model used during the experiments, instead of providing only a brief glimpse.",
            "3": "Additionally, exploring the synergy of different models and the proposed SL method could add an interesting perspective to the paper.",
            "4": "- **Missing limitation:** After reading the paper, I felt that there was one limitation that was not addressed.",
            "5": "From what I understood, if a rare field is not present in the initial set of documents, then it would be impossible for the extraction model to generate candidates for that missing field, meaning that field would never receive an annotation.",
            "6": "- **Hard reproducibility and no codebase:** The paper's reproducibility is challenging due to the private nature of the chosen datasets (as correctly mentioned in the limitation section).",
            "7": "To overcome this, conducting additional experiments on public datasets would enhance the broader dissemination of the work.",
            "8": "Moreover, even though the datasets are private, releasing the codebase could facilitate a larger adoption of the method and encourage benchmarking on other datasets."
        },
        "lsC358HyiD": {
            "0": "This paper focuses on a practical problem concerning information extraction from visually rich documents while addressing cost issues with training and human annotations.",
            "1": "Although it does not provide a SOTA model, it provides a novel combination to address the stated research problem.",
            "2": "While the authors acknowledge its limitations on dataset release, it would benefit from additional details regarding dataset creation and comparison with other baseline datasets.",
            "3": "Furthermore, a comparison with other state-of-the-art models would strengthen the paper's credibility and impact."
        }
    },
    "g4FAvRcSuf": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a self-supervised behavior cloning transformer for text games, which are challenging benchmarks for multi-step reasoning in virtual environments.",
            "1": "- The approach auto-generates training data by exploring trajectories that lead to rewards within the games, which is a novel method compared to traditional supervised training data reliance.",
            "2": "- The method shows promise in achieving about 90% performance of supervised systems across three benchmark text games, highlighting its potential significance in the field of reinforcement learning and natural language processing.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a significant challenge in reinforcement learning and text games by proposing a novel self-supervised approach.",
            "4": "- The empirical analysis demonstrates that the method consistently uncovers generalizable training data, achieving high performance without the need for human-generated playthroughs.",
            "5": "- The approach is validated on three benchmark games, providing a robust evaluation of its effectiveness.",
            "6": "- The paper includes detailed explanations of the methodology, experiments, and results, making it a comprehensive and well-documented study.",
            "7": "Potential reasons for rejection\n   - **Limited scope of evaluation:**\n     - The method is only tested on three specific text games, which may not be representative of the broader range of text-based or other types of games.",
            "8": "- The generalizability of the approach to other domains or more complex environments is not explored.",
            "9": "- **Computational complexity:**\n     - The path crawling method may struggle with very large action spaces, making it potentially intractable for more complex environments.",
            "10": "- The paper does not provide a detailed analysis of the computational resources required for the path crawling and model training processes.",
            "11": "- **Performance comparison:**\n     - While the self-supervised model achieves 90% of the supervised system's performance, it still falls short of the supervised baseline, which may limit its practical applicability.",
            "12": "- The comparison with other state-of-the-art models, such as GPT-4, shows mixed results, with the self-supervised model underperforming in certain tasks like mathematical reasoning.",
            "13": "Suggestions for improvement\n   - **Expand the evaluation:**\n     - Test the method on a wider variety of text games and other types of environments to better understand its generalizability and limitations.",
            "14": "- Include more complex and varied tasks to assess the robustness of the approach.",
            "15": "- **Address computational complexity:**\n     - Provide a detailed analysis of the computational resources required for the path crawling and model training processes.",
            "16": "- Explore optimizations or alternative methods to handle large action spaces more efficiently.",
            "17": "- **Enhance performance comparison:**\n     - Investigate ways to further improve the performance of the self-supervised model to close the gap with the supervised baseline.",
            "18": "- Conduct a more thorough comparison with other state-of-the-art models, including a detailed analysis of strengths and weaknesses in different tasks.",
            "19": "- **Incorporate additional baselines:**\n     - Include more diverse baselines, such as other reinforcement learning models or hybrid approaches, to provide a more comprehensive evaluation of the proposed method.",
            "20": "- Analyze the impact of different training hyperparameters and configurations on the performance of the self-supervised model."
        },
        "SkyUGCfgTw": {
            "0": "The method is simple and intuitive, almost like a beam search of trajectories and shows solid empirical results on all three games.",
            "1": "The writing is clear and easy to follow.",
            "2": "The proposed technique is also general enough to be applied to more complicated game settings with tweaks.",
            "3": "On concern with Table 1 is that the self-supervised method is only compared to an oracle i.e.",
            "4": "the supervised model, and two baselines to which it is not directly comparable since the way these are trained are so different i.e.",
            "5": "DRRN or GPT4.",
            "6": "It would be significantly improved by including a baseline that ablates the effect of the data creation using the iterative method i.e.",
            "7": "a random path crawler that generates an equal number of synthetic paths which is then used to train the agent.",
            "8": "This would show the effect of how helpful it is to perform the iterative selection step."
        },
        "AA1TuT43NQ": {
            "0": "* The text games are interesting benchmarks that mark the capability of language models solving real-world problems.",
            "1": "* The proposed method is intuitive and easy to understand.",
            "2": "This might potentially increase the impact of this work.",
            "3": "* The generalization of this method might be poor as it heavily relies on the forms of games.",
            "4": "Specifically, the authors set the grouping heuristic as the actions in each text game.",
            "5": "However, this practice is not generalizable as it is not likely to expect all text games to fit this heuristic.",
            "6": "Therefore, the proposed method might be limited to a small set of tasks.",
            "7": "* The description of the method is not clear or formal.",
            "8": "In Section 2, the authors illustrate their method mainly by figures and examples.",
            "9": "Although it might suffice to describe their implementation, it is unclear how readers should adapt this method to their test cases in a principled way.",
            "10": "* Lack of important baselines.",
            "11": "The GPT-4 baseline is only evaluated in a zero-shot fashion.",
            "12": "However, in recent years, the community has developed multiple ways to properly prompt the model.",
            "13": "The authors should evaluate these recent techniques to make sure the baseline is proper."
        },
        "XU6cyvoBVU": {
            "0": "The paper is well-written and the method clear.",
            "1": "Despite not performing as well as GPT-4 on some tasks, the T5-based model is substantially smaller and approaches the performance ceiling defined by supervised behavior cloning approaches.",
            "2": "To the best of my knowledge, the paper uses reasonable baselines and provides a clear description of tradeoffs between models.",
            "3": "The paper’s primary contribution seems unlikely to generalize beyond text games: in particular, searching over all possible actions (up to the initial reward) is only possible because language in text games is highly restricted (and we assume access to the set of valid actions).",
            "4": "Additionally, path grouping based on generalization seems to work only because the tasks evaluated in this paper are limited in their complexity, such that many instances of a task can be solved with the same sequence of macro-level actions.",
            "5": "Furthermore, intermediate rewards will not exist in most real-world tasks, limiting the usefulness of incremental path crawling.",
            "6": "**In response to the rebuttal**: I do not believe that these criticisms violate the *CL reviewing guidelines as the authors state.",
            "7": "The motivation behind text games is to \"simulate complex natural language problems in controllable settings\" (Osborne, et al.",
            "8": "2022, cited in the author response).",
            "9": "As a result, it is not at all unreasonable to suggest that methods for text games should (at least attempt to) generalize to the real-world domains which they are intended to simulate.",
            "10": "After reading the author response, I have chosen to keep my score unchanged."
        }
    },
    "OgK0kMz5Va": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the underexplored problem of zero-shot species recognition using Vision-Language Models (VLMs) like CLIP.",
            "1": "- It proposes a novel and simple method of translating scientific names to common English names to improve zero-shot recognition accuracy.",
            "2": "- The approach is significant for applications in education, ecological, and biodiversity research where automated species recognition is valuable.",
            "3": "Potential reasons for acceptance\n   - The paper presents a clear and practical solution to a real-world problem, demonstrating significant improvements in zero-shot species recognition accuracy.",
            "4": "- The method is straightforward and easy to implement, making it accessible for further research and practical applications.",
            "5": "- The experimental results are robust, showing consistent performance gains across multiple datasets and species types.",
            "6": "- The paper provides a thorough analysis and comparison with existing methods, highlighting the limitations of using scientific names in prompts.",
            "7": "Potential reasons for rejection\n   - **Limited novelty in methodology**\n     - The proposed method of translating scientific names to common names is relatively simple and may be seen as an incremental improvement rather than a groundbreaking innovation.",
            "8": "- The paper does not introduce new theoretical insights or significant advancements in the underlying technology of VLMs or zero-shot learning.",
            "9": "- **Dependence on external resources**\n     - The method relies on external resources like online collections and museums to obtain common names, which may not always be available or accurate for all species.",
            "10": "- The approach may not be scalable or applicable to species without well-documented common names, limiting its generalizability.",
            "11": "- **Insufficient exploration of limitations**\n     - The paper does not thoroughly explore the potential biases and limitations of using common names, such as regional variations or ambiguities in common names.",
            "12": "- There is a lack of discussion on the ethical implications and potential biases in the pretraining data of VLMs and LLMs, which could affect the performance and fairness of the proposed method.",
            "13": "Suggestions for improvement\n   - **Enhance novelty and theoretical contributions**\n     - Consider exploring more sophisticated methods for improving zero-shot species recognition, such as integrating additional contextual information or leveraging domain-specific knowledge.",
            "14": "- Provide a deeper theoretical analysis of why common names improve performance and how this insight can be generalized to other zero-shot learning tasks.",
            "15": "- **Address scalability and generalizability**\n     - Investigate methods to automatically generate or infer common names for species that lack well-documented common names, potentially using LLMs or other AI techniques.",
            "16": "- Evaluate the method on a broader range of species and datasets to demonstrate its scalability and generalizability.",
            "17": "- **Explore limitations and biases**\n     - Conduct a more comprehensive analysis of the potential biases and limitations of using common names, including regional variations and ambiguities.",
            "18": "- Discuss the ethical implications and potential biases in the pretraining data of VLMs and LLMs, and propose strategies to mitigate these issues.",
            "19": "- **Improve experimental rigor**\n     - Include more detailed ablation studies to isolate the effects of different components of the proposed method, such as the impact of using descriptions in prompts.",
            "20": "- Provide additional qualitative examples and visualizations to illustrate the improvements achieved by the proposed method and to better understand its strengths and weaknesses."
        },
        "49SLUeyD5y": {
            "0": "This paper presents a method to facilitate the conversion of scientific names into universal names  for the purpose of enhancing the accuracy of zero sample species recognition using Vision Transformer (ViT) models.",
            "1": "The proposed method is relatively simple yet effective.",
            "2": "While the researchers implemented a method for translating text during the model inference stage, the proposed approach is relatively simplistic and does not appear to introduce any new or innovative techniques or algorithms.",
            "3": "Additionally, the research fails to conduct a horizontal comparison of the prompt template, which could provide valuable insights into the efficacy and accuracy of the underlying model."
        },
        "E2BKcSmbtG": {
            "0": "The proposed idea is very simple and it is validated on 4 benchmark datasets by consistently outperforming the baselines.",
            "1": "The idea of using VLMs for fine-grained datasets is a bold move and can open-up an interesting research venue.",
            "2": "In real-world scenario with hundreds of species from the same genus or family, I wonder how this method will generalize.",
            "3": "To illustrate, there are more than 2000 beetles and not many of them has common names.",
            "4": "One of the reasons that the model works well on CUB dataset is that the CUB dataset is not very fine-grained.",
            "5": "Most of bird species are well known ones and have common names.",
            "6": "There are most 4-5 species belonging to the same genus in that dataset.",
            "7": "No ZSL methods are tested as a baseline.",
            "8": "It would be very beneficial for the paper to show how the proposed method (VLM + translated class names) performs against some of traditional ZSL methods [1, 2, 3, 4]\n\n\n\n[1] Xian, Y., Lampert, C. H., Schiele, B., & Akata, Z.",
            "9": "(2018).",
            "10": "Zero- shot learning— A comprehensive evaluation of the good, the bad and the ugly.",
            "11": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(9), 2251– 2265.",
            "12": "[2] E. Schonfeld, S. Ebrahimi, S. Sinha, T. Darrel, and Z. Akata.",
            "13": "Generalized zero- and few-shot\nlearning via aligned variational autoencoders.",
            "14": "In CVPR, 2019.",
            "15": "[3] S Badirli, Z Akata, G Mohler, C Picard, M Dundar.",
            "16": "Fine-Grained Zero-Shot Learning with DNA as Side Information.",
            "17": "In NeurIPS, 2021.",
            "18": "[4] B. Romera-Paredes and P. H. Torr.",
            "19": "An embarrassingly simple approach to zero-shot learning.In ICML, 2015."
        },
        "30vJrcKg8N": {
            "0": "Interesting finding backed by great improvement over the state of the art.",
            "1": "The technique itself is \"embarrasingly\" simple as per the authors own description.",
            "2": "While that is not a knock against it, the authors do not dig deeper into what the implications of their result are.",
            "3": "It is not shocking that CLIP would be trained with data that does not have scientific names in it.",
            "4": "However, scientific names are assigned so there are unambiguous names for species.",
            "5": "Common names on the other hand are often ambiguous and downright confusing.",
            "6": "For example, prairie dogs are rodents while dogs are canines, Similarly guinea pigs are not porcine at all.",
            "7": "A lion is a feline but a sea lion is a mariine mammal.",
            "8": "The authors are satisfied with the improvements in the results that they have achieved but do not probe for cases where there may not be a bijective mapping between scientific and common names.",
            "9": "update: The authors have provided a convincing response to this question."
        }
    },
    "XkexLrJDss": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant challenge in computer science education: the creation of code-tracing questions for introductory programming courses.",
            "1": "- It explores the use of large language models (LLMs), specifically GPT-4, to automate the generation of these questions, which is a novel application of LLMs in the educational domain.",
            "2": "- The study contributes a unique dataset of human and LLM-generated code-tracing questions, which can be valuable for further research in both education and natural language processing (NLP).",
            "3": "Potential reasons for acceptance\n   - The paper tackles a relevant and practical problem in computer science education, offering a potential solution to the time-consuming task of creating code-tracing questions.",
            "4": "- It provides a thorough evaluation of GPT-4's performance in generating code-tracing questions, including comparisons with human-generated questions and different LLM versions.",
            "5": "- The study introduces a well-defined human evaluation methodology, ensuring the reliability and validity of the results.",
            "6": "- The dataset and findings can serve as a foundation for future research and development in automated educational content generation.",
            "7": "Potential reasons for rejection\n   - **Limited generalizability of findings:**\n     - The study primarily focuses on GPT-4, raising concerns about the applicability of the results to other LLMs or future models.",
            "8": "- The evaluation is limited to introductory Java programming questions, which may not represent the diversity of programming languages and educational contexts.",
            "9": "- **Lack of real-world educational efficacy testing:**\n     - The study does not involve actual students, leaving the real-world impact and effectiveness of LLM-generated questions on learning outcomes untested.",
            "10": "- The paper does not address how these questions would be integrated into existing educational platforms or curricula.",
            "11": "- **Potential biases in question generation:**\n     - The few-shot generation approach showed a significant bias towards provided examples, which could limit the diversity and creativity of generated questions.",
            "12": "- The study does not explore potential biases in the LLM's training data that could affect the quality and fairness of the generated questions.",
            "13": "Suggestions for improvement\n   - **Expand the scope of evaluation:**\n     - Include a broader range of LLMs, such as CodeT5+, to assess the generalizability of the findings.",
            "14": "- Test the approach on different programming languages and educational contexts to ensure wider applicability.",
            "15": "- **Incorporate real-world testing:**\n     - Conduct experiments involving actual students to evaluate the educational impact and effectiveness of LLM-generated questions.",
            "16": "- Explore how these questions can be integrated into existing educational platforms and curricula, and assess their usability and acceptance by educators and students.",
            "17": "- **Address potential biases:**\n     - Investigate and mitigate potential biases in the LLM's training data that could affect the quality and fairness of the generated questions.",
            "18": "- Develop strategies to enhance the diversity and creativity of generated questions, especially in few-shot scenarios.",
            "19": "- **Enhance the dataset:**\n     - Expand the dataset to include a wider variety of code snippets and questions, covering different programming languages and difficulty levels.",
            "20": "- Provide detailed annotations and metadata for the dataset to facilitate further research and analysis."
        },
        "5ZzXWVvlUI": {
            "0": "This paper has several strengths that would make it a valuable contribution to the NLP community:\n\n- Novel application area: This is the first work exploring LLMs for code tracing question generation, an important educational application distinct from prior NLP work.",
            "1": "Expanding LLMs to this new domain is an interesting direction.",
            "2": "- Rigorous human evaluation: The paper introduces a thoughtful evaluation methodology including multiple human ratings and textual similarity metrics.",
            "3": "This provides a model for human evaluation of LLM-generated content.",
            "4": "- High-quality dataset: The human question dataset compiled is a unique asset that can enable further research.",
            "5": "Releasing this data would be very beneficial.",
            "6": "- Insightful analysis: The paper provides a nuanced analysis around the promise but also limitations of LLMs for this application.",
            "7": "The insights into quality, diversity, and discernibility of LLM questions are impactful.",
            "8": "- Interdisciplinary relevance: This work sits at the intersection of NLP, education, and human-AI collaboration.",
            "9": "Demonstrating the potential of LLMs for enhancing education would be appealing to a broad audience.",
            "10": "- Practical implications: With further development, the proposed techniques could lead to LLM integration in educational platforms and classrooms.",
            "11": "This could significantly aid programming education and learning.",
            "12": "Presenting this work would expand the NLP community's understanding of LLMs' capabilities on a non-traditional task.",
            "13": "It would also highlight an interesting new application domain with practical relevance.",
            "14": "The novel human evaluation framework and high-quality dataset are additional assets that could catalyze further work in this direction.",
            "15": "Overall, this is a timely exploration that would be well-suited for an NLP venue.",
            "16": "Some potential weaknesses and risks to consider:\n\n- Small dataset size: The dataset has only 176 question pairs from limited sources.",
            "17": "A larger, more diverse dataset could strengthen the evaluation and generalizability of findings.",
            "18": "- Limited scope: The focus is narrow - only on intro Java programming tracing questions.",
            "19": "Testing breadth by targeting other domains/question types would be informative."
        },
        "jCWdoU2Je1": {
            "0": "- The code-tracing scenario proposed by this paper is novel.",
            "1": "- A dataset has been constructed for this scenario.",
            "2": "- An exploration of LLM's application in this scenario.",
            "3": "- The metrics has been established to evaluate the quality of the generated code-tracing questions.",
            "4": "- The study only considered GPT-4 and has not incorporated other models.",
            "5": "- There is no personalized question recommendation based on different people's submissions, which shows a lack of thorough exploration of the scenario.",
            "6": "- The study lacks attempts to use this approach in practical testing scenarios.",
            "7": "- The metrics for evaluating code-tracing questions are not comprehensive enough.",
            "8": "- The setting of the metrics is somewhat subjective and lacks an automatic evaluation."
        },
        "2A43cCjeB2": {
            "0": "* The work well demonstrates how to prompt LLMs for a series of code-tracing questions when given a snippet of codes.",
            "1": "* When the generated data is released, it may bring benefits to the field of programming education and dialogue systems.",
            "2": "* The experiment results show the potential and limitations of the LLMs-generated questions for code-tracing."
        }
    },
    "6RuXWFEQzg": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the limitations of existing Open-world Relation Extraction (OpenRE) methods by proposing a more realistic setting called Generalized Relation Discovery.",
            "1": "- The proposed method, KNoRD, effectively classifies explicitly and implicitly expressed relations from known and novel classes within unlabeled data.",
            "2": "- The paper introduces a novel approach to relation discovery that includes negative instances and long-tail relation types, which aligns better with real-world data characteristics.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-motivated problem and provides a comprehensive analysis of the limitations of existing OpenRE methods.",
            "4": "- The proposed method, KNoRD, demonstrates significant performance improvements over state-of-the-art baselines on multiple OpenRE benchmarks.",
            "5": "- The experimental results are thorough and show the effectiveness of the proposed method in various settings, including with and without negative instances.",
            "6": "- The paper provides detailed ablation studies and error analysis, which help to understand the contributions of different components of the proposed method.",
            "7": "- The authors openly provide all code, experimental settings, and datasets used, which promotes reproducibility and further research in this area.",
            "8": "Potential reasons for rejection\n   - **Limited evaluation on out-of-distribution datasets**\n     - The paper focuses on evaluating the proposed method on datasets with no distribution drift, which may not fully represent real-world scenarios.",
            "9": "- The authors acknowledge this limitation and suggest future work to evaluate the method on out-of-distribution datasets.",
            "10": "- **Dependence on human-annotated data**\n     - The proposed method requires human-annotated data for training, which can be expensive and time-consuming to create.",
            "11": "- The paper does not provide a solution for reducing the reliance on human-annotated data.",
            "12": "- **Inability to automatically determine the number of novel classes**\n     - The method cannot automatically determine the ground truth number of novel classes in unlabeled data, which is a limitation acknowledged by the authors.",
            "13": "- The paper suggests future work to address this limitation but does not provide a concrete solution.",
            "14": "- **Focus on sentence-level relation classification**\n     - The proposed method is designed for sentence-level relation classification, and its applicability to document-level relation classification is not tested.",
            "15": "- The paper does not provide evidence that the method would work well for document-level relation classification.",
            "16": "Suggestions for improvement\n   - **Evaluate on out-of-distribution datasets**\n     - Conduct experiments on out-of-distribution datasets to demonstrate the robustness and generalizability of the proposed method in more diverse real-world scenarios.",
            "17": "- **Reduce dependence on human-annotated data**\n     - Explore semi-supervised or unsupervised approaches to reduce the reliance on human-annotated data and make the method more scalable.",
            "18": "- **Develop a method to determine the number of novel classes**\n     - Propose an automated method for determining the ground truth number of novel classes in unlabeled data to enhance the practicality of the approach.",
            "19": "- **Extend to document-level relation classification**\n     - Test the proposed method on document-level relation classification tasks to demonstrate its applicability beyond sentence-level classification.",
            "20": "- **Improve prompt-based methods for GPT 3.5**\n     - Investigate alternative prompting techniques, such as Chain-of-Thought (CoT) or Self-Consistency Prompting, to enhance the performance of GPT 3.5 in relation extraction tasks."
        },
        "f10ejH5fwz": {
            "0": "The issues and settings that this paper focuses on are important and meaningful.",
            "1": "This paper is well-written.",
            "2": "The experiments are sufficient and combined with large models such as ChatGPT for analysis.",
            "3": "My main concern with this paper is the Generalized Relation Discovery setting it claims to propose, which is actually not the first time the authors have proposed it, and has been proposed in earlier work such as ARD.",
            "4": "Unfortunately, I can not see enough comparison and elaboration of ARD in this paper."
        },
        "3vpUBjBvgJ": {
            "0": "The paper is well written.",
            "1": "The technical quality is good, with a method well designed and well presented.",
            "2": "The results are quite promising, with outperformance over the baselines, on three datasets (TA-CREAD, ReTACRED, and FewRel).",
            "3": "The assumption of no distribution shift is a bit strong.",
            "4": "It's likely that known relation classes found in labeled data do not appear in the unlabeled data.",
            "5": "Could you justify the construction of novel classes in 385 - 389?",
            "6": "E.g., why the ratio 15% is used?",
            "7": "Is there any previous standard setting on the evaluation of this problem?"
        },
        "mapmsb4qel": {
            "0": "The paper is well-written and easy to understand.",
            "1": "The assumptions and claims for building the KNoRD model are clearly stated.",
            "2": "It's laudable that the motivation behind creating the KNoRD model is to handle the realistic real-world data setting.",
            "3": "The KNoRD model has the potential to generate weak data labels, and it would be interesting to use it for synthetic data creation.",
            "4": "The KNoRD method works only on sentence inputs, can't easily be extended to document-level RE, and requires human-annotated labeled data.",
            "5": "There are various hyperparameters (top-3 tokens for relation representation, P% for weak labels in clustering, known cluster size, etc.)",
            "6": "used throughout the KNoRD modeling, which could make it hard to reproduce the results or even apply to a new unlabeled dataset."
        }
    },
    "TnpFFjHCcw": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the task of conversational semantic parsing over large-scale knowledge graphs (KGs), which is a less explored area compared to isolated question answering.",
            "1": "- The proposed approach introduces dynamic context graphs to represent and encode conversational context, which is a novel method in the field.",
            "2": "- The use of graph neural networks (GNNs) to encode the dynamically generated subgraphs is innovative and leverages the structural information of KGs effectively.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel approach to handling conversational context in semantic parsing, which is a significant contribution to the field.",
            "4": "- The experimental results demonstrate that the proposed method outperforms existing static approaches, showing its effectiveness.",
            "5": "- The paper provides a thorough evaluation on a large-scale dataset (SPICE), which adds credibility to the results.",
            "6": "- The approach is scalable and efficient, addressing the computational challenges associated with large KGs.",
            "7": "Potential reasons for rejection\n   - **Limited generalization to unseen domains:**\n     - The model relies on a pre-trained NER module for entity linking, which may not generalize well to unseen domains within Wikidata.",
            "8": "- The paper does not explicitly address how the model would perform on KGs other than Wikidata, limiting its applicability.",
            "9": "- **Handling of relations:**\n     - The paper does not explicitly consider relations in the KG, assuming that the correct information will always be available.",
            "10": "- This assumption may not hold in real-world scenarios, leading to potential errors in the generated SPARQL queries.",
            "11": "- **Error analysis and qualitative evaluation:**\n     - The paper lacks a detailed error analysis and qualitative evaluation of the generated SPARQL queries.",
            "12": "- Understanding the nature of parsing errors and providing examples of both successful and failed cases would strengthen the evaluation.",
            "13": "- **Complexity of the proposed method:**\n     - The dynamic context graph approach introduces additional complexity compared to simpler, static methods.",
            "14": "- The paper does not provide a detailed comparison of the computational costs and efficiency of the proposed method versus existing approaches.",
            "15": "Suggestions for improvement\n   - **Generalization to unseen domains:**\n     - Investigate and report the performance of the model on different KGs and unseen domains to demonstrate its generalizability.",
            "16": "- Consider incorporating domain adaptation techniques to improve the model's robustness across various domains.",
            "17": "- **Explicit handling of relations:**\n     - Develop methods to explicitly handle relations in the KG, ensuring that the model can accurately generate SPARQL queries even when some information is missing or ambiguous.",
            "18": "- Provide a detailed analysis of how relation handling impacts the overall performance of the model.",
            "19": "- **Error analysis and qualitative evaluation:**\n     - Include a comprehensive error analysis section, highlighting common types of errors and their potential causes.",
            "20": "- Provide qualitative examples of both successful and failed SPARQL queries to give readers a better understanding of the model's strengths and weaknesses.",
            "21": "- **Comparison of computational costs:**\n     - Conduct a detailed comparison of the computational costs and efficiency of the proposed dynamic context graph approach versus existing static methods.",
            "22": "- Discuss the trade-offs between the improved performance and the additional complexity introduced by the dynamic context graphs."
        },
        "2haOejygW1": {
            "0": "(1) This paper is easy to understand\n(2) The experiment results are strong In the era of foundation model, it is beneficial to check the performance of ChatGPT."
        },
        "eLHyS0FaQ0": {
            "0": "•\tThe strengths of the paper are in the good description of the model, detailed analysis of the results, reasonable experiment structure and baselines.",
            "1": "•\tResults are presented well with a detailed ablation study; The ablation analysis and case studies provide additional insight into the effectiveness of the approach.",
            "2": "•\tCode and data are going to be released\n•\tIt presents a novel approach with strong performance, and clear motivation.",
            "3": "•\tThis is a very well written paper, coherent and easy to understand.",
            "4": "•\tArchitecture of the model is well-defined in figure 2.",
            "5": "•\tOverall, this is an effective paper with strong results •Most of the improvement is coming from disambiguation based on popularity of an entity in training data.",
            "6": "More experiments are needed to see the generalizability of this approach on new dataset and unknown entities"
        },
        "UIQCR3zvwv": {
            "0": "- An interesting approach which builds upon previous work and introduces dynamic context graph for semantic parsing\n- Extended comparison and analysis with BertSP - Analysis and comparison with similar model like LasagneSP is missing.",
            "1": "-  The model lacks novel design"
        }
    },
    "KEH6Cqjdw2": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical societal issue by focusing on legally enforceable hate speech detection, which is essential for protecting groups against harmful and discriminatory language.",
            "1": "- The novelty lies in aligning hate speech detection systems with enforceable legal definitions, which is a significant step towards consistent and reliable outputs that can assist regulators.",
            "2": "Potential reasons for acceptance\n   - The introduction of a new task formulation grounded in legal definitions of hate speech is innovative and addresses a gap in existing research.",
            "3": "- The creation of a novel dataset annotated by legal experts adds significant value and credibility to the research.",
            "4": "- The empirical evaluation of state-of-the-art large language models (LLMs) provides a comprehensive analysis of their performance in this new task.",
            "5": "- The paper offers a detailed methodology for augmenting the dataset with expert-generated samples and automatically mined challenge sets, which enhances the robustness of the dataset.",
            "6": "Potential reasons for rejection\n   - **Limited scope of legal definitions:**\n     - The paper primarily focuses on Canadian legal definitions and a few social media policies, which may not be representative of global standards.",
            "7": "- The exclusion of EU definitions and other international standards could limit the generalizability and applicability of the research.",
            "8": "- **Dataset imbalance and augmentation:**\n     - The positive class in the dataset is significantly smaller compared to the negative class, which could affect the model's performance and bias.",
            "9": "- The process of expert augmentation and automatic augmentation might introduce biases or inaccuracies that are not thoroughly addressed.",
            "10": "- **Model performance and reasoning:**\n     - The performance of the models, especially in fine-grained classification, is relatively low, indicating that the task remains challenging for current LLMs.",
            "11": "- The reasoning capabilities of the models are not on par with legal experts, and the explanations provided by the models often lack depth and accuracy.",
            "12": "- **Ethical considerations and potential misuse:**\n     - The paper briefly mentions the potential for misuse but does not provide a comprehensive discussion on how to mitigate these risks.",
            "13": "- The high stakes nature of the task requires more detailed ethical considerations and guidelines for responsible use.",
            "14": "Suggestions for improvement\n   - **Expand the scope of legal definitions:**\n     - Include a broader range of legal definitions from different jurisdictions, including EU and other international standards, to enhance the generalizability of the research.",
            "15": "- Provide a more detailed comparison of how different legal definitions align or differ, and how this impacts the model's performance.",
            "16": "- **Address dataset imbalance and augmentation:**\n     - Implement techniques to balance the dataset, such as oversampling the positive class or using advanced data augmentation methods.",
            "17": "- Conduct a thorough analysis of the potential biases introduced by expert and automatic augmentation, and propose methods to mitigate these biases.",
            "18": "- **Improve model performance and reasoning:**\n     - Explore advanced training techniques, such as transfer learning or domain-specific fine-tuning, to improve the model's performance in fine-grained classification.",
            "19": "- Enhance the reasoning capabilities of the models by incorporating more detailed and structured prompts, and by providing additional context or definitions in the input.",
            "20": "- **Ethical considerations and potential misuse:**\n     - Provide a comprehensive discussion on the ethical implications of the research, including potential risks and guidelines for responsible use.",
            "21": "- Propose mechanisms for human oversight and intervention to ensure that the system is used as an assistive tool rather than a fully automated solution."
        },
        "6ML4dihyPQ": {
            "0": "* The built dataset will be useful for researchers to evaluate classifiers\nfollowing hate speech definitions based on legally enforceable definitions.",
            "1": "* The paper also presents a useful benchmark for comparison of future systems,\nwhich includes various models, model setups (zero-shot, tuning) and evaluation\nsetups.",
            "2": "Additionally, the presented experiments provide a useful analysis on the\nperformance and behaviour of LLMs on legally enforceable hate speech detection.",
            "3": "* The writing of the paper is high quality and detailed.",
            "4": "* I did not find any significant reasons for rejecting the paper.",
            "5": "Please see\nbelow for some of my concerns/issues."
        },
        "2c29qtLCmm": {
            "0": "The paper introduces a unique perspective on hate speech detection, which focuses on grounding the task in legally enforceable definitions.",
            "1": "The release of a gold label dataset annotated by legal experts is a substantial contribution to the research community.",
            "2": "Such a dataset can serve as a reliable benchmark for evaluating and comparing hate speech detection systems.",
            "3": "The paper primarily focuses on aligning hate speech detection with legal definitions.",
            "4": "It may lack a broader analysis of other aspects related to hate speech and its impact."
        },
        "w71JGVEYU5": {
            "0": "- Extend hate speech with prompt templating\n- zero-shot classification for legally defined hate speech with many LLMs\n- Fine0tuned models for legally enforceable hate speech\n- Model result explanation - How do we extend the approaches to other (countries' legal documents)?",
            "1": "- Data collection and annotation are not clear\n- The Enforceable Annotation might have ethical issues.",
            "2": "What will be the reward for the 10 law experts?",
            "3": "why did they volunteer?",
            "4": "Does it count toward their study (credit), or will they co-author the paper?",
            "5": "This is a serious issue, which might lead to the low quality of the data.",
            "6": "- The concept of \"editing samples\" is not clear\n- Majority voting from 11 definitions is not clear?",
            "7": "- You could compare your result with SoTA approaches, for example with HateXplain models."
        }
    },
    "GQ1rtVVIy2": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the identification of Early Maladaptive Schemas (EMS) from mental health question texts, which is a novel and significant contribution to the field of mental health and natural language processing (NLP).",
            "1": "- The study explores both Large Language Models (LLMs) and non-LLM approaches, providing a comprehensive analysis of their effectiveness in identifying EMS labels.",
            "2": "- The research has practical implications for improving mental health support in community QA forums, which are increasingly popular for seeking mental health assistance.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel application of NLP techniques to a critical area of mental health, demonstrating the potential for automated methods to assist in identifying underlying psychological issues.",
            "4": "- The authors provide a thorough evaluation of both LLM and non-LLM approaches, offering valuable insights into their strengths and weaknesses.",
            "5": "- The study includes a high-quality, expert-curated dataset, which adds credibility to the findings and serves as a benchmark for future research.",
            "6": "- The paper is well-structured and clearly written, making it accessible to a broad audience, including both NLP researchers and mental health professionals.",
            "7": "Potential reasons for rejection\n   - **Limited dataset size:**\n     - The dataset used for evaluation is relatively small, with only 30 questions annotated by experts, which may limit the generalizability of the findings.",
            "8": "- The small sample size may also affect the statistical significance of the results, making it difficult to draw definitive conclusions.",
            "9": "- **Lack of detailed comparison with existing methods:**\n     - The paper does not provide a detailed comparison with existing methods for identifying EMS or similar psychological constructs, which could help contextualize the contributions of the proposed approaches.",
            "10": "- A more comprehensive review of related work and a discussion of how the proposed methods improve upon or differ from existing techniques would strengthen the paper.",
            "11": "- **Explainability and interpretability issues:**\n     - The LLM approaches, while effective in terms of prediction performance, lack explainability and are sensitive to prompt variations, which could limit their practical applicability in real-world settings.",
            "12": "- The paper could benefit from a more in-depth discussion of the implications of these limitations and potential strategies to address them.",
            "13": "Suggestions for improvement\n   - **Expand the dataset:**\n     - Increase the size of the dataset by annotating more questions with EMS labels, which would enhance the robustness and generalizability of the findings.",
            "14": "- Consider including a more diverse range of mental health topics and question types to capture a broader spectrum of EMS manifestations.",
            "15": "- **Enhance comparison with existing methods:**\n     - Provide a more detailed comparison with existing methods for identifying EMS or similar constructs, highlighting the unique contributions and advantages of the proposed approaches.",
            "16": "- Include a discussion of related work in the fields of mental health and NLP to better contextualize the study within the broader research landscape.",
            "17": "- **Improve explainability and interpretability:**\n     - Explore techniques to enhance the explainability of LLM predictions, such as incorporating attention mechanisms or generating more interpretable explanations for the assigned labels.",
            "18": "- Investigate methods to reduce the sensitivity of LLMs to prompt variations, potentially through prompt engineering or fine-tuning with additional training data.",
            "19": "- **Address ethical considerations:**\n     - Discuss the ethical implications of using automated methods for identifying EMS in mental health contexts, including potential risks and strategies to mitigate them.",
            "20": "- Consider the impact of incorrect predictions on individuals seeking mental health support and propose guidelines for responsible use of the proposed methods."
        },
        "WFCmKs0LUI": {
            "0": "The paper introduces an interesting problem of Early Maladaptive Schema Identification in mental health texts.",
            "1": "Evaluated various non-LLMs and LLM-based methods and offered a thorough analysis of the potential and limitations of different techniques for EMS label prediction.",
            "2": "The paper maintains a formal tone and technical language appropriately.",
            "3": "However, some sections could be further polished for clarity, particularly in conveying the details of complex methodologies.",
            "4": "The methodology section introduces three distinct approaches logically, but it lacks detailed explanations for certain techniques.",
            "5": "The methodology section of the paper introduces various approaches for predicting early maladaptive schemas (EMS) labels but falls short in several critical aspects.",
            "6": "The description of the Similarity-based Voting Predictor (SVP) is insufficiently detailed.",
            "7": "The exact process of computing candidate EMS labels based on sentence similarity using multiple sentence transformer models is unclear.",
            "8": "The Entailment-based Prediction model (EPM) is introduced briefly but lacks clarity in its application.",
            "9": "The section does not elaborate on how entailment is determined between YSQ statements and question sentences."
        },
        "7Sd1Nil4gp": {
            "0": ">Describing two methods for predicting EMS labels through a novel application of sentence similarity and textual entailment.",
            "1": ">Providing an evaluation of the methods using LLMs and non-LLMs \n>Constructing a small EMS dataset.",
            "2": ">The overall novelty of this paper is limited.",
            "3": "The proposed SVP method exists.",
            "4": "In addition, the EMS prediction task is similar to depressive symptoms detection tasks.",
            "5": "Therefore, there are many related methods leveraging semantic information, such as:\nZhang, Z., Chen, S., Wu, M., & Zhu, K. Q.",
            "6": "(2022a).",
            "7": "Psychiatric scale guided risky post screening for early detection of depression.",
            "8": "arXiv preprint arXiv:2205.09497.",
            "9": "Zhang, Z., Chen, S., Wu, M., & Zhu, K. (2022b).",
            "10": "Symptom identification for interpretable detection of multiple mental disorders on social media.",
            "11": "In Proceedings of the 2022 conference on empirical methods in natural language processing (pp.",
            "12": "9970–9985).",
            "13": "In addition, although the LLMs' method and evaluation are useful for the EMS prediction task, there are no interesting findings, and the results can not help justify the lack of interpretability.",
            "14": "The authors could add a chain of thought to learn more about the performance of LLMs.",
            "15": ">Some contents need to be included and need to be more clearly described: The specific structure and usage of the EPM method.",
            "16": "Which F1 (micro, macro, average) has been used?",
            "17": "The label distribution of the dataset (imbalanced or balanced)?",
            "18": ">Some experiments should be added, such as ablation studies about non-LLMs-based models.",
            "19": ">The Fleiss Kappa is low.",
            "20": "What is the reason, are two annotators enough, and is the guideline set reasonable?"
        },
        "7mwAHx8Wja": {
            "0": "* A new idea to apply a concept from schema theory in psychology to NLP texts- the work can be beneficial to other clinical NLP researchers and be impactful for studying triage.",
            "1": "* The paper is well-written and easy to understand.",
            "2": "* Many obvious experiments come to mind given its a first attempt at understanding the concept in language domain -- for example, the models could be finetuned for the methods, or a model trained on mental health datasets specifically could be used.",
            "3": "Studies could be done across multiple models to check for inherent knowledge of identifying EMS.",
            "4": "The choice of models is not well-motivated either.",
            "5": "* Consistency and Explainability: although its a section, the discussion on non-LLM is limited to one line and doesn't have any specific experiments designed for either consistency or explainability.",
            "6": "* The work is small and limited in its applications, I would like to see more experiments and settings for a thorough evaluation.",
            "7": "The work in its current form might be better suited for a related workshop."
        },
        "OoBRH9iqKM": {
            "0": "The paper highlights a significant application of natural language processing in the realm of mental health.",
            "1": "The authors have curated a valuable dataset for identifying EMS from patient texts or conversations.",
            "2": "The paper's dataset presentation lacks clarity, making it challenging to follow without specific examples.",
            "3": "The discussion of dataset de-identification, critical for dataset release, is absent.",
            "4": "The inter-annotator agreement is notably low.",
            "5": "There's a lack of discussion on the adjudication strategy and revisions that shaped the final test dataset.",
            "6": "Incorporating few-shot learning, especially for null labels, and implementing contrastive learning could enhance the results.",
            "7": "Consider making the dataset publicly available, as it could be a valuable resource for the community.",
            "8": "I kindly request the authors to provide a more detailed discussion of the dataset and consider its public availability for the benefit of the community."
        }
    },
    "islVqaCzfa": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the underexplored area of automatic code editing using large language models (LLMs).",
            "1": "- Introduces InstructCoder, a novel dataset specifically designed for general-purpose code editing tasks.",
            "2": "- Demonstrates the potential of LLMs fine-tuned on InstructCoder to achieve performance levels on par with ChatGPT.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a relevant and practical problem in software development, enhancing development efficiency.",
            "4": "- The introduction of InstructCoder fills a significant gap in the availability of datasets for code editing tasks.",
            "5": "- The methodology for dataset creation, including the use of GitHub commits and scenario-conditional generation, is innovative and ensures high-quality, diverse data.",
            "6": "- Empirical results show substantial improvements in code editing performance, validating the effectiveness of the proposed approach.",
            "7": "- The paper provides a comprehensive analysis of the dataset and the performance of fine-tuned models, contributing valuable insights to the research community.",
            "8": "Potential reasons for rejection\n   - **Limited scope of evaluation:**\n     - The evaluation is primarily based on GPT-4 and human assessments, which may not cover all aspects of code editing performance.",
            "9": "- The test set is relatively small (134 samples), which might not fully represent real-world scenarios.",
            "10": "- **Potential biases in dataset:**\n     - The dataset is generated using LLMs and GitHub commits, which may introduce biases that do not reflect real-world code editing tasks.",
            "11": "- The dataset may lack diversity in programming languages, focusing primarily on Python.",
            "12": "- **Scalability and generalization:**\n     - The approach may not scale well to larger, more complex codebases or cross-file code changes.",
            "13": "- The generalization of the fine-tuned models to other programming languages or domains is not thoroughly explored.",
            "14": "- **Reproducibility and transparency:**\n     - The paper lacks detailed information on the specific prompts and hyperparameters used in the experiments, which may hinder reproducibility.",
            "15": "- The manual filtering process for the dataset is not fully transparent, raising concerns about the consistency and objectivity of the data.",
            "16": "Suggestions for improvement\n   - **Expand evaluation scope:**\n     - Include additional metrics and benchmarks to evaluate the performance of the fine-tuned models more comprehensively.",
            "17": "- Increase the size and diversity of the test set to better represent real-world code editing scenarios.",
            "18": "- **Address potential biases:**\n     - Ensure the dataset includes a wider range of programming languages and code editing tasks to reduce potential biases.",
            "19": "- Provide a detailed analysis of the dataset's diversity and potential biases to increase transparency.",
            "20": "- **Enhance scalability and generalization:**\n     - Explore the scalability of the approach to larger codebases and cross-file code changes.",
            "21": "- Investigate the generalization of the fine-tuned models to other programming languages and domains.",
            "22": "- **Improve reproducibility and transparency:**\n     - Provide detailed information on the prompts, hyperparameters, and training procedures used in the experiments.",
            "23": "- Clearly document the manual filtering process for the dataset to ensure consistency and objectivity.",
            "24": "- **Additional experiments and comparisons:**\n     - Conduct experiments comparing the performance of InstructCoder with other existing datasets and approaches.",
            "25": "- Include ablation studies to understand the impact of different components of the dataset creation process on the final performance."
        },
        "xFiek5kj4s": {
            "0": "- Code editing is a very important capability that has been less explored in the GPT-era (though there is extensive work in code editing in the past, see \"Missing References\"), so it is nice to see work that is focused on this.",
            "1": "- The CodeInstruct dataset offers a nice training set that can be used for further research.",
            "2": "- Incorporating the notion of \"scenarios\" when building the dataset is quite clever and novel.",
            "3": "- Establishing that the gap between ChatGPT and open-source LLMs with respect to code editing can almost be closed with fine-tuning on dataset generated using prompt-based data generation techniques is interesting and may have implications for further research.",
            "4": "- By comparing against Alpaca and CodeAlpaca, the authors demonstrate that their data generation technique is superior to other prompt-based automatic data generation techniques for this task.",
            "5": "However, it is not clear whether such techniques are better than just directly using human-written examples from GitHub.",
            "6": "In Lines 163-175, the authors claim that GitHub commits are too noisy to use directly; however, this is not empirically validated.",
            "7": "GitHub serves as an extremely large data source, and given that one of the findings of this paper is that the scale of the data is a profound factor of code-editing ability, it is important to understand whether the scale of the data reduces the impact of noise.",
            "8": "Moreover, commit messages are not the only source of NL instructions from GitHub.",
            "9": "Another source is pull request comments (and the corresponding code edits).",
            "10": "In fact, there is already a large-scale benchmark for this: CodeReviewer (see missing references).",
            "11": "As a point of reference, it would be important to understand what the effect is of fine-tuning open-source LLMs on CodeReviewer, and how this compares to fine-tuning on CodeInstruct.",
            "12": "- The evaluation is limited and weak.",
            "13": "First, the test set entails only 134 examples which are manually curated by the authors of the paper, and it appears that they are all in Python.",
            "14": "Next, the main form of evaluation is prompting GPT-4 to judge the correctness of model predictions.",
            "15": "While this has been explored for other tasks, this has not been established as a valid evaluation strategy for code editing.",
            "16": "In Appendix E, the authors provide a justification for using GPT-4 for evaluation by comparing with human evaluation.",
            "17": "However, I do not find this convincing for a few reasons.",
            "18": "1) The human evaluation is done by authors of this paper and not by external evaluators, and no information is given about annotator agreement, 2) The human evaluation entailed three classes (correct, partial, wrong), while GPT-4 evaluates based on 2 classes (\"Yes\" or \"No\").",
            "19": "For comparison, they group \"partial\" with \"correct\" while it should actually be considered wrong.",
            "20": "3) Finally, the consistency ratio is 68.4%, which is lower than what I would expect for strong evaluation."
        },
        "G8MeyofOTh": {
            "0": "* Introduces a new dataset, following plausible steps to construct.",
            "1": "* Empirically shows the effectiveness of the benchmark by consistently improving the test accuracies (boh by GPT-4 and human) throughout LLMs.",
            "2": "* To justify using the automatic evaluation by GPT-4 as a main metric, authors need more supported analyses.",
            "3": "I suggest Cohen's kappa between GPT-4 and human decisions, and analyze the (dis)agreed categories.",
            "4": "* (minor) This benchmark seems a bit easy-- ChatGPT Zero-shot: 90.5% by GPT-4 and 79.3% for correct scores by human (though it cannot be an apples-to-apples comparison, competition-level code generation like CodeContests by LLMs shows below 10% of Pass@1 (= the accuracy of generating a single code for each problem)).",
            "5": "Can you analyze (and report) the category-wise performance, then build a hard subset for both 1) giving directions that what LLMs still bad at and 2) room for improvements that future work may acheive?"
        },
        "kHuLf4l3MF": {
            "0": "I appreciate that the authors created the dataset used for fine-tuning LLM for code editing tasks.",
            "1": "I believe this dataset will draw more attention and inspire further research in this area.",
            "2": "The dataset is built on the task seeds mined from github which give a more realistic starting point compared to other related dataset.",
            "3": "The analysis and plots show that the dataset covers diverse topics and intents.",
            "4": "I like the analysis done by the authors.",
            "5": "The LLAMA-33B fine-tuned on this CodeInstruct dataset has compareble performance to ChatGPT.",
            "6": "The way to collect and filter the task seeds from git commits is not clearly described.",
            "7": "I think this process is the most challenging part in collecting code-related data from Github.",
            "8": "- Authors mentioned that \"We used Codex to clarify the changes made between versions and improve the commit messages, resulting in more precise and informative instructions\", but they did not described how they filter the commit messages and they did not provide any examples in this process.",
            "9": "- I hope authors can provide details on how they classifiy the commit messages into 768 seed tasks.",
            "10": "What is the criteria here.",
            "11": "Is it simply by manually check?",
            "12": "My concern in evaluation is that there is only 134 data for evaluation which might be relatively small.",
            "13": "Second, I would suggest evaluating on other out-of-box code editing dataset to gain more confidence about the quality of the dataset.",
            "14": "(see missing related work for other datasets)\n   - I am wondering if authors manually clean each example in the test set to ensure they are reasonable and correct.",
            "15": "I think it is important for evaluate the models.",
            "16": "- When comparing with Model fine-tuned on CodeAlpaca dataset, I am wondering if authors will also report the performance on test set of CodeAlpaca.",
            "17": "Maybe the difference in performance is because of the data distribution shift.",
            "18": "The dataset only contains Python edits.",
            "19": "It seems not hard to include other programming languages"
        }
    },
    "OwxjgsX68V": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenge of improving Named Entity Recognition (NER) in low-resource settings, which is a significant problem in Natural Language Processing (NLP).",
            "1": "- The proposed method, CASSI, introduces a novel augmentation scheme that combines semantically similar sentences to generate contextually diverse augmentations while avoiding annotation corruption.",
            "2": "- The approach leverages dependency parsing and a pretrained Masked Language Model (MLM) to ensure the fluency and semantic correctness of the generated sentences.",
            "3": "Potential reasons for acceptance\n   - The method shows consistent and significant improvements over existing augmentation techniques across multiple languages and resource levels.",
            "4": "- The approach is innovative in its use of dependency parsing and MLM scoring to generate high-quality augmentations.",
            "5": "- The paper provides extensive experimental results, demonstrating the effectiveness of CASSI in both monolingual and cross-lingual NER tasks.",
            "6": "- The method is shown to be robust even in noisy text conditions, which is a valuable contribution to the field.",
            "7": "Potential reasons for rejection\n   - **Dependency on dependency parsers:**\n     - The method requires a dependency parser for the target language, which may not be available for extremely low-resource languages.",
            "8": "- The quality of the augmentations is highly dependent on the accuracy of the dependency parser, which can be problematic for noisy text or languages with less reliable parsers.",
            "9": "- **Complexity and computational cost:**\n     - The process of generating and filtering candidate augmentations involves multiple steps, including dependency parsing, MLM scoring, and J-Score filtering, which can be computationally expensive.",
            "10": "- The pairwise BERTScore calculation for sentence similarity can be time-consuming for larger datasets.",
            "11": "- **Limited improvement for high-resource settings:**\n     - The method shows diminishing returns for high-resource languages with clean text, where the performance improvements are marginal beyond a certain dataset size.",
            "12": "- The paper does not provide a clear analysis of the trade-offs between the computational cost and the performance gains for high-resource settings.",
            "13": "Suggestions for improvement\n   - **Addressing dependency on dependency parsers:**\n     - Explore alternative methods for generating augmentations that do not rely on dependency parsers, such as using syntactic or semantic role labeling.",
            "14": "- Investigate the use of multilingual dependency parsers or transfer learning techniques to improve parser performance for low-resource languages.",
            "15": "- **Reducing computational complexity:**\n     - Optimize the pairwise BERTScore calculation by using more efficient sentence similarity metrics or by sampling a subset of sentences for similarity matching.",
            "16": "- Consider parallelizing the augmentation generation and filtering steps to reduce the overall computational cost.",
            "17": "- **Enhancing robustness and generalizability:**\n     - Conduct additional experiments on a wider range of low-resource languages and domains to further validate the robustness and generalizability of the method.",
            "18": "- Provide a more detailed analysis of the trade-offs between computational cost and performance gains, particularly for high-resource settings.",
            "19": "- **Improving augmentation quality:**\n     - Implement additional post-processing steps to address grammatical incorrectness and incoherence in the generated augmentations.",
            "20": "- Explore the use of more advanced language models or fine-tuning techniques to further improve the fluency and naturalness of the augmentations."
        },
        "tvy69nsa6K": {
            "0": "- A novel technique that aims to improve the performance of NER systems in low-resource settings.",
            "1": "- The technique is well-explained and backed by a solid experimental setup that includes numerous experiments in various settings, including monolingual and cross-lingual settings.",
            "2": "- Section 4 offers interesting insights, including a more in-depth discussion about the results on a specific language and an ablation study that goes through various hyper-parameters of the text augmentation pipeline.",
            "3": "- Since the authors have included the code as supplementary material, I assume that it will be open-sourced.",
            "4": "None"
        },
        "x3BdsnaBT9": {
            "0": "The motivation to augment data based on dependency parse tree is well-founded and interesting.",
            "1": "The proposed method outperforms previous data augmentation methods in monolingual NER.",
            "2": "It would be beneficial if the authors could demonstrate the effectiveness of the method on few-shot cross-domain NER, rather than only conducting experiments on cross-lingual settings, which are more common."
        },
        "JADAKj1d0a": {
            "0": "- The paper is well written and easy to follow.",
            "1": "Frequently, when doubts would arise, they were often addressed in the next sentence or paragraph.",
            "2": "Well done.",
            "3": "- CASSI explicitly considers both diversity and naturalness of the generations whereas many existing techniques yield ungrammatical, implausible, or incoherent texts.",
            "4": "Some would argue that low-quality synthetic texts are acceptable because they still improve downstream model performance and generalization.",
            "5": "However, in the real world, people often place significant weight on readability and having a technique that improves performance while being natural can help overcome objections from human decision makers and increase the likelihood of use.",
            "6": "- The evaluation setup acknowledges the limitations of the augmentation to low resource settings and sufficiently demonstrates its benefits relative to existing baselines.",
            "7": "- In general, the claims of “high-quality” and increased “contextual diversity” are left unquantified, at least directly.",
            "8": "The high-quality claim may potentially be substantiated via improvements to model performance but given the gist of the filtering criteria suggests that a quality == naturalness argument is being made.",
            "9": "Authors could use an approach like cleanlab (https://github.com/cleanlab/cleanlab) to support the claim that CASSI does not introduce any new label issues brought on by shifts in augmented semantics."
        }
    },
    "7O9bTjLgTQ": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the interpretability of transformer-based language models, specifically focusing on the attention mechanism and memory values.",
            "1": "- It introduces a novel tool for visualizing the information flow within Generative Pre-trained Transformers (GPTs), which simplifies the understanding of the model's internal processing.",
            "2": "- The study provides new insights into the role of layer norms and identifies neurons that act as regularization vectors.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a significant problem in the field of NLP by improving the interpretability of transformer models, which are often considered black boxes.",
            "4": "- The proposed visualization tool is innovative and has the potential to aid researchers and practitioners in understanding and debugging transformer models.",
            "5": "- The findings regarding layer norms and regularization neurons contribute to the existing body of knowledge and open up new avenues for research.",
            "6": "Potential reasons for rejection\n   - **Lack of generalizability:**\n     - The study primarily focuses on GPT-2 models, and it is unclear whether the findings and the visualization tool can be generalized to other transformer models or languages.",
            "7": "- The experiments are conducted on a specific dataset (CounterFact), which may limit the applicability of the results to other types of data or tasks.",
            "8": "- **Insufficient quantitative evaluation:**\n     - The paper relies heavily on qualitative analysis and case studies to demonstrate the effectiveness of the visualization tool.",
            "9": "More quantitative evaluations and comparisons with existing interpretability methods would strengthen the claims.",
            "10": "- The impact of the proposed tool on practical applications, such as model debugging or improving model performance, is not thoroughly evaluated.",
            "11": "- **Complexity and usability of the tool:**\n     - The tool's complexity and the need for detailed configuration may limit its accessibility and usability for a broader audience.",
            "12": "- The paper does not provide sufficient information on the computational resources required to run the tool, which could be a barrier for some users.",
            "13": "Suggestions for improvement\n   - **Expand the scope of the study:**\n     - Conduct experiments on a wider range of transformer models, including different architectures and languages, to demonstrate the generalizability of the findings.",
            "14": "- Use additional datasets and tasks to validate the effectiveness of the visualization tool in various contexts.",
            "15": "- **Enhance quantitative evaluation:**\n     - Include more quantitative metrics to evaluate the interpretability and usefulness of the visualization tool, such as user studies or comparisons with other interpretability methods.",
            "16": "- Assess the impact of the tool on practical applications, such as model debugging, error analysis, or improving model performance.",
            "17": "- **Improve tool accessibility and usability:**\n     - Simplify the configuration and usage of the tool to make it more accessible to a broader audience, including those with limited technical expertise.",
            "18": "- Provide detailed documentation and tutorials to help users understand and effectively utilize the tool.",
            "19": "- Include information on the computational resources required to run the tool and explore ways to optimize its performance for different hardware setups."
        },
        "lqzyFxlYwK": {
            "0": "This work might significantly impact the explainability research of NLP and DL communities.",
            "1": "The theoretical analysis part is thorough.",
            "2": "It appears that the authors have conducted sufficient research on GPT-related works.",
            "3": "The experimental part needs to be more sufficient.",
            "4": "The authors need to devise more persuasive comparative experiments to verify the superiority of the proposed method.",
            "5": "The author needs to emphasize academic innovation, clearly explaining their work's principles.",
            "6": "An excellent academic paper is always inspirational; it can motivate people to explore more.",
            "7": "It is suggested that the author redesign the structure of the paper.",
            "8": "The current structure of the article is more like a technical report than an academic paper."
        },
        "4gZra4ytyJ": {
            "0": "The main reason to accept would be the release of the tool to create the flow graphs, which may aid future research.",
            "1": "This is especially true for the most useful parts of the flow graph, which would be the less fine-grained ones, as will be discussed later.",
            "2": "Of the three main contributions, the one that strikes as new and interesting is the observed phenomena of semantic filters by layer norm layers.",
            "3": "It is interesting how function words contribution is decreased after applying them, highlighting the utility of these layers to increase the weight of content \"words\" (or at least hidden states close to those words.).",
            "4": "I would have enjoyed more quantitative results regarding this.",
            "5": "The method proposed seems too fine-grained.",
            "6": "It works when one already knows what to look for.",
            "7": "Each flow graph is focused on a single layer.",
            "8": "The projection of specific heads (either queries, keys or values) to the subdimensions of embeddings at the LM head (ie.",
            "9": "j:j+h/d as in line 236) doesn't seem to be that useful.",
            "10": "The fact it needs to be projected through W_O and that no quantitative results to properly measure  any patterns or useful information from these projections, hinders the overall approach.",
            "11": "Moreover, the projections for each head are not properly explained, one has to deduce just from line 236 that each hidden state is projected through W_O and then to the corresponding dimensions of the LM head, not just for A_j but also for keys, queries and values.",
            "12": "While \"full\" hidden states or weights that share the full dimension (d) are well projected to specific tokens as shown by work such as Dar et al.",
            "13": "(2022), the new contribution of this work which is looking at the head level is only demonstrated through a couple of examples and histogram figures, but I am not sure what one really learns other than that certain heads have higher norms and therefore their influence in the output of the attention block is bigger.",
            "14": "What is the contribution of the visual flow there?",
            "15": "When I look at Figure 1 and 5, I do not see any meaningful \"projections\" at the head level.",
            "16": "Its utility is only portrayed when one already knows of a specific head phenomena from other work, as shown in Figure 6.",
            "17": "And while some plots help grasp some degree of interpretability outside the head level (ie.",
            "18": "layer norm, attention block, feed-forward, residual, ie, hidden states that share the same dimension), this was already shown by previous work.",
            "19": "Still, a tool to properly visualise these flows, with top tokens for each hidden state is very nice, and considered a reason to accept.",
            "20": "The results themselves are unsurprising.",
            "21": "While this by itself is not a reason to reject, combined with the fact that other works have already discussed most of the points the paper sort of hinders the \"reasons to accept\".",
            "22": "This can be seen as a reason not to accept, rather than to reject.",
            "23": "Why only GPT2?",
            "24": "How can we know this tool will be useful in future research if it was only tested on a single model?",
            "25": "If the restriction is that only decoder systems benefit from this visualisation, why not try other decoder only models and show the flexibility of this approach?",
            "26": "It seems that most of what is discussed in the paper was already known for GPT2, so if the point is to present a new tool that helps in explainability of transformer models, as the title would suggest, showcasing other models would be important.",
            "27": "Overall I feel like the paper is trying too many things at once.",
            "28": "It presents as a tool for visualising the flow of information, and gives some nice examples on where it is useful, but at the same time focuses on contributions which were already known or a bit redundant/expected, with qualitative experiments/examples.",
            "29": "In truth, the tool itself should be the main contribution, and less words could be devoted to try to justify new contributions in a shallow way, which are not that surprising anyways, while more relevant examples on how the tool works are relegated to the Appendix.",
            "30": "To sum up, if the paper is trying to show how information flows in the model, other papers have already done so in a more clear and concise way, however, doing so through a visual tool projecting to specific tokens to help interpretability is novel."
        },
        "pqBlidzuKw": {
            "0": "* This work proposes a tool to visualize the simplified information flow.",
            "1": "Tools like this (if accessible) can facilitate research in understanding a model's decision.",
            "2": "* Although the analysis part is only based on a single dataset (CounterFact) and a single model variant (GPT), one of the case studies shows the applicability by checking the finding through their visualization tool aligns with the finding in a previous work.",
            "3": "* The core method used for analysis is *LogitLens*, which was originally proposed in a well-known blog post, but it was shown to be brittle in a  recent work (https://arxiv.org/pdf/2303.08112.pdf), as also cited by the authors.",
            "4": "The authors claim that the basic approach of LogitLens is applied because they are interested in the interim hypothesis instead of the final layer’s output, which may not be convincing enough because the Din et al.",
            "5": "paper also presented differences between the original LogitLens versus the improved version.",
            "6": "* EDIT: As mentioned in the authors' response, I think this is less of a concern because (1) The paper I mentioned was published too closed to the submission deadline, and (2) there are components where applying these methods would be difficult/inefficient.",
            "7": "* As mentioned in the strength section, with results shown from a single dataset and a single model variant can be a weakness, although not necessarily a reason to reject.",
            "8": "* The issues of the proposed visualization tool.",
            "9": "(These are more of presentation issues, but putting in this section because the visualization tool is the core contribution of this work)\n    * EDIT: This should've been addressed per the author's response.",
            "10": "* More careful consideration such as colorblind friendliness or contrast level should be taken towards the interface design of the visualization tool.",
            "11": "* As also mentioned by the authors in the Limitation section, the approach prunes out neurons that are less activated for better visualization, but it has to be under the hypothesis that the more activated neurons are more important for prediction, which may lead to misleading results if this hypothesis fails to hold."
        }
    },
    "1mGD6ZLTwv": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the field of NLP, specifically the privacy risks associated with large language models in summarization tasks.",
            "1": "- It introduces a novel approach to membership inference (MI) attacks on summarization models, which is a relatively unexplored area compared to classification models.",
            "2": "- The study provides a comprehensive evaluation of MI attacks using text similarity and model robustness as signals, contributing valuable insights into the privacy vulnerabilities of summarization models.",
            "3": "Potential reasons for acceptance\n   - The paper tackles an important and timely problem, given the increasing use of large language models in sensitive applications.",
            "4": "- It presents a well-defined and systematic approach to evaluating MI attacks on summarization models, including both baseline and advanced attack methods.",
            "5": "- The experimental results are thorough and demonstrate the effectiveness of the proposed attacks, highlighting significant privacy risks.",
            "6": "- The paper discusses potential defense mechanisms and the trade-off between privacy and utility, providing practical insights for mitigating privacy risks.",
            "7": "Potential reasons for rejection\n   - **Limited exploration of defense mechanisms:**\n     - The paper briefly mentions differential privacy and L2 regularization as defense methods but does not provide an in-depth analysis or comparison of their effectiveness.",
            "8": "- The discussion on the privacy-utility trade-off is not sufficiently detailed, leaving readers with limited guidance on how to balance these aspects in practice.",
            "9": "- **Lack of clarity on the generalizability of findings:**\n     - The study focuses on specific datasets (SAMsum, CNNDM, MIMIC) and models (BART, FLAN-T5), but it is unclear how well the findings generalize to other datasets and model architectures.",
            "10": "- The paper does not explore the impact of different types of summarization tasks (e.g., extractive vs. abstractive) on the effectiveness of MI attacks.",
            "11": "- **Insufficient analysis of attack success factors:**\n     - The paper identifies that certain samples are more susceptible to MI attacks but does not provide a detailed analysis of the characteristics that make these samples vulnerable.",
            "12": "- There is a lack of discussion on how different model training strategies (e.g., fine-tuning, transfer learning) might influence the success of MI attacks.",
            "13": "Suggestions for improvement\n   - **Expand the analysis of defense mechanisms:**\n     - Provide a more detailed evaluation of different defense methods, including their impact on model performance and privacy protection.",
            "14": "- Explore additional defense techniques, such as adversarial training or data anonymization, and compare their effectiveness against MI attacks.",
            "15": "- **Enhance the generalizability of findings:**\n     - Conduct experiments on a wider range of datasets and summarization models to assess the robustness of the proposed attacks.",
            "16": "- Investigate the impact of different summarization tasks and model architectures on the effectiveness of MI attacks.",
            "17": "- **Analyze attack success factors in more detail:**\n     - Perform a thorough analysis of the characteristics that make certain samples more vulnerable to MI attacks, such as document length, complexity, or redundancy.",
            "18": "- Examine how different model training strategies, such as fine-tuning on domain-specific data, affect the susceptibility to MI attacks.",
            "19": "- **Improve the clarity and organization of the paper:**\n     - Provide a clearer explanation of the experimental setup, including the rationale for choosing specific datasets and models.",
            "20": "- Organize the results section more logically, with separate subsections for baseline attacks, document augmentation attacks, and document-only attacks.",
            "21": "- Include more visual aids, such as diagrams or flowcharts, to illustrate the attack methods and experimental procedures."
        },
        "Dvb8ChBS58": {
            "0": "The authors present a simple but effective method for MI attacks that works specifically with summarization.",
            "1": "Unlike previous methods that focused on classification tasks, this approach doesn't need scores or probabilities and can work with just black-box API access.",
            "2": "The method can even be used without the summarized document, making it very useful for real-world situations.",
            "3": "The paper also includes extra tests and analysis, such as how well the method can be transferred to different setups.",
            "4": "The authors have thought about how to protect against these kinds of attacks and the balance between keeping data private and making it useful.",
            "5": "Overall, this is a solid paper that adds valuable insights to the field of natural language processing and information security.",
            "6": "It's a good read for anyone interested in these subjects.",
            "7": "At times the main inference was hard to follow and justify logical jumps, however, the abundance of figures/tables covered those minor discrepancies."
        },
        "xhK6IpC2Re": {
            "0": "- Tackling a problem of a MI attack for summarization\n- Proposal of input document only MI attack for summarization using input documents modified with word synonym replacement, sentence swapping and back translation.",
            "1": "-\tThe reason why this paper focuses on summarization is not presented.",
            "2": "Previous work focused on Machine Translation (MT)."
        },
        "azLHBSj30P": {
            "0": "- Novel task formulation \n\n- Extensive experimentation and ablation studies highlighting the impact of various factors \n\n- Variety of datasets employed in the study.",
            "1": "- Propose plausible defense mechanisms.",
            "2": "- Not the most up to date models employed."
        }
    },
    "8xyd9i1XLb": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the deployment of Large Language Models (LLMs): the potential for these models to unintentionally leak sensitive information from their training data.",
            "1": "- The proposed method, MoPe θ (Model Perturbations), introduces a novel approach to membership inference attacks by adding noise to the model parameters and measuring the resulting change in log-likelihood, which approximates the trace of the Hessian matrix.",
            "2": "- The paper provides empirical evidence that MoPe θ outperforms existing loss-based attacks and other perturbation-based methods across a range of model sizes.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a highly relevant and timely problem in the field of machine learning and privacy, contributing to the understanding of privacy risks in LLMs.",
            "4": "- The proposed method, MoPe θ, is innovative and demonstrates superior performance compared to existing methods, providing a new tool for evaluating and potentially mitigating privacy risks in LLMs.",
            "5": "- The empirical evaluation is thorough, covering a wide range of model sizes and providing detailed analysis of the results.",
            "6": "- The paper includes a theoretical grounding for the proposed method, connecting it to the Hessian matrix and providing a solid foundation for the approach.",
            "7": "Potential reasons for rejection\n   - **Computational complexity and scalability:**\n     - The MoPe θ method requires multiple perturbations of the model, which can be computationally expensive, especially for larger models.",
            "8": "- The paper does not fully address the scalability of the method to even larger models beyond the 12B parameters tested.",
            "9": "- **White-box setting assumption:**\n     - The method assumes white-box access to the model parameters, which may not always be practical or realistic in real-world scenarios where only black-box access is available.",
            "10": "- The paper does not explore the feasibility or effectiveness of the method in a black-box setting.",
            "11": "- **Limited exploration of hyper-parameter optimization:**\n     - The paper mentions that the noise level σ was chosen based on a limited grid search, suggesting that the method's performance might be further improved with more extensive hyper-parameter tuning.",
            "12": "- The impact of different noise levels on the method's performance is not thoroughly explored.",
            "13": "- **Generalizability to other domains:**\n     - The paper primarily focuses on language models and does not provide sufficient evidence of the method's applicability to other types of machine learning models or tasks.",
            "14": "- The results on the MNIST dataset indicate that the method may not perform as well in other settings, raising questions about its generalizability.",
            "15": "Suggestions for improvement\n   - **Address computational complexity:**\n     - Provide a more detailed analysis of the computational requirements of the MoPe θ method, especially for larger models, and explore potential optimizations to reduce the computational burden.",
            "16": "- Investigate the scalability of the method to models larger than 12B parameters and discuss any potential challenges or limitations.",
            "17": "- **Explore black-box setting:**\n     - Extend the study to include scenarios where only black-box access to the model is available, and evaluate the feasibility and effectiveness of the MoPe θ method in such settings.",
            "18": "- Propose potential adaptations or alternative approaches that could work in a black-box setting.",
            "19": "- **Hyper-parameter optimization:**\n     - Conduct a more comprehensive exploration of the noise level σ and other hyper-parameters to identify optimal settings and improve the method's performance.",
            "20": "- Include a sensitivity analysis to understand the impact of different hyper-parameter values on the results.",
            "21": "- **Generalizability to other domains:**\n     - Provide additional experiments and analysis to demonstrate the applicability of the MoPe θ method to other types of machine learning models and tasks beyond language models.",
            "22": "- Investigate the method's performance on a wider range of datasets and model architectures to establish its generalizability.",
            "23": "- **Theoretical analysis and robustness:**\n     - Include a more detailed theoretical analysis of the method's robustness and potential failure modes, particularly in different training regimes or with different types of data.",
            "24": "- Explore the impact of different training data characteristics (e.g., data distribution, presence of outliers) on the method's effectiveness."
        },
        "8hIbp2HsgJ": {
            "0": "The strengths of this paper include the development of a novel membership inference attack that has improved accuracy over existing loss-based attacks, the examination of the role of training point order and model size in attack success, and the demonstration that MoPe accurately approximates the trace of the Hessian in practice.",
            "1": "If this paper were to be presented at the conference or accepted into Findings, the main benefits to the NLP community would be a better understanding of the potential security risks associated with large language models and a new method for identifying whether a given text is in the training data of a pre-trained language model.",
            "2": "This could lead to improved security measures for language models and better protection of sensitive information.",
            "3": "One potential weakness of this paper is that it focuses solely on the development and evaluation of a membership inference attack, without discussing potential solutions or mitigation strategies to address the security risks identified.",
            "4": "Additionally, the paper only considers white-box access to the model's parameters, which may not be representative of real-world attack scenarios."
        },
        "zNCGHrULsT": {
            "0": "The authors demonstrate a strong and relatively efficient approach to MIA without requiring shadow model training.",
            "1": "The work is timely and relevant given the explosion of LLMs trained with data of dubious provenance.",
            "2": "Success of the attack is really only demonstrated on auto-regressive LLMs.",
            "3": "Experimental comparison with shadow-training approaches would be valuable, at least for smaller models, or explain why these are not explored.",
            "4": "An important limitation is white box access, but this is not a deal-breaker."
        },
        "6ckX1UpN5k": {
            "0": "+ Well written work except for the points below in the first weakness.",
            "1": "My notes include\n  \"nice\" in response to several well-made points throughout the paper which is not common for me.",
            "2": "+ Attack discovers is effective at a different set of instances than loss attacks (i.e.",
            "3": "answers the\n  points nicely made on line 96).",
            "4": "This and the AUC behaviour comparison is suggesting that there\n  may be a fundamentally different sort of signal being employed in the MoPE attack (as opposed to\n  being just a better LOSS).",
            "5": "This also means the methods can be combined to get non-trivial benefit\n  as shown in the Ensemble Attack.",
            "6": "This strength has some caveats under the second weakness below.",
            "7": "- The description of the principal method does not match the actual method and thus the reasoning\n  behind it is misplaced and/or potentially irrelevant.",
            "8": "From the start, the method is suggesting\n  that loss landscape around training points is sharper than around random (non-training) points:\n\n       \"based on the idea that when the model loss is localized around a training point, it is\n        likely to lie in sharper local minima than if the point was a test point—a previously\n        unexplored fact distinct from the magnitude of the loss\"\n\n       \"loss around x′ ∈ D_train should be sharper than around a random point\"\n\n  The actual method perturbs the model parameters though one would expect perturbations to points\n  given the above descriptions.",
            "9": "The initial point seemed intuitive while model perturbations are\n  not.",
            "10": "The connection between model perturbations and point perturbations are not described or\n  explained.",
            "11": "Suggestion: Explain why input perturbations are not done in favor of model perturbations.",
            "12": "(if\n  true) explain how model perturbation is the same as, or sufficiently similar to, input\n  perturbation.",
            "13": "Other motivations might include the difficulty of perturbing inputs of discrete\n  tokens (though this can be addressed by perturbing embeddings).",
            "14": "Finally, if input perturbations\n  are possible, compare the model perturbation approach to the input perturbation one,\n  experimentally.",
            "15": "In addition to this point, the connection to Hessian trace is specific to the (I claim) less\n  intuitive model perturbation.",
            "16": "The benefit of making such a connection are not discussed.",
            "17": "Suggestion: describe the theoretical or otherwise benefits of making the connection between model\n  parameter perturbation as per MoPE and Hessian traces.",
            "18": "- The actual method, perturbing model parameters, might have more connections to prior methods\n  other than LOSS.",
            "19": "Perturbing a model is moving it away from the set of parameters that was\n  attained with a particular training point and in same way, getting closer to a model that was\n  trained without that point.",
            "20": "There are membership methods based on comparisons of \"with point\" vs\n  \"without point\" models.",
            "21": "Cited works include [Carlini 2021].",
            "22": "Such methods, however, were not\n  compared to MoPE experimentally nor conceptually.",
            "23": "Suggestion: Include a deeper discussion on attacks based on reasoning with models known to not be\n  trained on target input.",
            "24": "Provide experimental comparisons of MoPE and such methods.",
            "25": "- Unexplained restrictions on target models.",
            "26": "In particular, the authors claim that single epoch\n  training is common as a justification of not experimenting with models trained with multiple\n  though the discussion of checkpoints (around line 410) suggests that not only is it also common\n  to train for more than one pass, these models were already readily available during\n  experimentation and had to be specifically avoided.",
            "27": "Suggestion: Compare MoPE to baselines (LOSS and otherwise) on models trained with multiple\n  passes.",
            "28": "Demonstrate that the benefit of MoPE over baselines is or is not associated with the\n  single-pass model training regimen.",
            "29": "Other suggestions:\n\n- Additionally I suggest the authors revisit their related work sections and be less quick to\n  dismiss comparisons against other works due to some difference in setting that could be easily\n  adopted to make a direct experimental comparison possible."
        }
    },
    "gd8TxhKoLv": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the problem of automatic question generation from domain-specific, long-form textual content, which is crucial for building rich and diverse knowledge bases for applications like chatbots and online knowledge-sharing communities.",
            "1": "- The proposed framework, PROTEGE, introduces a novel encoder-decoder based Large Language Model (LLM) architecture and a hill-climbing algorithm to balance diversity and fidelity in the generated questions, which is a significant contribution to the field of natural language processing and question generation.",
            "2": "Potential reasons for acceptance\n   - The paper presents a well-defined problem and proposes a novel solution that combines prompt-based question generation with a greedy algorithm to optimize diversity and fidelity.",
            "3": "- The experimental results on multiple datasets demonstrate significant improvements in diversity and fidelity metrics compared to strong baselines, showcasing the effectiveness of the proposed approach.",
            "4": "- The paper includes detailed ablation studies and qualitative analyses, providing a comprehensive evaluation of the proposed method.",
            "5": "- The authors provide a clear and thorough explanation of the methodology, making it easy to understand and replicate the experiments.",
            "6": "Potential reasons for rejection\n   - **Limited evaluation on real-world applications:**\n     - The paper primarily focuses on benchmark datasets and does not provide sufficient evidence of the practical applicability of the proposed method in real-world scenarios.",
            "7": "- The impact of the generated questions on actual user interactions and knowledge base improvements is not thoroughly evaluated.",
            "8": "- **Dependence on specific prompt signals:**\n     - The effectiveness of the proposed method heavily relies on the choice of prompt signals, which may not always be readily available or easy to generate in real-world applications.",
            "9": "- The paper does not explore alternative methods for generating or selecting prompt signals, which could limit the generalizability of the approach.",
            "10": "- **Scalability and computational requirements:**\n     - The proposed method involves training large language models and running a greedy algorithm, which may require significant computational resources and may not be feasible for all users or applications.",
            "11": "- The paper does not provide a detailed analysis of the computational costs and scalability of the proposed approach.",
            "12": "Suggestions for improvement\n   - **Evaluate real-world applicability:**\n     - Conduct experiments to evaluate the impact of the generated questions on real-world applications, such as user interactions with chatbots or improvements in knowledge base quality.",
            "13": "- Include user studies or feedback to assess the practical usefulness and relevance of the generated questions.",
            "14": "- **Explore alternative prompt generation methods:**\n     - Investigate and compare different methods for generating or selecting prompt signals, such as using external knowledge bases, user queries, or automated keyword extraction techniques.",
            "15": "- Provide a more detailed analysis of the impact of different prompt signals on the quality of the generated questions.",
            "16": "- **Analyze computational costs and scalability:**\n     - Include a detailed analysis of the computational requirements and scalability of the proposed method, considering different model sizes and hardware configurations.",
            "17": "- Explore potential optimizations or approximations to reduce the computational costs and make the approach more accessible to a wider range of users and applications.",
            "18": "- **Extend the evaluation to more diverse datasets:**\n     - Test the proposed method on a wider variety of datasets, including those from different domains and languages, to demonstrate its generalizability and robustness.",
            "19": "- Provide a more comprehensive comparison with additional state-of-the-art question generation methods to further validate the effectiveness of the proposed approach."
        },
        "7Y1UJp7qRb": {
            "0": "- The experimental results are promising, and the paper conducts a thorough analysis of the results in terms of diversity and fidelity.",
            "1": "The inclusion of human evaluations in Figure 2 supplements the reliability of the findings.",
            "2": "- The idea of employing separate cross-attention architecture for conditional generation is a novel contribution to my knowledge.",
            "3": "- The proposed method shows inferior performance against baselines when evaluated using NLG metrics (METEOR, BLEU, ROUGLE).",
            "4": "It would be beneficial to include additional examples in the Appendix, explaning why tehse scores are relatively low.",
            "5": "- Some important details are missing in the experiment setting.",
            "6": "For instance, the paper does not specify the number of examples used in human evaluation or the number of participants involved in the evaluation process.",
            "7": "Furthermore, there are concerns about the reliability of the fidelity metrics based on the trained neural network.",
            "8": "The suitability of the trained BERT model for measuring the fidelity score of generated questions is questionable, considering results in Table 2."
        },
        "RTopz2q5D6": {
            "0": "This study introduced a new method (PROTEGE) to generate questions based on LLM architecture.",
            "1": "The study performed various experiments on three datasets and provided a lot of insight into the results.",
            "2": "The article can open a period of generating Q&A data more automatically and faster than previous methods.",
            "3": "The study has not shown practical applications to help people for this research.",
            "4": "We want more empirical diversity (add more languages if possible)\nAnd what is the way to make sure the questions that are generated stick to the passage and are answerable?"
        },
        "xNdrKtzeOH": {
            "0": "Easy-to-read and well-written paper.",
            "1": "Analyzed through various experiments and metrics.",
            "2": "Study about the relevance between diversity and fidelity.",
            "3": "A relatively simple framework structure.",
            "4": "The key difference from existing question generation models is the addition of an extra layer at the end of the encoder stage to extract embedding of prompt.",
            "5": "Furthermore, these extracted embeddings are utilized in the decoder stage through cross-attention.",
            "6": "However, applying attention across different input modalities is one of the commonly used approaches in NLP.",
            "7": "Lack of detail explanation.",
            "8": "In Section 2.2, a detail explanation of heuristic method is missing.",
            "9": "In Table 4, it is unclear the meaning of pre-greedy and post-greedy.",
            "10": "Insufficient explanation about limited performance.",
            "11": "In table 2, the NLG metrics for fidelity are worse in NQ and MS MARCO dataset.",
            "12": "It needs more analysis."
        }
    },
    "glxrubmH91": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of few-shot document-level relation extraction (FSDLRE), which is essential for handling data scarcity in real-world scenarios.",
            "1": "- The proposed Relation-Aware Prototype Learning (RAPL) method introduces novel techniques to enhance the relational semantics of prototype representations, which is a significant advancement over existing methods.",
            "2": "Potential reasons for acceptance\n   - The paper presents a well-motivated problem and provides a comprehensive solution that significantly outperforms state-of-the-art approaches.",
            "3": "- The proposed method is thoroughly evaluated on two benchmarks, demonstrating its effectiveness and robustness across various settings.",
            "4": "- The paper includes detailed ablation studies and analyses, providing insights into the contributions of different components of the proposed method.",
            "5": "Potential reasons for rejection\n   - **Complexity and Efficiency:**\n     - The incorporation of a relation encoder and the search process for support NOTA instances add to both memory and time expenses.",
            "6": "- The overall efficiency of the proposed method may be a concern, especially for large-scale applications.",
            "7": "- **Assumption of Specified Entity Information:**\n     - The method assumes that entity information should be specified, which may affect its robustness in scenarios where entity information is not readily available.",
            "8": "- Recent studies in supervised DocRE explore joint entity and relation extraction, which could be beneficial for few-shot scenarios as well.",
            "9": "- **Performance on Cross-Domain Tasks:**\n     - The performance gain of RAPL on cross-domain tasks is lower than that on in-domain tasks, indicating room for improvement in handling domain adaptation.",
            "10": "- Techniques such as data augmentation and structured knowledge guidance could be explored to enhance cross-domain performance.",
            "11": "Suggestions for improvement\n   - **Efficiency Optimization:**\n     - Explore ways to optimize the overall efficiency of the proposed method, such as reducing memory and time expenses associated with the relation encoder and NOTA instance search process.",
            "12": "- **Joint Entity and Relation Extraction:**\n     - Investigate the integration of joint entity and relation extraction techniques to improve the robustness of the method in scenarios where entity information is not specified.",
            "13": "- **Enhancing Cross-Domain Performance:**\n     - Explore data augmentation techniques and structured knowledge guidance to improve the performance of the method on cross-domain tasks.",
            "14": "- **Additional Benchmarks and Datasets:**\n     - Evaluate the proposed method on additional benchmarks and datasets to further validate its effectiveness and generalizability.",
            "15": "- **Detailed Error Analysis:**\n     - Provide a more detailed error analysis to identify specific areas where the method struggles and propose targeted improvements."
        },
        "8Fqm2Q5BSy": {
            "0": "1.The proposed issues worth investigation for Few-shot Doc RE and the proposed method seems reasonable.",
            "1": "2.The proposed method achieves great performances on the public benchmarks.",
            "2": "3.It is glad to see a Preliminary Exploration of LLM for FSDLRE.",
            "3": "1.Some technical details are not clear enough.",
            "4": "Please refer to Question to authors for details.",
            "5": "2.An analysis, which is about how the issue of NOTA is tackled, should be added to support the claim."
        },
        "8MNX5PkSHI": {
            "0": "The paper is very well-written and easy to follow.",
            "1": "The RAPL model proposed in the paper handled an important task of the few-shot document-level relation extraction task.",
            "2": "Although the work can seem incremental on the FREDo benchmark, the claims and rationales proposed in the paper are intuitive and justified with experimental results.",
            "3": "The methodology is explained in detail, and combining contrastive learning with instance-level aggregation for prototype tuning is interesting.",
            "4": "The RAPL model is incremental work, and the architecture uses the weak BERT encoder, which might cause a performance bottleneck.",
            "5": "The method assumes that entity mentions should be specified for a query document, making it less robust.",
            "6": "One of the paper's main contributions is to build a task-specific NOTA prototype; however, precise experiments to evaluate NOTA relations are missing."
        },
        "N0L5RjFCOB": {
            "0": "The paper studies an important task and proposes an interesting approach to address the FSDLRE problem.",
            "1": "The experiments are overall clear and well structured.",
            "2": "The paper is clearly written and easy to access.",
            "3": "Overall I feel the paper is solid, presenting some interesting findings and demonstrate the effectiveness of its proposed approach via multiple experiments.",
            "4": "There are a few places where the authors can further improve the current writing:\n1.",
            "5": "Considering adding one more dataset (maybe modified one existing DocRE benchmark) to demonstrate this proposed method can generalize.",
            "6": "Adding a few more case studies in the experiment section to intuitively show the output and also to promote the FSDLRE problem setting.",
            "7": "Adding more recent DocRE literature in the reference."
        }
    },
    "BpibUh0aB3": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses an important and timely question about the creative capabilities of large language models (LLMs), specifically their ability to generate divergent semantic associations.",
            "1": "- The use of the Divergent Association Task (DAT) to measure creativity in LLMs is a novel approach that provides an objective metric for evaluating creative thinking in these models.",
            "2": "- The comparison of different models and decoding strategies adds depth to the analysis and provides valuable insights into the factors that influence the creative output of LLMs.",
            "3": "Potential reasons for acceptance\n   - The study provides a clear and well-structured methodology for evaluating the creativity of LLMs, which can be replicated and extended by other researchers.",
            "4": "- The findings that GPT-4 outperforms 96% of humans and that GPT-3.5-turbo exceeds the average human level in the DAT are significant and contribute to our understanding of the capabilities of these models.",
            "5": "- The exploration of different decoding strategies and their impact on creativity is a valuable contribution that can inform future research and development of LLMs.",
            "6": "- The paper includes a thorough validation of the DAT for LLMs, addressing potential concerns about the applicability of this method to artificial systems.",
            "7": "Potential reasons for rejection\n   - **Limited scope of creativity evaluation:**\n     - The study focuses solely on the \"little-C\" creativity, which may not fully capture the broader and more complex aspects of creativity, such as \"big-C\" creativity.",
            "8": "- The evaluation is limited to self-contained creativity and does not consider human-AI co-creation, which is an important aspect of AI's creative potential.",
            "9": "- **Potential confounding variables:**\n     - The influence of word frequency on the DAT scores is acknowledged but not fully controlled, which may affect the validity of the results.",
            "10": "- The study relies on pre-trained word embeddings (GLoVe) for calculating semantic distances, which may introduce biases and limitations inherent in these embeddings.",
            "11": "- **Generalizability of results:**\n     - The findings are based on a specific set of models and decoding strategies, and it is unclear how generalizable these results are to other models and tasks.",
            "12": "- The study does not explore the impact of different prompts or contexts on the creativity of LLMs, which could provide a more comprehensive understanding of their creative capabilities.",
            "13": "Suggestions for improvement\n   - **Expand the scope of creativity evaluation:**\n     - Include additional measures of creativity that capture different aspects, such as \"big-C\" creativity and human-AI co-creation, to provide a more comprehensive evaluation of LLMs' creative potential.",
            "14": "- Explore other domains of creativity, such as artistic and scientific creation, to assess the versatility of LLMs in generating creative content.",
            "15": "- **Control for confounding variables:**\n     - Implement more rigorous controls for word frequency and other potential confounding variables to ensure the validity of the DAT scores.",
            "16": "- Consider using multiple word embedding models to calculate semantic distances and compare the results to identify any biases or limitations.",
            "17": "- **Enhance generalizability:**\n     - Test a wider range of models and decoding strategies to determine the generalizability of the findings across different LLMs and tasks.",
            "18": "- Investigate the impact of different prompts and contexts on the creativity of LLMs to understand how these factors influence their creative output.",
            "19": "- **Provide more detailed analysis:**\n     - Include a more detailed analysis of the relationship between surprisal and the DAT scores, and how this relationship varies across different models and decoding strategies.",
            "20": "- Discuss the implications of the findings for the development and application of LLMs in creative tasks, and suggest potential directions for future research."
        },
        "hlqiLg6nUo": {
            "0": "- The paper provides a focused analysis of creativity in LLMs, and addresses various interesting factors related to creativity (surprisal, temperature).",
            "1": "- The authors propose a simple and computationally efficient way of assessing creativity in LLMs, leveraging findings from creativity assessment in human language processing.",
            "2": "The paper makes various assumptions that are not adequately addressed:\n- The findings for creativity found in humans where the DAT task predicts human creativity does not necessarily transfer to LLMs; taking this as a given skips the step of validating the finding of Olson et al.",
            "3": "(2021) for the domain of LLMs.",
            "4": "- Cosine similarity has traditionally been shown to correlate with semantic relatedness, but its one-dimensional nature makes it suboptimal to measure semantic similarity in isolation.",
            "5": "E.g., by requesting to only predict nouns we are already restricting token predictions to come from a particular \"region\" in the embedding space.",
            "6": "Since cosine similarity does not differentiate between semantic relatedness and other linguistic dimensions (syntact/morphological/etc.",
            "7": "), a model could \"cheat\" by generating words that are not only semantically unrelated, but also syntactically.",
            "8": "- DAT is computed only using the GLoVe cosine distances, but the impact of this choice is not addressed.",
            "9": "GLoVe vectors should not be taken as a gold truth for semantic similarity."
        },
        "Rg9aFkUE96": {
            "0": "The question is definitely timely and pressing, careful studies on the matter are needed and likely to get us new knowledge.",
            "1": "See above: I think the paper does not show what it purports to show due to a conceptually flawed experimental design.",
            "2": "-----------------------------------\nI think the analyses provided are now sufficiently informative.",
            "3": "The rhetoric should probably be toned down a bit about this being creativity rather than a narrow aspect of it, but I do see the pros of having a catchy title.",
            "4": "The presentations should also be improved."
        },
        "OnNLQbwkY9": {
            "0": "The paper discusses an interesting take on creativity of LLMs.",
            "1": "The authors specifically looks at single nouns but do not adequately justify why combinations of multiple words (specifically creativity in language is tightly related to the ability to put together two seemingly different words that could mean something interesting, taken together) were not taken into consideration or why words other than nouns were not considered.",
            "2": "These would be necessary points to be discussed in the limitations section."
        }
    },
    "WQamRhhbsf": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the performance of large language models (LLMs), specifically their tendency to produce factually incorrect responses due to reliance on co-occurrence statistics.",
            "1": "- The study provides a novel insight into the co-occurrence bias in LLMs and suggests a method to mitigate this bias through debiased finetuning.",
            "2": "- The research contributes to the understanding of how LLMs store and recall factual knowledge, which is essential for improving the reliability and accuracy of these models.",
            "3": "Potential reasons for acceptance\n   - The paper presents a thorough investigation into the impact of co-occurrence statistics on the factual knowledge of LLMs, supported by extensive experiments and analysis.",
            "4": "- The proposed debiased finetuning method is a practical approach to mitigate co-occurrence bias, and the results demonstrate its effectiveness in improving the recall of rare facts.",
            "5": "- The study is well-structured, with clear explanations of the methodology, experiments, and results, making it accessible and informative for readers.",
            "6": "- The research addresses a significant problem in the field of natural language processing, with potential implications for the development of more reliable and accurate LLMs.",
            "7": "Potential reasons for rejection\n   - **Limited scope of tested models:**\n     - The study only tests a limited set of LLMs, primarily focusing on open-source versions of GPT-3 and its variants.",
            "8": "- The results may not be generalizable to other LLMs or pre-training datasets, limiting the broader applicability of the findings.",
            "9": "- **Debiased finetuning limitations:**\n     - The proposed debiased finetuning method shows marginal improvements on the test set, indicating that it may not be a comprehensive solution to the co-occurrence bias problem.",
            "10": "- The method may not be effective in real-world scenarios where the distribution of facts is different from the training and test datasets used in the study.",
            "11": "- **Focus on factual knowledge probing:**\n     - The study primarily focuses on factual knowledge probing tasks, which may not fully represent the performance of LLMs in other downstream tasks such as question answering or summarization.",
            "12": "- The findings may not be directly applicable to other tasks where the impact of co-occurrence bias might differ.",
            "13": "Suggestions for improvement\n   - **Expand the scope of tested models:**\n     - Include a wider range of LLMs, including those from different architectures and pre-training datasets, to validate the generalizability of the findings.",
            "14": "- Test the proposed debiased finetuning method on more diverse models to strengthen the claims.",
            "15": "- **Enhance the debiased finetuning method:**\n     - Explore additional debiasing techniques or combine multiple approaches to improve the effectiveness of the method.",
            "16": "- Investigate the impact of different filtering ratios and develop a more sophisticated algorithm for selecting biased samples.",
            "17": "- **Broaden the evaluation to other tasks:**\n     - Conduct experiments on a variety of downstream tasks, such as question answering, summarization, and dialogue systems, to assess the impact of co-occurrence bias in different contexts.",
            "18": "- Provide a more comprehensive evaluation of the proposed method's effectiveness across various applications.",
            "19": "- **Address real-world applicability:**\n     - Discuss the potential challenges and limitations of applying the proposed debiased finetuning method in real-world scenarios.",
            "20": "- Suggest practical guidelines for implementing the method in different settings and provide insights into its scalability and efficiency."
        },
        "rfX6ne8ne4": {
            "0": "-- The paper addresses an important and timely topic, namely the ability of LLMs to act as knowledge bases.",
            "1": "-- The related work section is very elaborate and provides insight into the field at hand.",
            "2": "-- The arguments of the paper are well-presented, and the writing is generally clear.",
            "3": "-- I am somewhat confused as to the exact claim the paper is making.",
            "4": "While the results clearly show a correlation between the co-occurrence statistics of a triplet and the performance of the model on it, it is not clear to me whether this in fact proves that there is a bias where such simple surface statistics push the language model astray from making the right prediction.",
            "5": "What it does show is that questions where the degree of co-occurrence is smaller are more difficult for the model.",
            "6": "The introduction reads “in which frequently co-occurred words are preferred over the correct answer.” I could not see how the experiments directly make this point.",
            "7": "Simple correlation seems to me insufficient in this case, since making mistakes with little co-occurrence doesn’t mean that there is a different option with higher co-occurrence.",
            "8": "In order to show that the behavior is biased, I would expect the paper to shows that the surface statistics interfere in some sense with the prediction of the model, in a way that would make it predict such answer even when it is not true.",
            "9": "For example, I would have expected the paper to examine questions which we would expect (based on their prevalence in the training data) the model to answer correctly, and show that in these cases it tends to make more errors where there is a strong collocation and that the mistakes is towards the collocating words.",
            "10": "If the paper indeed makes this kind of more subtle claim and I have missed it, I would welcome a response from the authors on this matter.",
            "11": "Thank you.",
            "12": "Following rebuttal: the results you have posted are helpful and address this comment.",
            "13": "Please include them in the next version.",
            "14": "-- The results of the attempts to mitigate the bias are not very strong.",
            "15": "I should say that I do not see it in itself as grounds for rejection.",
            "16": "-- Some important presentational details are not sufficiently clear (see below)."
        },
        "EIKFZmuV6r": {
            "0": "In my opinion, the most interesting aspect of the paper are the frequency baselines introduced to analyse the structure of the training dataset and to explain the behaviour of the models.",
            "1": "They bring the co-occurrence/frequency bias influencing the behaviour of the models to fore, and the analysis is convincing.",
            "2": "The main point of the paper -- that LLMs suffer from co-occurrency bias -- is not particularly new.",
            "3": "Other papers investigating this issue are mentioned in the Related Work section.",
            "4": "The authors claim that their work is the fist \"to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs\" (ll.",
            "5": "136--139), but there is no discussion of why fine-tuning should help at all, and in the end it does not, which amounts to a weak negative result.",
            "6": "The section on mitigating occupies less than 1.5 pages and does not contain any methodological insights."
        },
        "6J9oy63MjV": {
            "0": "-\tThe paper contains detailed analysis of shortcut problem regarding token co-occurrence.",
            "1": "-\tThe paper investigates an important area of verifying the factual knowledge of LLMs.",
            "2": "-\tThe paper is well-written, and is free of significant presentation issues.",
            "3": "-\tAlong with the identification of the problem, the paper also proposes to mitigate the problem.",
            "4": "-\tOne more experiment should be done to verify the claim that \"answers with higher co-occurrence are more likely to be generated\": The authors should count in each question, whether the model's generated answer has a high count in the pretraining corpus.",
            "5": "Currently there is only a table (Table 1) showing similar results, i.e., the wrong answers have a relatively lower count in the pretraining corpus.",
            "6": "However, quantitative results over the whole dataset should be given.",
            "7": "-\tThere are existing work probing the shortcut learning problem of language models.",
            "8": "The authors should elaborate more on the work to claim that they are the first to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs.",
            "9": "For example: [2]."
        }
    },
    "6srsYdjLnV": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of gender bias in machine translation (MT), particularly when translating from English to Italian, a grammatical gender language.",
            "1": "- It introduces GeNTE, a novel bilingual test set specifically designed for evaluating gender-neutral translation, which is a significant step towards more inclusive language technologies.",
            "2": "- The study also explores automated evaluation methods for gender-neutral translation, proposing a reference-free method that could potentially offer a more accurate assessment.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a relevant and timely issue in the field of natural language processing (NLP) and machine translation, contributing to the ongoing efforts to mitigate gender bias in language technologies.",
            "4": "- The introduction of the GeNTE corpus is a valuable resource for the research community, providing a dedicated benchmark for gender-neutral translation.",
            "5": "- The comprehensive analysis of existing evaluation metrics and the proposal of a reference-free evaluation method demonstrate a thorough and innovative approach to the problem.",
            "6": "- The paper is well-structured, with clear explanations of the methodology, data collection, and evaluation protocols, making it accessible and reproducible for other researchers.",
            "7": "Potential reasons for rejection\n   - **Limited language scope**:\n     - The study focuses solely on the English-Italian language pair, which may limit the generalizability of the findings to other grammatical gender languages.",
            "8": "- The proposed solutions and resources may not be directly applicable to languages with different grammatical structures or gender systems.",
            "9": "- **Dependence on synthetic data**:\n     - The classifier training relies heavily on synthetic data generated by GPT, which may introduce biases or inaccuracies not present in natural language data.",
            "10": "- The quality and variability of the synthetic data could affect the performance and reliability of the classifier.",
            "11": "- **Evaluation limitations**:\n     - The evaluation of the proposed reference-free method is conducted on a controlled test-bed with partially post-edited outputs, which may not fully reflect real-world MT scenarios.",
            "12": "- The study does not include a detailed comparison with human evaluation, which could provide a more comprehensive assessment of the proposed methods.",
            "13": "Suggestions for improvement\n   - **Expand language scope**:\n     - Consider extending the study to include additional grammatical gender languages, such as Spanish, French, or German, to enhance the generalizability of the findings.",
            "14": "- Provide guidelines or frameworks for adapting the GeNTE corpus and evaluation methods to other languages.",
            "15": "- **Enhance data quality**:\n     - Incorporate more natural language data in the training and evaluation of the classifier to reduce potential biases introduced by synthetic data.",
            "16": "- Conduct a thorough analysis of the synthetic data to identify and mitigate any inaccuracies or biases.",
            "17": "- **Improve evaluation protocols**:\n     - Include a detailed comparison with human evaluation to validate the effectiveness of the proposed reference-free method.",
            "18": "- Explore additional evaluation metrics and methods to provide a more comprehensive assessment of gender-neutral translation quality.",
            "19": "- **Address practical implementation**:\n     - Discuss potential challenges and solutions for integrating gender-neutral translation into existing MT systems and workflows.",
            "20": "- Provide recommendations for MT developers and users on how to effectively implement and utilize gender-neutral translation strategies."
        },
        "U2gOwybJgt": {
            "0": "Very useful test set.",
            "1": "Excellent introduction and motivation for the problem of gender neutral translation.",
            "2": "The background and survey are really nicely presented, as is the small-scale study surveying translator's preferences wrt neutral translations.",
            "3": "Overall,  a very strong case is made for working on the problem and for the GeNTE corpus.",
            "4": "The work is likely to be influential.",
            "5": "As the authors note in their limitations section,  the collection focuses on English->Italian translation,   and so the data and classifier models can't be used directly for other target languages.",
            "6": "Personally,  I'm ok with this,  as I think these subtle issues are best analyzed for specific target languages.",
            "7": "While this is a limitation, the paper also provides a starting framework for approaching other target languages.",
            "8": "I may have missed this in the discussion,   and I did not read the appendices,  but apart from a summary statement (line 366 - `the linguist supported the translators throughout the process and finally checked all the neutralizations')  there isn't much discussion about the quality of the data set that was produced.",
            "9": "It would be interesting to know whether there were challenges with inter-annotator agreement, i.e.",
            "10": "whether the linguist needed to intervene often,  or how often the annotators 'failed to neutralise' (Table 2, iii-F,  line 395)."
        },
        "1VGyJYsna9": {
            "0": "The paper focuses on a special case of gender bias across translations -- when the source gender is ambiguous or ungiven whereas the translation defaults to specific genders.",
            "1": "An open-source survey is conducted on 100 participants to assess how acceptable and preferable are the gender-neutral translations for the general audience.",
            "2": "The demographical details of the participants are illustrated in the Appendix.",
            "3": "The paper points out that 97.2% of segments collected from Europarl are biased toward gendered references in Italian.",
            "4": "Thus, GeNTE devotes the effort to manually correcting referent gendered and neutral sentences.",
            "5": "Some major concerns:\n- Gender neutrality assumption in the source language.",
            "6": "Though some nouns are lexically gender-neutral in English, document context could render these nouns gender-specific.",
            "7": "Relevant questions: 1) is context taken into consideration while filtering gender-neutral English sources?",
            "8": "2) did the survey include questions regarding source gender neutrality?",
            "9": "- Over-neutralization.",
            "10": "For specific nouns as in example C of Table 1, much less participants prefer the gender-neural alternative in Italian.",
            "11": "Is it necessary for these target translations to be gender-neutral?",
            "12": "Also in Table 2, why would we need to neutralize ii and iii for translations if the source English is already gendered?",
            "13": "- On multiple N references.",
            "14": "How frequently do the three translations overlap or are identical?",
            "15": "Have you considered multi-reference evaluations?",
            "16": "- Sentence vs. document level.",
            "17": "How are the sentence-level N-vs-G tags migrated, are they inherited directly from the document level?",
            "18": "- Problematic reference-free gender-neutral MT evaluation.",
            "19": "The binary N-G classification accuracy only evaluates the scenario when a gender-neutral noun is translated into Italian.",
            "20": "It does not provide any information on the overall translation quality of the documents/sentences thus not linearly dependent and cannot be compared proportionally to standard MT metrics, BLEU, TER, and METEOR.",
            "21": "Unfortunately, the much higher \"classifier\" number in Table 5 does not lead to the conclusion that the reference-free evaluation is \"promising\" (line 625).",
            "22": "Minor clarifications are necessary:\n- There is no concrete accuracy assessment on the two rounds of GPT-generated training data;\n- \"Grammatical gender languages\" is a confusing term in this paper.",
            "23": "Only concepts that have M-F-N cognates are examined in this paper, but not other nouns that have inherent gender in Italian, such as desks, beds, etc."
        },
        "DLMyO1oIbf": {
            "0": "The main strengths of this paper are: \n1.",
            "1": "It presents the first test set and benchmark for measuring how well MT systems do at appropriately generating gender-neutral language.",
            "2": "This will be a valuable resource for understanding this dimension of the translation task.",
            "3": "Before launching the evaluation task, they ran a study assessing the naturalness of language that had been edited to be gender neutral and how speakers reacted to the edited language.",
            "4": "This was an important step in justifying the subsequent work, showing that people typically appreciated the neutral language, and that it was not overly contorted.",
            "5": "They show that conventional MT metrics don't capture this dimension of the MT task well, and that other methods are needed.",
            "6": "They train a classifier that is effective at predicting the use of gender-neutral language.",
            "7": "The main weaknesses are:\n1.",
            "8": "The methods used to collect the test set data may not have been ideal.",
            "9": "E.g., they used a regex to find instances of gender-unambiguous language, including gendered pronouns.",
            "10": "However, in English, gendered pronouns are sometimes used in what should be gender-neutral circumstances (\"To each his own\").",
            "11": "They don't supply statistics about how difficult/natural/realistic it was to create the eval set.",
            "12": "I.e., we don't see how often the translators tasked with creating the eval set had difficulty in creating natural gender-neutral edited language, only that one example was so difficult to make gender neutral that only one of the translators attempted it.",
            "13": "Minor:\n1.",
            "14": "To create the benchmark set, they post-edited MT outputs to use more gender-neutral language, thus not a fully realistic scenario.",
            "15": "However, they discuss this in the limitations section, and it seems like a warranted compromise, given the extreme bias of MT systems to use gendered language.",
            "16": "The reference-free gender classifier doesn’t account for how good the translation is overall, and thus would not penalize translations that are incorrect other than their handling of gender."
        },
        "HqA9jA2kG3": {
            "0": "A thorough review of existing research in neutralization and gendered translations in MT.",
            "1": "A carefully constructed parallel corpus containing sentences that facilitate evaluation in both gender-ambiguous and gender-neutral scenarios.",
            "2": "A review of how existing reference-based metrics fare in detecting the quality of neutral translations, and their particular shortcomings.",
            "3": "For example, the fact that lower frequencies of neutral expressions affect the neural metrics more than the n-gram overlap-based ones, and the final finding that even n-gram metrics are not fully reliable at the sentence level.",
            "4": "A new, reference-free evaluation metric that accounts for the shortcomings of the reference-based ones.",
            "5": "The reasons for a new metric are well-motivated, but it is unclear how good the new metric is at gauging the quality of the resulting translation, aside from detecting whether a translation is gendered or not.",
            "6": "This new classification metric perhaps needs to be validated with the human study to make sure it's measuring the quality of translations as well.",
            "7": "Because of this reason, it is unclear what it means when the authors say \"the classifier outperforms\" (line 614).",
            "8": "This is something I might have misunderstood, in which case I welcome the authors to clarify further.",
            "9": "Although, I really appreciate the authors being honest about the thoughtful limitations section."
        }
    },
    "Jk6LA0NGOU": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the important and long-standing problem of logical reasoning in artificial intelligence.",
            "1": "- It introduces LEAP, a novel system that incorporates explicit planning into the inference procedure of language models for multi-step logical reasoning.",
            "2": "- The proposed system demonstrates significant improvements over existing methods on multiple standard datasets, highlighting the importance of explicit planning in logical reasoning tasks.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-motivated and novel approach to enhancing logical reasoning in language models through explicit planning.",
            "4": "- Extensive empirical studies are conducted, demonstrating the effectiveness of the proposed system on various datasets.",
            "5": "- The paper provides a detailed explanation of the methodology, including the integration of explicit planning and the training strategy to mitigate model exploitation.",
            "6": "- The results show that the proposed system significantly outperforms existing methods, including GPT-3, on challenging datasets like PrOntoQA.",
            "7": "Potential reasons for rejection\n   - **Computational complexity and efficiency:**\n     - The proposed system requires more computation than baseline methods, which may limit its practical applicability in real-world scenarios.",
            "8": "- The planning-based inference method is significantly slower than the no-planning method, which could be a concern for large-scale applications.",
            "9": "- **Generalization to other domains:**\n     - The paper primarily focuses on specific datasets, and it is unclear how well the proposed system generalizes to other domains or types of logical reasoning tasks.",
            "10": "- The experiments on PrOntoQA show that the verification model trained on Entailment Bank data does not generalize well to out-of-domain data, indicating potential limitations in the system's robustness.",
            "11": "- **Complexity of implementation:**\n     - The implementation of the proposed system involves multiple components and intricate details, which may pose challenges for reproducibility and adoption by other researchers.",
            "12": "- The reliance on large language models like GPT-3.5 and the need for prompt tuning add to the complexity and resource requirements of the system.",
            "13": "Suggestions for improvement\n   - **Enhance computational efficiency:**\n     - Explore methods to reduce the computational overhead of the planning-based inference, such as more efficient planning algorithms or techniques to switch between no-planning and planning-based methods dynamically.",
            "14": "- Investigate ways to optimize the implementation for faster inference without compromising the performance gains from explicit planning.",
            "15": "- **Improve generalization and robustness:**\n     - Conduct experiments on a wider range of datasets and logical reasoning tasks to evaluate the generalization capabilities of the proposed system.",
            "16": "- Develop strategies to enhance the robustness of the verification model when applied to out-of-domain data, such as domain adaptation techniques or additional training on diverse datasets.",
            "17": "- **Simplify implementation and reproducibility:**\n     - Provide detailed implementation guidelines, including code and hyperparameter settings, to facilitate reproducibility and adoption by other researchers.",
            "18": "- Consider creating a modular framework that allows for easier integration and experimentation with different components of the system, such as selection, deduction, and verification models.",
            "19": "- **Explore alternative approaches:**\n     - Investigate the potential of other reasoning frameworks, such as neuro-symbolic methods or hybrid approaches, to complement or enhance the proposed system.",
            "20": "- Experiment with different types of language models and architectures to assess their impact on the performance and efficiency of the logical reasoning system."
        },
        "VO4x9DK0Pr": {
            "0": "The idea of combining LM and planning is novel and meaningful.",
            "1": "The idea is supported by detailed description of the method and extensive experiments and analysis.",
            "2": "The method applies to both small models and large models, which is a practical advantage (although the cost may be higher than other methods).",
            "3": "This paper is not well-organized.",
            "4": "Much important information is presented in the appendix, like figures of models (fig.",
            "5": "6) and the results of analysis (fig.",
            "6": "8, 9).",
            "7": "There also seems to be two sections for related work (sec 3.4 and 5).",
            "8": "The baselines methods are limited.",
            "9": "For small models, the method is based on tuning, so comparing to prompted GPT-3.5 is not convincing.",
            "10": "Instead, the paper may include some tuning baselines for deductive reasoning like RuleTaker [1] and Neural Unification [2]."
        },
        "kggjBSDaju": {
            "0": "The method makes an interesting connection between logical reasoning and planning / contrastive learning\n2.",
            "1": "The empirical results are pretty strong - small models can match the performance of GPT-3 1.",
            "2": "The writing and presentation of the paper can be improved.",
            "3": "I didn't have any idea of the method until I really read through the method section.",
            "4": "The paper would benefit from first giving a high-level overview of the method at the beginning of the paper than just saying they do planning.",
            "5": "The method is quite complex, making people question whether this can really be deployed in the real-world.",
            "6": "However, I still think there is value in studying this.",
            "7": "Not too much of a concern for me."
        },
        "Nx2W4ShVJk": {
            "0": "The work presented is of high quality.",
            "1": "The paper is easy to read and the ideas are explained in a clear way.",
            "2": "All the necessary information to understand the method is provided.",
            "3": "The proposed method is particularly interesting and promising for tackling challenging planning and reasoning tasks, in particular the fact that the algorithm is model-agnostic and can be integrated with other LMs.",
            "4": "The authors provide very extensive experiments demonstrating the quality of their method.",
            "5": "The various ablation studies also clearly identify the proposed systems A and B as the reasons for the improved performance, highlighting precisely the contributions of each system.",
            "6": "The paper contains many references to the appendix as it contains important results that should be in the main paper, including a part of the conclusion.",
            "7": "The back and forth while reading can be detrimental to comprehension.",
            "8": "Fortunately, the information needed to understand the method and assess the claim is in the main content.",
            "9": "While the experiments are extensive, there is little comparison with existing methods.",
            "10": "It would be great to include a comparison with the state-of-the-art for each dataset beyond the single GPT-3.5.",
            "11": "Further comparison with other planning methods, such as those mentioned in the related work, might be interesting (as far as I understand, the only comparison is with the work of [Creswell et al., 2023])."
        }
    },
    "HewtRLig9V": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper explores the potential of Large Language Models (LLMs) for learning conceptual spaces, a cognitive-linguistic framework for representing the meaning of concepts.",
            "1": "- It addresses the limitations of traditional methods that rely on human judgments and are restricted to narrow domains.",
            "2": "- The study extends the evaluation of LLMs to the taste domain, which has not been previously considered in this context, and compares the performance of different models, including GPT-3, ChatGPT, GPT-4, and fine-tuned BERT models.",
            "3": "Potential reasons for acceptance\n   - The paper tackles an important and underexplored research question about the ability of LLMs to learn perceptually grounded representations.",
            "4": "- It provides a comprehensive evaluation of various LLMs and fine-tuned models across multiple domains, including taste, mass, size, and height.",
            "5": "- The results demonstrate that fine-tuned models can outperform larger LLMs, highlighting the importance of high-quality training data.",
            "6": "- The study introduces a novel dataset for the taste domain and a new prompting strategy for collecting training data from ChatGPT, contributing valuable resources to the research community.",
            "7": "Potential reasons for rejection\n   - **Limited generalizability of findings**\n     - The study focuses primarily on the taste domain and a few physical properties, which may not be representative of other perceptual domains.",
            "8": "- The conclusions drawn from the experiments may not be applicable to a broader range of concepts and properties.",
            "9": "- **Issues with experimental design and evaluation**\n     - The evaluation of ChatGPT and GPT-4 required manual post-processing due to issues with missing items, paraphrasing, and duplicates, which could introduce biases and affect the reliability of the results.",
            "10": "- The choice of prompts and the impact of prompt variations on the performance of LLMs are not thoroughly explored, leaving room for potential improvements in the experimental setup.",
            "11": "- **Insufficient analysis of training data overlap**\n     - The analysis of the overlap between pre-training and fine-tuning data for the DeBERTa model is limited, and the impact of this overlap on the model's performance is not fully addressed.",
            "12": "- The study does not provide a detailed comparison of the generated dataset with existing resources, which could help assess the quality and diversity of the training data.",
            "13": "Suggestions for improvement\n   - **Expand the evaluation to other perceptual domains**\n     - Include additional domains such as texture, sound, and smell to provide a more comprehensive assessment of the ability of LLMs to learn perceptually grounded representations.",
            "14": "- Compare the performance of LLMs across a wider range of concepts and properties to enhance the generalizability of the findings.",
            "15": "- **Improve the experimental design and evaluation**\n     - Develop automated methods for handling issues with missing items, paraphrasing, and duplicates in the responses of ChatGPT and GPT-4 to reduce the need for manual post-processing.",
            "16": "- Conduct a systematic exploration of different prompts and their impact on the performance of LLMs to identify optimal prompting strategies.",
            "17": "- **Enhance the analysis of training data overlap**\n     - Perform a more detailed analysis of the overlap between pre-training and fine-tuning data for all models to better understand the sources of their knowledge.",
            "18": "- Compare the generated dataset with existing resources in terms of quality, diversity, and coverage to provide a clearer assessment of its value and limitations.",
            "19": "- **Provide additional insights and discussions**\n     - Discuss the implications of the findings for the development of explainable AI systems and the potential applications of conceptual spaces learned by LLMs.",
            "20": "- Highlight the limitations of the current study and suggest directions for future research to address these limitations and build on the presented work."
        },
        "98MsKVb8OF": {
            "0": "- The paper is a nice demonstration of the capability of LLMs to learn semantic properties of objects.",
            "1": "- The paper provides potentially useful data on which properties are easier or harder to learn (eg bitterness is harder than sweetness).",
            "2": "- Models are prompted in a careful way, using probabilities of completions rather than just a sample, and using a ranking and a pairwise comparison methodology.",
            "3": "- The results are straightforward to interpret.",
            "4": "- The findings are unlikely to move the needle for people who are unconvinced that LLMs can meaningfully learn semantics.",
            "5": "- The authors are not able to determine exactly why the LLMs succeed whereas smaller ones failed."
        },
        "wfqtzZU6tJ": {
            "0": "There is a lot of debate and discussion right now around if/how LLMs can encoded information about the nonlinguistic world.",
            "1": "Connecting LLMs to theory on conceptual spaces is interesting and a worthwhile direction.",
            "2": "The authors also introduce a new dataset on taste which seems interesting and others might like to study.",
            "3": "I have some concerns/confusion around both the experimental design and the theoretical connections the authors want to make.",
            "4": "Theory:\n* I am struggling a bit with how/why we should be interpreting the results of the experiments as evidence that LLMs encode conceptual spaces, and even more so with what we should actually require in order for something to count as having a conceptual space.",
            "5": "From my understanding of the Gardenfors work, this is a line of distributional semantics theory that is notable because the dimensions of the vectors correspond to sensory (symbolic) primatives.",
            "6": "It is often (as I have cited it and seen it cited) contrasted directly with distributional semantics models from NLP which are derived from text, because by-definition the dimensions of the space for text-based DSMs reflect word co-occurances and not perceptual primitives.",
            "7": "So, this is to say: in some ways, no matter what, LLMs just by-definition are not conceptual spaces.",
            "8": "What makes a conceptual space a conceptual space is that it is not derived from text.",
            "9": "That said, I think your project could still be enlightening if you could show that, e.g., the space that the LLMs learn is _isomorphic_ to the conceptual space, similar to the Abdou et al paper.",
            "10": "This seems like it would be a very interesting finding.",
            "11": "But from what I can gather from your experiments, we cannot conclude that, since the experiments themselves just measure word co-occurances/LM probabilities directly.",
            "12": "Thus, I think from the presented experiments, all we can conclude is that LLMs representations are correlated with perceptual features.",
            "13": "This isn't necessarily news, but still could be worth documenting.",
            "14": "However, I would like to see the paper rewritten with more nuance around the conceptual spaces theory, especially if published without changes to the experiments.",
            "15": "Experimental design:\n* The methods you use to get rankings are arguably not measures of the representations directly (as you claim in the intro).",
            "16": "Rather, all your experiments rely on e.g., LM continuations or perplexity scores.",
            "17": "These don't tell us much about the representation of a word itself (e.g., the representation of the word \"cheese\") but rather tell us about how the model uses the representation to perform the task of language modeling.",
            "18": "To make claims about the representations themselves, e.g., to try to show an isomorphism between the LLM space and the conceptual space, I think you need to work with the word representations themselves.",
            "19": "This is not trivial in a multilayered contextualized LM (the way it used to be easy to do with e.g., word2vec).",
            "20": "But I think you could find a way.",
            "21": "Then, you might consider using something like relational similarity analysis (RSA) in order to measure whether the LLM space and the conceptual space have similar geometries.",
            "22": "* Roberta performs well when fine-tuned on McRae.",
            "23": "Have you looked at whether there is overlap between the facts included in McRae and those included in your test sets?",
            "24": "I wouldn't be shocked if McRae contains some facts about size or taste..."
        },
        "PcHCB6iwnY": {
            "0": "* This paper presents an interesting exploration of yet unexplored conceptual spaces.",
            "1": "I like this human-inspired analysis direction.",
            "2": "* The authors focus on an interesting commonsense problem that suffers deeply from reporting bias, and show where it is possible to trust LLMs.",
            "3": "* The paper is mostly well-written.",
            "4": "* The choice of models is satisfactory.",
            "5": "* Table 1: Some error analysis due to the surprising results would be much better than just stating them and acknowledging they are indeed surprising.",
            "6": "* As I did not fully understand Table 2, I cannot judge it.",
            "7": "* The conclusion \"The key to achieving good results with such smaller models is to have access to suitable training data.\"",
            "8": "is rather trivial.",
            "9": "I wonder what is the real contribution of this paper besides being an interesting analysis."
        }
    },
    "rXn9WO4M2p": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical gap in the field of language model (LM) pre-training by proposing a novel method, PRESENCE, for data reweighting using self-influence (SI) scores.",
            "1": "- The approach is innovative as it introduces a two-stage reweighting strategy that emphasizes influential samples early in training and de-emphasizes them later to stabilize learning.",
            "2": "- The method is evaluated extensively across multiple model sizes, datasets, and tasks, demonstrating its potential to improve pre-training efficiency and effectiveness.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a significant problem in LM pre-training, which is the uniform treatment of data samples regardless of their quality or relevance.",
            "4": "- The proposed method, PRESENCE, is novel and well-motivated, leveraging SI scores to dynamically reweight training samples.",
            "5": "- The experimental results are comprehensive and show consistent improvements over baseline methods, indicating the robustness and generalizability of the approach.",
            "6": "- The paper provides a thorough analysis of the relationship between SI scores and sample quality, adding valuable insights to the field.",
            "7": "Potential reasons for rejection\n   - **Computational Complexity**\n     - The method involves additional computational overhead due to the calculation of SI scores and the two-stage reweighting process.",
            "8": "- The implementation of microbatch reweighting increases training time by approximately 30%, which may not be feasible for all research or production environments.",
            "9": "- **Hyperparameter Sensitivity**\n     - The two-stage reweighting strategy relies on specific hyperparameters (e.g., temperature values, switching step), which may require extensive tuning for different datasets and models.",
            "10": "- The choice of layers for calculating SI scores is fixed, and the impact of using different layers or more layers is not explored in detail.",
            "11": "- **Scalability and Generalization**\n     - While the method shows improvements for the tested models and datasets, its scalability to even larger models and more diverse datasets is not fully addressed.",
            "12": "- The paper does not provide a clear strategy for automating the selection of reweighting parameters, which could limit its applicability in varied settings.",
            "13": "Suggestions for improvement\n   - **Reduce Computational Overhead**\n     - Explore methods to parallelize the computation of SI scores and gradient reweighting to reduce the additional training time.",
            "14": "- Investigate the use of more efficient approximations or heuristics for calculating SI scores to make the method more computationally feasible.",
            "15": "- **Hyperparameter Optimization**\n     - Provide a more detailed analysis of the sensitivity of the method to different hyperparameters and offer guidelines or automated strategies for their selection.",
            "16": "- Experiment with different layers or combinations of layers for calculating SI scores to understand their impact on the reweighting process.",
            "17": "- **Scalability and Generalization**\n     - Conduct experiments with larger models and more diverse datasets to validate the scalability and generalizability of the method.",
            "18": "- Explore the integration of PRESENCE with other pre-training strategies, such as curriculum learning or domain adaptation, to enhance its applicability.",
            "19": "- **Automated Reweighting Strategies**\n     - Develop and test automated reweighting strategies that dynamically adjust the temperature and switching step based on training progress or loss curves.",
            "20": "- Consider using continuous temperature scheduling or other adaptive mechanisms to further refine the reweighting process."
        },
        "tbtyFtVBhA": {
            "0": "- Using gradient norm/self-influence for weighting the samples during pre-training is a novel idea for NLP\n- The paper is generally well-written and easy to follow\n - Lack of comparison to other pre-training data filtering methods.",
            "1": "- Significant pre-training computation overhead.",
            "2": "- Detailed explanations for the above two reasons to reject: PRESENCE is essentially a data filtering method, so I expect to see the comparison with some baselines that filter the pre-training data.",
            "3": "**The current paper only shows that PRESENCE works, but it does not show it is better than other methods**.",
            "4": "As the authors admitted in the Limitation, PRESENCE induces 30% pre-training computation overhead.",
            "5": "While the paper says that \"`we believe the training overhead of PRESENCE is significantly lesser compared to the overhead of existing methods such as offline filtering`\", this is not justified by any experiments and numbers.",
            "6": "Moreover, offline filtering methods can be done once, and the filtered datasets can be used to train any models, while PRESENCE (the online version) needs to be applied every time when pre-training a model.",
            "7": "This makes PRESENCE less unlikely to be applied in reality.",
            "8": "- Some experiment results cannot convince me that PRESENCE is good enough.",
            "9": "The relationship between temperature $\\tau$ and the downstream performance shown in Figure 5 does not seem to justify the two-stage pre-training.",
            "10": "It seems that using a positive or negative $\\tau$ can yield improvement over no reweighting.",
            "11": "This makes me curious if it is really important to reweight using a positive or negative $\\tau$.",
            "12": "The results from Table 4 (cross-lingual zero-shot transfer) also do not show the advantage of using PRESENCE.",
            "13": "- It is unclear why this paper selects multilingual transferability as the downstream task for evaluation.",
            "14": "In the mT5 paper, they also show the results on SQuAD.",
            "15": "I wonder if PRESENCE only works for multilingual PLMs and cross-lingual transfer downstream tasks."
        },
        "Xy7l8N861Z": {
            "0": "Data reweighting is an important research problem for the pre-training language models, especially in today's emergence of large LM models.",
            "1": "The experimental study is comprehensive, and the results are promising.",
            "2": "The SI score is used to filter out noisy samples for pre-training dataset selection is practicable.",
            "3": "The source code has yet to be made available in the reviewed version, preventing me from verifying the effectiveness of this method through the code.",
            "4": "I am also uncertain whether the code will be made public after accepting the paper.",
            "5": "The method's current process is undeniably intricate, entailing the optimization of multiple pipelines.",
            "6": "These pipelines encompass sophisticated components, such as a two-phase learning strategy for online adaptation and a two-stage offline filtering procedure.",
            "7": "By embracing this sophisticated methodology, we doubt its  potential  availability in real-world applications.",
            "8": "(minor)  More than involving three tasks and five datasets is required to verify the effectiveness of this method.",
            "9": "I suggest expanding to more General Language Understanding Evaluation (GLUE) tasks."
        },
        "dNEz1YyPxD": {
            "0": "The paper explores the area of training on high quality samples, which helps with improved performance on downstream tasks.",
            "1": "Various sizes of the model were used for experimentation to demonstrate impact of the quality framework on the models.",
            "2": "The experimentation is limited to T5 based models, it will useful to carry out similar experimentation on other LMs"
        }
    },
    "54WhV6RTzi": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical task in medical imaging: generating radiology reports from medical images, which is time-consuming and requires high expertise.",
            "1": "- The proposed two-step approach separates content extraction from style generation, leveraging RadGraph and large language models (LLMs) to improve the clinical accuracy and stylistic customization of the reports.",
            "2": "- The novelty lies in the disentanglement of content and style, which addresses the limitations of previous end-to-end image-to-report models that often produce clinically inaccurate reports.",
            "3": "Potential reasons for acceptance\n   - The approach demonstrates significant improvements in clinical accuracy metrics (RadGraph F1, CheXbert similarity, RadCliQ) compared to the baseline.",
            "4": "- The method allows for the generation of reports tailored to individual radiologists' styles, enhancing readability and communication between medical professionals.",
            "5": "- The human evaluation shows that clinical experts could not distinguish AI-generated reports from real ones, indicating high-quality and realistic report generation.",
            "6": "- The paper provides a comprehensive evaluation using both quantitative metrics and human assessments, strengthening the validity of the results.",
            "7": "Potential reasons for rejection\n   - **Reliance on RadGraph accuracy:**\n     - The effectiveness of the approach heavily depends on the accuracy of the initial RadGraph extraction, which may be prone to errors, especially with rare medical entities or ambiguous observations.",
            "8": "- Any inaccuracies in the RadGraph extraction could propagate through the pipeline, affecting the final report's quality.",
            "9": "- **Dependency on third-party LLMs:**\n     - The use of OpenAI's GPT-3.5 model introduces financial costs and potential reproducibility issues, as the model may change over time.",
            "10": "- The reliance on a third-party service may limit accessibility and scalability for broader adoption in clinical settings.",
            "11": "- **Limited evaluation on diverse datasets:**\n     - The study primarily uses the MIMIC-CXR dataset, which may not represent the full diversity of radiology reports and imaging modalities.",
            "12": "- The approach's generalizability to other types of medical imaging, such as mammograms or MRIs, remains unexplored.",
            "13": "Suggestions for improvement\n   - **Enhance RadGraph extraction:**\n     - Improve the accuracy and robustness of the RadGraph extraction model, particularly for handling rare and ambiguous medical entities.",
            "14": "- Consider incorporating additional validation steps or expert reviews to ensure the quality of the extracted RadGraphs.",
            "15": "- **Explore alternative LLMs:**\n     - Investigate the use of open-source or locally deployable LLMs to reduce dependency on third-party services and associated costs.",
            "16": "- Evaluate the performance of different LLMs to identify the most suitable model for this task.",
            "17": "- **Expand dataset diversity:**\n     - Conduct experiments on a wider range of datasets, including different imaging modalities and clinical settings, to assess the approach's generalizability.",
            "18": "- Include datasets from various institutions to capture diverse reporting styles and clinical practices.",
            "19": "- **Address ethical considerations:**\n     - Develop guidelines and protocols to mitigate the risks of LLM hallucination and ensure the clinical safety of AI-generated reports.",
            "20": "- Engage with stakeholders, including radiologists and patients, to address concerns and improve the transparency and trustworthiness of the AI system."
        },
        "Rfxegx1Ned": {
            "0": "The proposed approach is sound and the utilization of RadGraph is well-motivated.",
            "1": "The idea is interesting.",
            "2": "The beneficial performance shown in the evaluations validates the effectiveness of the proposed approach.",
            "3": "Several claims should be proved.",
            "4": "- The work argues that existing methods, which generate reports directly from images, may lead to clinically inaccurate reports due to conflating content and style.",
            "5": "Could you give some examples or evidence to prove it?",
            "6": "- Why is it more accurate to write a report that matches a physician's style?",
            "7": "What is the advantages of usefulness of matching a physician's style?",
            "8": "More model details should be provided and clarified.",
            "9": "- How to serialize the RadGraph into a condensed summary of the clinical content of the report?",
            "10": "- How to ensure/define the order of entities in the generated Serialized RadGraph?",
            "11": "- For example, in Figure 2, how to obtain the \"acute cadiopulmonary process\"?",
            "12": "How to obtain the \"no\"?",
            "13": "- How to obtain the Serialized RadGraph from the Graph?",
            "14": "- What is the purpose of introducing a text encoder?",
            "15": "If I understand correctly that the input is just the image, what and where is the input text?",
            "16": "- What is the full input of GPT-3.5 under both few-shot and zero-shot settings?",
            "17": "Could you give an example to illustrate the full input and full output of GPT-3.5?",
            "18": "- How many examples are used to prompt GPT-3.5?",
            "19": "More experimental details should be provided and clarified.",
            "20": "- Could you give more details about the Image to Serialization task?",
            "21": "What are the input and output of this task?",
            "22": "What is the meaning of comparing the content of the generated serialization with the ground-truth report?",
            "23": "Why is only the F1 score calculated?",
            "24": "What are the results of other metrics, e.g., BELU?",
            "25": "Therefore, it is important to clarify what are the input and output of this task.",
            "26": "- In Table 2, what is the baseline model?",
            "27": "Pure Transformer or R2Gen?",
            "28": "How many Transformer layers and the number of parameters are used in the baseline?",
            "29": "- Besides, while the paper reports beneficial performance, a detailed comparison with more existing image-to-report methods is missing.",
            "30": "Including such a comparison would provide a clearer understanding of the proposed approach's advantages.",
            "31": "- When evaluating the AI-generated reports, are the physicians given the test set CXR images?",
            "32": "- The criteria used in human study is subjective.",
            "33": "Authors are encouraged to introduce more information about three clinicians, i.e.",
            "34": "working experience and expert title.",
            "35": "Senior and junior clinicians may find differences when considering whether the same report is AI-generated or not.",
            "36": "- I strongly recommend the author to further evaluate the quality of the generated reports, instead of asking the physician to judge whether the report is AI-generated or not."
        },
        "1E3BHKUaPU": {
            "0": "The authors attempted to address the issue of clinical report generation from a new perspective in their methodology, and they achieved results indicating that ChatGPT can reference style well enough to generate reports that are indistinguishable from those written by humans.",
            "1": "This study doesn't persuasively explain \"why style should be considered in the medical domain\" as proposed by this study, and it doesn't sufficiently explore the clinical importance of such a consideration.",
            "2": "Experimental results lack comparisons with other studies.",
            "3": "(It's necessary to verify statistically significant results by conducting repeated experiments on the same data set.)",
            "4": "While this study incorporates a few ideas and uses RadGraph and ChatGPT, it does not offer considerable novelty in its methods or contribute significantly to the field."
        },
        "Ra70Uv3Ivg": {
            "0": "\n- The authors propose a straightforward yet effective approach that involves converting raw reports into structured data.",
            "1": "This process aids in the removal of non-semantic content.",
            "2": "- They also introduce a style-aware report generation technique and carry out a human evaluation to assess its effectiveness.",
            "3": "- Easy to follow - Is there no loss of information from the Serialization to Report process?",
            "4": "What are your thoughts on the reason behind the slight difference in performance, with a RadGraphF1 of 0.221 ± 0.004 for Table1 and 0.228 ± 0.004 for Table2?",
            "5": "- In the Image Encoder section (Section 3.4), it is mentioned that if there are multiple images in one study, they are aggregated.",
            "6": "It would be beneficial to evaluate the test performance based on the number of images present in each study, showcasing the performance for cases where a study contains one image, two images, and so on.",
            "7": "- In the Content Generation Model section (Section 3.4), it is mentioned that the Text Encoder receives the clinical document as input, but the specific details are not provided.",
            "8": "- It would be valuable to experiment with both the Baseline and the Proposed Model without the Text Encoder and share the results.",
            "9": "This paper does not propose a new architectural design but shows the effectiveness of serialized data.",
            "10": "Given this focus, I think that showcasing results across various scenarios would be valuable to illustrate the impact more comprehensively.",
            "11": "- In the 4.2 Serialization to Report experiment, the zero-shot performance is observed to be 0.722 RadGraphF1.",
            "12": "Despite using ground-truth serialization, the performance is not nearly perfect.",
            "13": "Is it plausible to attribute this discrepancy to the fact that the performance of the content extractor (Report -> RadGraph preprocessing) might not be flawless?"
        }
    },
    "wWFWwyXElN": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenge of data scarcity in multilingual commonsense reasoning datasets by leveraging Large Language Models (LLMs) for data augmentation.",
            "1": "- It explores the use of several LLMs, including Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to generate synthetic data for three specific datasets: XCOPA, XWinograd, and XStoryCloze.",
            "2": "- The study evaluates the effectiveness of fine-tuning smaller multilingual models (mBERT and XLMR) using the generated data and compares the performance with models trained on limited human-created data.",
            "3": "- The paper also includes a human evaluation to assess the naturalness and logical coherence of the generated examples across different languages.",
            "4": "Potential reasons for acceptance\n   - The paper addresses a significant challenge in multilingual NLP by proposing a novel approach to data augmentation using LLMs.",
            "5": "- It provides a comprehensive evaluation of the generated data's impact on model performance, demonstrating notable improvements in accuracy.",
            "6": "- The inclusion of multiple LLMs and datasets adds robustness to the findings and highlights the generalizability of the approach.",
            "7": "- The human evaluation component adds credibility to the results by assessing the quality of the generated data from a linguistic and logical perspective.",
            "8": "- The release of the generated datasets for public use promotes reproducibility and further research in the field.",
            "9": "Potential reasons for rejection\n   - **Limited evaluation on extremely low-resource languages:**\n     - The paper acknowledges that LLMs may struggle with extremely low-resource languages, but it does not provide a detailed analysis or solutions for these cases.",
            "10": "- The evaluation focuses on a subset of languages, potentially overlooking the performance in other low-resource languages.",
            "11": "- **Dependence on few-shot examples in target languages:**\n     - The approach requires few-shot examples in the target language for generating new examples, which may not always be feasible for all languages.",
            "12": "- The paper does not explore alternative methods for generating data without relying on few-shot examples.",
            "13": "- **Cost and accessibility of advanced models:**\n     - The use of closed models like GPT-4 is limited by licensing restrictions and high costs, which may hinder reproducibility and accessibility for other researchers.",
            "14": "- The paper does not provide a detailed cost analysis or discuss the feasibility of using more accessible models for similar tasks.",
            "15": "- **Inconsistent performance across languages:**\n     - The results show varying performance improvements across different languages, with some languages experiencing performance degradation.",
            "16": "- The paper does not thoroughly investigate the reasons behind these inconsistencies or propose solutions to address them.",
            "17": "Suggestions for improvement\n   - **Expand evaluation to more low-resource languages:**\n     - Include a broader range of low-resource languages in the evaluation to better understand the limitations and potential of the proposed approach.",
            "18": "- Provide a detailed analysis of the challenges faced by LLMs in generating data for these languages and explore potential solutions.",
            "19": "- **Explore alternative data generation methods:**\n     - Investigate methods for generating synthetic data without relying on few-shot examples in the target language, such as leveraging cross-lingual transfer learning or unsupervised techniques.",
            "20": "- Compare the performance of these methods with the current approach to identify the most effective strategies for different languages.",
            "21": "- **Address cost and accessibility concerns:**\n     - Provide a detailed cost analysis of using advanced models like GPT-4 and discuss the feasibility of using more accessible models for similar tasks.",
            "22": "- Explore the potential of open-source models and recent instruction-tuned or aligned LLMs, such as LLaMA 2 or TÜLU, for data augmentation.",
            "23": "- **Investigate performance inconsistencies:**\n     - Conduct a thorough investigation into the reasons behind the varying performance improvements across different languages.",
            "24": "- Propose and test solutions to address these inconsistencies, such as fine-tuning the data generation process or incorporating additional linguistic features.",
            "25": "- **Enhance human evaluation component:**\n     - Include a larger and more diverse set of languages in the human evaluation to provide a more comprehensive assessment of the generated data's quality.",
            "26": "- Consider incorporating additional evaluation criteria, such as cultural relevance and contextual appropriateness, to further validate the generated data's usefulness."
        },
        "mGkOBEKrQ4": {
            "0": "\n1.",
            "1": "The paper focuses on an important research problem in NLP: multilingual commonsense reasoning.",
            "2": "In the last few years, large language modeling research has primarily focused on English.",
            "3": "It's important to see how the benefits from English large language models can be used in other lower resource languages, and this paper does a good job studying this research question on commonsense reasoning tasks.",
            "4": "This paper has a comprehensive experimental setup, and experiments with 3 commonsense reasoning datasets, three multilingual models, and experiments with data generated by 3 large language models.",
            "5": "The paper has experiments generating English examples (zero-shot cross lingual generalization), translated examples, as well as examples in the language itself.",
            "6": "The paper also has some good analysis with human evaluation experiments on the quality of the generated examples.",
            "7": "This paper is very well written, and easy to follow.",
            "8": "The authors extensively discuss and are upfront about the limitations of their work.",
            "9": "I am wondering what is the effect of these data augmentation methods when the base model is made larger / more powerful.",
            "10": "The current experiments focus on fairly small multilingual models (mBERT / XLM), which are <500M parameters.",
            "11": "It would be interesting to see how these results generalize to larger models like mT5 or BLOOM.",
            "12": "This is important because it's possible that these larger models need lesser examples to generalize, and data augmentation will not as useful then.",
            "13": "A related point to weakness #1, the paper is missing baseline experiments which use the large language models themselves to perform the task, through few-shot learning or fine-tuning on original data using the GPT3 fine-tuning API (please me know in case I missed these experiments while reading the paper!).",
            "14": "Since models like ChatGPT / GPT-4 are effectively used to create training data for downstream fine-tuning, I'm suspecting they can already perform the task pretty well?",
            "15": "This has been acknowledged by the authors in their limitations section, but there are several commerical restrictions on using ChatGPT/GPT4 outputs, [1] says \"one may not use output from the Services to develop models that compete with OpenAI\".",
            "16": "This may restrict the applicability of the proposed approach for downstream applications.",
            "17": "In future versions of the paper, it will be helpful to see how stronger instruction-tuned / aligned open LLMs like LLAMA 2 [2] or TULU [3] can help with the data augmentation.",
            "18": "However, note that both these models came out in the last two months (during / after EMNLP submission period), so this is not a valid weakness for the current version of the paper.",
            "19": "Finally, in the long term I am not very convinced by using one language model \n\n[1] - https://openai.com/policies/terms-of-use  \n[2] - https://huggingface.co/blog/llama2  \n[3] - https://arxiv.org/abs/2306.04751"
        },
        "0D3s3Do2fs": {
            "0": "The paper is well-motivated and studies an important problem of performing data generation in multilingual, low-resource settings for a challenging task of commonsense reasoning.",
            "1": "Although the general-purpose LLMs exhibit impressive zero-shot learning capabilities, smaller task-specific models can often be more accurate, cost-effective, and practical, given that enough training data is available.",
            "2": "Therefore, improving the accuracy of task-specific models by generating synthetic data for fine-tuning is still an important research topic.",
            "3": "One of the weaknesses of the presented approach is that it does not provide a mechanism to control the diversity of the synthesized examples.",
            "4": "Although the authors claim that their method generates diverse examples (lines 61-62), it has neither been supported empirically nor discussed in the paper.",
            "5": "Notably, the results presented in Section 5.3 suggest that the benefits of generating larger amounts of data are rather limited, possibly due to the low diversity of the synthesized examples.",
            "6": "In addition, the clarity of the paper needs to be improved.",
            "7": "Specifically, the authors generated 2-4k new data points to fine-tune the models for each data set.",
            "8": "It is unclear whether all instances were synthesized in one inference step or the models were prompted multiple times for each data set.",
            "9": "Moreover, given that the number of valid examples obtained with different models differs substantially (see the success rates in Table 3), it needs to be clarified why an equal number of synthesized data points is reported for each LLM used for generation.",
            "10": "Presumably, the models were prompted multiple times until the required number of examples was reached, but the paper needs to explain this.",
            "11": "Generating small batches of examples by prompting the model multiple times, each time using a different set of seed examples, might also improve the diversity and, in turn, the quality of the synthesized data.",
            "12": "Another point is that the choice of the number of synthesized samples per data set should be discussed in the paper.",
            "13": "The number of examples is neither fixed nor proportional to the size of the corresponding original data set.",
            "14": "For XWinograd, which has the largest training set out of the examined benchmarks, additional 2k examples are generated, which doubles its size.",
            "15": "In contrast, the size of XCOPA and XStoryCloze increases 10 and  6.7 times, respectively.",
            "16": "Finally, the authors need to discuss their method's effectiveness in comparison with other data augmentation approaches.",
            "17": "Note that after deduplication and discarding invalid instances, only up to 42% of examples are retained (in the case of open-access models).",
            "18": "The usage of closed generative LLMs might also not be cost-effective.",
            "19": "The reader would benefit from a more in-depth discussion of the advantages and disadvantages of the proposed method.",
            "20": "The limitations section needs to be updated accordingly."
        },
        "yecuuLWASQ": {
            "0": "- well written paper, with well defined methodology and strong evaluation\n- promising experimental results\n- resources and findings that are useful to the community None, it's a very solid paper."
        }
    },
    "qo17ZiVnH2": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical challenge in Vision-Language tasks, specifically in Visual Question Answering (VQA), by proposing a novel framework that enables Large Language Models (LLMs) to proactively ask questions to fill the information gap caused by image-to-text conversion.",
            "1": "- The proposed method demonstrates a significant improvement in performance on OK-VQA and A-OKVQA datasets, showcasing its potential to enhance the reasoning capabilities of LLMs in vision-language tasks.",
            "2": "Potential reasons for acceptance\n   - The paper presents a novel approach that leverages the proactive questioning ability of LLMs to gather missing information, which is a significant advancement in the field of VQA.",
            "3": "- The experimental results show consistent improvements across different LLMs and datasets, indicating the robustness and effectiveness of the proposed method.",
            "4": "- The framework is model-agnostic and can be applied to existing baselines, making it a versatile solution for enhancing VQA performance.",
            "5": "- The paper provides a comprehensive analysis, including ablation studies and case studies, to validate the effectiveness of the proposed method.",
            "6": "Potential reasons for rejection\n   - **Limited scope of evaluation:**\n     - The paper primarily focuses on OK-VQA and A-OKVQA datasets, which may not fully represent the diversity of vision-language tasks.",
            "7": "- The applicability of the proposed method to other vision-language tasks, such as visual commonsense reasoning, is not explored.",
            "8": "- **Complexity of the framework:**\n     - The proposed framework involves multiple components, including question generation, summarization, and refinement modules, which may increase the complexity of implementation and integration with existing systems.",
            "9": "- The reliance on multiple pre-trained models (e.g., BLIP-2, CLIP) and the need for training the refinement module may pose challenges for practical deployment.",
            "10": "- **Potential noise in generated questions:**\n     - The paper acknowledges that the generated questions and answers can be noisy, which may affect the overall performance.",
            "11": "- The effectiveness of the refinement module in filtering out irrelevant or misleading information is crucial, and any shortcomings in this process could impact the final results.",
            "12": "Suggestions for improvement\n   - **Expand the evaluation to other vision-language tasks:**\n     - To demonstrate the generalizability of the proposed method, evaluate its performance on a broader range of vision-language tasks, such as visual commonsense reasoning or image captioning.",
            "13": "- **Simplify the framework:**\n     - Explore ways to simplify the framework by reducing the number of components or integrating some of the steps to streamline the process.",
            "14": "- Investigate the possibility of using fewer pre-trained models or reducing the dependency on external resources to make the framework more practical for real-world applications.",
            "15": "- **Enhance the refinement module:**\n     - Improve the refinement module to better handle the noise in generated questions and answers, possibly by incorporating more sophisticated filtering techniques or leveraging additional context.",
            "16": "- Conduct further experiments to fine-tune the refinement process and ensure that only the most relevant and accurate information is retained.",
            "17": "- **Provide more detailed analysis:**\n     - Include a more detailed analysis of the types of questions generated by the LLMs and their impact on the final performance.",
            "18": "- Discuss the limitations and potential failure cases of the proposed method to provide a more comprehensive understanding of its strengths and weaknesses."
        },
        "iL0PiBaWya": {
            "0": "This paper convincingly illustrates the necessity of providing nuanced, query-specific information to complement the coarse, generalized image captions diretly output from caption model without any query information.",
            "1": "It recognizes a shortcoming in existing approaches that convert image to text for application in large language models to solve visual question-answering tasks.",
            "2": "The one-off conversion process often loses important details, underscoring the importance of query-related information to infuse sufficient visual details into the Language Learning Model .",
            "3": "The proposed framework is thoughtfully and intuitively designed, reflecting an in-depth understanding of the problem at hand.",
            "4": "The research is supported by thorough and comprehensive experiments.",
            "5": "It includes a comparative analysis with most related works, and an exhaustive ablation study, all of which affirm the effectiveness of the proposed method.",
            "6": "The study presents a commendable motivation and idea, yet the implementation of the method seems disproportionately complex given the relatively marginal improvement.",
            "7": "The method encompasses several modules, including a caption model, VQA model, filter, and large language models.",
            "8": "However, the advancement over 'Prophet' does not appear to warrant the complexity and increased inference time of the method.",
            "9": "The articulation of the method and experiments is not clear.",
            "10": "For instance, (1) at line 214, the method for calculating scores in Table 1 is unclear; (2) lines 264-265 lack sufficient detail, relegating crucial information to the appendix and leaving the motivation and methods for obtaining ground-truth VQA accuracy unclear; in Appendix A, P_{i,q} and ACC_{soft} aren't sufficiently explained; (3) at lines 275-276, it is unclear how z_q is calculated.",
            "11": "While Equation 8 appears to use all information, it remains unexplained if only 'q' is used to get z_q; (4) in Section 3.3, the example prompt is not consistent with Figure 2, where the figure does not display `caption`; (5) the 'Prophet' results in Tables 2 and 3 are inconsistent; (6) for Table 4, the application of answers from 'Blip2' remains unclear.",
            "12": "Are they used for few-shot examples?",
            "13": "If so, how does 'ourts-w/o' function?",
            "14": "(7) Table 4 should provide an explanation for the meaning of 'n' and 'E' in its caption; (8) the statement at lines 414-415 claiming an improvement from 40.7 to 47.63 seems unreasonable, as 40.7 directly uses 'Blip2', a VLM, while 47.63 uses a different method involving LLM.",
            "15": "The improvement could be attributed not just to more detailed information but also enhanced reasoning ability.",
            "16": "In the case study, Figure 3 (1) suggests that 'Prophet's' output of 'grilled cheese' with a probability of 0.66 should indeed be the correct answer"
        },
        "FQmhsFMBJx": {
            "0": "- Interesting design choice requiring very minimal training of interfacing LLM and VLMs, instead of training adapters for image input to llm like llava etc.",
            "1": "- The accuracies on the VQA datasets look promising.",
            "2": "- They show their approach holds with various LLMs.",
            "3": "- The results section could be organized better.",
            "4": "Some lines like “Line (f) converts line (h) to few-shot setting” - are lacking details.",
            "5": "What does it exactly mean?",
            "6": "Fewer context?",
            "7": "Fewer VQA training data points?",
            "8": "How are they sampled?",
            "9": "- Another baseline to include is the accuracy of the LLM to answer the question without any image information or just the caption.",
            "10": "Table 6 compares the accuracy to using BLIP-2 in answering the question.",
            "11": "Since their final method has the LLM answering the question, it would be nice to see how much the BLIP and filtered information is helping the LLM."
        },
        "yB2wLxzrDs": {
            "0": "The idea of this paper is relatively interesting, proposing a method that leverages the interaction between LLM (Language Large Models) and VLM (Vision-Language Models) to obtain finer-grained image information, which in turn aids visual reasoning tasks.",
            "1": "It offers the research community a new perspective on interactive use with large models.",
            "2": "Simultaneously, it addresses, to some extent, the current limitation of LLMs being unable to access visual information.",
            "3": "The experiments in the article are comprehensive, with rich visualization results.",
            "4": "These contents validate the points proposed in the paper.",
            "5": "The performance comparisons and ablation studies demonstrate the efficacy of the proposed method.",
            "6": "Furthermore, tasks like OKVQA and AOKVQA are inherently challenging.",
            "7": "The method presented in this paper achieves competitive results on these tasks, further underscoring its effectiveness.",
            "8": "The method put forward in the paper is not only effective but also intuitive, easy to replicate, and clear-cut.",
            "9": "Generally speaking, I believe the novelty is somewhat lacking.",
            "10": "For instance, earlier this year (published on arxiv on March 12th, more than three months before the EMNLP deadline), there was a paper titled \"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions\" that employed an approach similar to what you've proposed, aiming to capture more visual information to produce image captions with richer details.",
            "11": "Admittedly, in terms of the overall workflow, the structure of your paper seems more comprehensive, encompassing broader considerations.",
            "12": "However, on the whole, I don't feel surprised when I read your paper, as the core content bears resemblances to previous work.",
            "13": "I believe you should explore one or two additional vision-language tasks.",
            "14": "As you've acknowledged in the limitation section, your experiments solely focus on OKVQA and AOKVQA tasks.",
            "15": "Essentially, these two tasks are quite similar, with AOKVQA being a more advanced version of OKVQA.",
            "16": "Based on the results in Table 3, the performance of OKVQA Prophet (ensemble) only improves by a mere 0.2% with the addition of your proposed method, which hardly convinces me of its efficacy in this context.",
            "17": "A 0.2% increment could even be attributed to minor experimental errors, rendering the results somewhat dubious.",
            "18": "Hence, rather than tackling both AOKVQA and OKVQA, it would be more sensible to solely concentrate on AOKVQA.",
            "19": "On the one hand, the outcomes from Table 3 for AOKVQA appear reasonable and credible; on the other hand, it avoids the redundancy of undertaking two similar tasks.",
            "20": "If a wider array of tasks were addressed, incorporating both wouldn't be an issue, but with just these two, their similarity is more glaring.",
            "21": "Therefore, if you decide to pursue two tasks, I'd recommend focusing on Visual Commonsense Reasoning (VCR https://visualcommonsense.com/) and AOKVQA.",
            "22": "I noticed that you mentioned VCR in the limitation section as well.",
            "23": "Personally, I'm more intrigued by the combination of VCR and AOKVQA.",
            "24": "I believe readers could glean more insights from these two tasks, given the significant differences in task format (direct generation vs. multiple-choice) and textual content between VCR and AOKVQA.",
            "25": "Such diverse content seems far more engaging to me.",
            "26": "Given the need to extract more detailed image information for enhanced visual reasoning, why haven't you considered dense captioning?",
            "27": "Would using dense captioning yield favorable results?",
            "28": "(You can refer to the approach on https://github.com/JialianW/GRiT) Naturally, integrating dense captioning might introduce the problem of hallucination, making the outcomes potentially noisier.",
            "29": "Despite the possibility of such issues arising, I'd like to see some experiments incorporating dense captioning, whether in the main content or the appendix.",
            "30": "This is a frequently discussed question.",
            "31": "Your work stems from the perspective that LLM cannot visualize images, which is indeed correct at present.",
            "32": "However, with the evolution of MLLM and the anticipated unveiling of GPT-4's multimodal capabilities, the value of your work is bound to face significant challenges.",
            "33": "From my viewpoint, I'd expect authors working on LLM Aided visual reasoning to proactively address the challenges presented by GPT-4 and its implications on their research.",
            "34": "However, I haven't observed this aspect in your work."
        }
    },
    "ILQnct9H4H": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces TRIGO, a new benchmark dataset for evaluating the formal mathematical proof reduction capabilities of generative language models, specifically focusing on trigonometric expressions.",
            "1": "- It addresses a gap in current Automated Theorem Proving (ATP) benchmarks, which primarily focus on symbolic inference and lack complex numerical reasoning tasks.",
            "2": "- The dataset includes both manually annotated real-world problems and automatically generated problems, providing a comprehensive evaluation tool for generative language models.",
            "3": "Potential reasons for acceptance\n   - The introduction of a novel benchmark dataset that fills a gap in the current ATP benchmarks.",
            "4": "- The thorough methodology for data collection, annotation, and formalization into the Lean formal language system.",
            "5": "- The extensive experiments conducted to evaluate the performance of state-of-the-art generative language models on the TRIGO dataset.",
            "6": "- The detailed analysis of the models' performance, including the impact of pre-training, proof search methods, and expert iteration.",
            "7": "Potential reasons for rejection\n   - **Limited generalization to real-world data:**\n     - The models trained on the generated data (TRIGO-gen) show a significant performance drop when evaluated on real-world data (TRIGO-real and TRIGO-web).",
            "8": "- The generated data may not fully capture the complexity and diversity of real-world trigonometric problems, leading to limited generalization.",
            "9": "- **Challenges with \"have\" tactics:**\n     - The models struggle with generating the correct \"have\" tactics, which are crucial for complex number combination reasoning.",
            "10": "- The performance gap between tactics with and without \"have\" tactics indicates a significant challenge in the models' ability to handle these steps.",
            "11": "- **Out-of-distribution generalization:**\n     - The models perform poorly on datasets with larger unseen numbers, revealing limitations in their numerical reasoning capabilities.",
            "12": "- The inability to generalize to more complex numerical values suggests that the current models may not be robust enough for real-world applications.",
            "13": "Suggestions for improvement\n   - **Enhance the diversity and complexity of generated data:**\n     - Improve the automatic data generation process to better mimic the complexity and diversity of real-world trigonometric problems.",
            "14": "- Incorporate more varied and challenging problems to enhance the models' generalization capabilities.",
            "15": "- **Improve handling of \"have\" tactics:**\n     - Develop specialized training techniques or additional pre-training tasks focused on generating correct \"have\" tactics.",
            "16": "- Explore alternative methods for teaching models to handle complex number combinations and intermediate steps.",
            "17": "- **Address out-of-distribution generalization:**\n     - Investigate techniques to improve the models' ability to generalize to larger and more complex numerical values.",
            "18": "- Consider incorporating additional training data or tasks that specifically target numerical reasoning and manipulation.",
            "19": "- **Refine evaluation metrics:**\n     - Introduce more nuanced evaluation metrics that go beyond the \"no goals\" output to assess the models' ability to generate diverse and correct proof paths.",
            "20": "- Consider metrics that evaluate the quality and efficiency of the generated proofs, not just their correctness."
        },
        "MXGGrOgjKr": {
            "0": "- The paper is very well-written and well motivated\n- TRIGO represents an extensive resource that can support future research on the evaluation and improvement of the mathematical and symbolic reasoning capabilities of Language Models and Machine Learning approaches in general.",
            "1": "- The paper proposes an original methodology for data generation that combines human experts with a formal environment.",
            "2": "The proposed methodology can serve as an inspiration for future work in the field.",
            "3": "- Extensive empirical evaluation on state-of-the-art models.",
            "4": "- I believe the paper would benefit from a deeper discussion on how the quality of the generated dataset is verified.",
            "5": "For example, human annotation is involved, but I could not find any discussion or report of inter-annotator agreement and quality check.",
            "6": "- The evaluation is performed using a single family of generative models (i.e.",
            "7": "GPT).",
            "8": "As GPT4 is not freely available to the community and much of the details are unknown, I believe the experiments would benefit from an additional comparison between open models (for instance comparing GPT2 with T5).",
            "9": "*Update*: This limitation is partially addressed in the authors's rebuttal with new results."
        },
        "HmdA5gKshB": {
            "0": "The paper contributes new data resources for mathematical proof reduction.",
            "1": "Trigonometry seems to be a relatively underexplored area in proof datasets.",
            "2": "The paper presents relatively thorough experiments to understand the gaps between the curated and generated datasets.",
            "3": "The paper also assess the impact of different proof search algorithms, expert iteration, and an expanded set of angles.",
            "4": "Although it’s a well-curated dataset and it’s always important to contribute additional benchmarks, the specifics of the impact are not clear to me from reading the paper alone.",
            "5": "Is the automatic proof generation the main methodological contribution?",
            "6": "Is it possible to train on Trigo and have performance improve on other proof benchmarks?",
            "7": "Some of the steps of the dataset curation process could be described with more precision.",
            "8": "* Line 203: “To expand our dataset, we further collect additional trigonometry reduction problems from different websites.” It seems like only a small percentage of problems were sourced from outside Tiku.",
            "9": "It would be great to specify the websites, the number from each website, and why additional sourcing was needed, since it seems that the total dataset size did not increase by much.",
            "10": "* Section 4.2 It would be great to show some examples, such as screenshots, of the annotation software and flow.",
            "11": "It’s also not clear if the annotators during this step are the authors themselves, or if there were other annotators.",
            "12": "* It would be great to attach an appendix to Section 4.3 to clarify on which additional steps that PhD students had to fix.",
            "13": "And again, it would be great to specify how they differ from the initial annotator pool."
        },
        "OSFNnKgb4Q": {
            "0": "- Studying mathematical reasoning capabilities of LLMs is an important research direction\n- Useful datasets are rare, the authors contribute to the data scarcity problem\n- I see the most value of this dataset in the manual annotation effort done by Ph.D. students (page 4, line 288+)\n- Experimental results show that the dataset can be used to effectively train even small LLMs; promising results - Some of the results are not surprising (with more human-level input, the model will improve in distribution)\n- The dataset is limited in size, making it only usable for fine-tuning\n- The comparison to GPT-4 is unfair, this needs to be more clearer in the paper (because despite the introduction/abstract, I doubt that GPT-4 has seen that much Lean)"
        }
    },
    "7LBhEJ1DII": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in quantitative social science by proposing a novel method for character similarity measurement using Vision Transformers (ViT).",
            "1": "- The approach is innovative as it leverages self-supervised learning and augmented digital fonts to create a metric space for character similarity, which is particularly useful for OCR'ed documents.",
            "2": "- The study's focus on CJK scripts and its extensibility to low-resource settings and ancient scripts adds significant value to the field.",
            "3": "Potential reasons for acceptance\n   - The method shows a clear improvement over existing string matching techniques, as demonstrated by the empirical results.",
            "4": "- The approach is highly extensible and can be applied to various scripts, including ancient ones, which broadens its applicability.",
            "5": "- The paper provides a comprehensive evaluation using both real-world and synthetic datasets, showcasing the robustness of the proposed method.",
            "6": "- The authors have made their implementation publicly available, which promotes reproducibility and further research.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper could benefit from a more detailed explanation of the training process and the specific augmentations used.",
            "8": "- The description of the hard-negative mining process and its impact on the results is somewhat vague and could be elaborated.",
            "9": "- **Limited comparison with state-of-the-art methods:**\n     - While the paper compares the proposed method with several string matching techniques, it does not include a comparison with more recent deep learning-based OCR correction methods.",
            "10": "- The performance of the proposed method in comparison to end-to-end deep neural models is not thoroughly discussed.",
            "11": "- **Insufficient discussion on limitations:**\n     - The paper briefly mentions the limitations of string matching methods but does not provide an in-depth analysis of the potential drawbacks of the proposed approach.",
            "12": "- The impact of different OCR engines on the performance of the proposed method is not fully explored.",
            "13": "Suggestions for improvement\n   - **Enhance methodological clarity:**\n     - Provide a more detailed explanation of the training process, including the specific augmentations and their parameters.",
            "14": "- Elaborate on the hard-negative mining process and its role in improving the model's performance.",
            "15": "- **Expand comparison with state-of-the-art methods:**\n     - Include a comparison with recent deep learning-based OCR correction methods to provide a more comprehensive evaluation of the proposed approach.",
            "16": "- Discuss the performance of the proposed method in relation to end-to-end deep neural models, highlighting the trade-offs and potential advantages.",
            "17": "- **In-depth analysis of limitations:**\n     - Provide a more thorough discussion of the limitations of the proposed method, including potential issues with scalability and generalizability.",
            "18": "- Explore the impact of different OCR engines on the performance of the proposed method and discuss any observed variations.",
            "19": "- **Improve readability and structure:**\n     - Consider reorganizing the paper to improve the flow of information, making it easier for readers to follow the methodology and results.",
            "20": "- Add more visual aids, such as diagrams and flowcharts, to illustrate complex processes and enhance understanding."
        },
        "VHKRTEI5SG": {
            "0": "I think the topic is interesting.",
            "1": "The main technical component of the paper is an application of contrastive learning with multiple fonts to learn the similarity between two characters.",
            "2": "The way the method uses contrastive learning looks straightforward, so the technical contribution is limited.",
            "3": "Details of string matching over the proposed similarity metric are not provided.",
            "4": "This may significantly reduce the reproducibility of the method if it does not publish the code.",
            "5": "Some minor points:\n\nThe paper is redundant, repeating similar sentences.",
            "6": "This makes the paper hard to read.",
            "7": "The paper claims that the method to train the similarity metric is self-supervised, but for me, this is not self-supervised because it has labels to represent the same characters."
        },
        "2HFs6DntKM": {
            "0": "The method is simple but looks very useful and straightforward to reproduce.",
            "1": "A quick reference (p. 3 column 1 second paragraph) suggests they have a package for distribution.",
            "2": "The new method outperforms previous string matching algorithms on this task.",
            "3": "They make the believable claim that previous methods (e.g.",
            "4": "end-to-end neural networks) may have somewhat higher accuracy but are costly to (re-)tune for each use case.",
            "5": "The paper is mostly clear and well written, with some exceptions noted below.",
            "6": "The idea isn't rocket science (but I do think it's nice).",
            "7": "The paper is somewhat repetitive.",
            "8": "The content seems a bit short for a long paper.",
            "9": "Then again, I think it would be quite difficult to squeeze it into the length of a short paper.",
            "10": "I guess it just needs to be a short long paper.",
            "11": "The abstract and the limitations/conclusions section have appropriate content but need editing (see below)."
        },
        "E10w7klUfi": {
            "0": "The approach is novel, well-argued, and effective, and addresses several inherent difficulties of the problem\n- Training data are reasonably augmented to focus on the composition & structure of characters rather than style of writing / font\n- The authors overcome a paucity of evaluation data by synthesizing their own, but they do provide some evaluation on some real data that are available — this may be a limitation of the evaluation, however the methods for data synthesis proposed seem sound and it seems likely results on a synthetic evaluation will generalize to real data\n- Limitations are clearly discussed and accurate, and the authors acknowledge that there are more sophisticated end-to-end deep neural methods that may solve the record linking problem more directly and decisively, and incorporate language understanding into the process, but these methods may not always be practical.",
            "1": "This is a well-written paper which clearly and effectively argues for a novel and interesting approach, providing an adequate evaluation of the approach and a clear discussion of its limitations — this reviewer sees no clear or overwhelming reason to reject.",
            "2": "One limitation not discussed in the paper (edit: the authors have, in their rebuttal, agreed to revise the paper to explicitly address this) is that these experiments focus on a single script, and it’s not demonstrated how the approach would generalize to other alphabets, but the authors picked a good example of a high cardinality alphabet where the limitations of an unmodified edit distance in record linking would be most pronnounced."
        }
    },
    "bWXIut4pNM": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the field of NLP, which is the high computational cost and environmental impact of training large pre-trained language models (PTLMs).",
            "1": "- The proposed method, INGENIOUS, introduces a novel approach to optimize the utility of training data by selecting highly informative subsets using submodular optimization.",
            "2": "- The paper demonstrates the effectiveness of this approach across multiple PTLMs (BERT, BioBERT, GPT-2) and provides empirical evidence that the models trained with these subsets achieve up to 99% of the performance of fully-trained models.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a significant problem in the field, offering a solution that can lead to substantial cost savings and reduced environmental impact.",
            "4": "- The proposed method is well-grounded in submodular optimization theory and is shown to be scalable for large datasets.",
            "5": "- The empirical evaluation is thorough, covering multiple models and tasks, and the results are promising, showing minimal performance degradation with significant cost savings.",
            "6": "- The paper includes detailed ablation studies and comparisons with relevant baselines, strengthening the validity of the proposed approach.",
            "7": "- The authors have made their framework publicly available, which promotes transparency and reproducibility.",
            "8": "Potential reasons for rejection\n   - **Scalability and Memory Constraints:**\n     - The submodular maximization based on pairwise sample similarity can be memory-intensive, potentially limiting its applicability to very large datasets.",
            "9": "- The paper does not provide a detailed analysis of the memory requirements and how they scale with the size of the dataset.",
            "10": "- **Generalization to Other Models:**\n     - While the method is tested on BERT, BioBERT, and GPT-2, it is not clear how well it would generalize to other types of models or tasks outside the scope of the experiments.",
            "11": "- The paper could benefit from a broader evaluation across different architectures and domains.",
            "12": "- **Impact on Knowledge Retention:**\n     - The paper acknowledges a potential reduction in knowledge retention due to the use of subsets, but the analysis is limited to the LAMA benchmark.",
            "13": "- A more comprehensive evaluation of the impact on knowledge retention across different types of knowledge and tasks would strengthen the claims.",
            "14": "- **Resource Limitations in Experiments:**\n     - The experiments are performed on relatively smaller PTLMs compared to state-of-the-art models like GPT-3, OPT, or PaLM, which may limit the generalizability of the results.",
            "15": "- The paper does not discuss the potential challenges and limitations of applying the proposed method to these larger models.",
            "16": "Suggestions for improvement\n   - **Detailed Memory Analysis:**\n     - Provide a detailed analysis of the memory requirements for the submodular maximization process and discuss potential strategies to mitigate memory constraints.",
            "17": "- **Broader Evaluation:**\n     - Extend the evaluation to include a wider range of models and tasks, including different architectures and domains, to demonstrate the generalizability of the proposed method.",
            "18": "- **Comprehensive Knowledge Retention Analysis:**\n     - Conduct a more comprehensive analysis of the impact on knowledge retention, covering different types of knowledge and tasks, to provide a clearer picture of the trade-offs involved.",
            "19": "- **Application to Larger Models:**\n     - Discuss the potential challenges and limitations of applying the proposed method to larger models like GPT-3, OPT, or PaLM, and provide insights or preliminary results if possible.",
            "20": "- **Environmental Impact Quantification:**\n     - Include a more detailed quantification of the environmental impact reduction achieved by the proposed method, providing concrete metrics and comparisons with traditional training methods."
        },
        "Yy52uMo34W": {
            "0": "This paper investigates the issue of resource consumption in training large language models (LLMs) by sampling high-quality information from the training dataset to ensure efficient learning of effective information.",
            "1": "This topic is crucial for the efficient training of LLMs.",
            "2": "The article provides a clear training strategy and validates its effectiveness on multiple models (but not large enough), which is inspiring.",
            "3": "While this paper's research is insightful, it has some limitations.",
            "4": "The current issues with training data quality for LLMs go beyond efficiency and information quantity, including concerns about values, toxicity, and biases.",
            "5": "Additionally, the paper's validation was limited to BERT and GPT2-small models and evaluated using GLUE metrics, which may not be sufficient to support the conclusions."
        },
        "e2q0mbEKGD": {
            "0": "The paper addresses an important problem of the pre-training cost of language models for efficient NLP.",
            "1": "The paper presents a novel approach to remove redundant texts in pre-training corpora in NLP domains.",
            "2": "C. Experimental results that language models trained with informative subsets outperform vanilla models in terms of convergence speed and final performance.",
            "3": "Although extracting data subsets work well in computer vision tasks, texts have different properties compared with images in nature.",
            "4": "The paper focuses on the size of datasets and lacks discussion about the different modality.",
            "5": "Most experiments are conducted on models with ~110M parameters.",
            "6": "It is unclear whether the proposed method consistently improves the efficiency with scaled models.",
            "7": "C. Lack of analysis and discussion leaves several questions about the reasons of performance improvements."
        },
        "GN7S7SQheN": {
            "0": "Firstly, the proposed method, INGENIOUS, demonstrates an impressive performance.",
            "1": "After fine-tuning, the model pre-trained using INGENIOUS-selected subsets achieves nearly identical performance compared to models pre-trained with the complete dataset, and even surpasses them with reduced training costs (as seen in Table 3).",
            "2": "Then, the experimental design is comprehensive, covering comparisons across various architectures, previous sampling methods, and different downstream tasks.",
            "3": "The evaluation is also multi-faceted, assessing both performance and cost considerations.",
            "4": "Additionally, the paper maintains a well-organized structure.",
            "5": "The experiment and analysis sections are good in depth and detail, though the methodology section could benefit from a more detailed elaboration.",
            "6": "First, I’m confused by Figures 3 and 4.",
            "7": "In these illustrations, INGENIOUS appears to consistently and significantly outperform the baseline BERT model which undergoes full pre-training, even when it is trained up to 1 million pre-training steps.",
            "8": "However, based on the context along with Tables 1&2 and the detailed Table 6 in Appendix F, it becomes evident that the most proficient model pre-trained with INGENIOUS achieves a 98.6% performance compared to the fully pre-trained BERT.",
            "9": "This contrast in results is confusing.",
            "10": "And if Figures 3&4 is sensible, what is the rationale behind the phenomenon that pre-training on selected subsets can surpass the performance of a fully pre-trained model?",
            "11": "Does this suggest that the original pre-training corpus contains harmful noise that interferes with learning?",
            "12": "Secondly,  for a thorough evaluation of INGENIOUS' effectiveness in selecting valuable subsets for pre-training, it is necessary to see a comparison between the pre-training performance of a model using INGENIOUS-selected data and one using the entire dataset, before any downstream fine-tuning.",
            "13": "The clean pre-training performance gains of INGENIOUS-selected data can demonstrate the evaluation process better."
        },
        "KCibErzysB": {
            "0": "Overall, the paper is well written.",
            "1": "The experiments are thorough, and I particularly appreciate the conversion of performance metrics to costs, which lets practitioners more easily evaluate trade-offs.",
            "2": "There are 3 potential changes that would improve this work:\n* First, something that didn't come across was the importance and intuition behind the choice of the similarity kernel.",
            "3": "What types of kernels work best?",
            "4": "Are there, e.g., cheap empirical metrics that can effectively estimate the clustering kernel in eq.",
            "5": "Could you estimate this similarity kernel through something very simple, such as SentenceBERT embeddings?",
            "6": "What features is it capturing that makes it particularly good?",
            "7": "* Including a comparison to one of the methods mentioned in the computer vision setting would have been more useful than comparing to, e.g.",
            "8": "loss-based sampling.",
            "9": "I understand that these are not always applicable and typically require a supervised set-up, but some of them can probably be adapted to language tasks relatively easily.",
            "10": "* Not sure I understand why increasing the subset size in some cases actually hurts performance?",
            "11": "e.g., Table 4.",
            "12": "I think the paper could benefit by elaborating on this, and why some subsets of the dataset are actually harmful to model performance.",
            "13": "--\n\nEdit: I acknowledge the authors' comments below.",
            "14": "Although I still have some questions/concerns about the subset ablations (and hope that the authors will eventually include a more granular analysis of why this happens, and if decoder only models exhibit some type of this behavior), I am moving to increase my soundness score as they have addressed my other comments."
        }
    },
    "R7Op9CHdPz": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenge of Out-of-Distribution (OOD) generalization in Visual Question Answering (VQA), which is a significant problem in the field.",
            "1": "- The proposed Cognitive pathways VQA (CopVQA) introduces a novel approach by emphasizing causal reasoning through two cognition layers, which is inspired by cognitive neuroscience.",
            "2": "- The method aims to improve multimodal predictions by decomposing the VQA task into interpreting and answering stages, each governed by distinct experts and a cognition-enabled component.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and well-motivated approach to improving OOD generalization in VQA, which is a critical issue in the field.",
            "4": "- The proposed method is thoroughly evaluated on multiple datasets, including real-life and medical data, demonstrating its effectiveness across different domains.",
            "5": "- The experimental results show that CopVQA achieves state-of-the-art performance on the PathVQA dataset and comparable results to current SOTAs on other datasets with significantly fewer parameters.",
            "6": "- The paper provides a comprehensive analysis of the proposed method, including ablation studies and qualitative results, which help to understand the contributions of different components.",
            "7": "Potential reasons for rejection\n   - **Complexity and clarity of the method:**\n     - The paper introduces a complex framework with multiple components, which may be challenging for readers to fully grasp.",
            "8": "- The explanation of the cognitive pathways and the role of the cognition-enabled components could be clearer and more concise.",
            "9": "- **Limited discussion on limitations:**\n     - The paper briefly mentions some limitations, such as sensitivity to data with limited occurrences and the need for careful finetuning, but does not provide an in-depth discussion or potential solutions.",
            "10": "- The impact of these limitations on the overall performance and generalizability of the method is not thoroughly analyzed.",
            "11": "- **Comparison with more baselines:**\n     - While the paper compares CopVQA with several baselines, it would benefit from a broader comparison with more recent and diverse VQA models to better position its contributions.",
            "12": "- The inclusion of additional baselines could provide a more comprehensive evaluation of the method's effectiveness.",
            "13": "Suggestions for improvement\n   - **Improve clarity and explanation:**\n     - Simplify the explanation of the cognitive pathways and the role of the cognition-enabled components to make the method more accessible to readers.",
            "14": "- Provide more intuitive examples or visualizations to illustrate the key concepts and the flow of the proposed method.",
            "15": "- **Expand discussion on limitations:**\n     - Provide a more detailed discussion of the limitations of the proposed method, including potential impact and ways to mitigate these issues.",
            "16": "- Analyze the sensitivity of the method to different types of data and the robustness of the model under various conditions.",
            "17": "- **Include more baselines:**\n     - Compare CopVQA with a wider range of recent VQA models to provide a more comprehensive evaluation of its performance.",
            "18": "- Consider including baselines that address different aspects of VQA, such as attention mechanisms, transformer-based models, and other multimodal approaches.",
            "19": "- **Detailed hyperparameter tuning:**\n     - Provide more details on the hyperparameter tuning process, including the range of values tested and the criteria for selecting the optimal configuration.",
            "20": "- Discuss the impact of different hyperparameters on the performance of the model and provide guidelines for tuning them in different scenarios."
        },
        "uDIpIJY37I": {
            "0": "- The intuition from cognitive neuroscience about knowledge modularity and cognitive pathways is interesting\n- This work first proposes a causal reasoning method, CopVQA  that formulates VQA as two layers of cognitive pathways (i.e.",
            "1": "interpreting and answering)\n- The proposed model CopVQA achieves SOTA for PathVQA dataset I am not an expert in causality, but I try my best to read this work, the implementation part is like an increment of [1].",
            "2": "And for the experiment part, I am willing to see more results of other baselines on PathVQA and VQA-RAD\n\n[1] Niu, Yulei, et al.",
            "3": "“Counterfactual VQA: A Cause-Effect Look at Language Bias.” 2021 CVPR"
        },
        "Iwb5e9uAcL": {
            "0": "- Interesting and timely problem statement\n- strong results on medical data\n- I like cognitive pathways angle brought in to VQA\n- well-written paper and interesting analysis These are not 'reasons to reject', but some questions I have:\n- I like the analysis in B.2, but was curious if authors have identified any possible data contaminations in the test set of the results they have reported in the paper?",
            "1": "- In NLP, Use of 'instructions' have strongly improved generalization (https://aclanthology.org/2022.acl-long.244.pdf, and https://openreview.net/forum?id=gEZrGCozdqR).",
            "2": "Wondering authors would like to discuss if they have considering using instructions in their setup?",
            "3": "Some of the issues authors see in sec 6.2 and 7 may be solved via instructions.",
            "4": "- How sensitive is the model to hyper-parameters?",
            "5": "I see a mention in the limitation section 'Require careful fine-tuning', but was looking for more details."
        },
        "hwqtKU2XZK": {
            "0": "The idea of CopVQA is interesting，which formulates VQA as two layers of cognitive pathway to  boost the causal reasoning.",
            "1": "CopVQA achieves the better performance on several VQA datasets.",
            "2": "There is doubt about the effectiveness of the complex reasoning module, as the improvement in downstream tasks by the model is limited.",
            "3": "Compared to sota pre-trained methods, there is a significant performance gap with this method.",
            "4": "Does it have scaling and generalization ability?",
            "5": "The method section is too redundant, and the experiments are not detailed enough, including experimental settings, model parameters, training time, inference time, etc."
        },
        "aZCXMmgwKy": {
            "0": "The quantitative results of the newly proposed framework indicate a new SOTA.",
            "1": "Newly designed cognitive pathways are innovative and the corresponding loss function described such reasoning paths well.",
            "2": "c. The qualitative analyses proves the effectiveness of such two-layer reasoning path way.",
            "3": "d. Enough case studies explains the advantages of such method.",
            "4": "e. Paper is well-organized.",
            "5": "Some details of such model remains unclear.",
            "6": "In 6.2 \"Discussion on the debiased samples\", it claims to reduce biases from the texts in question.",
            "7": "However, there is no statistics of such phenomenon.",
            "8": "c. The training details about training time and hardware requirements are missing."
        }
    },
    "D9oq45WsKq": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant limitation in current instruction-tuning methods, which rely on very large, proprietary language models.",
            "1": "- It proposes a novel approach, Ensemble-Instruct, which uses smaller, open-access language models to generate high-quality instruction-tuning data.",
            "2": "- The method introduces two key innovations: categorization and simplification of in-context learning (ICL) templates, and ensembling over multiple language model outputs to select high-quality examples.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-motivated and timely problem, addressing the need for more accessible and efficient instruction-tuning methods.",
            "4": "- The proposed method demonstrates significant improvements over existing approaches, such as Self-Instruct, in generating high-quality instruction-tuning data with smaller models.",
            "5": "- The empirical results are robust, showing that the method outperforms Self-Instruct and other baselines in various settings.",
            "6": "- The release of the codebase and synthetic instruction-tuning dataset adds value to the research community, promoting reproducibility and further research.",
            "7": "Potential reasons for rejection\n   - **Limited evaluation on diverse tasks**\n     - The evaluation primarily focuses on the SuperNI test set and a user-oriented test set, which may not fully capture the generalizability of the method across a wide range of tasks.",
            "8": "- Additional evaluations on more diverse and challenging datasets could strengthen the claims of the paper.",
            "9": "- **Dependence on specific models and configurations**\n     - The method relies on specific language models and configurations, which may limit its applicability to other models or settings.",
            "10": "- A more comprehensive analysis of the method's performance with different models and configurations would provide a clearer understanding of its generalizability.",
            "11": "- **Potential biases in synthetic data**\n     - The synthetic data generated by the method may inherit biases from the underlying language models, which could affect the quality and fairness of the instruction-tuning data.",
            "12": "- A thorough analysis of potential biases in the synthetic data and their impact on downstream tasks is necessary to ensure the robustness of the method.",
            "13": "Suggestions for improvement\n   - **Expand evaluation to more diverse tasks**\n     - Evaluate the method on a broader range of tasks and datasets to demonstrate its generalizability and robustness.",
            "14": "- Include tasks from different domains and with varying levels of complexity to provide a more comprehensive assessment of the method's performance.",
            "15": "- **Analyze performance with different models and configurations**\n     - Conduct experiments with a wider variety of language models and configurations to understand the method's applicability and limitations.",
            "16": "- Include models with different architectures, sizes, and training data to provide a more complete picture of the method's effectiveness.",
            "17": "- **Address potential biases in synthetic data**\n     - Perform a detailed analysis of potential biases in the synthetic instruction-tuning data generated by the method.",
            "18": "- Investigate the impact of these biases on downstream tasks and propose mitigation strategies to ensure the fairness and quality of the generated data.",
            "19": "- **Provide more detailed ablation studies**\n     - Include more detailed ablation studies to isolate the contributions of different components of the method.",
            "20": "- Analyze the impact of each component on the overall performance to provide a clearer understanding of the method's strengths and weaknesses."
        },
        "cAWjeiTWft": {
            "0": "- The simplicity of the methodology is a key advantage, as it facilitates easy application, which is commendable.",
            "1": "- Despite utilizing smaller models, the approach achieves notable performance, particularly demonstrating improved results even with a limited number of samples.",
            "2": "- The insightful analysis of the differences between ILM and LM, following experimentation with both models, adds valuable understanding to the study.",
            "3": "- The proposed methodology appears disconnected from the primary motivation of the paper, which revolves around the model's size and openness.",
            "4": "Although it was demonstrated using small models, there is potential for its applicability to all black-box models.",
            "5": "To better showcase the effectiveness of the methodology, conducting experiments with larger models would be beneficial.",
            "6": "- The analysis of the effectiveness of categorization and simplification lacks in-depth exploration, and it would be advantageous to delve further into this aspect.",
            "7": "While section 3.2 briefly mentions the balance between type A and B, a more comprehensive analysis would add valuable insights.",
            "8": "- The absence of a comparison with other datasets, such as Alpaca mentioned in the paper, is somewhat regrettable.",
            "9": "Including such comparisons would provide a more comprehensive evaluation of the proposed approach.",
            "10": "Additionally, a qualitative comparison with other methods would be valuable in further validating the proposed methodology."
        },
        "7vSjcBqptH": {
            "0": "Paper is well written and easy to follow.",
            "1": "Ensemble-Instruct technique shows clear gains over Self-Instruct across a wide variety of settings.",
            "2": "Ablation studies show that both aspects (simplified prompts, ensemble) are both useful in leading to superior performance.",
            "3": "Synthetic dataset of instruction-tuning examples can be used by others to build on.",
            "4": "The dataset only includes 45k examples, but I can see this being expanded to 100K+ examples if the codebase is opened up to others.",
            "5": "Anything that works with open-source, permissive licenses is great for the community, especially when doing so outperforms the black-box counterparts.",
            "6": "Ensembling is not a particularly novel way to denoise generated samples.",
            "7": "More sophisticated methods exists and it would have been nice to compare against them.",
            "8": "Could be improved with more experiments on how many candidates to ensemble, or how many examples to include during ICL.",
            "9": "Missing larger scale human evaluation for qualitative review."
        },
        "ESN4z28PjZ": {
            "0": "* The paper proposes a feasible method to create high-quality ICL samples for small (and hence less-capable) LLMs.",
            "1": "This topic has practical importance in many application areas.",
            "2": "* The evaluation is quite comprehensive, covering many settings and using several popular permissive LLMs.",
            "3": "* The main performance metric used is the Rouge-L score, which is far from being satisfactory.",
            "4": "If human evaluation is too costly, at least some AI evaluation could be done (e.g., using GPT-4).",
            "5": "* The key step is the high-quality sample selection from multiple LLMs.",
            "6": "Although these LLMs were built by different groups, there might still be some hidden correlation between their outputs (e.g., due to training on the same subset of data).",
            "7": "Hence, the current method is not really selecting \"high-quality\" samples, but more like selecting \"popular\" samples, which may further magnifies the latent biases in the training data.",
            "8": "* There is no detailed case study, e.g., comparing the samples generate by the proposed method and those by the Self-Instruction method.",
            "9": "Additional, such case studies can be carried out among different LLMs used in this study.",
            "10": "It is desirable to see insightful analysis or trends from these studies."
        }
    },
    "uemYdRTVvP": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the field of Natural Language Processing (NLP) by focusing on sentence-level AI-generated text (AIGT) detection, which is a more fine-grained approach compared to the existing document-level detection methods.",
            "1": "- The introduction of SeqXGPT, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection, represents a significant advancement in the field.",
            "2": "- The creation of a new dataset, SeqXGPT-Bench, specifically designed for sentence-level AIGT detection, adds value to the research community by providing a resource for further studies.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel approach to a relevant and timely problem in NLP, which is the detection of AI-generated text at the sentence level.",
            "4": "- The proposed SeqXGPT method demonstrates superior performance compared to existing methods in both sentence and document-level AIGT detection tasks.",
            "5": "- The experimental results are comprehensive and show strong generalization capabilities of SeqXGPT on out-of-distribution datasets.",
            "6": "- The paper provides a new dataset, SeqXGPT-Bench, which can be a valuable resource for future research in fine-grained AIGT detection.",
            "7": "Potential reasons for rejection\n   - **Lack of semantic feature integration:**\n     - The paper mentions that semantic features were not incorporated into SeqXGPT, which could potentially enhance the model's performance, especially for human-like sentence generation.",
            "8": "- The absence of semantic features might limit the model's ability to fully understand and differentiate between human and AI-generated text.",
            "9": "- **Limited exploration of instruction diversity:**\n     - The construction of GPT-3.5-turbo data did not extensively investigate the impact of more diversified instructions, which could affect the robustness and generalizability of the model.",
            "10": "- The paper does not provide a detailed analysis of how different instructions might influence the performance of AIGT detection.",
            "11": "- **Simplified dataset construction:**\n     - The dataset construction process involves only two distinct sources of sentences, which might not fully capture the complexity of real-world scenarios where documents contain sentences from multiple sources.",
            "12": "- The paper does not explore more complex scenarios, which could limit the applicability of the proposed method in diverse real-world contexts.",
            "13": "Suggestions for improvement\n   - **Incorporate semantic features:**\n     - Integrate semantic features into SeqXGPT to enhance its ability to understand and differentiate between human and AI-generated text, especially for more human-like sentence generation.",
            "14": "- Conduct experiments to evaluate the impact of semantic features on the model's performance.",
            "15": "- **Explore instruction diversity:**\n     - Investigate the impact of more diversified instructions on the construction of GPT-3.5-turbo data to improve the robustness and generalizability of the model.",
            "16": "- Provide a detailed analysis of how different instructions influence the performance of AIGT detection.",
            "17": "- **Expand dataset complexity:**\n     - Explore more complex scenarios in the dataset construction process, where documents contain sentences from multiple sources, to better reflect real-world contexts.",
            "18": "- Conduct experiments to evaluate the performance of SeqXGPT in these more complex scenarios.",
            "19": "- **Detailed ablation studies:**\n     - Conduct more detailed ablation studies to understand the contribution of each component of SeqXGPT, including the convolutional and transformer layers.",
            "20": "- Provide insights into how each component affects the overall performance of the model."
        },
        "RgLkzPfsqW": {
            "0": "Novel Contribution: The paper addresses the important and challenging task of fine-grained AI-generated text (AIGT) detection at the sentence level, which is a significant advancement over existing document-level AIGT detection methods.",
            "1": "The authors propose a new approach called SeqXGPT, which demonstrates promising results in both sentence and document-level AIGT detection challenges.",
            "2": "This novel contribution fills a gap in the literature and provides valuable insights into fine-grained AIGT detection.",
            "3": "Performance and Generalization: The experimental results show that SeqXGPT outperforms existing methods, such as DetectGPT and Sniffer, in sentence-level AIGT detection.",
            "4": "SeqXGPT exhibits excellent performance not only in discriminating human-generated sentences but also in detecting AI-generated sentences.",
            "5": "Furthermore, SeqXGPT demonstrates strong generalization capabilities on out-of-distribution datasets, indicating its robustness and potential for real-world applications.",
            "6": "Dataset Construction: The authors synthesize a sentence-level AIGT detection dataset, which is crucial for studying fine-grained AIGT detection 1.",
            "7": "Lack of Novelty: The paper does not present a significant advancement or novel contribution to the field of fine-grained AI-generated text (AIGT) detection.",
            "8": "The proposed approach, SeqXGPT, is similar to existing methods such as DetectGPT and Sniffer.",
            "9": "The paper fails to demonstrate how SeqXGPT significantly outperforms or improves upon these existing methods.",
            "10": "Insufficient Experimental Evaluation: The experimental results provided in the paper are limited and do not provide a comprehensive evaluation of the proposed approach.",
            "11": "The paper lacks a thorough comparison with state-of-the-art methods and fails to provide statistical significance tests to support the claimed performance improvements.",
            "12": "Additionally, the evaluation is primarily focused on synthetic datasets, which may not accurately reflect real-world scenarios.",
            "13": "Incomplete Analysis and Discussion: The paper lacks a thorough analysis and discussion of the limitations and potential drawbacks of the proposed approach.",
            "14": "For example, the authors do not explore the impact of incorporating semantic features or investigate the influence of diversified instructions on AIGT detection.",
            "15": "The paper also does not address more complex scenarios where a document contains sentences."
        },
        "1T77OByRWS": {
            "0": "1) The method proposed in this paper can effectively solve the difficulties of sentence-level AIGT detection.",
            "1": "2) The experiments designed in this paper cover 3 different sentence-level AIGT detection settings, all of which achieve SOTA.",
            "2": "3) This article is written smoothly and helps readers understand.",
            "3": "In the setting of Particular-Model Binary AIGT Detection, the table of experimental results only includes the results of GPT-2 and GPT-Neo, lacking test results on other LLMs."
        },
        "pHSePBU55u": {
            "0": "This paper will provide a practical guide to construct an AI-generated text detection software.",
            "1": "In particular, this method can be generally applied to any collection of LMs (already or to-be available), which makes the method presented in this paper a nice addition to the practical armory of any NLP researcher or industrial programmer.",
            "2": "The dataset used to train and test the proposed method appears to be somewhat narrow in how it is produced.",
            "3": "As bootstrapping is the first-step for any new task, so OOD test results the authors provided are a must-and-nice addition, but the intensity of the OOD test should have been greater in my opinion (as, once the paper published, interested people will be doing their own OOD tests by implementing their own versions)."
        }
    },
    "KfJffhdWO1": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a fundamental challenge in Natural Language Generation (NLG) model evaluation by proposing a new framework, METRIC EVAL, for evaluating the reliability and validity of NLG evaluation metrics.",
            "1": "- The framework is informed by measurement theory, which is a novel approach in the context of NLG evaluation.",
            "2": "- The paper provides a comprehensive analysis of existing evaluation metrics and demonstrates the application of the proposed framework through a case study on summarization metrics.",
            "3": "Potential reasons for acceptance\n   - The paper introduces a novel and theoretically grounded framework for evaluating NLG metrics, which can significantly improve the reliability and validity of NLG model evaluations.",
            "4": "- The use of measurement theory to inform the framework is innovative and provides a solid foundation for the proposed methods.",
            "5": "- The case study demonstrates the practical utility of the framework and provides valuable insights into the strengths and weaknesses of various summarization metrics.",
            "6": "- The paper is well-structured and clearly written, making it accessible to a broad audience in the NLG research community.",
            "7": "Potential reasons for rejection\n   - **Limited generalizability of the case study results**\n     - The case study focuses on a specific dataset (CNN/Daily Mail) and a specific task (summarization), which may limit the generalizability of the findings to other datasets and tasks.",
            "8": "- The paper does not provide sufficient evidence to support the generalizability of the proposed framework across different NLG tasks and datasets.",
            "9": "- **Reliance on expert ratings with potential biases**\n     - The framework relies on expert ratings as a reference criterion for evaluating metric validity, but the paper acknowledges potential issues with expert ratings, such as subjectivity and inconsistency.",
            "10": "- The paper does not provide a robust solution to address these potential biases in expert ratings, which could undermine the validity of the framework.",
            "11": "- **Complexity and practicality of the proposed framework**\n     - The framework involves complex statistical methods and requires a deep understanding of measurement theory, which may limit its practicality for some researchers and practitioners.",
            "12": "- The paper does not provide clear guidelines or tools to facilitate the implementation of the framework, which could hinder its adoption in the NLG community.",
            "13": "Suggestions for improvement\n   - **Expand the case study to include multiple datasets and tasks**\n     - To demonstrate the generalizability of the proposed framework, the authors should conduct additional case studies on different NLG tasks (e.g., dialogue generation, machine translation) and datasets.",
            "14": "- This would provide stronger evidence for the applicability of the framework across various NLG contexts.",
            "15": "- **Address potential biases in expert ratings**\n     - The authors should explore methods to mitigate potential biases in expert ratings, such as using multiple independent raters, providing detailed rating guidelines, and employing statistical techniques to adjust for rater biases.",
            "16": "- Including a discussion on how to ensure the reliability and validity of expert ratings would strengthen the framework.",
            "17": "- **Provide practical guidelines and tools for implementation**\n     - The authors should develop and share practical guidelines, tools, and code to facilitate the implementation of the METRIC EVAL framework.",
            "18": "- This could include step-by-step instructions, example scripts, and a user-friendly software package to help researchers and practitioners apply the framework to their own NLG evaluations.",
            "19": "- **Discuss the limitations and future directions in more detail**\n     - The authors should provide a more detailed discussion of the limitations of the proposed framework and potential areas for future research.",
            "20": "- This could include exploring additional types of validity (e.g., predictive validity), addressing other sources of measurement error, and developing new evaluation metrics based on the insights gained from the framework."
        },
        "Z8xMKqdE1f": {
            "0": "+ nice combination of existing methods (from measurement theory) to a very relevant current topic (evaluation of metrics for NLG)\n+ nice and intuitive showcase none"
        },
        "PzqntNVHpX": {
            "0": "- Interesting framework that addresses an important task: estimating the quality of commonly used metrics.",
            "1": "- Thorough description of measurement theory and how it is applied, enabling other researchers to use this framework for more cases.",
            "2": "- Case study extensive in regards to metrics (a good selection of reference-free and reference-based metrics).",
            "3": "- Uncertainty to what degree measurement theory (which has been developed to assess human evaluation) can also be applied to automatic metrics.",
            "4": "- Case study somewhat limited in scope in regards to the task (only summarization).",
            "5": "- Limited relevant work section."
        },
        "IJHv5vKK4z": {
            "0": "As NLG models improve over time, finding good metrics to accurately measure the quality of their output in a way that best approximates their real-world performance is an important task.",
            "1": "This paper presents an interesting idea from educational testing that breaks down the required qualities of a good metric and shows how they relate to different factors.",
            "2": "The paper is well written and the ideas are communicated clearly.",
            "3": "The authors also present a case study using text summarization as an example task and show how different metrics score along different dimensions, and what this means for their robustness and validity.",
            "4": "The authors plan to make the code for their framework publicly available as well.",
            "5": "There can be other sources of error beyond those discussed in the paper (and an investigation of where they occur in the evaluation process), although the authors concede this in the limitations section already.",
            "6": "The related work section could be more comprehensive and discuss the merits and limitations of prior works in more detail."
        }
    },
    "0SIyWZEOmJ": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the linearity of the effect of surprisal on reading times across multiple languages, which is a significant topic in psycholinguistics.",
            "1": "- It extends previous research that focused primarily on English by including seven languages: Danish, Dutch, English, German, Japanese, Mandarin, and Russian.",
            "2": "- The study uses eye-tracking corpora and different language models to estimate surprisal, providing a comprehensive analysis across languages.",
            "3": "Potential reasons for acceptance\n   - The paper contributes to the understanding of language processing by examining the effect of surprisal on reading times in multiple languages.",
            "4": "- It uses a robust methodology, including linear mixed-effects models and different language models, to analyze the data.",
            "5": "- The findings provide evidence for both linear and superlinear effects of surprisal, which could have implications for theories of language processing.",
            "6": "- The study includes a detailed discussion of the results and their implications, as well as limitations and suggestions for future research.",
            "7": "Potential reasons for rejection\n   - **Statistical Methodology:**\n     - The handling of spillover effects is standard but may not account for the full shape of the timecourse of the effect of surprisal, potentially leading to a lack of power to detect effects.",
            "8": "- The test for superlinear effects is relatively simple, comparing a linear model against a quadratic model, which may not capture the full complexity of the relationship.",
            "9": "- **Eyetracking Corpora:**\n     - The corpora used are not matched across languages, differing in genre and size, which could introduce variability that affects the results.",
            "10": "- The smaller datasets for some languages (e.g., German, Japanese, Mandarin, and Russian) may limit the generalizability of the findings.",
            "11": "- **Language Models:**\n     - The study only uses one language model architecture (GPT), which may not provide the most accurate surprisal values for all languages.",
            "12": "- The discrepancy between the genres of the eyetracking corpus texts and the Wikipedia text used to train the language models could affect the surprisal estimates.",
            "13": "- **Generalizability:**\n     - The study's conclusions are limited by the specific languages and corpora used, and further research is needed to confirm the findings across a broader range of languages and text types.",
            "14": "Suggestions for improvement\n   - **Statistical Methodology:**\n     - Consider using more sophisticated methods for handling spillover effects, such as deconvolutional time series methods, to better capture the timecourse of the effect of surprisal.",
            "15": "- Explore alternative statistical models that can quantify the shape of the superlinear effect, such as fitting models with varying exponents.",
            "16": "- Use error distributions that better reflect human reaction times, such as Log-Normal or Ex-Gaussian distributions, to improve the accuracy of the regression models.",
            "17": "- **Eyetracking Corpora:**\n     - Use matched corpora across languages, with similar genres and sizes, to reduce variability and improve the comparability of the results.",
            "18": "- Increase the size of the datasets for languages with smaller corpora to enhance the robustness of the findings.",
            "19": "- **Language Models:**\n     - Experiment with different language model architectures to determine which provides the most accurate surprisal values for each language.",
            "20": "- Train language models on text corpora that are more similar in genre to the eyetracking corpus texts to improve the accuracy of the surprisal estimates.",
            "21": "- **Generalizability:**\n     - Extend the study to include a broader range of languages and text types to confirm the findings and enhance their generalizability.",
            "22": "- Conduct additional analyses to explore potential typological patterns in the effect of surprisal on reading times across languages."
        },
        "7hdjUtyBDe": {
            "0": "(a) The authors address a timely topic that has been the focus of recent discussion in the literature\n\n(b) The authors go beyond previous monolingual analyses, which is a valuable contribution\n\n(c) The paper is well written and clear\n\n(d) Although the authors present their handling of spillover has a limitation (I see their point) their method allows for variable spillover between datasets.",
            "1": "This is better than many recent papers which set a fixed sized spillover window regardless of dataset.",
            "2": "(a) Previous studies have demonstrated an effect of unigram surprisal (i.e.",
            "3": "frequency) on reading times.",
            "4": "Frequency is absent from this paper and not used in the baseline model M0.",
            "5": "I’m worried that the overall picture of results could change if frequency was added in as an additional predictor.",
            "6": "(The authors do mention that word length with random by-participant intercepts was the maximal consistently random effects structure – does this mean that they tried random slopes, or that they tried random slopes plus also other main effects, such as frequency?)",
            "7": "(b) The likelihood ratio tests were computed on the log likelihoods of the training data, correct?",
            "8": "I think the paper could be made much stronger if the tests were conducted on a held-out portion of the dataset.",
            "9": "Especially as models grow more complex, testing on held-out data grows crucial to avoid charges of overfitting.",
            "10": "(My apologies if you are already doing this – it was not clear to me from the text!)"
        },
        "fnIToUVGUJ": {
            "0": "The paper contributes useful crosslinguistic data to an ongoing debate about the connection between surprisal and reading time.",
            "1": "The methods seem sound and the results are clearly presented.",
            "2": "The opening sections of the paper seem to slightly mischaracterize Surprisal Theory.",
            "3": "As presented in Levy 2008, Surprisal Theory posits that comprehension difficulty (e.g.",
            "4": "reading time) is proportional to surprisal -- meaning that the linking function is linear.",
            "5": "This would mean that theories that posit e.g.",
            "6": "a superlinear linking function do not fall under Surprisal Theory per se."
        },
        "V7LguMTFB9": {
            "0": "- The question is of interest for cognitive modeling\n- The principle cross-linguistic contribution is important and necessary for this subfield to progress\n- Data analysis is well-motivated and connects well with previous work in this domain - A key limitation concerns the different corpora that were available for the different languages.",
            "1": "These corpora differ substantially in size and in genre (e.g.",
            "2": "naturalistic text vs constructed sentences).",
            "3": "These differences in genre plausibly affect how adequate GPT2 -- given the idiosyncrasies of its own training set -- serves as a cognitive estimator.",
            "4": "The analysis across languages also differ in other ways that may impact the results, including the specific measures available (first-pass gaze duration vs first fixation duration) and the size of the pre-target context that was used to sum \"spill-over\" surprisal values.",
            "5": "Taken together, I found it difficult to understand where differences between \"linear\" and \"non-linear\" effects might reflect cross-linguistic differences, as opposed to idiosyncratic differences of the available corpora (like genre) or other analysis choices.",
            "6": "I think for this sort of work to be maximally impactful, it would be great to see a robustness analysis varying some of these parameters (e.g.",
            "7": "corpus size and genre via sub-sampling the available materials; spill-over size etc.)",
            "8": "A final point is that the main conclusions: that languages differ in whether the effect of surprisal is linear or super-linear, is grounded in comparing a set of \"significant\" model comparison findings against \"non-significant\" model comparisons.",
            "9": "As presented, I don't think this inference goes through (\"the difference between significant and non-significant is not itself statistically significant\" - German & Stern 2006).",
            "10": "I think this could be addressed by testing for a statistical interaction of language and model-type."
        }
    },
    "siiVduxdRz": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Language-specific Matrix Synthesis (LMS), a novel method for improving parameter efficiency in multilingual models.",
            "1": "- It also proposes Fuse Distillation (FD) to condense multilingual knowledge from multiple language-specific modules into a single shared module.",
            "2": "- The methods address scalability issues in multilingual models, particularly when dealing with hundreds of languages or experts.",
            "3": "Potential reasons for acceptance\n   - The proposed LMS method demonstrates significant improvements in parameter efficiency while maintaining or improving performance compared to existing methods.",
            "4": "- The FD method shows promising results in condensing multilingual knowledge, reducing the need for a large number of parameters during inference.",
            "5": "- The paper provides comprehensive experimental results across multiple tasks (multilingual machine translation, named-entity recognition, and question answering), showcasing the generalizability and effectiveness of the proposed methods.",
            "6": "- The authors release their code, promoting transparency and reproducibility.",
            "7": "Potential reasons for rejection\n   - **Limited novelty in the context of existing methods:**\n     - The LMS method is inspired by existing concepts such as \"intrinsic dimension\" and \"intrinsic rank,\" which may limit its perceived novelty.",
            "8": "- The FD method is similar to knowledge distillation techniques, which are well-established in the literature.",
            "9": "- **Complexity and practicality of implementation:**\n     - The requirement for homogeneous batches in LMS may complicate its implementation in real-world scenarios where mixed-language batches are common.",
            "10": "- The additional step of multiplying two small matrices in each forward pass may introduce computational overhead, which is not thoroughly evaluated in the paper.",
            "11": "- **Insufficient exploration of limitations and potential drawbacks:**\n     - The paper does not provide a detailed analysis of the potential limitations and drawbacks of the proposed methods, such as their performance in extremely low-resource languages or their behavior in highly imbalanced datasets.",
            "12": "- The scalability of the methods to even larger language sets (beyond the 100 languages in OPUS-100) is not thoroughly investigated.",
            "13": "Suggestions for improvement\n   - **Enhance the novelty and contribution:**\n     - Clearly differentiate the proposed methods from existing techniques and highlight their unique contributions.",
            "14": "- Provide a more in-depth theoretical analysis of the LMS and FD methods to strengthen their novelty and significance.",
            "15": "- **Address implementation complexity:**\n     - Explore and propose solutions for handling mixed-language batches in LMS to make the method more practical for real-world applications.",
            "16": "- Evaluate the computational overhead introduced by the additional matrix multiplication step and provide optimization strategies to mitigate it.",
            "17": "- **Thoroughly investigate limitations and scalability:**\n     - Conduct experiments on extremely low-resource languages and highly imbalanced datasets to understand the limitations and robustness of the proposed methods.",
            "18": "- Investigate the scalability of LMS and FD to even larger language sets and provide insights into their performance and parameter efficiency in such scenarios.",
            "19": "- **Improve clarity and presentation:**\n     - Provide more detailed explanations and visualizations of the LMS and FD methods to enhance the reader's understanding.",
            "20": "- Include a comprehensive discussion of the results, highlighting the strengths and weaknesses of the proposed methods in comparison to existing techniques."
        },
        "owFPRACujx": {
            "0": "-  Both techniques proposed in the paper seem interesting.",
            "1": "I like the idea of LMS, and language-pair LMS looks an interesting.",
            "2": "- Idea of the fused distillation is very interesting as well.",
            "3": "Even though I am not fully convinced by the LMS-FD-Shared results, the fact that it outperforms NMT-Naive model is very interesting, and I believe different directions could be explored to further improve the results - The training details of different variants are not fully documented and would harm reproducibility , eg.",
            "4": "1) do you strat from pretrained model and add LMS layers on top of it or do you train everythin from scratch?",
            "5": "2) Do LMS-FD-*  models start from LMS model or do you train LS and shared modules simultaneously?",
            "6": "- I do not have full understanding about how do authors compute inference parameters.",
            "7": "Eg.",
            "8": "we could distinguish the case of language-specifc from multilingual inference.",
            "9": "In the case when we know the language of interest (mandatory when we rely on LMS) we could load only the parameters specific to this language(s) and it would mean that we would have much less parameters for inference than for training: thus in my understanding LMS-FD-LS should have the same amount of inference parameters as LMS-FD-Share .",
            "10": "Similarly, CLSR should only activate language-specific modules, and have less parameters at inference that at training.",
            "11": "But this doesn't seem to be the case in the tables reported by authors.",
            "12": "Do they consider multilingual inference?",
            "13": "Some clarifications would help.",
            "14": "-  Moreover, if the authors wanted to reinforce their argument about efficiency, only counting the amount of the parameters doesn't give the full picture (parameter count only address memory efficiency): inference speed, the amount of flops (in training and inference) would provide additional and important view to get a full picture.",
            "15": "- I am not sure the choice of CLSR as a baseline.",
            "16": "There are many PEFT methods these days that could serve as a reasonable baseline for such approach (eg.",
            "17": "laguage-specific or language-pair specific adapter layers seems like a very natural baseline to me, and should probably be comparable in terms of parameter efficiency )"
        },
        "0P1oepcpIg": {
            "0": "* The idea of using LoRA to tune the FFNs to be language specific is interesting.",
            "1": "The proposed approach for distillation is also interesting.",
            "2": "* The authors call their method \"Language-Specific Matrix Synthesis\", but their method is essentially LoRA for multilingual settings.",
            "3": "I still think it's interesting, but this attempt to make it sound more novel than it really is is a negative point in my opinion.",
            "4": "* The paper claims their method \"significantly outperforms previous language-specific or mixture of experts methods\" (lines 77-78), yet they don't compare with any of those methods.",
            "5": "There is plenty of related work that is cited on section 7, but no comparisons were made.",
            "6": "While it is interesting that the authors test their method on different tasks, the lack of any meaningful baseline is a serious omission.",
            "7": "I do think the idea is interesting and that this can be an interesting paper, but as it stands I strongly believe it would benefit from further work."
        },
        "FxxaQIBi0A": {
            "0": "The idea is simple yet smart and intuitive, which is supported by the strong performance in the desired downstream task.",
            "1": "Beside the performance, it scales to a number of languages, and even propose a technique to pack all the knowledge into a single module, to further reduce the complexity.",
            "2": "It's well grounded and solid study.",
            "3": "I'm mostly convinced that the paper should get accepted, as there is nothing to complain."
        }
    },
    "qMSG8S7zh0": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of miscalibrated predictive uncertainty in modern deep models for summarization, which is significant for improving the reliability and trustworthiness of these models in real-world applications.",
            "1": "- The study is novel in its comprehensive investigation of various state-of-the-art probabilistic methods for improving uncertainty quality in neural summarization models, using a newly introduced evaluation protocol across multiple large-scale benchmarks.",
            "2": "Potential reasons for acceptance\n   - The paper provides a thorough and well-structured evaluation of different probabilistic methods, offering valuable insights into their effectiveness in improving uncertainty calibration and summarization quality.",
            "3": "- The introduction of a new evaluation protocol tailored for summarization tasks is a significant contribution that can be useful for future research in this area.",
            "4": "- The findings reveal important failure patterns of widely-adopted probabilistic methods, which can guide researchers in choosing appropriate methods for different data settings.",
            "5": "Potential reasons for rejection\n   - **Limited adaptation of methods to PLM setting**\n     - The paper mentions that only basic adaptations of existing probabilistic methods were made for the PLM setting.",
            "6": "- More extensive adaptations and explorations could provide a deeper understanding of the interplay between these methods and the unique properties of PLMs.",
            "7": "- **Insufficient exploration of diverse datasets**\n     - The study focuses on three datasets (XSUM, CNN/DailyMail, RedditTIFU-long), which, while diverse, may not fully capture the range of challenges faced in real-world summarization tasks.",
            "8": "- Including additional datasets with different characteristics could strengthen the generalizability of the findings.",
            "9": "- **Lack of detailed analysis on computational efficiency**\n     - The paper provides a high-level comparison of the time and space complexity of the methods but lacks a detailed analysis of their computational efficiency in practice.",
            "10": "- A more in-depth discussion on the trade-offs between computational cost and performance improvements would be beneficial for practitioners.",
            "11": "Suggestions for improvement\n   - **Enhance method adaptations for PLM setting**\n     - Conduct more extensive adaptations of the probabilistic methods to better align with the unique properties of PLMs, such as autoregressive generation and the mismatch between learning and inference.",
            "12": "- Explore additional techniques that could further improve the calibration and performance of PLMs.",
            "13": "- **Expand dataset diversity**\n     - Include a wider range of datasets with varying characteristics, such as different domains, lengths, and levels of abstractiveness, to better assess the generalizability of the findings.",
            "14": "- Consider using datasets that pose specific challenges, such as those with high linguistic diversity or those requiring high factual accuracy.",
            "15": "- **Detailed computational efficiency analysis**\n     - Provide a more detailed analysis of the computational efficiency of each method, including practical considerations such as training and inference time, memory usage, and scalability.",
            "16": "- Discuss the trade-offs between computational cost and performance improvements to help practitioners make informed decisions when choosing methods for their specific use cases.",
            "17": "- **Incorporate additional evaluation metrics**\n     - Include additional evaluation metrics that capture different aspects of summarization quality and uncertainty calibration, such as human evaluation scores, factual accuracy, and diversity of generated summaries.",
            "18": "- Provide a more comprehensive assessment of the methods' performance across these metrics to offer a holistic view of their effectiveness."
        },
        "CmTZFRIeMS": {
            "0": "- In-depth Analysis: The paper provides a comprehensive comparison of various calibration techniques, including Monte Carlo Dropout, Deep Ensembles, Batch Ensemble, and Spectral-normalized Neural Gaussian Process (SNGP), and adapts these probabilistic methods to LLM setup for summarization task.",
            "1": "This detailed analysis includes both generation quality assessment for calibrated model and quality of its uncertainty (length-normalized log-probabilities), which would be invaluable for future research in the field of NLP.",
            "2": "- Clear Presentation: The overall logic flow of the paper is fluent and easy to follow.",
            "3": "Both figures and tables are nicely drawn with subtle explanations.",
            "4": "Overall, I like this paper.",
            "5": "My main concern is that some details seems missing and need further explanations:\n- For the main method (section 3), formal definitions (preliminaries) are necessary before showing equations.",
            "6": "For example in Line 186-200, what are the meanings of x, y, t, T?",
            "7": "Is u(y|x) a vector or a number (I expect it to be a vector because it is a distribution among the vocabulary space)?",
            "8": "Otherwise, readers may find it hard to follow the theoretical part.",
            "9": "- For the evaluation (section 4.2), how is the sequence-level ECE calculated?",
            "10": "For example, in line 256-257, if the predicted sequence is “i made fun of a guy walking in giant slippers on father’s day” and the ground truth is “thought a guy’s shoes were the reason he walked funny, turns out he has cerebral palsy.”, does y^ equal y and what is p^?",
            "11": "Lack of concrete examples makes it difficult to understand how the accuracy of a prediction (sequence) and the confidence are calculated.",
            "12": "- For the abstention (section 4.3), what is the meaning of the quality in the y-axis of Figure 1?",
            "13": "If it is ROUGE, then make it 'ROUGE score vs Abstention', instead of introducing an undefined new term."
        },
        "l4usVikbCH": {
            "0": "- Well written paper, and interesting and informative to read\n- Investigation of state-of-the-art probabilistic methods in improving uncertainty calibration in neural summarization models.",
            "1": "-  Clear demonstration of the positive impact of probabilistic methods on summary quality and calibration performance.",
            "2": "- In-depth analysis of failure patterns in widely-adopted probabilistic methods, providing insights for choosing appropriate methods based on data settings.",
            "3": "- The significance and insights from experiment 4.1 are not fully clear, requiring more detailed explanations.",
            "4": "- Although the paper provides good experiments and analysis, it is not entirely novel given the advancements in probabilistic methods for classification tasks."
        },
        "4GVSu1VX3L": {
            "0": "The paper addresses an important topic regarding how to make the output of neural language models more reliable and trustworthy.",
            "1": "The problem of miscalibration is understudied in text summarization, meaning that this work could open new research directions toward this topic.",
            "2": "The experimental setup is poor.",
            "3": "The authors compared several probabilistic methods over a single language model (T5-base).",
            "4": "They should evaluate multiple language models of different sizes (e.g., BART-base, BART-large, FLAN-T5-XXL).",
            "5": "Furthermore, they should include a long document summarization dataset (and thus a long sequence model such as LED or PEGASUS-X) to make the evaluation analysis more robust.",
            "6": "There is not a novel method to address the problem of miscalibration.",
            "7": "The authors state to experiment with large language models (LLMs), but in practice, they use T5-base, which is far from being an LLM (like GPT-3).",
            "8": "The dataset statistics (i.e., number of source/target words and sentences) should be included to show and demonstrate the difference between the benchmarked datasets.",
            "9": "The paper contains many typos (see \"Typos Grammar Style And Presentation Improvements\" for a non-exhaustive list), meaning the document's drafting has not been curated enough.",
            "10": "The authors did not mention how costly each benchmarked method is regarding time and space."
        },
        "k35il7zGoS": {
            "0": "The paper performs systematic evaluation of the probabilistic methods in uncertainty calibration in neural summarization models.",
            "1": "The evaluation is performed on classic and state-of-the-art methods, on three datasets, and with evaluation metrics of ROUGE, ECE, correlation between quality and predictive confidence, and quality vs abstention rate.",
            "2": "It is not quite reassuring to see that the authors use the \"average\" performances from the three datasets (as in Tables 1 and 2) to prove the effectiveness of the probabilistic models.",
            "3": "The average rank as in Table 1 is more reasonable.",
            "4": "And yet, the average rank is not used in Table 2, which needs explanation.",
            "5": "The evaluation is systematic, covering quality (with ROUGE), calibration (sequence-level and token-level ECE), quality vs predictive confidence, and quality vs abstention rate.",
            "6": "However it is a bit farfetched to say it's novel."
        }
    },
    "AEkFAAprvF": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces ViStruct, a novel training framework for vision-language models (VLMs) aimed at improving visual structural knowledge extraction.",
            "1": "- The approach leverages the inherent structure of programming languages to depict visual structural information, which is a unique and innovative method.",
            "2": "- The introduction of curriculum-based learning for progressively comprehending visual structures is a significant contribution to the field.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a critical limitation in current VLMs, which is their inability to effectively capture visual structural information.",
            "4": "- The proposed method demonstrates significant improvements in visual structure prediction tasks, as evidenced by the experimental results.",
            "5": "- The release of a comprehensive dataset collection tailored for visual structural knowledge extraction adds value to the research community.",
            "6": "- The use of a weakly-supervised approach to generate visual event structures from captions is a practical and scalable solution.",
            "7": "Potential reasons for rejection\n   - **Complexity and Implementation Feasibility**\n     - The proposed method involves multiple stages and components, which may be challenging to implement and reproduce.",
            "8": "- The reliance on programming language representations may introduce additional complexity that could hinder practical applications.",
            "9": "- **Evaluation and Benchmarking**\n     - The paper primarily focuses on specific tasks (visual relation detection, scene graph classification, and situation recognition) and may not provide a comprehensive evaluation across a broader range of tasks.",
            "10": "- The comparison with baseline methods may not be exhaustive, and additional benchmarks could strengthen the validity of the results.",
            "11": "- **Generalization and Scalability**\n     - The approach may face challenges in generalizing to other domains or datasets that are not included in the ViStruct Suite.",
            "12": "- The scalability of the method, especially in terms of computational resources and training time, is not thoroughly discussed.",
            "13": "Suggestions for improvement\n   - **Simplify and Clarify Methodology**\n     - Provide a more detailed and step-by-step explanation of the implementation process to make it easier for others to reproduce the results.",
            "14": "- Include visual aids or diagrams to illustrate the workflow and key components of the proposed method.",
            "15": "- **Expand Evaluation and Benchmarking**\n     - Conduct additional experiments on a wider range of tasks and datasets to demonstrate the generalizability and robustness of the method.",
            "16": "- Compare the proposed method with a more extensive set of baseline models, including state-of-the-art approaches in related fields.",
            "17": "- **Address Scalability and Practicality**\n     - Discuss the computational requirements and potential challenges in scaling the method to larger datasets or more complex tasks.",
            "18": "- Explore ways to optimize the training process and reduce the computational overhead associated with the proposed approach.",
            "19": "- **Enhance Dataset and Annotation Quality**\n     - Provide more details on the dataset curation process and the quality control measures taken to ensure the accuracy and consistency of the annotations.",
            "20": "- Consider releasing additional annotated datasets or tools to facilitate the adoption and application of the proposed method by other researchers."
        },
        "A4jdslRJo0": {
            "0": "- This paper defines a visual structural knowledge pyramid and collects ViStruct Suite including multi-level structural knowledge aligned with the pyramid.",
            "1": "This knowledge pyramid and the curated dataset would help to promote future studies.",
            "2": "- This paper devises an elaborated training framework for VLMs to improve their visual structural knowledge extraction ability.",
            "3": "- Even though the curriculum-guided code-vision representation is an interesting idea, this paper lacks an in-depth ablation study on the proposed method.",
            "4": "For example, the additional experiments on the diverse order of curriculum and the design of a replay buffer help to convince the effectiveness of the proposed method.",
            "5": "- In ViStruct Suite, annotations in the visual event detection stage might contain noises since they are obtained by the off-the-shelf semantic role labeling system.",
            "6": "A detailed analysis of the curated dataset would be needed."
        },
        "BlR9jfm4BU": {
            "0": "The idea of encoding an image using programming language is quite interesting.",
            "1": "This programming language based approach could potentially further achieve improved performance using code based LLM.",
            "2": "The paper converts the visual relation prediction task into a masked language prediction problem.",
            "3": "This is very interesting.",
            "4": "I would recommend either add a section describing the inference stage or move the section A from appendix to main text.",
            "5": "For several rounds of reading the main text, I couldn't figure out how to do the inference.",
            "6": "After reading the appendix, I realized it is doing mask prediction.",
            "7": "I hope the author could provide more implementation details for how to reproducing the results in either main text or appendix.",
            "8": "I found it might be a little bit hard to find those information.",
            "9": "I think the author might also want to mention that the program language (generated) is not intended for directly running from some python environment.",
            "10": "In the first round of reading, I misunderstood that the generated code is intended for directly running."
        },
        "sQkTaUMrL5": {
            "0": "I like the idea of structuralizing the vision information (concept, attribute, location, relation, etc) into codes.",
            "1": "Compared with text, codes are more organized and more efficient to express high-level logic.",
            "2": "The curriculum learning framework propagates from the basic concept level up to the event level according to the complexity, which is intuitively reasonable.",
            "3": "Although almost no new methods are proposed, combining previous methods to adapt to the new problem is still meaningful.",
            "4": "Main concern: Only one backbone (OFA-base) is evaluated and the model size is too small.",
            "5": "At least two or three different pre-trained VLM backbones are required to demonstrate the generalizability of the proposed method as it's a plug-in method on top of pre-trained VLMs.",
            "6": "Also, the model scale is too small (just base).",
            "7": "I wonder if the conclusion that curriculum learning helps can still hold with larger pre-trained VLM, i.e., more pre-trained natural text knowledge injected.",
            "8": "Too few previous works are compared in Tab1 and Tab2 and they are too outdated.",
            "9": "I didn't see any evaluation on grounding and attribute prediction.",
            "10": "As they are also one of the curriculum levels, it's necessary to evaluate them."
        },
        "iN85PRiqUA": {
            "0": "Novel idea: Representing visual knowledge as code is a new idea as per my knowledge, including masked code prediction, and curriculum learning.",
            "1": "Scalable w.r.t new/larger/better models: I believe as VLMs improve, this technique should also improve.",
            "2": "The ViStruct Suite might be useful for benchmarking/training of future visual structure extraction models.",
            "3": "Evaluation is performed on multiple tasks 1.",
            "4": "Relatively low performance increase:  Although Table 3 and Table 4 show best results are achieved by Vistruct, the numerical values do not seem to be consistent with the value which are made bold in all columns (some non bold numbers are the highest it seems?).",
            "5": "I would request the authors to correct this.",
            "6": "Also see my next point regarding baselines considered.",
            "7": "Insufficient comparison with baselines: There are numerous methods for scene graph generation/relation prediction which do not seem to be included in Table3 and Table 4.",
            "8": "They are for example [1,2,3] and it is possibly many other scene graph generation methods (also see this somewhat old repository for baselines https://github.com/microsoft/scene_graph_benchmark).",
            "9": "In light of these advances in SGG, the utility of the current method is questionable.",
            "10": "Comparison with all are not necessary, but some latest/recent baselines should be included.",
            "11": "Limited datasets used: This work uses visual genome for benchmarking the VRD and SGC tasks.",
            "12": "Prior methods routinely used datasets like OpenImages.",
            "13": "Benchmarking on an extra dataset can improve this work.",
            "14": "Generalizability to different pre-trained models: It seems necessary to study the effect of changing pre-trained models.",
            "15": "Currently, table 3 baselines and OFA based model of this paper is not really comparable I believe.",
            "16": "I do not have a direct solution to this, but having more pre-trained backbones can improve this work and give the reader more clarity of the performance of the model w.r.t pretrained backbone.",
            "17": "Curriculum learning: more details are required to claim performance improvements.",
            "18": "for eg.",
            "19": "is the current ordering of curriculum necessary?",
            "20": "are same computation resources/epochs/data size used for the ablation which compares curriculum based learning with non curriculum based learning?",
            "21": "Applications of visual structure: Although there are many papers that tackle the problem of visual structure/scene graph generation (and probably some are better than this method as per the previous point), the utility of generating a scene graph alone is questionable.",
            "22": "Instead, applications of using scene graphs might be more interesting to improve this work, for eg.",
            "23": "this work cites Yüksekgönül et al., 2022 which shows that VLMs have a bag of words nature.",
            "24": "Can the current method extract visual relation ships which can then be used to make progress on ARO task provided by Yüksekgönül et al.",
            "25": "Joint training for image-text matching and code completion as done in this work, is a possible direction.",
            "26": "This point is largely a suggestion and not exactly a reason to reject.",
            "27": "References\n1.",
            "28": "Graphical Contrastive Losses for Scene Graph Generation https://arxiv.org/abs/1903.02728\n1.",
            "29": "RelTR: Relation Transformer for Scene Graph Generation: https://arxiv.org/abs/2201.11460v3\n1.",
            "30": "Noisy label correction for robust scene graph generation https://openaccess.thecvf.com/content/CVPR2022/papers/Li_The_Devil_Is_in_the_Labels_Noisy_Label_Correction_for_CVPR_2022_paper.pdf"
        }
    },
    "JHd4FSJSC5": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenge of few-shot classification in NLP, which is significant due to the high cost and time required for data annotation.",
            "1": "- The proposed method, AncSetFit, introduces a novel approach by anchoring task and label information through sentence embeddings, which is a unique contribution to the field.",
            "2": "- The use of contrastive learning and triplet loss to enforce class-specific embedding separation is an innovative technique that enhances model performance in data-sparse scenarios.",
            "3": "Potential reasons for acceptance\n   - The method demonstrates strong empirical performance across multiple datasets, outperforming existing methods in extremely few-shot settings.",
            "4": "- AncSetFit maintains computational efficiency with small model sizes, making it practical for real-world applications.",
            "5": "- The paper provides a thorough comparison with state-of-the-art methods, including detailed ablation studies and benchmarks, which strengthens the validity of the results.",
            "6": "- The code is publicly available, promoting transparency and reproducibility of the research.",
            "7": "Potential reasons for rejection\n   - **Limited scope of application**\n     - The method is specifically designed for text classification tasks and may not generalize well to other NLP tasks.",
            "8": "- The reliance on short, opposing textual statements for class descriptions may limit its applicability to more complex or nuanced classification tasks.",
            "9": "- **Dependence on anchor statements**\n     - The performance of AncSetFit heavily depends on the quality and relevance of the anchor statements, which may not always be easy to construct.",
            "10": "- The method's effectiveness decreases as the training data size increases, suggesting that its advantage is primarily in extremely low-data scenarios.",
            "11": "- **Comparison with larger models**\n     - While AncSetFit is efficient, it still shows a performance gap compared to larger models like ADAPET_XXL in some tasks, which may limit its appeal for applications where computational resources are not a constraint.",
            "12": "- The paper does not provide a detailed analysis of the trade-offs between model size, training time, and performance, which could be important for practitioners.",
            "13": "Suggestions for improvement\n   - **Expand the scope of application**\n     - Investigate the applicability of AncSetFit to other NLP tasks beyond text classification, such as named entity recognition or question answering.",
            "14": "- Explore methods to handle more complex class descriptions, potentially using hierarchical or multi-sentence representations.",
            "15": "- **Enhance the robustness of anchor statements**\n     - Develop techniques to automatically generate or refine anchor statements to reduce the manual effort and improve consistency.",
            "16": "- Experiment with different types of semantic information, such as using external knowledge bases or contextual embeddings, to enhance the anchor statements.",
            "17": "- **Provide a detailed analysis of trade-offs**\n     - Include a comprehensive analysis of the trade-offs between model size, training time, and performance, comparing AncSetFit with both smaller and larger models.",
            "18": "- Discuss the potential impact of different hardware configurations on the efficiency and scalability of AncSetFit.",
            "19": "- **Broaden the evaluation**\n     - Test AncSetFit on a wider range of datasets, including those with more complex or less structured class labels, to demonstrate its versatility.",
            "20": "- Include additional benchmarks against other state-of-the-art few-shot learning methods to provide a more comprehensive evaluation of its performance."
        },
        "3ZUQ9HOQwQ": {
            "0": "This work utilizes semantic label Information and improves the performance of SetFit.",
            "1": "This work aims to improve SetFit but compared with experimental design in SetFit, existing results in this work fail to demonstrate AncSetFit effectiveness."
        },
        "PAFjDzU7Kx": {
            "0": "- This paper proposes a simple but effective few-shot text classification method.",
            "1": "- Compared with ADAPET, the performance of their proposed method is lower or on par, although their method has some merits over ADAPET."
        },
        "KJDhb8Y7jw": {
            "0": "AncSetFit improves the original work (SetFit) without being more computationally demanding.",
            "1": "The experimental comparisons are lacking.",
            "2": "The authors focuses on the comparison with SetFit but neglects most other methods.",
            "3": "They only report ADAPET as comparison (and only for some of the datasets) which performs better than AncSetFit in 3 out of the 4 instances.",
            "4": "Moreover, the paper of SetFit has more comparisons that in AncSetFit have not been considered at all."
        }
    }
}