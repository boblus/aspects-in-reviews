{
    "UNDERSTANDING HTML WITH LARGE LANGUAGE MODELS": {
        "llm_review": {
            "0": ": Strengths: The paper addresses an under-explored yet critical area of leveraging LLMs for HTML understanding, relevant for web-based automation and information retrieval.",
            "1": "Demonstrates significant improvements in accuracy and sample efficiency over existing models by fine-tuning pre-trained LLMs on HTML understanding tasks.",
            "2": "The comprehensive experiments across different models, tasks, and a novel dataset contribute valuable insights to the field.",
            "3": "Open-sourcing the large-scale HTML dataset distilled from CommonCrawl is a significant contribution that will likely spur further research.",
            "4": "Weaknesses: The paper could benefit from a more detailed discussion on the limitations and challenges faced during the snippet extraction process and how they impact model performance.",
            "5": "A comparison with state-of-the-art models in related fields, such as document understanding or web page segmentation, might have provided a broader context.",
            "6": "The ethical considerations of automating web navigation tasks, especially in terms of privacy and security, were not discussed.",
            "7": "Clarity, Quality, Novelty, and Reproducibility: Clarity: The paper is well-written, with a clear structure that guides the reader through the objectives, methodology, results, and conclusions.",
            "8": "Quality: The research is of high quality, with rigorous experimentation and a significant dataset contribution.",
            "9": "Novelty: The approach of fine-tuning pre-trained LLMs for HTML understanding tasks is novel, especially the application across diverse tasks like semantic classification, description generation, and web navigation.",
            "10": "Reproducibility: With the detailed methodology, open-sourced dataset, and clear descriptions of the fine-tuning process, the study appears highly reproducible.",
            "11": "Summary of the Review: The paper presents a novel approach to HTML understanding using fine-tuned Large Language Models, demonstrating impressive performance improvements across several tasks.",
            "12": "The research is significant for its potential applications in web automation and information retrieval.",
            "13": "The open-source dataset further contributes to the field's advancement.",
            "14": "Despite minor weaknesses in discussing limitations and ethical considerations, the paper's quality, novelty, and clarity make it a valuable addition to the literature."
        },
        "review#1": {
            "0": ": Strengths: 1.That pre-trained natural language LLM can be effective for tasks involving HTML pages is interesting and can potentially find use in several interesting practical applications.",
            "1": "2.As no retraining of LLM with large HTML datasets is necessary, models for tasks involving HTML pages can be developed quickly and less expensively.",
            "2": "3.That raw HTML text can be used as input without needing parsing is an advantage.",
            "3": "4.Experimental results are very encouraging and validate the claim that pretrained LLMs can be effective for the three tasks.",
            "4": "Weaknesses: 1.It is claimed that these three tasks require understanding of both structure and content of the web-page.",
            "5": "While it is easy to see that textual content plays a key role in each of the three tasks, the role played by the structure of the web-page is not clear.",
            "6": "It can be argued that no significant HTML structure analysis or understanding is needed for these tasks.",
            "7": "For example, in Semantic Classification, what is most important for classifying HTML element 'input' into, say, 'username' is the value of its two attributes, 'type' and 'id'.",
            "8": "As these attributes are in the close neighbourhood of 'input', parsing of HTML is not strictly necessary.",
            "9": "Therefore, it might a good idea to do some experiments that demonstrate unequivocally the need for HTML structure analysis or understanding in these tasks.",
            "10": "One such experiment could be to map all HTML tags in the web-page except the salient tags to the same token (say, ***) so that the input is now a sequence of salient tags, and ***.",
            "11": "2.There is not much novelty in the methodological aspects of the work.",
            "12": "Clarity, Quality, Novelty And Reproducibility: The submission is well written and easy to understand.",
            "13": "The three canonical tasks are described well and the adaptation of the various LLM for building models for these tasks are well explained.",
            "14": "The proposed solution is simple and appears to be effective for the tasks considered and the datasets chosen.",
            "15": "There is not much novelty in methodological aspects and the work is primarily empirical in nature.",
            "16": "Experiments are designed well and should be easy to reproduce.",
            "17": "Datasets used in the experiments have been promised to be released.",
            "18": "The work should be interesting for practitioners.",
            "19": "Summary Of The Review: This work asks the question can off-the-shelf LLM trained on natural language text be used effectively for tasks that involve HTML pages.",
            "20": "It proposes three tasks as canonical tasks in understanding HTML.",
            "21": "It employs a variety of LLM to build models for the three tasks using a small amount of HTML data for fine tuning.",
            "22": "It shows that LLM does help these tasks significantly.",
            "23": "One key question not answered in this context is how much of HTML structure analysis and understanding is truly required for these questions."
        },
        "review#2": {
            "0": ": Strength The paper is easy to follow.",
            "1": "The study of how LLMs perform on raw HTML texts is interesting and novel.",
            "2": "Weaknesses Models that are trained on task specific data to compare against is questionable.",
            "3": "I expect to see more task-specific models such as StructuralLM [1], MarkupLM [2] that respect the tree structure of HTML (or some other methods mentioned in the paper) to be evaluated against.",
            "4": "However, the results seem to focus on LLMs only.",
            "5": "I don't see the point of compare different variants of LLMs with or without pre-training.",
            "6": "Basically, I need to see the following question answered: \"For specific web tasks, do I want to use a HTML specific model trained on the small dataset or just to fine-tune LLMs\"?",
            "7": "[1] Li, C., Bi, B., Yan, M., Wang, W., Huang, S., Huang, F. and Si, L., 2021.",
            "8": "StructuralLM: Structural Pre-training for Form Understanding.",
            "9": "arXiv preprint arXiv:2105.11210.",
            "10": "[1] Li, J., Xu, Y., Cui, L. and Wei, F., 2021.",
            "11": "MarkupLM: Pre-training of text and markup language for visually-rich document understanding.",
            "12": "arXiv preprint arXiv:2110.08518.",
            "13": "Clarity, Quality, Novelty And Reproducibility: Clarity: The paper is easy to follow as the content of the paper is simple.",
            "14": "Quality: The experiments seem to be well-done.",
            "15": "Novelty: The novelty is limited in terms of methodology.",
            "16": "It is mostly an empirical study for large LLMs on HTML texts.",
            "17": "Reproducibility: Some details are missing.",
            "18": "Either code or more detailed experimental setup should be provided in the supplementary material Summary Of The Review: I like the idea of studying LLMs on HTML texts.",
            "19": "However, it seems to me that the experiment setup (re.",
            "20": "models to compare against) cannot support the main claim of why one would like to do so.",
            "21": "Besides, the technical novelty of the paper is limited for a ML conference like ICLR; a NLP conference might be a better fit for this submission."
        },
        "review#3": {
            "0": ": Strengths: The paper is well-written and easy to follow.",
            "1": "The authors conduct comprehensive evaluations and analyses over a range of architectures, dataset sizes, and baselines to make the experimental conclusion convincing.",
            "2": "The authors create and open-source a new dataset/benchmark for HTML understanding.",
            "3": "Weaknesses: The novelty is primarily empirical, and the technical approach is essentially an application of known techniques (the standard pretrain-finetune paradigm of LLMs).",
            "4": "I examined the supplementary material provided by the authors and only found the data files.",
            "5": "It would be better if there were codes to prove its reproducibility.",
            "6": "Clarity, Quality, Novelty And Reproducibility: Clarity: Most things are explained clearly either in the main paper or the appendix, except for the few points that are raised in the Weaknesses above.",
            "7": "Quality: well-written work.",
            "8": "Novelty: The novelty is primarily empirical, and the technical approach is essentially an application of known techniques (the standard pretrain-finetune paradigm of LLMs).",
            "9": "Reproducibility: The Supplementary Material only provides the data file.",
            "10": "It is better to make the codes available for reproducing these results.",
            "11": "Summary Of The Review: In my opinion, the audience could gain insights from the thorough empirical analysis of HTML understanding from this paper.",
            "12": "However, the lack of technical novelty & contribution makes this work slightly below the acceptance bar of the conference.",
            "13": "Overall, I recommend a weak reject."
        },
        "review#4": {
            "0": ": Pros: The direction of HTML understanding is increasingly important, as recent work has started to show the promise of utilizing the semantics of HTML and web navigation.",
            "1": "The paper adds solid contributions to this direction by creating a novel and interesting task, and conducting many empirical studies with insights about architecture, (sub-linear) scaling effect, importance of context window/action history, etc.",
            "2": "Cons: On the proposed Description Generation task, authors only spend two paragraphs presenting results, where WebD-T55-3B can obtain 90.8% accuracy (exact string match??).",
            "3": "Does it mean the task is too easy?",
            "4": "How to justify it from the existing semantic classification task, as in Figure 1(a) they look similar?",
            "5": "Why in Table 3, dev numbers are much lower than test set?",
            "6": "I feel the new task/dataset needs more explanation and justification.",
            "7": "The modeling part lacks novelty, just basic pre-training + fine-tuning on LLMs, and seems there's no model design based on the features of HTML?",
            "8": "On MiniWoB, it's not very surprising that using a larger model with pre-training, you can beat smaller models from scratch.",
            "9": "Instead, the fact that LLMs are worse than RL performances should be discussed more -- does it mean the task is bad, or RL is important (but it's hard to do RL for LLM)?",
            "10": "(Minor) In intro authors claim \"no processing of HTML\", but there is still some pre-processing?",
            "11": "Clarity, Quality, Novelty And Reproducibility: Clarity: The paper is generally clear and easy to understand, some tables and figures in experiments are confusing or hard to read.",
            "12": "Novelty: The new task is somewhat interesting and novel.",
            "13": "The modeling part is not that novel.",
            "14": "Reproducibility: Authors claim to open-source the Description Generation dataset.",
            "15": "Summary Of The Review: I believe the paper makes solid empirical contributions by introducing a new task, and studying many LLM variants on three HTML tasks, with some insights about what's important for modeling HTML.",
            "16": "My main concern is that the new task is not clearly justified, and the modeling part might be too standard -- there is no HTML-specific model design involved, just studying basic LLMs.",
            "17": "I think the paper can benefit from more evidence about the value of the new benchmark (e.g.",
            "18": "what new insights/modeling capabilities it enables), or more interesting modeling."
        }
    },
    "WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus": {
        "llm_review": {
            "0": ": The paper presents a clear, technically sound, and experimentally rigorous approach.",
            "1": "The introduction of the WebBrain-Raw dataset is a notable strength, providing a valuable resource for the research community.",
            "2": "The proposed ReGen framework's emphasis on factual correctness through improved retrieval and pre-training is innovative and addresses a significant challenge in generating content based on web-sourced evidence.",
            "3": "However, the paper lacks a thorough discussion on the limitations of their approach, potential biases in the dataset, and the scalability of their solution to other languages or domains.",
            "4": "Clarity, Quality, Novelty, and Reproducibility: The paper is well-written, making it easy to understand the proposed task, methodology, and the significance of the results.",
            "5": "The research quality is high, with thorough experimentation and a clear presentation of results.",
            "6": "The novelty lies in the task formulation and the ReGen framework, contributing new insights into factual content generation from web sources.",
            "7": "The authors' commitment to releasing the WebBrain-Raw dataset and the detailed description of the ReGen framework enhance the reproducibility of their work.",
            "8": "Summary of the Review: Overall, this paper makes a significant contribution to the field of NLP by introducing a challenging new task and a corresponding large-scale dataset.",
            "9": "The proposed ReGen framework effectively addresses the task, demonstrating superior performance over existing methods.",
            "10": "While the paper is strong in most aspects, further discussion on its limitations and broader applicability would strengthen its impact."
        },
        "review#1": {
            "0": ": Strengths This paper promises a massive new dataset for retrieval augmented text generation, that could be very useful to the NLP community.",
            "1": "Having said that, I do have questions about the release with respect to copyright and privacy concerns (detailed below in the ethics section).",
            "2": "The end system can retrieve references, and link them to generated sentences, which is a very nice end product which could have real utility beyond unconstrained text generation, which cannot provide any form of attribution for its predictions.",
            "3": "There are a number of interesting modeling and training choices made, which lead to significant improvements over a very large language model (GPT3) and also similar retrieval agumented approaches with different choices of retriever and training procedure.",
            "4": "Weaknesses There are a lot of separate contributions here that are not independently evaluated (data filtering / retrieval filtering / warmup strategy).",
            "5": "It would be nice to see evaluations that validate these choices.",
            "6": "The reference passage filtering process selects the references included in the retrieval corpus references according to term overlap with the target Wikipedia article.",
            "7": "If I'm understanding this process correctly, this means that the references stored in the corpus have been selected according to observations of the test time targets, so there is some leakage of information from the test targets into the model input.",
            "8": "The paper should discuss this.",
            "9": "The prompt given to GPT3 \"Introduce [Page Title]\" does not mention that the target is Wikipedia-style text, and the example in Figure 1. does not look much like the start of a Wikipedia article.",
            "10": "Meanwhile, GPT3 is definitely able to generate Wikipedia style text if prompted to do so.",
            "11": "The comparision to GPT3 would be stronger if the prompt was better tied to the actual task.",
            "12": "Clarity, Quality, Novelty And Reproducibility: Clarity & Reproducability The paper is clearly written and all parts are pretty well described.",
            "13": "The authors commit to releasing the data, which will allow easy replication.",
            "14": "Novelty The dataset is similar in form to previous work, but extends that previous work to the scenario where references must be retrieved from a very large corpus.",
            "15": "Similarly, the model is a small modification of existing approaches but it appears to work better than well chosen baselines for this task.",
            "16": "Quality There are a lot of details in the filtering procedures used to select the contents of the dataset, and assign references to sentences.",
            "17": "These are all justified int the text, but are not supported by any sort of analysis.",
            "18": "If this is going to become a benchmark dataset, going forward, it would be good to see a discussion of how these choices affect the task.",
            "19": "In particular, see my question about test target leakage into the retrieval corpus above (under Weaknesses).",
            "20": "Summary Of The Review: This paper presents a significant new dataset and task that could be useful to the community, going forward, in benchmarking methods of retrieval augmented generation.",
            "21": "The new model is also quite different from previous work, which has generally relied on dense DPR-style retrievers.",
            "22": "Overall, I think this paper is a nice contribution and I am currently leaning toward acceptance.",
            "23": "However, I also have some serious questions below about the data release strategy, and how the authors propose to handle concerns about releasing multiple terrabytes of data which may include copyrighted works or personal information."
        },
        "review#2": {
            "0": ": Strengths: The task examined in this paper (retrieval + multi-document summarization) is important.",
            "1": "The dataset released by this paper is much larger than comparable datasets and could be useful for furthering work on this topic.",
            "2": "The authors describe in detail many technical details of ReGen.",
            "3": "These details can be useful to practitioners for reproducing ReGen’s results and in their own work.",
            "4": "Weaknesses: Limiting the generation to only the first passage of wikipedia pages is a pretty strong limitation, also making this work very close to existing multi-document summarization works.",
            "5": "The authors do acknowledge this similarity though.",
            "6": "In “Reference Passage Selection” and “Dataset Generation”, the authors select input passages for training with simple word overlap or BM25.",
            "7": "It seems like this could be easily improved by using an entailment model, a dense retriever or even SPLADE.",
            "8": "The proposed \"WebBrain-Raw\" dataset would be more interesting if it was multilingual instead of English-only.",
            "9": "Clarity, Quality, Novelty And Reproducibility: The paper is clear and good quality.",
            "10": "The system is described in detail and should be reproducible.",
            "11": "The novelty of the approach is mainly in the size of the released corpus and in including retrieval as part of the task, together with multi-document summarization.",
            "12": "The techniques proposed by the authors are very interesting, but they are only applied to the dataset they created, so it's not totally clear how their modeling choices would stack up against other methods on a more competitive benchmark.",
            "13": "Summary Of The Review: This looks like a useful dataset+baseline contribution for the important task of multi-document summarization.",
            "14": "The techniques used by the authors seem solid."
        },
        "review#3": {
            "0": ": Strength The paper introduces WebBrain, which lets the model retrieve supporting evidence and generate factual articles given a factual query.",
            "1": "The proposed dataset is somewhat similar to the Wizard of Wikipedia (Dinan et al., 2018).",
            "2": "The newly proposed dataset is interesting and large-scale.",
            "3": "The authors crawled and cleaned Wikipedia.",
            "4": "The proposed task and corresponding dataset are very interesting and worthy of future research.",
            "5": "The paper proposes a new retrieval-augmented generation framework based on SPLADE and FiD.",
            "6": "The proposed methods achieve the best results over automatic and human evaluation.",
            "7": "The experiment section is very comprehensive.",
            "8": "The authors conduct an ablation study with different retrieval models and show the impact of the different numbers of retrieved references.",
            "9": "The paper also checks the impact of a number of references.",
            "10": "Those results are clearly represented in tables or charts with detailed explanations.",
            "11": "The paper shows the case study, human evaluation, and reference mark correction strategy in the appendix.",
            "12": "Weaknesses The paper uses n-gram overlapping metrics for automatic evaluation.",
            "13": "The paper needs to include some newer metrics such as BERTscore (Zhang et al., 2019), and BARTScore (Yuan et al., 2021) which can check semantic similarity.",
            "14": "Most of the experiment analyses are in quantitative way.",
            "15": "I would like to see more qualitative analysis.",
            "16": "Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y.",
            "17": "(2019).",
            "18": "Bertscore: Evaluating text generation with bert.",
            "19": "arXiv preprint arXiv:1904.09675.",
            "20": "Yuan, W., Neubig, G., & Liu, P. (2021).",
            "21": "Bartscore: Evaluating generated text as text generation.",
            "22": "Advances in Neural Information Processing Systems, 34, 27263-27277.",
            "23": "Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., & Weston, J.",
            "24": "(2018).",
            "25": "Wizard of Wikipedia: Knowledge-powered conversational agents.",
            "26": "arXiv preprint arXiv:1811.01241.",
            "27": "Clarity, Quality, Novelty And Reproducibility: Some parts of the paper are not very clear.",
            "28": "The steps to create WebBrain-R and WebBrain-G is unclear.",
            "29": "The paper attached the implementation details in the appendix.",
            "30": "It also provides examples from the dataset for readers to check.",
            "31": "However, it does not provide any code for reproduction.",
            "32": "It shows the limitation and system demonstration in the Appendix.",
            "33": "Summary Of The Review: Overall, the paper proposes a new interesting task with a corresponding large-scale Wikipedia-based dataset.",
            "34": "The experiment part is quite comprehensive."
        },
        "review#4": {
            "0": ": Strengths: This is an interesting and important task, and I think it's one that could be a fruitful combination of lot of existing work on both generation and understanding.",
            "1": "Weaknesses: Despite the grandiose motivations, the authors consider an extremely narrow instantiation of this task: Only the first paragraph is used as the generation target, rather than entire articles.",
            "2": "When I read the abstract and introduction I was excited to see how the authors would handle generation / understanding of long sequences, since this is an area where I think this benchmark would be particularly useful, but was disappointed to see that they largely punted on this problem.",
            "3": "Furthermore, the authors don't use the entirety of the reference articles, instead taking only a single paragraph per article that is specifically chosen to maximize informativeness, which I think gives a bit of an unfair advantage to the retrieval-based systems (since they're much more likely to retrieve something useful).",
            "4": "In general, I feel like the paragraph \"Reference Passage Selection \" in 3.2 completely misses the mark.",
            "5": "In particular: The original Wiki articles and reference articles tend to be very long.",
            "6": "To adapt the capacity of most pre-trained language models (e.g., BERT has a 512 token limit), we use the first section of Wiki articles as the generation target, and we select the passage from the reference article with the highest PST value as the supporting passage.",
            "7": "I'm not sure that I feel very good about our benchmarks having to be \"adapted\" to be consumable for existing models while trading off the essence of the task (synthesizing information from multiple long documents to generate a long document).",
            "8": "We should set the benchmarks as goals, see how current models and techniques perform on them, and allow them to be targets for future work.",
            "9": "I feel like the baselines are pretty weak, and the ReGen model is a pretty standard retrieve-and-generate baseline.",
            "10": "I don't think there's too much modeling novelty here.",
            "11": "Furthermore, the query distribution seems pretty unnatural.",
            "12": "For example, in the appendix, the query used is \"how google translate work\".",
            "13": "This is a query with a pretty clear information-seeking / question intent that can be answered by a paragraph (or paragraphs).",
            "14": "However, the wikipedia page for \"Google Translate\" answers far more than just this question, it contains a variety of general information about the query \"google translate\" (which itself could have many different question intents).",
            "15": "So, I feel like the source of supervision provides a different sort of information than what a query like this would actually want (let alone that in practice, only the first passage is used, which contains even less information because of the way that lead sections in wikipedia are structured to contain general high-level information: https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Lead_section).",
            "16": "The original motivation mentioned \"unseen factual queries\", but it's not really clear that generating an article is what you want for this.",
            "17": "If it's factual, you can probably answer it with a paragraph or two.",
            "18": "I think this work would benefit from a clearer exposition of what these queries would actually be, and what the right source of supervision / data for effectively addressing these queries would look like.",
            "19": "Clarity, Quality, Novelty And Reproducibility: This paper was reasonably clear, and I thought that the setup was pretty solid.",
            "20": "It's an entry in a long line of work on generating Wikipedia paragraphs from references or other external knowledge.",
            "21": "Summary Of The Review: While I like where this paper is going, I feel like the practical artifact and problem tackled here are a bit too small-scale and narrow.",
            "22": "We're at a point in NLP where things actually kind of (?)",
            "23": "begin to work, and there have been a variety of impressive results on generating and understanding long documents.",
            "24": "I think this work would really shine in pushing this direction, but instead, it compromises itself to fit the limitations of existing models, limiting its usefulness for future work.",
            "25": "Thus, I'm not sure that I can recommend acceptance."
        }
    },
    "Pre-trained Language Models can be Fully Zero-Shot Learners ": {
        "llm_review": {
            "0": ": Strengths: 1.Novelty: NPPrompt introduces a novel approach for fully zero-shot learning, leveraging PLMs without additional data requirements.",
            "1": "2.Performance: Demonstrates superior performance over existing methods across diverse NLP tasks, indicating broad applicability.",
            "2": "3.Methodological Rigor: The paper provides a clear and detailed explanation of the methodology, supported by extensive experiments.",
            "3": "Weaknesses: 1.Reproducibility: While the paper mentions experiments and results, detailed settings for reproducibility (e.g., hyperparameters, computational resources) could be better emphasized.",
            "4": "2.Comparison with Baselines: The paper could benefit from a deeper analysis of why NPPrompt outperforms baselines, including insights into cases where it might not perform as expected.",
            "5": "3.Generalization to other languages or domains: The paper primarily focuses on English; extending and evaluating the approach for other languages or specific domains could provide further insights.",
            "6": "Clarity, Quality, Novelty, and Reproducibility: - The paper is clearly written and structured, making it accessible to readers.",
            "7": "- The quality of research is high, with rigorous experimentation and a novel contribution to the field.",
            "8": "- The novelty of the method is one of the paper's strengths, presenting a new pathway for zero-shot learning in NLP.",
            "9": "Reproducibility could be enhanced with more detailed experimental settings and access to code or datasets.",
            "10": "Summary of the Review: This work is a significant contribution to the field of NLP, introducing a novel approach for zero-shot learning that leverages the inherent capabilities of PLMs without additional data requirements.",
            "11": "The method shows impressive performance across a range of tasks, indicating its potential impact.",
            "12": "However, improvements in documenting reproducibility and extending analyses could further strengthen the paper."
        },
        "review#1": {
            "0": ": Strengths The proposed NPPrompt method is simple and effective.",
            "1": "It only requires the label strings for doing zero-shot classification.",
            "2": "The empirical results show that it is on the same level as other methods that need more information about the labels.",
            "3": "Weakness The proposed method is pretty limited to the classification tasks, particularly for news classification, where the label names are informative and have many semantic relevant words in the vocabulary.",
            "4": "It is also limited when the label names are multi-word expressions or phrases.",
            "5": "It does not support the cases where the label names are not informative enough and additional label description is needed.",
            "6": "The title and narrative can be misleading and an overclaim.",
            "7": "As mentioned before, it is limited to masked LMs and the advantage is mainly for tasks like news classification.",
            "8": "Suggestion: I suggest the authors extend the proposed method to multi-label classification settings where an example can have multiple acceptable news.",
            "9": "A major limitation is the multi-word expression and out-of-vocabluary label names.",
            "10": "How do you generalize the method to support that?",
            "11": "Also, I suggest the authors try some multiple-choice QA problems.",
            "12": "Clarity, Quality, Novelty And Reproducibility: The paper is mostly clear and the novelty is incremental.",
            "13": "I think the reproducibility should be okay since the method is pretty naive.",
            "14": "Can you show the kNN results for sentiment analysis tasks and NLI tasks?",
            "15": "I don't find them in the paper but they are quite important for qualitative analysis.",
            "16": "Summary Of The Review: Overall, I think the paper presents a novel zero-shot classification paper but the limitations are there and the performance is not that significant.",
            "17": "I think if the authors can extend the paper with my suggestions in the above section, the paper will be more suitable to the ICLR community.",
            "18": "Otherwise, it will have much less impact to the field."
        },
        "review#2": {
            "0": ": Strength Proposed method is simple, intuitive and effective.",
            "1": "Conduct extensive experiments on a wide range of NLU tasks.",
            "2": "Weaknesses The proposed method is not very novel.",
            "3": "Generalizing LMs to NLU tasks by checking the vocabulary distribution over the masked token is a common practice.",
            "4": "The method which leverages related words to label category does not look very novel to me.",
            "5": "Clarity, Quality, Novelty And Reproducibility: This paper is well-written.",
            "6": "The work is original, but the proposed method is kind of similar to many existing works, which makes it less novel.",
            "7": "Summary Of The Review: This paper proposed NPPrompt, which aims to enable LMs' zero-shot ability to NLU tasks without requiring any labeled sample or relying on humans to construct prompt label words.",
            "8": "Results show that NNPrompt outperforms strong baselines with a large margin.",
            "9": "However, the overall novelty of this paper is limited."
        },
        "review#3": {
            "0": ": The authors put significant effort on proving effectiveness of their method in a variety of NLP tasks.",
            "1": "However, I wanted to see significant test results to make sure that the improvements are not random.",
            "2": "Clarity, Quality, Novelty And Reproducibility: The paper was well written and easy to follow.",
            "3": "I would require authors to add the github link for the code.",
            "4": "Summary Of The Review: Overall, zero shot learning is an interesting topic in natural language processing as so many new categories and topics are being produced on the web.",
            "5": "The authors proposed a simple and easy to implement method for pre trained language models to minimize human effort in terms of labeling and building training data.",
            "6": "Overall I am satisfied with the current draft of the paper and request to move forward with discussion."
        }
    },
    "Leveraging Large Language Models for Multiple Choice Question Answering": {
        "llm_review": {
            "0": " Strengths: - The paper is well-structured, making it easy to follow the authors' thought process and the logical progression of their work.",
            "1": "- Novelty in addressing the MCQA task by changing the interaction pattern between the question-answering model and the input data.",
            "2": "- Comprehensive evaluation across 20 diverse datasets demonstrates the method's effectiveness and generalizability.",
            "3": "- The provision of code and detailed methodological descriptions supports the reproducibility of the results.",
            "4": "Weaknesses: - The paper could benefit from a more in-depth discussion of the limitations of the proposed MCSB approach, including potential biases introduced by symbol binding.",
            "5": "- Comparison with state-of-the-art methods focuses primarily on performance metrics, with less emphasis on computational efficiency and scalability.",
            "6": "- The impact of MCSB's effectiveness across languages or domains (beyond the datasets tested) remains unclear.",
            "7": "Clarity, Quality, Novelty, and Reproducibility The paper is clearly written and structured, presenting a novel approach to MCQA tasks that contribute to the field's understanding of leveraging LLMs in a more efficient and effective manner.",
            "8": "The quality of research is high, demonstrated by rigorous experimental design and thorough evaluation.",
            "9": "The novelty of the MCSB method and its impact on LLM performance in MCQA tasks are well-articulated.",
            "10": "Reproducibility is supported by the availability of code and detailed experimental setup.",
            "11": "Summary of the Review This paper presents an innovative and effective approach to improving LLMs' performance on MCQA tasks through MCSB.",
            "12": "The method's generalizability and potential to simplify computational processes in MCQA tasks are significant contributions to the field.",
            "13": "While the paper is of high quality and novelty, further exploration of its limitations and a broader comparison with existing methods could strengthen its impact."
        },
        "review#1": {
            "0": ": (Strengths) Reveals problematic ingredients for likelihood-based answering.",
            "1": "Introduce the concept of MCSB and measure it by PPA.",
            "2": "Concentrated results that significantly improves QA performance by using multiple-choice prompting.",
            "3": "(Weaknesses) Individual problematic ingredients are neither being theoretically-proven nor empirically-proven.",
            "4": "No novel/brand new ideas.",
            "5": "Mostly empirical analysis based on OpenAI playground.",
            "6": "Some major arguments are less supported.",
            "7": "Clarity, Quality, Novelty And Reproducibility: The submission is an analysis paper rather than finding something new.",
            "8": "Summary Of The Review: (Major concerns) How to make sure Codex model clearly outperforms Instruct model?",
            "9": "This is a critical question as the authors measures the main experiments (Table 2) that compare Multiple Chocie Prompting (MCP) and Cloze Prompting (CP) only with Codex model.",
            "10": "The capability to perform MCSB could be due to human feedback alignment by Reinforcement Learning rather than other points indicated by the authors.",
            "11": "Are the PPA difference between Codex and Instruct (in Figure 2) statistically significant?",
            "12": "While no statistical test has been provided, it seems not easy to decline null hypothesis that says the difference is a random effect.",
            "13": "Only Codex tested on OpenBookQA shows strong performance gain when using MCP, whereas Instruct outperforms Codex on the other two tasks in Table 1.",
            "14": "More detailed experiments are necessary to convince how Codex achieve such higher accuracy.",
            "15": "(Minor concerns) Any reason to choose OpenBookQA which also matters the performance of retriever?",
            "16": "Do you know how Codex model is exactly trained?",
            "17": "Codex model that you used could be first based on Instruct, then being further trained on code data.",
            "18": "Equally likely, Codex model might perform it's own alignment similar to Instruct but based on the preference of generated codes."
        },
        "review#2": {
            "0": ": Strength: This paper is well-written and easy to understand.",
            "1": "The authors propose a simple but effective prompting method, which outperforms previous CP methods, approaching or even surpassing SOTA performance.",
            "2": "Experimental results show that the MCQA ability of LLMs has been previously underestimated.",
            "3": "And there is a better way to prompt a single LLM.",
            "4": "The potential of multiple choices prompts can be further tapped.",
            "5": "Future work include prompt engineering is still promising.",
            "6": "Weakness: The novelty of this paper is limited.",
            "7": "Multiple choices prompting (MCP) has been used in other QA tasks, such as TruthfulQA and RACE.",
            "8": "Although the experimental results prove that the proposed multiple choices prompting (MCP) methods can outperform existing cloze prompting (CP) methods, the reasons behind it are still unclear.",
            "9": "Since the authors have listed several problems within CP methods, I'm curious about whether these problems are all solved or avoided by their MCP methods.",
            "10": "More analysis is needed to show this.",
            "11": "Clarity, Quality, Novelty And Reproducibility: Clarity Good.",
            "12": "But it would be better to include more analysis.",
            "13": "Quality Good.",
            "14": "A simple but effective prompting method.",
            "15": "Novelty Novelty is limited.",
            "16": "Details can be found in the weakness part.",
            "17": "Reproducibility Good.",
            "18": "The code is attached by the authors.",
            "19": "Summary Of The Review: Interesting paper but not good enough.",
            "20": "It would be better to include more analysis on whether MCP can deal with these several problems that CP faces."
        },
        "review#3": {
            "0": ": Strengths: The authors explain their approach well.",
            "1": "They also discuss the (somewhat surprising) variance between different models in their ability to separate the letter from the answer.",
            "2": "(They call this Multiple Choice Symbol Binding.)",
            "3": "The approach is evaluated on a wide range of (20) datasets.",
            "4": "Weaknesses: The approach is not new, just discussed and evaluated.",
            "5": "The authors differentiate their suggested prompting from “prompt engineering”, which they seem to define as fine-tuning of prompts to increase model performance.",
            "6": "However, I’m not convinced that these are fundamentally different, and would include research such as theirs in the general domain of prompt engineering.",
            "7": "Clarity, Quality, Novelty And Reproducibility: The paper is well written and I believe the experiments are verifiable with the given information, i.e.",
            "8": "it should be possible to reproduce them.",
            "9": "Regarding novelty, I am less convinced.",
            "10": "The authors mention others having used the MCP approach.",
            "11": "So the main addition here is the systematic discussion and wide range of experiments.",
            "12": "Summary Of The Review: The authors discuss an alternative (but not novel) way to prompt LLMs for better results on multiple-choice tasks.",
            "13": "The prompt is well-motivated and thoroughly discussed with a good range of experiments that support the author's arguments.",
            "14": "However, it is not novel: it is a fairly obvious way to prompt and has been tried before."
        }
    },
    "Words are all you need? Language as an approximation for human similarity judgments": {
        "llm_review": {
            "0": " Strengths: Comprehensive Evaluation: The evaluation of 611 pre-trained models across three distinct modalities presents a thorough and broad analysis, showcasing the extensive work undertaken.",
            "1": "Innovative Language-based Methods: The introduction of language-based methods for similarity approximation, alongside the novel STEP-Tag pipeline, represents a significant contribution to the field, particularly in terms of efficiency and scalability.",
            "2": "High-Quality Dataset: The large dataset of human judgments collected and released provides a valuable resource for future research, enhancing reproducibility and further study.",
            "3": "Practical Guide for Researchers: The concise guide based on experimental results is a practical tool for researchers facing limitations in judgment collection or model access, facilitating more effective research design.",
            "4": "Weaknesses: Model-Specific Analysis Limitations: The paper's focus on pre-trained models might limit its applicability to domains where such models are not available or suitable.",
            "5": "Additionally, the performance gap noted between human judgments and DNNs might be model-specific rather than a general limitation of DNNs.",
            "6": "Potential Bias in Language Data: The reliance on language-based methods might introduce biases related to language use and representation, which the paper does not fully address in terms of potential impacts on similarity judgments.",
            "7": "Ethical Considerations: While the paper briefly mentions the risk of embedding human biases in ML datasets, a deeper analysis of ethical implications, especially concerning the use of language data and the broader societal impacts of these biases, would strengthen the paper.",
            "8": "Clarity, Quality, Novelty, and Reproducibility Clarity and Quality: The paper is well-structured and clear, with a strong emphasis on methodological rigor and comprehensive data analysis.",
            "9": "The quality of the research is evident in the extensive dataset and detailed experimental setup.",
            "10": "Novelty: The novel use of language-based methods and the STEP-Tag pipeline for approximating human similarity judgments is a significant and innovative contribution to the field.",
            "11": "Reproducibility: With the release of the dataset and detailed descriptions of the methodologies and models used, the study sets a high standard for reproducibility.",
            "12": "Summary of the Review The paper presents a comprehensive and innovative approach to approximating human similarity judgments using pre-trained DNNs and novel language-based methods.",
            "13": "The strengths lie in the extensive model evaluation, the innovative methodology for data collection, and the practical guide for researchers.",
            "14": "However, the study could be improved by addressing potential biases introduced through language data and by expanding the discussion on the ethical implications of their findings.",
            "15": "Overall, the contributions are significant, providing valuable insights and resources for the NLP and broader AI research community."
        },
        "review#1": {
            "0": ": [Strength] The concept of the paper is nicely presented for the next AI abundant society where demand for human judgments is more increased.",
            "1": "They proposed the novel similarity approximation method with a smaller number of judgements.",
            "2": "They provide the evaluation data in the supplemental material, which is beneficial.",
            "3": "Overall, I believe what the authors done is quite systematic and very beneficial for the future refinement of the DNN model.",
            "4": "[Weakness] Representational similarity For beginners, the word of representation similarity is not easy to follow.",
            "5": "At least, authors should define the terminology in the introduction.",
            "6": "The title In particular, the linkage between title and abstract seems not clear at the first glance.",
            "7": "Please consider to update the title and abstract so that readers can easily understand the topic theme.",
            "8": "Is English best?",
            "9": "There are many languages.",
            "10": "In my opinion, I view that authors selected English language to represent the basis of all things in nature.",
            "11": "Why English(e.g., language system such as polysemous, grammar, dataset availability…)?",
            "12": "Justification of English employment would make the paper better.",
            "13": "Related to above topic, the paper report “N = 1,492 US participants for the new behavioral experiments reported in this paper.” Are they all Americans and share the same contexts?",
            "14": "(e.g., cultures…) Crowd scouring How long does it take to STEP-Tag?",
            "15": "It is good that measure actual time that participants consumed.",
            "16": "Comparing with that with the time used for writing captions may strengthen your claim.",
            "17": "Discussion I feel strange because the paper end with “Discussion”.",
            "18": "Please consider adding conclusion section or rename the last section as “Discussion and Conclusion”.",
            "19": "Clarity, Quality, Novelty And Reproducibility: Quality Good.",
            "20": "Issues for human judgment cost have been solved with the proposed idea.",
            "21": "Clarity Good.",
            "22": "In the experiment, model they used is sufficient.",
            "23": "Originality Good.",
            "24": "The idea of methods themselves are not very novel.",
            "25": "However, I believe the exhaustive amount of examination is valuable, which increase the originality, thus it should be highly evaluated.",
            "26": "Summary Of The Review: I think this paper should be accepted for the “Human like” evaluation point of view as DNN becomes more sophisticated.",
            "27": "I believe their finding is also beneficial for the community."
        },
        "review#2": {
            "0": ": Strength: The paper is overall well-written and easy to follow.",
            "1": "Multiple datasets are evaluated for the proposed method.",
            "2": "Multiple modalities are explored and evaluated on the proposed method.",
            "3": "Weakness: The technical contribution of the proposed method is weak.",
            "4": "No baselines are compared to the proposed method in the experiment section.",
            "5": "Clarity, Quality, Novelty And Reproducibility: The paper is generally well-written.",
            "6": "The authors consider multiple modalities and datasets to evaluate their proposed method.",
            "7": "But the novelty of the proposed method is weak.",
            "8": "Summary Of The Review: The main issue of the paper is the novelty of the proposed method.",
            "9": "Based on technical novelty and insufficient experiments, I don’t think the current version meets the standard of the ICLR."
        },
        "review#3": {
            "0": ": Strengths: I think this is a well-motivated problem.",
            "1": "Learned representations are often used as features in the small-data regimen or sometimes directly for getting proximity scores in an AI setting.",
            "2": "This paper address the human interpretability of these representations by (i) confirming that human similarities and proximity scores from models can vary a lot, (ii) text-descriptions or tags can be leveraged and stacking these representations with the model-learned representations can help.",
            "3": "I also appreciate that the technique is scalable and in many cases not that much of an overhead to implement.",
            "4": "I appreciate the arguments in the related text that leverage cognitive science literature.",
            "5": "In addition, the paper is easy to follow.",
            "6": "Weaknesses: The paper doesn't have too many weaknesses.",
            "7": "I was wondering if we could get some numbers on if the stacked representations help in additional downstream tasks like say classification (i.e.",
            "8": "does the performance on imagenet improve if you use imagenet + text).",
            "9": "However, I understand that this can be significant undertaking and do not want to base my review on this experiment but it is a potential future direction.",
            "10": "Clarity, Quality, Novelty And Reproducibility: The paper presents a straightforward but well-motivated idea.",
            "11": "Having representations match human similarity judgement is indeed useful.",
            "12": "Implementing this technique would require (i) ability to acquire free-text or tag values which are domain-dependent but not prohibitively expensive, (ii) ability to obtain LLM representations which are straightforward since high quality implementations and libraries exist for these now.",
            "13": "Reproducibility is not an issue.",
            "14": "Summary Of The Review: Well motivated problem Clearly described technique that is scalable, easy to implement Techniques like these that are easy to implement and help with interpretability are of great use in the small-data regimen (where the bulk of us are).",
            "15": "I would like to see this paper at ICLR."
        },
        "review#4": {
            "0": ": Reconstructing pairwise human similarity judgments across N objects is an interesting task, and the author's idea of gathering textual descriptions is clever.",
            "1": "The performance of the stacked models (which depend on domain-specific pretrained models and text-text similarities) and text-only models are surprisingly strong across a variety of domains --- this finding has implications for the relative surprising representational strength of text.",
            "2": "The annotation process they describe for their tag corpus is interesting --- multiple rounds of annotations are undertaken with crowdworkers fixing each-other's errors.",
            "3": "The authors consider a large number of models across many domains --- the benchmark experiments are extensive, and the \"size vs. accuracy\" plots in the end are cool.",
            "4": "Figure 5 is amazing --- few works offer such clear practical guidance.",
            "5": "Finally, I liked the hopeful message in the end --- that a combined multimodal approach seems to work best, so don't completely discard the domain-specific representations.",
            "6": "It should be noted that the cited work where some of the data is from, Marjieh et al.",
            "7": "2022, is quite similar in the sense that it also proposes to estimate human similarity judgments.",
            "8": "This work appears to extend that work by exploring more types of models and their combinations.",
            "9": "My main concerns are that this set of experiments suggests some clear next setups that I don't nesc.",
            "10": "think are out of scope for this work.",
            "11": "Specifically: If the goal in practice is to reconstruct human judgments --- I would have liked the authors to compare against a supervised setup.",
            "12": "How much of the remaining misalignments can be fixed by gathering n << N^2 additional pairwise judgments and then training a supervised model ontop of the models?",
            "13": "I would have liked to have seen more experiments with even less supervision --- is it possible to gather n < N captions, and train a model on those very few pairs to map to text, and then use the resulting modality --> text model as input to the models shown here?",
            "14": "I would have appreciated more discussion of tags vs. captions for the images.",
            "15": "In figure 3A, I assume that the text only methods use captions unless specified otherwise.",
            "16": "But --- because the tags were not fed to, e.g., BERT/RoBERTa, I can't tell if the performance gains are due to the full captions being used, or if the gains are due to the more expressive models.",
            "17": "A trivial linearization of, e.g., \"A photo of a tag1, tag2, and tag3\" handed to a LLM would have been a nice comparison to see.",
            "18": "Finally, I would have been interested in a fully LLM setup where instead of pairwise cosine similarities, both text pairs are fed to a very large LLM (e.g., OPT or GPT3) and the model outputs a likert rating as text.",
            "19": "Clarity, Quality, Novelty And Reproducibility: As I mentioned, the novelty is somewhat limited because of the prior human judgements work the authors get their data from.",
            "20": "But, the extensive set of experiments is interesting/new.",
            "21": "Summary Of The Review: Overall, this work address an interesting task/approach --- approximating pairwise similarity judgments over N objects with only O(N) annotation via language models.",
            "22": "While this setup has been considered by Marjieh, this work presents more experiments that show the text representations are surprisingly strong.",
            "23": "They consider a very large number of models, modalities, etc.",
            "24": "The released tag sets would likely be useful to someone.",
            "25": "But, a handful of missing additional experiments, which are arguably in-scope for the present work, are not presented.",
            "26": "And, prior work has already made the observation that LLMs can approximate similarity judgments across non-textual modalities, which appears to somewhat limit the novelty of this work.",
            "27": "After response: The authors seemingly ran all of the experiments I suggested and have incorporated them into the paper.Amazing :-) Furthermore, the authors clarified the difference with prior work."
        }
    },
    "Pretrained Language Model in Continual Learning: A Comparative Study": {
        "llm_review": {
            "0": ": Strengths: The paper provides an extensive comparative analysis across multiple PLMs and CL methods, contributing significantly to understanding their interactions.",
            "1": "Layer-wise and task-wise probing analyses offer deep insights into how different layers of PLMs are affected by CL, revealing critical aspects of model susceptibility to forgetting.",
            "2": "The methodology for benchmarking and analysis is rigorous and well-designed, covering a comprehensive range of settings and metrics.",
            "3": "Weaknesses: The paper primarily focuses on quantitative analysis and could benefit from more qualitative examples to illustrate the practical impact of the findings.",
            "4": "While the study is extensive, it covers only a subset of available PLMs and CL methods, potentially limiting the generalizability of the conclusions.",
            "5": "Clarity, Quality, Novelty, and Reproducibility: The paper is clearly written and well-structured, with high-quality research and novel insights into the interaction between PLMs and CL methods in NLP.",
            "6": "The detailed methodology and provision of code and datasets support the potential for reproducibility and further research.",
            "7": "Summary of the Review: The paper offers valuable contributions to the NLP and CL communities by thoroughly analyzing the performance of different PLMs and CL methods.",
            "8": "Its rigorous benchmarking approach and novel layer-wise probing analysis provide deep insights and open up new research avenues.",
            "9": "While expanding the range of PLMs and CL methods studied could further enhance its impact, the paper's clarity, quality, and potential for reproducibility make it a significant contribution to the field."
        },
        "review#1": {
            "0": " I think a comparative study paper should suffice at least two conditions to be considered for a publication at a venue like ICLR.",
            "1": "First, it should present a novel view on the problem, and second, it should draw a novel conclusion out of the experiments.",
            "2": "Although the paper could be a good survey for readers who want to learn about continual learning, I think its viewpoint is not new and its conclusion is not surprising.",
            "3": "While it is helpful to know that rehearsal works better than regularization in most datasets, this is not entirely a surprising result.",
            "4": "I think it is a common belief that rehearsal-based is more robust against catastrophic forgetting, while regularization-method is more space-efficient in that it doesn't have to store examples.",
            "5": "The fact that the last layer suffers from catastrophic forgetting is also not a surprising result, given that the lower layers are known to encode linguistic features and the upper layers encode task-specific features.",
            "6": "Summary Of The Review: While the paper can be helpful for readers who want to learn about continual learning in text classification using pretrained language models, the paper does not seem to bring sufficient a novel viewpoint or conclusion to be published at ICLR."
        },
        "review#2": {
            "0": " Although the authors have conducted quite a lot of experiments, the phenomena shown in experiment results is hardly surprising to me.",
            "1": "It is not surprising that the pre-trained language models would have forgetting issues when fine-tuned on downstream tasks.",
            "2": "It is also not surprising that rehearsal-based methods perform the best for pre-trained models.",
            "3": "Moreover, the paper draws a conclusion that BERT is the most robust one and is a good option if a continual learning process is going to be conducted.",
            "4": "Based on this, the authors provide a few analyses on BERT’s ‘secret’ for continual learning.",
            "5": "However, compared with other pre-trained models, I don’t see that BERT is significantly better than others given the figures and tables.",
            "6": "I feel from the figures and tables, BERT and other models look similar.",
            "7": "The authors didn’t give a comprehensive explanation on how they read such information or a concrete quantitative comparison to support this claim.",
            "8": "Summary Of The Review: A thorough empirical analysis with unsurprising conclusions"
        },
        "review#3": {
            "0": " Strengths: Overall the study is very thorough covering both the correct range of options for each axis studied and a set of relevant cross-axis multiple variable experiments.",
            "1": "The organization is good (but not perfect, see below), its strengths are that the different options considered along each axis are clearly laid out ahead of time, with the exception of the continual learning strategies.",
            "2": "The layer-wise analysis in particular is interesting and tells a coherent story, despite the challenges of displayed complicated 3D data.",
            "3": "Overall, it seems that recently many studies compare quantitatively against multiple PLMs, which ultimately appear similar due to only slightly different performance numbers.",
            "4": "This study's most successful contribution in my opinion is an exploration of the qualitative differences among PLMs in the continual learning setting.",
            "5": "Weaknesses: It would be really great to see how the insights after analysis can be used to improve performance.",
            "6": "It's probably not absolutely required given the focus on probeing, but it would go a long way towards validating the insights.",
            "7": "Adjusting the numbers to achieve the 5/4/3/2 cuteness gets slightly in the way of understanding, unfortunately.",
            "8": "The main reason for this is that the \"veins of CL methods\" has a different number over the course of the paper, which makes it hard to identify when a given list of N things is a list of the \"veins of CL methods\".",
            "9": "Specifically, in the abstract and intro this number is 4, in section 2.3 this number is 3, in section 3.1 this number is 6, in Table 1 this number is 5 (two different sets of 5), in Figure 1 this number is 6, and in Figure 2 this number is 4.",
            "10": "For a paper that has so much going on and so many different lists of different sizes, keeping these consistent would make it much easier for the reader to understand at any point what exactly this given list of N items is referring to.",
            "11": "Along the same lines, it would help to be consistent with the language around each set of N things.",
            "12": "For example, the \"veins of CL methods\" are called at least \"veins\", \"schemes,\" and \"approaches\" at different points.",
            "13": "Section 3.2, Table 1, and Figure 1 are relatively weak in my opinion.",
            "14": "What am I supposed to conclude?",
            "15": "I can look at the table and see the different results, but so what?",
            "16": "What should I be drawing my eye to in the table (bold would help)?",
            "17": "The Figure here is too small to even attempt to parse.",
            "18": "It would be very helpful to have a sentence that gives intuition about what's being measured with the accuracy metric.",
            "19": "The definition is there, but it took me a second to realize that the intuitive idea is that it's measuring the accuracy of past tasks after the model has moved on to learning new tasks.",
            "20": "Summary Of The Review: Overall the authors perform a quite deep study of using pretrained language models for continual learning.",
            "21": "Despite some weak points in the analysis of the quantitative results and inconsistent organization/language around the CL approaches, the thoroughness of the study, in particular the analysis at a layer-by-layer level, is likely of interest to the broader community."
        },
        "review#4": {
            "0": " Strenghts: The analysis of PLM is carried out over different continual learning techniques, NLP tasks, PLM variants and finally by layer-wise probing analysis.",
            "1": "Understanding the strengths and weaknesses of each PLM is very desirable research progress.",
            "2": "The authors also provide new research questions that arise from the analysis and point to interesting unsolved research challenges.",
            "3": "Evaluation starts off with the expected lower and upper bound of performance, and then moves on to disentangle FWT and BWT (backward, forward performance) when using various CL techniques.",
            "4": "The chosen data sets are not well behaved, i.e.",
            "5": "experience imbalances, which makes the results more realistic (less artificial).",
            "6": "Overall, this study provides a necessary step towards exploring future continual learning methodology and explores many important factors on eight pages.",
            "7": "From the batch-learning adaptation literature on PLMs one may expect baselines such as various adapter block versions or 'anti forgetting hacks', but it is understandable that the authors did not test these, since adapters would likely introduce complexity per increment and quickly become impractical.",
            "8": "As the authors mention, CL specific future extensions to adapters are conceivable, but a work of their own.",
            "9": "Minor weaknesses (easily fixable suggestions for improvement -- 1 content page left for improvements): Some plots seem to be a very small, and may be enlarged to use the 9th page.",
            "10": "Fig 2 plots should share a larger model legend (on the figure top or bottom), so the bars can become wider and easier to distinguish Fig 2 color could be made more distinguishable, especially since the plots are narrow.",
            "11": "Tab 1 could underline the best non joint performance — makes it easier to glance sec 4.",
            "12": "That Transformer layers, except the classification layer, do not adapt much during fine-tuning, is a known result (hence assumptions 1 and 2 in the paper), which should be cited — see BERTOLOGY Primer by Anna Rogers for references.",
            "13": "Here intermediate layers are shown to forget as well, so this is a nice new finding, that can be contrasted.",
            "14": "Fig 3: I assume the figure color-scale is probe performance?",
            "15": "Also, the buffer size, e.g.",
            "16": "er-200 should be explained with an example.",
            "17": "Is it 200 er samples, 200 samples/ class?",
            "18": "In section 5.1 it should be (re-)mentioned what the buffer size means.",
            "19": "Spelling errors (not all listed): using a TTS app/function makes it easy to find these sec 2.3.",
            "20": "\"eg\".",
            "21": "sec 3.2: could be find ... can be found tendency to forgetting ... to forget/ towards forgetting reach the highest .. reaches 4 interferes ... with the PLMs ability to retrain _ representations 4.1 Joint ... Joint multi-task training from sketch ... from scratch 4.2. methods making sense ... make sense The sentence \"(2) The classification layer clf is typically the most fragile of BERT, where continual learning learning methods making sense.\"",
            "22": "is not understandable.",
            "23": "Questions to the authors: None, regarding clarity.",
            "24": "Summary Of The Review: The problem, proposed CL algorithms, benchmarks and evaluation metric are suitable to answer the research questions.",
            "25": "The performance and layer-wise analysis is conductive to deepen an understanding for the shortcomings and potential opportunities of PLMs for continual learning.",
            "26": "Unsurprisingly, variants of ER are the most effective technique in incremental CL with PLMs, which is somewhat disappointing, but also a standard outcome in CL.",
            "27": "The paper is mostly well written, and the experiments (sub research questions) are logically structured.",
            "28": "Some minor details can be improved and have been pointed out in the review.",
            "29": "Insights are valuable and provide a solid foundation for followup studies.",
            "30": "The details in the appendix were interesting and added to the paper and its reading flow, e.g.",
            "31": "by moving cumbersome details like hyperparameters to a dedicated appendix section.",
            "32": "I thus feel confident to recommend this paper for acceptance."
        }
    },
    "Exploring extreme parameter compression for pre-trained language models": {
        "llm_review": {
            "0": " Strengths: The paper tackles the significant issue of PLM size, making a compelling case for the need for more efficient, smaller models without compromising performance.",
            "1": "It introduces an innovative application of Tucker decomposition for PLM compression, which is both novel and technically sound.",
            "2": "The empirical results are impressive, especially the achievement of a tiny BERT variant that retains most of the performance of the full model while being significantly smaller and faster.",
            "3": "Weaknesses: The discussion on the scalability of the proposed approach as the sets produced by the decomposition grow larger is somewhat lacking.",
            "4": "There might be concerns about the reliability of probabilistic inference in such cases.",
            "5": "While the paper provides a good comparison with existing methods, it could benefit from a deeper theoretical discussion on why Tucker decomposition outperforms other techniques in this specific application.",
            "6": "Clarity, Quality, Novelty, and Reproducibility The paper is well-written and clear, making it accessible to readers familiar with PLMs and tensor decomposition techniques.",
            "7": "The research is of high quality, employing rigorous experimental methodologies and providing a comprehensive comparison with baseline models.",
            "8": "The novelty lies in applying Tucker decomposition to compress PLMs efficiently, which is a relatively unexplored area.",
            "9": "The experiments appear reproducible, with sufficient details on the implementation and evaluation settings.",
            "10": "Summary of the Review The paper presents a significant contribution to the field of NLP by proposing an effective method for compressing PLMs using Tucker decomposition.",
            "11": "Its approach not only preserves the performance of the original models on benchmark tasks but also improves inference speed, addressing a critical need for more efficient NLP models.",
            "12": "While the paper is strong in its empirical evaluation, further exploration into the theoretical underpinnings of its method and its scalability would enhance its contribution."
        },
        "review#1": {
            "0": " Large scale pre-trained language models have demonstrated their effectiveness.",
            "1": "However the large model size makes it difficult to deploy and compressing such models have drawn a lot of interest.",
            "2": "This paper aims to compress PLMs to extremely small size mainly from the perspective of decomposition.",
            "3": "It introduces several decomposition methods and makes a comprehensive comparison among them from the perspective of compressing Transformer layers.",
            "4": "The Tucker decomposition is chosen to be the final solution due to its compression ratio.",
            "5": "The motivation is clear and the methods are technically sound.",
            "6": "Though the introduced decomposition methods are not new, the adaption to the Transformer layers and corresponding analysis are comprehensive.",
            "7": "The experimental results demonstrate the effectiveness of the method.",
            "8": "Especially, the compressed model size is really competitive.",
            "9": "Some weaknesses: The authors do not include embedding layer and prediction layer size in experiments, while only report the Transformer encoder size.",
            "10": "I know that this can make the size of compressed model really amazing (e.g., 1.8M) and the compression ratio amazing (e.g., 86M/12.3M=7) but is not fair as the whole model including the embedding layer are used when deploying.",
            "11": "If the embedding layer is added, the model size will increase a lot, and the compression ratio will decrease, which make the experimental results less surprising.",
            "12": "But this should be made clear.",
            "13": "The authors name a lot of related works, but compare only very few of them in the experiments.",
            "14": "Some other method(s) are missing in the related works.",
            "15": "For example: [1] Some typos: Section 5.1, \"...are not exactly equal to the the raw weights...\", duplicate \"the\"?",
            "16": "Section 6.2, \"...outperforms ALBERT - the latter needs...while the latter does not...\", two \"latter\"?",
            "17": "reference: [1] Xu, Jin, et al.",
            "18": "\"NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search.\"",
            "19": "Summary Of The Review: The paper presents extreme compression on pre-trained language models.",
            "20": "Though the introduced methods are not new, the adaptation to the Transformer layers and the analysis are interesting, and the experiments are convincing.",
            "21": "Though there exist some weaknesses, I think the paper is of good quality, if the authors could mitigate them."
        },
        "review#2": {
            "0": " Reasons for score: I think the idea proposed in the paper is novel, but some design choices can be further elaborated and there should be more experiments on larger models and more ablation studies.",
            "1": "Detailed comments: Strengths: Applying tensor decomposition across layers to utilize the similarity between layers is novel.",
            "2": "This is a valuable contribution to the model compression community.",
            "3": "The paper analyzes the optimal way to perform matrix multiplication given the compression method proposed in the paper.",
            "4": "Weaknesses: The paper proposed multiple potential ways of compressing weight matrices (matrix decomposition and tensor train decomposition) as some alternatives to the proposed Tucker decomposition.",
            "5": "However, the author didn't compare with tensor train decomposition due to time constraints.",
            "6": "I believe the paper will be more solid by adding ablation studies on tensor train decomposition.",
            "7": "The paper only performs experiments on BERT-base and TinyBERT models, but I believe that the compression method proposed in the paper should be more demanded by larger models.",
            "8": "Some design choices in the paper seem arbitrary.",
            "9": "For example, why do you jointly compress all the layers, including all the FFN weights and attention weights?",
            "10": "I can understand the similarity of the attention query weight vectors across the layers, but I can’t understand why all the weights are merged.",
            "11": "In addition, the reason for splitting each FFN weight matrix into 4 seems to make it possible to combine it with attention weights.",
            "12": "Other comments: How does this method compare with previous works in terms of training/inference latency?",
            "13": "Summary Of The Review: The paper would be better with more experiments on larger models and ablation studies.",
            "14": "Also, the presentation and the rationale behind the idea are not clear to me."
        },
        "review#3": {
            "0": " Strengths: The framework of decomposability and tensor decomposition allows this paper to encompass and explain the benefit of multiple previous work using a single viewpoint The results of compression are strong.",
            "1": "Possibility of 50x compression, albeit at 1.5-2% accuracy loss opens a lot of possibilities for on device deployment Weakness: The use of tensor decomposition for compressing neural networks has been explored extensively for CNNs, RNNs and Embeddings.",
            "2": "The use here to compress transformers is a natural extension of the idea Decomposability and low-rank nature of FFN and MHA layers has been discussed previously in the literature.",
            "3": "The authors themselves refer to these prior works Cordonnier et al 2021, further discusses the use of tucker decomposition to express MHA layers, albeit in a slightly different context.",
            "4": "In order to improve the paper, I recommend providing more insights into the workings of the method and the bias that the fixed structure like tucker decomposition can lead to.",
            "5": "I also recommend exploring the systems impact of running training and inference using tucker decomposed layers and why say 50x reduction in parameter count, does not lead to a 50x improvement in RPS (Table 4).",
            "6": "Further, I would encourage the authors to explore and understand why finetuning with KD in Table 6 leads to such large accuracy drop.",
            "7": "GD+TD should lead to better accuracy, but improving accuracy by 40% is an interesting data point.",
            "8": "Questions: 1.",
            "9": "In section 4.2, the authors say \"During the inference phase, the terns that do not involve batch size b or seq length n could be calculated in an offline way...\".",
            "10": "Could you expand on what you were referring to?",
            "11": "2.",
            "12": "Table 4 should have comparisons to sparsified BERT, esp for data points with 2-3x parameter compression.",
            "13": "Both structured sparsity and random sparsity could achieve said compression.",
            "14": "Eg - https://arxiv.org/abs/2109.04838 3.",
            "15": "Tucker decomposition of matrices across layers forces common parameters between the matrices.",
            "16": "A possible way this methodology could go wrong is if these matrices have different scales.",
            "17": "Is the norm value across matrices similar for different layers?",
            "18": "Have you looked at the problem from this point of view?",
            "19": "Summary Of The Review: Overall, I think this paper is an incremental improvement to previous state-of-the-art.",
            "20": "Tucker decomposition has been used extensively in NN to compress RNNs, CNNs and Embeddings.",
            "21": "Thus use of tucker decomposition and its ability to compress BERT MHA and FFN layers is incremental improvement over the previous results, especially given the fact that prior work has also shown that MHA and FFN layers can be decomposed in a low rank structure and talked about the redundancy in the parameters in those layers.",
            "22": "However, the results of the paper are interesting from an engineering point of view."
        },
        "review#4": {
            "0": " Strengths: The paper is well written and the motivating analyses (e.g.",
            "1": "Fig 1) are interesting.",
            "2": "I also appreciated the thorough appendix.",
            "3": "The main technical contributions of the paper (i.e.",
            "4": "using all the matrices to perform matrix decomposition, using a bank of matrices) is novel to my knowledge.",
            "5": "However, I have some reservations about whether this is sound (see weaknesses).",
            "6": "Weaknesses: The paper introduces many variants of decomposing the matrices (Table 2).",
            "7": "However the results only seem to be based on one of the methods (i.e.",
            "8": "IV).",
            "9": "I realize that some of variants have already been studied in the literature (e.g.",
            "10": "Mao et al., Noach and Goldberg), but since the setup is not identical, it is crucial that the proposed approach is compared against both II and III.",
            "11": "Therefore, it is not clear whether the improvement in performance is coming from the actual proposed method, or something else.",
            "12": "Since the matrices across layers often have different scaling, and since tensor decomposition is approximating some reconstruction error (i.e.",
            "13": "L2 in the case of SVD), it's not clear that performing decomposition with all the matrices makes sense.",
            "14": "The method requires a pipelined approach where one must first perform generalized distillation against the BERT model before doing task-specific distillation.",
            "15": "Questions: In GD, did you try distilling also the masked LM logits, in addition to (or instead of) the last layer hidden states and attention maps?",
            "16": "Have you checked the norms of the matrices?",
            "17": "Are they of similar scales?",
            "18": "If not, do you obtain better performance by normalizing the matrices such that they are in the same scale?",
            "19": "Summary Of The Review: A new approach to model compression with strong-ish empirical results.",
            "20": "Some reservations about the soundness of the method.",
            "21": "And it is furthermore not clear that the improvements are coming from the proposed approach as opposed to something else, since an ablation study across the different decomposition methods is missing."
        }
    },
    "Recitation-Augmented Language Models": {
        "llm_review": {
            "0": ": Strengths include the innovative approach to leveraging internal knowledge for generating answers, the comprehensive evaluation across different models and datasets, and the clear demonstration of RECITE's effectiveness in improving CBQA task performance.",
            "1": "Weaknesses might include the potential for enhanced biases due to the reliance on internal model knowledge and the lack of consideration for time-sensitive information updating.",
            "2": "Clarity, Quality, Novelty, and Reproducibility: The paper is clearly written, with a well-structured methodology and detailed experimental setup that facilitates reproducibility.",
            "3": "The concept of recitation-augmented generation is novel, addressing the limitations of both direct generation and retrieval-augmented methods in LLMs.",
            "4": "The extensive experiments and comparisons underline the quality and novelty of the research.",
            "5": "Summary of the Review: RECITE represents a significant advancement in leveraging LLMs for knowledge-intensive tasks, offering a promising direction for enhancing factual accuracy without external data reliance.",
            "6": "The approach is innovative, well-executed, and backed by compelling experimental evidence.",
            "7": "Despite potential limitations, the paper's contributions to the field are substantial."
        },
        "review#1": {
            "0": ": Strengths The proposed method is intuitive and well motivated.",
            "1": "Evaluation is sensible and yield positive results both wrt performance and data/resource efficiency.",
            "2": "Writing is mostly clear.",
            "3": "Weaknesses The analysis section on what works and why is a bit thin; Table 5 overall is quite confusing.",
            "4": "I believe answering the following questions may help add more clarity and strength to this part.",
            "5": "What does Hits mean?",
            "6": "What's the difference between @1 and @20?",
            "7": "What is Hits@1 (no explanation in text)?",
            "8": "What is (EM) following Hits@1?",
            "9": "I'm guessing that's the subset of examples that the model answers correctly?",
            "10": "If I'm guessing correctly, the definition of Not Recit, hits@20-recit., and hits@20-path follows a binary decision tree, on whether the ground truth answer appears in any recitation or any QA output.",
            "11": "It is rather difficult to figure this out based on the text in 4.3.4.",
            "12": "Maybe a logical tree of some sort can help improve readability here.",
            "13": "Are there cases where, e.g., the ground-truth answer is not recited but the model still answers the question correctly?",
            "14": "E.g., a more detailed characterization of Hits@1 would be interesting, unless reciting the gt answer is an empirical necessity – which by itself is interesting to point out if true.",
            "15": "How are the 1024 examples selected?",
            "16": "On a more minor note, the names for categories are very not self-explanatory and could be improved for better readability.",
            "17": "There are some missing details that might slightly hurt readability Judging from Fig 2-(1), <Recitation N> is generated from – and directly related to – <Question N>, with other Q-R pairs as in-context examples.",
            "18": "Moving on to Fig 2-(2), it seems there are multiple QA pairs (Q0, A0, ...) inserted between <Recitation N> and <Question N>.",
            "19": "Are the questions in these QA pairs the same as those in the QR pairs in the top-left block of Fig 2?",
            "20": "Are they somehow related to <Question N>?",
            "21": "If not, is there going to be some long-distance memory issues between <Recitation N> and <Question N>?",
            "22": "In Fig 2-(3), are the multiple paths (containing Recitations-dots-Answers) repeating Fig 2-(2)?",
            "23": "In the multi-hop setting, do the numbered prompts like \"Recitation 1\" and \"Recitation 2\" naturally give rise to generating recitations of different topics, or does one have to train/update the LM to solicit such behaviour?",
            "24": "Since the multiple recited passages are generated in one-pass from the LLM decoding sequentially, ... How to use multiple numbered prompts to generate multiple recitations in one-pass sequentially?",
            "25": "I'm personally curious about more details on the generated recitations, e.g., in NQ dataset, do the recitations resemble the \"ground truth\" passages in number of words/sentences/paragraphs?",
            "26": "What percentage of recitations contain the correct answers?",
            "27": "Minor points on motivations One motivation mentioned for recitation is to match the form of causal LM pre-training objective (with the decimal of π example).",
            "28": "Personally, I tend to believe that recitation helps by making certain knowledge more explicit (value of π), and when primed on this explicit piece of information (in the form of recitation), the LM is then more likely to arrive at the correct answer (the 10th decimal of π).",
            "29": "In other words, I'd argue that with or without recitation, the objective remains the same (i.e., masked or next-word prediction), so this motivation to me feels a bit mis-directed.",
            "30": "Human's ability to recite relevant factoid knowledge before answering knowledge-intensive questions.",
            "31": "Do humans have this ability?",
            "32": "Regardless of the answer, having this ability doesn't necessarily help humans answer knowledge-intensive questions more accurately, which I guess should be the true human-inspired motivation for the proposal.",
            "33": "In any case, citations are perhaps needed to make either claim.",
            "34": "Clarity, Quality, Novelty And Reproducibility: Both the core idea and writing are both clear and comprehensible.",
            "35": "Missing details may well be compensated by open sourcing the code for reproducibility.",
            "36": "The idea itself is well-motivated and novel.",
            "37": "Summary Of The Review: The paper is of overall good quality, with a sound idea and good execution by putting many moving parts together into a working system with empirical gains.",
            "38": "The analysis part is a bit unclear and underwhelming."
        },
        "review#2": {
            "0": ": (Strengths) (1) Intuitive ideas (2) No need to retrieve documents.",
            "1": "(3) Compatible with other approaches like chain-of-thought prompting.",
            "2": "(Weaknesses) (1) Major concerns that will be listed below.",
            "3": "(2) Unclear cost for running the proposed methods against the standard-prompting models.",
            "4": "(3) Insufficient rationale to support gains and no gains Clarity, Quality, Novelty And Reproducibility: Mostly clear and novel.",
            "5": "Summary Of The Review: (1) As the passage-hint model is a corpus-specific treatment that additionally utilizes title hierarchies, the prompt-based model must have great added benefits.",
            "6": "The core concern is that the model performs in-context learning based on the model-generated recitations.",
            "7": "No matter how you tune the temperature parameter, there must be a pathological trade-off between sacrificing diversity and sacrificing factuality.",
            "8": "The current draft does not address impacts of less-accurate or incorrect recitations.",
            "9": "(2) It will be great to have the performance graph (given two metrics) over increasing number of few-shot prompts.",
            "10": "If you could approximately measure the correctness of generated recitations, another figure that demonstrate the performance dependency with respect to the number of correct shots will benefit readers.",
            "11": "(3) Another major concern is the computational trade-off from using many shots.",
            "12": "Table 1 shows that the proposed recite-and-answer model (with 5 shots) does not have significant impacts against standard-prompting (with more shots).",
            "13": "Can you address the claim that using the standard-models with more shots is more expensive than running your prompt-based model with less-shots?",
            "14": "(4) It is less clear that how you decide the number of shots you would use for standard-prompting model and your recite-and-answer model.",
            "15": "For example, UL2-20B is tested with 16 shots, whereas the in-house version used 64 shots.",
            "16": "Justify the rationale of picking the number of shots and the reason not to test on more shots for OPT-30B.",
            "17": "(5) Excluding UL2-20B (mostly encoder-decoder T5 structure except the mixture of denoiser variations with multiple special tokens), is your In-house model of decoder-only?",
            "18": "If so, why In-house has significant performance gain on HotpotQA but no visible boost on TriviaQA, which is the entirely opposite to OPT-30B?",
            "19": "Any specific example-based explanation than a conjecture would be beneficial.",
            "20": "(6) The passage-hint model consists of significantly more steps.",
            "21": "It will be useful to see overall time-cost trade-off between your model and standard models.",
            "22": "(7) Is the passage-hint model flexible if the evidence documents do not have a clear and fine-grained title/subtitle structures?",
            "23": "What if some of title components from this hierarchy is missing?",
            "24": "Should we expect performance degradation or performance boost because the passage hints are also generated being possibly incorrectly if the information structure is too granular.",
            "25": "(8) Overall, not much qualitative examples are provided.",
            "26": "Having some successful and failure cases by examples will be highly"
        },
        "review#3": {
            "0": ": Strengths This is an interesting idea, and an exciting way to incorporate the ideas of \"chain-of-thought\" prompting and \"self-consistency\" for open-domain QA tasks where the answer is a factoid.",
            "1": "The authors observe 2-6% improvements over standard direct prompting across all 3 tasks / 3 models.",
            "2": "This is quite good and surprising for me --- I had thought the model would be able to answer the factoid question directly if it's able to generate a much longer paragraph containing the answer.",
            "3": "It is interesting that the method requires multiple recitation paths to work (in Figure 4 performance is lower than standard prompting with just one recitation path).",
            "4": "However, I think of this as a strength of the proposed method, since you cannot really use multiple paths if you are generating a direct answer (since the answer is so short, sampling doesn't make sense).",
            "5": "The authors perform several insightful analysis experiments discussing robustness to prompts, comparison to BM25 retrieval, and an error analysis.",
            "6": "Weaknessess The paper would be much stronger with experiments on GPT3, Instruct-GPT3 (davinci-002), and larger language models (larger in-house LMs?).",
            "7": "It's not really clear from the paper whether recitation helps with larger scale, which I think is important for the generalizability of the method [1].",
            "8": "This could work both ways --- I'm suspecting larger LMs will be better at both recitation and directly performing QA.",
            "9": "I think experiments on InstructGPT [4], T0 [3] or FLAN [2, 7] will be especially interesting, since it's been fine-tuned on instructions / examples / human preferences.",
            "10": "A major advantage of retrieval augmented systems is their applicability on (1) tail distribution information; (2) generalization to information which was not present in the model's training set (like COVID for BERT).",
            "11": "I think these are important limitations of the proposed method, and (1) is not really discussed (2 is just mentioned in the conclusion).",
            "12": "Are most of the correct recitations cases which were seen often during training?",
            "13": "Overall, the performance of closed-book models in this paper seems to significantly lag behind recent few-shot retrieval-augmented systems [5, 6].",
            "14": "For instance, ATLAS [5] gets 42% on NQ with 64 examples and a smaller model, while the best number in this paper is 32% (5-10x larger model).",
            "15": "While I agree that setting up retrieval is technically cumbersome, there are very good retrieval APIs available, which were utilized in [6] without any extra LM fine-tuning.",
            "16": "Note that I do think it's incredible that closed book LMs are doing so well, but practically (from a performance stand-point) it may be better to just retrieve some text from the web rather than ask an LM to generate it with few-shot examples.",
            "17": "Also, retrieval augmented LMs often have lesser parameters [5], so it's unclear which is a better method from an efficiency perspective.",
            "18": "I have mixed thoughts about the passage hints fine-tuning experiments, since it requires fine-tuning a large LM on Wikipedia data.",
            "19": "Perhaps the performance gains are because of the dedicated fine-tuning on Wikipedia data for the recitation LM model (which makes it overfit to Wikipedia)?",
            "20": "Did you remove the passages from the test set questions while doing this fine-tuning?",
            "21": "Also I don't think enough experiments are done in the paper to justify its added complexity over vanilla LM-Recitation.",
            "22": "I would suggest moving it to the appendix, or performing experiments on all 3 datasets / models to show its benefit.",
            "23": "[1] - https://twitter.com/_jasonwei/status/1526589104758042624 [2] - https://arxiv.org/abs/2109.01652 [3] - https://arxiv.org/abs/2110.08207 [4] - https://arxiv.org/abs/2203.02155 [5] - https://arxiv.org/abs/2208.03299 [6] - https://arxiv.org/abs/2203.05115 [7] - https://arxiv.org/abs/2210.11416 Minor This paper is relevant to https://arxiv.org/abs/2004.05483 and https://arxiv.org/pdf/2110.08387.pdf, it would be great to cite them.",
            "24": "In Table 4 (LM-Recitation_5), why is the number for different from Table 1 (Recite and answer)?",
            "25": "(16.34 in Table 1 vs 14.16 in Table 4) Clarity, Quality, Novelty And Reproducibility: Clarity - Very clear Quality - Very thorough experiments overall except for the experiments on passage hints.",
            "26": "I would have liked other models being tested (weakness #1), but the experiments on the current set of tasks look good to me.",
            "27": "Novelty - Good novelty.",
            "28": "The idea has similarity to chain-of-thought prompting, self-consistency prompting, and self-talk, but overall I think the idea is pretty new (especially in the context of large LMs and QA).",
            "29": "Reproducibility - Should be fully reproducible except the experiments on the in-house LM.",
            "30": "Summary Of The Review: The paper has interesting ideas and surprising results, but I have two main concerns - (1) the paper does not evaluate the method on larger LMs which are available; (2) I don't think there's justification that this method is a replacement for retrieval in any way (weakness #2, #3).",
            "31": "I am currently leaning reject, but will be happy to move to the accept range if weakness #1 is addressed via experiments on GPT3-170B and InstructGPT3-170B.",
            "32": "After rebuttal: Thanks to the authors for the very detailed response!",
            "33": "I've decided to raise my score to 6 (accept range) due to the improvements shown on Codex.",
            "34": "I would still suggest the authors to take a more balanced take in their conclusion, mentioning that while there are improvements over direct generation, there is still a gap behind retrieval-augmentation on NQ."
        },
        "review#4": {
            "0": ": Strength Retrieval free QA model are an important topic to study, and RECITE enable midsize LMs to achieve competitive performance in the close-book QA.",
            "1": "Weaknesses As also mentioned is Limitation, the main drawback is the the updated of the model with new knowledge.",
            "2": "This is even more evident when compared to simple DB retrieval in Table 5, where BM25 performs better that LM-Recitation 5-shot with 20-path (the best proposed model).",
            "3": "Questions/Suggestion: What is the size of the index + docs (compressed) of BM25?",
            "4": "is the size larger or smaller of the model parameter?",
            "5": "if the size of the DB is larger of the model parameter, why not constrain it (e.g., randomly removing articles) and check if the performance in Table 5 changes?",
            "6": "What is the inference speed of LM-Recitation K-shot with t-path?",
            "7": "How's compared to BM25?",
            "8": "Does scale matter?",
            "9": "what's the performance of larger LM (PaLM-500B or GPT-3/Instruct-GPT3) or smaller (GPT-J 6B)?",
            "10": "Figure 4 shows the performance difference by sampling more self-consistency paths, but what append if the model samples more responses for the direct method (or chain-of-thoughts)?",
            "11": "Clarity, Quality, Novelty And Reproducibility: Clarity: Very clear Quality: Very high quality Novelty: somehow novel Reproducibility: 2 out of three LM are available for the research community.",
            "12": "Summary Of The Review: The paper propose a two step approach to improve knowledge-intensive NLP tasks by using only LMs (no external retriever).",
            "13": "However the model performance are a bit lagging compare to simple BM25 + LMs, plus the clear weakness of updated knowledge."
        }
    },
    "P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts": {
        "llm_review": {
            "0": ": Strengths: - The methodology presents a significant innovation in handling variability in natural language prompts for querying LLMs, offering a practical solution to improve the robustness of factual information extraction without additional annotation costs.",
            "1": "- Experimental results show clear benefits in terms of precision and consistency, with P-Adapters outperforming baseline approaches by a notable margin across different settings, including out-of-distribution scenarios.",
            "2": "- The paper is well-organized, articulating the problem, proposed solution, and results in a coherent and comprehensible manner, contributing positively to its clarity and overall quality of research.",
            "3": "Weaknesses: - While the paper shows the effectiveness of P-Adapters in improving precision and consistency, there is limited discussion on the potential scalability and adaptability of the proposed method to LLMs beyond BERT and RoBERTa or to languages other than English.",
            "4": "- The empirical evaluation primarily focuses on the quantitative benefits of P-Adapters, with less emphasis on the qualitative analysis of why certain prompts work better and how P-Adapters manage to improve upon them.",
            "5": "Clarity, Quality, Novelty, and Reproducibility: The paper is clearly written, detailing the methodology, experimental setup, and results with sufficient clarity to enable reproducibility.",
            "6": "The approach is novel, addressing the inconsistency problem in querying LLMs with natural language prompts in a unique and effective way.",
            "7": "The authors provide a comprehensive comparison with existing methods, demonstrating the quality and novelty of their contributions.",
            "8": "Additionally, they mention that the code and data used for experiments will be made available, supporting the paper's reproducibility.",
            "9": "Summary of the Review: The paper introduces a promising approach to enhance the consistency and precision of factual information extraction from LLMs using P-Adapters.",
            "10": "By innovatively transforming natural language prompts into continuous prompts, the authors address a significant challenge in NLP, making a notable contribution to the field.",
            "11": "Despite some limitations in the scope of evaluation and qualitative analysis, the paper stands out for its clear presentation, novelty, and potential impact on the usage of LLMs as knowledge bases."
        },
        "review#1": {
            "0": " Strengths of the paper: The problem of extracting factual and consistent information from large language models is of high interest to the NLP community.",
            "1": "Given how LLMs dominate NLP at the moment, making sure these models are robust and consistent is a timely problem, The paper is overall well written, with only a couple of confusing parts (see below), The proposed architecture for intervening between the input embeddings and the first hidden layer of the language model is quite comprehensive.",
            "2": "I enjoyed seeing the different options, and in particular, thought the use of the MoE for relation classification to be quite insightful, The experimental analysis of the work is well executed, and demonstrated convincingly which interventions were most useful in make predictions more accurate and consistent, I liked the analysis in Figure 6, showing the importance of the subject entity on the precision of the fact extraction task, Weaknesses of the paper: The main weakness in this work is one that relates to the overall goal of fact extraction from language models.",
            "3": "The “Oracle'' results from Table 1 are thought provoking: with perfect knowledge regarding the predicate/relation of test examples, and a subsequent 100% consistent response, the LLM is only able to obtain ~50% correct responses from T-Rex, which is an admittedly limited evaluation (41 “head” predicates, mostly of well known entities).",
            "4": "While I understand that this work is clearly focused on the consistency issue, not necessarily correctness, it puts into question whether fact extraction from LMs is a worthwhile pursuit.",
            "5": "I would have liked for the paper to dig a little deeper into this headroom question from the previous point.",
            "6": "Would it be possible to conduct a sampled qualitative evaluation of errors of the Oracle model in the ID cases?",
            "7": "Are the errors due to unseen triples during training time (e.g., not in Wikipedia), or maybe there are issues with model capacity (maybe a 10x version of the LM would be able to recall the prompted fact)?",
            "8": "In terms of writing, the most confusing section in the paper is Section 4.1.",
            "9": "After re-reading it twice, I was still not able to ascertain: (1) what data was used to train the models, and (2) what data was used to evaluate the models.",
            "10": "The section makes reference to LAMA’s T-REX, LPAQA, ParaRel, as well as augmentations using BERT lexical replacements, as well as data from “Shin et al, 2020”.",
            "11": "The section also talks about examples from these sources as well as templates (presumably filled in with WikiData triples?).",
            "12": "I really think this section needs to be rewritten and the training, eval and test datasets should be much more precisely described.",
            "13": "I would also encourage authors to release the exact datasets and splits to allow others to reproduce/improve on this work.",
            "14": "But even with a data release, a precise description of how this data was constructed is very important.",
            "15": "For the MoE and Oracle layers, the description in the paper is insufficient to determine the outputs presented to the first layer of the model.",
            "16": "The depiction in Figure 2 hints that the entire sequence is rewritten using the fixed-length learned embeddings, and perhaps the subject or MASK embeddings are preserved?",
            "17": "But actually sub-section 4.2 never formally describes how the embeddings are used to create the continuous prompts?",
            "18": "Are they prepended/appended to the original inputs?",
            "19": "Or do they rewrite the original inputs?",
            "20": "Do either the MASK or subject tokens get copied?",
            "21": "The LAMA benchmarks have one unfortunate characteristic: since it was constructed for BERT-style single token prediction, it has stripped down the original datasets (see the original version of T-Rex, which contains over 600 unique predicates vs. the 41 from LAMA: https://hadyelsahar.github.io/t-rex/ and https://aclanthology.org/L18-1544.pdf ).",
            "22": "I wonder if a more comprehensive version of this would be to evaluate on a larger sequence-to-sequence model like BART https://arxiv.org/abs/1910.13461 or T5 https://arxiv.org/abs/1910.10683 (both available as HuggingFace models).",
            "23": "Given that this work leverages frozen LLMs, it seems that training and evaluation could be done relatively cheaply even for larger models with proper decoders.",
            "24": "Other comments: With respect to the MoE solution, the paper claims that the model does not use a weighted combination and opts to use the top-1 predicted relation.",
            "25": "I wonder if authors have tried using a weighted combination instead?",
            "26": "If the relation classifier is trained with cross-entropy softmax loss, most of the weights will be close to one-hot (similar to top-1) except when the model is uncertain.",
            "27": "Therefore combining prompt embeddings may yield some benefit over top-1.",
            "28": "Does this make sense?",
            "29": "Note sure this is a good idea, but: given that the LLM is frozen, it seems plausible that the continuous prompt embeddings learned in some of the models resemble existing embeddings from the original vocabulary.",
            "30": "As such, would it make sense to attempt to “decode” the continuous prompt embeddings into the existing vocabulary?",
            "31": "One could use a greedy decoding strategy of extracting the nearest neighbor (via dot product or cosine distance) from each continuous prompt embedding to the vocabulary input embedding table.",
            "32": "Have the authors tried inspecting the continuous prompts in this way?",
            "33": "I wonder if the output is informative or whether these prompts are modeling purely latent variables.",
            "34": "Typo in Figure 1 “Canada si” -> “Canada is”, Typo in page 6: “Cannonical” -> “canonical” Summary Of The Review: The problem of extracting factual and consistent information from large language models is of high interest to the NLP community, and this work in particular should be of interest to the ICLR community.",
            "35": "Overall, this work was well-written throughout (easy to follow in most places except for a few rough parts detailed above).",
            "36": "The experimentation work was also of high quality, with interesting results.",
            "37": "To highlight a few findings: (1) the use of a relation-classification MoE and its consistently high performance on consistency metric seems promising, (2) the analysis demonstrating the importance of the “subject” is correct fact prediction, and (3) analysis demonstrating the negatives effects of uniformizing objects in train/test sets, which is strong indication that LLMs still do not generalize well to unseen objects."
        },
        "review#2": {
            "0": " Authors also presented a Mixture of Experts models that learn a set of prompts and select one to query the LLM.",
            "1": "Experimenta results show that P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations.",
            "2": "The model and training procedure are well described and results are promising.",
            "3": "It is also great that the data used is in the Github repositories.",
            "4": "This said, the reader could benefit from better error analysis, as it is not clear how much hallucination and grounding the model is producing.",
            "5": "Summary Of The Review: The obtained results and model itself will benefit the reader and researchers working in this important research area."
        },
        "review#3": {
            "0": " Strengths: The problem setting--mapping natural language prompts to more optimal continuous prompts--is interesting.",
            "1": "Most prior work on prompting attempts to find optimal prompt templates for each task (or in this setting, KB relation type), with the assumption that the task label is known at test time.",
            "2": "Methods like P-Adapters for learning a transformation of natural language prompts could be an interesting alternative to standard prompting (with a fixed template per task) and fine-tuning, with possible applications to meta-learning/transfer learning.",
            "3": "The authors compare methods with different assumptions about the availability of annotations, and under different kinds of distribution shifts.",
            "4": "The paper is clearly written and the proposed methods are sensible and easy to understand.",
            "5": "Major comments: The paper does not draw a connection to prior work on robust optimization (for example, see references in [1]), which offers a more principled framework for formulating the objective of invariance to a class of input perturbations.",
            "6": "At the very least, it would be good to give a formal definition of robustness/consistency and cite this line of prior work.",
            "7": "The paper does not provide a clear justification for why P-Adapters would be expected to improve consistency, and nothing in the training objective encourages the model to be consistent.",
            "8": "The argument might be more convincing if you could compare it to a training objective that does explicitly promote consistency--for example, by encouraging the hidden representations of different prompt paraphrases to be similar.",
            "9": "As this paper notes, the factual probing benchmark has class imbalances and represents a very particular use case, so while P-Adapters appear to improve consistency here it’s unclear if these results will generalize to other settings.",
            "10": "Would it be possible to evaluate P-Adapters on a wider variety of tasks, following prior work on prompt optimization [2, 3]?",
            "11": "For example, [4] provide 100 prompt templates for a wider variety of NLP tasks.",
            "12": "The paper would be more compelling if you could show that P-Adapters improve consistency in other settings too.",
            "13": "Much of the motivation of this paper is based on assertions about user preferences, but there is no human evaluation to validate these claims.",
            "14": "For example, would a typical user prefer to give a natural language input but get an inconsistent response?",
            "15": "Or would they rather pick a pre-defined relation type with better promise of consistency?",
            "16": "In particular, it’s not clear why user preferences would place any restrictions on training.",
            "17": "MoE model: How accurate is the relation classifier?",
            "18": "There is a big performance drop in the “OOD Prompts” setting, which leads me to wonder if the relation classifier was adequately trained.",
            "19": "Either way, relation classification results should be included and discussed in the main paper--a perfect relation classifier would lead to 100% consistency.",
            "20": "All of the models suffer a performance drop on the “OOD Objects” setting, which indicates that the models have over-fit to the (imbalanced) distribution of entities in Wikidata.",
            "21": "It’s unclear how to interpret consistency in this setting, because the models are consistently producing the incorrect response.",
            "22": "It might be more informative to report something like the “Consistent-Acc.” measurement from [5].",
            "23": "Table 2 does not compare results to any prior work.",
            "24": "For example, can these results be compared to the “Consistency Improved PLM” results from [5]?",
            "25": "Minor comments: The metric called “precision@1” should be called “accuracy@1” Table 2 does not explain how the results are aggregated (prior work takes either the micro- or macro-average over relation types).",
            "26": "The description of the methods and the illustration (Figure 2) are somewhat unclear.",
            "27": "Is there one MLP applied at each position?",
            "28": "Three MLPs?",
            "29": "The meaning of the different-colored tokens could also be explained in the caption.",
            "30": "The paper is missing some implementation details, such as the size of the BiLSTM.",
            "31": "The results section (section 5) might be easier to read if it were divided into subsections or paragraphs.",
            "32": "It would be interesting to see the accuracy breakdown by relation type.",
            "33": "[1] Sinha, Aman, Hongseok Namkoong, and John Duchi.",
            "34": "Certifying Some Distributional Robustness with Principled Adversarial Training.",
            "35": "International Conference on Learning Representations.",
            "36": "2018.",
            "37": "[2] Shin, Taylor, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh.",
            "38": "\"Eliciting Knowledge from Language Models Using Automatically Generated Prompts.\"",
            "39": "EMNLP.2020.",
            "40": "[3] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang.",
            "41": "GPT understands, too.",
            "42": "arXiv preprint arXiv:2103.10385, 2021b.",
            "43": "[4] Gao, Tianyu, Adam Fisch, and Danqi Chen.",
            "44": "\"Making pre-trained language models better few-shot learners.\"",
            "45": "ACL.2021.",
            "46": "[5] Elazar, Yanai, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg.",
            "47": "\"Measuring and improving consistency in pretrained language models.\"",
            "48": "arXiv preprint arXiv:2102.01017.",
            "49": "2021.",
            "50": "Summary Of The Review: The paper proposes methods for mapping natural language prompts to continuous prompts, with the goal of improving accuracy and consistency on a factual probing benchmark.",
            "51": "The problem is interesting and the method appears to work on this benchmark, but I there are three main changes I would like to see before recommending this paper for acceptance, possibly at a future conference: Drawing a connection to prior work on robust optimization, and providing a clearer formal justification for why this method will improve consistency.",
            "52": "Providing more detailed empirical results and discussion (in particular relation classification accuracy).",
            "53": "The current results are hard to interpret because models can be consistent but inaccurate, and because models can perform well by over-fitting the entity distribution.",
            "54": "Ideally applying the method to a wider range of prompting tasks, to show whether the results will extend beyond the particular setting of factual probing."
        },
        "review#4": {
            "0": " Pros: This paper tackles one of the most important issues in the large language model: inconsistency results obtained from different prompts which have the same information needs.",
            "1": "The problem itself is real and must be resolved.",
            "2": "The proposed adaptor, P-adaptor is a simple but effective solution to alleviate the inconsistency.",
            "3": "This paper is well-written and easy to follow.",
            "4": "Furthermore, this paper provides comprehensive experiments including several qualitative analyses and discussions to show the effectiveness proposed method.",
            "5": "Concerns: Although the proposed method involves experiments in four different settings including OOD keyword error, it might be valuable to investigate an OOD syntax error setting.",
            "6": "That is, in a real scenario, users write the natural language prompt which may have a grammatical error.",
            "7": "Assessing the robustness of the proposed method in terms of grammatical errors can enhance the quality of the paper.",
            "8": "In Table 2, the performances of P-tuning in RoBERTa-large are better than the ones of Oracle.",
            "9": "Without any explanation, it is not convincing.",
            "10": "Instead of listing the result in the tables, it would be clearer if the authors provide more explanations about them although they are in the appendix.",
            "11": "Summary Of The Review: The proposed method is reasonable and analyses of experiments are well described."
        },
        "review#5": {
            "0": " +ves: Overall the paper is clear Use of LLM for extracting factual information is well motivated The motivation behind the use of natural language prompts to improve the user experience is generally clear.",
            "1": "However, it can benefit from further citation of the literature in that domain.",
            "2": "Having a few citations of works that compare the user experience with and without the additional information will make your argument in the paper stronger.",
            "3": "Empirical results suggest that P-Adapters improve the consistency -ves: research questions are dispersed all over the paper.",
            "4": "What I mean by this: Abstract: you say that you tackle the inconsistency problem and you want to use a lightweight approach.",
            "5": "Hence, I can see that there are two research questions: 1) inconsistency of LLM and 2) efficiency - how to tackle the inconsistency problem with a light weight model.",
            "6": "Introduction: you say that focus is the continuous prompts and user-friendly prompts.",
            "7": "These are another two research questions.",
            "8": "Discussion: you hypothesising that the success of P-Adapter is due to not updating the LLM parameters.",
            "9": "Such that it is likely to maintain all of factual knowledge learned during pretraining.",
            "10": "Which to me is yet another research question - addressing the inconsistency of LLM with frozen weights vs fine-tuning.",
            "11": "In Introduction you talk about the frozen weights of LLM but do not motivate why you decide to keep them frozen until the Discussion section.",
            "12": "Can you please state and motivate clearly your (full) research question somewhere in Introduction?",
            "13": "Lack of experiments: (Minor) Why didn’t you train BERT fully with the proposed natural language prompts.",
            "14": "You emphasise the importance of this experiment yourself in the Discussion section.",
            "15": "(Minor) Why didn’t you train (fine-tune) BERT with P-Adapters?",
            "16": "This experiment will also allow you to answer your hypothesis in the Discussion section.",
            "17": "(Major) Did you try to freeze the parameters of LLM (or use an adapter) in the MoE model?",
            "18": "You claim that MoE performs better than P-Adapters because of the additional supervision information (in form of relations) the model gets during the training.",
            "19": "With the present experiments it is hard to tell.",
            "20": "Maybe MoE perform better because you also tune parameters of LLM?",
            "21": "Summary Of The Review: I cannot recommend the acceptance of the paper in its present state.",
            "22": "There are two main reasons for this.",
            "23": "The first reason is the lack of explicitly stated (unified) research question.",
            "24": "Without it, it is hard to tell if the authors have conducted enough experiments to support their research question.",
            "25": "I did my best to identify these research questions in the paper and then try to link them to the experiment they conducted.",
            "26": "However, I had a hard time doing this.",
            "27": "Sometimes motivation given by the authors can be ambiguous.",
            "28": "For example, do you use adapters just for efficiency or you are interested in exploring how one can address the inconsistency problem with the adaptors?",
            "29": "The second reason is the lack of experiments with the frozen LLM parameters of MoE.",
            "30": "Depending on the outcome of the experiment the conclusions that are made in the paper may change.",
            "31": "For example: “While MoE models usually perform better, the small increase comes at the cost of requiring relation annotations at training time and specifying the subject of the entity at inference time.” However, if those two issues are addressed, in my opinion, the paper has a good chance to be accepted.",
            "32": "Reduction of supervision data that is needed to tackle the inconsistency problem has both academic interest and also can be of the great interest to practitioners.",
            "33": "Moreover, adapters are presently has high research interest in the NLP community - demonstrating how they can be used to address the inconsistency problem will also be valuable to the community."
        }
    },
    "Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models": {
        "llm_review": {
            "0": ": Strengths: The paper provides a fresh perspective on evaluating language models by focusing on the model's flatness rather than just pre-training loss.",
            "1": "It combines theoretical insights with empirical evidence, offering a comprehensive analysis of the impact of training algorithms' implicit biases on model performance.",
            "2": "The findings have practical implications for designing and training language models more efficiently, emphasizing the quality of the training process over simplistic loss metrics.",
            "3": "Weaknesses: The experiments are conducted on simplified datasets, which may limit the generalizability of the findings to more complex, real-world datasets.",
            "4": "While the paper introduces the concept of flatness, it does not provide concrete guidelines on how to achieve or measure flatness in practical settings.",
            "5": "Clarity, Quality, Novelty, and Reproducibility: Clarity: The paper is well-written, with clear explanations of the theoretical background, methodology, and significance of the findings.",
            "6": "Quality: The research is of high quality, combining theoretical proofs with rigorous empirical testing to validate the hypotheses.",
            "7": "Novelty: The focus on the flatness of models and the exploration of training algorithms' implicit biases present novel insights into language model evaluation and training.",
            "8": "Reproducibility: The authors make an effort to ensure reproducibility by detailing the implementation of algorithms and dataset generation, along with providing code in supplementary materials.",
            "9": "Summary of the Review: This paper makes a significant contribution to the field of language modeling by challenging the conventional reliance on pre-training loss as a metric for evaluating model performance.",
            "10": "By highlighting the importance of model flatness and the implicit biases of training algorithms, it opens new avenues for research and development in language model training methodologies.",
            "11": "While the study's reliance on simplified datasets may raise questions about the applicability of its findings to more complex tasks, its combination of theoretical and empirical analyses offers valuable insights.",
            "12": "The clarity, novelty, and quality of the research are commendable, making it a valuable addition to the literature."
        },
        "review#1": {
            "0": ": Strength: The paper is well-written and easy to follow.",
            "1": "The paper is supported by many experiments to verify the claims that the authors proposed.",
            "2": "Besides empirical results, the author also provided theoretical points of view to support their claim.",
            "3": "Weaknesses: The claim that flatness can decide the downstream performance is not well-supported.",
            "4": "There are many factors that can affect the downstream performance, for example, the model size.",
            "5": "One possible explanation is that the flatness could be a consequence of scaling up the model size, and the good performance is brought by the large model size, too.",
            "6": "If that is the case, then flatness is not the reason for good downstream performance.",
            "7": "The authors need to have more evidence to prove that flatness is the main reason that leads to good performance, otherwise, they may be both the consequence of another factor (like model size).",
            "8": "When computing Hessian, I assume that you are using the loss of pretraining datasets, not downstream datasets.",
            "9": "Then how can the flatness on the pretraining task reflect the situation on downstream tasks?",
            "10": "This claim is not very intuitive.",
            "11": "It may need more explanations.",
            "12": "Clarity, Quality, Novelty And Reproducibility: Clarity&Quality: The paper is clear, well-written and easy to follow.",
            "13": "Novelty: As far as I know, there is no existing paper discussing the same topic.",
            "14": "Reproducibility: The authors didn't provide the code.",
            "15": "I think it's hard to reproduce the results by ourselves.",
            "16": "Summary Of The Review: The paper is good in terms of its qualities, but there are some problems that need to be answered to further prove the correctness of their claims.",
            "17": "I would tend to accept this paper if they can give good answers to my questions.",
            "18": "For now, I will give a borderline score."
        },
        "review#2": {
            "0": ": This work seeks to better understand an important question in representation learning: how does pretraining performance correlate with downstream performance?",
            "1": "This question is adequately answered in controlled settings where we observe the non-correlation between pretraining loss and downstream performance and the correlation between solution flatness and downstream performance.",
            "2": "The investigation is both well-motivated and nicely executed.",
            "3": "In addition, a theoretical result is provided in support of the empirical observations.",
            "4": "It is, however, unclear what the practical implications of this work are.",
            "5": "First of all, current large language models are not in the saturation regime, and it is hard to estimate when they will be as datasets grow with model size in tandem.",
            "6": "Second, while this paper points that pretraining loss is not a reliable indicator of downstream performance, a simple remedy is to evaluate on downstream tasks during pretraining and compare models accordingly, which is likely already done in practice.",
            "7": "Finally, this paper does not demonstrate if the insight gleaned in this work can lead to additional “flatness regularization” that induces better downstream performance on real datasets.",
            "8": "It is understandable that large-scale experiments are expensive and are not expected, but given the rather empirical motivation of the paper, some validation on real data seems desirable.",
            "9": "Clarity, Quality, Novelty And Reproducibility: The paper is well-written and presents novel results.",
            "10": "Summary Of The Review: Interesting empirical observations and theoretical result.",
            "11": "However, more empirical results on real data, especially on how the discovered insight can enable better downstream performance, would strength this work given the rather empirical motivation."
        },
        "review#3": {
            "0": ": ----Strength---- the submission found a correlated trend between the decay of the trace of the Hessian matrix evaluated on the training data and the increase of performance on the downstream tasks.",
            "1": "given the above observation, it becomes straightforward to infer that a pre-trained model with parameters at a flat minimum would lead to decent performance on the downstream tasks.",
            "2": "----Weaknesses---- the trace of a Hessian matrix in the formulation from this paper is very easy to obtain and one doesn't even need to materialise the matrix.",
            "3": "The issue is that the Hessian matrix is evaluated around the global minimum of a model being trained on the pre-training data, and, without any knowledge of the pre-training data, it becomes very difficult to compute the Hessian matrix.",
            "4": "The other issue is that it does require many samples to obtain a reasonable estimate of the trace.",
            "5": "the trace of the Hessian matrix is only an overview of the magnitude of all eigenvalues.",
            "6": "A smaller trace could mean that the minimum might be flatter than other minima with the same loss, but it could also mean that the minimum might be steep in only a few directions, and completely non-informative in others.",
            "7": "The latter case wouldn't necessarily give us a strong transferable model, and yet the trace of the Hessian matrix wouldn't be able to tell.",
            "8": "maybe I am missing the point and I am happy to be corrected, but the arguments made in this submission or the observations obtained are already well-known empirically, especially in the case of zero-shot and few-shot learning with very large pre-trained transformer models.",
            "9": "In addition, the theorems mentioned in the submission don't seem necessary, or, in any case, support the arguments or the observations.",
            "10": "Clarity, Quality, Novelty And Reproducibility: clear presentation, but not very original Summary Of The Review: I don't recommend accepting the submission since the empirical results are well-known and the theorems don't provide theoretical justifications to the observations."
        },
        "review#4": {
            "0": ": Strength The paper explores the interesting and practical research question --- correlation between pre-training loss and generalization performance of downstream tasks.",
            "1": "In the controlled experiments, the authors empirically show the correlation between flatness and generalization.",
            "2": "Even though language model achieves optimal pre-training loss, we can improve generalization performance of downstream task as we continue pre-training, which results in flatter minima.",
            "3": "Authors provide some theoretical analysis that connects flatter solution to the pre-training loss and generalization performance of downstream tasks in the limited settings.",
            "4": "Please address my following concerns.",
            "5": "If they are properly addressed, I am willing to raise my score.",
            "6": "Weakness My expertise not learning theory, but in my humble opinion, what the authors claims is too bold.",
            "7": "They claim that they \"prove that SGD with standard mini-batch noise implicitly prefers flatter minim in language models\", but it is limited to the special case --- Dyck language experiment which is far from practical scenario.",
            "8": "It would be better to tone down the claim.",
            "9": "I am a bit confused about the correlation between flatness and generalization.",
            "10": "For example, Dinh et al., [1] show that we can build equivalent models but with sharper minima, which might contradict to what the authors claim, specifically theorem 5.1.",
            "11": "However, the authors did not properly tackle this issue even in the related work section.",
            "12": "It is not clear how the size of models and flat minima is related to each other.",
            "13": "The authors claim that smaller transformer architecture is a subset of the larger transformer architecture family, but it does not make sense.",
            "14": "If we increase the width of transformer or increase the vocabulary size, the smaller model cannot be a special case of larger model.",
            "15": "It would be better to explain why larger model converges to flatter minima than smaller one.",
            "16": "In the experiments, it is unclear why the authors compare vanilla sgd training with adversarial training.",
            "17": "I think adversarial weight perturbation [2] is more relevant baseline since it explicitly regularize the flatness of weight loss landscape.",
            "18": "Minor typo: In page 4, (2) training for different number of steps -> (1) training for different number of steps Questions Regarding the flat minima, what happens if we explicitly enforce the language model to converge flatter minima?",
            "19": "For example, we can explicitly regularize the trace of Hessian to be small or use SAM [3] optimizer to enforce such regularization.",
            "20": "Why is the cross-entropy loss is typically non-zero at the global minimizers?",
            "21": "Why do you assume that we only mask a single token in a sentence for masked language model?",
            "22": "In practice, we usually mask 15% of tokens of a sentence (e.g.",
            "23": "BERT [4], RoBERTA [5]).",
            "24": "References [1] Dinh, Laurent, et al.",
            "25": "\"Sharp minima can generalize for deep nets.\"",
            "26": "International Conference on Machine Learning.",
            "27": "PMLR, 2017.",
            "28": "[2] Wu, Dongxian, Shu-Tao Xia, and Yisen Wang.",
            "29": "\"Adversarial weight perturbation helps robust generalization.\"",
            "30": "Advances in Neural Information Processing Systems 33 (2020): 2958-2969.",
            "31": "[3] Foret, Pierre, et al.",
            "32": "\"Sharpness-aware Minimization for Efficiently Improving Generalization.\"",
            "33": "International Conference on Learning Representations.",
            "34": "2021.",
            "35": "[4] Devlin, Jacob, et al.",
            "36": "\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\"",
            "37": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).",
            "38": "2019.",
            "39": "[5] Liu, Yinhan, et al.",
            "40": "\"Roberta: A robustly optimized bert pretraining approach.\"",
            "41": "arXiv preprint arXiv:1907.11692 (2019).",
            "42": "Clarity, Quality, Novelty And Reproducibility: Although this paper tackles interesting research question, it is unclear that flatter minima actually leads to better generalizations since it does not compare relevant baselines and lack of justification.",
            "43": "Please see detain in strength and weakness section.",
            "44": "Summary Of The Review: As previously mentioned, the authors do not properly tackle the open question --- flat minima and generalization which might still seem to be controversial topic.",
            "45": "Moreover, they need to tone down their claim since their theoretical analysis is limited to very specific controlled experiment which is far from realistic scenario.",
            "46": "Thus, I am inclined to reject."
        }
    },
    "Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models": {
        "llm_review": {
            "0": ": Strengths: Novelty and Impact: KiC introduces a unique approach to integrating external knowledge into language models, addressing the limitations of fully-parametric models in adaptability and scalability.",
            "1": "This could significantly impact areas requiring rapid adaptation to new knowledge.",
            "2": "Technical Quality: The model design is robust, leveraging a diverse set of knowledge sources and an innovative instance-adaptive knowledge selector.",
            "3": "The experiments are comprehensive, covering a wide range of tasks and demonstrating significant improvements.",
            "4": "Clarity and Reproducibility: The paper is well-organized, with clear explanations of the methodology and experimental setup.",
            "5": "The inclusion of implementation details and a commitment to releasing code further enhance its reproducibility.",
            "6": "Weaknesses: Limitations in Knowledge Coverage: While KiC covers a broad range of knowledge types, the scalability of adding new knowledge types or updating existing ones remains unclear.",
            "7": "Potential for Bias: The reliance on external knowledge sources, especially those automatically extracted, may introduce biases or inaccuracies that could affect the model's outputs.",
            "8": "Clarity, Quality, Novelty, and Reproducibility: Clarity: The paper is well-written, providing clear explanations of the model architecture, knowledge integration mechanism, and experimental results.",
            "9": "Quality: The research is of high quality, presenting a thorough analysis of the model's performance across a diverse set of tasks.",
            "10": "Novelty: KiC's approach to integrating external knowledge into a semi-parametric model is novel and represents a significant contribution to the field.",
            "11": "Reproducibility: The detailed description of the experimental setup and the promise to release code make this work highly reproducible.",
            "12": "Summary of the Review: The Knowledge-in-Context (KiC) model represents a significant advance in semi-parametric language modeling, effectively leveraging external knowledge to enhance performance across a broad range of tasks.",
            "13": "Its innovative approach to knowledge integration and the ability to adaptively select relevant knowledge for each input instance are particularly noteworthy.",
            "14": "While there are limitations related to the scalability of knowledge coverage and potential biases, the overall impact, technical quality, and novelty of the work are commendable.",
            "15": "The clarity of presentation and commitment to reproducibility further strengthen this submission."
        },
        "review#1": {
            "0": ": Strength: Thorough evaluation on a wide range of tasks.",
            "1": "Providing a broad picture of the effectiveness of the supplied context.",
            "2": "Weakness: 1, The paper is weak on ablations.",
            "3": "The author should include more baseline results, in particular the result where the same (initial) model is trained with the same hyperparameters with no non-trivial knowledge sources available.",
            "4": "In their language this baseline should have the generalist only.",
            "5": "They also did not analyze e.g.",
            "6": "how retrieval noise/quality affect the results.",
            "7": "Ideally the author should look deeper into how are the retrieval results helping the model to make predictions that are otherwise difficult.",
            "8": "2, A unified retrieval system that works across different knowledge sources and end-to-end trained with the language model would be more elegant and removes the expert dispatcher and make the method more freely generalizable to expanded knowledge sources without retraining.",
            "9": "But this reviewer acknowledge the value of attempting to use out-of-the-box retrieval systems.",
            "10": "3, The paper claims to have experimental evidence that instance-adaptive selection is superior to task-adaptive selection of the context type.",
            "11": "But this claim did not seem to be backed up by numbers.",
            "12": "4, The author should comment in the paper on the degradation on tasks such as OpenBookQA and PIQA when going from the no-context finetuned baseline of table 1 to their flagship model in table 4.",
            "13": "Some analysis on the win/loss patterns would be valuable.",
            "14": "Clarity, Quality, Novelty And Reproducibility: The work is original enough and provides good information on training general purpose language models that can utilize external knowledge sources.",
            "15": "The writing quality leaves a lot to be desired: 1, There is no listing of the 39 finetuning tasks.",
            "16": "Making it difficult to evaluate the generalization ability of the resulting model.",
            "17": "2, The author should include a reference on the \"P3 task categorization framework\" and the identity of the \"P3 tasks\".",
            "18": "3, SwithTransformer -> SwitchTransformer, Summary Of The Review: The paper attempts at a valuable direction of context augmented language models and provided thorough eval for their framework.",
            "19": "The authors demonstrated incremental value of retrieved context from various datasources.",
            "20": "It is light on ablations and analysis and rough on the edges in terms of presenting all necessary information and baselines.",
            "21": "But overall it is a valuable paper.",
            "22": "Improvements on the weaknesses of the paper could lead to better review ratings."
        },
        "review#2": {
            "0": ": Strengths The paper proposes a novel semi-parametric method to use the external memory of heterogeneous types of structured knowledge, which demonstrates its effectiveness across many tasks in various setups with a relatively small number of parameters.",
            "1": "Weaknesses The paper lacks ablation studies to investigate the effectiveness of each component of the proposed method.",
            "2": "Several important details of the method are missing, making it less clear and the experimental results difficult to reproduce.",
            "3": "Suggested Ablation Studies & Analyses In order to see how important is the knowledge-selecting mechanism, it would be interesting to see the performance of KiC without the knowledge selector where all knowledge memories are merged into one and then retrieved (however, it might be difficult if the retrieval mechanism is different for entity and script knowledge types, which is not clearly written in the paper).",
            "4": "It would be interesting to compare the performance with the cases where unstructured knowledge source(s) is used to augment the input, in order to show the effect of using heterogeneous structured knowledge resources.",
            "5": "It would be interesting to see the ratio of the knowledge source types selected by the knowledge selector to solve each task.",
            "6": "It would demonstrate the effectiveness of using the balancing loss and might provide the readers with further insights.",
            "7": "Clarity, Quality, Novelty And Reproducibility: Several important details of the method are missing, making it less clear and the experimental results difficult to reproduce.",
            "8": "It is not described in the paper what kind of model architecture the knowledge selector uses.",
            "9": "For script data, it is not explained how the authors “retrieve the most relevant scenario” given the query (Is MPNet used here as well to encode the input query and each utterance in the script?).",
            "10": "It is not described how a word is selected from the dictionary given the query, and how the authors preprocess the Wiktionary to create the dictionary knowledge that would serve as the input, especially when the word contains more than one meaning.",
            "11": "It is not clear how many knowledge instances are retrieved from the knowledge memory and attached to the query (It seems like one).",
            "12": "The actual storage footprint of each external knowledge memory is not reported in the paper.",
            "13": "[Minor point] It is not clearly written in Section 2 whether the authors further train MPNet or use it off-the-shelf (It seems to be the latter case).",
            "14": "[Minor point] The motivation behind why the authors explored the use of only structured knowledge resources and did not include unstructured knowledge sources as a type of knowledge memory such as Wikipedia, remains unclear.",
            "15": "[Minor point] The formula for the balancing loss refers to the paper of SwitchTransformer, and is missing in this paper.",
            "16": "Summary Of The Review: The paper proposes a novel semi-parametric method to use the external memory of heterogeneous types of structured knowledge, which demonstrates its effectiveness across many tasks in various setups with a relatively small number of parameters.",
            "17": "However, many important details of the method and ablation studies are missing in the paper, making the need to update the manuscript essential to make the explanations clear and add several ablation studies and analyses which could provide more insights to the community."
        },
        "review#3": {
            "0": ": The strength of the paper is that it utilizes a novel semi-parametric language model architecture that retrieves from multiple knowledge sources dynamically via an MoE layer.",
            "1": "This simple yet effective approach boosts zero-shot task generalization results significantly.",
            "2": "Dividing the knowledge sources into 6 different resources helps in different aspects of solving NLP tasks, which can be decided at a instance level.",
            "3": "The weakness is that it does not show any computational comparison compared to prior multitask prompted finetuning approaches.",
            "4": "Initial thought is that the proposed method might require much more computation (fine-tuning stage) since the MoE layer has to be trained to be able to dynamically select which knowledge source to route to AND also train the underlying LM.",
            "5": "Clarity, Quality, Novelty And Reproducibility: The paper is very easy to read will clear explantations in the technical parts such as the explanations regarding the MoE layer.",
            "6": "I think the proposed method is very novel in the sense that they utilized a MoE layer to retrieve from multiple external sources and that they utilized this to tackle the important task of generalizing to unseen tasks.",
            "7": "Summary Of The Review: This paper suggests a novel semiparametric architecture, retrieving from multiple fine-grained to coarse-grained knowledge sources to solve unseen tasks and achieving significant performance enhancement compared to previous approaches while having a much smaller number of parameters.",
            "8": "Thus, I highly recommend this paper be accepted at this conference."
        },
        "review#4": {
            "0": ": Strength: The paper is well-written and clear about the contributions.",
            "1": "The performance of KiC shows significant improvement compared to baselines.",
            "2": "The experiments are extensive, containing zero-shot task transfer (MMLU and datasets evaluated in T0), and in-domain evaluation.",
            "3": "Weakness: This paper lacks detailed ablation and analysis of the proposed method.",
            "4": "The performance of retrieving from a mixture of 6 categories (treating a mixture of 6 categories as a single large corpus) is needed to show the effectiveness of MoE architecture for knowledge selection.",
            "5": "A naive mixture of 6 categories would not need any knowledge selection step.",
            "6": "Please also report the standard deviation across different evaluation prompts for each task in Table 2.",
            "7": "Prompt sensitivity is also important in addition to median accuracy.",
            "8": "For each task, what is the occurrence of selection of each knowledge category for Table 2?",
            "9": "This might answer the question of \"why does KiC works?",
            "10": "\", if there are some patterns of the target task and selected expert.",
            "11": "Is there any ablation result showing the necessity of generalist (no external memory)?",
            "12": "How does the performance vary if the generalist is absent?",
            "13": "Questions: Are there any results of KiC on a 3B (or larger) scale?",
            "14": "Is there any quantitative result of showing the benefit of using structured knowledge resources than plain text such as Wikipedia or C4 dataset?",
            "15": "If the effect is similar, using plain text would be simpler because it does not require any knowledge selector.",
            "16": "Clarity, Quality, Novelty And Reproducibility: This paper is clear about the contribution and the proposed method.",
            "17": "Although there are many lines of work related to semi-parametric zero-shot task adaptation, the method proposed in this paper is novel enough.",
            "18": "Summary Of The Review: This paper proposes KiC, an effective semi-parametric LM that outperforms much larger parametric LMs on various downstream tasks.",
            "19": "The paper is well-written and the proposed method is novel enough.",
            "20": "However, this paper lacks detailed analysis and ablation.",
            "21": "Ablation studies on how each component of the proposed method benefits the target downstream task are needed, especially."
        }
    },
    "Recursion of Thought: Divide and Conquer Reasoning with Language Models": {
        "llm_review": {
            "0": ": Strengths: RoT addresses a critical limitation in LMs regarding context size and demonstrates a significant improvement in solving complex reasoning tasks.",
            "1": "The framework is model-agnostic, allowing its application across various models, including Transformers and LSTMs.",
            "2": "The paper provides extensive experiments and comparisons, showing that RoT can enable even small models to perform complex reasoning at a level that may be challenging even for humans.",
            "3": "Weaknesses: The approach requires supervised learning for each task, potentially limiting its applicability to problems where such detailed supervision is feasible.",
            "4": "The paper does not explore the generalizability of models trained with RoT to tasks or problem sizes not seen during training.",
            "5": "Despite its potential, the approach's dependence on recursive problem-solving may not be suitable for all types of reasoning tasks.",
            "6": "Clarity, Quality, Novelty, and Reproducibility: The paper is well-structured, presenting a clear methodology and comprehensive experimental results.",
            "7": "The quality of research is high, supported by rigorous experiments and a broad application of the proposed framework.",
            "8": "The novelty of the approach lies in addressing the context size limitation by leveraging recursive problem-solving, a significant step forward in the field.",
            "9": "The inclusion of source code for reproducing experiments enhances the paper's reproducibility.",
            "10": "Summary of the Review: This paper presents a promising approach to overcoming the context size limitations of current language models.",
            "11": "The Recursion of Thought framework not only demonstrates the potential for dramatically improved performance in complex reasoning tasks but also opens new avenues for research in natural language processing and artificial intelligence.",
            "12": "While there are limitations, such as the need for task-specific supervision, the strengths and contributions of this work significantly outweigh its weaknesses."
        },
        "review#1": {
            "0": ": My main concern with this approach is that the paper does not discuss how to apply this approach to more general problems.",
            "1": "The problems considered here are very simple, and an algorithm to solve them needs to be known in order to generate the training data.",
            "2": "So, in its current form this does not enable any new abilities as we could use the base algorithm instead of using expensive LMs.",
            "3": "In terms of the insight from a learning point-of-view, I also do not see anything surprising in this work.",
            "4": "After decomposing the problem manually (i.e.",
            "5": "by writing an algorithm to generate training data), the learning task is pretty simple (for addition, for example, all the model has to do is to extract the last digit from two numbers).",
            "6": "So the high accuracy of the resulting method is not that surprising.",
            "7": "Clarity, Quality, Novelty And Reproducibility: The writing style is a bit \"flashy\", which gives room for miscommunications.",
            "8": "For example, the paper opens with the following statement: Although neural networks have achieved amazing results on various domains, e.g., images, texts, audios, videos, games, etc., nearly all of them are classified as System 1 tasks (Kahneman, 2013), ... Afaik, this statement is not supported by the literature, and in particular not by the given reference.",
            "9": "Following Kahneman's theory, tasks like playing the board game go would certainly require system 2 thinking.",
            "10": "IMO, dropping the first paragraph would help the paper.",
            "11": "The paper also makes claims such as \"the length of CoT can grow rapidly with the problem’s complexity\", without further explanation and I am not even sure what exactly this means.",
            "12": "Summary Of The Review: This paper demonstrates that decomposing simple algorithmic problems can circumvent the limit in context length language models.",
            "13": "However, the technique only seems to work for problems for which we already have algorithms, and it is unclear to me if this could be extended to more general problems.",
            "14": "The writing style should be improved."
        },
        "review#2": {
            "0": ": On the positive side, the paper is well written and easy to read.",
            "1": "On the negative side, the solution proposed is formulated as a fully supervised problem, meaning for each task the exact steps needed to solve the problem needs to be annotated.",
            "2": "This is not a scalable approach as for realistic real-world applications obtaining detailed annotations for reasoning problems is an expert task which is expensive to scale to large datasets.",
            "3": "As a result, outside of tasks such as arithmetic operations where the dataset can be created automatically (and one could argue that you wouldn't use a task specific LM as the paper doesn't deal with multiple tasks or out-of-distribution problems), the applicability of this approach is minimal.",
            "4": "Clarity, Quality, Novelty And Reproducibility: The paper is written clearly.",
            "5": "The novelty is minimal as the solution proposed is rather trivial as it doesn't try to solve the hard problem of reasoning without direct supervision, or other settings such as few-shot/zero-shot/multi-task and formulates the problem as a fully supervised task.",
            "6": "Due to simplicity of approach, I think it's reproducible.",
            "7": "Summary Of The Review: Unfortunately, I don't see much impact from this work in practical terms.",
            "8": "The formulation doesn't scale to general reasoning problems, the tasks considered aren't tasks that you would train a specific LM to solve, and the algorithmic solution is very simple/trivial extension.",
            "9": "Hence, I'm not able to recommend this paper for publication."
        },
        "review#3": {
            "0": ": Strength: [+] The proposed framework (RoT) is novel and well-motivated.",
            "1": "[+] The work has conducted multiple experiments which demonstrate the great power of RoT on much more complex arithmetic and algorithmic reasoning problems.",
            "2": "Weakness: [-] The training of RoT requires (non-trivial) human inputs to design proper subproblems.",
            "3": "More specifically, I wonder what is the limitation of the problems that RoT could solve: for example, Does RoT have the ability to learn backtrack?",
            "4": "Some problems may require people to modify the earlier part of the answer based on the new observed information.",
            "5": "Does RoT have the ability to learn a problem where its subproblems have different structures (as recursion usually requires the same structure in the subproblems)?",
            "6": "What is the performance of RoT on (maybe small-scale) NP-hard algorithmic problems that may not have the divide-and-conquer structure such as TSP？ how to design proper subproblems in order to use RoT to train the models?",
            "7": "What are the criteria of the subproblems for RoT?",
            "8": "What kinds of structures do they need to have？ Are there systematic approaches to design subproblems for reasoning tasks in general?",
            "9": "Clarity, Quality, Novelty And Reproducibility: This work has a good and clear presentation of its idea, its specific methodology, and its experiment settings.",
            "10": "The quality and originality of the work should meet or exceed the conference standard.",
            "11": "My main concern is on how to properly train the RoT framework as it requires people to feed it with hand-designed subproblems.",
            "12": "Therefore, it would be great if the authors could explain the limitation of the problems that RoT could solve and how to design proper subproblems in order to use RoT to train the models.",
            "13": "Summary Of The Review: This work proposed a novel model-agnostic framework, Recursion of Thought (RoT), to release the capacity constraint by the maximum size of a single context in language models.",
            "14": "RoT teaches a language model to divide and conquer complex problems by recursively creating multiple contexts; therefore, a complex problem could be solved by utilizing multiple contexts.",
            "15": "It then conducts extensive experiments on arithmetic and algorithmic reasoning tasks to show the power of RoT in helping language models solve problems that require hundreds of thousands of tokens.",
            "16": "The proposed framework (RoT) is novel and well-motivated, and this work has a good and clear presentation of its idea, its specific methodology, and its experiment settings.",
            "17": "The quality and originality of the work should meet or exceed the conference standard.",
            "18": "My main concern is on how to properly train the RoT framework as the training of RoT requires (non-trivial) human inputs to design proper subproblems.",
            "19": "Still, I think this work has great potential to allow language models to achieve better reasoning abilities."
        }
    },
    "Thrust: Adaptively Propels Large Language Models with External Knowledge": {
        "llm_review": {
            "0": ": Strengths: Innovation and Relevance: The methodology addresses a crucial limitation of PTLMs, which is the opaque and static nature of their encoded knowledge.",
            "1": "By introducing a selective approach to external knowledge utilization, it paves the way for more efficient and effective NLP solutions.",
            "2": "Empirical Evidence: Extensive experimentation validates the approach's effectiveness, showcasing significant improvements in performance across diverse tasks.",
            "3": "Reproducibility: The promise to release code and data after acceptance encourages reproducibility and further research.",
            "4": "Weaknesses: Lack of Comparison: The paper does not thoroughly compare its approach with existing methods in the domain of external knowledge utilization, which could have strengthened the argument for its novelty and effectiveness.",
            "5": "Potential for Bias: The reliance on the novel metric Thrust for determining the necessity of external knowledge could introduce bias if the metric itself is not universally applicable across different types of tasks and domains.",
            "6": "Clarity, Quality, Novelty, and Reproducibility: The paper is well-written, presenting a novel approach to a significant problem in NLP.",
            "7": "The research is of high quality, supported by rigorous experimentation.",
            "8": "However, the novelty is somewhat diminished by the lack of comparison with existing methods.",
            "9": "The commitment to releasing code and data post-acceptance is commendable and will aid in reproducibility.",
            "10": "Summary of the Review: This paper introduces a novel and impactful approach to optimizing the use of external knowledge in large language models, addressing a critical issue of efficiency and effectiveness.",
            "11": "The proposed method, supported by extensive empirical evidence, marks a significant advancement in the field.",
            "12": "However, a more comprehensive comparison with existing methods could have further highlighted its novelty and superiority.",
            "13": "The commitment to open-source principles enhances the paper's value to the research community."
        },
        "review#1": {
            "0": ": Overall the idea of this work is novel, but the description is very hard to follow.",
            "1": "The lack of formal definitions, make it hard to understand how thrust is computed e.g., \"casting a set of instances ( the training data ) into the representation space \" The writing is mostly sloppy, e.g., c_0 is used before it is defined The experiment result also need more explanations.",
            "2": "For example in Table 3 it is good to see that using knowledge 25% of the time is as good as using it 75% of the time for many tasks.",
            "3": "However, one might wonder why only the result of UnifiedQA is shown?",
            "4": "why not also compare to the case using knowledge 100% of the time?",
            "5": "Clarity, Quality, Novelty And Reproducibility: This paper is very unclear.",
            "6": "Summary Of The Review: This paper is very unclear."
        },
        "review#2": {
            "0": ": Strengths The core hypothesis of the work makes sense to me, especially for larger models.",
            "1": "Proposed method is quite simple and works without further fine-tuning or training and offers efficiency gains.",
            "2": "Authors show that using thrust is better at selecting instances which require knowledge than just random selection.",
            "3": "Weaknesses / Questions Evaluation with recent models like RETRO [1] and Atlas [2] is missing.",
            "4": "Table 2, if my understanding is correct, with knowledge case is presented after the vertical bar (|).",
            "5": "If that's true, please fix the description of the model which states the opposite.",
            "6": "Table 2, is the with knowledge same as using full knowledge?",
            "7": "Table 4, when comparing thrust with full knowledge, what are the exact performance numbers?",
            "8": "Please mention the evaluation times for both cases as well.",
            "9": "Citations: [1] Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Driessche, G.V., Lespiau, J., Damoc, B., Clark, A., Casas, D.D., Guy, A., Menick, J., Ring, R., Hennigan, T.W., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J.W., Elsen, E., & Sifre, L. (2022).",
            "10": "Improving language models by retrieving from trillions of tokens.",
            "11": "ICML.",
            "12": "[2] Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Yu, J.A., Joulin, A., Riedel, S., & Grave, E. (2022).",
            "13": "Few-shot Learning with Retrieval Augmented Language Models.",
            "14": "ArXiv, abs/2208.03299.",
            "15": "Clarity, Quality, Novelty And Reproducibility: Overall, the work is presented well and the motivation is well-founded.",
            "16": "Summary Of The Review: Authors have proposed a new metric to effectively select cases which require external knowledge.",
            "17": "Authors have done a decent job of evaluating the method with current literature, however a few important evaluations are missing.",
            "18": "Since this paper revolves around retrieval based LMs, authors should have done more evaluation on using thrust in conjunction with some of the well known retrieval based LMs.",
            "19": "Overall, I think in it's current form this work requires more refinement in terms of more robust evaluation and I would vote for rejecting this paper."
        },
        "review#3": {
            "0": ": Strengths: The topic and general ideas are interesting Weaknesses: A main weakness is that the work is not reproducible, with respect to both the methods and datasets.",
            "1": "Clarity: some important terms are not defined at all or until later in the paper, including the key term of external knowledge (which commonly means something else actually, as in knowledge graphs).",
            "2": "The experiments miss some important baselines.",
            "3": "There are some technical inaccuracies, e.g., the definition of the F1 measure is incorrect.",
            "4": "Clarity, Quality, Novelty And Reproducibility: A main weakness is that the work is not reproducible.",
            "5": "It is not clear if the external knowledge used is published or not, and if and how it is possible to obtain it.",
            "6": "The methods (and experiments) are described informally, or obscurely, where the specific choices made are not motivated or compared to alternatives.",
            "7": "The related work section comes at a late phase of the reading, and finally makes it clear that the modeling of explicit knowledge as discussed throughout the paper is not at all a common practice (\"In this work, we pioneer the study by adding the knowledge in the plain text format.\")",
            "8": "This leap is hardly discussed or motivated in comparison to the literature.",
            "9": "While the motivation for identifying `hard cases' is 'limited bandwidth or budget to retrieve external knowledge', efficiency and computation time is not evaluated.",
            "10": "Also, the choice or selecting some fixed portion of the queries (e.g., 25 percent) for expansion is not motivated.",
            "11": "Why not use a threshold over the query difficulty score for example?",
            "12": "A baseline of selecting random instances rather the based on difficulty may be informative.",
            "13": "There are some technical inaccuracies, e.g., the definition of the F1 measure is incorrect.",
            "14": "There are some typos and style issues.",
            "15": "Summary Of The Review: The ideas are interesting, but the writeup is not solid.",
            "16": "The presentation of ideas and experiments lack comparison with existing works.",
            "17": "Some terms are not well defined or misused.",
            "18": "Reproducibility is a main issue."
        },
        "review#4": {
            "0": ": Strengths The proposed method to calculate the Thrust score seems novel.",
            "1": "It is interesting to use the Thrust score to see if the task requires external knowledge or not.",
            "2": "Weaknesses The motivation and suggested method are not well-aligned.",
            "3": "The authors claim that they simulate real-world cases, but the experimental setup is highly artificial, making the practicality of the algorithm dubious.",
            "4": "The experiments are insufficient to show the effectiveness of Thrust.",
            "5": "Explanations on Weaknesses The authors claim that they try to simulate the real-world case where they have limited bandwidth to retrieve external knowledge from the perspective of cost-efficiency, but the claim is not persuasive because the way they choose the portion of the queries to use external knowledge is through relative comparison between queries after scoring all other queries, and not through absolute scoring on each independent query, e.g., by applying a score threshold.",
            "6": "In other words, the suggested method seems impractical because each query would come to the system independently in the real world, not as a batch as in the experiments.",
            "7": "Therefore, if cost-effectiveness is a concern, the authors should have proposed a method that works on each query independently to determine whether that query itself requires external knowledge or not.",
            "8": "Comparing the results in Table 2 and 3, on many of the tasks, using Thrust to partially utilize the external knowledge drops the performance compared to when all queries are answered with external knowledge (e.g., 70.2 → 56.8 (75% of queries) on e-SNLI UnifiedQA-base, 80 → 73.4% (75% of queries) for TriviaQA UnifiedQA-3b).",
            "9": "Therefore, the cost-effectiveness of Thrust, if it exists, comes with a sacrifice of accuracy for many cases, creating a trade-off.",
            "10": "Meanwhile, it is also unclear whether the method is actually cost-effective because the calculation of the Thrust score would also take up some inference time.",
            "11": "The authors should compare the average inference speed between when all queries use the external knowledge as the input, and when part of the queries use the external knowledge while the Thrust score is calculated for all queries.",
            "12": "The authors only compare Thrust with random baseline, but in order to show the effectiveness of Thrust as a scoring metric, they should compare the algorithm with baselines simpler than Thrust but better than random, e.g., using BM25 scores.",
            "13": "[Minor point] While the number of external knowledge instances to utilize and the method of utilizing knowledge might vary, the authors seem to test the effectiveness of Thrust with only one combination of them.",
            "14": "The authors did not clearly indicate how many external knowledge instances are used for a query, but it seems like one by inferring from the context.",
            "15": "Also, they test only the case of using external knowledge as part of the input and did not explore the cross-attention-based method (e.g., FiD).",
            "16": "Clarity, Quality, Novelty And Reproducibility: The misalignment between the motivation and the experiments makes the paper’s point unclear.",
            "17": "Some experimental setup details are missing, making the results difficult to be reproduced for now.",
            "18": "For example, the authors did not indicate the number of clusters they used in the experiments, and how many knowledge instances they used to augment a query (which seems to be one).",
            "19": "In the caption of Table 2, “Performances with/without knowledge external knowledge are presented before/after the vertical bar, respectively,” it should be without/with based on the description of the results in the paper.",
            "20": "Such an important typo makes the paper difficult to understand.",
            "21": "Summary Of The Review: The authors’ motivation for this work is interesting, but their proposed method and experimental setup do not align well with their motivation.",
            "22": "While they claim to simulate real-world cases, the experimental setup is artificial and highly restricted, making it far from real-world cases.",
            "23": "The experiments are insufficient to show the effectiveness of the proposed method, making the usefulness of the algorithm questionable."
        }
    },
    "Cramming: Training a language model on a single GPU in one day": {
        "llm_review": {
            "0": ": Strengths: Innovative Approach: The paper creatively addresses the challenge of resource-efficient language model training, providing a valuable perspective opposite to the prevailing trend of scaling up.",
            "1": "Comprehensive Analysis: It thoroughly investigates various components of the training pipeline and their impact on model performance, offering deep insights into efficient training strategies.",
            "2": "Empirical Evidence: The study is well-supported by empirical results, showing that models trained under these constraints can achieve respectable performance on downstream tasks, close to that of BERT.",
            "3": "Weaknesses: Limited Scope: The research mainly focuses on transformer architectures and the MLM objective, which may overlook potential benefits from other models or training objectives.",
            "4": "Generalization of Findings: While the study provides valuable insights into scaling down, the specific modifications and their impacts might not generalize across different datasets, tasks, or larger computational settings.",
            "5": "Clarity, Quality, Novelty, and Reproducibility: Clarity: The paper is well-structured and clear, with a detailed explanation of the methodology and findings.",
            "6": "However, the complexity of some descriptions might require prior knowledge in the field for full comprehension.",
            "7": "Quality: The research is of high quality, with a rigorous experimental setup and analysis that contribute to the understanding of efficient language model training.",
            "8": "Novelty: The study introduces novel insights into scaling down language models effectively, challenging the prevalent scaling-up paradigm and offering a new direction for research with limited computational resources.",
            "9": "Reproducibility: The paper includes a reproducibility statement and provides sufficient details for others to replicate the experiments, which is crucial for validating and building upon the findings.",
            "10": "Summary of the Review: The paper presents an insightful investigation into training transformer-based language models with limited computational resources, demonstrating that it is possible to achieve performance close to BERT's under such constraints.",
            "11": "The research's strengths lie in its innovative approach, comprehensive analysis, and empirical support, though its scope and generalizability of findings could be expanded.",
            "12": "Overall, the study makes a significant contribution to the field of efficient language model training and opens up new avenues for research with modest computational budgets."
        },
        "review#1": {
            "0": ": Strengths: The motivation for the study proposed in the paper is interesting for a number of reasons.",
            "1": "The volume of computation required by many modern transformer models has been prohibitively expensive and therefore out of reach for most researchers for quite a while.",
            "2": "By studying the implications of constraining the computational resources on the ability of the model to perform well on certain tasks the authors could provide a way for researchers with limited budgets to participate and utilize these models in fundamentally new ways.",
            "3": "The trend in the paper to consider modifications that mainly reduce the gradient update cost without significantly impacting the total number of parameters in the model, based on the scaling laws, provides an interesting and unifying theme throughout.",
            "4": "The persistence of the scaling laws to influence the performance of the model on tasks is reinforced through empirical evidence throughout and yields interesting insights.",
            "5": "Performance evaluation on a shoe-string budget of FLOPs compare to other prominent models is impressive.",
            "6": "Weaknesses: Similar studies were conducted on a single node with 8 GPUs as noted by the authors.",
            "7": "Though that setup had considerably more computational resources the total volume of computation was still a fraction of the amount used by many large research institutions.",
            "8": "In light of that work, the scenario presented in this paper may seem somewhat derivative and only marginally interesting.",
            "9": "It is not clear if or how the observations made in the cramming regime may be used to make more informed decisions regarding the training process in the normal training setting.",
            "10": "Clarity, Quality, Novelty And Reproducibility: The writing is clear and the presentation of issues motivating the current work is adequately articulated in the text.",
            "11": "While I am not an expert in the transformer field I feel the authors did a good job explaining the connection between the scaling laws and the downstream performance of the models under consideration.",
            "12": "The novelty of the work pertains to the training strategies used to reduce computational costs without removing the total number of model parameters.",
            "13": "Although previous works looked at training with limited resources the author's study and extreme training scenario that is likely to be more pertinent and representative of the resources available to typical, non-institutional, researchers.",
            "14": "Summary Of The Review: Overall I find the motivation for the work and claims made by the authors to be an interesting departure from the traditional language training papers that use exorbitant computational resources.",
            "15": "It seems more practical to answer questions about how researchers can do more with less when it comes to allocating resources for training transformer models.",
            "16": "My remarks should be taken with a grain of salt as I am not an expert in this particular area but I would feel more inclined to experiment with transformer models if I felt I could train them to a reasonable level of ability on my modest desktop setup.",
            "17": "I believe this sentiment represents the spirit of the paper and the results should be of interest to other members of the research community that are hesitant to participate in this research area because of the perceived computational overheads."
        },
        "review#2": {
            "0": ": Strengths: This paper adds more insights on scaling-down, which is less understood as most concurrent efforts are around scaling-up.",
            "1": "The experiments have a good coverage in terms of testing various architectural changes from recent literature.",
            "2": "The final performance on downstream tasks (GLUE) are impressive given the limited compute budget.",
            "3": "Weaknesses: The biggest missing piece is an ablation study on the improvement from various architecture changes.",
            "4": "In the current version, the authors provided a comprehensive list of things they tried, what helped and what didn't.",
            "5": "But it's unknown which change(s) brought the bigger improvement.",
            "6": "Related to this is the poor performance on CoLA.",
            "7": "Although the authors provided several hypotheses, some of them should be tested to verify whether they're actually related to any of the architecture or data changes, or mostly due to reduced model size.",
            "8": "For example, one possible cause provided by the authors is that reasonable performance CoLA would need more training data.",
            "9": "But are models in these experiment trained on less data compared to BERT-base?",
            "10": "Is it possible to see whether the performance gap can actually be closed if the models were trained longer than one day (i.e.",
            "11": "seeing more data)?",
            "12": "Clarity, Quality, Novelty And Reproducibility: Clarity: For all the architectural modifications in Sec 4.2, does \"no improvement\" refer to pretraining loss or downstream tasks?",
            "13": "In Sec 4.3 batch size schedule, you found optimal performance from different batch size for pretraining (1536) and downstream tasks (4032).",
            "14": "Why do you think pretraining loss benefit from smaller batch size?",
            "15": "Similarly, could any of the architectural changes in Sec 4.2 have different effects on pretraining vs. downstream?",
            "16": "For all the changes in Sec 4.2 and 4.3, when you test a specific modification, what were used for the rest of the architecture and training setup?",
            "17": "How were they chosen?",
            "18": "Quality: The 128 sequence length differs drastically from common choice in language models pretraining (e.g.",
            "19": "1024, or at least 512).",
            "20": "To make sure conclusions from this work would apply, some additional experiments with longer sequence length would be helpful.",
            "21": "Summary Of The Review: Overall, this paper tackles an important problem, the experiments design are sound and the empirical findings are informative for language models pretraining.",
            "22": "If the authors could add more clarity to generalizability and robustness of the findings, e.g.",
            "23": "experiments with sequence lengths longer than 128, further ablate on the causes of drop in CoLA, etc.",
            "24": "then the results would be more valuable to language models pretraining."
        },
        "review#3": {
            "0": ": Strength: In this paper, several modifications (architecture, training setup and datasets) are explored to check whether there is any improvement.",
            "1": "All of these aspects are important and interesting.",
            "2": "These can give good insights for the community.",
            "3": "Some interesting conclusion are got, for example, training recipe and data setup lead to decent downstream performance on GLUE.",
            "4": "Weaknesses: The investigation of modifications lack convincing experiments.",
            "5": "For example, only one task performance (MNLI) is reported for when studying the impact of training hyper-parameters.",
            "6": "Other tasks can have a different trend.",
            "7": "And when exploring the effect of the architecture, only MLM loss is report.",
            "8": "The performance of downstream tasks can be also important.",
            "9": "The technical novelty of this paper is a little limit.",
            "10": "The total contributions are also limit.",
            "11": "Clarity, Quality, Novelty And Reproducibility: Clarity is clear, however, novelty is limit.",
            "12": "Summary Of The Review: This paper does a lot of Interesting investigations."
        },
        "review#4": {
            "0": ": This work presents a thorough empirical investigation on a topic that is of interest to many researchers who do not have access to large compute clusters.",
            "1": "The overall methodology appears to be sound and the final result promising.",
            "2": "However, I am not sure if ICLR is the best venue for this work.",
            "3": "There are no new theoretical or algorithmic contributions nor any new insight into representation learning.",
            "4": "This does not change my belief that this is an informative paper to many in the community, but it might find a more suitable audience if submitted to venues such as EMNLP.",
            "5": "(Disclaimer: I do not regularly publish in this area.",
            "6": "I am happy to defer to more experienced reviewers and the AC on this point.)",
            "7": "An additional concern is that it is not clear if the MLM loss is the best metric to track when comparing different model/training design choices.",
            "8": "This paper (https://openreview.net/pdf?id=F5uYcwABMu), for example, clearly demonstrates that models with near-identical pretraining loss can perform very differently on downstream tasks due to implicit biases.",
            "9": "This should be taken into consideration since many conclusions in this work are based on the MLM loss alone.",
            "10": "I encourage the authors to also observe downstream performance and note if they agree or disagree with the MLM loss.",
            "11": "The connection between “cramming” and scaling laws can be clarified.",
            "12": "The scaling laws mentioned in this work are empirical observations that a model’s performance strongly correlates its size but not necessarily its shape.",
            "13": "The empirical results from this work show that this holds for the low-compute regime, which is somewhat surprising.",
            "14": "However, these “laws” are merely empirical observations.",
            "15": "It is not clear what the authors mean by “we discuss scaling laws in the low compute regime and find that cramming is hard because the conclusions of Kaplan et al.",
            "16": "(2020) so easy to reproduce” (emphasis mine) in the conclusion.",
            "17": "It would be better to simply state that the empirical observation from Kaplan et al.",
            "18": "holds in the setups investigated, which motivated using architectures that parallelize well.",
            "19": "A related concern is that since the dominating factor for performance is the number of FLOPs we can squeeze out of a GPU within a given timeframe, this makes the conclusions of this work somewhat hardware-specific, e.g., they might not hold on TPUs or newer/older GPUs.",
            "20": "Clarity, Quality, Novelty And Reproducibility: The paper is mostly well-written with just a few typos.",
            "21": "I am not aware of similar investigations in the low-compute regime and believe that many in the community might find this work informative.",
            "22": "Summary Of The Review: Neat empirical investigation with conclusions that might interest many.",
            "23": "However, it is not clear if ICLR is the best venue.",
            "24": "Moreover, one might argue that MLM loss is not the best criterion to study model and training design choices."
        }
    },
    "Mapping Language Models to Grounded Conceptual Spaces": {
        "llm_review": {
            "0": " Strengths: Novel Approach:The paper proposes an innovative method to explore grounding in LMs without the need for direct sensory input, suggesting a pathway to grounding LMs in more abstract, conceptual spaces.",
            "1": "Significant Empirical Evaluation:The thorough evaluation across several LMs and conceptual domains (colors, directions) provides strong evidence of the varying abilities of LMs to understand and generalize grounded concepts.",
            "2": "Insightful Analysis:Error analysis and investigation into the models' performances offer valuable insights into how LMs may leverage linguistic structures to understand grounded concepts.",
            "3": "Weaknesses: Limited Scope of Grounded Concepts:The paper focuses on relatively simple and easily text-representable concepts like colors and directions, which might not fully capture the complexity of grounding in more abstract or nuanced concepts.",
            "4": "Dependency on Textual Representations:The approach relies on the assumption that textual representations can sufficiently capture the essence of grounded concepts, which may not always be the case, especially for more complex sensory experiences.",
            "5": "Clarity, Quality, Novelty, and Reproducibility Clarity:The paper is well-written, with clear explanations of the methodology, experiments, and findings.",
            "6": "Quality and Rigor:The research is of high quality, with rigorous experimental design and thorough analysis of results.",
            "7": "Novelty:The paper contributes novel insights into the possibility of grounding LMs in conceptual spaces using only textual information.",
            "8": "Reproducibility:Detailed descriptions of the experimental setup and access to model details enhance the reproducibility of the results.",
            "9": "Summary of the Review The paper presents an intriguing exploration of grounding language models in conceptual spaces without direct sensory input.",
            "10": "It significantly contributes to our understanding of how large LMs can leverage their linguistic knowledge to understand and generalize grounded concepts.",
            "11": "While the study is limited by its focus on simple, text-representable concepts and assumes the sufficiency of textual representations for grounding, its innovative approach, rigorous evaluation, and insightful analysis make a valuable contribution to the field.",
            "12": "The findings suggest that large text-only models like GPT-3 might be capable of being grounded in a data-efficient manner, opening new avenues for research into grounding LMs."
        },
        "review#1": {
            "0": " I think this paper investigates a very interesting problem.",
            "1": "The experiments are rather thorough, with different levels of controls (e.g., semantic-invariant transformations to the world representations, generalization to unseen worlds or unseen concepts).",
            "2": "The writing is clear and structured as well.",
            "3": "However, I think the concerns that some readers might raise and complain include: (1) Metric.",
            "4": "Is the top-3 accuracy meaningful for the task, especially for the spatial and cardinal problems where the concept space is very small, and for GPT-3 that knows to output in-domain words only?",
            "5": "Is the substring metric suitable for the color problem, especially in the “unseen concept” setup?",
            "6": "For example, if “light blue” is a seen concept and “dark blue” is the test-time unseen concept, then answering the seen concept “light blue” in the unseen concept setup would result in a perfect accuracy.",
            "7": "Would that defeat the purpose of testing generalization to unseen concepts, like in Table 2?",
            "8": "(2) Conclusion drawn from the results.",
            "9": "The authors argue that if the LM successfully generates the correct concept based on the grounded representation (likely “unseen” in the pretraining data), it means that the model knows to ground the concept to the non-text world.",
            "10": "However, is it possible that the model doesn’t understand the relationship between the concepts and the grounded representations, but instead utilizes a similarity between the test grounded representation and the grounded representations in the in-context prompts?",
            "11": "For example, upon seeing the test representation (e.g., [0,1,0,0] in the spatial domain, or RGB (140, 0, 255) in the color domain), the model can use a simple strategy: copying the concept of a bunch of most similar representations in the in-line prompt examples (e.g., [0, 1, 0, 0, 0], or RGB (145, 0, 255)).",
            "12": "This strategy would not involve the concept of “left” or “pink”, and is robust to the rotation transformation (while not robust to the random transformation, if each point in the world was transformed independently).",
            "13": "This would align with the results in Table 1.",
            "14": "To check whether this hypothesis is (partially) true, we can look at experiments like: A: Test on some real unseen concepts.",
            "15": "This was done in the paper, like in spatial and cardinal columns in Table 2, Table 9, 10, 11 (in the appendix).",
            "16": "But the performance is not very strong in these cases even for GPT-3 (top-1 accuracy).",
            "17": "B: Test with fewer prompts.",
            "18": "This is to prevent the model from memorizing similarity with the prompting examples too much.",
            "19": "This was also done in the paper (Figure 6 in the appendix).",
            "20": "Again, the performance is not strong, if the number of prompts goes below 20 or 60.",
            "21": "C: Replace all of the concept names with concepts in an unrelated domain (e.g., substituting all “left” and “right” with “apple” and “orange”).",
            "22": "If the performance is above baseline in this setup, should we conclude that the LM implicitly learns to map fruit concepts to the grounded spatial world?",
            "23": "This was not done in the paper and may be a good control experiment.",
            "24": "(3) Though the paper investigates an interesting problem, the overall takeaway of this work is not very clear to me.",
            "25": "How is the analysis useful for future work?",
            "26": "(4) Some details in the paper should be checked.",
            "27": "For example, in Section 2.1, the authors say that all models are pretrained on the 40GB OPENAI-WT dataset, but this is not true for GPT-3?",
            "28": "Also for the color experiments, it is not clear whether 60 or 6+57 or 70 (as mentioned in B.1.1 in the appendix) prompts were used.",
            "29": "Summary Of The Review: Overall I think this work investigates an interesting problem, but the main argument needs to be justified more carefully (as mentioned in (1), (2) above).",
            "30": "Also, the takeaway and impact of the work are not very clear to me, other than showing the somewhat inscrutable power of GPT-3."
        },
        "review#2": {
            "0": " I think the modelling work presented is useful and interesting — and importantly: explained well and coherently.",
            "1": "However, I find many statements not thought out as much as they deserve.",
            "2": "For one, colours are as relative as left/right.",
            "3": "Something red looks dramatically different under white light than it does under other colours of light, and so on.",
            "4": "Also see visual illusions where the same shade of grey looks like different colours as a function of where it is in the image.",
            "5": "This is why categorical cognition plays a role in colours in humans.",
            "6": "The authors could at minimum note this, if they wish.",
            "7": "Another example is on the equivocation of \"text\" and \"language\" throughout the manuscript.",
            "8": "I also find it a bit strange to say \"north\" in humans is not grounded.",
            "9": "I assume it is always grounded to where the Sun is and what hemisphere we are in and so if somebody were to lie to us about where north is, we would eventually figure it out when we saw the movement of shadows, etc.",
            "10": "There is a lot of research on what humans indeed do here, so some literature review could be done if claims need to be made about actual human behaviour.",
            "11": "Overall, however, I find the modelling here very interesting and compelling.",
            "12": "I like that the authors explored a pre-trained model.",
            "13": "Perhaps providing a coherent definition of \"grounded\" versus \"ungrounded\" might be useful to help formalise the distinction as used by the authors.",
            "14": "Summary Of The Review: The work is good quality and well-explained, but could benefit from slightly more details on the rationale behind some jargon and claims about human cognition."
        },
        "review#3": {
            "0": " Overall I really enjoyed this paper.",
            "1": "The scope was fairly narrow and the experiments were not diverse enough to provide a sense of a thorough understanding of how LM's are forming this space, what possible confounds might be at play, or what other domains might have similar effects.",
            "2": "But apart from these concerns, the paper is well-written and presented in an organized manner, and essentially all the discussion points contributed something to my understanding of the work.",
            "3": "The topic is also very timely and important, and the implications of this work could help unify work in NLP, vision, and other grounded/situated learning.",
            "4": "An obvious main strength of the paper is that empirically the results are clear and consistent across all experiments.",
            "5": "The similarities between induced and gold spaces, between smaller models and larger models, is very significant.",
            "6": "The experiments themselves are quite simple and straightforward, but my biggest worry -- that confounds from the LM training data may be leaking in and influencing these conceptual spaces -- were strongly considered by the authors.",
            "7": "Their solution of using rotations to test against isomorphic spaces makes intuitive sense.",
            "8": "One thing I am considering is whether it's possible that huge parameter language models may be inducing mechanisms for performing spatial rotations on subspaces, though at some point it might become difficult to separate a model's ability to memorize the data and perform on-the-fly rotations, from something that might be called a non-trivial understanding of these concepts/spaces.",
            "9": "That is mostly an aside as I can't present it as anything more than speculation.",
            "10": "One of the few questions about the color experiment results was to what extent the model was backing off to predict less specific color names, since it is something that is not well-captured in top-1 accuracy (though maybe some combination of this and distance evaluation starts to get at it).",
            "11": "And why were 67 other colors inserted in the color prompts?",
            "12": "It seemed arbitrary.",
            "13": "How are grids incorporated into the model?",
            "14": "In Appendix B2, it appears to be just as a string following \"World\", but is it a \"linearized\" string of one row after another?",
            "15": "Typos: Zellers reference \"within the prompt .\"",
            "16": "Summary Of The Review: The paper provides new insights into whether large pre-trained LMs induce conceptual spaces that mirror those found in the real world.",
            "17": "This could be an important finding, that together with many related works, gives us a better understanding of what sorts of higher reasoning abilities such LMs are capable of.",
            "18": "I found the experiments straightforward, but well-done, and some foresight was given to what confounding factors may be creeping into these results.",
            "19": "And across all model sizes we learn that similarities between these spaces increases with model size, and that low model sizes, though still big in comparison to many models, here are truly insufficient to capture concept spaces anywhere comparable to the largest models.",
            "20": "The paper is well-written and references are good, although I may have missed an important reference or two since this is slightly outside of my research area, but barring the existence of similar uncited work, I think it's a paper worth publishing."
        },
        "review#4": {
            "0": " Strengths I think this is a nice set of experiments.",
            "1": "The choices to use both unseen worlds and unseen concepts was good.",
            "2": "The rotated baseline is also a great idea, and frankly I find the results on rotated spaces quite surprising, especially when it comes to color.",
            "3": "I would have expected a serious performance decrease.",
            "4": "It speaks very well to the model's ability to construct an alignment between the spaces.",
            "5": "I think the framing and conceptual situating this paper does is pretty good.",
            "6": "The discussion of the difficulty of \"grounding\" complex concepts that can't be encoded easily in text is important and I'm glad it was included.",
            "7": "The principal theoretical complaint that I foresee people having regarding the overall framing of the paper is that it's not \"really\" testing grounding because of the way these the concepts are serialized into text, but I think the authors are very clear about how this approach relies on the faithful encoding of a space into text and that it's a serious limitation particularly when it comes to more complex concepts.",
            "8": "Weaknesses I think there are some subtle but pretty important problems with the paper.",
            "9": "To fix them, it might be okay to just change the framing and back off from the \"isomorphism\" angle, but that feels like losing the whole point of the paper and leaving it kind of uninteresting.",
            "10": "On the other hand, I think maybe adding some tougher experiments and better baselines could patch up the holes and make the paper really nice.",
            "11": "Details follow: Abdou et al.",
            "12": "is mentioned as a data source, but I think deeper comparisons and more credit to that work should be provided.",
            "13": "This paper presents itself as providing clear evidence of learning \"isomorphic\" conceptual spaces from text alone, I think the Abdou et al.",
            "14": "paper makes a case that is very similar (though meaningfully different, yes) and stronger than this one.",
            "15": "At the very least, these results should be put in context of theirs (especially as their data is being used).",
            "16": "In particular the Abdou et al.",
            "17": "paper already provides pretty good evidence of conceptual grounding of colors using representational similarity analysis and linear mappings on color representations extracted from the LMs.",
            "18": "These methods have some underlying assumptions but it seems to me that those assumptions are much weaker than the ones underlying the method used in this paper, which relies on the few-shot learning capabilities of the language model.",
            "19": "More on that in the next point.",
            "20": "I don't think the R-IV baseline is more fair than the R-ID one, as claimed in Sec.",
            "21": "2.5.",
            "22": "This paper is about producing the correct term for the provided grounding, which is a different problem from producing an in-domain term.",
            "23": "Generating an in-domain term is a priori significantly easier (e.g., by hard-coding it).",
            "24": "By including the requirement to generate an in-domain term, these experiments conflate the two problems.",
            "25": "So GPT-2, for example, does badly because in the few-shot format, it often can't decide on any answer with high enough confidence to make it to the top of the beam (see in Figure 3 how GPT-2 skips giving an answer at all and just moves on to the next prompt).",
            "26": "This is a classic example of \"boring\" sequence generation that happens for sequence models over high entropy output distributions (the literature on this focuses generally focuses on beam search, but I think the argument can reasonably apply in cases like this; see Holtzman et al.",
            "27": "2019, Stahlberg and Byrne 2019, Eikema and Aziz 2020), which is an orthogonal issue to the grounding problem.",
            "28": "The issue here is a combination of that and the mere fact that the smaller GPT-2 models don't make sense of few-shot prompts in the effective way that GPT-3 does — again, arguably a separate issue from grounding.",
            "29": "In short, by relying on the few-shot learning paradigm to extract the conceptual structure of concern, we're limited to extracting conceptual structure that can surface through few-shot learning (just like, for example, linear probes can only extract linearly separable features).",
            "30": "This ends up being more a measure of the few-shot learning ability of the model than the conceptual grounding capabilities of the model.",
            "31": "This is particularly problematic because it makes the baselines (ie, GPT-2 models) seem weaker than they probably should be, and it limits the baselines that can be used.",
            "32": "I would point particularly to Abdou et al., who obtained alignments similar in quality to those from masked LMs (BERT, RoBERTa, ELECTRA) using just PMI statistics and fastText embeddings.",
            "33": "It's not clear to me if the large models are exhibiting conceptual generalization/grounding abilities much larger than those afforded by such baselines (once we move out of the few-shot learning format).",
            "34": "To add to this issue, the headline experiment of generalizing to new concepts has at least one big issue that I find concerning.",
            "35": "Including all n-1 concepts besides the target one, or many such concepts (eg, 60 colors) provides a very strong constraint in the input (in my view contradicting the \"only a small number of examples\" claim in the paper's abstract).",
            "36": "While, yes, generalizing to new concepts does demonstrate that the model understands something about the domain, I think it's important that we understand this in graded terms.",
            "37": "The question this paper seems designed to address, based on the framing in the introduction and conclusion, is about whether language models can or do construct conceptual spaces that are roughly \"isomorphic\" to grounded ones.",
            "38": "In order to test this, of course, we have to construct the isomorphism.",
            "39": "But — and this is one of the classic problems with probing — if we are looking at it in terms of set isomorphism (i.e., without extra homomorphism structure — and we are, if we're measuring prediction accuracy), then it is always possible to reconstruct such an isomorphism if we have enough supervision (See Pimentel et al.).",
            "40": "The question is how much supervision we need versus how much is usefully obtainable from the pretrained model, hence approaches like Voita and Titov (2020)'s information-theoretic probing.",
            "41": "So it's not really a black and white question whether there is an \"isomorphic\" conceptual space inside the model, but a graded one.",
            "42": "And when we talk about things in terms of an isomorphism, what we're really implying is that the vast majority of the information required to construct the isomorphism is easily accessible from the model representations, so less supervision is required.",
            "43": "This is the basic thinking behind the idea of a \"minimal grounding toehold\" which has been discussed as a prerequisite to grounding language models in conceptual spaces.",
            "44": "Some of this discussion in the NLP community subsequent to Bender and Koller's 2020 paper was catalogued in a blog post that goes into a lot more detail on the issue.",
            "45": "That post proposed a color experiment extremely close to what is done in this paper, but an important difference in their proposal is that the input used by the LM to reconstruct the grounded lexicon is the minimal size that is required to span the conceptual space (i.e., constraining only the necessary degrees of freedom).",
            "46": "One might use light/dark, red/green, and blue/yellow as the three basis dimensions to align with CIELAB, or even just to try red, green, and blue.",
            "47": "I think such a minimal experiment would be required to effectively address the isomorphism question.",
            "48": "This paper, by providing many more inputs, is doing something of a mix of probing (because of the few-shot/supervision aspect) and behavioral testing (because it's relying on inherent generalization behavior in the LM distribution).",
            "49": "[Edit Nov 3: One extra thought.]",
            "50": "The inclusion of a bunch of distinct concepts in the input can potentially also provide the model with a pragmatic cue that it will be presented with a new/unseen concept label if it receives a new/unseen kind of input, which adds another confound.",
            "51": "This may not be much of an issue in the case of color since the space is fairly open and only 1/6 of the set of colors was covered anyway, but it still means a totally ungrounded baseline could do better than random.",
            "52": "To sum up: to really address the question of interest in this paper, I think the unseen-concept experiments need to more strongly limit their inputs to a very small number (or even better, report curves showing performance dependent on number of inputs),  LM experiments which constrain the output space should be included as well, and predictive baselines outside of the few-shot paradigm should be provided.",
            "53": "The results of Abdou et al.",
            "54": "seem to indicate (to me) that it should be possible to do quite a bit better than the GPT-2 results in this paper using some kind of simple model (for example with word embeddings or a probing model on LM representations).",
            "55": "It's important to have these baselines so as not to misrepresent the performance trend with respect to model growth, which I suspect reflects more the ability of the model to do \"few-shot learning\" than to ground concepts.",
            "56": "(And to be clear: I don't think it would be a strike against this paper if few-shot learning for unseen concepts with very few colors didn't give good performance.",
            "57": "That's still a cool experiment and an interesting result!)",
            "58": "Relatively minor issues: I understand you would probably want to express the color in RGB for the purposes of LM testing, but Rotations in RGB seems like it isn't the best way of doing the isomorphism control: rotating in a space where distances are more perceptually meaningful (such as CIELAB) seems to me like it would have been a better approach, as while rotation preserves distances, the same distances in different parts of RGB space may correspond to different perceptual distances.",
            "59": "On that note, I'm not sure if I totally understand the random world baseline.",
            "60": "It sounds from the description in the text (Sec.",
            "61": "2.4) that you ground words entirely randomly, but Figure 2 illustrates a \"random rotation\" which is much more constrained than this.",
            "62": "It suggests that a random rotation should destroy the underlying structure, but I don't see why this would be the case.",
            "63": "As long as distances are preserved (under the assumption necessary for the control that distances are meaningful in the first place) the structure should be preserved.",
            "64": "So any rotation should work.",
            "65": "In what sense does a random rotation destroy the structure?",
            "66": "I don't buy it based just on the example shown in Figure 2.",
            "67": "The string matching Top-1 accuracy metric seems quite permissive — are there examples, or is there a human verification step you can provide to show that it is reasonable in this case and not overly permissive?",
            "68": "There seems to be some confusion about the training data.",
            "69": "When you first mention it, you use the term OPENAI-WT with no further description or reference (Sec2.1).",
            "70": "I don't think that is the dataset's name — I found it used as an abbreviation in one paper, Gehman et al.",
            "71": "2020, but nowhere else.",
            "72": "The dataset is just called WebText.",
            "73": "Can you just call it WebText (or OpenAI WebText) and cite the GPT-2 paper directly when you name it (so it's clear where it's from)?",
            "74": "Also, in Section 2.1 it says all models are trained on this data, but this is incorrect — GPT-3 was trained on a different mix of data including two corpora of books, Wikipedia, and Common Crawl.",
            "75": "Also, Appendix A calls the data \"OpenWT\", which sounds like \"OpenWebText\", which is an open dataset scraped using the same methods as WebText — but isn't the same dataset as was used to train GPT-2.",
            "76": "Then, Section 2.4 does cite the GPT-2 paper for the data, but calls it CommonCrawl, which is incorrect — that's a different web scrape.",
            "77": "Typos etc.",
            "78": "Sec.",
            "79": "2.2, fourth line: stray space after \"prompt\" End of page 2: stray comma after \"detail\" The citation to Abdou et al.",
            "80": "can be updated — the work is appearing in CoNLL 2021.",
            "81": "Last sentence before Sec.",
            "82": "3: stray comma at \"R-IV, a fairer\" Summary Of The Review: I think this is overall a nice paper with some nice ideas and interesting results, but it falls short of adequately addressing the research question that it is designed to.",
            "83": "The biggest flaws in my view are the inclusion of too much supervision in the unseen-concept experiments, which make the results less relevant to the \"isomorphism\" question inasmuch as it is interesting to the community, and the exclusive reliance on the few-shot learning paradigm, which adds a big confounding factor to the results that makes it hard to interpret any clear trends.",
            "84": "I do think that this is really close to being a very nice paper as long as these issues are adequately addressed at least in the framing of the work if not the experiments."
        }
    },
    "Capturing Structural Locality in Non-parametric Language Models": {
        "llm_review": {
            "0": ": Strengths: Innovative approach: The paper presents a unique method to integrate structural locality into language models, addressing a gap in current modeling approaches.",
            "1": "Empirical evidence: Demonstrated improvements on two diverse datasets underscore the method's effectiveness and general applicability.",
            "2": "Significant contribution: This work sheds light on the limitations of existing similarity metrics in capturing locality, potentially influencing future research directions in language modeling.",
            "3": "Weaknesses: Limited domain analysis: While the paper provides insights into Java and Wikipedia datasets, the exploration of other domains where structural locality could play a crucial role is missing.",
            "4": "Model complexity: The introduction of locality features adds complexity to the model, which might pose challenges in understanding and implementation for some practitioners.",
            "5": "Clarity, Quality, Novelity, and Reproducibility: The paper is well-written, clearly presenting the methodology, experiments, and findings.",
            "6": "The quality of research is high, with thorough experimentation and analysis.",
            "7": "The novelty lies in the use of structural locality within non-parametric language models, a direction not extensively explored previously.",
            "8": "The inclusion of source code and detailed experiment setups enhances the paper's reproducibility.",
            "9": "Summary of the Review: The paper makes a compelling case for the inclusion of structural locality in enhancing non-parametric language models, supported by experiments across two domains.",
            "10": "Its strengths lie in its novel approach and the significant improvements demonstrated.",
            "11": "However, the exploration of additional domains and the complexity introduced by the locality features are areas that could be addressed further.",
            "12": "Overall, the paper presents a valuable contribution to the field of NLP, offering new insights and methodologies for future research."
        },
        "review#1": {
            "0": " This work concerns itself about utilizing structural locality inherent in real-world datasets in improving the effectiveness of non-parametric language models.",
            "1": "It makes a claim that a) structural locality is not implicitly fully captured by the distance metric used in non-parametric language models and further that b) explicitly plugging in structural locality into non-parametric language models can improve their effectiveness.",
            "2": "It validates this claim first by doing analysis of two datasets with the help of custom locality functions and then by plugging in the locality functions into a non-parametric language model with the help of learnable parameters.",
            "3": "Positives: 1.Well stated hypothesis and analysis that shows that structural locality is not implicitly fully captured by the distance metric used in K-nearest neighbour non-parametric language model of Khandelwal et al.",
            "4": "2.Locality features for two datasets - wikipedia and Java projects 3.Incorporation of locality features in non-parametric language models using simple learnable functions of the distance metric 4.Analysis that shows that incorporating locality features leads to improved distance distribution among nearest neighbours Negatives: 1.Structural locality inherent in datasets need to be captured by a set of custom locality feature functions which requires prior knowledge of the domain of the datasets.",
            "5": "2.",
            "6": "Marginal improvements in results 3.",
            "7": "No detailed discussion of learned parameters presented in Table 3. a.",
            "8": "It appears that w doesn't matter so much for all non l_0 features.",
            "9": "So it would be interesting to set w to 1 for these features and learn only b and for l_0 learn w. In Eqn 4. y should be w_t?",
            "10": "Summary Of The Review: The paper makes an interesting hypothesis and goes about validating the hypothesis.",
            "11": "However, the improvements due to the proposed method are marginal."
        },
        "review#2": {
            "0": " The paper models structural information into the non-parametric language models.",
            "1": "While the results demonstrate that the method improves upon the existing methods, there are some weaknesses too.",
            "2": "Strengths: The model or the loss function developed by the authors that incorporate structural information is novel.",
            "3": "The authors have also clearly explained the model.",
            "4": "Results demonstrate that the method improves upon existing methods.",
            "5": "Weaknesses: While there are clear strengths, one weakness is that one may need to define structural properties in different types of datasets that one might use.",
            "6": "For instance, it is clear that the model works for source codes and Wikipedia because associated structural information can be mined from the data.",
            "7": "It is unclear how does the method generalise across different tasks and datasets, i.e., beyond two datasets.",
            "8": "While the authors have addressed these limitations towards the end of the paper, the question is will the work be useful only to a small set of audience, or people from different domains can manually or automatically build such prior knowledge and incorporate it in this model.",
            "9": "The key advantages are clear from the paper, this seems to be the weakness that is hard to defend.",
            "10": "One possible way to improve the argument so that we could obtain Wikidata-type structure for most datasets is to exploit entity detection and linking including automatically learning their relation (vector) information in a completely unsupervised way.",
            "11": "The authors must note that I am simply giving ideas on how to strongly defend this weakness of the model.",
            "12": "In terms of experiments, these can be further improved by conducting some downstream application tasks.",
            "13": "Can the model be useful for document classification tasks?",
            "14": "Currently, it is very difficult to gauge the usefulness of the model through the limited experiments presented in the paper which mainly revolves around perplexity analysis and table 5 in the appendix has additional token prediction results.",
            "15": "Summary Of The Review: Overall, the paper indeed has some merits.",
            "16": "The paper can be made stronger by considering some comments mentioned above."
        },
        "review#3": {
            "0": " The paper is well written and easy to read What the paper is suggesting is simple, yet useful.",
            "1": "The example depicted in Figure 1 describes their motivation as well as what they are doing effectively +The concept of structural locality as used in their paper is defined clearly +Results are compared with other state of the art models (Table 2) The only weakness that I can identify is that the authors have used one dataset of each domain.",
            "2": "Since there is minor improvement in the results, it is difficult to gain confidence that the results will indeed improve on other datasets as well Summary Of The Review: Same as above"
        },
        "review#4": {
            "0": " Strengths: Authors propose a way to include structural locality into kNN-LM models Authors demonstrate improvement on retrieval tasks for Wikipedia and Java source code domains Weaknesses: It seems that the structural locality and its value are known and not new.",
            "1": "The paper's contribution then is adding the structural locality to the non-parametric language models.",
            "2": "The way to add structural locality is straightforward.",
            "3": "It seems to me that this contribution is not significantly novel.",
            "4": "There is no comparison or discussion of other ways to add structural locality to LMs.",
            "5": "It seems authors tried just a single approach that they present in the paper.",
            "6": "Using locality info in authors' experiments leads to minor improvement.",
            "7": "This is not a significant weakness since there might be other tasks where locality will contribute more.",
            "8": "However, it would have been interesting to see tasks or domains where improvement is more significant.",
            "9": "Summary Of The Review: In my opinion, the paper makes a minor contribution by proposing a straightforward way to add structural locality to kNN-LM models.",
            "10": "The contribution is possibly useful, but not major.",
            "11": "For tested tasks, structural locality information improves results but not significantly.",
            "12": "I believe the paper is marginally below acceptance threshold."
        }
    },
    "Using Document Similarity Methods to create Parallel Datasets for Code Translation": {
        "llm_review": {
            "0": ": Strengths: Innovative approach to generating parallel code datasets, addressing a significant gap in resources for code translation tasks.",
            "1": "Empirical validation demonstrates the effectiveness of the method across various programming languages, expanding the applicability of code translation techniques.",
            "2": "The study thoroughly explores the impact of noise on model performance, providing insights into the robustness of machine learning models in this domain.",
            "3": "Weaknesses: The reliance on document similarity methods may introduce biases based on the chosen method, potentially affecting the generalizability of the approach.",
            "4": "Limited exploration of the impact of different levels of noise across more diverse programming languages and contexts.",
            "5": "The reproducibility of the results may be challenging due to the complexity of the setup and potential variability in the document similarity measures.",
            "6": "Clarity, Quality, Novelty, and Reproducibility: The paper is well-structured, clearly presenting its objectives, methodology, experiments, and findings.",
            "7": "The research is of high quality, employing rigorous experimental setups and analyses to validate the proposed approach.",
            "8": "The novelty lies in the method for generating parallel datasets for code translation, a significant contribution to the field.",
            "9": "However, reproducibility might require access to specific datasets and clarity on the document similarity measures used.",
            "10": "Summary of the Review: The paper presents a significant contribution to the field of code translation by proposing a novel method for generating parallel datasets using document similarity methods.",
            "11": "This approach addresses a critical bottleneck in the field and opens new avenues for research and application in code translation across a wider range of programming languages.",
            "12": "The study's thorough empirical validation and exploration of the noise tolerance of models add to its strength.",
            "13": "However, the potential for bias introduced by document similarity methods and challenges in reproducing the results warrant further investigation."
        },
        "review#1": {
            "0": " The proposed technique can be useful for legacy code translation where annotated data is scarce.",
            "1": "The empirical investigations are solid.",
            "2": "The paper is well-written and easy to understand.",
            "3": "The authors did not provide any explanation/intuition/evidence why such a simple data augmentation technique works.",
            "4": "The main problem with this approach is that the performance suffers significantly with CodeNet dataset.",
            "5": "The reason behind similarity-based data augmentation will work is because for implementing similar functionalities often similar variable names are used.",
            "6": "However, such an assumption may not be true for CodNet where developers did independent implementations.",
            "7": "Thus, the dropping of performance for CodeNet is not surprising.",
            "8": "However, this raises questions about the usability of this technique.",
            "9": "The contrast between performance with noise is useful, but again that shows how sensitive this technique is with noise and for some legacy code (e.g., COBOL --- a language authors use to motivate the study) the amount of noise might be high.",
            "10": "Summary Of The Review: The authors propose an approach of data augmentation for code translation for document similarities.",
            "11": "However, the results show the approach suffered for independently developed code (CodeNet), which is the most realistic case.",
            "12": "The approach is also susceptible to noise."
        },
        "review#2": {
            "0": " Strengths The paper is easy to follow.",
            "1": "Although I am not super familiar with code translation, I can catch most of the points.",
            "2": "The proposed method is simple and easy to use.",
            "3": "The experiments are relatively thorough, covering different types of document similarity methods, programming languages, model architectures, and evaluation metrics.",
            "4": "The results seem to be convincing.",
            "5": "Weaknesses My main doubt is the results presented in \"Section 5.3 RQ3: TRANSLATING BETWEEN A WIDER SET OF PROGRAMMING LANGUAGES\".",
            "6": "The authors report CA@5 scores here but in the other place, BLEU/CodeBLEU/EM is used.",
            "7": "So, what is the intuition that uses different metrics?",
            "8": "BLEU can also handle multi-reference evaluation.",
            "9": "It would be nice if the authors could add these results in the author response.",
            "10": "The research is somewhat superficial.",
            "11": "The authors only show the models can tolerate certain noises but do not propose any simple heuristics to alleviate the impact of noises.",
            "12": "For example, penalizing the noises (i.e., the targets not belonging to the source) during model training (fine-tuning).",
            "13": "[1] might inspire the authors.",
            "14": "[1] Wu, Lijun, Jinhua Zhu, Di He, Fei Gao, Tao Qin, Jianhuang Lai, and Tie-Yan Liu.",
            "15": "Machine translation with weakly paired documents.",
            "16": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.",
            "17": "4375-4384.",
            "18": "2019.",
            "19": "Typos Section 3 Proposed Method: curated -> created Summary Of The Review: This paper is interesting, but the research is somewhat superficial."
        },
        "review#3": {
            "0": " Although I recognized that the main contribution of this paper is not proposing a sophisticated method to align samples, I thought the paper need more attention to design efficient strategy.",
            "1": "The entire alignment algorithm takes O(D*f(D')) where f(D') is the complexity of GetSimilarDocuments().",
            "2": "Unless any assumption this takes O(DD') meaning that it tends to be infeasible by increasing the data.",
            "3": "Also, the algorithm seems a simple greedy method and may be heavily affected by the sampling order.",
            "4": "According to Table 2 and 3, I observed that the pseudo-match accuracy introduced here does not reasonably reflect the real characteristics needed to be considered in code translation models since the BLEU of several systems compete with each other.",
            "5": "I recognized that Figure 1 shows every range of noise ratio, except around 100%, has certain gradient against the accuracy, and got the opposite conclusion \"the less noise the better\" against that the paper concluded \"we expect a certain amount of noise, we can expect the models to perform reasonably well.\"",
            "6": "The root cause of this is that the paper did not provide a reasonable criterion about acceptance of this experiment first.",
            "7": "As far as I saw the Figure 2 and 3, the accuracy is heavily reflected by the amount data available during training and some normalization should be needed to compare these metrics each other without biases.",
            "8": "Figure 3 also show some remarkable characteristics in translating only into C++, but the paper did not mention about this point.",
            "9": "The motivation of the paper includes \"modernizing legacy applications,\" but the experiments does not reflect this perspective.",
            "10": "For example, the first section repeatedly referred to the COBOL applications, but we maybe not be able to reflect the experiments into this application due to large discrepancy between languages.",
            "11": "Summary Of The Review: The alignment algorithm need to be improved.",
            "12": "This is not critical: this is still acceptable for the first observation.",
            "13": "Conclusion sounds sometimes not reasonable according to experiments.",
            "14": "Some results are hard to discuss due to unnormalized data."
        },
        "review#4": {
            "0": " In its current form, the problem statement and approach seem very trivial.",
            "1": "First, the data sets seem rather unsophisticated and it is not clear what the pathway is from this method to say mining parallel corpora from github or some large-scale code repository websites.",
            "2": "In its current form, it seems like this is preliminary work and with some more sophistication (in method + data sets) it is going to be a good future task.",
            "3": "I suggest that this be submitted to an appropriate workshop and a future attempt be made to submit this to ICLR.",
            "4": "In particular, good directions can include in-the-wild performance when tested on something outside of coding-problem data sets (especially since those are so well-organized by language and problem).",
            "5": "Summary Of The Review: The method and data set in this paper in its current form are rather trivial and more sophistication is required.",
            "6": "The techniques seem highly tuned for mining well-organized data sets from coding-problem websites.",
            "7": "A better and more improved model that can scale in the wild is recommended and would be a better fit for this venue."
        }
    },
    "Out-of-Distribution Detection and Selective Generation for Conditional Language Models": {
        "llm_review": {
            "0": ": Strengths: The methodology introduces a practical and effective solution for OOD detection in CLMs without requiring a separate detection model.",
            "1": "The paper provides a comprehensive evaluation framework that includes human quality ratings, addressing a critical need in the community.",
            "2": "The combined use of perplexity and the novel OOD score for selective generation is innovative and demonstrates a significant improvement over existing methods.",
            "3": "Weaknesses: The approach's dependency on specific embeddings and the potential for its performance to vary across different models or tasks is not extensively discussed.",
            "4": "While the method shows improvements, the real-world applicability and scalability of the proposed solution could be further explored, particularly in more diverse or adversarial settings.",
            "5": "Clarity, Quality, Novelity, and Reproducibility: The paper is well-written, presenting a clear and logical progression of ideas.",
            "6": "The research is of high quality, leveraging both theoretical insights and empirical evaluations.",
            "7": "The novelty of the approach lies in its effective combination of perplexity scores and OOD detection for improving CLM outputs.",
            "8": "The provided details and evaluation framework contribute to the paper's reproducibility, although additional information on the implementation and potential limitations would further enhance it.",
            "9": "Summary of the Review: This paper introduces an innovative method for enhancing the safety and reliability of conditional language models through OOD detection and selective generation.",
            "10": "It addresses a critical gap in the field, providing both theoretical contributions and practical improvements.",
            "11": "Despite some areas for further exploration, the paper's strengths in novelty, methodological clarity, and potential impact make a compelling case for its acceptance."
        },
        "review#1": {
            "0": ": Strengths: The paper has a clear motivation and succeeds at what it's trying to do.",
            "1": "The experiments are well designed.",
            "2": "Weaknesses: Nothing major.",
            "3": "There are a couple of typos.",
            "4": "Clarity, Quality, Novelty And Reproducibility: The paper is very clear and its quality is high.",
            "5": "It's also novel, though inspired by a similar score for classification tasks.",
            "6": "The experiments seem to be reproducible.",
            "7": "Summary Of The Review: I believe this paper will be useful for the community.",
            "8": "It should get accepted."
        },
        "review#2": {
            "0": ": Strength: (1) It addresses an interesting OOD problem for conditional generation, where OOD errors can get easily accumulated via auto-regressive generation.",
            "1": "(2) It proposes an effective OOD score.",
            "2": "The score is based on the MD to the input and output embeddings of the language model.",
            "3": "Relative MD is used to measure the inference time deviation from training data and a background distribution model.",
            "4": "(3) The OOD score in combination with perplexity provides a good metric for selective generation of near-domain samples with quality control.",
            "5": "It correlates well with generation quality.",
            "6": "It is simple to deploy to address the domain shift issue at inference time.",
            "7": "(4) Experiments and results are well-explained.",
            "8": "Analysis is comprehensive.",
            "9": "The analysis of the correlation of perplexity and generation quality with OOD score changes is insightful.",
            "10": "Weaknesses: (1) Novelty is limited as MD-based OOD metrics have been widely studied and used, though this paper used it in a different setting (non-classification).",
            "11": "(2) It is not compared with other common OOD detection strategies, such as threshold based, energy based, contrastive based approaches.",
            "12": "(3) It’s not clear how the proposed method generalizes to other tasks and model structures.",
            "13": "(4) It is unclear why the embeddings from the final layers are chosen.",
            "14": "(5) It is unclear how the background data is selected, and how well the background Gaussian is expected to generalize.",
            "15": "It seems fitting the background Gaussian is nontrivial and the proposed method is not as light-weight as it claimed to be.",
            "16": "More importantly, what if we know nothing about the OOD?",
            "17": "It seems the background Gaussian would fail to work when there is no good definition of background data.",
            "18": "Clarity, Quality, Novelty And Reproducibility: Clarity is good.",
            "19": "Experiments are well-explained.",
            "20": "Analysis is comprehensive.",
            "21": "However, there is too much important material in the Appendix.",
            "22": "Would suggest to improve the paper structure and format.",
            "23": "Novelty is limited.",
            "24": "It addresses an interesting OOD problem in conditional generation.",
            "25": "The proposed approaches are effective.",
            "26": "However, MD-based OOD metrics have been extensively studied and used.",
            "27": "Reproducibility is limited.",
            "28": "The OOD score fitting and computation are nontrivial.",
            "29": "Though the experiment settings and results are thoroughly explained, it is relatively challenging to reproduce all of them due to lack of necessary implementation details.",
            "30": "Summary Of The Review: The problem is interesting, the proposed approach is effective, and experiments and analysis are comprehensive.",
            "31": "Novelty is a bit limited, lack of comparison with other common OOD approaches, and the generalization of the proposed approach is questionable."
        },
        "review#3": {
            "0": ": Strengths: The authors address a timely research question, since LMs popularity is (still) on the rise and LMs are deployed almost as out-of-the-box tools for \"language understanding\" or \"language generation\".",
            "1": "Quite a number of experiments on different aspects of OOD detection.",
            "2": "I like the in-depth investigation of the different ideas proposed/studied in the paper.",
            "3": "Weaknesses: There are some statements/claims that are either not well developed in the text (leaving to the reader to guess the intention of the authors when saying something) or that seem overeaching/inconsistent.",
            "4": "Please refer to my comments for detailed examples of these.",
            "5": "The paper can be hard to follow at times, and there are a lot of different experiments.",
            "6": "Perhaps the authors would have a perceivingly stronger paper by making it less wide and more deep (i.e., cover less different points but go deeper into the points it does cover).",
            "7": "Clarity, Quality, Novelty And Reproducibility: The paper is clear enough (nonwithstanding the points mentioned as weaknesses), and includes many interesting experiments.",
            "8": "I am not so positive about its reproducibility, since there are no statements about (a plan for open sourcing) code for reproducing the experiments.",
            "9": "One point I did not like very much is the fact the authors mention two external baselines at the end of their related work section (the ensemble methods proposed for e.g.",
            "10": "machine translation), but do not compare their methods against those baselines.",
            "11": "The paper seems to be novel enough, although this is an educated guess by the reviewer (for lack of detailed knowledge about the state-of-the-art).",
            "12": "Summary Of The Review: My recommendation is a borderline accept.",
            "13": "The reason for \"accept\" is the fact the paper is on a timely research topic and that it includes many varied and interesting experiments.",
            "14": "The reson for \"borderline\" is detailed in my comments/questions below.",
            "15": "The examples of existing works on OOD detection could be greatly improved (2nd paragraph in the introduction).",
            "16": "Not sure the cow example is the best one, since it is more an example of \"noisy\" rather than \"out-of-distribution\" input.",
            "17": "It is also an example in the vision domain, should you not include an example in the (conditional) language modelling domain?",
            "18": "Page 2, paragraph 1, last sentence (\"In Section 4, we show that while model perplexity is a reasonable choice for [performing selective generation with] in-domain examples, combining with our OOD score works much better when the input distribution is shifted\") -> Please make it clear that you are talking about selective generation, e.g., I added the text in-between brackets to do that as a suggestion.",
            "19": "Page 2, section 2: In the first paragraph, it is not clear what is the (practical) relationship between maximum softmax probability (MSP) and perplexity.",
            "20": "Please better elaborate your point, the paragraph reads like two more-or-less independent points glued together.",
            "21": "A method that comes to mind is the mean over the MSP for a sequential output, for instance.",
            "22": "Page 3, last paragraph (\"MD0(ztest) := MD(ztest; μz0 , Σz0 ) is the MD to a background Gaussian N (μz0 , Σz0 ), fit using a large, broad dataset to approximately represent all domains.\")",
            "23": "-> Even though I understand the point, in your introduction you mention that, compared to OOD detection in a standard classification problem, performing OOD detection for CLMs have the added difficulty that CLMs have virtually infinite, unbounded output spaces.",
            "24": "This idea of \"approximately\" representing \"all domains\" goes directly against that point.",
            "25": "Please defend why this claim/idea makes sense for OOD detection of CLMs.",
            "26": "Is the point made in the introduction not so important afterall?",
            "27": "Page 4, 2nd paragraph (\"While we use the ground-truth outputs to fit N (μw, Σw), we do not have ground-truth in general for the background examples (e.g.",
            "28": "C4).",
            "29": "We decode outputs from the trained CLMs and use those output embeddings to fit the background output Gaussian, N (μwδ , Σwδ ).\")",
            "30": "-> This sentence is not clear.",
            "31": "Please rephrase.",
            "32": "Your model requires input/output for the two tasks, i.e., summarisation and translation.",
            "33": "You have C4 for summarisation, and ParaCrawl for translation.",
            "34": "Please elaborate that ParaCrawl does not have translation pairs, but paraphrases (right?",
            "35": "), and that C4 does not include summaries (to the best of my knowledge).",
            "36": "Your descriptions seem incomplete and assume that the reader will make the leap about important points that should be made clear in your manuscript.",
            "37": "Page 4: \"Since we have implicitly defined two classes (...)\" -> There was no implicit definition, I would say this definition of the two classes was explicitly made.",
            "38": "You are using AUROC as your metric, but I would like to see the plots rather than the summary metric (i.e., the actual curves), perhaps in the appendix.",
            "39": "As baselines, you are using the perplexity score but not the maximum softmax probability MSP.",
            "40": "Could you not use the mean MSP (over the sequence) as a proxy for OOD?",
            "41": "Does it consistently underperform compared to using perplexity?",
            "42": "Page 6: \"Though RMD and Binary logits OOD scores both perform well at OOD detection, RMD OOD score is better at distinguishing near-OOD from far-OOD.\"",
            "43": "-> Why does that matter?",
            "44": "That is not what models were trained for.",
            "45": "If you had trained models (RMD vs. Binary logits) to distinguish between near-OOD and far-OOD, perhaps the outcome would have been different.",
            "46": "This split of near- vs. far-OOD is also a bit arbitrary, if you will make it central in your evaluation/discussion (like you do), perhaps it is a good idea to be more principled about it (e.g., using some objective/concrete measure to make one dataset \"more OOD than\" another).",
            "47": "You hint at unigram overlap as a measure of OOD-ness later on, perhaps you should use the \"degree of OOD-ness\" as the percentage of unigram overlap between the two domains, for example.",
            "48": "Page 6: \"We observed that law has the highest unigram overlap rate (48.8%) and the second highest overall overlap with the in-domain data (Table A.7).",
            "49": "This confirms that law is actually not OOD data and explains why no method can detect it.\"",
            "50": "-> If your definition of OOD only depends on unigram overlap, you do not need anything other than computing the unigram overlap between domains to perform OOD detection.",
            "51": "I find this a very misleading comment, since the fact a dataset has high unigram overlap with another does not automatically rule out this dataset being OOD.",
            "52": "You should include more nuance in these statements.",
            "53": "Page 7: \"Amazon Mechanical Turk workers were asked to evaluate summaries generated by the xsum model on a scale of 1-5 (bad-good) using 100 examples from xsum, cnn dailymail, reddit tifu, and samsum.",
            "54": "We collected 3 ratings per example and took the median to reduce inter-rater noise\" -> This is a bad strategy to reduce inter-annotator agreement.",
            "55": "You are basically hiding the (possible) disagreement by only using the median, whereas in order to reduce the disagreement (and not mask it) you should instead 1) select high-quality annotators, 2) make the annotation guidelines clear including concrete examples of how to annotate difficult cases, 3) include control examples for annotators, etc.",
            "56": "Please at least report the variance of the annotators, so the reader knows how noisy they are.",
            "57": "Page 8, Section 4.4: \"To evaluate that, we propose using the Quality vs Abstention Curve (QA), analogous to accuracy versus rejection curve used for selective prediction in the classification.\"",
            "58": "-> Citation missing.",
            "59": "You mention Malinin & Gales (2020); Xiao et al.",
            "60": "(2020) later, but it is not clear how exactly they relate to QA.",
            "61": "If you are the one proposing this measure, please make it clear.",
            "62": "Page 8, Section 4.4: \"For translation, the OOD score is better than perplexity when abstention rate α > 0.5 and worse than perplexity when α < 0.5.\"",
            "63": "-> From Figure 5, the cut-off point looks more like 0.6 or 0.65 rather than 0.5.",
            "64": "Is that so?",
            "65": "Page 9, Section 5, last paragraph: \"In this work, we focus on developing scores that can be readily derived from the generative model itself, without much increase in computation.\"",
            "66": "-> You did not compute how much time it takes to run your proposed models compared to the baselines.",
            "67": "I find it could be a limitation of your work, especially in light of the existing work using ensembles mentioned in your related work section.",
            "68": "You also did not include any such ensembles as your baselines.",
            "69": "Why?"
        }
    },
    "Interactively Generating Explanations for Transformer Language Models": {
        "llm_review": {
            "0": ": Strengths: Introduces an innovative approach to interpretability in transformer LMs, enhancing transparency without sacrificing performance.",
            "1": "The interactive learning setup (iProto-Trex) leverages user feedback to refine explanations and model accuracy, demonstrating a novel way to bridge human insights and AI.",
            "2": "Provides comprehensive experimental evaluation, demonstrating the effectiveness of Proto-Trex networks across multiple datasets and against various baselines.",
            "3": "Weaknesses: The complexity of the Proto-Trex architecture might pose challenges for widespread adoption, especially in resource-constrained environments.",
            "4": "While promising, the evaluation of user interactions and their impact on model performance and interpretability could benefit from further in-depth studies involving a broader range of participants.",
            "5": "Clarity, Quality, Novelty, and Reproducibility: The paper is well-written, presenting a clear and concise explanation of the Proto-Trex and iProto-Trex frameworks.",
            "6": "The research is of high quality, contributing novel insights into interpretability in NLP.",
            "7": "The proposed methods are novel, offering a practical approach to improving the transparency of transformer LMs.",
            "8": "The experiments are well-designed, and the paper provides sufficient detail for reproducibility, though the complexity of the models might pose some challenges.",
            "9": "Summary of the Review: The paper presents an innovative approach to enhancing the interpretability of transformer language models through prototype networks and interactive learning.",
            "10": "It demonstrates the effectiveness of Proto-Trex networks in providing interpretable and accurate model predictions, supported by comprehensive experimental evaluations.",
            "11": "While the complexity of the proposed methods might challenge their adoption, the paper significantly contributes to the field of NLP by offering a novel solution to the interpretability problem in deep learning models."
        },
        "review#1": {
            "0": " Strengths S1 - The idea of choosing a similar example from the train data as an explanation is an interesting idea.",
            "1": "Weaknesses and Questions: W1 - I'm quite unsure about the set of prototypes.",
            "2": "It seems not described in the main contents.",
            "3": "I checked examples of prototypes in the appendix but the 1:1 label distribution (positive vs. negative) seems to me they are human-picked.",
            "4": "W1-Q1 - If the set of prototypes pre-defined, do we really need to train the prototype?",
            "5": "what if we just compute sentence similarity between input and the prototype?",
            "6": "W2 - The main table is not clear, especially on faithfulness.",
            "7": "What if we just consider the random prototype as a prototype?",
            "8": "what is the faithfulness of that random prototype then?",
            "9": "Q2 - Just curious, in the introduction, why the post-hoc explanation is potentially reporting-bias?",
            "10": "Isn't this method also potentially reporting-bias since it only uses samples from train data as prototypes?",
            "11": "Summary Of The Review: My overall disposition towards the paper is indifferent although the paper proposes an interesting idea.",
            "12": "It is a bit difficult to follow the method due to a lack of information.",
            "13": "More details are needed especially on experimental settings, and methods."
        },
        "review#2": {
            "0": " This paper seems motivated by a prior NeurIPS 19 work \"This Looks Like That...\" in the sense that the architecture and loss designs are sourced from there.",
            "1": "The nice thing about this paper is that it focuses on NLP tasks, so the framework could potentially benefit the explanation community.",
            "2": "The early part of this paper is very straightforward and intuitive.",
            "3": "Related works have limited coverage.",
            "4": "The explanation generation part is vague.",
            "5": "The experiment section is weak.",
            "6": "Analysis on generated explanation is also weak.",
            "7": "The architecture relies on using prototypes which are nicely discussed in this paper.",
            "8": "The problem if improving interpretability without trading off downstream task F1 is interesting since the trade-off was common among prior works.",
            "9": "However a couple confusing points still.",
            "10": "There are many lines of explanation works, such as those use prompt engineering, information bottleneck, and purely generative approaches.",
            "11": "This paper has limited coverage on these topics.",
            "12": "Touching different approaches is important here since the way ProtoTrex handle explanation might not easily extend to all other cases.",
            "13": "Even though this design was from the NeurIPS 19 paper, but in the task of NLP, how does the prototype embeddings compare against the label-wise weights in the final classification layer?",
            "14": "This is to imagine that, without the use of the complicated loss in Eq 1, how does simply treating the label-wise embeddings as prototypes perform?",
            "15": "The explanation generation (sec 3.5) needs elaboration.",
            "16": "It seems this paper uses prototype embedding to find a training example as it nearest neighbor, and then use this data point as explanation to its prediction.",
            "17": "This design has certain limitations: a) not context/example dependent; b) this is hardly generation, instead, it is more in line with salience-based explanation works.",
            "18": "Tab2 indeed shows some examples with explanation that partially depends on the input example.",
            "19": "But no idea how they were generated.",
            "20": "Tab 1b is confusing.",
            "21": "I don't understand what each number means.",
            "22": "This paper very briefly went over some statements without getting into details.",
            "23": "I don't see a solid explanation evaluation in this paper.",
            "24": "Tab 1c shows rationale performances however these numbers are quite low compared with prior works (e.g.",
            "25": "Paranjape's work at EMNLP 20).",
            "26": "And not sure if rationale performances are based on token or sentence selection.",
            "27": "Either way, this evaluation has nothing to do with generation.",
            "28": "And when it comes to generative explanations, ideally, there should be some human-based evaluation over a subset of testing data.",
            "29": "But no such thing in this paper.",
            "30": "Summary Of The Review: I think the architecture has novelty when it comes to NLP tasks.",
            "31": "And this work could benefit the explanation community.",
            "32": "However, I found the experiment results are confusing.",
            "33": "To the best degree, it offers marginal improvement over the best baselines in terms of task F1.",
            "34": "When it comes performance of explanation, I only see confusing numbers, thus no conclusion can be made.",
            "35": "Analysis on generated explanation is another weak point since it's absent."
        },
        "review#3": {
            "0": " Strengths: The proposed Proto-Trex/iProto-Trex are technically sound.",
            "1": "Through several example cases, the proposed model could generate reasonable explanations.",
            "2": "Weaknesses: The training loss of the model seems to be too complicated.",
            "3": "In Eq.1, there are 6 parts for the overall training loss, and each of them associates with a lambda term (I checked the Appendix, but failed to find how you set lambda terms).",
            "4": "Each of them sounds reasonable, but unfortunately, there is no specific ablations on how these parts attribute to the final prediction/explanation.",
            "5": "According to the illustrations at the end of Section 3.4, there are many hyper-parameters should be set (along with lambda terms in training loss).",
            "6": "How to determine these hyperparameters?",
            "7": "If we move to a new dataset, we might lost in tuning these hyperparams.",
            "8": "As the model incorporates additional modules (compared to pure classification model), it is questionable how efficient is Proto-Trex (both for parameter size and inference time).",
            "9": "Comments: It is unclear where are the results from in Table 1b and 1c.",
            "10": "Yelp?",
            "11": "Movie?",
            "12": "Toxicity?",
            "13": "Summary Of The Review: The paper addresses an important issue in explainable natural language processing models.",
            "14": "The proposed model is reasonable, and the results seem to be OK.",
            "15": "However, many issues are left unclear, such as its efficiency, hyper-parameter tuning, etc.",
            "16": "I tend to lean towards weak rejection at the moment."
        },
        "review#4": {
            "0": " Strengths: The paper touches upon several interesting threads: interpretability, case based reasoning and using user interactions for improving models.",
            "1": "It is well written, with a comprehensive set of experiments.",
            "2": "Weaknesses: Hyperparameters: My initial impression of the method is that it involves far too many hyperparameters (various $\\lambda$ for combining losses, choosing similarity metrics etc).",
            "3": "How hard was tuning hyperparameters?",
            "4": "Results: different models respond differently to the proposed approach.",
            "5": "For instance, we see large performance gains on BERT but performance drops for SBERT.",
            "6": "It would be good to know why this happens.",
            "7": "User interaction results are not convincing.",
            "8": "From Table-3, it seems like the interactions barely help, even though the prototype changes sometimes.",
            "9": "Could the authors comment more about this?",
            "10": "Can the authors provide with confidence intervals for all results so we can compare across different settings better?",
            "11": "Prototype quality: prototypes do not look very convincing.",
            "12": "I glanced at the prototypes in the Appendix and they seem to be quite redundant / of low diversity.",
            "13": "For instance, from Table-6 a lot of the prototypes either model “bad service and bad food” or “good service and good food” (P2/P4/P6/P8/P10).",
            "14": "However yelp also has examples with “bad service, good food”, “good food, bad service”, examples not related to restaurants at all but the prototypes do not uncover these.",
            "15": "Faithfulness: How competitive are these numbers with post-hoc methods?",
            "16": "I think for presentation purposes, it would be good to also compare this with SHAP/LIME/IG.",
            "17": "Overall, I think this paper is currently borderline.",
            "18": "However, I’m happy to increase my score if the weaknesses can be quickly addressed —- the main ones are providing evidence that the learnt prototypes can be diverse and cover a lot more of the “modes” in the training distribution, providing a more comprehensive comparison with posthoc interpretability approaches, and confidence intervals.",
            "19": "Summary Of The Review: While i think this work combines many interesting threads in interpretability, using user feedback and case based reasoning, the paper in its current state is not ready.",
            "20": "I’m happy to increase my score / recommendation if the weaknesses are addressed, however."
        }
    },
    "SaMoE: Parameter Efficient MoE Language Models via Self-Adaptive Expert Combination": {
        "llm_review": {
            "0": " Strengths: Clarity and Methodological Rigor: The paper is well-structured, presenting a clear problem statement, detailed methodology, and extensive experimental results.",
            "1": "The proposed solution is innovative, addressing a significant issue in the scaling of MoE models.",
            "2": "Significant Parameter Efficiency: SaMoE achieves remarkable reductions in parameter count while maintaining or even improving model performance on various tasks.",
            "3": "This advancement could make large-scale language models more accessible and cost-effective.",
            "4": "Comprehensive Evaluation: The authors conduct thorough experiments, including ablation studies and comparisons with several baselines and MoE configurations.",
            "5": "They demonstrate SaMoE's superiority across multiple benchmarks, providing a solid foundation for the claims.",
            "6": "Weaknesses: Limited Discussion on Limitations and Broader Impact: While the paper extensively discusses the technical aspects and benefits of SaMoE, it lacks a detailed discussion on potential limitations, broader impacts, and ethical considerations of deploying such models.",
            "7": "Reproducibility Concerns: Though the paper states that the code will be open-sourced, the lack of detailed hyperparameter settings in the main text could hinder reproducibility.",
            "8": "A dedicated section for reproducibility, including data preprocessing and model initialization details, would enhance the paper's value.",
            "9": "Clarity, Quality, Novelty, and Reproducibility Clarity: The paper is well-written, with a logical flow that facilitates understanding of complex concepts.",
            "10": "The use of figures and tables effectively illustrates the methodology and results.",
            "11": "Quality: The research is of high quality, demonstrating a deep understanding of MoE models and presenting a novel approach to improving their parameter efficiency.",
            "12": "Novelty: SaMoE introduces a unique method for combining expert layers, distinguishing it from previous works on MoE models.",
            "13": "The self-adaptive expert combination mechanism is a novel contribution to the field.",
            "14": "Reproducibility: Given the promise to open-source the code, the paper meets the reproducibility criterion.",
            "15": "However, detailed hyperparameter settings and training procedures in the supplementary material or the main text would further strengthen this aspect.",
            "16": "Summary of the Review The paper introduces SaMoE, a novel MoE architecture that significantly improves parameter efficiency by adopting a soft combination of global expert layers.",
            "17": "This work stands out for its methodological rigor, extensive experimentation, and the significant reductions in parameter counts while maintaining high model performance.",
            "18": "The novelty of the self-adaptive expert combination and the thorough evaluation solidify the paper's contributions to the field of NLP and large-scale language models.",
            "19": "Despite minor weaknesses in discussing broader impacts and limitations, the paper presents a compelling advancement in making large-scale MoE models more efficient and accessible."
        },
        "review#1": {
            "0": ": Strengths: I think the authors found the right critical bottlenecks for the MoE models, which are the trainability (model quality aspect) and the total number of parameter count (system performance aspect).",
            "1": "The proposed solution with the empirical results shows it's on the promising direction, although it's still far from completely addressing those foundational limitations of MoE, The ablation and scaling laws section are very helpful to the research community to understand how to set the hyperparameters.",
            "2": "Weaknesses: How the speedup in table 1 is evaluated?",
            "3": "The gains in the downstream tasks are marginal.",
            "4": "It's better to report the variation of zero-shot results at nearby checkpoints as well.",
            "5": "I feel it's important to report the inference step time during autoregressive decoding to best demonstrate the gains from a smaller number of parameters.",
            "6": "Because during decoding on accelerators, it's more often memory bound instead of compute bound.",
            "7": "When decoding a single token given the prefix, the flops to compute each token is relatively small.",
            "8": "However, the whole model parameters (in billions or even trillions) needs to send more HBM to the actual compute units during each decoded step.",
            "9": "This HBM-cache communication is usually the dominant factor in the inference cost.",
            "10": "Double check the multi-rc results?",
            "11": "It's a big jump from 1+ to 70+.",
            "12": "Clarity, Quality, Novelty And Reproducibility: The flow of this paper is crystal clear.",
            "13": "The authors first identified the bottlenecks of an existing algorithm, found the root cause, proposed a solution, and finally demonstrated the effectiveness of the proposed solution with well prepared experimental evaluations.",
            "14": "The text, tables, and figures are all of high quality.",
            "15": "Summary Of The Review: This paper worked on an important research topic: how to reduce the training and serving costs for large language models.",
            "16": "The proposed algorithm is only marginally novel but empirically significant.",
            "17": "The 5x reduction in the number of total parameters would improve the serving speed for MoE models by a lot.",
            "18": "However, the gains in the downstream tasks are only marginal.",
            "19": "So it would be better if the authors clearly demonstrated why the large reduction in the parameter count matters using the metrics people care about: the serving latency, the step time, etc.",
            "20": "Alternatively, the authors can show the quality difference when matching the inference cost."
        },
        "review#2": {
            "0": ": Strengths: The proposed method is simple in design and implementation but achieves reasonably good results.",
            "1": "Weaknesses: The paper does not provide a fair comparison by fixing total parameters but ignore the computational cost (FLOPs) or activated parameters.",
            "2": "In traditional MoE research, the general goal is to achieve better quality with a fixed computational cost (FLOPs), not with a fixed total parameters.",
            "3": "The reviewer understand that this method provides a efficient way saving total parameters, but the reviewer suspects to achieve better quality, this method would also significantly increase the FLOPs compared to traditional top1 or top2 based routing used in switch transformer and GLaM.",
            "4": "Table 1 does not provide any details on computational cost.",
            "5": "According to multiple prior works including T5 and the Chinchilla [3], there is always a tradeoff between model capacity and training tokens, the larger the model is, the lower training data/steps can be achieved within the same computational cost budget.",
            "6": "According to Table 1, activated parameters are made fixed around 1B, however, it might not be clear about computational cost in this work's setting.",
            "7": "The paper misses fair comparisons with many significant related work including autoregressive sparse MoE, GLaM [1].",
            "8": "GLaM adopts a top-2 based routing, that can yield much better results than top-1 based routing.",
            "9": "Various efficient routing functions should be compared with in this work, as intelligent routing functions achieve similar effects of improving parameter efficiency.",
            "10": "For example, Expert Choice [2] routing achieves heterogeneous experts such that different tokens can utilize a variable number of parameters.",
            "11": "[1] https://arxiv.org/pdf/2112.06905.pdf [2] https://arxiv.org/abs/2202.09368 [3] https://arxiv.org/abs/2203.15556 Clarity, Quality, Novelty And Reproducibility: The paper is clearly written.",
            "12": "The proposed method is a minor improvement over traditional parameter sharing scheme like suggested in Universal Transformer.",
            "13": "The paper can be reproduced but the results are not valid.",
            "14": "Summary Of The Review: No free lunch in deep learning: reducing parameters and reducing training time will not come for free without sacrificing model quality.",
            "15": "The paper's results are based on fixing total parameters but not on fixing computational cost (activated parameters and FLOPs), which can be unfair to many related works including GLaM.",
            "16": "The reviewer would not believe in any results that purely relying on parameter sharing, we could improve quality without introducing additional computational cost.",
            "17": "For example, whether this method increases activated parameters (experts per token) is unclear and should be explained.",
            "18": "For example, the paper can be increasing the number of layers or expert width or number of experts per token compared to GLaM.",
            "19": "All these increase activated parameters, thus inference time.",
            "20": "The paper should be also more proactive in explaining why quality gains can be achieved."
        },
        "review#3": {
            "0": ": Weaknesses: (1).This paper carried out analysis first and listed three challenges from analysis.",
            "1": "However, I did not know which MoE model does this paper study.",
            "2": "In Figure 1, it shows “MoE” but I don’t know which MoE model is used to carry out experiments.",
            "3": "There are plenty of MoE models such as Gshard (Lepikhin et al., 2020), Switch Transformer (Fedus et al., 2021), Base Layers (Lewis et al., 2021), HASH Layers (Roller et al., 2021), and etc.",
            "4": "Different MoE models may lead to different conclusions.",
            "5": "The author needs to announce which model they used for analysis and add citations.",
            "6": "(2).This article used only one MoE model to draw analysis conclusions, which I cannot agree with.",
            "7": "Because different MoE models may have different performance, analysis conclusions need to conduct experiments with at least two representative MoE models when talking about common challenges with MoE models.",
            "8": "(3).I am very suspicious about the expert pool method proposed in this article.",
            "9": "How to choose the size of the expert pool.",
            "10": "I speculate that the amount of experts required by a MoE model may be related to the diversity of the dataset.",
            "11": "Table 1 in BASE layers paper [1] shows similar words usually gathered to the same expert unit.",
            "12": "However, this article only uses one dataset for pretraining, and does not use multiple datasets to test the required expert pool size.",
            "13": "(4).Followed by the third problem, this paper selected the Pile dataset as the pre-training dataset.",
            "14": "However, the Pile dataset is full of duplicate documents (see [2] page 2), and this paper does not perform additional de-duplication processing.",
            "15": "Because the dataset selected in the article has a lot of repetition and the tokens are not diverse, the size of the expert pool does not need to be large.",
            "16": "The conclusion is likely to change when changing to a different (diverse) pre-training dataset.",
            "17": "(5).As a MoE model, it is basically necessary to control the number of flos and compare it with the dense models and sparse models with the same number of flops, but this paper does not report total training flops number and total train computer (PF-days).",
            "18": "In addition, this paper doesn’t compare it with a dense model with the same amount of flops in table 1.",
            "19": "(6).I also have some questions about the experimental results of table 2.",
            "20": "When we compared SaMoE (350M-128E) with dense model (350M), SaMoE should have more flops since it needs additional all2all communication cost.",
            "21": "However, I notice usually a dense model (350M) could get a score of 70.2 on piqa.",
            "22": "This SaMoE with more flops achieves a score 68.9.",
            "23": "(7).Minor suggestion: usually we reported pretraining perplexity instead of validation loss in figure 3.",
            "24": "References: [1].",
            "25": "Lewis, Mike, et al.",
            "26": "\"Base layers: Simplifying training of large, sparse models.\"",
            "27": "International Conference on Machine Learning.",
            "28": "PMLR, 2021.",
            "29": "[2].",
            "30": "Zhang, Susan, et al.",
            "31": "\"Opt: Open pre-trained transformer language models.\"",
            "32": "arXiv preprint arXiv:2205.01068 (2022).",
            "33": "Clarity, Quality, Novelty And Reproducibility: Quality: Due to the above weaknesses, this paper does not have a high quality.",
            "34": "Summary Of The Review: This paper proposes a new MoE model.",
            "35": "However, in the analysis part, it only carried out analysis experiments with one MoE model, which is hard to tell if findings applied to all MoE models.",
            "36": "In addition, this paper proposes to have a fixed number of global MoE layers, which is probably not suitable when a pre-training dataset has very diverse tokens.",
            "37": "It happens that this paper selects the Pile as the pretraining dataset, and Pile is widely considered to contain many repeated sentences.",
            "38": "(see [2] page 2)."
        }
    }
}