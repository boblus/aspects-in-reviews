{
    "tmsqb6WpLz": {
        "llm_review": {
            "0": "**Novel Analysis Approach**: The paper introduces a unique method to dissect the impacts of finetuning on language models by analyzing topic, style, and factual knowledge.",
            "1": "This approach provides a granular understanding of how different aspects of text are affected during finetuning.",
            "2": "**Use of Instruction-Following LLMs**: Leveraging instruction-following LLMs like ChatGPT to generate controlled-variable text examples is innovative.",
            "3": "This method allows for a systematic and scalable way to probe language models without extensive manual data curation.",
            "4": "**Comprehensive Experiments**: The study is thorough in its experimental setup, using multiple models (GPT-2 XL, LLaMA 2 7B, and LLaMA 2 13B) and datasets (PubMed, C4, Pile of Law, Amazon reviews).",
            "5": "This diversity strengthens the validity of the findings.",
            "6": "**Insightful Findings**: The paper provides valuable insights into the dynamics of learning and forgetting in language models.",
            "7": "For instance, it highlights that topic and style biases are learned rapidly and require minimal model capacity, whereas factual knowledge is learned more slowly and requires significant capacity.",
            "8": "**Practical Implications**: The findings have practical implications for improving domain adaptation and addressing challenges in continual learning.",
            "9": "The paper suggests potential strategies, such as masking the loss from the first few tokens or using low-rank adapters to mitigate bias learning.",
            "10": "**Public Availability of Data and Code**: Making the data and code publicly available enhances the reproducibility of the study and allows other researchers to build upon this work.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Approximate Decomposition**: The decomposition of text into topic, style, and factual knowledge is acknowledged as approximate.",
            "13": "This approximation may not fully capture the complexities and interdependencies of these factors, potentially limiting the accuracy of the analysis.",
            "14": "**Quality of Generated Text**: While the use of ChatGPT for generating controlled-variable text is innovative, the quality and adherence to instructions can vary.",
            "15": "Issues such as safety alignment, pre-training bias, and hallucination in LLMs may affect the reliability of the generated data.",
            "16": "**Data Dependency**: The observations and findings are dependent on the specific datasets used (e.g., PubMed, C4).",
            "17": "While the paper attempts to generalize by using multiple domain corpora, the results may still be influenced by the characteristics of these datasets.",
            "18": "**Limited Training Size**: The experiments are conducted with a maximum of 1 million documents, which is relatively small compared to the scale of data typically used in large-scale language model training.",
            "19": "The trends observed may not generalize to much larger training sizes.",
            "20": "**Potential Overemphasis on Bias Learning**: While the paper provides a detailed analysis of bias learning, it may overemphasize its impact relative to other factors.",
            "21": "The practical significance of these biases in real-world applications and their interaction with other aspects of model performance could be explored further.",
            "22": "**Limited Exploration of Mitigation Strategies**: Although the paper suggests potential strategies to mitigate bias learning, these are not explored in depth.",
            "23": "Empirical validation of these strategies would strengthen the practical contributions of the study.",
            "24": "#### Conclusion\n\nOverall, \"Dissecting Learning and Forgetting in Language Model Finetuning\" is a well-executed and insightful study that advances our understanding of the dynamics involved in finetuning language models.",
            "25": "The novel approach, comprehensive experiments, and practical implications make it a valuable contribution to the field.",
            "26": "However, the study's reliance on approximate decomposition, potential quality issues with generated text, and limited exploration of mitigation strategies are areas that could be addressed in future work."
        },
        "6MAbHbYsrO": {
            "0": "* I find the topic of the investigation quite novel.",
            "1": "I believe that the approach taken is original and innovative, in particular building a corpus that allows disentangling style/topic/factual knowledge.",
            "2": "I also like the way LoRa was used to measure the capacity required for learning different facets.",
            "3": "* The authors are sharing the data and code.",
            "4": "* The reported experiments have provided some applicable insights, e.g.",
            "5": "wrt the data mixing.",
            "6": "* Using synthetic data, generated by ChatGPT, might introduce some hidden biases.",
            "7": "It is not given that the same findings could be found if we had natural data.",
            "8": "* It is not clear if the same approach can be generalized to any other characteristics?",
            "9": "Typos:\n* “by just changing the order of decomposition in 1” -> “...in Eq."
        },
        "KX6fDquf1N": {
            "0": "Investigating the changes language models undergo after finetuning continues to be a highly relevant and evolving area of study, despite prior coverage in academic literature.",
            "1": "The research offers key empirical insights into the differential impact of finetuning on language models, revealing a more pronounced effect on style and topic preferences compared to factual knowledge.",
            "2": "These findings enhance our understanding of language model training dynamics and are instrumental in formulating more effective training methodologies.",
            "3": "The researchers conducted extensive experiments on three language models of considerable size, particularly from an academic perspective.",
            "4": "* The assertion that each prediction by a language model can be broken down into components of writing style, topic, and factual knowledge requires further justification or explanation.",
            "5": "The paper should present a stronger argument or provide additional evidence to substantiate this claim.",
            "6": "* The primary message or conclusion of the paper is ambiguous.",
            "7": "The authors need to clarify the central thesis to ensure that readers can grasp the main contribution of the work.",
            "8": "what is the takeaway from this research?",
            "9": "* While the prose is generally lucid, the paper's structure, particularly the introduction, could use refinement to enhance its readability and impact.",
            "10": "For improved clarity and presentation, the following suggestions are offered:\n1.",
            "11": "The introduction would benefit from concrete examples illustrating the key domains of style, topic, and factual knowledge to help contextualize the subsequent findings.",
            "12": "Details regarding the fine-tuning process are scant.",
            "13": "Clarification of which specific models are assessed and the precise nature of the fine-tuning would provide a more robust understanding of the study's scope.",
            "14": "The transition from discussing fine-tuning effects to probing methods in the third paragraph of the introduction is somewhat abrupt.",
            "15": "A smoother segue that connects these topics would aid reader comprehension.",
            "16": "In the fourth paragraph, where style and topic biases are introduced, it would be helpful to include examples or elaborate on what these biases entail to furnish readers with a clearer picture of these concepts."
        },
        "IeQh1AMom9": {
            "0": "- The method how the analysis is performed is novel.",
            "1": "Creating training and evaluation corpora with controlled differences (topics, style, factual knowledge) by prompting instruction following LLMs is interesting and inspiring.",
            "2": "- The analysis is extensive and is performed under various configurations (like the choice of LM, size of the training corpora)\n- The outcomes of analysis are interesting and relevant to future research that study lifelong learning of LMs.",
            "3": "- Although other configurations are very extensive, the choice of training and evaluation corpora and exclusively original or variants of PubMed and C4.",
            "4": "- The three text-generating factors (styles, topics, facts) may not always be clearly separable of extensive enough in every corpora.",
            "5": "The authors discussed this limitation in their limitation section.",
            "6": "- Clarity issue: I feel the plots very hard to read because the captions are too generic and not self-contained.",
            "7": "I suggest to briefly summarize the findings or implications in the captions.",
            "8": "- Clarity issue: some legends in plots such as Figure 4 are not explained in text (e.g.",
            "9": "readers may be confused about \"C4 -factuals\" before they associate them with \"C4-counterfactual\" in Table 1)\n- Though the authors pointed out the hardness of learning factual knowledge without learning style and topic bias, the authors' attempts failed to improve such performance at the end of Sec.",
            "10": "I suggest to provide some future directions about how the analysis will be beneficial the challenge of learning factual knowledge above.",
            "11": "- The authors focused on evaluation of LM loss throughout the paper.",
            "12": "I think this is fine for style and topics, but factual knowledge, evaluating LM loss is not clean enough because only a few tokens in a sentence are related to facts.",
            "13": "The authors could create cloze-style  or question answering evaluation sets that focus exclusively on generation of factual knowledge."
        },
        "tAmKnN5JGR": {
            "0": "- Quality\n\t- The motivation is well-founded and the claims are sound.",
            "1": "- Experimental analysis is very detailed and explanatory.",
            "2": "- Clarity\n\t- Paper is clearly presented and easy to follow.",
            "3": "- Quality\n\t- As the topic of a document can be determined by the factual knowledge it contains then it might be redundant to keep the topic as a relevant factor in the text generation process and only style and factual knowledge might suffice which then could directly align with the syntax and semantics of the underlying text respectively.",
            "4": "- Significance\n\t- This paper presents a detailed technical analysis of the fine-tuning process of a language model on domain-specific downstream tasks/datasets.",
            "5": "However, the outcomes of the study conform with the expected outcomes of fine-tuning a model on domain-specific data and hence this paper misses to provide any significant gainful insight into the fine-tuning process due to the following reasons:\n\t\t- In the PubMed dataset, as academic style is present across all abstracts with different factual knowledge, it is expected that the model will readily adapt to the academic style first before capturing the diverse type of factual knowledge.",
            "6": "- Just like in topic modeling, the topic of a document is a broad sentiment and can be easily determined using a set of keywords.",
            "7": "So, it will be easy for the model to detect/understand the topic of a document before reading the whole document and capturing the factual knowledge inside it.",
            "8": "Therefore, it is expected for the model to easily understand the topic and style factors before capturing the factual knowledge inside it.",
            "9": "- I am keen to hear the response of the authors on this and hope that they can change my point of view."
        }
    },
    "pszewhybU9": {
        "llm_review": {
            "0": "**Novelty and Innovation**: The paper introduces a novel method, INSTAG, for tagging instructions in supervised fine-tuning (SFT) datasets.",
            "1": "This method leverages the capabilities of advanced chatbots like ChatGPT to generate fine-grained tags, which is a significant step forward in understanding and quantifying the diversity and complexity of SFT data.",
            "2": "**Comprehensive Analysis**: The authors provide a thorough analysis of the diversity and complexity of various open-source SFT datasets using the tags generated by INSTAG.",
            "3": "This analysis is insightful and helps in understanding the characteristics of different datasets, which is crucial for improving the alignment of large language models (LLMs).",
            "4": "**Effective Data Selection**: The paper proposes a data sampling procedure based on the tags generated by INSTAG.",
            "5": "The resulting models, TAGLM, demonstrate superior performance compared to other open-source models, highlighting the effectiveness of the proposed method in selecting high-quality SFT data.",
            "6": "**Robust Evaluation**: The authors employ both human and GPT-4 annotations to evaluate the quality of the tags generated by INSTAG.",
            "7": "The use of multiple evaluation metrics, including precision and consistency, adds robustness to the evaluation process.",
            "8": "**Potential for Extension**: The paper discusses the potential of INSTAG to be extended to other applications beyond data selection, such as creating comprehensive evaluations and tag-based self-instruct.",
            "9": "This indicates the broader applicability of the proposed method.",
            "10": "**Detailed Experimental Setup**: The experimental setup is well-documented, including the data pool, configuration, and baselines.",
            "11": "This transparency allows for reproducibility and provides a clear understanding of how the experiments were conducted.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Dependence on Proprietary Models**: INSTAG relies heavily on the capabilities of advanced proprietary chatbots like ChatGPT for generating tags.",
            "14": "This dependence may limit the accessibility and scalability of the method, especially for researchers or organizations without access to such models.",
            "15": "**Tagging Quality Variability**: While the paper reports high precision and consistency in tagging, there is still some variability in the quality of tags generated by ChatGPT.",
            "16": "The authors acknowledge this by providing standard deviations in their results, but it remains a potential limitation.",
            "17": "**Complexity and Diversity Metrics**: The metrics used to quantify complexity and diversity, such as the average number of tags and tag coverage rate, are somewhat simplistic.",
            "18": "More sophisticated metrics could provide a deeper understanding of these attributes.",
            "19": "**Limited Scope of Evaluation**: The evaluation primarily focuses on MT-Bench and AlpacaEval.",
            "20": "While these are relevant benchmarks, additional evaluations on other benchmarks or real-world tasks could provide a more comprehensive assessment of the model's performance.",
            "21": "**Scalability of Tagging Process**: The process of generating and normalizing tags, although automated, may still be computationally intensive and time-consuming, especially for very large datasets.",
            "22": "This could be a bottleneck in scaling the method to even larger datasets.",
            "23": "**Generalization to Other Domains**: The paper primarily focuses on SFT datasets for LLMs.",
            "24": "It would be beneficial to see how well the proposed method generalizes to other domains or types of datasets, such as those used for different machine learning tasks.",
            "25": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the analysis and selection of SFT data for LLMs through the introduction of INSTAG.",
            "26": "The method's ability to generate fine-grained tags and its application in selecting high-quality data subsets are noteworthy contributions.",
            "27": "However, the reliance on proprietary models and the potential limitations in scalability and generalization warrant further investigation.",
            "28": "Future work could focus on addressing these limitations and exploring the broader applicability of the proposed method."
        },
        "ZxWsaXyKKX": {
            "0": "- The tagging method is interesting, and well-validated both as an analysis and as a selection method.",
            "1": "The use of ChatGPT as a tagger, and the rules used to simplify the generated tags, are novel and effective.",
            "2": "- The data selection method appears well-validated, and achieving strong results with only 6,000 examples is impressive.",
            "3": "The method outperforms a random selection baseline and other popular open models such as Vicuna and WizardLM.",
            "4": "- The analyses of performance against complexity, diversity, and dataset size are useful, providing useful guidelines for future researchers in data collection and selection.",
            "5": "- The method’s reliance on a strong tagging model (in this case, ChatGPT) is not explored, despite a very brief mention of training a distilled tagger model at the end of the work.",
            "6": "It would be interesting and useful to see how well this method works with openly available models, or over a variety of different quality models (e.g.",
            "7": "how does using GPT-4 as a tagger compare?",
            "8": "How about Vicuna?",
            "9": "etc.).",
            "10": "- The tag-based analysis is somewhat restrictive, as discussed in section 4.3.",
            "11": "It would be interesting to take the semantics of the tags themselves into account somehow, since (a) some tags may be closer to each other and so overlap in terms of diversity, and (b) certain tags may express queries that are naturally more complex than others (e.g.",
            "12": "‘solve’ vs ‘inquiry’ tags).",
            "13": "- MT-bench evaluation involves a relatively small number of questions (80), which may be easier to cover with 6,000 examples.",
            "14": "I wonder how well the TagLM model would perform with more questions (e.g.",
            "15": "the alpacaEval setting, which has 800 examples), or on more traditional benchmarks such as MMLU, Big Bench, etc.",
            "16": "In general, it would be interesting to see if the selection method is biased towards certain capabilities compared to others.",
            "17": "Overall, I think this work is strong, and my main qualm is that the evaluation of the data selection method is somewhat weak, only exploring one evaluation setting.",
            "18": "However, the method proposed is interesting and novel, and produces useful insights for data selection in instruction tuning."
        },
        "wxgOxtE7Lk": {
            "0": "- The proposed method is straightforward, intuitive, and simple to implement.",
            "1": "- The study demonstrates that metrics driven by the statistical analysis of automatically generated tags—specifically complexity and diversity—can effectively probe the characteristics of IFT datasets.",
            "2": "- This work provides further empirical evidence that the quality, rather than the quantity, of IFT datasets is crucial for aligning language models successfully.",
            "3": "- The primary mechanism of the proposed method is dependent on the automated, open-ended tagging capability of GPT-4; therefore, there is a risk that the analysis in this paper might be influenced by any inherent biases present within GPT-4.",
            "4": "- Further examination of data instances categorized by the proposed metrics would be advantageous.",
            "5": "Specifically, exploring the semantic or syntactic traits defining IFT datasets identified as diverse and complex by these metrics would be informative.",
            "6": "- The presented work is largely empirical, which may raise concerns within the community regarding the foundational grounding of its results."
        },
        "15E4wpMZSn": {
            "0": "This paper proposes an automatic method to evaluate the diversity and complexity of human instructions.",
            "1": "Based on the proposed InsTag, this paper further presents a method of selecting human instructions, which can potentially reduce the cost of the alignment phase of large language models.",
            "2": "Selecting more diverse and complex samples is an existing idea, and this article is more similar to the implementation and application of this idea.",
            "3": "The definition of complexity seems a little strange.",
            "4": "Is there any explanation from other papers?",
            "5": "If not, I hope the authors can give a more detailed explanation for it.",
            "6": "The author is recommended to verify the performance of the proposed method on more benchmarks."
        },
        "JWoXQgCtoG": {
            "0": "The proposed data measurement and data selection methods are simple, both concept-wise and practice-wise.",
            "1": "The experimental results are strong – TagLM is the only model that achieves such high performance on MT-bench with <10K data examples as far as I know.",
            "2": "The proposed approach can be utilized to measure existing datasets, as shown in Figure 2 which are interesting.",
            "3": "Table 4 indicates that more sft data does not necessarily give better performance and InsTag is able to select the effective ones.",
            "4": "Evaluation is a bit weak, MT-Bench scores are the only metric used across entire paper – other datasets such as AlpacaEval and human evaluation could further strengthen the claims in the paper.",
            "5": "While I appreciate that the authors distill a local tagger in Section 5, it is unknown how much SFT performance would be sacrificed by using the tags from this local tagger.",
            "6": "The strong results on Table 3 is from ChatGPT tagger if I understand correctly, and it may be too expensive to use ChatGPT in practice when we have a large data pool to measure."
        }
    },
    "LWqeF78c1q": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper presents a novel method for probing multimodal video transformers by leveraging neuroscientific evidence.",
            "1": "This interdisciplinary approach bridges the gap between artificial intelligence and neuroscience, providing a fresh perspective on model evaluation.",
            "2": "**Comprehensive Analysis**: The authors conduct a thorough analysis of the alignment between model representations and brain activity.",
            "3": "They use brain recordings from participants watching a TV show, which adds a real-world dimension to their study.",
            "4": "**Detailed Methodology**: The paper provides a detailed description of the methods used, including the model representations, brain recordings, and the alignment process.",
            "5": "This transparency allows for reproducibility and further exploration by other researchers.",
            "6": "**Significant Findings**: The study finds that vision enhances masked prediction performance during language processing, suggesting that cross-modal representations can benefit individual modalities.",
            "7": "This is a valuable insight for the development of more effective multimodal models.",
            "8": "**Fine-Tuning Insights**: The paper shows that fine-tuning the pre-trained model on a vision-language inference task improves brain alignment.",
            "9": "This finding is crucial for future work on improving the brain relevance of multimodal models.",
            "10": "**Interdisciplinary Impact**: The work has implications for both machine learning and neuroscience.",
            "11": "It provides evidence that multimodal models can partially align with brain activity and suggests ways to improve this alignment.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Limited Scope**: The study focuses on one type of multimodal video model and uses a single brain dataset.",
            "14": "This limits the generalizability of the findings.",
            "15": "Future work should include a broader range of models and datasets to validate the results.",
            "16": "**Unimodal Bias**: The TVQA dataset used for fine-tuning may have unimodal biases, which could affect the results.",
            "17": "The authors acknowledge this limitation but do not provide a solution.",
            "18": "Future studies should consider using more balanced multimodal datasets.",
            "19": "**Complexity of Multimodal Interactions**: The paper finds no evidence of brain-relevant multimodal interactions in the pre-trained model.",
            "20": "While this is an important finding, the authors do not explore alternative methods for capturing these interactions.",
            "21": "Further research is needed to address this gap.",
            "22": "**Task-Specific Representations**: The study shows that task-specific representations in the top layers of the fine-tuned model do not align well with brain activity during passive viewing.",
            "23": "This suggests a disconnect between task performance and brain relevance, which needs further investigation.",
            "24": "**Residual Analysis**: The residual method used to investigate the contribution of masked language prediction is insightful but may not capture all aspects of cross-modal interactions.",
            "25": "More sophisticated techniques could provide a deeper understanding of these interactions.",
            "26": "**Limited Brain Regions**: The study focuses on a few brain regions, such as the angular gyrus, for analyzing multimodal integration.",
            "27": "A more comprehensive analysis of additional brain regions could provide a fuller picture of how multimodal information is processed in the brain.",
            "28": "#### Conclusion:\n\nOverall, the paper presents a promising approach for evaluating multimodal video transformers using neuroscientific evidence.",
            "29": "It provides valuable insights into the alignment of model representations with brain activity and suggests ways to improve this alignment through fine-tuning.",
            "30": "However, the study is limited by its focus on a single model and dataset, and further research is needed to generalize the findings and explore alternative methods for capturing multimodal interactions.",
            "31": "Despite these limitations, the paper makes a significant contribution to the fields of machine learning and neuroscience, and it opens up new avenues for future research."
        },
        "nokzsylf4t": {
            "0": "The paper has some interesting novel findings such as - a)no evidence of brain-relevant multimodal interactions encoded in the pre-trained model.",
            "1": "b) Early and middle layers of the pre-trained and fine-tuned model are similarly brain aligned c)Task-dependent changes in top layers are not aligned with brain representations.",
            "2": "d) Fine-tuning for vision-language question-answering improves brain alignment in some regions.",
            "3": "The paper suggest that while multi-modal transformers show promise in integrating vision and language, there's room for enhancement in aligning them with brain processes.",
            "4": "But it is not clear what might be the potential approaches for better alignment\n2.",
            "5": "The experimentation settings lacks some clarity.",
            "6": "For example, did all the participants watching the TV show actually seeing it first time?",
            "7": "Or some of them might have seen this show before?",
            "8": "Have they heard about the name of the show/synopsis of the show before?",
            "9": "The flow of the paper is hard to follow, the writing could be more concise\n4.",
            "10": "The paper has some interesting novel findings on brain alignment but it is not clear how findings like these are actually impactful"
        },
        "NBefQS6WW5": {
            "0": "The authors use state-of-the-art transformer models\n\nThe authors ask interesting questions about the potential for shared representation of vision and language information\n\nThe highly uncontrolled nature of the stimuli (TV shows) makes the interpretation complex but is also interesting in bringing the questions to real-world relevance.",
            "1": "I could not find any evidence in the paper that the fMRI signals provide either visual or language information during the task.",
            "2": "The first figure after the initial definitions goes on to show correlations between the neural network representations and the fMRI signals but there is no indication of what those fMRI signals are actually representing.",
            "3": "The overall results are extremely weak.",
            "4": "In the best case scenarios the Pearson correlations are about 0.1 and in most cases, they hover between 0.01 and 0.05.",
            "5": "The fraction of explained variance is the square of the Pearson correlation coefficient.",
            "6": "With a correlation of 0.1, that means that the neural network, in the best-case scenario, can explain around 1% of the fMRI signals.",
            "7": "To make matters worse, the conclusions that the authors are interested in drawing are based on comparisons between different conditions.",
            "8": "Take for instance, the first two columns in the second panel in fig.",
            "9": "2 (pCingulate).",
            "10": "Vision+audio yields a correlation of about 0.03 (that is, about 0.0009 of variance explained), whereas only audio yields a correlation of 0.02 (that is, 0.0004 of variance), and vision yields a slightly negative (!)",
            "11": "correlation.",
            "12": "Conclusions are drawn based on a difference of 5x1^(-4) in variance!"
        },
        "0AGfeQZ2Dh": {
            "0": "Utilizing the brain as a benchmark for multi-modal integration provides an intriguing perspective, especially for the machine learning community.",
            "1": "Over the past decade in neuroscience, spurred by seminal works like that of Yamins et al.",
            "2": "(2014), there has been a surge in efforts to correlate deep neural networks with brain activity, yielding significant insights for neuroscience.",
            "3": "In this study, the authors ingeniously invert this approach, seeking to extract insights from neuroscience to better comprehend expansive machine learning models.",
            "4": "This innovative approach is not only captivating but also holds promise if appropriately substantiated.",
            "5": "While the premise of the study is intriguing, it could benefit from further refinement.",
            "6": "A primary concern is the treatment of multi-modal integration in the human brain as a static rather than a dynamic process[2].",
            "7": "Human sensory processing occurs in sequences — visual information traverses from the retina through the thalamus to the visual cortex before reaching the prefrontal cortex.",
            "8": "Audio processing follows a distinct timeline, and semantic systems kick in later.",
            "9": "Additionally, fMRI signals exhibit a delay spanning several seconds.",
            "10": "In contrast, vision-language models lack these temporal delay characteristics.",
            "11": "The study's definition and measurement of alignment seem to bypass these temporal nuances, making it a potential oversight.",
            "12": "Furthermore, the research could have delved deeper into cross-comparisons of different fMRI datasets and vision-language models.",
            "13": "It's imperative to discern whether findings remain consistent across various model choices.",
            "14": "If one model aligns more closely with the brain than another, what implications arise from this?",
            "15": "Does a higher alignment score for Model A over Model B necessarily denote its superiority?",
            "16": "And if so, by what margin?",
            "17": "An intriguing proposition would be to investigate the effects of fine-tuning a model based on its brain alignment score.",
            "18": "What outcomes would this entail, and what do these outcomes signify?",
            "19": "Lastly, the authors' claim about audio-visual information being processed in the Angular Gyrus — a prominent language region — isn't groundbreaking.",
            "20": "This observation is already documented in the neuroscience literature [1,2,3,4].",
            "21": "[1] Thakral, Preston P., Kevin P. Madore, and Daniel L. Schacter.",
            "22": "\"A role for the left angular gyrus in episodic simulation and memory.\"",
            "23": "Journal of Neuroscience 37.34 (2017): 8142-8149.",
            "24": "[2] Chambers, Christopher D., et al.",
            "25": "\"Fast and slow parietal pathways mediate spatial attention.\"",
            "26": "Nature neuroscience 7.3 (2004): 217-218.",
            "27": "[3] Fang, Mengting, et al.",
            "28": "\"Angular gyrus responses show joint statistical dependence with brain regions selective for different categories.\"",
            "29": "Journal of Neuroscience 43.15 (2023): 2756-2766.",
            "30": "[4] Bonnici, Heidi M., et al.",
            "31": "\"Multimodal feature integration in the angular gyrus during episodic and semantic retrieval.\"",
            "32": "Journal of Neuroscience 36.20 (2016): 5462-5471."
        },
        "wjUPrb8BI4": {
            "0": "The topic and study are interesting.",
            "1": "It is interesting to study to what extent the current large models align with human brain.",
            "2": "This paper has also shown some interesting findings.",
            "3": "For example, vision modality significantly improves brain alignment over language regions, etc.",
            "4": "I would need to say sorry, this paper is out of my domain.",
            "5": "I am not able to provide a valid assessment, and I am unable to fully understand this paper.",
            "6": "There are so many terms that I don't understand without background.",
            "7": "For example, \"repetition time\", \"voxel\", \"vision-language integration\", etc.",
            "8": "In addition, I think the writing is unclear and unconcise to me, which might be one of the reasons that make me not able to understand this paper.",
            "9": "There are many very long sentences.",
            "10": "For example, \"This improvement cannot be due to the further processing of language-specific information in the joint encoder since the depth of language input processing is the same in both conditions and is unlikely to be due to vision-only information since these regions are known to support language processing.\"",
            "11": "These make it very hard to read and understand the whole paper in additional to unknown terms.",
            "12": "If that is not my problem, then I think the paper writing needs to be improved.",
            "13": "I do have one suggestion: Section 3.1 \"Model Representations\" should be named as \"Transformer Representations\" or \"Artificial Neural Network Representations\".",
            "14": "As this paper is across domains, it is better to clearly name each term for that specific domain.",
            "15": "\"Model\" can represent many things.",
            "16": "But here I think you are referring to the transformer."
        }
    },
    "hUs8YHAUEr": {
        "llm_review": {
            "0": "**Novelty and Innovation**:\n   - The paper introduces a novel decoding strategy, Multi-Perspective Self-Consistency (MPSC), which leverages both inter-consistency across multiple perspectives and intra-consistency within a single perspective.",
            "1": "This approach is innovative and addresses the limitations of previous methods that only consider intra-consistency.",
            "2": "**Comprehensive Evaluation**:\n   - The authors conduct extensive experiments on four widely used code generation benchmarks: HumanEval, HumanEval+, MBPP, and CodeContests.",
            "3": "The results demonstrate significant improvements in performance, particularly in Pass@1, which is crucial for real-world applications.",
            "4": "**Model-Agnostic Framework**:\n   - MPSC is designed to be model-agnostic, meaning it can be applied to various large language models (LLMs) without modification.",
            "5": "This flexibility is a significant advantage, as it allows the framework to be used with different LLMs, including ChatGPT, GPT-4, Code Llama, and WizardCoder.",
            "6": "**Detailed Analysis**:\n   - The paper provides a thorough analysis of the impact of different perspectives, the effect of edge sparsity, and the influence of sampling numbers on the performance of MPSC.",
            "7": "This detailed analysis helps in understanding the robustness and efficiency of the proposed framework.",
            "8": "**Clear and Structured Presentation**:\n   - The paper is well-organized, with a clear structure that guides the reader through the introduction, methodology, experiments, and results.",
            "9": "The use of figures and tables to illustrate the differences between previous works and the proposed framework, as well as the performance improvements, is effective.",
            "10": "#### Weaknesses:\n\n1.",
            "11": "**Complexity of Implementation**:\n   - While the MPSC framework is innovative, its implementation involves constructing multipartite graphs and defining various consistency measures.",
            "12": "This complexity might pose challenges for practitioners who wish to adopt the framework without a deep understanding of graph theory and optimization techniques.",
            "13": "**Dependence on Quality of Perspectives**:\n   - The effectiveness of MPSC heavily relies on the quality and accuracy of the perspectives (solution, specification, and test case).",
            "14": "Generating high-quality specifications, in particular, can be challenging and may require significant manual effort or sophisticated automated tools.",
            "15": "**Limited Scope of Application**:\n   - Although the paper claims that MPSC can be applied to other textual generation tasks, the current evaluation is limited to code generation.",
            "16": "Applying MPSC to other tasks like math problem solving and question answering may require additional research to define appropriate perspectives and consistency measures.",
            "17": "**Scalability Concerns**:\n   - The framework involves generating multiple outputs from different perspectives and constructing a graph based on these outputs.",
            "18": "This process can be computationally expensive, especially for large-scale applications.",
            "19": "The paper does not provide a detailed analysis of the computational overhead and scalability of the framework.",
            "20": "**Evaluation Metrics**:\n   - The primary evaluation metric used in the paper is Pass@k, which measures the probability that at least one out of k solutions passes unit tests.",
            "21": "While this metric is relevant for code generation, it may not fully capture the quality and correctness of the generated code.",
            "22": "Additional metrics, such as code readability and maintainability, could provide a more comprehensive evaluation.",
            "23": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of code generation using large language models.",
            "24": "The Multi-Perspective Self-Consistency (MPSC) framework is a novel and effective approach that leverages both inter- and intra-consistency to improve the accuracy of generated code.",
            "25": "The comprehensive evaluation and detailed analysis demonstrate the potential of MPSC to enhance the performance of various LLMs.",
            "26": "However, the complexity of implementation, dependence on the quality of perspectives, and scalability concerns are notable challenges that need to be addressed in future work.",
            "27": "Additionally, extending the application of MPSC to other textual generation tasks and exploring additional evaluation metrics would further strengthen the contributions of this research."
        },
        "Pev718ZXIq": {
            "0": "- The authors did a thorough exploration on what factors could potentially affect code generation performances in LLMs, and it's interesting to see that all three perspectives, solution, specification, and test cases all play important roles in improving the final performance.",
            "1": "- The experiments are done on a fairly comprehensive set of code generation models, including GPT, Code Llama and WizardCoder.",
            "2": "- For intra-consistency, the authors also did a comprehensive study over what kind of measure functions can help the most.",
            "3": "- Novelty: [1] already proposed to construct a graph and explore more fine-grained consistency via MAX-SAT solving (although for natural language based reasoning tasks).",
            "4": "The similarities and differences should be better discussed in the current paper.",
            "5": "[1] Jung et al.",
            "6": "Maieutic prompting: Logically consistent reasoning with recursive explanations.",
            "7": "EMNLP 2022.",
            "8": "- Generalizability: although framed as \"multi-perspectives\", this current paper only explores a single use-case of code generation, and with very specific perspectives: solution, specification, and test cases.",
            "9": "It would be interesting to show if this method can be generalized to other tasks (e.g., math tasks, commonsense reasoning, or symbolic reasoning).",
            "10": "- Added complexity and ad-hoc design choices: The current framing adds a lot of complexity to the existing baselines, and many of the design choices are not well justified.",
            "11": "In practice it would be difficult to deploy such a method to ensure optimal performance.",
            "12": "E.g., \n1) designing the perspectives: for each task, how much manual effort is needed to design those perspectives?",
            "13": "(and writing the prompts for each perspective?)",
            "14": "How sensitive is the final performance w.r.t.",
            "15": "the prompts written for each perspective?",
            "16": "2) constructing the graph: each edge needs to be specifically designed (section 3.2), why those choices and how much difference does it make if designed differently?",
            "17": "For intra-consistency, similarly the authors designed many different measures, and based on the experiment results the best measure varies depending on the task.",
            "18": "How would one pick which measure to use in practice?",
            "19": "3) solving the optimization: depending on the number of nodes and edges, the solving part could be very expensive; even with the iterative algorithm, it might take many rounds to reach convergence.",
            "20": "This point needs to be better discussed.",
            "21": "4) choice of parameters: what is the value of \\alpha in experiments?",
            "22": "From Figure 4, on human-eval the performance varies a lot depending on \\alpha.",
            "23": "The final reported performance seems to be the \\alpha that achieves the best performance.",
            "24": "But without a training/dev set, how could one pick \\alpha in an unsupervised way?",
            "25": "- Fair evaluation: Table 2 shows the proposed method yields some gains over several baselines.",
            "26": "But digging deeper, a fair comparison should be made between methods that use *the same number of generated samples*.",
            "27": "MPSC uses many more samples (200 solutions, 100 specifications, 500 test cases) while most of the baselines only use solutions (200 samples only).",
            "28": "In addition, MPSC-label is not a fair comparison given it uses human labels.",
            "29": "1) if given the same number of samples, i.e., baselines with 800 samples vs MPSC, how does the performance compare?",
            "30": "2) what is the variance of the proposed method, given that more samples from different perspectives are drawn?",
            "31": "3) how does the proposed method compare with [2]?",
            "32": "[2] also uses test cases to improve language models' code generation.",
            "33": "4) Table 4 shows the test cases give the most gains, so maybe a simple baseline could be added: use the generated test cases to filter out incorrect generated solutions, and then apply self-consistency on the filtered set.",
            "34": "[2] Chen et al.",
            "35": "Teaching Large Language Models to Self-Debug.",
            "36": "2023."
        },
        "yIlPVyh9AM": {
            "0": "This work investigates both intra-consistency and inter-consistency of LLMs\n2.",
            "1": "The proposed MPSC achieves significant improvement on code generation\n3.",
            "2": "The proposed MPSC can act like a plug-in to enhance other LLMs 1.",
            "3": "The MPSC is complicate and unstable which limits the reproducibility\n2.",
            "4": "The cost of MPSC is very high which limits its application\n3.",
            "5": "The narrative of the paper is not clear"
        },
        "daZ9VTBkql": {
            "0": "The improvement over the baselines is significant.",
            "1": "However, it's unclear whether this improvement is due to the allowance of code execution.",
            "2": "The paper is easy to read.",
            "3": "The method proposed in this paper can be applied to both closed-source large models (like ChatGPT) and open-source large models.",
            "4": "A key issue with the methodology and experiments of this paper is that the proposed method actually requires the execution of code, rather than merely generating it.",
            "5": "For instance, in Listing 3 of Appendix 3, one needs to execute the solution to complete the validation with test cases.",
            "6": "The authors need to emphasize this distinction from other methods.",
            "7": "Clearly, when code execution is allowed, the task becomes easier.",
            "8": "This issue makes the main experimental results of the paper, as shown in Table 2, unfair, as other methods do not require code execution.",
            "9": "Can the method proposed in this paper be applied under conditions where code execution is not allowed?",
            "10": "If so, what are the results?",
            "11": "The authors have not demonstrated this.",
            "12": "Is the learning approach designed in this paper overly complicated?",
            "13": "Is it possible to avoid using w(vi,vj) and f(vi) and directly employ an MLP or other ensemble methods to obtain the answer?",
            "14": "For instance, self-consistency actually uses max-vote directly.",
            "15": "Overly complex optimization algorithms make the methodological contributions of this paper ambiguous.",
            "16": "The specific \"perspectives\" used in this paper do not entirely align with intuition.",
            "17": "For example, in Figure 2, the paper views the solution, specification, and test case as three perspectives, whereas conceptually, they are three answer components."
        },
        "h2IKpFA0Y6": {
            "0": "MPSC is a natural extension of self-consistency for code generation, where the consistency among the solution, test cases and specifications can be precisely verified by code execution.",
            "1": "The experiments show remarkable performance improvement compared to strong baselines that utilize multiple samples and/or test cases.",
            "2": "The main weaknesses of this work are: (1) the implementation details are unclear; and (2) some ablation studies are missing.",
            "3": "Specifically, I have the following questions:\n\n1.",
            "4": "How is MBR-Exec implemented?",
            "5": "I do not understand why MBR-Exec can perform worse than self-consistency.",
            "6": "To my understanding, self-consistency selects the final program based on the exact match; i.e., selecting the most frequently appeared code in all samples.",
            "7": "On the other hand, MBR-Exec selects programs based on the frequency of execution results.",
            "8": "Does MBR-Exec utilize the given test cases as in the original paper?",
            "9": "For MPSC-Label, how are the golden test cases utilized?",
            "10": "Do you directly filter out those programs that do not pass the test cases?",
            "11": "In general I do not understand why MPSC-Weighted Cardinality can sometimes outperform MPSC-Label.",
            "12": "It is interesting to see that GPT-3.5 with MPSC can outperform GPT-4.",
            "13": "However, sampling 200 solutions is still very expensive.",
            "14": "Do you have results with fewer number of samples, e.g., 10 or 100?",
            "15": "What is pass@200, which should be the upper bound of the performance?",
            "16": "It is helpful to add discussion on the quality of generated test cases and specifications.",
            "17": "For example, what are the true positive and false negative rates?",
            "18": "Also, I think MPSC is well-suited for code generation, but how to extend it to other domains remains unclear."
        },
        "5ADLi1fzbC": {
            "0": "- The multi-perspective method is well-motivated and well-suited for tasks like code generation.",
            "1": "- The authors conduct comprehensive evaluation and show significant performance improvement over various baselines.",
            "2": "- The paper is well-written.",
            "3": "- The main limitaiton of the work is the useability on a broader range of tasks.",
            "4": "Despite MPSC is claimed to be task-agnostic, only code generation was presented in the paper, which greatly limits the impact of this work.",
            "5": "On one hand, the authors only study code competition task, and it is unknown whether the framework can work in code generation in the wild.",
            "6": "On the other hand, the authors should consider including at least one more NL task to demonstrate the extensibility of the framework.",
            "7": "- It is unclear whether and how well the framework can generalize towards more perspectives.",
            "8": "In code generation, there are only three perspectives, which is quite limited.",
            "9": "It would be great to think about and demonstrate that MPSC can work with arbitary number of perspectives.",
            "10": "- The perspectives are manually curated now, which can be a limitation for tasks with vague perspective definitions.",
            "11": "It would be great to discuss whether manual curation of perspectives is required and if not how that would impact the end performance."
        }
    },
    "RadQVWAucN": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel framework, LLM-REC, which leverages large language models (LLMs) to augment input text for personalized recommendations.",
            "1": "This approach is innovative and taps into the recent advancements in LLMs, showcasing their potential beyond traditional natural language processing tasks.",
            "2": "**Comprehensive Prompting Strategies**: The study explores four distinct prompting strategies—basic prompting, recommendation-driven prompting, engagement-guided prompting, and a combination of recommendation-driven and engagement-guided prompting.",
            "3": "This comprehensive exploration provides a thorough understanding of how different prompting techniques can enhance recommendation performance.",
            "4": "**Empirical Validation**: The paper presents extensive empirical experiments on two diverse datasets (Movielens-1M and Recipe), demonstrating the effectiveness of the proposed framework.",
            "5": "The results show significant improvements in recommendation performance, highlighting the practical applicability of the approach.",
            "6": "**Comparison with Baselines**: The study compares the performance of LLM-REC with several baseline models, including advanced feature-based recommendation methods.",
            "7": "The results indicate that LLM-REC can achieve comparable or superior performance, even with simpler models like MLPs, which is a notable achievement.",
            "8": "**Explainability and Transparency**: The paper emphasizes the increased transparency and explainability of recommendations when using augmented text.",
            "9": "This is a valuable contribution, as it helps users and system designers understand the rationale behind the recommendations.",
            "10": "**Domain-Agnostic Nature**: The framework does not require domain-specific knowledge, making it versatile and applicable across various domains and item types.",
            "11": "This is a significant advantage over traditional methods that often rely on domain expertise.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Dataset Variety**: While the paper evaluates the framework on two datasets, the diversity of these datasets is somewhat limited.",
            "14": "Including more datasets from different domains (e.g., e-commerce, music, news) could provide a more comprehensive validation of the framework's generalizability.",
            "15": "**Neighbor Sampling Method**: The engagement-guided prompting strategy relies on the descriptions of important neighbor items, but the paper does not delve deeply into the neighbor sampling method.",
            "16": "A more detailed explanation and analysis of this method would strengthen the study.",
            "17": "**Potential Overfitting**: The paper does not address the potential risk of overfitting when using LLMs for text augmentation.",
            "18": "Given the complexity and size of LLMs, there is a possibility that the model might overfit to the training data, especially with smaller datasets.",
            "19": "**Scalability Concerns**: The computational cost and scalability of using LLMs for real-time recommendation systems are not discussed.",
            "20": "LLMs are resource-intensive, and their deployment in large-scale systems could pose practical challenges.",
            "21": "**Qualitative Analysis**: While the paper provides quantitative results, a more in-depth qualitative analysis of the augmented text and its impact on recommendations would be beneficial.",
            "22": "Examples of how the augmented text improves user satisfaction or aligns with user preferences could provide more insights.",
            "23": "**User Study**: The paper lacks a user study to validate the practical impact of the recommendations on end-users.",
            "24": "Conducting a user study to gather feedback on the quality and relevance of the recommendations would add significant value to the research.",
            "25": "#### Conclusion\n\nOverall, the paper presents a promising approach to enhancing personalized recommendations using LLMs.",
            "26": "The innovative use of prompting strategies to augment input text is well-executed and empirically validated.",
            "27": "However, addressing the identified weaknesses, such as expanding dataset variety, providing more details on the neighbor sampling method, and conducting a user study, would further strengthen the study and its contributions to the field."
        },
        "hj8ybdu8yG": {
            "0": "* Although the idea of using LLM-augmented item description as textual feature for enhancing existing recommender models is in gist similar to the KAR approach, the proposed four prompt templates and the way of using the augmented textual feature is mildly novel in the context of LLM for recommendation.",
            "1": "* The design of prompt templates as well as the combination of different prompts are sound and justified.",
            "2": "* The empirical study supports the effectiveness claim of the proposed approach.",
            "3": "* The limitation discussion of the proposed approach seems to be missing from the paper.",
            "4": "Inherited from LLMs, the proposed approach might have issues like computation efficiency, lacking awareness of latest event and the cost of keeping the model up-to-date, hallucination in general.",
            "5": "Discussing these limitations would provide more practical insights about the proposed approach.",
            "6": "* The proposed approach applied on movie recommendation and recipe recommendation, explore its capability in e-commerce (Like in the KAR paper) or other scenario would further strengthen the generality claim of the paper."
        },
        "jVnOF98Ty5": {
            "0": "A nice part of the paper is the comprehensive evaluations of the 4 prompting techniques.",
            "1": "There are some valuable insights obtained from the evaluations.",
            "2": "However, I do not see too much novelty from the paper beyond the evaluations.",
            "3": "The paper specifically targets personalized recommendation evident from the title.",
            "4": "I suppose the engagement-guided and recommendation+engagement mixed prompting can achieve some degree of personalization with the addition of important neighbors in the prompting.",
            "5": "However, to achieve true personalization, identifying important neighbors is itself a critical problem.",
            "6": "For example, what if there are many neighbors, how do you choose most important neighbors?",
            "7": "Do you take raw engagement counts into account?",
            "8": "Do you value more recent engagements?",
            "9": "The paper does not have not many details on the topics at all.",
            "10": "Without these technical details, the paper becomes just empirical evaluating of several prompting rules."
        },
        "bW73GOmzc1": {
            "0": "Demonstrates the potential of large language models for data augmentation in Recommender Systems (RecSys).",
            "1": "Introduces four prompting methods to extract information from Large Language Models (LLMs).",
            "2": "Inadequate experimental validation of the proposed methods.",
            "3": "Limited applicability to datasets lacking textual information.",
            "4": "Fails to make a clear contribution to the field of RecSys."
        },
        "Ye7rAISHTn": {
            "0": "LLM-Rec investigates diverse prompting strategies to enhance personalized text-based recommendations.",
            "1": "The prompting strategies of recommendation-driven and engagement-guided exhibit the capability to tap into the language model's comprehension of both general and personalized item characteristics.",
            "2": "Empirical experiments show discernible improvements in recommendation performance by incorporating augmented input text generated by LLMs.",
            "3": "Provides insights into the potential of leveraging LLMs for personalized recommendation.",
            "4": "Further experiments explore the potential of concatenating descriptions from multiple prompting strategies, enabling better performance.",
            "5": "The overall presentation is clear.",
            "6": "Some of the mistakes make it confused.",
            "7": "(1) In Table 1, there is a wrong bold in the second column.",
            "8": "(2) “Table 3”in the 7th page is typed as “Figure 3” by mistakes.",
            "9": "(3) The citation at the end of the 5th page is not in the right format.",
            "10": "The evaluation of the prompting strategies could benefit from increased interpretability.",
            "11": "For instance, there is no experiment validating whether engagement-guided prompting effectively utilizes neighbor information to guide the LLM.",
            "12": "It may seem counterintuitive to assert that important neighbor information is considered merely by including item names without a more detailed description and categorization.",
            "13": "The paper lacks a detailed discussion on the potential limitations and challenges of using LLMs for personalized recommendation, such as computational complexity, model interpretability, and potential biases in the generated prompts.",
            "14": "In the main experiment, Table 1 indicates that most of the time, combining both recommendation-driven and engagement-guided strategies leads to a decrease in performance, which appears to contradict the findings in Figure 6 and raises doubts about the generalization of these strategies.",
            "15": "The contribution of this work can be further enhanced if it provides more explainability instead of focus more on improving performance with additional features."
        }
    },
    "XnyZfCerSX": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method for detecting AI-generated text by leveraging weaker language models and structured search over their features.",
            "1": "This approach is innovative and addresses the challenge of detecting text generated by black-box models or unknown model versions.",
            "2": "**High Performance**: Ghostbuster achieves impressive results, with an average F1 score of 99.0 across three diverse datasets (student essays, creative fiction, and news).",
            "3": "This performance significantly outperforms existing methods like GPTZero and DetectGPT, demonstrating the effectiveness of the proposed approach.",
            "4": "**Comprehensive Evaluation**: The authors provide a thorough evaluation of Ghostbuster, including in-domain and out-of-domain performance, as well as generalization across different models and prompt variations.",
            "5": "This comprehensive evaluation strengthens the validity of their findings.",
            "6": "**New Datasets**: The release of three new datasets for benchmarking AI-generated text detection is a valuable contribution to the research community.",
            "7": "These datasets cover multiple domains and can serve as useful benchmarks for future research.",
            "8": "**Feature Selection and Classifier Training**: The detailed explanation of the feature selection process and classifier training provides clarity on how the model works.",
            "9": "The use of both structured search and handcrafted features ensures a robust feature set for classification.",
            "10": "**Ethical Considerations**: The paper addresses potential ethical concerns, particularly the risk of misclassifying text by non-native English speakers as AI-generated.",
            "11": "The authors' efforts to mitigate these risks and their caution against using Ghostbuster for automatic penalization are commendable.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Domain Representation**: While the paper covers three domains, it acknowledges that these datasets are not representative of all writing styles or topics.",
            "14": "The model's performance on other domains, varieties of English, or non-English languages remains uncertain.",
            "15": "**Generalization Across Models**: Although Ghostbuster performs well on text generated by different models, the performance on Claude-generated text (92.2 F1) is lower than on GPT-generated text.",
            "16": "This indicates room for improvement in generalizing across different AI models.",
            "17": "**Dependence on Weaker Models**: The reliance on weaker language models for feature extraction might limit the scalability and adaptability of the approach.",
            "18": "As language models evolve, the definition of \"weaker\" models may change, potentially affecting the model's performance.",
            "19": "**Complexity of Feature Selection**: The structured search for feature selection, while effective, adds complexity to the model.",
            "20": "This complexity might make it challenging to adapt the approach to new datasets or models without significant computational resources.",
            "21": "**Human Evaluation Baseline**: The human evaluation baseline, with an average accuracy of 59%, suggests that the task is difficult for humans.",
            "22": "However, the paper could benefit from a more detailed analysis of the types of errors made by human evaluators to better understand the challenges in detecting AI-generated text.",
            "23": "**Robustness to Edits**: The paper mentions that robustness to lightly edited AI-generated text is an area for future work.",
            "24": "This is a significant limitation, as minor edits can often bypass detection systems, reducing their practical utility.",
            "25": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the detection of AI-generated text.",
            "26": "The innovative approach, high performance, and comprehensive evaluation make Ghostbuster a valuable contribution to the field.",
            "27": "However, there are areas for improvement, particularly in terms of domain representation, generalization across models, and robustness to edits.",
            "28": "Addressing these weaknesses in future work could further enhance the utility and applicability of Ghostbuster in real-world scenarios."
        }
    },
    "d3UGSRLbPo": {
        "llm_review": {
            "0": "**Timely and Relevant Topic**: The paper addresses a critical issue in the deployment of large language models (LLMs) - hallucinations.",
            "1": "Given the increasing reliance on LLMs in various applications, improving their reliability is of paramount importance.",
            "2": "**Comprehensive Approach**: The proposed method is thorough, involving both detection and mitigation of hallucinations.",
            "3": "The approach is broken down into clear, manageable steps, which makes it easier to understand and implement.",
            "4": "**Use of Logit Values**: Leveraging the model's logit output values to identify potential hallucinations is a novel and effective strategy.",
            "5": "This method provides a quantifiable measure of the model's uncertainty, which is a strong indicator of hallucinations.",
            "6": "**Validation and Mitigation**: The two-stage process of validating uncertain concepts and then mitigating detected hallucinations is well thought out.",
            "7": "The use of web search for validation ensures that the information is up-to-date and accurate.",
            "8": "**Experimental Rigor**: The paper includes extensive experiments with GPT-3.5 and Vicuna, demonstrating the effectiveness of the proposed approach across different models and tasks.",
            "9": "The results show a significant reduction in hallucinations, which is a strong validation of the method.",
            "10": "**Wide Applicability**: The approach is tested on various tasks, including article generation, multi-hop questions, and false premise questions.",
            "11": "This demonstrates the versatility and robustness of the method.",
            "12": "**Detailed Analysis**: The paper provides a detailed analysis of the results, including the impact of different techniques for each step of the approach.",
            "13": "This thoroughness adds to the credibility of the findings.",
            "14": "**Ethics and Reproducibility**: The authors address ethical considerations and provide a clear path for reproducibility, which is essential for the adoption and further development of their method.",
            "15": "#### Weaknesses\n\n1.",
            "16": "**Complexity and Computational Cost**: The proposed method, while effective, is complex and may be computationally expensive.",
            "17": "The need for web searches and multiple validation steps could increase the latency and resource requirements, which might not be feasible for all applications.",
            "18": "**Dependence on External Knowledge**: The reliance on web search for validation introduces a dependency on external sources, which may not always be reliable or available.",
            "19": "This could limit the applicability of the method in environments with restricted internet access or in cases where the web search results are not trustworthy.",
            "20": "**Mitigation Failures**: While the paper acknowledges and analyzes the cases where the mitigation technique fails, it does not provide a clear path for addressing these failures.",
            "21": "Further research is needed to improve the mitigation step and handle cases where the retrieved knowledge is insufficient or incorrect.",
            "22": "**Limited Scope of Evaluation**: Although the experiments are extensive, they are conducted on a relatively small set of topics and questions.",
            "23": "A larger and more diverse dataset would provide a more comprehensive evaluation of the method's effectiveness.",
            "24": "**Generalization to Other Models**: The paper demonstrates the approach on GPT-3.5 and Vicuna, but it is unclear how well the method would generalize to other LLMs, especially those with different architectures or training paradigms.",
            "25": "Further experiments with a wider range of models would strengthen the claims.",
            "26": "**User Experience**: The paper does not discuss the potential impact on user experience.",
            "27": "The additional steps for validation and mitigation could introduce delays, which might affect the usability of the system in real-time applications.",
            "28": "**False Positives Handling**: While the paper claims that the mitigation technique does not introduce new hallucinations in the case of false positives, it would be beneficial to have a more detailed analysis of how often false positives occur and their impact on the overall system performance.",
            "29": "#### Conclusion\n\nOverall, the paper presents a well-structured and effective approach to detecting and mitigating hallucinations in LLMs.",
            "30": "The strengths of the method, including its comprehensive nature, use of logit values, and extensive experimental validation, make it a valuable contribution to the field.",
            "31": "However, the complexity, computational cost, and dependence on external knowledge are notable weaknesses that need to be addressed in future work.",
            "32": "Expanding the scope of evaluation and improving the handling of mitigation failures would further enhance the robustness and applicability of the proposed approach."
        },
        "gWakadDjSk": {
            "0": "- The multi-step approach presented is generally sound.",
            "1": "The approach can use black-box models hidden behind an API, and several possible solutions for each individual step are presented and evaluated to some extent.",
            "2": "- The experiments are primarily based on GPT-3.5, but there are also experiments with Vicuna-1.5 to validate the results.",
            "3": "In addition, the use of an open model supports easier reproduction and improves the overall accessibility of the presented methodology.",
            "4": "- Hallucinations are a relevant problem with current LLMs and are a limitation to their general applicability.",
            "5": "- The proposed multi-step approach is likely to increase generation latency significantly.",
            "6": "While this is noted superficially, and an improvement for one of the many steps is roughly sketched out, an in-depth discussion is missing - in particular, there are no experiments or theoretical discussions about the overall latency.",
            "7": "I am not of the opinion that high latency is a problem for all use cases, but it would be important to have a proper discussion about this limitation and where it is a problem.",
            "8": "- The overall experimental design is not described in sufficient detail.",
            "9": "In particular, it is not clear how the data used for Section 3.1 relate to those used for Sections 3.2 and 3.3.",
            "10": "If they were the same data, I would be concerned about the reliability of the results in the later sections, since the hyperparameters of each step, such as the aggregation method used to obtain concept uncertainty, are chosen to maximize the metrics in a data set.",
            "11": "- It is not clear to what extent retrieval alone explains the reduction in hallucinations.",
            "12": "Given that the proposed method uses (multiple) web search queries, a natural baseline would be to consider the article generation task based on retrieved facts about the article topic, which would have some favorable properties (e.g., lower latency, less technical complexity) compared to the proposed multi-step approach.",
            "13": "A proper ablation/evaluation against this baseline could help to delineate this effect.",
            "14": "- Some of the design decisions seem to be taken quite ad-hoc; for instance, the choice of a method for key concept identification seems to be based on qualitatively looking at a few examples (Table 4, Section B.1)"
        },
        "Mcbg2Brtfa": {
            "0": "- The proposed method is clearly described.",
            "1": "- The \"propagation of hallucination\" analysis very nicely show the necessity of actively reducing hallucination from the generation.",
            "2": "Although  sentence-by-sentence actively doing retrieval and rewrite has been explored in prior work, there's little quantitive analysis studying how previous hallucination can affect future generations.",
            "3": "- Experimental results indicate that the proposed method is very effective at reducing span-level hallucinations for long-form generation.",
            "4": "- The improvements on multi-hop QA is large, and the gains can be well explained by the \"active\" hallucination detection and revision mechanism.",
            "5": "- It would be nice to highlight the novelty of proposed framework from existing work.",
            "6": "A very related work is [1], where the authors do active retrieval and rewrite actively when decoding each sentence, and they also use LLM output logits to find low-confidence spans for query generation.",
            "7": "There are also several previous works that reduces LLM hallucinations at the response-level, using a similar framework as this work by prompting LLMs for span extraction, query generation, retrieval, and revise.",
            "8": "For example, [2] and [3] uses such a framework to revise LLM responses and reduces hallucination; [4] prompted LLMs for extracting and checking claims as an automatic evaluation framework.",
            "9": "This paper should discuss these related work, discuss the main differences, and maybe consider them as baselines.",
            "10": "- The paper lacks ablations to justify some of its key components.",
            "11": "For example, though there is a strong motivation for applying the method \"actively\" when generating every sentence, the end-to-end evaluation does not show how it helps reduce hallucination compared to applying it at the end of the generation.",
            "12": "Similarly, I couldn't find ablation for only fact-checking low-confidence phrases v.s.",
            "13": "fact checking all key phrases.",
            "14": "- The presentation quality can be improved.",
            "15": "Section 2 enumerates many modeling choices for each component, but it is difficult to tell what is the final method being used, and why it works better than the others.",
            "16": "A suggestion is to describe the best approach in section 2, and leave other choices to ablation studies.",
            "17": "Section 3 and 4 cover many experiments, making it confusing to tell which is the most experiment and what are the main messages.",
            "18": "[1] Jiang, Zhengbao, et al.",
            "19": "\"Active retrieval augmented generation.\"",
            "20": "arXiv preprint arXiv:2305.06983 (2023).",
            "21": "[2] Gao, Luyu, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao et al.",
            "22": "\"Rarr: Researching and revising what language models say, using language models.\"",
            "23": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.",
            "24": "16477-16508.",
            "25": "2023.",
            "26": "[3] Chen, Anthony, et al.",
            "27": "\"PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions.\"",
            "28": "arXiv preprint arXiv:2305.14908 (2023).",
            "29": "[4] Min, Sewon, et al.",
            "30": "\"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.\"",
            "31": "arXiv preprint arXiv:2305.14251 (2023)."
        },
        "30KmVVyPDS": {
            "0": "The paper works on an important problem of mitigating hallucinations \nThe paper presents an early detection approach and demonstrates effectiveness with two LLMs \nThe paper is extremely well-written, with clear goals, a well-described approach, and a detailed Appendix\n\nWhile it is always possible to nitpick on experimental design issues, we need to be mindful of the fact that this work is presented within the scope of a single ICLR submission.",
            "1": "With that in mind, the paper does an excellent job.",
            "2": "It is unclear how effective this method will be for generations beyond the first five sentences.",
            "3": "Post-rebuttal response: After going over the discussions, reviews, and rebuttals.",
            "4": "I feel that my initial assessment of the paper had gaps.",
            "5": "I lean towards the majority view that the paper needs improvement.",
            "6": "I have updated my scores accordingly."
        },
        "wAzNud1tNl": {
            "0": "The paper addresses the topic of hallucinations which is a relevant and timely topic.",
            "1": "The paper does not mentioned the highly relevant work of Kadavath et al., [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221), which also uses the model uncertainty to detect hallucinations.",
            "2": "Since uncertainty is used as a major signal in the proposed pipeline, the novelty of the proposed approach is not clear.",
            "3": "The paper does not study important choices in details.",
            "4": "For instance, the web search procedure is not very clear.",
            "5": "The paper says “In some cases, multiple web searches were required to check the correctness of different facets of a sentence”.",
            "6": "Are these searched human-supervised?",
            "7": "What are the stopping criteria?",
            "8": "I would suggest adding the web-search procedure in an algorithm block so that the readers can understand it better.",
            "9": "Similarly, the paper does not discuss exactly what kind of \"important\" concepts are identified by the model.",
            "10": "Could you provide some examples?",
            "11": "Are the models supposed to extract all relevant concepts?",
            "12": "Is the concept extraction supposed to work well across different application domains (e.g., questions answering)?",
            "13": "What if we are working with non-instruction tuned models?",
            "14": "It is not clear how good the instruction models were at following different instructions in Table 3.",
            "15": "Did the authors perform a systematic analysis here?"
        }
    },
    "vg31nvdrzF": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces Self-Context Adaptation (SeCAt), a novel self-supervised method that enables small visual language models to perform open-ended few-shot learning.",
            "1": "This is a significant contribution as it challenges the prevailing notion that only large models can handle such tasks effectively.",
            "2": "**Efficiency**: SeCAt demonstrates that small models (around 1B parameters) can outperform much larger models (up to 70B parameters) in few-shot learning tasks.",
            "3": "This is a crucial finding for democratizing access to advanced AI capabilities, making them available to individuals and organizations with limited computational resources.",
            "4": "**Comprehensive Evaluation**: The authors conduct extensive experiments on various multimodal few-shot datasets, including Real-Names miniImageNet and Open-Ended miniImageNet, as well as custom semantically easy and hard datasets.",
            "5": "This thorough evaluation provides strong evidence of the method's effectiveness and flexibility.",
            "6": "**Detailed Methodology**: The paper provides a clear and detailed description of the SeCAt method, including the clustering process, pseudo-caption generation, and self-context construction.",
            "7": "This transparency allows for reproducibility and further exploration by other researchers.",
            "8": "**Generalization**: The method shows strong generalization capabilities across different prompt templates and varying levels of task difficulty.",
            "9": "This robustness is a significant advantage for practical applications.",
            "10": "**Qualitative Analysis**: The inclusion of qualitative examples helps illustrate the model's ability to bind visual concepts to relevant words, providing a deeper understanding of its performance.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Limited Task Scope**: While the paper demonstrates impressive results on few-shot learning tasks, it would benefit from extending the evaluation to more complex and diverse tasks, such as image captioning and visual question answering.",
            "13": "This would provide a clearer picture of the method's potential applications.",
            "14": "**Cluster Naming Strategy**: The paper uses random nouns, numbers, and nonsense words for cluster naming.",
            "15": "While this approach works, it might be interesting to explore more sophisticated naming strategies that could potentially enhance performance further.",
            "16": "**Scalability**: Although the method is efficient for small models, the paper does not discuss the scalability of SeCAt to even smaller models or its performance on extremely large datasets.",
            "17": "This could be an area for future research.",
            "18": "**Inference Time**: The paper mentions that the model is kept entirely frozen during inference, but it does not provide detailed information on the inference time and computational requirements.",
            "19": "This information would be valuable for understanding the practical implications of deploying SeCAt in real-world scenarios.",
            "20": "**Ablation Studies**: While the paper includes several ablation studies, it could benefit from additional experiments to explore the impact of different clustering algorithms and the choice of visual and language backbones on the overall performance.",
            "21": "**Limitations and Future Work**: The paper briefly mentions the limitations and potential future work but could provide a more detailed discussion on the challenges and possible directions for improving SeCAt.",
            "22": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of few-shot learning with small visual language models.",
            "23": "The introduction of SeCAt is a novel and efficient approach that challenges the need for large models in open-ended few-shot learning tasks.",
            "24": "The comprehensive evaluation and detailed methodology make this work a valuable contribution to the research community.",
            "25": "However, addressing the identified weaknesses and expanding the scope of evaluation could further strengthen the impact and applicability of this method."
        },
        "17SSIbwA9s": {
            "0": "It is an interesting and important topic to explore the potential few-shot learning ability on small-scale vision language models.",
            "1": "The overall method is simple and clear.",
            "2": "The main problem of this paper is that their design and experiments can not support the claim that the proposed model has the few-shot or in-context learning ability.",
            "3": "Few-shot learning for vision language models requires the model to first infer the underlying task according to the context and give the accordingly response.",
            "4": "The paper only includes the fast concept binding task which exactly matches the proposed training paradigm as an evaluation for the few-shot learning ability.",
            "5": "The proposed training method only teaches the model to pick one \"category\" name from the given context based on the visual similarity between the query image and context images instead of learning general multi-modal abilities.",
            "6": "Therefore, it is foreseeable to have good performance on those fast concept binding tasks compared with other real vision-language models.",
            "7": "The authors should test the model on different tasks (eg.",
            "8": "VQA) in the few-shot setting to validate its claim.",
            "9": "Otherwise, the position of this paper should be reconsidered and might need to be compared with meta-learning methods."
        },
        "W4BwlcQnkH": {
            "0": "They authors define an effective recipe for few-shot visual language models alignment that boosts performance of small models.",
            "1": "The introduction of the \"self-context\" is an extremely interesing concept as it can be controlled arbitrarily to match granularity/complexity of the task at hand.",
            "2": "Kudos for the plan to release the code.",
            "3": "The concept of k-ways should be explained better when it is introduced.",
            "4": "I found myself having to go back multiple times while reading the work.",
            "5": "The quantitative comparisons are missing the strongest baseline (OpenFlamingo, the upper bound).",
            "6": "For completeness and transparency it should be included."
        },
        "ahAOwhDKnl": {
            "0": "- This paper investigates a question that is both scientifically interesting and practically important: is large scale necessary for emerging few-shot learning capabilities in VLMs?",
            "1": "- The paper is well-written and easy to follow for the most part\n- It is an interesting finding that smaller language models can match or exceed the performance of larger ones on some tasks - Some related work is missing.",
            "2": "The authors should cite work that attempts to automatically construct few-shot learning tasks without supervision / class labels from the ‘older’/’traditional’ few-shot literature too (e.g.",
            "3": "in the context of few-shot classification on vision tasks) and newer.",
            "4": "Some examples are [A, B] (see references below, and several additional references therein).",
            "5": "- The few-shot learning problems used for evaluation are quite simple.",
            "6": "It would be great to apply this approach on other multi-modal few-shot learning tasks from related work (e.g.",
            "7": "Flamingo)\n\n- I’m a bit concerned that the few-shot binding task in particular is one that the proposed method is particularly suited for, and thus the reported results may be more optimistic than they may have been for different few-shot learning tasks.",
            "8": "The reason I think this is that the particular way in which self-contexts are formed for finetuning can be thought of as ‘binding’ visual concepts to new (random / unrelated) nouns.",
            "9": "So the proposed approach can be viewed as finetuning the LM specifically on ‘binding few-shot tasks’ and thus as being ‘tailor-made’ for such tasks.",
            "10": "- The motivation of the proposed pseudo-labeling and self-context creation of SeCAt should be strengthened.",
            "11": "Why is it better than finetuning on actual paired image-text as is done in previous works?",
            "12": "If I understand the argument correctly, the point is to simulate *any* new concept, and teach the model to in-context learn by figuring out to match ‘symbols’ to new items, rather than recalling known semantic relationships between words and visual concepts.",
            "13": "And the argument is that this is more important for smaller models?",
            "14": "It would however greatly strengthen the motivation of the current approach to actually demonstrate empirically the failure of using ‘standard’ interleaved text-image finetuning versus their proposed method (in the context of exactly the same small language models).",
            "15": "It would be very interesting to investigate how the relative performance of those two (usual interleaved text-image finetuning versus SeCAt) changes as the size of the model increases.",
            "16": "Please let me know if this is already one of the baselines and I’m missing it.",
            "17": "- Related to the above point, for the ablation in Table 3b, aside from ‘nonsense’, ‘numbers’ and ‘nouns’, an additional row could be for assigning the actual ‘matching’ noun.",
            "18": "The implicit hypothesis if I understand correctly is that that would be worse, but it would be good to examine this empirically.",
            "19": "- For ablation 3e that studies the impact of the model size on few-shot learning capabilities, there is an important aspect that isn’t studied, namely, how much does adding SeCAt *improve* the few-shot learning capabilities of models of different sizes (that is, plotting the relative improvement of few-shot learning in the ‘raw’ model versus after SeCAt finetuning).",
            "20": "And ideally, tying this in with what I wrote above, this would also be compared to the relative improvement that would have been obtained by more ‘standard’ interleaved text-image finetuning.",
            "21": "References\n=========\n- [A] Unsupervised Learning Via Meta-Learning.",
            "22": "Hsu et al.",
            "23": "ICLR 2019.",
            "24": "- [B] Unsupervised Meta-learning via Few-shot Pseudo-supervised Contrastive Learning.",
            "25": "Jang et al.",
            "26": "ICLR 2023."
        },
        "TjEGM8CLNT": {
            "0": "(1) The paper is easy to follow and well-written.",
            "1": "Especially, figures are well-designed and ideas are well presented.",
            "2": "(2) Empirical results show the strong performance of the proposed method.",
            "3": "They provide sufficiently good ablation studies to deeper understand the logic behind the performance gain.",
            "4": "While the paper is interesting to read, I have several major concerns about novelty and effectiveness of the method.",
            "5": "(1) Using the unrelated name to improve the in-context learning ability is not new in language model [1].",
            "6": "While the domain is slightly different, author need to discuss the difference with the study and clarify the contribution.",
            "7": "Especially, according to [1], they show that symbol tuning does not improve performance on small scale model (with relevant labels) does not improve performance, which is somewhat contradicting with the claim in this paper.",
            "8": "[1] Symbol tuning improves in-context learning in language models\n\n\n(2) While the paper claim that it is open-ended, the evaluated task is limited to the standard few-shot classification task.",
            "9": "In other words, the trained model is strongly biased toward generate shorter sentences compared to the existing methods like Frozen and FROMAGe, which are not designed to just solve classification.",
            "10": "In that sense, I think author should compare the method with the standard few-shot baseline, rather than just comparing with the pre-trained vision language models."
        }
    },
    "iSAgvYhZzg": {
        "llm_review": {
            "0": "**Innovative Approach**:\n   - The paper introduces Auto-UI, a novel multimodal agent that directly interacts with user interfaces without the need for environment parsing or application-specific APIs.",
            "1": "This is a significant advancement over existing methods that rely heavily on external tools and APIs, which can introduce inefficiencies and errors.",
            "2": "**Chain-of-Action Technique**:\n   - The proposed chain-of-action technique, which leverages a series of intermediate previous action histories and future action plans, is a creative solution to improve the agent's decision-making capabilities.",
            "3": "This method helps in maintaining context and planning future actions effectively.",
            "4": "**Comprehensive Evaluation**:\n   - The authors evaluate Auto-UI on a new device-control benchmark, AITW, which includes 30K unique instructions spanning various multi-step tasks.",
            "5": "The experimental results demonstrate that Auto-UI achieves state-of-the-art performance with an action type prediction accuracy of 90% and an overall action success rate of 74%.",
            "6": "**Generalization and Efficiency**:\n   - The paper highlights the generalization ability of Auto-UI across different task domains and its efficiency in terms of inference speed and GPU memory usage.",
            "7": "The model's ability to infer actions in less than one second is particularly noteworthy.",
            "8": "**Detailed Analysis**:\n   - The authors provide a thorough analysis of the model's performance, including an ablation study to verify the contributions of different components, such as the chain of actions and coordinate normalization.",
            "9": "This detailed analysis helps in understanding the strengths and limitations of the proposed approach.",
            "10": "**Public Availability**:\n   - The commitment to making the code publicly available is commendable, as it promotes transparency and allows other researchers to build upon this work.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Limited Comparison with Other Models**:\n   - While the paper compares Auto-UI with several baselines, including in-context learning LLMs and fine-tuned LLMs, it would benefit from a more extensive comparison with other state-of-the-art models in the field.",
            "13": "This would provide a clearer picture of where Auto-UI stands in the broader context of autonomous UI agents.",
            "14": "**Data Imbalance Issue**:\n   - The paper mentions that the performance does not improve when using all the training data of GoogleApps due to data imbalance.",
            "15": "However, it does not provide a detailed analysis or potential solutions to address this issue.",
            "16": "Exploring techniques to handle data imbalance could further enhance the model's performance.",
            "17": "**Scalability Concerns**:\n   - Although the paper discusses the model's efficiency, it does not delve deeply into the scalability of Auto-UI for very large-scale applications.",
            "18": "It would be beneficial to understand how the model performs when scaled up to handle more complex and larger datasets.",
            "19": "**Limited Real-World Application Scenarios**:\n   - The evaluation is primarily conducted on the AITW benchmark.",
            "20": "While this is a comprehensive dataset, additional real-world application scenarios could provide more insights into the practical utility and robustness of Auto-UI.",
            "21": "**Future Work and Improvements**:\n   - The paper could benefit from a more detailed discussion on future work and potential improvements.",
            "22": "For instance, exploring advanced vision features to improve the model's understanding of screen layouts or addressing the data imbalance issue in more depth.",
            "23": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of autonomous UI agents with the introduction of Auto-UI.",
            "24": "The innovative chain-of-action technique and the model's ability to interact directly with user interfaces without relying on external tools or APIs are noteworthy contributions.",
            "25": "The comprehensive evaluation and detailed analysis further strengthen the paper.",
            "26": "However, addressing the mentioned weaknesses, such as providing more extensive comparisons, handling data imbalance, and exploring scalability, could further enhance the impact and applicability of this work."
        },
        "pS9WH5HwaI": {
            "0": "The proposed Auto-UI approach demonstrates a level of originality in addressing the challenges of autonomous user interface agents.",
            "1": "By directly interacting with the interface instead of relying on environment parsing or application-specific APIs, it offers a novel solution that bypasses common inefficiencies and risks associated with existing approaches.",
            "2": "The introduction of the chain-of-action technique also adds a unique element to the decision-making process of the agent.",
            "3": "The approach is evaluated through experiments with the AITW benchmark.",
            "4": "The inclusion of 30,000 unique instructions covering various multi-step tasks provides a comprehensive assessment of the Auto-UI system.",
            "5": "Achieving a state-of-the-art performance demonstrates the effectiveness and reliability of the proposed solution.",
            "6": "Overall, the paper is clear and easy to follow.",
            "7": "The text provides a clear description of the challenges faced by existing approaches, introduces the Auto-UI solution, and explains the chain-of-action technique.",
            "8": "The inclusion of experimental results contribute to a clear understanding of the proposed methodology and its performance.",
            "9": "By addressing the challenges of inference inefficiency and error propagation, Auto-UI offers a more efficient and reliable approach to task automation.",
            "10": "The multimodal solution and the elimination of environment parsing and reliance on application-specific APIs provide a significant advancement in the development of autonomous UI agents.",
            "11": "Furthermore, the state-of-the-art performance achieved on the AITW benchmark showcases the practical applicability and potential impact of the proposed approach.",
            "12": "While the authors highlight the chain-of-action technique as a contribution, it appears to primarily concatenate the output actions, which can be confusing.",
            "13": "It would be helpful to provide a more detailed explanation or clarification of how the chain-of-action technique enhances the decision-making process and contributes to the overall effectiveness of the Auto-UI approach.",
            "14": "The experiment section lacks an explanation for the rationale behind selecting specific baselines.",
            "15": "It would be valuable to include a justification for choosing the particular baselines used in the evaluation.",
            "16": "Additionally, providing information on the performance of a GPT4 model, if available, would offer a useful benchmark to compare the performance of the proposed Auto-UI approach."
        },
        "ydnTigJlJ8": {
            "0": "- The proposed framework is claimed to be much lighter weight than methods that try to take the whole web information into textualized format for agents to comprehend.",
            "1": "- The formatted action is sound and should be generalizable to other web-search domains.",
            "2": "- The paper is pretty easy to follow, with illustrations onto the points.",
            "3": "- The generalization ablation studies are helpful to gauge the capacity of the proposed framework.",
            "4": "- The paper does not describe much about the actual training details, in that sense, to me, the proposed method is still a kind of BC, where the target decoding is optimized towards mimicking the golden action sequences.",
            "5": "(Unless some RL or other mechanism is used here, which is not described.)",
            "6": "In my opinion, the novelties here mainly lie in the multimodal representations (both modality taken into account) and the format of the action performed.",
            "7": "- I’m a bit skeptical about the ICL baseline, first of all more details (e.g., how actions are represented, how OCRed results are used) of that baseline need to be described, at least in the appendix.",
            "8": "Secondly, it also needs to be evaluated at the action plan level, my guess is that this method should be quite accurate on those but might fail more on the lower-level executions.",
            "9": "Thirdly, it is indeed unfair simply because the model is not taking the images into account, which could be the key towards the success of the proposed method in this work.",
            "10": "So, at least a multimodal version of it needs to be taken into consideration, or, a better spatial representation of the html syntax is required.",
            "11": "(HTML can be many times too coarse to represent a spatial layout.)",
            "12": "- Similar to above, the third baseline, fine-tuning LLMs, need to have a version with multimodal inputs.",
            "13": "- An error analysis is required both on the quantitative and qualitative sides, what are the major errors that these models exhibit?"
        },
        "kltAhQRXQy": {
            "0": "It is novel that the paper pays attention to the limitations in the real-world applications of autonomous agents and seeks to provide an agent that does not need extra intermediate environment parsing or interval application-dependent APIs.",
            "1": "The paper proposes a chain-of-action technique which helps the agent to decide step-by-step.",
            "2": "The Figure 1 in this paper is somewhat not clear enough, making it difficult to understand the two paradigms in (a) and (b).",
            "3": "The author does not provide a specific explanation of the Sandbox Paradigm and the First Principles Thinking Paradigm, which is confused.",
            "4": "We find some grammar mistakes in the paper, for example, on page 2, paragraph 2, line 5, do you want to express inefficiency instead of efficiency?",
            "5": "The authors don't explain exactly what touch_point, lift_point, etc.",
            "6": "mean in the first place, causing some confusion.",
            "7": "The authors do not provide a specific example between Auto UI and other baselines in Section 5, which is not clear to understand the effectiveness of the provided Auto UI."
        },
        "8rGkFYabUM": {
            "0": "This work proposes a chain of action operation, leveraging the action history and future actions for current action prediction.",
            "1": "Based on Llama 2, it incorporates a pretrained image encoder into the pretrained LLM for action decision, and shows promising results on AITW dataset.",
            "2": "A potential weakness is where is the gain from?",
            "3": "It looks PaLM and ChatGPT are pretty low on this dataset, while they only take text input, and BC models and Auto-UI models take image screen as input, and get very high results, it is unclear where is the gain from?",
            "4": "image encoder?",
            "5": "or a chain of action input?"
        }
    },
    "DQCZiKb3Uy": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel method, PR2L, which leverages vision-language models (VLMs) to provide promptable representations for reinforcement learning (RL).",
            "1": "This approach is innovative as it utilizes the vast amounts of general-purpose knowledge encoded in VLMs to enhance RL tasks.",
            "2": "**Effective Use of VLMs**: The method effectively uses VLMs to generate task-specific embeddings by prompting them with relevant context.",
            "3": "This allows the RL agent to access semantic features that are grounded in visual observations, which is a significant improvement over traditional methods that learn from scratch.",
            "4": "**Comprehensive Evaluation**: The authors conduct thorough experiments on visually complex RL tasks in Minecraft.",
            "5": "The results demonstrate that policies trained on promptable embeddings from VLMs outperform those trained on generic image encoder features and instruction-following methods.",
            "6": "**Ablation Studies**: The paper includes detailed ablation studies that highlight the importance of VLM promptability and text generation in yielding good representations for RL.",
            "7": "This adds robustness to the findings and provides insights into the components that contribute to the success of the approach.",
            "8": "**Prompt Optimization**: The authors propose a simple yet effective method for evaluating and optimizing prompts without running expensive RL trials.",
            "9": "This is a practical contribution that can save significant computational resources.",
            "10": "**Clear Presentation**: The paper is well-structured and clearly presents the methodology, experiments, and results.",
            "11": "The use of figures and tables helps in understanding the approach and its effectiveness.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Limited Scope of Tasks**: The experiments are limited to Minecraft tasks.",
            "14": "While these tasks are visually complex and relevant, it would be beneficial to see the approach tested on a wider range of RL environments to demonstrate its generalizability.",
            "15": "**Manual Prompt Design**: The prompts used in the experiments are manually crafted.",
            "16": "While the authors provide a method for evaluating and optimizing prompts, the initial design still relies on human intuition.",
            "17": "Automating the prompt generation process could make the approach more scalable and less dependent on domain expertise.",
            "18": "**Comparison with Fine-Tuned Models**: The paper compares PR2L with MineCLIP, which is fine-tuned on Minecraft data.",
            "19": "However, the VLM used in PR2L is not fine-tuned.",
            "20": "A direct comparison with a fine-tuned VLM could provide a more balanced evaluation of the approach's effectiveness.",
            "21": "**Computational Overhead**: While the authors mention that fine-tuning the VLM incurs substantial computational overhead, they do not provide a detailed analysis of the computational costs associated with their approach.",
            "22": "Including such an analysis would help in understanding the trade-offs involved.",
            "23": "**Potential for Overfitting**: The reliance on task-specific prompts raises the question of overfitting to specific tasks.",
            "24": "It would be interesting to see how the approach performs on tasks that are significantly different from those used in training, to assess its robustness and adaptability.",
            "25": "**Limited Discussion on Failure Cases**: The paper does not provide a detailed discussion on the failure cases or limitations of the approach.",
            "26": "Understanding the scenarios where PR2L might not perform well could provide valuable insights for future improvements.",
            "27": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in leveraging vision-language models for reinforcement learning.",
            "28": "The novel approach of using promptable representations shows promising results in visually complex tasks and provides a new direction for integrating pre-trained models with RL.",
            "29": "However, the scope of tasks, manual prompt design, and lack of detailed computational analysis are areas that could be improved.",
            "30": "Future work could focus on automating prompt generation, testing on a broader range of tasks, and providing a more comprehensive evaluation of the approach's limitations and computational costs."
        },
        "YCh26JGSxU": {
            "0": "[1] The method proposed in this paper, PR2L, is very straightforward, and the story sounds reasonable.",
            "1": "The organization of the paper is very clear, and it is polished well.",
            "2": "[2] The experimental analysis is quite comprehensive, and ablation experiments have demonstrated the effectiveness of each part of the proposed method.",
            "3": "Conducting experiments in a challenging environment like Minecraft is persuasive.",
            "4": "[1] Concerns about computational cost.",
            "5": "Obtaining promptable representation requires running a complex VLM on every image returned from the environment, along with the prompt and answer.",
            "6": "This cost is prohibitively high and not practical.",
            "7": "[2] Concerns about the performance of the learned policy.",
            "8": "The visually rich representation obtained at such a high computational cost should greatly improve the performance of the policy.",
            "9": "However, the authors did not provide videos of rollouts on relevant tasks, making it difficult to judge the effectiveness of the promptable representation in a real Minecraft environment (the three tasks used in the paper are not very complex).",
            "10": "[3] Insufficient literature review.",
            "11": "Since the authors conducted experiments in Minecraft, they should have provided a more comprehensive discussion of articles that control and plan within Minecraft.",
            "12": "However, the authors left out the following important literature.",
            "13": "Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction.",
            "14": "Video pretraining (vpt): Learning to act by watching unlabeled online videos.",
            "15": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.",
            "16": "CLIP4MC: An RL-Friendly Vision-Language Model for Minecraft.",
            "17": "GROOT: Learning to Follow Instructions by Watching Gameplay Videos.",
            "18": "Learning from Visual Observation via Offline Pretrained State-to-Go Transformer."
        },
        "xI0hli5jGN": {
            "0": "+The study is relevant and could be of interest to many audiences with a background in large models, reinforcement learning, and representation learning.",
            "1": "+The method is well-motivated and technically sound.",
            "2": "Pretrained VLM indeed provides open vocabulary and even knowledge-aided representations for multimodal input, which can be quite beneficial to open-world environments.",
            "3": "Plus, it is plausible to tweak the representation further via prompting.",
            "4": "This is a very neat idea.",
            "5": "+The results look impressive.",
            "6": "Although the method is only evaluated on limited (3) tasks in a single environment (Minecraft), the advantages over the baselines and ablative approaches are significant.",
            "7": "I do think the authors did a good job of comparing it against several interesting baselines, including no generation, no prompt, etc.",
            "8": "Having said those above, I do have some major concerns about the evaluation part of this paper.",
            "9": "I also would like to point out some minor issues as well.",
            "10": "-Albeit the promises shown by the results on 3 tasks on Minecraft, I don't think the approach is thoroughly evaluated, especially given their claim on \"leverage contextual prior information\" and \"visually-complex RL tasks\" (see abstract).",
            "11": "I have the following suggestions:\n\n1) There are some other approaches that are designed to tackle similar issues, especially in Minecraft, ex.",
            "12": "[1,2,3].",
            "13": "Although I agree some settings could be different (RL vs. IL, etc), they all deliver some backbone design or objective functions that could facilitate better representations.",
            "14": "Comparisons against these missing baselines would help the reader with a better understanding of the significance of the proposed method.",
            "15": "2) Minecraft is indeed a challenging domain in terms of open-world and complex visual observations.",
            "16": "However, the tasks being evaluated here (spider, cow, sheep) do not seem to be challenging enough to justify the effectiveness of the proposed method, especially on the claimed \"leverage contextual prior information\".",
            "17": "These mobs are indeed very common and the tasks themselves do not seem to involve complex visual stimuli.",
            "18": "My suggestion is to try some long-term and open-ended tasks like surviving, collecting items, etc.",
            "19": "[7] offers a few of them and worth taking a look at.",
            "20": "Minor: some references on open-world representation learning and Minecraft agents should be cited: [1-6].",
            "21": "[1] open-world control: https://arxiv.org/abs/2301.10034\n\n[2] VPT: https://arxiv.org/abs/2206.11795\n\n[3] STG transformer: https://arxiv.org/abs/2306.12860\n\n[4] DEPS: https://arxiv.org/abs/2302.01560\n\n[5] Plan4MC: https://arxiv.org/abs/2303.16563\n\n[6] GITM: https://arxiv.org/abs/2305.17144\n\n[7] MCU: https://arxiv.org/abs/2310.08367"
        },
        "nZPUgdAv07": {
            "0": "PR2L presents an interesting and creative way of utilizing pre-trained VLMs as representations for visual policy learning.",
            "1": "It is unlike prior pre-trained representations for control work in which the features are generic (i.e., directly encoding the image observation); it is also different from recent Vision-Language-Action work (Brohan et al., 2023) in that it does not require fine-tuning a pre-trained VLM and enable high-frequency policies that are disentangled from the VLM backbone.",
            "2": "As VLMs are increasingly adopted in decision-making pipelines, PR2L is a timely work that presents a lightweight and simple alternative to the existing literature.",
            "3": "The paper itself is generally well-written and free of grammatical errors.",
            "4": "This paper's weaknesses mainly lie in its experimental methodologies.",
            "5": "The only form of prompt that PR2L uses essentially amounts to object detection in the scene.",
            "6": "This introduces a confounding factor of whether PR2L outperforms baselines because it is able to recognize objects better in the scene.",
            "7": "The paper claims that the prompts are different from instruction following; however, the prompts are still manually constructed and task-specific.",
            "8": "It's unclear the advantage of doing so as instructions, by construction, should exist as it is a direct form of task specification.",
            "9": "The improvements of PR2L over its various ablations appear only moderate.",
            "10": "Furthermore, the best prompt format for the tasks are not consistent; for Spider, it is helpful to include contextual information of what a spider looks like in MineCraft, whereas for the other two tasks, it is more helpful to disregard such information.",
            "11": "Therefore, applying PR2L to a new task may require substantial prompt engineering for the best performance.",
            "12": "PR2L is only evaluated on 3 tasks; these tasks are also the simplest in the MineDojo benchmark.",
            "13": "The paper would be strengthened if more tasks and domains are evaluated.",
            "14": "Currently, it is not convincing that PR2L can be generally applied to other visuomotor control domains.",
            "15": "Relatedly, PR2L does not outperform MineCLIP on most tasks; given that MineCLIP exists and is open-sourced, PR2L's stated advantages can be better demonstrated via a new domain in which foundation pre-trained representations do not already exist.",
            "16": "BLIP-2's vision encoder may not be the strongest baseline for pre-trained vision encoders.",
            "17": "Several prior works such as VC-1, R3M, MVP, VIP are trained for decision-making and robotics tasks and may constitute stronger baselines in that category."
        },
        "BZDCpA1kdy": {
            "0": "- The main contribution of the paper “Prompting VLM via auxiliary information and task context” allows extracting more meaningful representation from a VLM is quite interesting and easily applicable to a range of tasks.",
            "1": "Instead of fine-tuning VLM for specific domains, it’s easier to plug-and-play existing VLM and extract meaningful representations via prompting.",
            "2": "- Overall, the paper is well written and is systematic in its evaluation.",
            "3": "I also appreciate the authors willingness to address concerns preemptively (lack of visual tokens as input to the policy MLP, not fine-tuning VLM similar to RT-1).",
            "4": "- Given that the approach is using a VLM, it’d be nice to test the model for “unseen” tasks, containing objects and instructions not seen during training.",
            "5": "For instance, does the policy generalise form “Combat Spider” to “Combat Zombie”?",
            "6": "- I also recommend a stronger evaluation on Minecraft benchmark consistent with the evaluations done in MineDOJO.",
            "7": "Currently, the paper shows result on only three tasks.",
            "8": "For a more exhaustive evaluation, MineDOJO recommends evaluation on a collection of starter tasks (32 programmatic and 32 creative tasks).",
            "9": "- While I understand that the authors didn’t have the resources to train a RT-1 style baseline, would it still be possible to train an action decoder on top of the VLM to produce actions.",
            "10": "I think having a strong RT-1 style baseline is very important to properly evaluate the question (2) mentioned in the paper — “How does PR2L compare to approaches that directly “ask” the VLM to generate the best possible actions for a task specified in the prompt?”\n\nOverall, I liked the main contribution of the paper.",
            "11": "But I believe, in its current form, the evaluation in the paper is a bit weak and could be made more exhaustive (unseen tasks, more exhaustive MineCraft evaluation).",
            "12": "**Update**: After reading the rebuttal, most of my questions are adequately answered.",
            "13": "I still believe that directly training an action decoder (while keeping VLM frozen is a good baseline) and should be included in the paper.",
            "14": "I also feel that the paper will be stronger if they include more exhaustive minecraft experiments (to check for generalisation on unseen tasks / objects) and more environments which are visually more complex like Habitat / AI2 Thor.",
            "15": "Based on authors response, I am increasing my score."
        }
    },
    "HgVEz6wwbM": {
        "llm_review": {
            "0": "A Control Theory of LLM Prompting\"\n\n#### Strengths\n\n1.",
            "1": "**Novel Theoretical Framework**: The paper introduces a novel approach by framing prompt engineering for large language models (LLMs) within the context of control theory.",
            "2": "This is a significant contribution as it provides a structured and mathematical perspective to an area that has been largely empirical and heuristic-driven.",
            "3": "**Mathematical Rigor**: The authors provide a rigorous mathematical analysis of the controllability of LLMs, particularly focusing on the self-attention mechanism.",
            "4": "The derivation of upper bounds on the reachable set of outputs based on the singular values of parameter matrices is a strong theoretical contribution.",
            "5": "**Empirical Validation**: The paper complements its theoretical findings with empirical analysis on several LLMs, including Falcon-7b, Llama-7b, and Falcon-40b.",
            "6": "The experiments are well-designed to test the theoretical predictions and provide practical insights into the controllability of these models.",
            "7": "**Comprehensive Analysis**: The authors explore various aspects of prompt engineering, including the impact of prompt length and the likelihood of reaching specific outputs.",
            "8": "The use of different datasets and synthetic data to test the reachability of tokens adds depth to the analysis.",
            "9": "**Practical Implications**: The findings have significant practical implications for improving the performance and reliability of LLMs in real-world applications.",
            "10": "The control-centric analysis offers a foundational perspective that can guide the development of more robust and capable LLM systems.",
            "11": "**Clear Presentation**: The paper is well-organized and clearly presents both the theoretical framework and the empirical results.",
            "12": "The use of definitions, theorems, and proofs is appropriate and aids in understanding the complex concepts discussed.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Limited Scope of Empirical Analysis**: While the empirical analysis is comprehensive, it is limited to a few specific models and datasets.",
            "15": "The generalizability of the findings to other LLMs and more diverse datasets remains an open question.",
            "16": "**Assumptions in Theoretical Analysis**: The theoretical analysis makes several assumptions, such as the use of greedy decoding and the specific structure of self-attention.",
            "17": "These assumptions may limit the applicability of the results to more complex or varied LLM architectures.",
            "18": "**Focus on Single-Token Outputs**: The empirical analysis primarily focuses on the reachability of single-token outputs.",
            "19": "While this is a fundamental step, the controllability of multi-token sequences and more complex tasks remains unexplored.",
            "20": "**Comparison Across Models**: The paper mentions the challenge of comparing controllability scores across different model families due to differences in tokenizers.",
            "21": "However, it does not provide a detailed discussion or solution to this issue, which is crucial for a fair comparison.",
            "22": "**Scalability of Control Methods**: The proposed control methods, such as greedy back-generation and greedy coordinate gradient, may not scale well to larger prompts or more complex tasks.",
            "23": "The computational cost and efficiency of these methods are not thoroughly discussed.",
            "24": "**Future Work and Open Questions**: While the paper identifies several open problems and future research directions, it could benefit from a more detailed discussion on how to address these challenges.",
            "25": "Providing concrete steps or preliminary results on these fronts would strengthen the paper.",
            "26": "#### Conclusion\n\nOverall, \"What’s the Magic Word?",
            "27": "A Control Theory of LLM Prompting\" is a strong and innovative paper that makes significant contributions to the field of prompt engineering for LLMs.",
            "28": "The integration of control theory provides a new lens to understand and improve LLM performance.",
            "29": "However, the paper could be further strengthened by addressing the limitations in empirical scope, theoretical assumptions, and scalability of the proposed methods.",
            "30": "Future work should aim to generalize the findings to a broader range of models and tasks, and explore the practical implementation of control-theoretic approaches in real-world LLM applications."
        },
        "SaHhisLK4E": {
            "0": "- I think the papers main selling point is to offer the perspective to analyze prompt engineering through a control theory lens.",
            "1": "Unfortunately, it not novel and has been previously suggested in [1].",
            "2": "- Nevertheless, the idea to use control theory---and notions of controllability/reachability in particular---to analyze LLMs is quite elegant.",
            "3": "- The paper asks a number of interesting questions in section 7.",
            "4": "[1] Soatto, Stefano, et al.",
            "5": "\"Taming AI Bots: Controllability of Neural States in Large Language Models.\"",
            "6": "arXiv preprint arXiv:2305.18449 (2023).",
            "7": "- It is not clear that the analysis in section 4 is of any relevance as it stands for two reasons:\n 1)  I am not sure it makes sense to give a \"topological/norm/metric\" controllability condition as in (4).",
            "8": "Is there a natural topology here to justify this?",
            "9": "As far as I understand tokenization imposes a more or less arbitrary choice of topology so it is not clear to me that characterizing controllability (an algebraic concept) by a topological one makes any sense.",
            "10": "2) Moreover, I cannot find any suggestion in the paper that the bound is of any practical use---here the obvious questions is: is it sharp and if not how loose?",
            "11": "- Control theory is fundamentally a study of dynamical systems.",
            "12": "The statement that \" For simplicity, the time set T and the transition map ϕ are omitted from this\ndefinition\" made in section 3 then suggests that you are actually no longer really in the realm of control---which was your main selling point to begin with.",
            "13": "- The experiments are somewhat sparringly commented and the reader is left wondering how these were conducted.",
            "14": "What is the precise definition of a \"solved instance\" and how were these instances generated?",
            "15": "My critique pertains to section 6 mainly but the appendix also suffers from uncommented plots.",
            "16": "- While satisfactory, the level of writing could be better.",
            "17": "There are a number of grammatical errors, but beyond that and more importantly, the paper does not feel very well structured.",
            "18": "I think this is due in part to a lack of clear delineation of what the paper's contribution is.",
            "19": "- It would be better if the analysis was stated in standard thm/proof style.",
            "20": "Currently, commentary is interwoven with the proof of claim (4) making the derivation unnecessarily obtuse.",
            "21": "Ideally, the proof should be prefaced by exactly that which is to be shown and then broken down into an overview of the relevant components, followed by the proof itself.",
            "22": "Let me finally say that I find this particularly strange since the paper has a number of definitions (potentially too many...) stated in the standard mathematical style."
        },
        "5I8nziqpFc": {
            "0": "The idea of introducing $k-\\epsilon$ controllability is interesting and it is a nice connection between LLM and control theory.",
            "1": "The theoretical bound presented in Section 4, though hard to check in reality, is an initial step towards understanding the steerability of LLM theoretically.",
            "2": "In addition, I do appreciate the authors provide an interesting discussion on the open problems of LLM in control.",
            "3": "Section 4 only considers a self-attention head, which is quite simple and limited (compared to the current model used in LLM).",
            "4": "What are the difficulties in generalizing such results to a more complex model?",
            "5": "The presentation of Section 4 can be further improved.",
            "6": "The relationship between state controllability (Definition 7) and $k-\\epsilon$ controllability (Definition 6) should be discussed, i.e., implications of your theory result in Section 4.",
            "7": "In addition, I am confused about some notations: are $u_i$, $x_i$ the embeddings of the tokens?",
            "8": "Previously the u and x are presented as tokens, it does not make sense to make $\\|u_i\\|\\le 1$ and $\\|x_i\\| \\le 1$ if they are tokens.",
            "9": "In addition, is this assumption valid in real LLMs?",
            "10": "Although introducing the controllability of LLM from a control perspective is interesting, the experimental results of checking the controllability of the LLMs are not very exciting given the existing results from previous work [Zou 2023].",
            "11": "The experiment setup is almost identical to GCG work and the obtained results are also within expectation.",
            "12": "Instead, proposing a new method to study the controllability of black-box LLMs will be more interesting."
        },
        "xGQRJOmZaI": {
            "0": "- The paper presents a compelling perspective, which could be fruitful.",
            "1": "- Despite being a little bit disorganized, I appreciate the basic idea behind the theoretical analysis, showing that there is some fundamental bottleneck that makes LLMs not arbitrarily steerable (under some assumptions).",
            "2": "- I have concerns on the significance of the empirical results.",
            "3": "In particular, I suspect LLMs might be very easily steerable, and that any limitation in the ability to push them to a particular output is just due to limitations in the optimization method that is used to find the prompt.",
            "4": "As a noticeable example, one might say that, for the definition of steering that has been employed in the paper, a sufficiently capable LLM can always be steered by prepending to it an prefix that reads like \"after reading n words/token, output this particular word/token\".",
            "5": "If the optimization procedure does not find such a solution, it seems more a limitation of that than a fact related to the inability of LLMs to be steered.",
            "6": "- The paper is at times not well-organized.",
            "7": "The discussion section, that is usually a summary of the takeaways from the paper, looks more like a discussion of related work, and also has some incomplete points, the theoretical results would be better understood inside of a theorem latex environment, and so on."
        },
        "CKo9hA40d5": {
            "0": "The primary novelty of this paper lies in its formulation of the prompt engineering problem as an optimal control problem.",
            "1": "I truly appreciate this idea.",
            "2": "It is widely acknowledged that the choice of prompts heavily influences the LLM performance.",
            "3": "The interplay between the LLM's weights and the input prompt jointly determines the \"states\" of the LLM.",
            "4": "This insight is intuitive but non-trivial, very different from conventional supervised learning approaches.",
            "5": "It is commendable that the authors have translated this observation into a mathematical framework and provided an initial analysis.",
            "6": "Weakness or Questions\n\n1.",
            "7": "I have a concern regarding the controllability metric.",
            "8": "When an LLM is controllable, there exists a prompt capable of compelling the LLM to produce a desired output, even if that output is factually incorrect.",
            "9": "Thus, this formulation does not seem to establish a real connection with the specific capabilities of LLM, such as reasoning and knowledge memorization.",
            "10": "Moreover, The attribute of controllability appears to lean more towards a negative property, signifying the LLM's susceptibility to prompt manipulation and potential vulnerabilities.",
            "11": "The theory only requires the weight matrices to meet a particular bound on their largest eigenvalues.",
            "12": "The analysis framework does not depend on any training data or training algorithms.",
            "13": "Even randomly generated weights (as long as they satisfy the specific bound) can render the LLM entirely controllable.",
            "14": "This raises concerns about whether this theoretical framework can effectively explain the underlying mechanism of a well-trained LLM.",
            "15": "The results do not seem to offer practical guidance for prompt design.",
            "16": "The analysis does not appear to provide insights into how to stimulate the capabilities of an LLM effectively.",
            "17": "The proposed methods both require access to the desired output (the ground truth) during the search process."
        },
        "wtIbFkDig2": {
            "0": "This paper is one of the first of its kind in applying a control theoretical concepts to the study of LLMs.",
            "1": "Control theory offers very powerful tools that have the potential of providing a more formal understanding of LLMs behavior.",
            "2": "For this reason, the approach introduced in this paper has great potential to advance our understanding of language technologies.",
            "3": "This reviewer positively values the originality of this paper.",
            "4": "Moreover, the topic addressed of controlling language generation with the appropriate prompt is very relevant, and having rigorous tools opens the door to a formal treatment of LLMs.",
            "5": "In this paper, they provide an algorithm to find “magic words” based on a mathematical result inspired by control theory concepts.",
            "6": "More importantly, they list a series of open questions that could be addressed from a control-theoretical point of view.",
            "7": "This paper has some issues with the formalization of the theoretical concepts, as well as with the presentation of the results.",
            "8": "Since this paper introduces for the first time control theoretical concepts in the light of LLMs, it is paramount that the definitions are accurate and properly capture the ideas underpinning dynamical systems.",
            "9": "This is not the case for this paper.",
            "10": "The control theoretical concepts in this paper are not properly communicated, and Definition 4 has several flaws.",
            "11": "The definition given corresponds to an input-output system, as opposed to a dynamical system: in order for it to be a dynamical system, the state space should be V as opposed to V^*.",
            "12": "Moreover, the definition given for k-\\epsilon controllability is very far the definition of controllability in dynamical systems.",
            "13": "Please refer to Feedback Control Systems (Amstrom and Murray, 2009) for details.",
            "14": "This reviewer is concerned that, if published in the current form, this paper can introduce more confusion than clarifications in the realm of using control theoretical tools for LLMs analysis.",
            "15": "Aside from this, the main result should be framed as a theorem and provided in the body of the paper, together with the algorithms, not in the appendices, as these are the main results of the paper.",
            "16": "Furthermore, the paper currently lacks necessary formal definitions (such as the definition of the V^* set), and the control theoretical section lacks clarity in the exposition.",
            "17": "Substantial modifications are needed to improve the rigor of the presentation in Sections 3 and 4."
        }
    },
    "3fEKavFsnv": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel multi-population aware optimization method for Maximum Mean Discrepancy (MMD), termed MMD-MP.",
            "1": "This method addresses the high variance issue in MMD when dealing with multiple text populations generated by various large language models (LLMs).",
            "2": "This is a significant contribution as it enhances the stability and reliability of MMD in detecting machine-generated texts (MGTs).",
            "3": "**Comprehensive Evaluation**: The authors conduct extensive experiments on various LLMs, including GPT2, GPT3, ChatGPT, and GPT-Neo series.",
            "4": "The results demonstrate the superior performance of MMD-MP in both paragraph-based and sentence-based detection tasks.",
            "5": "This thorough evaluation across different models and datasets strengthens the validity of the proposed method.",
            "6": "**Theoretical Analysis**: The paper provides a solid theoretical foundation for the proposed method.",
            "7": "The authors delve into the optimization mechanism of kernel-based MMD and offer a detailed analysis of the variance components.",
            "8": "This theoretical insight is valuable for understanding the underlying principles of the proposed approach.",
            "9": "**Practical Implications**: The proposed method has significant practical implications, especially in scenarios where distinguishing between human-written texts and MGTs is critical, such as in detecting fake news, spam, and phishing attempts.",
            "10": "The ability to detect MGTs with high accuracy can enhance the credibility and security of online information.",
            "11": "**Transferability**: The paper highlights the enhanced transferability of MMD-MP in detecting unknown MGTs.",
            "12": "This is a crucial advantage, as it suggests that the method can generalize well to new, unseen types of machine-generated texts, making it robust and versatile in real-world applications.",
            "13": "**Visualization and Interpretability**: The visualization of kernel features using t-SNE provides an intuitive understanding of how MMD-MP differentiates between human-written and machine-generated texts.",
            "14": "This interpretability is a valuable addition, making the method more transparent and easier to comprehend.",
            "15": "#### Weaknesses:\n\n1.",
            "16": "**Complexity of Implementation**: While the proposed method is theoretically sound and empirically effective, the implementation complexity might be a barrier for practitioners.",
            "17": "The optimization process and the need for a deep kernel might require significant computational resources and expertise in kernel methods and deep learning.",
            "18": "**Limited Real-World Testing**: Although the paper includes extensive synthetic and controlled experiments, it would benefit from more real-world testing scenarios.",
            "19": "For instance, evaluating the method on diverse and noisy real-world datasets could provide additional insights into its robustness and practical applicability.",
            "20": "**Comparison with More Baselines**: The paper compares MMD-MP with several state-of-the-art methods, but including more recent and advanced baselines could further strengthen the evaluation.",
            "21": "For example, comparing with other advanced anomaly detection methods or hybrid approaches that combine multiple detection strategies might provide a more comprehensive performance assessment.",
            "22": "**Scalability Concerns**: The scalability of the proposed method to very large datasets or real-time applications is not thoroughly discussed.",
            "23": "Given the increasing size of datasets and the need for real-time detection in many applications, addressing the scalability aspect would be beneficial.",
            "24": "**Parameter Sensitivity**: The paper does not extensively discuss the sensitivity of the proposed method to various hyperparameters.",
            "25": "Understanding how different parameter settings affect the performance and stability of MMD-MP would be valuable for practitioners looking to apply this method in different contexts.",
            "26": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of detecting machine-generated texts by introducing a novel multi-population aware optimization method for MMD.",
            "27": "The comprehensive theoretical analysis, extensive empirical evaluation, and practical implications make it a valuable contribution.",
            "28": "However, addressing the implementation complexity, scalability, and providing more real-world testing scenarios could further enhance the impact and applicability of the proposed method."
        },
        "1xfleiUOdP": {
            "0": "Innovative Approach to a Contemporary Challenge: The paper addresses the pressing and contemporary issue of distinguishing machine-generated texts (MGTs) from human-written texts, especially given the sophisticated capabilities of modern LLMs.",
            "1": "The introduction of the multi-population aware optimization method, MMD-MP, is a novel approach that seeks to enhance the accuracy and stability of MGT detection, providing a fresh perspective on the problem.",
            "2": "The authors haven't limited their research to just theoretical propositions; they have put their methods to the test.",
            "3": "They conducted extensive experiments across various LLMs, providing empirical evidence of the superiority of their approach.",
            "4": "Such thorough testing not only validates their methods but also offers a benchmark for future research in this domain.",
            "5": "The paper offers clear theoretical insights, particularly the exploration of the optimization mechanism of MMD and its associated challenges.",
            "6": "Propositions and corollaries, such as the asymptotics of MPP and test power, further underscore the research's depth and provide a strong theoretical foundation for their method.",
            "7": "The use of statistical properties and theoretical analysis makes the paper robust and comprehensive.",
            "8": "The paper relies heavily on mathematical formulations, such as the objective function \n$$ J(P, Q; k_u) = MPP(P, Q; k_u)/σ(P, Q; k_u) $$\nand the estimator equation \n$ MPP_u(S_p, S_q; k_u) = \\frac{1}{n(n − 1)} \\sum H^{ij} $, \nwhere  $ H^{ij}=k_u(x_i, x_j) − k_u(x_i, y_j) − k_u(y_i, x_j) $.",
            "9": "While these equations are crucial for understanding the paper's methodology, they can be intimidating and might make the content less accessible to readers who aren't well-versed in mathematical notation or kernel-based methodologies.",
            "10": "The paper introduces MMD-MP to handle the variance issue in MMD when dealing with multiple text populations.",
            "11": "However, the mathematical representation \n$ σ^2_{s} = 4 (E[H^2_{i3}] − E[H^2_{i2}] ) $, \nwhere \n$ H^2_{i3}, H^2_{i2} $\ndenote different $ H^{ij} $, suggests that the variance calculation is still inherently tied to the distance metrics.",
            "12": "This could lead to concerns about the real-world applicability and stability of the proposed solution, especially in scenarios with vast and diverse text samples.",
            "13": "The algorithms presented, such as the training deep kernel with MMD-MP (Algorithm 1) and testing with MMD-MP for 2ST (Algorithm 2), are specifically tailored to address the MGT detection problem.",
            "14": "Given the intricate mathematical formulations, it may be challenging to adapt or generalize these algorithms to other domains or problems without significant modifications.",
            "15": "For researchers or practitioners looking for broader applications, this specificity might be a limitation."
        },
        "fna9T6YXvs": {
            "0": "The paper's motivation for using MMD to discriminate between highly similar distributions, particularly in short sentences, is compelling.",
            "1": "Short sentences often pose a greater challenge due to their simpler structure and wording, and MMD offers an effective solution.",
            "2": "The proposed method is supported by both theoretical analysis and experimental results.",
            "3": "The experimental design is comprehensive.",
            "4": "The experiments involving the detection of text generated by unknown or unseen LLMs are interesting.",
            "5": "Additionally, evaluating both short and long text is a well-rounded approach.",
            "6": "The investigation into how the number of training data instances impacts performance strengthens the effectiveness of the proposed MMD method.",
            "7": "One recent SOTA baseline for detecting machine-generated texts is missing [1].",
            "8": "The paper has also shown impressive performance in short texts and needs to be discussed and compared in the paper.",
            "9": "[1]Stylometric Detection of AI-Generated Text in Twitter Timelines, Kumarage et.",
            "10": "al."
        },
        "cndAm7SqsR": {
            "0": "- This paper introduces MMD-MP, a multi-population aware optimization method, which achieves better machine-generated text detection rate than baseline.",
            "1": "- This paper has a very clear mathematical formulation of the MMD-MP method, as well as provide reasoning on why they choose to make this optimization on top of vanilla MMD.",
            "2": "- Test power is not very straightforward to interpret.",
            "3": "I suggest adding an easier-to-understand explanation in additional to the current definition in section 2.",
            "4": "In my understanding, the higher the test power number is, the more “confident” the model is to the idea that the two distribution is different, right?",
            "5": "- Easier-to-read figures:\n    - This might be a nit-pick but figure 1 and 2 are very packed.",
            "6": "I understand that the authors want to plot MMD-D and MMD-MP out for better comparison, but maybe consider removing the triangles & squares & circles?",
            "7": "- What does the last two column mean in table 1?",
            "8": "Is it when $q=3$, so the three “clusters” are human-written text, ChatGPT text, and GPT3-S text?",
            "9": "- The method is overall, slightly complicated.",
            "10": "The experiments when $q=3$ are conducted on “different enough” language models.",
            "11": "I wonder if better LMs are included in the comparison (GPT-4, chatGPT), how will the method perform."
        },
        "SwJw48fdSS": {
            "0": "The paper attacks a crucial problem and it is clearly written.",
            "1": "The paper is technically very sophisticated, and provides extensive theoretical justification for the proposed methods.",
            "2": "Results on the tested setting are very clearly positive.",
            "3": "The paper considers variations in text due to using different LLMs, but it basically ignores variation due to different genres, styles, and human authors.",
            "4": "I think this is an artifact of sticking to a single small corpus (in the main paper), where it is easy to generate new LLM outputs by using the provided prompts, but all other attributes have to remain fixed.",
            "5": "These other attributes are probably more important in real-world applications.",
            "6": "I am not sure whether experiments on such a small training corpus are meaningful.",
            "7": "Due to the cross-validation setup, it is very possible that all models that are trained on the corpus are over-fitting to this setting.",
            "8": "Baselines like ChatGPT-D that may be trained on much larger external corpora are therefore at a disadvantage.",
            "9": "Another potential disadvantage is that the single-sentence detection test is not very realistic; it would have been better to perform this test at the paragraph level.",
            "10": "A final problem with the experiment section is that many details are missing - see questions below.",
            "11": "The proposed method is quite complicated.",
            "12": "Apart from complexity of implementation, a potentially large disadvantage is that the loss is quadratic in training corpus size.",
            "13": "The authors don’t address this issue at all.",
            "14": "Although it has a lot of supporting math, the main contribution is actually a small modification (dropping a term in the loss), that is quite intuitive.",
            "15": "There is nothing wrong with this, but the paper would have been stronger had it focused more on providing this intuition up front, and less on the formal details."
        }
    },
    "KOTsHW6mBI": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method, Multi-bit Watermark via Position Allocation (MPAC), which extends the capabilities of zero-bit watermarking to embed traceable multi-bit information in the outputs of large language models.",
            "1": "This is a significant advancement as it allows for the tracing of adversary users, which is crucial for counteracting malicious uses of language models.",
            "2": "**Robustness**: The proposed method demonstrates robustness against strong attacks such as interleaving human texts and paraphrasing.",
            "3": "This is a critical feature for practical applications where adversaries might attempt to evade detection by modifying the generated text.",
            "4": "**No Model Access Required**: One of the key advantages of MPAC is that it allows for the extraction of the watermark without requiring access to the model parameters or the API.",
            "5": "This makes the method highly practical and scalable, as it can be used by third parties to trace the origin of the text.",
            "6": "**Maintains Text Quality**: The method maintains the quality of the generated text, which is essential for ensuring that the watermarking process does not degrade the user experience or the utility of the language model.",
            "7": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of the method, including experiments on different model scales, datasets, and various types of attacks.",
            "8": "This comprehensive analysis helps in understanding the strengths and limitations of the proposed approach.",
            "9": "**Theoretical Foundation**: The use of coding theory to analyze and improve the performance of the watermarking method is a strong point.",
            "10": "It provides a solid theoretical foundation for the approach and opens up avenues for further technical improvements.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Detection Performance Trade-off**: The paper acknowledges a trade-off between the load capacity (bit-width) and the detection performance.",
            "13": "As the bit-width increases, the ability to distinguish between machine-generated and human text decreases.",
            "14": "This is a significant limitation, especially for applications requiring high detection accuracy.",
            "15": "**Complexity and Overhead**: While the method does not require model access for extraction, the encoding and decoding processes involve additional computational steps, such as permuting the vocabulary and sampling positions.",
            "16": "The paper does not provide a detailed analysis of the computational overhead and its impact on real-time applications.",
            "17": "**Scalability Concerns**: Although the method is designed to be scalable, the performance under different real-world scenarios, such as varying text lengths and diverse language models, is not fully explored.",
            "18": "More experiments on a wider range of models and text types would strengthen the claims of scalability.",
            "19": "**Limited Comparison with Existing Methods**: The comparison with other watermarking methods is somewhat limited.",
            "20": "While the paper compares MPAC with a few recent methods, a more extensive comparison with a broader range of existing techniques would provide a clearer picture of its relative advantages and disadvantages.",
            "21": "**User Privacy Concerns**: The paper briefly touches on the ethical implications of watermarking, but a more in-depth discussion on user privacy and the potential misuse of watermarking technology would be beneficial.",
            "22": "Addressing these concerns is crucial for gaining user trust and ensuring ethical deployment.",
            "23": "**Empirical Results on Real-World Data**: The experiments are primarily conducted on synthetic datasets and controlled environments.",
            "24": "Evaluating the method on real-world data, such as social media posts or news articles, would provide more insights into its practical effectiveness and robustness.",
            "25": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of watermarking for large language models.",
            "26": "The proposed MPAC method offers a robust and practical solution for embedding traceable multi-bit information, which is crucial for counteracting malicious uses of language models.",
            "27": "However, there are some limitations related to detection performance, computational overhead, and scalability that need to be addressed.",
            "28": "Additionally, a more comprehensive comparison with existing methods and a deeper discussion on ethical implications would strengthen the paper.",
            "29": "Despite these weaknesses, the paper makes a valuable contribution to the field and opens up new research directions for proactive countermeasures against the misuse of language models."
        },
        "HOEnxgDW6i": {
            "0": "The introduction of a multi-bit watermark presents an innovative concept within this research domain.",
            "1": "Experimental results affirm the robustness of the proposed watermark algorithm.",
            "2": "The multi-bit watermark methodology, at a cursory glance, seems akin to an expansion of the red-green list from [1] into a more nuanced multi-color list.",
            "3": "Given that the detection methodologies remain consistent, the paper should elucidate how the multi-bit approach enhances the robustness intrinsic to [1].",
            "4": "As this constitutes the crux of the paper's contribution, it is imperative for the authors to furnish theoretical insights and a robust analysis concerning the multi-bit watermark's robustness.",
            "5": "There is a noticeable absence of a comparative robustness analysis between the proposed method and that established in [1] within the experimental evaluations.",
            "6": "Considering the resilience of the [1] watermark against text alterations, it is essential for the authors to demonstrate superior robustness to substantiate their contributions effectively.",
            "7": "While the authors purport that the multi-bit watermark sustains the caliber of the text, the empirical evidence, particularly in Table 1, suggests parity in quality between texts generated by the multi-bit and zero-bit watermarks.",
            "8": "It is incumbent upon the authors to extend their comparison to texts generated without watermarks to convincingly affirm that the multi-bit watermark preserves text quality.",
            "9": "Figure 5(a) depicts a correlation wherein an augmentation in bit-width correlates with diminished robustness.",
            "10": "This trend ostensibly advocates for the utilization of the zero-bit watermark, prompting the question of whether, in terms of robustness, [1] might present a more formidable solution than the proposed multi-bit watermark.",
            "11": "[1] A Watermark for Large Language Models.",
            "12": "John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein.",
            "13": "ICML (2023)"
        },
        "4LLc4kidSQ": {
            "0": "### Originality\n\nThe proposed method is relatively simple and is a nice extension of the zero-bit watermarking scheme.",
            "1": "This means that any intuition as well as tools and analysis that are based on Kirchenbauer et al.",
            "2": "[2023a] should also carry over and benefit the understanding of this scheme.",
            "3": "### Quality\n\nThis paper contains thorough experiments considering multiple sets of parameters, models, metrics, and attacks.",
            "4": "There is a good amount of numerical results and textual analyses in both the main text and the appendix, making the results convincing.",
            "5": "### Clarity\n\nThe paper is well-written and easy to follow.",
            "6": "Section 3 and Figure 1 provide a simple description of the scheme.",
            "7": "The messages are effectively conveyed; all the figures and tables are easy to interpret.",
            "8": "### Significance\n\nWatermarking for detecting machine-generated content is an intellectually interesting work, but I’m not sure about its practical benefits for two reasons.",
            "9": "First, with open-source LLMs, it is virtually impossible to enforce the “post-processing” watermarks.",
            "10": "Second, under the assumption that there’s an adversary trying to remove the watermark (e.g., via back-translation or paraphrase), it is very difficult to achieve a satisfactory TPR with a low enough FPR.",
            "11": "This problem is heavily asymmetric in the sense that the cost of an FP is gigantic (wrongly detecting a human-written text as a machine’s) compared to an FN so an acceptable FPR has to be very low in practice (~1e-5).",
            "12": "### Comparison to Steganography\n\nThe paper briefly mentions steganography and claims that it is different from watermarking.",
            "13": "However, I’m not convinced that they are different.",
            "14": "The undetectability property of steganography is exactly desired by LLM watermarking, i.e., a watermark should not modify the distribution of the “cover text” (or LLM-generated text) in a noticeable way.",
            "15": "Generally, steganography and watermarking differ in their purpose, but in the instance of LLM, they are identical concepts.",
            "16": "So it seems important that this paper (and other watermarking papers) is **compared (theoretically and/or empirically) to steganography on natural language** such as [1] and [2].",
            "17": "### Methodology\n\n- List decoding can be better parameterized by a confidence threshold rather than a fixed length $|\\mathbb{L}|$ like a 95%-confidence interval.",
            "18": "More precisely, given a confidence threshold (says 95%), we want to make a statement like: “With probability 95%, the true message is among these five messages” (think of [conformal prediction](https://en.wikipedia.org/wiki/Conformal_prediction), for example).",
            "19": "There should be a principled way to either model this directly or compute this by aggregating the confidence score at each position ($c_i$)—This can be a bit tricky since $c_i$’s are not independent.",
            "20": "- Also including the list decoding as part of the bit accuracy metric (e.g., in Figure 4 and 5) seems irrelevant and unjustified to me.",
            "21": "It’s not clear how the practitioners would be able to utilize the 16 other less-confident messages in the LLM detection/attribution setting.",
            "22": "- One very nice property of the zero-bit watermarking scheme proposed by Kirchenbauer et al.",
            "23": "[2023a] is the theoretical guarantee on the number of tokens in the green list (Theorem 4.2).",
            "24": "It would be nice to see a similar theoretical analysis on this paper.",
            "25": "### Experiments\n\n**Metric.",
            "26": "** AUC is a relatively misleading metric in practice for watermarking.",
            "27": "As mentioned earlier, each FP is very costly so an appropriate metric in this scenario is usually something like TPR at a very low level of FPR (e.g., 1e-2, 1e-4, 1e-6, etc.).",
            "28": "**Comparison to zero-bit watermarks.",
            "29": "** I would like to see a comparison of the efficiency of this multi-bit watermark to the previously proposed zero-bit ones (at least, Kirchenbauer et al.",
            "30": "[2023a]) for just detecting machine-generated texts.",
            "31": "This is important because prior to attributing the text to which model/service, we have to first decide whether the text is generated by a machine or a human.",
            "32": "In theory, this scheme should trade off the ability to encode longer messages with the watermark detection efficiency.",
            "33": "### Other Minor Issues\n\n- Figure 2 caption: “Clean bit error - corrupted bit error” was a little bit unclear as the metrics have not been defined at this point.",
            "34": "- [Typo] Page 9: “**Across Model Scales, Datasets, Hash Schemes.",
            "35": "** The results for larger models (13B, 70B) and other datasets are in Appendix A.4.” should be Appendix A.6?",
            "36": "References\n- [1] https://arxiv.org/abs/1909.01496\n- [2] https://arxiv.org/abs/2210.14889"
        },
        "VmMJAs5HNt": {
            "0": "Strength:\n•\tThe introduction gives a concise overview of the significance of machine-generated text identification, the approaches used in the past, and the reasons why more sophisticated techniques are required.",
            "1": "•\tThe paper provides various graphs and visual representations, aiding in understanding the performance under different conditions.",
            "2": "•\tOne of the strengths of the paper is its demonstration of the model's performance under copy-paste attacks, which tests the robustness of their watermarking method.",
            "3": "Weakness:\n•\tThe performance of the model seems to be reliant on token length.",
            "4": "When messages are longer, there seems to be a dip in performance, which may not be suitable for all applications.",
            "5": "For example: As mentioned under Figure 5, embedding longer messages at fixed bits per token seems to decrease the performance, especially when reaching up to 64-bit.",
            "6": "•\tMetrics like AUC and bit-accuracy are essential, and the paper might benefit from a more qualitative or user-centric analysis.",
            "7": "Reliance solely on quantitative metrics might not capture the full user experience or real-world applicability.",
            "8": "For example: The paper primarily discusses results in terms of AUC, bit-accuracy, and other such metrics.",
            "9": "A discussion on real-world applications potential user feedback could provide a more complete picture of the method's usefulness.",
            "10": "•\tWhile the paper evaluates against human-induced copy-paste attacks, it doesn't delve deeply into other potential real-world challenges.",
            "11": "For instance, how would the system perform against more sophisticated adversarial attacks or in scenarios with heavy noise?",
            "12": "•\tThe method's reliability on decoding depends heavily on the pseudo-random generator function.",
            "13": "If someone can reverse-engineer or predict the pseudo-random function, the watermark can potentially be tampered with or removed."
        },
        "edvoPdr4qg": {
            "0": "(1) The paper proposes an innovative and practical method to address the problem of misuse of large language models, extending beyond mere identification to traceability.",
            "1": "(2) The proposed method, Multi-bit Watermark via Position Allocation (MPAC), allows embedding and extraction of long messages without model access or fine-tuning, which is a significant improvement over existing methods.",
            "2": "(3) The authors also offer detailed empirical findings from their experiments, showing that their method can effectively embed 8-bit messages in short text lengths with over 90% bit accuracy.",
            "3": "(1) The paper did not provide comparisons to other multi-bit watermarking methods, such as the method presented by Yoo et al.",
            "4": "in \"Robust multi-bit natural language watermarking through invariant features\" Providing comparisons to related work would have strengthened the paper.",
            "5": "(2) The detection of the watermark is an important part.",
            "6": "More details on the watermark detection methodology would have been beneficial.",
            "7": "(3) Embedding capacity limited by the text distribution and model.",
            "8": "Low entropy distributions like code have an inherently lower capacity.",
            "9": "(4) The author could consider testing on other models besides LLAMA."
        },
        "1qAQTh8O65": {
            "0": "Encoding multi-bit information is an important problem, and a useful extension to 0-bit watermarking.",
            "1": "This lets us specific identify users abusing LLMs, going beyond the problem of simply detecting LLM generated content.",
            "2": "The approach presented seems reasonable, and an improvement over simple baselines of: 1) a unique watermarking key for each message we want to encode,  or 2) r=2 (the Kirchenbauer set up) and then simply picking the red and green lists based on the position.",
            "3": "The experiments are thorough, and the method seems to work for encoding multi-bit messages without degrading the quality anymore than the 0-bit scheme (the 0-bit scheme already degrades the LLM quality a bit).",
            "4": "I found the presentation a bit difficult to follow.",
            "5": "I think the paper would be much easier to follow if there were fewer forward references, and the paper was a bit more self contained.",
            "6": "It might also be worth dedicating space to central contributions in the paper, and having a list of claimed contributions in the paper.",
            "7": "I would like to see the decoding scheme in the main paper, and not the appendix since that is central to understanding a lot of the other details that are in the main paper.",
            "8": "There are concerns that there's not enough bandwidth to even do 0-bit watermarking, particularly for LLMs that have gone through RLHF/SFT as that process significantly reduces the entropy available for watermarking.",
            "9": "It is unclear to me if the proposed approach is useful in practice for encoding multi-bit messages.",
            "10": "There are a few details missing.",
            "11": "It is claimed that list decoding improves performance, but it is unclear to me how the decoded list is used in this accuracy computation.",
            "12": "If one of the messages in the list matches the encoded message, is that counted as a success?",
            "13": "I think the innovation over the 0-bit UMD scheme is somewhat incremental for a machine learning conference, at least from an ML standpoint."
        }
    },
    "BkvdAYhyqm": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel method, Summarize and Score (SASC), to generate natural language explanations for black box text modules.",
            "1": "This is a significant contribution to the field of interpretability in machine learning, particularly for large language models (LLMs).",
            "2": "**Comprehensive Evaluation**: The authors evaluate SASC in three different contexts: synthetic modules, BERT model internals, and fMRI voxel responses.",
            "3": "This comprehensive evaluation demonstrates the versatility and robustness of the proposed method.",
            "4": "**High Accuracy**: The method shows high accuracy in recovering ground truth explanations for synthetic modules, with an average accuracy of 74.3% across different settings.",
            "5": "This indicates that SASC is effective in generating reliable explanations.",
            "6": "**Comparison with Human Explanations**: The paper compares SASC-generated explanations with human-given explanations for BERT transformer factors.",
            "7": "The results show that SASC explanations are often of comparable quality to human explanations, which is a strong validation of the method.",
            "8": "**Potential Applications**: The application of SASC to fMRI voxel modules is particularly interesting.",
            "9": "It suggests potential applications in neuroscience for fine-grained brain mapping, which could lead to new insights in understanding brain functions.",
            "10": "**Open Source Code**: The authors provide all code for using SASC and reproducing results on GitHub.",
            "11": "This transparency and openness are commendable and will facilitate further research and development in this area.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Dependence on Pre-trained LLMs**: SASC relies heavily on the capabilities of pre-trained LLMs for summarization and generation tasks.",
            "14": "If the LLMs used are not sufficiently capable, the quality of the explanations may suffer.",
            "15": "This dependency could limit the applicability of SASC in scenarios where access to powerful LLMs is restricted.",
            "16": "**Limited to Top Responses**: SASC focuses on explaining the top responses of a module, which may not capture the full behavior of the module.",
            "17": "This limitation could be addressed by extending the method to explain a broader range of responses.",
            "18": "**Ngram-based Summarization**: The use of ngrams for summarization may not capture more complex patterns or dependencies in the text.",
            "19": "This could limit the effectiveness of SASC in explaining modules that rely on such patterns.",
            "20": "**Manual Evaluation**: The evaluation of explanations, particularly for BERT transformer factors and fMRI voxel modules, involves manual inspection.",
            "21": "This introduces subjectivity and may not scale well for larger datasets or more complex modules.",
            "22": "**Potential for Hallucination**: Despite efforts to mitigate LLM hallucination, there is still a risk that the generated explanations may not accurately reflect the true behavior of the module.",
            "23": "This is a common challenge in using LLMs for interpretability tasks.",
            "24": "**Limited Scope of Explanations**: SASC assumes that the behavior of a module can be concisely described in a natural language string.",
            "25": "This may not be feasible for more complex modules or those with non-coherent input sets.",
            "26": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of interpretability for large language models.",
            "27": "The proposed SASC method is innovative and shows promising results across different contexts.",
            "28": "However, there are some limitations related to the reliance on pre-trained LLMs, the focus on top responses, and the use of ngrams for summarization.",
            "29": "Addressing these limitations in future work could further enhance the effectiveness and applicability of SASC.",
            "30": "The open-source nature of the project is a strong positive, encouraging further research and collaboration in this important area."
        },
        "gfFWnxt087": {
            "0": "I think on balance the authors did a good exploration of their method, and the method is generally well described.",
            "1": "By evaluating on embedding models, you have a ground truth target to evaluate against.",
            "2": "Assuming the embedding model is accurate, which is not an unreasonable assumption.",
            "3": "By interpreting BERT transformer factors against previous human evaluations -- this is a direct comparison against a human baseline.",
            "4": "By looking at fMRI data, this evaluates SASC on a noisy scenario where the various brain regions are can only be observed under a noisy setting.",
            "5": "The assumptions used in the paper are weak and reasonable, and do not require differentiability assumptions, a residual/skip connection assumption (for example in the logit lens paper).",
            "6": "Compared to \"Language models can explain neurons in language models\" paper from OpenAI, their approach of using a corpus rather than per-token scoring is more sound, and it allows for more context dependent explanations.",
            "7": "Given that interpretability is a significant issue in the use of large language models, this paper addresses a timely and important problem.",
            "8": "I have a couple of concerns on the evaluation done in this paper.",
            "9": "I think their corpus + score approach is more sound than the OpenAI's approach of token-wise scoring.",
            "10": "But there is one problem I would like to see resolved:\n\n   a.",
            "11": "There is no ablation study on the corpus size and the effect on explanation scores.",
            "12": "Ideally the authors could explore the outcome using for example 10k, 20k, 40k, 80k, .... all n-grams.",
            "13": "The scoring step in my view is suspect, and introduces an unnecessary confound in terms of their procedures.",
            "14": "It seems very wrong to evaluate $E(f(\\text{Text}^{+})-f(\\text{Text}^{-}))$.",
            "15": "In this case, you are asking the language model to generate the phrases unrelated to an explanation, so the final score is not just related to how good the positives are, but also how \"good\" the negatives are.",
            "16": "Perhaps a more correct step should be $E(f(\\text{Text}^{+})-f(\\text{Text}^{\\text{all}}))$.",
            "17": "This is less problematic if this was just used as an internal component, but I think it is problematic when the authors use it as a point of comparison in Table 4, Figure 3, Table 6, fMRI experiments, Table A4, and Table A8.",
            "18": "The \"Default\" setting in \"synthetic modules\" experiment is not very sound in my view.",
            "19": "Ideally the exploration of the 54 modules should not be relying on a corpus which is known to contain the relevant ngrams, but the exploration of the modules should use the same (random) selection of ngrams.",
            "20": "It is a bit questionable that the authors use manual inspection to evaluate their method.",
            "21": "Scanning both the main text and the supplemental, the authors do not use Amazon Mechanical Turk or Prolific, and do not include any details on this human study.",
            "22": "I can only assume that this experiment was done by the authors themselves, which is prone to bias.",
            "23": "A more accurate approach would be to take a different text embedding model (for example bge-large, other models are fine too), compute the n-way cosine similarities among the 54 models to the 54 explainations, and compute the argmax agreement (is the n-th explanation generated by the SASC cosine similarity maximized by the n-th model), or even the cosine distance when n-way classification is not appropriate.",
            "24": "The clarity in some specific methods are poor.",
            "25": "There is no details in either the main text or the supplemental.",
            "26": "Concretely, I was unable to find details for the following:\n\n    a. ngram summarization baseline used in Table 1.",
            "27": "I looked at the references provided by the author (Kadar et al.",
            "28": "and Na et al.",
            "29": "), neither are actually performing ngram summarization.",
            "30": "The first citation uses a omission based approach, while the second uses a parsing + text replication approach.",
            "31": "b .",
            "32": "Can the authors describe how they get the spearman rank correlation sem?",
            "33": "I would not otherwise ask if the effect is strong, but the correlation is very low.",
            "34": "Do the authors find the sem via a bootstrap?",
            "35": "Could the authors report a t-statistic instead?",
            "36": "It seems like this proposed method would only work on semantic selectivity, and it is unclear if this method can work on low-level text patterns.",
            "37": "This is okay, but the authors should more clearly discuss this limitation in their paper.",
            "38": "Overall I like the paper.",
            "39": "And the proposed method is well described, but the author's evaluation broadly does not strike me as sound.",
            "40": "I would happily re-evaluate if the authors can answer my questions."
        },
        "STLKWfafgu": {
            "0": "This paper tackles the timely and important topic of interpreting black box natural language processing models like large language models.",
            "1": "The proposed SASC framework enables inspecting model internals and explaining model predictions in an interpretable way.",
            "2": "The SASC framework is novel and theoretically grounded.",
            "3": "The approach of using a separate helper model to generate explanations is creative.",
            "4": "The experiments reveal fascinating patterns in how different models generate explanations.",
            "5": "The results indicate SASC has potential for analyzing text models in fields like social science, medicine, and neuroscience.",
            "6": "Please refer to the questions section"
        },
        "uP5JKi3wVD": {
            "0": "The paper proposes a new method called Summarize and Score (SASC) that generates natural language explanations for black-box text modules.",
            "1": "The authors evaluate SASC on synthetic modules, explaining modules found within a pre-trained BERT model and generating explanations for the response of individual fMRI voxels to language stimuli.",
            "2": "The results show that SASC can generate high-quality explanations that are both accurate and human-readable.",
            "3": "The proposed method has significant implications for improving the interpretability of machine learning models.",
            "4": "The paper does not compare SASC to other state-of-the-art methods for generating natural language explanations for black box models.",
            "5": "It would be useful to compare SASC to other methods, such as LIME and SHAP, to determine how it performs in comparison.",
            "6": "In SASC design, the first step is to input the n-grams from the reference corpus into the text module, which could contain many instances.",
            "7": "How to ensure the efficiency of the proposed method remains undiscussed.",
            "8": "It would be better for authors to provide a detailed analysis of the computational complexity of SASC.",
            "9": "It would be useful to provide information on the computational requirements of the proposed method and potential ways to optimize it.",
            "10": "The design of synthetic scoring is heuristic and without any theoretical analysis to support this design.",
            "11": "And most importantly, there is no ablation study to verify the effectiveness of this model design."
        }
    },
    "4bUeP3qrNu": {
        "llm_review": {
            "0": "**Comprehensive Analysis**: The paper provides a thorough examination of syllogistic reasoning in both humans and language models (LMs).",
            "1": "It covers various aspects of reasoning, including accuracy, systematic errors, and specific biases such as ordering effects and logical fallacies.",
            "2": "**Human vs. LM Comparison**: The study effectively compares the performance of LMs with human reasoning, highlighting both similarities and differences.",
            "3": "This comparison is crucial for understanding the extent to which LMs can replicate or surpass human reasoning capabilities.",
            "4": "**Detailed Methodology**: The authors provide a clear and detailed methodology, including the data used, the models evaluated, and the specific prompts and settings for LM evaluation.",
            "5": "This transparency allows for reproducibility and further exploration by other researchers.",
            "6": "**Use of Cognitive Models**: The paper employs the Mental Models theory to interpret the reasoning behavior of LMs.",
            "7": "This approach provides a deeper understanding of the underlying mechanisms of LM reasoning and offers a novel way to analyze and interpret LM behavior.",
            "8": "**Insightful Findings**: The results are insightful, showing that larger LMs tend to be more accurate than humans and smaller LMs, but still exhibit systematic errors.",
            "9": "The identification of specific biases and fallacies in LMs, and the correlation with human reasoning patterns, adds significant value to the study.",
            "10": "**Future Directions**: The paper outlines potential future work, including the exploration of different elicitation approaches and the application of other cognitive theories to interpret LM reasoning.",
            "11": "This forward-looking perspective is valuable for guiding subsequent research in this area.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Scope of Syllogisms**: The study focuses on a specific type of logical reasoning (syllogisms).",
            "14": "While this is a well-defined and studied area, it may not fully capture the broader reasoning capabilities of LMs.",
            "15": "Expanding the scope to include other forms of logical reasoning could provide a more comprehensive assessment.",
            "16": "**Training Data Influence**: The paper hypothesizes that the inclusion of source code in the training data might contribute to the superior reasoning performance of LMs.",
            "17": "However, this hypothesis is not empirically tested within the study.",
            "18": "A controlled comparison of LMs trained on different corpora would strengthen this claim.",
            "19": "**Elicitation Methods**: Although the paper uses zero-shot chain-of-thought prompting, it acknowledges the existence of various other elicitation methods.",
            "20": "A more systematic evaluation of these different approaches could provide a clearer understanding of the best practices for eliciting reasoning from LMs.",
            "21": "**Human Data Limitations**: The human data used for comparison comes from a single study with 139 participants.",
            "22": "While this is a reasonable sample size, additional data from diverse populations and experimental setups could enhance the robustness of the human-LM comparison.",
            "23": "**Interpretation of Deliberative Reasoning**: The interpretation of LMs' behavior as deliberative reasoning based on the Mental Models theory is intriguing but speculative.",
            "24": "Further empirical validation of this interpretation, possibly through additional experiments or alternative cognitive models, would be beneficial.",
            "25": "**Handling of \"Nothing Follows\"**: The paper notes that LMs rarely conclude \"nothing follows,\" even when it is the correct answer.",
            "26": "While this behavior is acknowledged, the analysis of its implications and potential solutions is limited.",
            "27": "A deeper investigation into this issue could provide more insights into improving LM reasoning.",
            "28": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the understanding of syllogistic reasoning in LMs compared to humans.",
            "29": "It provides a detailed and insightful analysis, employs innovative methods for interpretation, and outlines valuable directions for future research.",
            "30": "Addressing the identified weaknesses, particularly through broader scope, empirical validation, and systematic evaluation of elicitation methods, would further enhance the impact and robustness of the study."
        },
        "Lpv1x3Fzgy": {
            "0": "The subject matter is very interesting and very well-grounded in theories in cognitive science.",
            "1": "The whole systematic investigation is very well-designed.",
            "2": "My only concern is that it is quite unclear how the human data was post-processed as each syllogism is associated with 139 responses.",
            "3": "If there is no further post-processing as the one on LM-produced data, then this is an insurmountable issue for me.",
            "4": "I am saying this because the LM's outputs are actually majority voting results based on 30 runs.",
            "5": "If humans did not do the same, then, for me, machines and humans were doing two very different tasks, making the conclusions extremely biased and, therefore, unreliable.",
            "6": "Thinking of a situation where one asks an LM to run 30 times on a test case and 20 of them are correct, the accuracy is 100% after majority voting.",
            "7": "Nevertheless,  if one asks humans to do the same but without majority voting, then the accuracy is only 66%, but their (i.e., humans and the LM) behaviours are very similar to each other.",
            "8": "For me, a better solution is dropping the majority voting and computing distributional similarities when comparing LMs with humans (I mean directly rather than the one in section 4.2)."
        },
        "vuOjrTOyoE": {
            "0": "The paper makes a clear contribution in understanding how humanlike LMs are by exploiting the already existent study of human performance on syllogisms.",
            "1": "This study allows us to make a fine-grained examination of 64 syllogism types (2 premises * 4 orderings * 8 moods), each of which has human performance recorded from 139 participants.",
            "2": "This approach makes the claims made in the submission believable.",
            "3": "- The findings are useful but not surprising.",
            "4": "- Some natural settings are omitted, including few-shot prompting which is only mentioned.",
            "5": "While the paper can't do everything, it seems a bit incomplete to not check the possibility that few-shot (which is more aligned with the human data) makes a big difference.",
            "6": "- Only the PaLM 2 model family is considered, though that's understandable from a limited budget point of view.",
            "7": "But we may not be able to discount the possibility that the LM behavior changes significantly with, e.g., GPT-4."
        },
        "HN2A3mqiRF": {
            "0": "This work covers various human biases in syllogistic reasoning, presenting an in-depth comparison through extensive experiments.",
            "1": "It examines the distribution of responses, variable ordering, and generation of syllogistic fallacies in both language models and humans.",
            "2": "By employing the Mental Models Theory, this work provides a fresh perspective on understanding the logical reasoning abilities of language models.",
            "3": "The deductive reasoning capability of LLMs has been widely explored.",
            "4": "The comparison of reasoning abilities between humans and language models in syllogism has already been explored in Dasgupta et al (2022).",
            "5": "And Saparov & He (2022) also utilizes similar controlled techniques for analyzing the capabilities of language models.",
            "6": "Besides, this work directly utilizes human results on syllogistic reasoning from previous work and only explore PaLM 2 models on a small set of data, which limits the contribution of this paper.",
            "7": "The motivation to use mReasoner and the explanation of its details are not well presented.",
            "8": "The description of the datasets and models for experiments are insufficient, such as the size of model parameters.",
            "9": "Conducting experiments on different language models would enhance the conclusion of the study.",
            "10": "There are some errors:\n> - In Section 2.3, Saparov & He (2022) and Saparov et al.",
            "11": "(2023) both analyze natural language, contrary to what the authors claim about formal logic forms.",
            "12": "> - There are spelling typos, such as \"efffect\" in the third paragraph of Section 4.2."
        },
        "iZTJetucbC": {
            "0": "- Uses cognitive science experiments and methods to understand large language models and make relatively direct comparison with human behavior.",
            "1": "- Detailed analysis of model responses, e.g.",
            "2": "looking at variable-ordering effects, beyond just summary statistics like accuracy.",
            "3": "- Compares LM responses to a cognitive model fit to human data, to identify which factors drive model behavior at scale.",
            "4": "- Appendix compares different prompting strategies as well.",
            "5": "- Results are only presented on one family of models, which are behind a closed API.",
            "6": "This has two problems: (1) While it is nice that this family has four scale variants, the results are still presented as about \"LMs\" in general, whereas we have learned about one in particular.",
            "7": "Another model family or two would help understand whether the results are in fact general.",
            "8": "(2) Behind a closed API, the results are not necessarily reproducible.",
            "9": "I'd like to see at least one open model as well.",
            "10": "- The dataset used for evaluation is relatively small.",
            "11": "- Difficult choices are made for parsing LM generation to decide when it has generated a conclusion to the syllogism.",
            "12": "I would have liked to hear more about this choice point and whether they looked at other formats (e.g.",
            "13": "multiple choice).",
            "14": "- The tendency of the model to _never_ output \"nothing follows\" means that all of the interesting analyses in the paper were on a subset of the syllogisms (27/64) where nothing follows is never correct.",
            "15": "This also limits the generality of the results."
        }
    },
    "VaZa8zj0Yw": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces several novel techniques inspired by the human brain to reduce computational costs while maintaining high-level autonomy and social reasoning in generative agents.",
            "1": "The hierarchical action selection, asynchronous self-monitoring, and Summarize-and-Forget memory mechanism are well-conceived and address significant challenges in the field.",
            "2": "**Cost Efficiency**: One of the standout contributions of this work is the significant reduction in computational costs.",
            "3": "The authors claim that Lyfe Agents operate at a computational cost 10-100 times lower than existing alternatives, which is a substantial improvement and could make these agents more accessible for real-world applications.",
            "4": "**Real-Time Responsiveness**: The paper emphasizes the importance of real-time interactions, which is crucial for applications involving human users.",
            "5": "The techniques proposed, such as limiting LLM queries and using fast, computationally-light processes, effectively address the latency issues.",
            "6": "**Comprehensive Evaluation**: The authors provide a thorough evaluation of Lyfe Agents in various multi-agent scenarios within the LyfeGame 3D virtual environment.",
            "7": "The scenarios, including solving a murder mystery and deciding on school club memberships, are well-designed to test the agents' self-motivation and sociability.",
            "8": "**Detailed Analysis**: The paper includes detailed analyses of information transmission, opinion change, and the impact of different architectural components through ablation studies.",
            "9": "These analyses provide valuable insights into the functioning and effectiveness of the proposed techniques.",
            "10": "**Scalability**: The ability of Lyfe Agents to operate efficiently in scenarios with varying numbers of agents (3, 6, 9) demonstrates their scalability.",
            "11": "This is an important aspect for applications involving large numbers of interacting agents.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Interaction Modalities**: While the paper focuses on natural language interactions, it does not explore other interaction modalities such as vision or physical actions in the 3D environment.",
            "14": "Incorporating these aspects could provide a more comprehensive evaluation of the agents' capabilities.",
            "15": "**Lack of Standardized Benchmarks**: The evaluation is conducted using custom scenarios within the LyfeGame environment.",
            "16": "While these scenarios are well-designed, the absence of standardized benchmarks makes it difficult to compare the performance of Lyfe Agents with other state-of-the-art generative agents.",
            "17": "**Complexity of Implementation**: The proposed techniques, while innovative, add complexity to the implementation of the agents.",
            "18": "The hierarchical action selection, self-monitoring, and memory mechanisms require careful tuning and integration, which might pose challenges for practitioners looking to adopt these methods.",
            "19": "**Generalization to Other Domains**: The paper focuses on social interactions in a virtual environment.",
            "20": "It is not clear how well the proposed techniques would generalize to other domains or applications, such as task-oriented dialogue systems or real-world robotics.",
            "21": "**Evaluation Metrics**: The success rates and probabilities used to evaluate the agents' performance are informative, but additional metrics such as user satisfaction, engagement, and qualitative assessments of the agents' behavior could provide a more holistic evaluation.",
            "22": "**Human-Agent Interaction**: While the paper mentions real-time interactions with humans, it does not provide detailed results or user studies involving human participants.",
            "23": "Such studies would be valuable to assess the agents' effectiveness and user experience in real-world settings.",
            "24": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the development of cost-effective, real-time generative agents for social interactions.",
            "25": "The brain-inspired techniques are innovative and address key challenges in the field.",
            "26": "However, the paper could benefit from exploring additional interaction modalities, incorporating standardized benchmarks, and providing more detailed evaluations involving human participants.",
            "27": "Despite these limitations, the contributions of this work are substantial and have the potential to impact the design and deployment of autonomous generative agents in various applications."
        },
        "xjW141wtk0": {
            "0": "- Tackles the problem of cost efficient generative agents and effective memory systems.",
            "1": "The topic is very relavant to the conference and as a field.",
            "2": "The paper is presented in a simpe manner, and the presented material is easy to understand.",
            "3": "- The economic value seems like a valuable topic to explore, since agents should not need a heavy LLM to perform low level actions, which is very expensive\n- provided clear advantage over generative agents: cost, which is also significant reduction - The paper uses mainly people in the field already practice, e.g., reflecting on top of reflections in generative agents is similar to the self-monitoring in lyfe agents, and time as a inverse importance metric in generative agents is similar to summary-then-forget\n- paper is missing alot of key implementation details in order to reproduce, see questions section for a list of them\n- the environment seem like a navigatable chatroom (only walk/talk actions), seems a bit simplified to incentivize more complex social behaviors"
        },
        "FUZ77x6kwl": {
            "0": "**Novelty**: The specific architecture of the generative agents developed in this work appears novel, although I am not an expert in this particular field.",
            "1": "**Quality**: The development of a comprehensive framework for social interactions by multi-generative agents likely required a significant engineering effort.",
            "2": "From an engineering perspective, the quality of the proposed work is high.",
            "3": "Each technical component of the proposed agent is thoroughly evaluated in the ablation study.",
            "4": "**Clarity**: The paper is mostly well-written and easy to understand.",
            "5": "Although many technical implementation details are omitted from the main sections, this is acceptable for a conference paper submission with a page limit.",
            "6": "**Significance**: The demonstrated results appear significant (although I'm not certain if they truly outperform other work, as discussed in the next section.)",
            "7": "While the proposed work seems to be of high quality from an engineering perspective, its significance as a conference paper is somewhat limited in its current form, due to what I perceive as insufficient experimental evaluation.",
            "8": "Here are my concerns:\n- Although I understand that all the components of the proposed agent contribute to task success rates, it's unclear how crucial they are for the agent to be \"cost-effective\", which was the original motivation of this work as stated in the introduction.",
            "9": "- While the proposed agent is performant, it's unclear if this performance is consistent across various base LLMs.",
            "10": "The experiment appears to use GPT-3.5.",
            "11": "What would happen if we use different LLMs such as Alpaca and GPT4ALL, or LLMs with various model capacities?",
            "12": "Without this information, it remains unclear whether using GPT-3.5 is a crucial requirement or not.",
            "13": "Also, from the perspectives of reproducibility and transparency, using open-sourced LLMs would be more desirable.",
            "14": "- The cost analysis was not very convincing.",
            "15": "Currently, \"cost per agent per human hour\" is reported.",
            "16": "However, for tasks with specific goals such as murder mystery, isn't it more essential to show the cost required until the task is successfully completed?",
            "17": "Otherwise, agents with very low throughput (due to, say, insufficient compute resources) will be evaluated as low cost, which I don't believe this paper intends to argue.",
            "18": "Furthermore, reporting costs in USD seems unhelpful.",
            "19": "This would result in running open-sourced models in a region with cheap electricity as an optimal choice.",
            "20": "What happens if the service provider changes API usage prices in the future?",
            "21": "Why not just report the number of interactions or number of tokens consumed until task completions?"
        },
        "mIER9ZndTm": {
            "0": "The pipeline in Figure 2 is clear and interesting.",
            "1": "The paper is well-written and presents its ideas in a clear, concise manner.",
            "2": "This paper is difficult to regard as a research paper, or even as a technical report.",
            "3": "It more closely resembles a manual for implementing an LLM-based agent framework in two scenarios designed by the authors.",
            "4": "The logic of this paper is not rigorous.",
            "5": "The authors claim that existing LLM-based agents are redundant in leveraging fewer queries to empower the agents.",
            "6": "However, the problem they address is not convincing.",
            "7": "The authors treat the redundancy issue as common knowledge within the community, which lacks objectivity.",
            "8": "The overall logic of this paper is unclear.",
            "9": "What motivates the proposal of the three modules for Lyfe Agents, and how do these modules differ from those in other LLM-based agents?",
            "10": "The improvements offered by these modules are difficult to discern.",
            "11": "The authors do not fairly compare Lyfe Agents with existing LLM-based agents.",
            "12": "The paper's most significant claim pertains to the 'low-cost' agents depicted in Figure 6.",
            "13": "However, the authors should attempt to keep all variables consistent when comparing the queries of LLMs.",
            "14": "Comparing generative agents with Lyfe Agents in different scenarios is unjust!",
            "15": "Also, the lyfe agents should compare with other general social simulation agents such as [1,2].",
            "16": "There are no objective evaluation metrics for Lyfe Agents other than cost per hour (which, as stated in point 3, is an unfair comparison).",
            "17": "The police success rate shown in Figure 3b is absurd.",
            "18": "It is based on a self-designed script with predetermined and clear answers, which is not convincing and should not serve as an evaluation metric in a scientific paper.",
            "19": "The proposed framework in figure 2 should be a general LLM-based agents framwork.",
            "20": "The lyfe agent should be evaluated in other well-know scenarios such as games and generative agents [3,4].",
            "21": "Generative agents [3] propose reflection to summarize the high-level memory.",
            "22": "What is the difference between the proposed memory mechanism and the one in generative agents [3]?",
            "23": "[1] Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen.",
            "24": "Agentsims: An open-source sandbox for large language model evaluation.",
            "25": "arXiv preprint arXiv:2308.04026, 2023.",
            "26": "[2] Lei Wang, Jingsen Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, and Ji-Rong Wen.",
            "27": "Recagent: A novel simulation paradigm for recommender systems.",
            "28": "arXiv preprint arXiv:2306.02552, 2023.",
            "29": "[3] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein.",
            "30": "Generative agents: Interactive simulacra of human behavior.",
            "31": "In In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST ’23), UIST ’23, New York, NY, USA, 2023.",
            "32": "Association for Computing Machinery.",
            "33": "[4] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.",
            "34": "Voyager: An open-ended embodied agent with large language models.",
            "35": "arXiv preprint arXiv:2305.16291, 2023."
        },
        "d3q8pxHDxw": {
            "0": "- I like how the authors have grounded their three new modules in brain-inspired research.",
            "1": "All three of the modules appear to be valid and useful in terms of improving performance of the agents, and I appreciate that they are also designed in order to be cost effective.",
            "2": "- I think the scenarios that the authors have developed in order to evaluate their agents are smart and well-designed, and useful for picking out interesting emergent behaviours amongst LLM agents.",
            "3": "In particular the murder mystery scenario, and the corresponding ablation that demonstrates the importance of all of the proposed modules.",
            "4": "I have two main problems with the paper that, in my opinion, need to be addressed.",
            "5": "1) Whilst the intentions and role of all of the modules are well explained, the details of their implementations are not particularly well explained within the main body of the text.",
            "6": "For example, take the option-action selection module: what are the implementation details for the LLM call that is used to output an options along with a subgoal?",
            "7": "What are the available options, is this defined by the LLM or a discrete list provided beforehand?",
            "8": "What are the details of the exit conditions that lead to the option-action selection module being called?",
            "9": "Whilst there are some more details included in the appendix, I believe some more important implementation points should be included in the main body of the text.",
            "10": "2) I am not currently convinced by the cost analysis that is provided by the authors.",
            "11": "This is particularly important as one of the key-selling points of the framework, according to the authors, is the cost effectiveness of the framework.",
            "12": "I am not disputing the project cost of the Stanford GenAgent work.",
            "13": "However, I just have a few queries about the cost provided for the Lyfe Agent.",
            "14": "Firstly, is the difference in cost primarily coming from the fact that GPT3.5 is used for LyfeAgent vs. GPT-4 for Stanford GenAgent?",
            "15": "In my opinion, this isn't necessarily a fair comparison - instead would it not be fairer to evaluate more in terms of tokens inputted / outputted when solving the scenarios?",
            "16": "For example, how many tokens need to be inputted / outputted by LyfeAgent vs. Stanford GenAgent in order to arrive at a Murder mystery solution?",
            "17": "In my opinion this is a fairer comparison."
        },
        "QuCG0BSAP0": {
            "0": "•\tAgent “Brain”: The authors combine and enhance different modeling techniques that are motivated by the human brain.",
            "1": "This approach shows promising results in the simulation of emergent social behavior.",
            "2": "I want to highlight the combination of the self-observation and the dynamic memory system.",
            "3": "In combination with the retrieval process based on vector similarity, I find this a fair contribution to the conference.",
            "4": "•\tMultiple Experiments + Ablation: The work contains three experiments covering different domains of collaboration and conversation.",
            "5": "The results show that the agents successfully manage all scenarios.",
            "6": "In addition, the ablation setup provides insights into the importance of each submodule in the larger context.",
            "7": "Thus, I find the results sound in terms of technical claims and experimentation setup.",
            "8": "•\tImplementation of the virtual world: I see no benefit in highlighting the 3D environment work this work contribution.",
            "9": "The interaction is purely text-based, and the agents do not perceive their visuals as image inputs.",
            "10": "Thus, the work could be reduced to a text-based role-play scenario.",
            "11": "•\tAppendix Structure: I found relevant information about the agent architecture missing in the content but given inside the appendix.",
            "12": "Thus, moving relevant information into the body of the paper would emphasize the main contribution.",
            "13": "•\tReliance on GPT3.5: Like previous agent-based work (Park et al.",
            "14": "), the observations and consequences are limited to a singular foundation model.",
            "15": "As this work employs complex memory and decision mechanisms, the behavior of competitive models like Llama-2 70B or Falcon 180B, when prompted with this approach, may strengthen impact."
        }
    },
    "w49jlMWDSA": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces GIST, a novel method for generating fine-grained, image-specific text descriptions from image-only datasets.",
            "1": "This is a significant contribution as it addresses the challenge of fine-tuning vision-language models in domains where paired image-text data is scarce.",
            "2": "**Comprehensive Evaluation**: The authors evaluate GIST across four diverse fine-grained classification datasets, demonstrating its effectiveness in both full-shot and few-shot scenarios.",
            "3": "The consistent improvement in accuracy over state-of-the-art methods highlights the robustness of the proposed approach.",
            "4": "**Detailed Methodology**: The paper provides a thorough explanation of the GIST methodology, including the use of large language models (LLMs) for generating diverse text descriptions and the use of vision-language models to match these descriptions to images.",
            "5": "This detailed exposition helps in understanding the underlying mechanisms and the rationale behind the design choices.",
            "6": "**Strong Baseline Comparisons**: The authors compare GIST against several strong baselines, including CLIP, LaBo, CaFo, and FLYP.",
            "7": "This comprehensive comparison provides a clear picture of where GIST stands in relation to existing methods.",
            "8": "**Qualitative Analysis**: The paper includes qualitative examples that illustrate the effectiveness of GIST in generating concise and relevant text descriptions.",
            "9": "This helps in understanding the practical impact of the proposed method.",
            "10": "**Ablation Studies**: The authors conduct ablation studies to analyze the impact of different design choices, such as the number of matched captions and the length of the captions.",
            "11": "This adds depth to the evaluation and provides insights into the factors that influence the performance of GIST.",
            "12": "**Contribution of a New Dataset**: The paper introduces Fitzpatrick40, a cleaned subset of the Fitzpatrick17k dataset, which is a valuable contribution to the community.",
            "13": "This dataset can be used for further research in fine-grained classification tasks.",
            "14": "#### Weaknesses\n\n1.",
            "15": "**Dependence on LLMs**: The method relies heavily on the capabilities of large language models like GPT-3 and GPT-4.",
            "16": "While these models are powerful, they are also resource-intensive and may not always be accessible to all researchers.",
            "17": "Additionally, the paper mentions that GPT does not always output accurate information, which could limit the applicability of GIST in certain domains.",
            "18": "**Manual Checking Requirement**: The paper acknowledges that some manual checking is required to ensure the accuracy of the generated text descriptions.",
            "19": "This introduces a level of human intervention that could be a bottleneck, especially for large-scale applications.",
            "20": "**Limited Analysis on Generalization**: While the paper demonstrates the effectiveness of GIST on four datasets, it would be beneficial to see more analysis on how well the method generalizes to other fine-grained classification tasks.",
            "21": "This could include experiments on additional datasets or different types of fine-grained categories.",
            "22": "**Complexity of Implementation**: The proposed method involves multiple steps, including prompting LLMs, matching text descriptions with images, and summarizing the matched text.",
            "23": "This complexity might make it challenging for practitioners to implement and reproduce the results without detailed guidance and code.",
            "24": "**Evaluation Metrics**: The paper primarily focuses on top-1 accuracy as the evaluation metric.",
            "25": "While this is a standard metric, additional metrics such as precision, recall, and F1-score could provide a more comprehensive evaluation of the method's performance, especially in imbalanced datasets.",
            "26": "**Impact of Caption Length**: The paper mentions that summarizing the matched text improves performance, but it would be useful to have a more detailed analysis of how different lengths of captions impact the classification accuracy.",
            "27": "This could help in understanding the trade-offs between information richness and conciseness.",
            "28": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of fine-grained image classification by introducing GIST, a method for generating image-specific text descriptions.",
            "29": "The strengths of the paper lie in its innovative approach, comprehensive evaluation, and detailed methodology.",
            "30": "However, the reliance on large language models, the need for manual checking, and the complexity of implementation are notable weaknesses.",
            "31": "Addressing these issues in future work could further enhance the applicability and impact of GIST."
        },
        "NndbacTh7J": {
            "0": "+ The proposed method is simple, straightforward, and sound.",
            "1": "+ The paper is clearly written and easy to understand.",
            "2": "+ The proposed method achieves better results than comparable methods.",
            "3": "- The proposed method requires domain (dataset) specific designs such as prompts for LLM.",
            "4": "However, in a few-shot setting, it is hard to determine the designs because of the scarcity of validation and test sets.",
            "5": "This makes the proposed method less general to real-world few-shot tasks.",
            "6": "- The paper lacks of comparison with other fine-tuning techniques such as prompt tuning approaches.",
            "7": "Please analyze and compare with other families of VL finetuning.",
            "8": "- If I understood correctly, the proposed method requires full images for the target domain.",
            "9": "In a real-world setting, it may be hard to obtain such images, especially for medical images.",
            "10": "- Why is a summary is required?",
            "11": "We can see the performance improvement by doing this, but I found no proper explanations for why.",
            "12": "- Manual cleaning mentioned in Section 5 seems not fancy.",
            "13": "In a specific field, manual cleaning requires an expensive effort of experts such as the medical field.",
            "14": "In addition, instead of manually cleaning, we can make labels for more images under the same effort, which is likely to produce much more improvements than manual cleaning."
        },
        "CgNbobrj43": {
            "0": "(1) The motivation of prompting LLM for fine-grained classification is well presented.",
            "1": "(2) The explanations and illustrations of the three-step workflow is well-formulated and mostly clear.",
            "2": "(1) The motivation of prompting LLM for visual classification is not that novel to the community and the authors listed (Maniparambil et al., 2023) as an example.",
            "3": "(2) The contribution is limited since the core idea is the same as typical data labeling workflow that uses LLM.",
            "4": "More specifically, during prompting LLM, the user-provided prompt is not coming for free, it is also a kind of human knowledge or preference prior.",
            "5": "There is no discussion on this difficulty of prompt preparation as compared to typical easy prompt such as “a photo of class name”.",
            "6": "Thus, the workflow of generating image-text pairs has no difference from data labeling workflow that uses LLM.",
            "7": "(3) In the experiments, Table 1, the proposed workflow which uses GPT-3, CLIP and intricate prompts, does not get significant improvement over FLYP which uses CLIP and a typical easy prompt, though the comparison setting is unfair.",
            "8": "(4) The most conflicting method (Maniparambil et al., 2023) is not compared in the experiments."
        },
        "v1Oe5Vuj0z": {
            "0": "This paper presents a new method named GIST for fine-grained classification.",
            "1": "The proposed method adopts a large language model to generate domain-specific class descriptions and matches the texts and images with a pre-trained vision-language model.",
            "2": "This paper trains a CLIP classifier with the generated and matched descriptions.",
            "3": "The proposed GIST achieves good results on several benchmarks with full/few-shot settings.",
            "4": "I'm concerned about the technical contribution of this paper.",
            "5": "Using a large language model to generate/augment text for CLIP training has been explored in several works [1,2].",
            "6": "This paper lacks the ablations about matching texts and images.",
            "7": "It's unclear whether the matching based on a pre-trained CLIP will impact the downstream tasks.",
            "8": "In addition, I'm concerned about how many captions are matched to one image and whether more captions will help.",
            "9": "This paper lacks studies on the impact of fine-tuning CLIP and whether more data (images) will further improve.",
            "10": "[1] Fan et.al.",
            "11": "Improving CLIP Training with Language Rewrites.",
            "12": "NeurIPS 2023.",
            "13": "[2] Maniparambil et.al.",
            "14": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts.",
            "15": "ICCVW 2023."
        }
    },
    "L4nOxziGf9": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel framework, REPARE, which addresses the issue of underspecification in vision-language tasks by rephrasing and augmenting questions.",
            "1": "This is a significant contribution as it leverages the strengths of large vision-language models (LVLMs) without requiring additional training data or fine-tuning.",
            "2": "**Empirical Validation**: The authors provide extensive empirical evidence demonstrating the effectiveness of REPARE.",
            "3": "The framework shows substantial improvements in zero-shot accuracy across multiple datasets (VQAv2, A-OKVQA, and VizWiz) and different LVLM architectures (BLIP-2, MiniGPT-4, and LLaVA-1.5).",
            "4": "**Comprehensive Analysis**: The paper includes a thorough analysis of the design choices within REPARE, such as the importance of rationales, captions, and question entities.",
            "5": "This helps in understanding the contribution of each component to the overall performance.",
            "6": "**Complexity Metrics**: The use of complexity metrics like average dependency distance (ADD) and idea density (ID) to quantify the syntactic and semantic complexity of the rephrased questions is a strong point.",
            "7": "It provides a clear indication that REPARE effectively reduces underspecification.",
            "8": "**Asymmetric Strength Utilization**: The framework smartly leverages the asymmetric strengths of LVLMs, particularly the strong reasoning and planning abilities of the LLM components.",
            "9": "This is a practical approach given the current state of LVLMs.",
            "10": "**Oracle Setting**: The inclusion of an oracle setting to establish an upper bound for REPARE's performance is insightful.",
            "11": "It shows the potential maximum improvement that can be achieved, providing a benchmark for future work.",
            "12": "**Public Code Availability**: The authors have made their code publicly available, which promotes transparency and allows other researchers to replicate and build upon their work.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Cost of Implementation**: One of the main limitations of REPARE is the increased computational cost due to the generation and evaluation of multiple question candidates.",
            "15": "This linear scaling with the number of candidates can be a significant overhead, especially for large-scale applications.",
            "16": "**Dependency on LVLMs' Capabilities**: The framework heavily relies on the existing capabilities of LVLMs for generating rationales and captions.",
            "17": "If the underlying LVLMs are not robust in these tasks, the performance of REPARE could be adversely affected.",
            "18": "**Redundancy in Generated Questions**: While the added complexity in rephrased questions can improve model performance, it may also introduce redundancy that is not always necessary for human understanding.",
            "19": "This could potentially lead to inefficiencies in practical applications.",
            "20": "**Limited Scope of Evaluation**: The evaluation is primarily focused on VQA tasks.",
            "21": "It would be beneficial to see how REPARE performs on other vision-language tasks, such as image captioning or visual dialog, to understand its broader applicability.",
            "22": "**Potential for Language Bias**: The paper acknowledges that REPARE questions might leverage language biases inherent in the datasets.",
            "23": "While the authors argue that the information is sourced from the image, this aspect could still be a concern and warrants further investigation.",
            "24": "**Manual Verification**: The manual verification of a subsample of generated questions to check for malicious or offensive content is a good practice, but it is not scalable.",
            "25": "Automated methods for ensuring the appropriateness of generated content would be more practical for large-scale deployment.",
            "26": "**Generalization to Future Models**: The paper notes that the asymmetric strength of LVLMs is not a permanent feature and future models might have more balanced components.",
            "27": "This could limit the long-term applicability of REPARE as it is currently designed.",
            "28": "#### Conclusion\n\nOverall, the paper presents a significant advancement in addressing underspecification in vision-language tasks through the REPARE framework.",
            "29": "The strengths of the approach, including its innovative use of LVLMs' capabilities and comprehensive empirical validation, outweigh the weaknesses.",
            "30": "However, addressing the limitations related to computational cost, dependency on LVLMs, and potential language bias would further enhance the robustness and applicability of the framework."
        },
        "wKRcZg36pl": {
            "0": "- The motivation is straight-forward, clear, and reasonable.",
            "1": "- The improvements seem solid and the analyses support the improvements that come from the proposed pipeline.",
            "2": "- The code and the data are provided and the authors promised public release.",
            "3": "This is very important because the proposed pipeline is complicated and not easily reproducible.",
            "4": "- The pipeline seems overcomplicated and involves many steps that are indispensable to the overall performance.",
            "5": "Unlike plain CoT, which usually conducts inference once, the proposed pipeline conducts inference multiple times using the LVLM and involves off-the-shelf tools twice.",
            "6": "The complexity may affect the reproduction of the method and the incurred (computation and time) cost may hinder the adoption of the method in application.",
            "7": "- The main results (Table 1) need more explanation.",
            "8": "(1) For example, the standard deviation considering the oracle implementation is high.",
            "9": "I did not expect that using the optimal candidate would lead to higher variance.",
            "10": "Are there results regarding the choice of the number of the question candidates?",
            "11": "(2) I would love to see more QA datasets (from diverse sources) tested on.",
            "12": "(3) I wonder if the asymmetric strength hypothesis holds, is it possible that stage 2 and stage 3 can be changed to using the original question and the extracted details in texts without the image?",
            "13": "(4) All analyses are based on BLIP-2, an encoder-decoder model.",
            "14": "I don't think I find a discussion on the effect of model architecture (encoder-decoder or decoder-only).",
            "15": "- The writing and the organization can be improved.",
            "16": "Personally, I would like Section 3 Methodology to be more straightforward.",
            "17": "From what I understand, Stage I (ii) adopts different post-processing from (i) and (iii).",
            "18": "The paragraph before Section 3.2 states \"we prompt the LVLM to answer each question\" and the first paragraph in Section 3.2 states \"To select which question to answer\", which are contradictory.",
            "19": "I had to check the appendix and the footnotes multiple times to guess what's going on."
        },
        "hcbaENDxKB": {
            "0": "The performance improvement from the method seem to be solid.",
            "1": "The paper is mostly clear with extensive experiments.",
            "2": "Some of the text requires further clarification.",
            "3": "The underlying hypothesis should be stated more clearly, which in my understanding is that distribution of more specified questions are more aligned with the training data of LVLMs and the answers with higher confidence are more likely to be correct."
        },
        "ndBQp2mfUP": {
            "0": "- The paper is easy to read and generally well-written\n- Interesting idea of improving VL models in VQA tasks by just modifying one modality (e.g text).",
            "1": "- Improvements over baselines (BLIP2, MiniGPT4) looks reasonable.",
            "2": "- Evaluation suite should be improved.",
            "3": "For example including: TextVQA, VizWiz.",
            "4": "Additionally, authors should consider evaluating tasks such as HatefulMemes which might be more challenging to the proposed approach.",
            "5": "Also, consider recent evaluation tasks such as MME.",
            "6": "- Including recent instruct multimodal models (e.g.",
            "7": "LLaVa, Qwen-VL) would be an interesting experiment to see if the gain with Repare is still relevant in these models.",
            "8": "- Minor:\n    - >One approach involves additional VL pretraining ...",
            "9": "In this case, LENS does not involve additional multimodal pretraining.",
            "10": "- Why not include BLIP2-XXL as one of your main baselines and improve it with Repare?"
        }
    },
    "9F0xInGNBF": {
        "llm_review": {
            "0": "**Innovative Framework**: The proposed framework, VideoPrompter, introduces a novel approach by combining pre-trained discriminative vision-language models (VLMs) with generative video-to-text and text-to-text models.",
            "1": "This dual enhancement of both visual and textual features is a significant advancement over existing methods that typically focus on one modality.",
            "2": "**Comprehensive Evaluation**: The paper evaluates the framework across three different zero-shot settings: video action recognition, video-to-text and text-to-video retrieval, and time-sensitive video tasks.",
            "3": "This comprehensive evaluation demonstrates the versatility and robustness of the proposed method.",
            "4": "**Consistent Improvements**: The results show consistent improvements across multiple benchmarks and various VLMs, including CLIP, ViFi-CLIP, Action-CLIP, and AIM.",
            "5": "This indicates the generalizability and effectiveness of the proposed framework.",
            "6": "**Class-Specific Prompts and High-Level Action Context**: The introduction of class-specific prompts and the creation of a Tree Hierarchy of Categories for class names provide additional semantic context, which enhances the descriptiveness of class labels and improves classification performance.",
            "7": "**Interpretability**: The framework offers interpretability by showing the contribution of individual attributes to the model's decision-making process.",
            "8": "This is a valuable feature for understanding and debugging model predictions.",
            "9": "**Plug-and-Play Module**: The framework is designed as a plug-and-play module, making it adaptable to various existing VLMs without requiring extensive modifications or retraining.",
            "10": "**Reproducibility**: The authors provide sufficient details for reproducing the experiments, and they commit to releasing the code upon publication, which is crucial for the research community to validate and build upon their work.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Complexity and Computational Overhead**: While the framework achieves significant improvements, it introduces additional computational complexity by combining multiple models (video-to-text and text-to-text).",
            "13": "The paper does not provide a detailed analysis of the computational overhead and how it compares to existing methods.",
            "14": "**Dependence on Pre-trained Models**: The framework heavily relies on the availability and performance of pre-trained models like GPT-3.5 and VideoChatGPT.",
            "15": "Any limitations or biases in these models could propagate through the framework, potentially affecting the results.",
            "16": "**Limited Analysis of Failure Cases**: The paper primarily focuses on the improvements and successes of the proposed method.",
            "17": "A more detailed analysis of failure cases and scenarios where the framework does not perform well would provide a more balanced view and help identify areas for further improvement.",
            "18": "**Scalability to Larger Datasets**: While the framework shows improvements on several benchmarks, it is not clear how well it scales to larger and more diverse datasets.",
            "19": "The paper could benefit from additional experiments on larger datasets to demonstrate scalability.",
            "20": "**Impact of Hyperparameters**: The paper mentions the use of different temperature settings for generating diverse video textual descriptions but does not provide a thorough analysis of the impact of these hyperparameters on the overall performance.",
            "21": "A more detailed study on the sensitivity of the framework to various hyperparameters would be beneficial.",
            "22": "**Comparison with State-of-the-Art**: Although the framework shows improvements over several methods, a more extensive comparison with the latest state-of-the-art methods, especially those that do not require fine-tuning, would strengthen the claims of the paper.",
            "23": "#### Conclusion:\n\nOverall, the paper presents a significant contribution to the field of zero-shot video understanding by introducing a novel framework that enhances both visual and textual features using pre-trained generative models.",
            "24": "The comprehensive evaluation and consistent improvements across multiple benchmarks demonstrate the effectiveness of the proposed method.",
            "25": "However, the paper could benefit from a more detailed analysis of computational overhead, failure cases, scalability, and hyperparameter sensitivity.",
            "26": "Despite these weaknesses, the framework shows great promise and provides a solid foundation for future research in this area."
        },
        "EYpZBP60Ni": {
            "0": "The experiments and ablation studies are conducted comprehensively, validated on different pre-existing architectures, and taken into account various types of video data.",
            "1": "Employing the video-to-text model (not limited to VGPT and caption models) is novel and worthy to explore in the video field.",
            "2": "Fusing the text and video representations is depicted to be beneficial in bridging the gap between video and textual labels in the embedding space.",
            "3": "The manuscript provides a detailed explanation and examples of prompting the GPT to refine the simple textual label, which in turn enhances reproducibility.",
            "4": "In the 'video-to-text guided visual feature enhancement' (section 2.2), the adopted VGPT relies on CLIP-ViT-L and vicuna, where the computational cost of performing multiple inferences (including text embedding and filtering) far exceeds that of the basic video understanding model.",
            "5": "This limits the practical value of the proposed approach.",
            "6": "Except for CLIP, an image-language pre-trained model targeted specifically for the image field, the proposed approach shows relatively limited performance gain in other video-based models (ViFi-CLIP, AIM, ActionCLIP), considering the additional computational requirements.",
            "7": "The configurations of adopted pre-trained models (AIM, ActionCLIP, …) remain unclear, which datasets are these models pre-trained on (e.g.",
            "8": "K400, K700, …)?",
            "9": "For AIM, do the authors directly remove the classification layers?"
        },
        "7IhGy5eD3U": {
            "0": "This paper studies an important problem of adapting pre-trained vision-language models to downstream tasks in zero-shot settings.",
            "1": "The introduced method is lucid and holds promise for extension across a wide range of VLMs.",
            "2": "The experimental results look good.",
            "3": "VideoPrompter is able to increase the zero-shot performance of VLMs across multiple tasks.",
            "4": "The paper is well-presented.",
            "5": "The efficiency of VideoPrompter hasn't been thoroughly examined.",
            "6": "Given that VideoPrompter appears to require generating 10 times the number of samples and the use of an additional text-to-video model, it could substantially raise the inference costs, both for evaluating existing VLMs and in practical applications.",
            "7": "The selection of Video-ChatGPT as the video-to-text model seems arbitrary.",
            "8": "Alternative models, such as Video-LLaMA [A], should be considered and discussed.",
            "9": "An ablation study on the video-specific language descriptors is missing.",
            "10": "[A] Zhang, H., Li, X., & Bing, L. Video-llama: An instruction-tuned audio-visual language model for video understanding.",
            "11": "arXiv preprint arXiv:2306.02858."
        },
        "Gt5qdFFKFi": {
            "0": "The studied problem is interesting.",
            "1": "Video understanding with large foundation models is of wide interest in the community.",
            "2": "The authors put together state-of-the-art large foundation models and improve the zero-shot inference performance on video understanding tasks.",
            "3": "The idea of generating more descriptions for class names and using high-level context is not new in prompting large foundation models (e.g.",
            "4": "the prior works cited in this paper).",
            "5": "This is model ensembling for enhancing zero-shot performance.",
            "6": "Can the authors justify the main novelty of this paper?",
            "7": "VGPT is used to generate the text description of the query video, and which is then converted to an image-like text embedding.",
            "8": "Why not just prompting VGPT for the downstream applications (e.g.",
            "9": "action classification)?",
            "10": "Comparison to this baseline is an important justification to the proposed method.",
            "11": "Several components are added to the solution, while the ablations are not sound enough.",
            "12": "For example, how important are the three description types (parent context, language attributes, and language descriptions)?",
            "13": "The claim for the comparison to CUPL (Pratt et al., 2022) is not very clear (section 3.1.4).",
            "14": "The authors claim that VideoPrompter only requires 3 text descriptions instead of 50 descriptions adopted in CUPL.",
            "15": "However, VideoPrompter adopts a VGPT model while CUPL does not.",
            "16": "Is using VGPT a better choice in terms of the cost?",
            "17": "The paper criticizes prior work that “these methods require access to the true distribution of the target task, which can be prohibitive in test-time adaptation and data-scarce environments”.",
            "18": "However, the proposed method optimizes the selection of hyperparameters (e.g.",
            "19": "temperature) directly on the target dataset (see Figure 3).",
            "20": "The high-level action context is restricted to a tree-type relation.",
            "21": "However, some child classes may belong to multiple parent concepts.",
            "22": "For example, “surfing” can belong to both “playing sports” and “water activities”."
        },
        "CsiiZNAJmc": {
            "0": "- The paper presents a set of experiments on various problem settings: action recognition, video-to-text and text-to-video retrieval, time-sensitive tasks and on different datasets.",
            "1": "- The ablations are solid and thorough.",
            "2": "- Experiments show strong improvement w.r.t baselines.",
            "3": "- Since at least 3 foundation models have been used (CLIP, GPT, VGPT), how do we know if those models are trained with examples overlapped with the downstream datasets (e.g., HMDB-51, UCF101, SSv2, K400, MSR-VTT, Charades).",
            "4": "- The novelty seems moderate if not low.",
            "5": "As the paper mentions the main contributions are 1) introducing video-to-text to enhance visual embeddings and 2) applications to videos.",
            "6": "- The written presentation could be further improved:\n     1) section 2.1 could be renamed to \"Overview\" and try to capture the big picture of the framework.",
            "7": "The author(s) can refer back to Fig.",
            "8": "1 for the big picture (in the current flow of presentation, there is no big picture and it flows in with overwhelming many details and notations).",
            "9": "Then sections 2.2 and 2.3 can be further followed up from 2.1 to provide detailed of components.",
            "10": "2) table 6 is presented in page 7, yet never been referred from the text?"
        }
    },
    "QXRScRrwNr": {
        "llm_review": {
            "0": "**Comprehensive Analysis**: The paper provides a thorough analysis of the state-of-the-art ATLAS model, identifying its limitations in in-context learning.",
            "1": "This sets a solid foundation for the proposed improvements.",
            "2": "**Novel Contributions**: The introduction of RAVEN, which combines retrieval-augmented masked language modeling and prefix language modeling, is a significant contribution.",
            "3": "The paper also introduces Fusion-in-Context Learning (FiCL) to enhance few-shot performance, which is innovative and practical.",
            "4": "**Empirical Validation**: The extensive experiments conducted demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to larger models like GPT-3 and PaLM, despite having fewer parameters.",
            "5": "This is a strong validation of the proposed methods.",
            "6": "**Practical Implications**: The proposed methods, especially FiCL, are designed to be implemented without additional training or model modifications, making them highly practical for real-world applications.",
            "7": "**Detailed Methodology**: The paper provides a clear and detailed description of the methodologies used, including the prompting strategies, the combination of language modeling techniques, and the implementation of FiCL.",
            "8": "This transparency aids in reproducibility and understanding.",
            "9": "**Performance on Multiple Benchmarks**: The evaluation on diverse datasets, including Natural Questions, TriviaQA, and MMLU, showcases the versatility and robustness of RAVEN across different types of tasks.",
            "10": "**Future Research Directions**: The paper encourages further research in the direction of retrieval-augmented encoder-decoder language models, highlighting the potential for these models to be scaled up and applied to more specialized domains.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Limited Comparison with Other Models**: While the paper compares RAVEN with ATLAS and some other models, a more extensive comparison with a broader range of state-of-the-art models, especially those using different architectures or training paradigms, would strengthen the claims.",
            "13": "**In-Context Example Retrieval**: The paper suggests using RAVEN’s retriever for in-context example retrieval but does not provide a detailed analysis or ablation study on the impact of this component.",
            "14": "More empirical evidence on its effectiveness would be beneficial.",
            "15": "**Scalability Concerns**: Although the paper demonstrates the effectiveness of RAVEN with fewer parameters, it does not address potential scalability issues when applying the proposed methods to much larger models or datasets.",
            "16": "A discussion on computational efficiency and resource requirements would be useful.",
            "17": "**Generalization to Other Tasks**: The focus is primarily on open-domain question answering and MMLU.",
            "18": "It would be interesting to see how RAVEN performs on other NLP tasks, such as text generation, summarization, or dialogue systems, to better understand its generalization capabilities.",
            "19": "**Instruction Tuning**: The paper mentions that RAVEN could benefit from instruction tuning, similar to FLAN-T5, but does not explore this avenue.",
            "20": "Including experiments with instruction tuning would provide a more comprehensive evaluation of RAVEN’s potential.",
            "21": "**Complexity of Implementation**: While the methods are described as practical, the actual implementation of FiCL and the combination of different language modeling techniques might be complex.",
            "22": "A more detailed discussion on the implementation challenges and potential solutions would be helpful.",
            "23": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of retrieval-augmented encoder-decoder language models.",
            "24": "The introduction of RAVEN and FiCL, along with the comprehensive analysis and empirical validation, makes this work a valuable contribution.",
            "25": "Addressing the identified weaknesses, particularly through more extensive comparisons, detailed analysis of components, and exploration of additional tasks, would further strengthen the impact of this research."
        },
        "smCgUYVjKv": {
            "0": "Propose an interesting question “in-context learning in retrieval augmented language model”.",
            "1": "Decoder-only language models are good at in-context learning while hard to use retrieved information.",
            "2": "Encoder-decoder language models are good at retrieval augmented generation due to their separated encoder while unstable for in-context learning.",
            "3": "This paper aims to introduce in-context learning ability into encoder-decoder language models.",
            "4": "This paper is well-written and easy to follow.",
            "5": "The proposed method is well-motivated and very easy to implement.",
            "6": "Detailed experimental results on diverse datasets and the additional analysis can answer the main concern of motivation.",
            "7": "Although I admit that exploring the in-context learning (ICL) in retrieval-augmented language model is an interesting research question, I think the specific research points of this paper are still limited.",
            "8": "This paper only study how to make the language model perform better on question-answering task under the retrieval-augmented paradigm.",
            "9": "This is inconsistent with our accepted definition and expectation of ICL.",
            "10": "ICL enables the language model to learn and perform various tasks using a few input examples.",
            "11": "Just focusing on knowledge-intensive tasks such as question-answering cannot be called ICL.",
            "12": "It is more like few-shot learning for question-answering.",
            "13": "The technical contribution of this paper is limited.",
            "14": "- In order to bridge the gap between pre-training and inference in ATLAS, this paper introduces prefix language modeling.",
            "15": "It just puts the special token at the end of the texts.",
            "16": "Prefix language modeling is a well-known method in the pre-training of decoder-only language models such as GPT.",
            "17": "- In order to input more examples to the retrieval-augmented language model, this paper just feeds different examples to FID encoder with each passage.",
            "18": "The above two points are the core technical contribution of this paper which is limited.",
            "19": "This paper emphasizes the advantages of the encoder-decoder architecture model in retrieval-augmented compared to the decoder-only model, but its core technical contribution is to change the masked language modeling of the encoder model to the prefix language modeling of the decoder-only model.",
            "20": "This point is very contradictory.",
            "21": "Some decoder-only pre-trained language models with considerable size such as LLama-7B should be used as baselines.",
            "22": "Many papers show LLama-7B can achieve better performance in Table 3.",
            "23": "I hope the author can carefully analyze the advantages of the encoder-decoder architecture model compared to the decoder-only model in terms of retrieval-augment."
        },
        "gLxRpjl35S": {
            "0": "(1) The explored topic, combining in-context learning (ICL) and retrieval augmented generation (RAG) is very interesting.",
            "1": "(2) The paper is well-written, and the proposed idea is simple and easy to follow.",
            "2": "(3) The paper identifies two drawbacks of ATLAS on ICL, including (1) the mismatch between pretraining and testing and (2) the restricted context length, which makes sense to me.",
            "3": "(4) Compared with ATLAS, the performance is significantly improved.",
            "4": "(1) Though the proposed method has a significantly better few-shot performance than ATLAS, the work seems to be incremental compared with ATLAS in the aspect of the modeling.",
            "5": "The first method is to continuously pretrain ATLAS by masking the later part of the sequence.",
            "6": "The second method is to blend different limited in-context examples into different passages in the Fusion-in-Decoder architecture.",
            "7": "The whole architecture is exactly the same as ATLAS, which makes me feel the proposed method is more like the tricks for ATLAS for better ICL performance, including a fine-tuning method and a prompt engineering method.",
            "8": "(2) Though the proposed method outperforms ATLAS, it still cannot match decoder-only LLM as shown in Tables 3 and 5.",
            "9": "(I do raise questions for this comparison in the question section.)"
        },
        "CufqsO7GdG": {
            "0": "Innovative Approach: The introduction of Fusion-in-Context Learning (FiCL) to address the limitations of constrained context length in base models like T5 or ATLAS is a significant innovation.",
            "1": "This strategy improves scalability and performance in in-context learning scenarios.",
            "2": "Comparative Performance: RAVEN models demonstrate superior performance compared to other models like GPT-3, PaLM, Codex, and RETRO in tasks such as Natural Questions and TriviaQA, particularly in zero-shot and few-shot settings.",
            "3": "The paper demonstrates the effectiveness of the proposed method primarily in the context of encoder-decoder models.",
            "4": "However, its effectiveness in popular left-to-right language models, which are widely used, is not explicitly addressed.",
            "5": "This omission can limit the understanding of how the proposed method might perform or be adapted to these prevalent LMs such as LLaMA and GPT."
        }
    },
    "J7AwIJvR3d": {
        "llm_review": {
            "0": "**Novel Research Question**: The paper addresses a critical and underexplored question about the systematic differences between language models (LMs) and human brains in language processing.",
            "1": "This is a significant step forward from the existing literature that primarily focuses on the similarities.",
            "2": "**Use of MEG Data**: The use of Magnetoencephalography (MEG) data provides high temporal resolution, which is crucial for understanding the fine-grained temporal dynamics of language processing.",
            "3": "This is a notable improvement over studies that use fMRI, which has lower temporal resolution.",
            "4": "**Systematic Approach**: The paper employs a systematic approach to identify divergences between LMs and human brains.",
            "5": "The use of an LLM-based hypothesis proposer to automatically generate hypotheses about these divergences is innovative and reduces the bias that might come from manual hypothesis generation.",
            "6": "**Fine-Tuning Experiments**: The study demonstrates that fine-tuning LMs on specific datasets related to identified phenomena (emotion, figurative language, and physical commonsense) can improve their alignment with human brain responses.",
            "7": "This is a practical contribution that can guide future improvements in LM design.",
            "8": "**Comprehensive Analysis**: The paper provides a thorough analysis of the temporal and spatial patterns of MEG responses, validating the alignment of LMs with human brain activity.",
            "9": "The detailed comparison of fine-tuned models with the base model adds robustness to the findings.",
            "10": "**Reproducibility**: The authors have made the data and code available in an anonymized repository, which enhances the reproducibility of the study.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Limited Dataset for Fine-Tuning**: The datasets used for fine-tuning (Social IQa, Fig-QA, and PiQA) may not fully capture the complexity and diversity of the phenomena they represent.",
            "13": "For instance, the physical commonsense dataset might not align well with the fantasy elements in the Harry Potter text used for MEG data collection.",
            "14": "**Generalizability**: The study focuses on a specific narrative (Harry Potter) and a particular LM (GPT-2 XL).",
            "15": "While the methods can be extended to other models and texts, the findings might not generalize across different types of texts or more recent LMs like Llama-2.",
            "16": "**Annotation Process**: The annotation of the Harry Potter text for emotion, figurative language, and physical commonsense was done by three raters.",
            "17": "The paper does not provide details on the inter-rater reliability, which is crucial for ensuring the quality and consistency of the annotations.",
            "18": "**Hypothesis Validation**: While the LLM-based hypothesis proposer is innovative, the validation of these hypotheses relies on a relatively small dataset.",
            "19": "Some of the p-values for the top hypotheses are not statistically significant, which raises questions about the robustness of these findings.",
            "20": "**Limited Modalities**: The study focuses solely on the language modality.",
            "21": "Human language processing is inherently multimodal, involving visual, auditory, and motor information.",
            "22": "The paper acknowledges this limitation but does not explore multimodal fine-tuning, which could provide a more comprehensive understanding of the divergences.",
            "23": "**Temporal Selectivity**: The improvements in alignment for the fine-tuned models are temporally selective, primarily during language processing time windows.",
            "24": "This suggests that the fine-tuning might not fully capture the broader cognitive processes involved in language understanding.",
            "25": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the field by systematically exploring the divergences between LMs and human brains in language processing.",
            "26": "The use of MEG data, innovative hypothesis generation, and fine-tuning experiments are notable strengths.",
            "27": "However, the study has limitations related to the datasets used for fine-tuning, generalizability, and the focus on a single modality.",
            "28": "Future research should address these limitations by incorporating more diverse datasets, exploring multimodal fine-tuning, and validating findings across different texts and LMs."
        },
        "UsPEfRLk2J": {
            "0": "The overall approach is interesting.",
            "1": "The authors attempt to learn in a data-driven way stimuli that are poorly predicted by the model and then try to leverage these insights to improve the model.",
            "2": "This approach seems relatively novel and potentially promising.",
            "3": "The writing is generally clear and the approach was well motivated.",
            "4": "I don’t think the statistical procedure used to establish significance is sufficient and it is not described in enough detail to evaluate it.",
            "5": "In Appendix F, the authors say they permute the “data” using “random permutation”.",
            "6": "What is the data matrix?",
            "7": "Are they computing timepoints?",
            "8": "Computing timepoints is not valid as this destroys the temporal structure of the data.",
            "9": "In addition, the null hypothesis being tested is that there is no relationship between the response and either of the two predictors, which is not the correct null.",
            "10": "The correct null is that there is no difference in prediction between the two predictors (but both predictors explain real variance nonetheless).",
            "11": "The authors need a valid and clearly described statistical procedure to establish their main finding.",
            "12": "In the main figure of the paper, the authors should report a meaningful measure of effect size.",
            "13": "The fraction of significant channels does not give one a sense of the magnitude of the improvement both because significance does not reflect effect size and MEG channels are highly correlated, so it is not particularly impressive when all channels pass significance.",
            "14": "For example, the authors could plot the correlation between the measured and predicted response (as in the appendix scatter plots) for their base LM and a control model such as GloVE.",
            "15": "They could then first test whether their LM showed improved predictions compared with GloVE, which would be a good sanity check, and then could compare the improvement from their fine-tuned model with this increment.",
            "16": "I would also find it interesting to evaluate the performance improvement from fine-tuning with the differences between various LM models.",
            "17": "Could you get a similar improvement by using a slightly larger vs. smaller model or just training the LM model on a bigger language dataset?",
            "18": "The analyses used to motivate the fine-tuning metric is not compelling.",
            "19": "None of the descriptions are significant (which the authors acknowledge) and the procedures used to establish significance are not described.",
            "20": "Even if they were significant it is not clear how reliable the effects are.",
            "21": "For example, if you ran the procedure twice on different subjects would you get the same thing?",
            "22": "Moreover, the labels the authors give feel arbitrary and do not obviously (to me) capture the consensus from the proposed results.",
            "23": "I am not confident for example that if you showed this list to two sets of cognitive scientists that they would reliably yield the same categories (i.e., emotional, physical, figurative).",
            "24": "Moreover, the authors show that performance improves for 2 of the 3 tasks they select with weak improvements for one task.",
            "25": "Is there any reason to believe that this is better than what you would do if you had simply skipped the whole initial procedure and just tried to come up with any three cognitively relevant and distinct tasks?",
            "26": "I am not convinced that the data-driven approach, while interesting, adds value here.",
            "27": "There is no investigation of what it is about the emotion task that yields the best improvements.",
            "28": "For example, the emotion task has more training examples than the other tasks.",
            "29": "Perhaps, the emotion task is simply more difficult in some sense than the other tasks and training the network on harder tasks yields bigger improvements?",
            "30": "There is also no evaluation of whether the fine-tuned models perform well on their task on a different dataset from the one they are fine-tuned on.",
            "31": "If they do not generalize well to another dataset for the same task, it would not be surprising if they do not generalize well to neural predictions.",
            "32": "I am assuming for Figure 5 that the authors are reporting MSE improvement for the corresponding model, i.e.",
            "33": "when comparing emotion words with non-emotional words, they are using the emotion-trained model.",
            "34": "Please clarify this.",
            "35": "Assuming this is the case, it would be helpful to know if the effect is specific to the training task, i.e.",
            "36": "do you also see a boost for the emotional words when you train on the figurative task, and do you see a boost for figurative words when you train on the emotional task.",
            "37": "Please also clarify whether there was correction for multiple comparisons in this figure.",
            "38": "In addition, in panel C of Figure 5, the authors seem to report MSE improvements for the physical task that are at least as large if not larger than those in the other two panels, which seems to conflict with the findings from Figure 4.",
            "39": "Please clarify/resolve this apparent discrepancy.",
            "40": "It is not clear why they did not select the top 100 sentences that were best and worst predicted, averaging across words.",
            "41": "This was the unit of analysis for the proposer-verifier system and so it would be good to know that the sentences were indeed predicted well and badly, respectively.",
            "42": "I think it would be helpful to have a slightly longer (i.e., more than one sentence) description of the propser-validator model.",
            "43": "From the description, it is not clear how the hypotheses were validated during trained.",
            "44": "It would be nice if the authors could list all of the emotional and non-emotional words somewhere (same for the other two categories).",
            "45": "I understand if that is not feasible, but I think that might provide an easier way to validate whether the labels are good.",
            "46": "The Krippendorff’s alpha values seem modest, which made me wonder whether they are high quality."
        },
        "oirNSVwniu": {
            "0": "I like the freshness of the ideas in this paper, however (see below) some of them are too fresh.",
            "1": "But I still would like to note originality of the experimental paradigm and the questions posed in this work.",
            "2": "The idea of comparing representations and looking for differences and then figuring out what were the common properties leading to the most different representations is insightful and very interesting.",
            "3": "MEG signal is very noisy and it captures not only the activity associated with the task (reading the text), but also almost all of the cortical activity that happened to happen at the time.",
            "4": "For the rest of the analysis of this paper it is very important that MEG responses we are predicting are actually responses to the text stimuli, but, knowing MEG, I would say this is very unlikely the case.",
            "5": "So while you are predicting something from LM representations, it is very unlikely that what you are predicting are responses to text stimuli.",
            "6": "Please let me know if MEG source separation or some other new technique has made this possible and I am wrong on this point.",
            "7": "But here is a simple experiment you can use to see where the problem is: out of 9,651 words you have recorded responses to, how many of those words you can identify from the MEG signal alone?",
            "8": "By just training a decoder from MEG to words.",
            "9": "Most probably not many, if any.",
            "10": "So how do we know that the MEG signal you are trying to predict contains _any_ signal produced by text processing by the brain?",
            "11": "Figure 1: \"LLM-based hypothesis proposer is employed to formulate natural language hypotheses explaining the divergence\" -- this sentence is a bit too vague for a scientific paper, the role and function of the Proposer remains unclear after reading such explanation.",
            "12": "Page 2, Contribution 2: How do we know that the \"explanations\" provided by such a model are even true?",
            "13": "Obviously it will generate _an_ explanation, and it would sound plausible as most of the LLM-generated texts do, but how is this approach a method of scientific discovery?",
            "14": "How do we replicate / falsify / validate these \"hypotheses\" that the Proposer is generating?",
            "15": "They might be complete rubbish, how do we know they are not?",
            "16": "Page 2, Contribution 3: Too far-reaching conclusions.",
            "17": "The experiments provided in this work do not do enough to prove these claims to be scientific discoveries.",
            "18": "Figure 2: How much of this correlation can be attributed to the fact that the regression models were trained on very similar (MEG) data?",
            "19": "In order to know if those correlations of up to 0.45 are meaningful one needs to conduct an ablation test, where instead of actual LM representations you feed into your regression models just some random noise.",
            "20": "Or to compute the correlations with MEG activity that you know is not the right activity for the presented stimuli.",
            "21": "My suspicion is that due to the structure of the data that was modelled the correlations would still be quite high.",
            "22": "Page 3, Section 2.3: \"As shown in Figure 2, we observe a temporal pro- gression of accurately predicted areas after word onset\" -- What you see is not necessarily a response to the text that the subject is reading, it might be just a response to a visual stimulus appearing in front of their eyes, or some other stimulus-related brain process that does not necessarily contain any signal generated by the neurons that capture the word representation in the subject's brain.",
            "23": "Page 3, Section 3.1: This idea is novel, but relies on too many assumptions and simplifications to be taken seriously as a tool for scientific discovery.",
            "24": "The responses given by such an LLM will always contain _some_ analysis, no matter which pairs of D0 and D1 corpora you give it, you will _always_ be able to pick Top 10 reasons for differences.",
            "25": "But it does not mean that those differences are actual, valid, or have any relation to the MEG signal.",
            "26": "And there was not explanation provided why we should think that they are or do.",
            "27": "What if you measure the same set of subjects on the same task on a different day, and/or use a different numpy random seed for your analysis pipeline - would the same top 10 (and top 3) differences come to the top?",
            "28": "What if you use another chapter of the book?",
            "29": "Would the top 3 differences still be consistent?"
        },
        "iog3wWwqpc": {
            "0": "The exploration of task-based language model representations and their alignment with the brain is an emerging research direction.",
            "1": "Pretrained model representations contain various sources of information sources, whereas task-based representations focus specifically on task-related details.",
            "2": "Considering that three NLP tasks, the authors' examination of their task-based encoding model's performance on MEG recordings is intriguing.",
            "3": "The idea of task-based modeling and their alignment with Brain is not new.",
            "4": "Recently, several research works utilized task-based representations and shown better brain alignment [1], [2], [3], [4].",
            "5": "[1] Oota et al.",
            "6": "2022, Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?, NAACL-2022\n\n[2] Sun et al.",
            "7": "2023, Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?",
            "8": "IJCAI-2023\n\n[3] Aw et al.",
            "9": "2023, Training language models to summarize narratives improves brain alignment, ICLR-2023\n\n[4] Sun et al.",
            "10": "2023, Tuning In to Neural Encoding: Linking Human Brain and Artificial Supervised Representations of Language, ECAI-2023\n\n2.",
            "11": "The novelty in the paper is limited as it primarily delves into a comparison between three task-based model representations and their brain alignment.",
            "12": "Notably, the authors have not discussed or compared with 3 previous works [1], [2] and [3].",
            "13": "There are lot of questions left in the methodology:\n* Why do authors consider the last layer representations of GPT-2?",
            "14": "All the previous linguistic brain encoding studies reveal that intermediate layer representations are well aligned with brain [5], [6], [7].",
            "15": "* Why do authors consider longer context length i.e.",
            "16": "100?",
            "17": "Previous linguistic brain encoding studies reveal that context-length of 10-50 have better brain alignment [5], [8].",
            "18": "[5] Toneva et al.",
            "19": "2019, Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain), NeurIPS-2019\n\n[6] Jain et al.",
            "20": "2018, Incorporating context into language encoding models for fmri, NeurIPS-2018\n\n[7] Oota et al.",
            "21": "2023, Joint processing of linguistic properties in brains and language models, NeurIPS-2023\n\n[8] Oota et al.",
            "22": "2023, MEG Encoding using Word Context Semantics in Listening Stories, Interspeech-2023\n\n4.",
            "23": "The paper lacks clarity regarding its implications.",
            "24": "Given the emphasis on comparing fine-tuned representations with vanilla model representations, the influence of the dataset is crucial.",
            "25": "If the authors had fine-tuned the model on other emotional datasets, different findings might have emerged.",
            "26": "Additionally, the authors present these task-based findings specifically related to narrative story reading.",
            "27": "It's essential to ascertain whether these findings hold true for the listening modality, for instance, by considering datasets like MEG-MASC story listening.",
            "28": "The focus of the authors is primarily on contextual word representations from the last layer of Language Model Models (LLMs).",
            "29": "It would be valuable to explore the representation similarity between pre-trained and fine-tuned layer representations.",
            "30": "Moreover, investigating which layers are predominantly affected during the fine-tuning process would add depth to the analysis.",
            "31": "The clarity can be improved:\n* providing more explicit details concerning the methodology and experimental procedures.",
            "32": "* Figure 4 is hard to follow.",
            "33": "More details are need.",
            "34": "Several citations are missing [1], [2], [4], and [8]"
        },
        "zeuPwnI7ya": {
            "0": "It is an interesting to explore the connection and difference between LMs and human brain, and finetunes the LMs on a human brain datasets to show the improved alignment with brain responses on three phenomena.",
            "1": "So far No.",
            "2": "(Sorry, I am not an expert for this topic, though I try my best to read this paper)"
        }
    },
    "EDPxCjXzSb": {
        "llm_review": {
            "0": "**Innovative Approach**:\n   - The paper introduces a novel, training-free method for Zero-Shot Compositional Image Retrieval (ZS-CIR) by leveraging existing pre-trained Vision-Language Models (VLMs) and Large Language Models (LLMs).",
            "1": "This approach is innovative as it sidesteps the need for costly and labor-intensive annotation of triplets (query image, textual modification, and target image).",
            "2": "**Modularity and Scalability**:\n   - The proposed CIReVL framework is highly modular, allowing for easy replacement and scaling of individual components without the need for retraining.",
            "3": "This flexibility is a significant advantage over traditional methods that require specific training for each component.",
            "4": "**Human-Understandable Process**:\n   - By operating primarily in the language domain, CIReVL makes the compositional retrieval process human-understandable.",
            "5": "This transparency allows for user-level intervention to correct or improve retrieval results, which is a unique and valuable feature.",
            "6": "**Strong Performance**:\n   - The method achieves competitive, and in some cases state-of-the-art, performance on multiple ZS-CIR benchmarks (CIRCO, CIRR, FashionIQ, and GeneCIS).",
            "7": "The results demonstrate the effectiveness of the approach, even outperforming some supervised methods.",
            "8": "**Comprehensive Evaluation**:\n   - The paper provides a thorough evaluation of the proposed method across various datasets and benchmarks.",
            "9": "It also includes detailed ablation studies and qualitative analyses, which help in understanding the impact of different components and the overall robustness of the approach.",
            "10": "**Scalability Insights**:\n   - The study on scaling laws for ZS-CIR is insightful, showing that increasing the size of the retrieval model can significantly boost performance.",
            "11": "This finding is valuable for future research and practical applications.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Dependency on Pre-trained Models**:\n   - While the training-free nature is a strength, it also means that the method is heavily dependent on the quality and capabilities of the pre-trained models used (e.g., BLIP-2, CLIP, GPT-3.5-turbo).",
            "14": "Any limitations or biases in these models could affect the performance of CIReVL.",
            "15": "**Limited Analysis of Failure Cases**:\n   - Although the paper provides some examples of failure cases, a more in-depth analysis of these cases and potential solutions could be beneficial.",
            "16": "Understanding the common failure modes and how to address them would strengthen the paper.",
            "17": "**Generalization to Other Domains**:\n   - The paper demonstrates the method's effectiveness on specific benchmarks, but it would be useful to see more discussion on how well the approach generalizes to other domains or types of compositional queries.",
            "18": "This would help in assessing the broader applicability of the method.",
            "19": "**Computational Resources**:\n   - The reliance on large pre-trained models like GPT-3.5-turbo and GPT-4 may require significant computational resources, which could be a barrier for some users.",
            "20": "Discussing the trade-offs between performance and computational cost would provide a more balanced view.",
            "21": "**User Intervention Complexity**:\n   - While user intervention is a valuable feature, the paper could provide more practical examples and guidelines on how users can effectively intervene in the retrieval process.",
            "22": "This would make the feature more accessible and easier to use in real-world scenarios.",
            "23": "#### Conclusion:\n\nOverall, the paper presents a compelling and innovative approach to training-free compositional image retrieval.",
            "24": "The strengths of the method, including its modularity, scalability, and human-understandable process, are well-demonstrated through comprehensive evaluations and analyses.",
            "25": "Addressing the identified weaknesses, particularly in terms of dependency on pre-trained models and providing more practical insights into user intervention, would further enhance the impact and applicability of the work."
        },
        "1jh3IV7b9O": {
            "0": "The proposed framework is training-free and achieves promising retrieval results.",
            "1": "The retrieval accuracy could be improved by adopting better off-the-shelf VLM and LLM, due to the modularity of the framework.",
            "2": "Since the query is mainly processed in language domain, the framework is human-interpretable and could also improve the retrieval accuracy by involving human intervention.",
            "3": "The method itself is very intuitive and not quite novel, which is more like an engineering extension of existing models like VLM and LLM.",
            "4": "Additionally, the proposed method does not achieve better results on all evaluation datasets.",
            "5": "In Table 1, CIReVL underperforms SEARLE on CIRR dataset in terms of CIRR, which shows the unstability.",
            "6": "This work only adopts datasets of everyday life and natural scenes (CIRR, CIRCO, GeneCIS) as evaluation.",
            "7": "It is necessary to include datasets of various domains, such as fashion domain datasets like fashioniq and fashion200k to evaluate the generalization ability.",
            "8": "Furthermore, for GeneCIS dataset, the comparison against other ZSCIR methods such as Pic2word and SEARLE should be included for complete comparison.",
            "9": "The proposed framework is not very time-efficient since for each query, there will be two auto-regressive processes: query image captioning and target caption generation, which are very time-consuming.",
            "10": "Both processes involve large models like VLM and LLM, which may further lead to low efficiency for image search.",
            "11": "Therefore, it is necessary to compare the retrieval efficiency (time) with the previous methods."
        },
        "r3DhoH5bBg": {
            "0": "The paper effectively integrates the VLM and LLM for ZS-CIR, offering a flexible and intervenable CIR system.",
            "1": "The authors have conducted thorough experiments to validate the effectiveness of the proposed method, making the evaluation rather reliable.",
            "2": "The figures in this paper are clear, effectively conveying the processing pipeline.",
            "3": "Limited contribution: The proposed method appears to be a combination of several foundation models and simply employs the basic modelling capacity of these models (e.g., BLIP-2 for image captioning, GPT for text editing and CLIP for image retrieval), presenting a naïve and straightforward solution for CIR task.",
            "4": "As a result, it is challenging to identify insightful and significant contributions to this field.",
            "5": "The authors should conduct further in-depth research to enhance their contributions.",
            "6": "Incorrect experimental results: The Recall@10 of CIReVL (ViT-G/14∗) on CIRR dataset in Table 1 is obviously incorrect.",
            "7": "Inconsistent formatting of table data: In some cases, the data is presented with two decimal places, while in others, only one decimal place is used.",
            "8": "Furthermore, there is a case with a mix of formatting in Table 3.",
            "9": "The authors may revise the paper more carefully.",
            "10": "The paper effectively combines the existing powerful VLM and LLM models.",
            "11": "However, it would be better to provide a more insightful analysis of the caption and reason processes.",
            "12": "For example, from Table 3, compared to utilizing different LLM models, the use of various state-of-the-art (SOTA) captioning models has a relatively minor impact on retrieval.",
            "13": "Are there any potential explanations for this phenomenon?",
            "14": "Does this mean the choice of caption model is not strict, as long as the model can catch the main object or attribute of images?",
            "15": "I think it would be better to provide more analysis.",
            "16": "The description about other works in Section 3.1 is a bit obscure for me who is not so familiar with the task but proficient other related tasks.",
            "17": "Missing references, such as in ‘Similar to existing ZS-CIR methods’ in the first paragraph of Section 3.2.",
            "18": "Experiments: (a) In Table 1, the author only presents the experimental results of ‘image only’ and ‘image+text’ methods for reference while missing that of the ‘text only’ method under the ViT-B/32 setting.",
            "19": "However, the authors mention that the results on CIRR benchmark are primarily dependent on the modifying instruction while the actual reference image has much less relation to the target image, which indicates that the ‘text-only’ method can be an important reference for measuring the performance of the proposed method.",
            "20": "Better to present the result of ‘text-only’ method and make a fair comparison and discussion.",
            "21": "(b) In Table 1, the authors miss the results of ‘image only’, ‘text only’ and ‘image+text’ method for ViT-L/14 and ViT-G/14 settings.",
            "22": "Better to include them for reference.",
            "23": "(c).",
            "24": "When evaluating on the GeneCIS benchmark, the authors do not specify the architecture of the vision backbone adopted in the experiment.",
            "25": "Better to specify it clearly for reference.",
            "26": "Discussions and evaluation on the potential limitations: The paper shows that the proposed method has several merits including free of training, good flexibility and scalability.",
            "27": "However, it may also have some potential limitations.",
            "28": "For example, (a) Inference costs and efficiency: The proposed method utilizes large VLMs and LLMs to conduct the image captioning, language reasoning and cross-modal retrieval during inference.",
            "29": "Will it take more computational costs and have longer inference time compared with the previous methods?",
            "30": "(b) Limitations of each module: Since the proposed method is composed of three different modules, the effect of each module plays an important role on the final retrieval results.",
            "31": "For example, if the image caption module generates partial or false descriptions for the given images, the reasoning and retrieval process will be misled.",
            "32": "Thus, it is necessary to analyze the potential negative impact brought by each module in an in-depth manner and quantify them if possible (which factor contributes more to the failure cases).",
            "33": "(c) Compatibility of different modules: Since the proposed method conducts cross-modal retrieval by cascading three separate modules, the compatibility of different modules seems to be an important factor.",
            "34": "For example, if the captions generated by LLMs have different styles with pretraining data of the VLMs, the VLMs used for cross-modal retrieval may produce some bias, which may hinder the final performance.",
            "35": "Overall, it will be appreciated if the authors can present more in-depth discussions and evaluation on the potential limitations to fully demonstrate the properties of the proposed method.",
            "36": "Writing: Some parts of the paper don’t flow well.",
            "37": "The overall writing requires further improvement."
        },
        "kKiMeQX6wj": {
            "0": "While there are a lot of works on compositional image retrieval, previous works typically require training several components like textual inversion and lack human interpretability.",
            "1": "The method proposed in this paper avoids all these problems by simply composing image and textual query in the language domain.",
            "2": "On the surface this is ingenious because you do it in a modular way and each module is a highly-generalisable large-scale pre-trained model.",
            "3": "For example, when training a textual inversion, we do not really guarantee it scales to open-set setups (given we train them in limited data and compute).",
            "4": "We just use a small-scale network and hope the rest of powerful models take care of it.",
            "5": "The proposed method works zero-shot and you know it works for open-set setups.",
            "6": "Additionally, unlike prior trainable compositional image retrievals, there is a lot of effort that goes into interpretability -- \"how was the image and query text was composed\".",
            "7": "Since the proposed method does this composition entirely in the language space, you know exactly what information was extracted from the image and you can see the generated caption from LLM for the desired target image.",
            "8": "This not only makes the retrieval process highly transparent, but also allows post-hoc edits -- you can literally make changes to adjust your retrieval results.",
            "9": "I encourage more works that reuses as much as large-scale models and combines them as modules with minimal or training-free way.",
            "10": "Despite its appeal, there are a few important drawbacks.",
            "11": "This entire process sticks on the underlying assumption that our image captioning module (e.g., BLIP-2 or CoCa) can provide a \"detailed caption\" that captures all information.",
            "12": "This, in my opinion, is a strong assumption.",
            "13": "A lot depends on your captioning module.",
            "14": "While the captioning module may be super accurate, it can miss some \"less important\" details that I want to change.",
            "15": "For example, given a photo, the sky was orange and my textual query is \"make the colour of the sky darker\".",
            "16": "If the image captioning module omits the colour of sky (i.e., orange) and focuses on the foreground (e.g., a person holding a flower, sitting in a bench near a park where kids are playing) -- the rest of the modules have no way of combining my textual query \"make the colour of sky darker\".",
            "17": "This is an example, where the entire pipeline fails -- due to no fault of the image captioning module.",
            "18": "It provided a detailed image description, but it is not possible to describe every little \"unimportant things\".",
            "19": "Looking at the same problem from a different direction, is the information bandwidth of an image the same as language?",
            "20": "(an image can be worth a thousand words)"
        },
        "xHnx7C2jfO": {
            "0": "- The paper is technically sound and widely applicable not only for text-image use case, but also text-video and probably other problem domains.",
            "1": "- The paper investigates three different datasets against multiple baselines and various additional ablations make the paper and results more appealing.",
            "2": "Limitations of the proposed method also gets discussed with examples.",
            "3": "- The paper is easy to follow / well-written.",
            "4": "- [Novelty & Literature review] There are a wider number of papers in the field since last year and some of those papers requires mention (at least the ones before the ICLR submission deadline).",
            "5": "The most similar paper ((https://arxiv.org/pdf/2310.05473.pdf) proposes the same approach but with an additional training method for merging the automatically generated image caption and the desired modification's text description.",
            "6": "This paper contains additional papers/baselines to cite/include.",
            "7": "It would be great if there is a way to understand whether the proposed training-free method would perform better or worse compared to the paper mentioned above.",
            "8": "- [Experiments] Baseline PALAVRA attack different set of problems, uses different datasets and metrics.",
            "9": "It does not mention CIR task in the text.",
            "10": "Not clear if PALAVRA serves as a baseline."
        }
    },
    "22pyNMuIoa": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel framework, PromptAgent, which leverages Monte Carlo Tree Search (MCTS) for strategic planning in prompt optimization.",
            "1": "This is a significant advancement over existing methods that rely on heuristic or iterative sampling approaches.",
            "2": "**Human-like Exploration**: The method mimics human trial-and-error processes by generating error feedback and refining prompts iteratively.",
            "3": "This approach effectively integrates domain-specific knowledge and detailed instructions, which are crucial for expert-level prompt engineering.",
            "4": "**Comprehensive Evaluation**: The authors evaluate PromptAgent on a diverse set of 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-specific tasks (biomedical), and general NLP tasks.",
            "5": "The results show that PromptAgent significantly outperforms strong baselines, including Chain-of-Thought (CoT) and recent prompt optimization methods.",
            "6": "**Efficiency and Generalizability**: The paper demonstrates that PromptAgent not only enhances the performance of initial human prompts but also does so with great efficiency and generalizability.",
            "7": "The exploration efficiency analysis and convergence analysis further support the robustness and effectiveness of the proposed method.",
            "8": "**Qualitative Analysis**: The qualitative analysis provides a clear illustration of how PromptAgent iteratively refines prompts based on error feedback, leading to substantial improvements in task performance.",
            "9": "This helps in understanding the practical impact of the proposed method.",
            "10": "**Transferability**: The study shows that the optimized prompts can be transferred to other base LLMs, highlighting the robustness and transferability of expert-level prompts across different models.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Complexity and Scalability**: While the MCTS-based approach is effective, it may introduce complexity and scalability issues, especially for very large prompt spaces or highly complex tasks.",
            "13": "The computational cost associated with strategic planning and extensive exploration might be a concern for practical applications.",
            "14": "**Dependence on Optimizer LLM**: The effectiveness of PromptAgent heavily relies on the capabilities of the optimizer LLM (e.g., GPT-4).",
            "15": "In domains where the optimizer LLM lacks sufficient domain knowledge, the performance of PromptAgent might be limited.",
            "16": "The paper acknowledges this limitation but does not provide a concrete solution.",
            "17": "**Limited Domain-Specific Evaluation**: Although the paper evaluates PromptAgent on a range of tasks, the domain-specific tasks are primarily focused on the biomedical domain.",
            "18": "It would be beneficial to see evaluations in other specialized domains to further validate the generalizability of the method.",
            "19": "**Error Feedback Quality**: The quality of error feedback generated by the optimizer LLM is crucial for the success of PromptAgent.",
            "20": "However, the paper does not delve deeply into the mechanisms for ensuring high-quality error feedback, which could be a potential area for improvement.",
            "21": "**Human-in-the-Loop Considerations**: While the method aims to minimize human intervention, there might still be scenarios where human expertise is required to guide the optimization process, especially in highly specialized domains.",
            "22": "The paper could benefit from discussing how to effectively integrate human feedback when necessary.",
            "23": "**Reproducibility and Implementation Details**: The paper provides a high-level overview of the methodology and results but lacks detailed implementation specifics that are crucial for reproducibility.",
            "24": "Providing more granular details on the experimental setup, hyperparameters, and code would enhance the reproducibility of the study.",
            "25": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of prompt optimization by introducing a strategic planning approach with MCTS.",
            "26": "The method shows promising results across a variety of tasks and demonstrates the potential to achieve expert-level prompt engineering autonomously.",
            "27": "However, addressing the limitations related to complexity, scalability, and domain-specific evaluations would further strengthen the impact and applicability of PromptAgent."
        },
        "jNNdTLgmRG": {
            "0": "- The paper is well written and easy to follow.",
            "1": "- Related literature is covered.",
            "2": "- Authors have conducted comprehensive experiments to evaluate the performance of their proposed method against a set of alternative methods and ablation studies shed further lights on the effectiveness of the proposed strategic exploration of the search space using MCTS.",
            "3": "- Appendix covers useful details about the hyper parameters and the evaluation data sets and details of the studied methods.",
            "4": "- **Novelty** of the proposed method is arguable.",
            "5": "Use of MCTS in prompt engineering is not novel.",
            "6": "However, the way authors have incorporated a base and an optimizer model in their implementation and the prompts used to incorporate the error feedback into the existing state (+ the empirical studies) can be considered as the main contributions of this work.",
            "7": "- **Contribution**: Based on the details provided in the Appendix section, it can be seen that the amount of details covered in prompts generated by APE is not comparable with that of the proposed method.",
            "8": "The prompts generated by the proposed method are significantly lengthier and cover more details compared which is hard to justify.",
            "9": "APE supposedly uses Monte Carlo Search to iteratively propose and select prompts, therefore, it is expected to see more details getting added to the original prompt over each iteration which is not reflected in the samples that I see in the Appendix section.",
            "10": "**This makes me conclude that the additional gain from the proposed method by authors come from the way they instruct the optimizer method to incorporate the feedback into the existing state rather than utilization of MCTS** which is the main claim of the paper.",
            "11": "However, \n- Clarity [minor]: It's ok that authors have covered the details of the selection, expansion, simulation, and back-propagation in MCTS and it can be very useful for general audience with less context, however, I was hoping to read more details about how authors have implemented the expansion stage.",
            "12": "Appendexi briefly touches base under \"Implementation details\" sub-section and mentions \"We sample 3 batches, and for each batch, we generate multiple new prompts\".",
            "13": "It would be good if authors can further explain how they generate new prompts for each batch."
        },
        "x1jyknFng7": {
            "0": "The idea of this work is clear and easy to follow.",
            "1": "The writing is in general clear.",
            "2": "This idea can be useful from the engineering/ deployment side.",
            "3": "Technical contribution is limited.",
            "4": "Comparing the performance of an average user with LLM in prompting is somewhat unfair.",
            "5": "Also, even human experts will be posited under an unfair setting where LLMs can do multiple-round prompting.",
            "6": "Some of the experiment settings are suspicious to be unfair (please see questions below)"
        },
        "13zVQ1rDdF": {
            "0": "This paper is well-written and easy to follow.",
            "1": "The method seems clean, straightforward, and promising.",
            "2": "There are several clarity issues in the experimental section regarding human prompts and reward functions.",
            "3": "See questions below."
        },
        "fWoXSxqtki": {
            "0": "- The proposed PromptAgent leveraged a Monte Carlo Tree Search framework to utilize errors and feedback identified by LLMs for the iterative refinement of prompts.",
            "1": "This approach is theoretically sound and can enhance navigation through the expansive search space of potential prompts.",
            "2": "- PromptAgent showed promising experimental results across 12 tasks, and the optimized prompt can be generalized to different LLMs.",
            "3": "- PromptAgent showed better performance and exploration efficiency than other prompt optimization methods, including Automatic Prompt Engineer (APE).",
            "4": "- PromptAgent relies on a key hypothesis: the optimizer LLM (GPT-4 in this study) possesses adequate domain knowledge to identify the errors in the response from the base LLM and give meaningful feedback.",
            "5": "However, this may not be a valid hypothesis, especially in some specialized areas such as medicine [1,2], where the data is relatively sparse due to strict data protection regularization like HIPAA.",
            "6": "- In order to refine the prompt, PromptAgent needs to concatenate the \"error_string\", \"error_summarization and \"trajectory_prompts\" as one input.",
            "7": "Challenges may arise in tasks demanding the interpretation of extensive contexts, such as the analysis of detailed medical documents, where the \"state_transit\" could become prohibitively large due to the number of training examples and the depth of the Monte Carlo Tree Search, potentially diminishing the LLMs' performance.",
            "8": "Reference\n\n[1] Bhayana, R., Krishna, S., & Bleakney, R. R. (2023).",
            "9": "Performance of ChatGPT on a radiology board-style examination: Insights into current strengths and limitations.",
            "10": "Radiology, 230582.",
            "11": "[2] Azizi, Z., Alipour, P., Gomez, S., Broadwin, C., Islam, S., Sarraju, A., ... & Rodriguez, F. (2023).",
            "12": "Evaluating Recommendations About Atrial Fibrillation for Patients and Clinicians Obtained From Chat-Based Artificial Intelligence Algorithms.",
            "13": "Circulation: Arrhythmia and Electrophysiology, e012015."
        }
    },
    "gisAooH2TG": {
        "llm_review": {
            "0": "**Innovative Integration of LLMs and VLMs**: The paper presents a novel framework that leverages the strengths of both Large Language Models (LLMs) and Vision Language Models (VLMs) to enable real-time replanning capabilities for long-horizon tasks.",
            "1": "This integration is innovative and addresses a significant gap in the current state of robotic planning.",
            "2": "**Hierarchical Planning Approach**: The hierarchical structure of REPLAN, which includes high-level and low-level planners, is well-conceived.",
            "3": "This structure allows for effective decomposition of complex tasks into manageable subtasks, enhancing the robot's ability to handle long-horizon tasks.",
            "4": "**Real-time Replanning**: The ability to replan in real-time based on feedback from the environment is a significant advancement.",
            "5": "This feature allows the robot to adapt to unforeseen obstacles and changes in the environment, which is crucial for practical applications.",
            "6": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of the proposed framework across multiple environments and tasks.",
            "7": "The comparison with baseline models and ablation studies adds credibility to the results and demonstrates the effectiveness of REPLAN.",
            "8": "**Detailed Methodology**: The paper offers a detailed description of the methodology, including the roles of the High-Level Planner, VLM Perceiver, Low-Level Planner, Motion Controller, and LLM Verifier.",
            "9": "This clarity in presentation makes it easier for readers to understand the workflow and the contributions of each component.",
            "10": "**Promising Results**: The experimental results are impressive, with REPLAN achieving a significantly higher success rate compared to the baseline model.",
            "11": "The detailed roll-out examples further illustrate the practical effectiveness of the framework.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Reliance on VLM Accuracy**: One of the limitations mentioned in the paper is the reliance on the VLM's ability to accurately recognize objects and interpret spatial states.",
            "14": "While the authors have attempted to mitigate this through specific questioning and summary generation, the framework's performance is still dependent on the VLM's accuracy, which can be a bottleneck.",
            "15": "**Communication Disconnect Between LLM and MPC**: The paper highlights a communication disconnect between the LLM and the Motion Predictive Controller (MPC).",
            "16": "The LLM receives task failure feedback as a floating point number without understanding the specific reasons for failure.",
            "17": "This can lead to misinterpretations and unnecessary task abandonment or repeated attempts.",
            "18": "**Limited Generalization**: While the framework shows promising results in the tested environments, the paper does not provide sufficient evidence of its generalization capabilities across a wider range of tasks and environments.",
            "19": "More diverse testing scenarios would strengthen the claims of the framework's robustness.",
            "20": "**Complexity and Computational Overhead**: The hierarchical structure and the multiple verification steps, while effective, add complexity and computational overhead to the system.",
            "21": "The paper does not discuss the computational requirements and the potential impact on real-time performance in detail.",
            "22": "**Dependency on Pre-trained Models**: The framework heavily relies on pre-trained LLMs and VLMs.",
            "23": "The paper does not address the potential limitations and biases inherent in these models, which could affect the overall performance and reliability of the system.",
            "24": "**Limited Discussion on Failure Cases**: While the paper provides some examples of error cases, a more detailed analysis of failure cases and the reasons behind them would be beneficial.",
            "25": "This would help in understanding the limitations of the current approach and identifying areas for improvement.",
            "26": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of robotic planning by integrating LLMs and VLMs for real-time replanning.",
            "27": "The hierarchical approach and the ability to adapt to environmental changes are noteworthy contributions.",
            "28": "However, the framework's reliance on the accuracy of VLMs, the communication disconnect between LLM and MPC, and the limited generalization evidence are areas that need further exploration.",
            "29": "Addressing these weaknesses in future work could enhance the robustness and applicability of the proposed framework."
        },
        "gJYXqJnk1T": {
            "0": "The core idea of using a VLM to generate textual feedback for a model makes sense and is a good addition to this research direction in robot learning.",
            "1": "The videos are very helpful for showcasing the resulting method.",
            "2": "Although the high level idea of the paper is reasonable, it is both not evaluated as extensively as would be helpful and is not put in context of recent literature very well.",
            "3": "As a minor note: please fix the references to surround the authors of citations in parentheses.",
            "4": "This would make the paper much easier to read.",
            "5": "On the experimental front:\n\n* the exact prompts used do not appear in the paper or appendix, which makes it hard to judge how the LLM was told to incorporate past feedback\n* The evaluation setting is 4 tasks, done for 3 trials each.",
            "6": "This is an incredibly small number of tasks and trials for the method.",
            "7": "As a point of comparison: the Language to Rewards paper used as a baseline was tested on 17 tasks with 10 generated rewards functions per task, run through MPC 50 times each.",
            "8": "My understanding is that in Table 1, RePLan without replanning is almost identical to Language to Rewards, but is only successful 1/3 times in the easiest Task 1 setting, compared to 3/3 from Language to Rewards.",
            "9": "To me this seems like it is just because of random noise, but if that's the case, why should we trust any of the other numbers in the table?",
            "10": "This is the point of issue I find most important about the paper.",
            "11": "On the prior literature front:\n\nThere are number of prior works based on providing LLM feedback from the environment, either ground truth or from VLMs.",
            "12": "Examples are Voyager by Wang et al, Inner Monologue by Huang et al, and Towards a Unified Agent with Foundation Models by Di Palo et al.",
            "13": "I would appreciate some discussion about such lines of work, since to me it is not so clear if this is doing anything very different from these works.",
            "14": "I believe at most you can argue that this paper is using MPC instead of RL or imitation learning, but otherwise prior work has used chain-of-thought style prompting to decompose high-level language to low level language, generate rewards from said low level language, provide feedback via VLMs, etc.",
            "15": "That is not to say that the combination of prior work cannot be novel, but in this instance, it does not feel like much is coming from said combination.",
            "16": "Especially given the weakness of the experimental results.",
            "17": "Edit: some more experimental results were provided and I have adjusted score from 3 -> 5."
        },
        "9G66vyiBLD": {
            "0": "- The presented idea is clear and well-motivated — leveraging LLMs in a hierarchical framework for both high-level task planning and low-level motion planning.",
            "1": "Compared to prior work, “Language to Reward”, it is clear that such hierarchical approach is needed for long-horizon tasks and can also offer additional robustness as the system can replan its high-level action.",
            "2": "- The literature review is also thorough, covering many recent works in this domain.",
            "3": "However, this part can be improved because it is now more like a laundry list instead of putting the work in the context of prior works.",
            "4": "- Currently the biggest limitation seems to be the lack of thorough experiments, which can use some improvement along two axes.",
            "5": "One is the breadth of the tasks: there are only four tasks investigated in this work while there are also quite some similarities between them.",
            "6": "An important advantage of using LLMs is that it is possible to apply to a wider set of tasks more easily.",
            "7": "The other axis is the quantitative evaluation: currrently only 3 runs are performed for each entry in Table 1, which makes the quantitative results not very convincing as it is also pointed out in the paper that there is “high variance of completion”.",
            "8": "In addition, the paper does not compare to prior methods that are not based on LLMs, e.g., task and motion planning methods or hierarchical RL methods.",
            "9": "- Another limitation lies in the use of the simulator ground-truth for MPC.",
            "10": "This raises the question whether the approach can be applicable to real-world settings.",
            "11": "However, the high-level planning part does use VLM for grounding image observations, but it’s unclear how this can be achieved for the attributes referred in MPC, e.g.",
            "12": "“block_r_side”, “cabinet_handle”.",
            "13": "- Currently the intro reads more like related works, where it may be confusing to readers what the actual motivation of the work is.",
            "14": "More care can be taken to improve the intro while appropriately contextualizing the work.",
            "15": "- The citation format in many places are currently incorrect — parenthesis often should be used."
        },
        "xr2EagjKkn": {
            "0": "- The paper is well-written and easy to follow\n- The motivation of the paper is topical since feedback and adaptive replanning is important for foundation models which may hallucinate or require grounding in physical interactions\n- The method does not require additional human input compared to the baseline method Language2Reward (just one human input at the beggining, the rest of the replanning and execution is autonomously completed by the foundation model submodules) - The VLM Perceiver is one of the most critical parts of the method, but it is not sufficiently explained.",
            "1": "Due to the lack of details, I can only assume how it is utilized based on Algorithm 1, in which case I have some major concerns.",
            "2": "Since the VLM is the bottleneck for providing feedback for grounding LLM plans and rewards for future LLM iterations.",
            "3": "However, details are not shared about how the Perceiver is used, even though it is mentioned that \"The High-Level Planner [is used] to decide what it wants to query from the Perceiver\".",
            "4": "From Algorithm 1, the VLM needs to be used for two use cases: #1 scene state generation `VLM(image_observation)` and #2 `VLM(image_observation, motion_error, language_instruction)`.",
            "5": "However, these seem to be quite challenging tasks to naively query for off-the-shelf VLMs.",
            "6": "While modern VLMs are fairly robust at narrow vision task domains like object detection or image captioning for internet images, more extended reasoning (such as failure explanation with multiple input contexts) or domain-specific understanding (like robotics reasoning from vision) is still an open problem.",
            "7": "- The evaluation complexity is very limited and does not justify the claims of a \"large-scale and long-horizon kitchen environment\": it is only in simulation, with relatively high-level and short-horizon tasks.",
            "8": "There are two issues: 1) the granularity of intermediate primitives (\"pick up the block\") is coarse, 2) the horizon length is short, going only up to 4 subtasks required.",
            "9": "Previous works in BEHAVIOR-1K, ALFRED, SayCan have studied robotic reasoning with much longer horizons with similar granularity of intermediate primitives.",
            "10": "- The evaluation has very few trials (3 seeds only), so it is hard to draw confident conclusions about the method's quantitative performance.",
            "11": "- The core claim of the work should clarified.",
            "12": "If the contribution is the incorporation of LLM and VLM feedback into high level planning, it needs to be compared/discussed against prior works that ground robot planning with additional foundation model feedback.",
            "13": "Specifically, [1] incorporates LLM and VLM feedback for closed-loop environment feedback for a High-Level LLM Planner for robot subtasks.",
            "14": "If the contribution is the integration of LLM and VLM feedback into code generation, it needs to compared/discussed with other replanning works from code generation [2] or LLM tool use.",
            "15": "If the core contribution is the verifier, this needs to be stated more clearly and more details about this module should be provided.",
            "16": "If the contribution is the admittedly impressive integration of existing modules, this should be clarified and more details about the bottlenecks of the system (VLM Perceiver, Verification) should be shared in the main text.",
            "17": "[1] \"Inner Monologue: Embodied Reasoning through Planning with Language Models\", Huang et al.",
            "18": "2022\n[2] \"Improving Code Generation by Training with Natural Language Feedback\", Chen et al.",
            "19": "2023"
        },
        "jbwFssE1h5": {
            "0": "The overall proposed method seems to be interesting and probably could work for a large variety of planning tasks.",
            "1": "The tasks are a bit more complex then simple pick and place and illustrate more complex reasoning.",
            "2": "==================\nI am increasing the score to 6 after rebuttal discussions.",
            "3": "1. the authors need to provide more environments or tasks to show the robustness of their method.",
            "4": "The authors can use LLMs to generate tasks which are long-horizon to come up with more varieties of task so that the proposed method could be more thoroughly tested.",
            "5": "I think this remains to be verified."
        }
    },
    "aN4Jf6Cx69": {
        "llm_review": {
            "0": "**Clear Objective and Motivation**: The paper addresses a significant and timely problem in the field of machine learning, specifically the mechanistic understanding of in-context learning (ICL) in transformer models.",
            "1": "The motivation is well-articulated, linking the study to the broader context of large language models (LLMs) and their emergent abilities.",
            "2": "**Simplified Model for Clarity**: The authors successfully use a minimal attention-only network to replicate the core phenomena observed in more complex models.",
            "3": "This simplification aids in isolating and understanding the fundamental mechanisms behind ICL and in-weights learning (IWL).",
            "4": "**Comprehensive Experiments**: The paper includes a thorough set of experiments that explore various parameters affecting ICL and IWL, such as burstiness, number of classes, within-class variability, and rank-frequency distribution.",
            "5": "This comprehensive approach provides a robust understanding of the factors influencing the trade-off between ICL and IWL.",
            "6": "**Mechanistic Insights**: The identification of induction heads as a key mechanism driving ICL is a significant contribution.",
            "7": "The paper goes further to construct a minimal two-parameter model that captures the essential computations of an induction head, providing deeper mechanistic insights.",
            "8": "**Phenomenological Model**: The development of a phenomenological model to explain the abrupt transitions in ICL is a novel and valuable addition.",
            "9": "This model helps in understanding the sequential learning of nested logits and the resulting cliffs in the loss landscape.",
            "10": "**Reproducibility and Robustness**: The experiments are repeated with multiple seeds, and the results are consistent across different runs, which adds to the robustness and reliability of the findings.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Limited Scope of Models**: While the minimal model is useful for clarity, it may not capture all the complexities of larger, real-world transformer models.",
            "13": "The paper acknowledges this but does not explore how the findings might scale to more complex architectures.",
            "14": "**Simplified Data Distribution**: The use of a simplified dataset and input statistics, while beneficial for isolating mechanisms, may limit the generalizability of the results to more complex and varied real-world data distributions.",
            "15": "**Assumptions in Phenomenological Model**: The phenomenological model makes several simplifying assumptions, such as the orthogonality of subspaces and the specific form of the attention weights.",
            "16": "These assumptions, while necessary for tractability, may oversimplify the actual dynamics in more complex networks.",
            "17": "**Lack of Real-World Validation**: The paper does not include experiments on real-world datasets or tasks, which would help validate the findings in practical scenarios.",
            "18": "This limits the immediate applicability of the results to real-world problems.",
            "19": "**Potential Overemphasis on Induction Heads**: While the paper provides strong evidence for the role of induction heads in ICL, it may overlook other potential mechanisms or interactions that could also contribute to ICL in larger models.",
            "20": "**Curriculum Learning**: The discussion on the role of intrinsic curriculum in guiding networks towards ICL solutions is insightful, but the paper does not provide concrete methods or experiments to explore this further.",
            "21": "This leaves an open question on how to effectively implement such curricula in practice.",
            "22": "#### Conclusion\n\nOverall, the paper makes significant contributions to the understanding of in-context learning in transformer models.",
            "23": "It provides clear mechanistic insights and a robust experimental framework.",
            "24": "However, the findings are somewhat limited by the simplified models and data distributions used.",
            "25": "Future work should aim to validate these insights in more complex and real-world scenarios, and explore additional mechanisms that may contribute to ICL in larger models.",
            "26": "The discussion on curriculum learning is promising and warrants further investigation."
        },
        "2ZCcyMAYL9": {
            "0": "This paper tackles a timely and interesting topic, and contains several insights and useful contributions.",
            "1": "-- The synthetic task family introduced is a clean and intuitive way of exposing dependencies of learning strategies on pretraining data distributions.",
            "2": "Showing that key phenomena identified in (Chan et al., 2022) can be replicated in this setting is a useful contribution.",
            "3": "-- The characterization of the initial slow learning phase as driven by an increase in context-label accuracy is interesting\n\n-- The idea that a strategy that results in good context-label accuracy can facilitate (despite not being necessary for) learning of a true ICL strategy is very interesting, and some evidence is provided for this idea\n\n-- The evidence provided that the emergence of induction heads is strongly linked with the development of the ICL strategy, while not entirely novel, is nice to see I have the following concerns about this paper.",
            "4": "Many of them involve claims that I feel are made too strongly in the paper relative to the level of evidence provided.",
            "5": "-- The paper illustrates a set of phenomena in figure 2, and promises a mechanistic understanding of these phenomena.",
            "6": "But the mechanistic analysis provided later in the paper does not speak to most of the phenomenology -- for instance, the dependence of the ICL/IWL tradeoff on B, epsilon, K, and alpha.",
            "7": "In fact, the mechanistic analysis focuses on the p_C > 0 case, which is different from the p_c = 0 regime that gives rise to all the tradeoffs observed in Figure 2.",
            "8": "Thus, the connection between pages 1-4 of the paper and the rest is not entirely clear.",
            "9": "-- -- The following sentence, while intuitively reasonable, is written as a key strong claim and as far as I can tell is not really justified with evidence: \"Therefore, the relative rates at which the network acquires ICL and IWL control the fraction of loss explained by each mechanism after convergence.\"",
            "10": "-- The paper makes strong causal claims based only on correlational evidence.",
            "11": "For instance, \"Induction head formation drives the abrupt transition during ICL.\"",
            "12": "As far as I can tell no evidence is given for this claim, other than the (very suggestive, I agree!)",
            "13": "fact that they coincide in time.",
            "14": "-- The paper makes strong claims about the three-parameter model proving or ruling out certain hypotheses.",
            "15": "An example is the sentence \"This rules out hypothesis V as only the factors corresponding to the progress measures (a) through (d) have been included in the minimal model.\"",
            "16": "In my opinion, such claims are much too strong.",
            "17": "The three-parameter model is ultimately a different model from the original transformer architecture being used!",
            "18": "While the analysis of its behavior is suggestive of the learning strategies used by the original architecture, it is not conclusive.",
            "19": "The strength of the claims should be adjusted accordingly."
        },
        "VokzE19go0": {
            "0": "The paper is excellently written and fairly easy to follow despite the depth of insights proven.",
            "1": "The scientific investigation is very well conducted : of note is that it alternates particularly well between empirical elements, formulating subsequent hypotheses (section \"Induction head formation drives the abrupt transition during ICL\"), disproving some of those, and finally introducing a theory that accounts for those findings, replicating empirical stylized facts, whilst much simplifying the problem.",
            "2": "In particular, the phenomenological model Equation 10 (and its illustration Figure 7) is a standout novel contribution, and clearly worthy of publication, in our view.",
            "3": "In a sense, the paper is tantalizing, as it invites further work, for instance on the interplay of overlap difference \\xi and data Zipfianity parameter \\alpha."
        },
        "oxmXlHjg2U": {
            "0": "I have read several papers recently that try to provide some insights about the emergent capabilities of LLMs -- through abstract modeling and experiments with tasks such as linear regression (learned through ICL).",
            "1": "This paper is the best I have read so far in that direction.",
            "2": "The experiments are wisely designed, allowing us to understand the complex tradeoff between IWL and ICL -- as well the effect of some key data distributional parameters such as the rank-frequency exponent.",
            "3": "The simple model proposed in the second part of the paper is also intriguing, explaining how the abstract model of an induction head can explain mechanistically the ICL capabilities of an attention-based network.",
            "4": "The paper can be improved in terms of writing/presentation.",
            "5": "For example, you can explain early on in the paper what \"induction head\" means for readers that are less familiar with this area.",
            "6": "There are also several other parts of the paper in which the writing can be improved -- mostly by writing simpler/shorter/more clear sentences."
        },
        "zbDUCdPi5c": {
            "0": "The ability to use information in context to respond appropriately to later queries (called 'in context learning' or ICL), is central to the capabilities of AI systems like ChatGPT.",
            "1": "ICL was enabled by the attention mechanism in transformer-based neural networks.",
            "2": "ICL is exemplified by the simple item-label association task (introduced by others) that the authors have selected for the focus of their analysis.",
            "3": "By shedding light on how this task is solved (building on an earlier paper taking initial steps in this direction) the current paper deepens our understanding of this core property of today's performant AI systems.",
            "4": "The authors have created new variants of the task that further simplify it, and have introduced minimally-sufficient transformer architecture containing two attention layers, which together implement what they call an 'induction head', arguably the core emergent computational structure enabling ICL.",
            "5": "They have provided a insightful analysis of the (ultimately simple, but nevertheless important) computations performed by the network that allow the effective use of information in context in their task.",
            "6": "They gone on to attempt to understand how this attention head computation emerges as the network learns to solve the item-label association task.",
            "7": "They identify progress measures in both the network's input-output performance and of its attention head computations and establish clear alignments of several of these measures.",
            "8": "I consider these measures and their alignments enlightening contributions and consider them to be strengths of the paper.",
            "9": "They go on to further support their analysis by developing a three-parameter reduction of the induction head, and show that the learning dynamics of this reduction is sufficient to reproduce many of the features of the learning dynamics of their complete neural network; they then use the reduction to test hypotheses about the relationships between the progress measures, showing that a further reduction that eliminates one of the progress measures makes learning success initialization dependent.",
            "10": "Finally, they make an even further reduction in the form of a 2- or 3- parameter 'phenomenological model' whose loss landscape can be fully characterized.",
            "11": "The parameters now directly reflect the efficacies of the two attention layers making up the induction head and of their mapping to the correct label, and allow the loss landscape of each of the variants to be visualized.",
            "12": "This phenomenological model provides an abstract characterization of the emergent learning dynamics of the 2- and 3 parameter reduction models that allows a full explanation of why these models learn reliably under the condition that the number of possible labels is greater than the number of item-label pairs in context, and fails to learn reliably when the number of possible labels is equal to the number of item-label pairs in context.",
            "13": "All the paper provides us with important clues toward understanding the computations performed by transformers and of the processes that give rise to their learning dynamics.",
            "14": "Along the way the paper provides an approach to analysis of neural network learning dynamics that others could adopt to understand the learning dynamics as they arise in other setting, another valuable contribution to the effort to understand the complex computations performed by neural networks.",
            "15": "Although I consider the analysis presented a tour de force, possessing all the strengths describe above, it is not perfectly clear that the analysis of the 2-3 parameter reduction would carry over to the full 2-attention-head network of Figure 1c.",
            "16": "A hunch I have is that the L=N case might not be quite as susceptible to failure in the full network because the full network might have a more complex loss landscape with a lower likelihood of being initialized in a place in that landscape that doesn't allow a complete solution.",
            "17": "An important and simple step toward addressing this would be to repeat the L = N simulation in the full network.",
            "18": "If the full network fails to learn in that case, it would confirm the applicability of the analysis to the full network.",
            "19": "Success would not fully invalidate the analysis, but would leave something left to explain.",
            "20": "More generally, I believe more consideration of what will happen in a larger model will be useful for the field.",
            "21": "Clearly things will not work just in the way they do in these reductions when the task is learned in a larger transformer.",
            "22": "While fuller characterization of that will be a task for future work, noting this issue as a limitation of the present effort and pointing considering how these results inform us about what is happening in LLMs will be valuable.",
            "23": "There are two less important weaknesses I'd like to see addressed.",
            "24": "First, I don't feel I have an intuitive understanding of why the loss landscape of the 3 parameter model does not have a saddle point at the point were all three parameters are equal to 0.",
            "25": "Perhaps an understanding of this is latent in the equations and I could work it out with a bit of effort, but to help me (and possibly others) understand, it would be useful if the authors could work out such an intuitive understanding.",
            "26": "Such an understanding could help address reasons why the behaviors of the 2- and 3-parameter reduced models might or might not be applicable to the full model.",
            "27": "Second, paper is harder to read than it should be.",
            "28": "The main deficiency of the paper was its failure to take cognizance of the difficulty of extended chains of arbitrary associative bindings requiring long-distance leaps across context.",
            "29": "It is just such binding that lie at the heart of the mechanisms the authors are investigating, but they are hard for human readers when arbitrary as they often were in this paper.",
            "30": "As examples, we are treated to terms like the former vs the latter as referring expressions, arbitrary labels (a-d) for key phenomena, random ordering of the assignments of these labels to lines in graphs, arbitrary labels for hypotheses (I-V), and the unhelpful placement of figures (esp fig 4) on pages remote from the place in the paper where they are discussed.",
            "31": "Although ultimately the conclusions are stated in (what I find myself to be) conceptual terms, there should be engagement with this conceptual structure in the referential expressions used.",
            "32": "I know space is limited, but I'm sure it is possible to do a better job.",
            "33": "As examples, H3 could be abbreviated sCLA -> ILA+TILA (slow-learned context-label attention -> Item-label attention and Target-item-label association).",
            "34": "Just let a,b,c,d and I-V go.",
            "35": "H4 and H5 should each be expressed directly, or at the very least the order of defining the symbols x and 0/ should correspond to their order of appearance in these hypotheses.",
            "36": "I am also not sure that the difficulty of the L=N case in the"
        }
    },
    "b3LNKq6tfA": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel vision-code transformer (ViCT) architecture that combines a vision encoder and a language decoder to generate HTML/CSS code from UI screenshots.",
            "1": "This approach leverages pre-trained models like ViT/DiT and GPT-2/LLaMA, which is innovative and shows promise in the field of automated code generation.",
            "2": "**Synthetic Dataset Creation**: The authors created two synthetic datasets, RUID and RUID-Large, with over 75,000 unique (code, screenshot) pairs.",
            "3": "This is a significant contribution as it provides a robust dataset for training and evaluating models in this domain.",
            "4": "**Evaluation Metrics**: The introduction of the htmlBLEU metric is a notable contribution.",
            "5": "It aims to provide a more accurate assessment of HTML and CSS code similarity, addressing the limitations of traditional metrics like BLEU in this context.",
            "6": "**Actor-Critic Fine-Tuning**: The use of an actor-critic fine-tuning approach with a visual critic (ViCR) to predict visual discrepancy without rendering is a clever solution to the non-differentiable rendering problem.",
            "7": "This method reduces computational overhead and improves the model's performance.",
            "8": "**Comprehensive Evaluation**: The paper evaluates the proposed model using a combination of automated metrics (MSE, BLEU, IoU, htmlBLEU) and human evaluation.",
            "9": "This thorough evaluation provides a well-rounded understanding of the model's performance.",
            "10": "**Human Evaluation**: Conducting a survey with 59 volunteers to assess the visual similarity between the original and generated UI screenshots adds a valuable human perspective to the evaluation process.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Synthetic Dataset Limitations**: While the synthetic datasets are a significant contribution, they may not fully capture the complexity and diversity of real-world web pages.",
            "13": "The paper acknowledges this limitation, but it remains a critical point that could affect the generalizability of the model.",
            "14": "**Limited Scope**: The current pipeline is limited to generating static text pages and small samples.",
            "15": "This limitation restricts the applicability of the model to more complex and dynamic web pages, which are common in real-world scenarios.",
            "16": "**Hyperparameter Tuning**: The Visual Critic pipeline requires careful tuning of hyperparameters, such as learning rate and rewards, to ensure stable training.",
            "17": "This adds an overhead and complexity to the training process, which could be a barrier for practical implementation.",
            "18": "**Comparison with Existing Methods**: The paper could benefit from a more detailed comparison with existing methods in the field.",
            "19": "While it mentions that traditional pipelines cannot directly address the task, a more in-depth analysis and comparison would strengthen the paper's claims.",
            "20": "**Reproducibility**: Although the authors provide comprehensive details of their experimental setup and include an anonymized copy of the research source code, the reproducibility of the results could be further enhanced by providing more detailed instructions and documentation for using the code and datasets.",
            "21": "**Scalability**: The paper does not address the scalability of the proposed approach to larger and more complex datasets.",
            "22": "Future work should explore the model's performance and scalability on more extensive and diverse datasets.",
            "23": "#### Conclusion:\n\nOverall, the paper presents a novel and promising approach to generating HTML/CSS code from UI screenshots using a vision-code transformer architecture.",
            "24": "The creation of synthetic datasets, the introduction of the htmlBLEU metric, and the use of actor-critic fine-tuning are significant contributions.",
            "25": "However, the limitations related to the synthetic dataset, scope, hyperparameter tuning, and scalability need to be addressed in future work.",
            "26": "The paper provides a solid foundation for further research in this area and demonstrates the potential of transformer architectures for automated code generation."
        },
        "lpa1zDpHk2": {
            "0": "The paper works on an interesting problem of automated reverse engineering of HTML/CSS code from UI screenshots.",
            "1": "The authors propose to formulate the task as a Reinforcement Learning problem to tackle the problem of non-differentiable web rendering.",
            "2": "- The paper is not clearly written.",
            "3": "Some sections are hard to follow (e.g.",
            "4": "Section 3.3).",
            "5": "- Some parts of the paper are inconsistent.",
            "6": "- In Section 4.1, the authors claim to test InstructBLIP[1] as a baseline, but I could not find it in the experimental results.",
            "7": "- In Section 4.1, the authors mention an experiment \"identifying the number of distinct shapes\", which is absent in the paper.",
            "8": "- The main and only datasets the authors use for evaluation are fully synthesized.",
            "9": "The UIs in the dataset only contain three types of elements, Rectangle, Ellipse and Button.",
            "10": "From the examples in Figure 3, I find them quite unrealistic and do not resemble real-world web UIs, which shadows the effectiveness and practical applicability of the model in genuine scenarios.",
            "11": "- Important details on dataset construction and algorithm design are missing (see Questions).",
            "12": "- Experiments are limited.",
            "13": "- Missing baselines, e.g.",
            "14": "Pix2Struct [2].",
            "15": "- The models are only evaluated on two synthetic datasets.",
            "16": "Can you run experiments on other datasets, such as the dataset of pix2code [3]?",
            "17": "- \"DiT-LLaMA\" is missing in Figure 3.",
            "18": "(Minor)\n- In Section 1, \n> In this paper, we take the first step towards reverse-engineering a UI screenshot, i.e., generating an HTML/CSS code that can reproduce the image.",
            "19": "There are prior works on UI-to-Code tasks, such as Pix2Struct[2] and pix2code[3], as you mentioned in Related Works.",
            "20": "Do you mean you are the first to directly generate runable UI code without any postprocessing from images?",
            "21": "- Some typos, e.g.",
            "22": "a missing period at the end of Section 2.",
            "23": "[1] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung and Steven Hoi.",
            "24": "\"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.\"",
            "25": "arXiv preprint arXiv:2305.06500.",
            "26": "2023.",
            "27": "[2] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang and Kristina Toutanova.",
            "28": "\"Pix2struct: Screenshot parsing as pretraining for visual language understanding.\"",
            "29": "International Conference on Machine Learning.",
            "30": "PMLR.",
            "31": "2023.",
            "32": "[3] Tony Beltramelli.",
            "33": "\"pix2code: Generating code from a graphical user interface screenshot.\"",
            "34": "Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems.",
            "35": "2018."
        },
        "8qFNpBSf2r": {
            "0": "The paper is original, as it presents an interesrting problem that can be solved through transformers.",
            "1": "The state of the art is not able to re-create the same results as the proposal.",
            "2": "Interesting technique for generating HTML synthetic data.",
            "3": "**Why RL?",
            "4": "** I understand that it is not possible, given the render, to propagate gradients to the tokens.",
            "5": "However, for the same reason, it is not clear from the paper why this is not a problem when optimising the RL policy.",
            "6": "The authors should better explain the passage in 3.3, as now it is very confusing to understand.",
            "7": "**Missing ablation study.",
            "8": "** The RL algorithm is given some fixed rewards.",
            "9": "How the results changes by varying them?",
            "10": "And how these values have been chosen?",
            "11": "**Confusion around the htmlBLEU** While the authors write a generic description of the metric, it would be easier for readers to read an algorithm.",
            "12": "Also, the proposed metric does not score too different results with respect to BLEU.",
            "13": "**Synthetic data might be harder to parse than real webpages.",
            "14": "** While the introduction of the RUID dataset (and its creation) are very interesting and useful, I argue if the randomness of the approach could generate many samples that are very hard to transform to code, thus impeding the improvement of performance at training time."
        },
        "C4tj81Ru62": {
            "0": "The paper is well-written and easy to follow.",
            "1": "The experimental results are good, demonstrating the effectiveness of proposed method.",
            "2": "The method is incremental in terms of scientific research value, just simply modifying the normal pattern of inserting vision encoder into language models.",
            "3": "The proposed framework is effective in tackling the UI-to-code generation, but not such a fundamental research in representation learning from my perspective."
        },
        "Wq4dZAVupc": {
            "0": "- S1.",
            "1": "The main idea of fine-tuning an image-conditioned text generation model with a reward model and reinforcement learning is very interesting.",
            "2": "Even though the concept of an image-conditioned code generation was proposed before, using foundation models (DiT and Llama) and fine-tuning the model with RL (Policy Gradient method) seems novel.",
            "3": "- S2.",
            "4": "To demonstrate the proof of concept, this paper builds a new dataset for UI to code generation, which contains about 50K pairs of UI and HTML (RUID-Large, Random UI Dataset).",
            "5": "- W1.",
            "6": "Overall architecture of the proposed method (ViCT) seems reasonable.",
            "7": "However, I am not sure that the design choice for the reward modeling and RL fine-tuning is effective.",
            "8": "The overall method is similar to Reinforcement Learning with Human Feedback (RLHF), a recent prevailing method for LLM alignment.",
            "9": "In RLHF, the reward model (RM) is usually modeled by relative feedback (preference or superiority) over a pair of inputs.",
            "10": "Also, the prevalent RL algorithm is Proximal Policy Optimization (PPO) rather than vanilla Policy Gradient (PG).",
            "11": "It would be better to provide some considerations on these design choices.",
            "12": "And, it would be much better to provide a comparison between ViCR (absolute feedback + PG) and RLHF methods (relative feedback + PPO).",
            "13": "- W2.",
            "14": "I am not sure how effectively ViCR models an intermediate reward in Eq 2.",
            "15": "According to Eq 2., \\hat{q_theta}(w_t^s), a value function for the token w_t^s is used.",
            "16": "Can the reward model (ViCR) estimate the value for an intermediate token in partially generated code?"
        }
    },
    "wTJoOqxYUk": {
        "llm_review": {
            "0": "**Innovative Approach**:\n   - The paper introduces a novel method, FuzzPretrain, which leverages dynamic information from program executions to enhance code representation learning.",
            "1": "This is a significant step forward from traditional static analysis methods.",
            "2": "**Comprehensive Evaluation**:\n   - The authors conducted extensive experiments on multiple code understanding tasks, including code search, clone detection, defect detection, and text-to-code search.",
            "3": "The results consistently show that FuzzPretrain outperforms existing methods, demonstrating its effectiveness.",
            "4": "**Detailed Methodology**:\n   - The paper provides a thorough explanation of the FuzzPretrain method, including the use of fuzzing to generate test cases, the integration of static and dynamic information modeling, and the dynamic information distillation process.",
            "5": "This clarity helps in understanding the implementation and potential applications of the method.",
            "6": "**Generalizability**:\n   - The method is shown to be effective across different programming languages and tasks, indicating its robustness and generalizability.",
            "7": "The performance improvements on unseen languages like Ruby further highlight this strength.",
            "8": "**Visualization and Qualitative Analysis**:\n   - The paper includes qualitative studies and visualizations (e.g., t-SNE plots) that provide intuitive insights into how FuzzPretrain improves code representation.",
            "9": "These visual aids help in understanding the practical benefits of the proposed method.",
            "10": "**Comparison with State-of-the-Art**:\n   - The paper compares FuzzPretrain with several state-of-the-art models, demonstrating its competitive edge.",
            "11": "This comparison is crucial for establishing the method's relevance and superiority in the field.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Complexity and Practicality**:\n   - While the method shows significant improvements, the complexity of integrating fuzzing and dynamic information modeling might be a barrier for practical adoption.",
            "14": "The need for a customized fuzzer and the additional computational resources required for pre-training could limit its applicability in real-world scenarios.",
            "15": "**Dependence on Fuzzing**:\n   - The reliance on fuzzing to generate test cases might introduce variability in the quality and coverage of the test cases.",
            "16": "The paper does not thoroughly discuss the potential limitations or challenges associated with fuzzing, such as handling large-scale or highly complex codebases.",
            "17": "**Limited Discussion on Negative Results**:\n   - The paper briefly mentions the performance degradation when certain components (DIM or DID) are removed but does not provide an in-depth analysis of these negative results.",
            "18": "A more detailed discussion on why these components are crucial and how they interact would be beneficial.",
            "19": "**Scalability Concerns**:\n   - The scalability of the proposed method to very large codebases or industrial-scale applications is not addressed.",
            "20": "It would be helpful to understand how FuzzPretrain performs in such scenarios and whether any optimizations are needed.",
            "21": "**Evaluation on More Diverse Benchmarks**:\n   - While the paper evaluates the method on several benchmarks, it would be stronger if it included more diverse and challenging datasets, especially those that are representative of real-world software development environments.",
            "22": "**Clarity on Hyperparameters and Training Details**:\n   - The paper could benefit from more detailed information on the hyperparameters and training settings used in the experiments.",
            "23": "This would aid in reproducibility and allow other researchers to build upon this work more effectively.",
            "24": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in code representation learning by incorporating dynamic information from program executions.",
            "25": "The proposed FuzzPretrain method demonstrates notable improvements over existing approaches and has the potential to impact various code understanding tasks positively.",
            "26": "However, the complexity and practical challenges associated with the method, as well as the need for more detailed discussions on certain aspects, are areas that could be addressed in future work."
        },
        "N6baaG1zNz": {
            "0": "incorporating program executions to pre-train LLMs is new despite a narrow and low-level contribution.",
            "1": "- My biggest concern is the key idea of using program execution to learn code models is just not new.",
            "2": "Authors seem to be completely unaware of the vast literature of neural code models based on dynamic executions.",
            "3": "Here are some papers for authors' reference:\n\n  1. dynamic neural program embedding for program repair\n  2. blended precise semantic program embeddings\n  3.",
            "4": "Improving Neural Program Synthesis with Inferred Execution Traces\n  4.",
            "5": "Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages\n  5.",
            "6": "Code vectors: understanding programs through embedded abstracted symbolic traces\n\n   Even though they do not directly target LLM rightfully so given that LLMs are not even around at the time those papers are published, their works already share the insight of how dynamic execution can benefit learning of code embeddings.",
            "7": "Therefore, it's entirely inappropriate for authors to totally ignore them.",
            "8": "- The evaluation task of clone detection is poorly handled.",
            "9": "First and foremost, clone detection is almost an entirely syntactic task as tools are asked to detect the syntactic similarity of code, however, incorporating execution traces are semantic information that totally ignores the syntactic features.",
            "10": "So this clone detection task does not even match the insight of the paper.",
            "11": "Of course, I am aware of the type 4 semantic clones, however, the question is, how many are those in POJ-104, authors provide no information in this regard, and it's more than reasonable to assume there are very few if any used in the evaluation.",
            "12": "- In ablation study, Fig 3 demonstrates that in Defect DIM and DID is not necessary because removing them actually yields a bigger gain over FuzzPretrain.",
            "13": "This casts doubt on the effectiveness of your technique."
        },
        "C0xywQ15Fm": {
            "0": "- This paper proposes leveraging fuzzing for pre-training which may inspire future techniques for building better pre-training datasets and objectives for code models.",
            "1": "- The approach proposed by this paper yields impressive improvements for code search and smaller improvements across three other code understanding tasks, relative to comparable models which use only static information.",
            "2": "- The pre-training tasks that the authors propose are quite interesting, and the extensive analyses and ablation studies that are included in the paper are helpful for understanding the contributions of these tasks.",
            "3": "- The paper is very well-written.",
            "4": "- The authors seem to suggest novelty in using dynamic program information for learning code representations through claims like \"To the best of our knowledge, this is the first attempt to unveil the benefits of dynamic program information on code representation pre-training\" and \"...we make the first attempt to derive the program functionality from the dynamic information collected during executions.\"",
            "5": "However, there is work that does similar things, one of which they have cited, and others that they have cited.",
            "6": "Namely, they have not cited \"TRACED: Execution-aware Pre-training for Source Code\" (https://arxiv.org/pdf/2306.07487.pdf) leverages dynamic information, specifically executable inputs and corresponding execution traces, for pre-training.",
            "7": "Though fuzzing is not used there, it is used for building code representations in a paper that is cited: \"Understanding Programs by Exploiting (Fuzzing) Test Cases\" (https://arxiv.org/pdf/2305.13592.pdf), though they are not actually using them for pretraining.",
            "8": "It seems that the contribution is more around using specifically fuzzing for pre-training.",
            "9": "I believe this should more clearly be conveyed.",
            "10": "- Related to the previous point, they present results for an approach that does use dynamic information for pre-training: CodeExecutor (https://arxiv.org/pdf/2305.05383.pdf).",
            "11": "However, they do not present this on the main code search task in Table 1 (which is also more aligned with what CodeExecutor was actually benchmarked on).",
            "12": "In fact, many of the \"state-of-the-art\" models listed in Table 3 are not included in Table 1 for code search.",
            "13": "It is not clear why these results were excluded from the paper.",
            "14": "The same goes for the analyses in Figures 4-5.",
            "15": "Since much of the focus was on code search, the ablations and analyses would be based on that.",
            "16": "- As the authors themselves acknowledge in the limitations section, this work is focused on code understanding tasks and no generative tasks.",
            "17": "I find this a bit troublesome because the underlying model that is used, UniXcoder, was originally designed to also handle generative tasks.",
            "18": "CodeExecutor was also benchmarked on code generation.",
            "19": "The authors do not report results for generative tasks like code generation or summarization."
        },
        "W26gahavxB": {
            "0": "+The idea to incorporate program dynamic behaviors in pre-training seems ok to supplement current code pre-trained models.",
            "1": "+This paper is easy to follow and understand.",
            "2": "-The technique novelty is lack.",
            "3": "I agree that the program dynamic information is important and can benefit code pre-trained models, however in this paper, the usage of dynamic information is too easy.",
            "4": "It just concatenates the test cases with the original code for model training.",
            "5": "I am not sure how much dynamic information is contained in the test cases.",
            "6": "Furthermore, I am confused that why test cases are enough for using dynamic information?",
            "7": "Lastly, is there any other way to use dynamic information rather than such a simple way?",
            "8": "-In terms of model design, the novelty is limited.",
            "9": "It uses BERT-style model as the model architecture for pre-training, why not use more powerful encoder-decoder model and there are some works such as CodeT5[1] and CodeT5++[2] have proved encoder-decoder is better than CodeBERT.",
            "10": "Furthermore, the designed pre-trained tasks DIM and DID are also simple, DID is similar to InfoNCE[3].",
            "11": "-In terms of downstream tasks, the evaluation tasks are also limited.",
            "12": "There are only four downstream understanding code tasks, more code-related tasks are need to evaluate to confirm the effectiveness of the proposed approach.",
            "13": "[1] Wang et al.",
            "14": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation.",
            "15": "[2] Want et al.",
            "16": "CodeT5+: Open Code Large Language Models\n\n[3] Liu et al.",
            "17": "Contrabert: enhancing code pre-trained models via contrastive learning"
        },
        "LbiNe1k7zO": {
            "0": "S1: This paper explores a very important domain, as a reliable, semantic-aware code embedding could be helpful for many code-related tasks;  \nS2: This paper proposed an interesting method that combines representation learning and code execution via fuzzing, and provides a simple way of incorporating the test cases into the pretraining tasks;  \nS3: The ablation studies are rather complete for the readers to understand the contribution of each part in the pretraining objectives.",
            "1": "W1: The presentation of this work is quite poor.",
            "2": "More specifically:\n* The notations are a bit (unnecessarily) complex (e.g., as a matter of fact, you can hardly find a symbol without any super/sub-script).",
            "3": "An example is with Equation 2, I don't think it is necessary to write down the formulation of a linear layer for binary classification, nor the cross-entropy loss;\n* Table 1 is a bit hard to parse, I assume they are some kind of \"transfer\" between different programming languages (nothing is mentioned in the caption), but even with that assumption, there are many questions needed to be answered;\n\nW2: Some of the experiment settings are questionable.",
            "4": "See the \"questions\" section for details;  \nW3: Compared with previous work, the improvements are quite marginal (i.e., 1 point or less) on 2 of the 3 reported tasks.",
            "5": "Also, it would be great to mark the parameter count in table 3."
        }
    },
    "gjfOL9z5Xr": {
        "llm_review": {
            "0": "**Novelty and Relevance**: The paper introduces DYVAL, a dynamic evaluation protocol for large language models (LLMs), addressing significant issues such as data contamination and static complexity in existing benchmarks.",
            "1": "This is a timely and relevant contribution given the rapid advancements in LLMs.",
            "2": "**Comprehensive Framework**: DYVAL is presented as a general and flexible protocol that can dynamically generate evaluation samples with controllable complexities.",
            "3": "The use of directed acyclic graphs (DAGs) to generate these samples is innovative and well-justified, leveraging the structural advantages of DAGs for reasoning tasks.",
            "4": "**Diverse Evaluation**: The paper evaluates various LLMs, including state-of-the-art models like GPT-4 and GPT-3.5-Turbo, across multiple reasoning tasks such as mathematics, logical reasoning, and algorithm problems.",
            "5": "This comprehensive evaluation provides valuable insights into the performance and limitations of these models.",
            "6": "**Detailed Analysis**: The authors provide a thorough analysis of the failure cases and the impact of different prompting methods.",
            "7": "This detailed examination helps in understanding the specific areas where LLMs struggle and offers directions for future improvements.",
            "8": "**Practical Utility**: The DYVAL-generated samples are not only used for evaluation but also for fine-tuning LLMs, demonstrating their practical utility in improving model performance on existing benchmarks.",
            "9": "This dual-purpose approach enhances the value of the proposed protocol.",
            "10": "**Human Study**: The inclusion of a human study involving 82 evaluators provides a benchmark for comparing LLM performance against human capabilities.",
            "11": "This adds an additional layer of validation to the findings.",
            "12": "**Open Source**: The availability of the code on GitHub promotes transparency and reproducibility, encouraging further research and development in this area.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Limited Task Scope**: While the focus on reasoning tasks is well-justified, the paper could benefit from exploring a broader range of tasks.",
            "15": "The authors acknowledge this limitation and suggest that DYVAL can be extended to other tasks, but concrete examples or preliminary results in other domains would strengthen the paper.",
            "16": "**Sample Size and Diversity**: The experiments are conducted on a limited set of test samples due to resource constraints.",
            "17": "Evaluating on larger and more diverse datasets could provide a more comprehensive understanding of the models' capabilities and limitations.",
            "18": "**Fine-Tuning Scope**: The fine-tuning experiments are primarily conducted on Llama2-13B-chat.",
            "19": "Extending these experiments to a wider range of models and datasets would provide deeper insights into the generalizability and effectiveness of DYVAL-generated samples for fine-tuning.",
            "20": "**Prompt Sensitivity**: The paper acknowledges that LLMs are highly sensitive to prompts, and the results are based on the authors' prompt design.",
            "21": "While this is a known issue, providing a more systematic approach to prompt engineering or exploring the impact of different prompt designs could add robustness to the findings.",
            "22": "**Complexity Control**: The paper discusses various methods to control the complexity of generated samples, but the impact of each method on model performance is not deeply explored.",
            "23": "A more detailed analysis of how different complexity constraints affect model performance would be beneficial.",
            "24": "**Human Study Demographics**: The human study involves participants with at least a bachelor's degree, but the demographics are not fully detailed.",
            "25": "Providing more information on the participants' backgrounds and expertise levels would help in interpreting the human vs. LLM performance comparison.",
            "26": "**Future Work and Limitations**: While the paper outlines several limitations and future work directions, a more detailed discussion on how these limitations can be addressed in future research would be helpful.",
            "27": "For instance, specific strategies for extending DYVAL to other tasks or improving the generation algorithm could be elaborated.",
            "28": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the evaluation of LLMs with the introduction of DYVAL.",
            "29": "The strengths of the paper lie in its innovative approach, comprehensive evaluation, and practical utility.",
            "30": "However, addressing the identified weaknesses, particularly in terms of task scope, sample size, and prompt sensitivity, would further enhance the robustness and impact of the research."
        },
        "62SrNlTWFL": {
            "0": "DYVAL presents an innovative approach to evaluating LLMs by dynamically generating evaluation samples, mitigating concerns about data contamination and providing a more realistic assessment of LLMs' capabilities.",
            "1": "The paper conducts extensive experiments across various reasoning tasks and LLMs, offering valuable insights into LLM performance, failure patterns, and the impact of different prompt engineering methods.",
            "2": "DYVAL's ability to improve LLMs' performance on existing benchmarks through fine-tuning with DYVAL-generated data demonstrates its practical utility in enhancing LLM capabilities beyond evaluation 1.",
            "3": "The claim on \"co-evolution\" is not clear.",
            "4": "I do not quite understand what co-evolution means.",
            "5": "It seems that the evaluation process is not dependent on the LLM, then how they are correlated from each other.",
            "6": "The data contamination problem is not clear.",
            "7": "Notably, the data generated by the proposed method is rather limited type as it can not generate narrative generation tasks and others related to common sense.",
            "8": "I am wondering how the existing datasets have the contamination problem.",
            "9": "I think such a problem may not happen frequently in the logical reasoning and algorithm domains (especially, these abilities may be majorly from finetune from code and scientific papers).",
            "10": "However, they are much easier to happen on those storytelling data.",
            "11": "The potential bias may exist in the graph generation.",
            "12": "The paper focuses on how to conduct constraints for the graph to avoid illegal ones.",
            "13": "Nonetheless, there may be lacked of details on how the graph is generated to meet those constraints.",
            "14": "I am concerned that the graph generation algorithms remain biased.",
            "15": "Therefore, there will be bias in the generated text, leading to the potential issue."
        },
        "DG1xErIvIc": {
            "0": "- Extensive experiments are conducted.",
            "1": "- Graph-based notions of complexities can be used as a means to control the compositional complexity of the examples.",
            "2": "- Address data contamination and static complexity of the benchmarks.",
            "3": "- A common challenge associated with this framework is the need to manually specify a problem as a computation graph with valid constraints.",
            "4": "This requirement is only understandable if LLM is intended to acquire specific skills written in these formats.",
            "5": "- Before reading this paper, I believed that generating a large number of mathematical problems of specific types and evaluating LLMs on them was primarily for debugging specific LLM capabilities, such as compositionality, rather than as an evaluation framework.",
            "6": "I'm not sure if these types of problems are fundamental questions about LLMs.",
            "7": "In fact, prior studies, such as those by Dziri et al., have already highlighted the limitations of transformers in these settings, using a very similar setup for demonstration.",
            "8": "- It's not clear if LLMs are losing some skills when fine-tuned on DyVal as DyVal examples and the chosen existing benchmarks are from very similar domains.",
            "9": "The generalization of the fine-tuned model on DP is interesting though.",
            "10": "Recommendation:\nAs a person who has worked on dynamic adversarial data collection, or more broadly dynamic benchmarks, I think your review of this literature is underestimating their importance.",
            "11": "In fact, in dynamic adversarial data collection annotators can be provide interesting problem instances hard to find in static benchmarks and even hard to manually specify as a DyVal task.",
            "12": "So, I encourage you to include a better review of these works.",
            "13": "If you are concerned with the human-in-the-loop, I believe the recent theoretical frameworks of dynamic benchmarking are still valid if humans are replaced by generative tools which you may consider mentioning.",
            "14": "So, I encourage you to revisit page 2 paragraph 1 at your discretion.",
            "15": "Overall, I believe that in the era of LLMs, we should explore new methods of evaluation, and this paper's framework might be one of them.",
            "16": "The ICLR audience may find this work interesting, so I will maintain a positive rating despite the concerns I have."
        },
        "mkhIBGrSpw": {
            "0": "The motivation of this paper is clear.",
            "1": "As many LLMs tend to memorize static data for evaluation, this paper proposes a dynamic approach to avoid this kind of problem.",
            "2": "The idea of generating tasks with different difficulties in a DAG style sounds interesting.",
            "3": "The problem is clearly described with sufficient notations and examples.",
            "4": "Experiments are conducted in various aspects, including 7 reasoning tasks, 1 human evaluation, on about 8 well-known LLMs.",
            "5": "Fine-tuning experiments are also conducted to demonstrate that the LLMs' ability in learning to reason.",
            "6": "The title is somewhat misleading.",
            "7": "The evaluation tasks in this paper are mostly about reasoning on maths, logic, algorithms, etc.",
            "8": "However, the title reflects no information about this point.",
            "9": "The abstract could be also clearer if this point can be mentioned earlier.",
            "10": "For the fine-tuning results in Section 5, I wonder when these LLMs are fine-tuned for the reasoning tasks proposed in this method, will the general abilities be influenced?",
            "11": "Or to what extent will they be influenced?",
            "12": "As the samples for evaluation are dynamic, the comparison may be unfair when the generated data are different in different evaluation stages."
        },
        "9mU1CVhcbM": {
            "0": "S1.",
            "1": "Simple, yet flexible framework.",
            "2": "S2.",
            "3": "Dynamic task generation with controllable complexity\nS3.",
            "4": "Extensive evaluation of selected LLMs / prompting strategies for seven simple reasoning tasks.",
            "5": "On S1.",
            "6": "The general idea of the proposed benchmarking framework is to generate tasks that can be described by a directed acyclic graph.",
            "7": "This includes \"compute graphs\" (e.g., evaluate a numerical expression or perform logical reasoning) or \"data graphs\" (e.g., determine connectivity between vertices).",
            "8": "The framework takes care of graph generation, task implementations add contraints, labels, solutions, and verbalization.",
            "9": "This is a very natural approach and (most probably) how many of the existing benchmarks of this form are generated in the first place.",
            "10": "Such a framework may increase usability, especially when many tasks were implemented in it.",
            "11": "On S2.",
            "12": "Tasks are generated automatically and with varying complexity (mainly graph size).",
            "13": "Again, this is a simple, very natural approach.",
            "14": "Here the framework proposed by the paper may make comparative evaluation across a range of tasks more feasible, as all share the same notion of \"complexity\".",
            "15": "On S3.",
            "16": "The paper reports performance results on simple computational tasks (such as evaluating simple equations).",
            "17": "Generally, all models break down when complexity goes up so that the benchmark may be used as a way to evaluate progress.",
            "18": "Also, the performance reported on these simple tasks sometimes contradict performance results published on related, static benchmarks.",
            "19": "W1.",
            "20": "Certain computational tasks only\nW2.",
            "21": "Discussion of related work / results lacking\nW3.",
            "22": "Limitations in generated graphs\nW4.",
            "23": "Code/data availability unclear\nW5.",
            "24": "Limited insight of experimental study\n\nOn W1.",
            "25": "By the nature of the benchmark, it focuses on problems that can be expressed as (currently small) compute graphs or data graphs and are somewhat artificial.",
            "26": "It only tests a very limited field of LLM functionality.",
            "27": "On W2.",
            "28": "There are benchmarks for all of the tasks that are implemented in this framework already.",
            "29": "The paper states that its performance results contradict the ones on some of these benchmarks, but does not say which ones and, perhaps more importantly, does not provide any insight into why this is the case.",
            "30": "Also, the data generation strategies used by existing benchmarks are not discussed.",
            "31": "Finally, to what extent the benchmark can be used to really do new things (beyond existing benchmarks) is not discussed.",
            "32": "On W3.",
            "33": "First, the paper focuses solely on DAGs, but it's unclear why this is done for data graphs (e.g., reachability, max-sum).",
            "34": "Second, it's unclear whether graph size is the right complexity measure.",
            "35": "E.g., for reachability appears easier is source and target are neighbors, no matter how large the graph.",
            "36": "Finally, the system does not seem to generate balanced datasets.",
            "37": "For example, the paper reports in the appendix that the proportion of true answers for reachability is not controlled, leading to \"paradoxical\" results.",
            "38": "On W4.",
            "39": "It's important for benchmarking papers such as this one to make all code, datasets, prompts, results, etc.",
            "40": "public.",
            "41": "The paper currently does not provide any ressources (or, at least, I did not see them).",
            "42": "On W5.",
            "43": "The insight that can be drawn from the experiments is somewhat limited.",
            "44": "I do not count this against the paper, however.",
            "45": "It does show exposed limitations of LLMs and prompting strategies, and it does show that the generated tasks are useful for fine-tuning.",
            "46": "Minor points:\n\nI am not sure how useful the comparison to human performance is.",
            "47": "Clearly, all of the tasks can be solved \"easily\" by humans, it's just a pain to do so."
        }
    },
    "PhJUd3mbhP": {
        "llm_review": {
            "0": "**Innovative Framework**: The introduction of AutoAgents is a significant advancement in the field of multi-agent systems.",
            "1": "The framework's ability to dynamically generate and coordinate specialized agents based on task requirements is a novel approach that addresses the limitations of predefined agents in existing systems.",
            "2": "**Dynamic Agent Generation**: The paper effectively demonstrates the importance of dynamically generating agents tailored to specific tasks.",
            "3": "This adaptability is crucial for handling complex and diverse scenarios, which is a notable improvement over static, predefined agents.",
            "4": "**Two-Stage Process**: The division of the process into Drafting and Execution stages is well-conceived.",
            "5": "This separation allows for thorough planning and refinement before execution, which enhances the overall efficiency and effectiveness of the agent team.",
            "6": "**Self-Refinement and Collaborative Refinement**: The incorporation of self-refinement and collaborative refinement actions is a strong point.",
            "7": "These mechanisms ensure that agents can improve their performance individually and collectively, leading to more accurate and coherent task execution.",
            "8": "**Comprehensive Evaluation**: The paper provides extensive experimental results on various benchmarks, demonstrating the superiority of AutoAgents over existing methods.",
            "9": "The use of both quantitative experiments and case studies adds robustness to the evaluation.",
            "10": "**Practical Applications**: The case study on software development showcases the practical applicability of AutoAgents in real-world scenarios.",
            "11": "This example highlights the framework's potential to enhance collaborative efforts in complex tasks.",
            "12": "**Detailed Methodology**: The paper offers a detailed explanation of the methodology, including the roles of predefined agents (Planner, Agent Observer, Plan Observer) and the execution process.",
            "13": "This clarity helps in understanding the framework's inner workings and its advantages.",
            "14": "#### Weaknesses\n\n1.",
            "15": "**Complexity and Scalability**: While the framework is innovative, its complexity might pose challenges in terms of scalability and implementation.",
            "16": "The need for multiple rounds of communication and refinement could become resource-intensive, especially for very large tasks or agent teams.",
            "17": "**Dependence on LLMs**: The framework heavily relies on large language models (LLMs) like GPT-4.",
            "18": "This dependence might limit its applicability in scenarios where access to such powerful models is restricted due to cost or computational constraints.",
            "19": "**Evaluation Metrics**: Although the paper uses FairEval and HumanEval for evaluation, the metrics could be further diversified.",
            "20": "Including additional metrics such as task completion time, resource utilization, and user satisfaction could provide a more holistic assessment of the framework's performance.",
            "21": "**Generalization to Other Domains**: While the case study in software development is compelling, the paper could benefit from additional case studies in other domains to demonstrate the generalizability of AutoAgents.",
            "22": "This would strengthen the claim that the framework can handle a wide range of tasks.",
            "23": "**Token Limitation**: The issue of token limitation in LLMs is acknowledged, but the proposed solution of short-term, long-term, and dynamic memory might not be sufficient for extremely large tasks.",
            "24": "Further exploration of memory management techniques could enhance the framework's robustness.",
            "25": "**User Involvement**: The framework aims to minimize human supervision, but some level of user involvement might still be necessary for optimal performance.",
            "26": "Clarifying the extent of required user intervention would provide a more realistic picture of the framework's usability.",
            "27": "**Comparative Analysis**: While the paper compares AutoAgents with several existing frameworks, a more detailed comparative analysis, including a discussion on the limitations of these frameworks, would provide a clearer context for the improvements introduced by AutoAgents.",
            "28": "#### Conclusion\n\nOverall, \"AutoAgents: A Framework for Automatic Agent Generation\" presents a significant advancement in the field of multi-agent systems.",
            "29": "The framework's ability to dynamically generate and coordinate specialized agents based on task requirements is a notable innovation.",
            "30": "The detailed methodology, comprehensive evaluation, and practical applications demonstrate the framework's potential to enhance collaborative problem-solving in complex tasks.",
            "31": "However, addressing the identified weaknesses, such as complexity, scalability, and generalization to other domains, would further strengthen the framework and its applicability."
        },
        "XUrmOMqHzo": {
            "0": "This study provides a valuable clarification of its position,\nespecially in the context of LLM-based Agent frameworks.",
            "1": "In comparison\nto AgentVerse and SSP, this research stands out by highlighting the\nsignificance of Self-Refinement agents and Collaborative Refinement\nAction as key differentiators.",
            "2": "The paper is perceived as having low readability and insufficient\nreproducibility.",
            "3": "The reviewer kindly requests a more granular\ndescription of the methodology that enables readers to implement the\nprocedures step by step.",
            "4": "For instance, while Table 1 is highly\nbeneficial for positioning this research within the LLM-based Agent\nframework, in comparison to AgentVerse and SSP, it distinctly\nhighlights the significance of Self-Refinement agents and\nCollaborative Refinement Action.",
            "5": "Nevertheless, the two points\nmentioned above are not clearly articulated in Section 3.2, \"EXECUTION\nSTAGE.\"",
            "6": "They are mentioned in the text and Figure 2, but their\npresentation as steps is absent, making it challenging for readers to\ncomprehend and evaluate reproducibility.",
            "7": "The evaluation in the experiments lacks qualitative insights.",
            "8": "In the\nexperiments, it remains unclear how the introduction of\nSelf-Refinement agents and Collaborative Refinement Action has led to\ndifferential outcomes compared to SSP, and what specific effects these\ntwo points have had.",
            "9": "While accuracy has undeniably improved, it is\nessential to qualitatively demonstrate how these two aspects have\ncontributed to the observed results.",
            "10": "There are concerns regarding the reproducibility of the\nexperiments.",
            "11": "It is unclear whether the CASE STUDY has been practically\nrealized or if it serves as an imagined example for\napplication.",
            "12": "The paper lacks an ablation study to assess the impact of modifying or\nomitting certain components within the system, particularly in the\nDraft and Execution phases where multiple agents are involved, such as\nAgent Observer, Plan Observer, Researcher, Planner, Writer, Character\nDeveloper, and others.",
            "13": "This study could help elucidate the\nsignificance of each component and its contribution to the overall\nsystem.",
            "14": "Furthermore, the absence of an ablation study regarding\nShort-term memory, Long-term memory, and Dynamic memory raises\nconcerns.",
            "15": "Investigating the effects of altering or removing these\nmemory components could provide valuable insights into their\nrespective roles and importance within the framework.",
            "16": "Overall,\nconducting such ablation studies would enhance the paper's depth and\nprovide a more comprehensive understanding of the system's inner\nworkings and the role of its individual components."
        },
        "KUTJW9Ftwy": {
            "0": "- Clear presentation of high-level idea: the overall framework and process is clearly presented through well-drawn figures like Fig.",
            "1": "1 and 2.",
            "2": "- Strong reproducibility: the author provides source code and the temperature of LLM is set to 0, which makes it easy to reproduce the result in the paper.",
            "3": "- Limited novelty: according to Table 1, the main difference between the proposed framework and other existing methods like Social Simulacra, Epidemic Modeling, SSP, and AgentVerse is that this work uses self-refinement and collaborative refinement.",
            "4": "This difference is more of a prompting technique and has already been used in many existing works like [1, 2, 3]\n- Unclear presentation of detailed techniques: though the high-level idea is well-presented, the details of many technique are unclear.",
            "5": "For example, how to determine when and which agents should engage in collaborative refinement?",
            "6": "This is the main differnce from other methods but there is very little detailed description.",
            "7": "More questions are in the Question part.",
            "8": "- Insufficient evaluation: \n    1.",
            "9": "Lack of baselines: Table 1 lists 12 existing frameworks, but none is used as baseline in task 1, and only 1 is used in task 2.",
            "10": "Comparisons with existing methods are needed to show the effectiveness of the proposed methods.",
            "11": "Lack of ablation study: there is no quantitive ablations on different components of the framework like self-refinement, collaborative refinement, etc.",
            "12": "Unfair comparisons and potential problem in metric: in task 1, it is unfair to compare AutoAgents using GPT-4 with ChatGPT and Vicunna-13B.",
            "13": "In task 2, the metric only considers the QA quality, how about the quality of the story around the given topic?",
            "14": "Reference:\n\n[1] Noah Shinn, Beck Labash, and Ashwin Gopinath.",
            "15": "Reflexion: an autonomous agent with dynamic memory and self-reflection.",
            "16": "arXiv preprint arXiv:2303.11366, 2023.",
            "17": "[2] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.",
            "18": "Critic: Large language models can self-correct with tool-interactive critiquing, 2023.",
            "19": "[3] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.",
            "20": "Inner monologue: Embodied reasoning through planning with language models.",
            "21": "arXiv preprint arXiv:2207.05608, 2022."
        },
        "CZlayiAqtq": {
            "0": "This paper presents a framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks.",
            "1": "The paper is technically sound and the research question is clear.",
            "2": "The contribution of the paper is relevant for LLM-based multi-agent collaboration.",
            "3": "The results of this paper is interesting and significant in automatic agent generation.",
            "4": "The proposed AutoAgents framework generates more coherent and accurate solutions than the existing multi-agent methods.",
            "5": "How the proposed AutoAgents framework expands the scope of collaborative applications and reduces the consumption of resources should be elaborated.",
            "6": "The authors do not explain how to determine the number of agents in the section of the framework for automatic agent generation.",
            "7": "The section about automatic agent generation is too tedious to introduce too much related works\n4.",
            "8": "In addition to ChatGPT, Vicuna-13B and GPT4 in Table 2, it has not enough recent models to further show the superiority of the proposed framework-AutoAgents in open-ended question answer task in the experimental part.",
            "9": "In the experimental part, the performance on N=10 is better than N=5 in trivia creative writing task, but there is no explanations."
        },
        "XZLGgAbXIr": {
            "0": "The idea of dynamically generating agents who play different roles to solve team tasks is interesting and useful.",
            "1": "I found the idea to be novel.",
            "2": "It is easy for the reader to get a good overview of the idea of AutoAgents.",
            "3": "However, there was a need to look at supplementary materials to understand aspects of what the different predefined roles were supposed to do.",
            "4": "The visuals helped me understand the idea better.",
            "5": "The background was sufficient, in my opinion, and well-written.",
            "6": "This discussion and Table 1 made the contributions clear.",
            "7": "Section 3:\nFor the agent generation, the motivation for the format of the Prompt P is unclear.",
            "8": "Additionally, when we look at the supplementary material, the specific elements of the prompt are not explained -- are these taken from existing works?",
            "9": "Others:\nI also found details that needed to be included in a few other sections, such as the self-refinement process.",
            "10": "Furthermore, I had questions about specific choices of parameters during the evaluations.",
            "11": "I have included my questions in the next part to capture the specific places where I needed more information.",
            "12": "Minor typos:\nPage 2: effectiveness of AutoAgents.",
            "13": "[we] also conduct\nPage 7: at = lt ∪ pt ∪ ot, [where lt,] where lt denotes"
        }
    },
    "h1SSQ6Dekc": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method for simultaneous machine translation (SiMT) using decoder-only large language models (LLMs).",
            "1": "This is a significant departure from the traditional encoder-decoder transformer architectures, showcasing the versatility and potential of LLMs in new application areas.",
            "2": "**Policy-Free System**: The proposed system, TRANS LLAMA, eliminates the need for a separate policy to control input segmentation.",
            "3": "Instead, it uses a special \"wait\" token generated by the LLM, simplifying the architecture and potentially reducing the complexity of the system.",
            "4": "**Performance**: The results demonstrate that the fine-tuned LLM can achieve BLEU scores comparable to state-of-the-art SiMT systems for English-German and English-Russian translation tasks.",
            "5": "This is a strong indicator of the effectiveness of the proposed method.",
            "6": "**Zero-Shot Evaluation**: The paper also evaluates closed-source models like GPT-4 in a zero-shot setting, showing promising results.",
            "7": "This highlights the potential for using pre-trained LLMs without additional fine-tuning for SiMT tasks.",
            "8": "**Comprehensive Evaluation**: The authors provide a thorough evaluation of their system, including comparisons with recent baselines, analysis of the quality-latency tradeoff, and the impact of ASR errors on performance.",
            "9": "This comprehensive evaluation adds credibility to their findings.",
            "10": "**Detailed Methodology**: The paper provides a detailed description of the methodology, including data preparation, fine-tuning, and inference processes.",
            "11": "This transparency allows for reproducibility and a clear understanding of the approach.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Language Pairs**: The experiments are limited to English-German and English-Russian language pairs.",
            "14": "While these are significant language pairs, the generalizability of the approach to other languages remains untested.",
            "15": "**ASR Integration**: The integration of the ASR system (Whisper) is somewhat naive, as acknowledged by the authors.",
            "16": "The fixed audio windowing approach can lead to clipped words, which affects the overall performance.",
            "17": "More sophisticated ASR integration could potentially improve results.",
            "18": "**Inference Speed**: The real-time factor (RTF) for the proposed system, especially for the larger LLMs, is quite high.",
            "19": "This could be a bottleneck for real-world applications where low latency is crucial.",
            "20": "The paper does discuss potential solutions, but these are not implemented or tested.",
            "21": "**Dependence on Pre-trained Models**: The approach heavily relies on the capabilities of pre-trained LLMs like Llama-2 and GPT-4.",
            "22": "While this showcases the power of these models, it also means that the system's performance is tied to the availability and quality of these pre-trained models.",
            "23": "**Prompt Engineering**: The system's reliance on prompt engineering, especially the use of a system message, adds an additional layer of complexity.",
            "24": "The impact of different prompt structures on performance is not thoroughly explored.",
            "25": "**Evaluation Metrics**: While BLEU scores are a standard metric for translation quality, they may not fully capture the nuances of simultaneous translation, such as the naturalness and fluency of the output.",
            "26": "Additional metrics or human evaluations could provide a more comprehensive assessment.",
            "27": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of simultaneous machine translation by leveraging the capabilities of large language models.",
            "28": "The innovative policy-free approach and the promising results on standard benchmarks are commendable.",
            "29": "However, there are areas for improvement, particularly in terms of generalizability, ASR integration, and inference speed.",
            "30": "Future work addressing these issues could further enhance the practicality and performance of the proposed system."
        },
        "GheNouW7iH": {
            "0": "- Though there are many papers about training LLMs to as decision-making agent, I consider doing so with Simultaneous translation, which is predominantly about speech, is novel and the task of SiMT can improved with the help from LLM.",
            "1": "- The results show comparable with existing high-quality SiMT baselines, though I highly doubt the actual computational cost is anywhere comparable (see weakness).",
            "2": "Future work should make up for this by achieve higher translation quality and latency, as well as in other lower-resource languages.",
            "3": "- Repeated inference of LLM is a huge computational cost, everytime a <wait> token is generated, the text prompt is updated and many tokens representations have to be recalculated without a theoretical room for caching.",
            "4": "As such, real-life inference, with a fixed physical hardware, will be much slower compared to existing lightweight translation model.",
            "5": "This is true for closed-source GPT models as well, as more tokens called to the API leads to more expensive bill.",
            "6": "- Therefore, I urge the authors to provide a real-life inference cost/wall-time comparison to have a better picture of the cost trade-off here and makes the paper complete.",
            "7": "I would appreciate and change scores if such report is produced."
        },
        "cWrZiwwm87": {
            "0": "The setup is described clearly and is very straightforward, which makes this work easily reproducible.",
            "1": "I also appreciate the results section, which includes the most natural ablations and is not overselling the results.",
            "2": "In fact, the most obvious concern of using LLMs for SiMT - inference time - is acknowledged in the paper.",
            "3": "The evaluation is based on (just) two language pairs and two LLM (sizes), which is definitely on the slim side, but it meets the minimum bar for me.",
            "4": "I don't think that this paper is particularly innovative.",
            "5": "On a high level, it strikes me as one of the \"we tried LLMs for task X and it worked\" papers that are very common these days.",
            "6": "That being said, I think that this is one of the better papers in that category due to the sober evaluation and clear writing.",
            "7": "So although I wasn't inspired by this work, there is still value in publishing it for the sake of completeness of the body of literature on LLMs.",
            "8": "Fig.",
            "9": "2 looks broken..",
            "10": "I guess the key point here is that \"away\" is aligned to \"befreite\", but the alignment link is not shown in the original en-de alignment."
        },
        "tw0pmS8qWr": {
            "0": "- The concept of employing large language models for simultaneous translation appears both novel and exciting.",
            "1": "- The paper is clearly written and easy to understand.",
            "2": "- The related work section provides a comprehensive summary of simultaneous translation research in the field of natural language processing.",
            "3": "- Figure 1 lacks clarity in terms of distinguishing when specific actions (READ/WRITE) occur.",
            "4": "It would be more reader-friendly if the figure illustrated a step-by-step walkthrough (e.g., t=1, t=2, t=3).",
            "5": "- In simultaneous translation, wall-clock time (actual speed) is a critical factor.",
            "6": "It would be important to report or at least mention how long it takes to generate translations in this setting.",
            "7": "- The experiment only presents BLEU scores; it lacks concrete examples of output, which would be beneficial for understanding the translation quality."
        },
        "jfebyH4La1": {
            "0": "The paper introduces a novel approach to improving SiMT systems by fine-tuning a pre-trained Language Model (LLM) with direct supervision on causally aligned source-target sentence pairs.",
            "1": "This approach differs from previous methods that rely on separate policies or incremental decoding.",
            "2": "By leveraging the capabilities of LLMs, the paper offers a fresh perspective on enhancing SiMT performance.",
            "3": "One of the main concerns regarding this paper is the reliance on a reference-based approach for the causal alignment introduced.",
            "4": "While the paper claims to propose a novel method, similar ideas have been studied in previous simultaneous translation literature (e.g.",
            "5": "[1]).",
            "6": "However, the paper lacks a comparative analysis with these existing approaches in the experiment section, making it difficult to assess the novelty and superiority of the proposed method.",
            "7": "Furthermore, a significant limitation of the reference-based approach is the potential mismatch between full sentence translation and simultaneous translation.",
            "8": "The references used to generate the causal alignment are derived from complete sentence translations, which may not be suitable for the dynamic nature of simultaneous translation.",
            "9": "Simultaneous translation requires the model to begin translation based on partial context, and the reference-based approach may not adequately capture the challenges and nuances specific to this task.",
            "10": "[1] Simultaneous translation policies: from fixed to adaptive.",
            "11": "ACL, 2020"
        }
    },
    "kvcbV8KQsi": {
        "llm_review": {
            "0": "**Novel Concept Introduction**: The paper introduces the concept of \"successor heads,\" a type of attention head in large language models (LLMs) that increment tokens in natural sequences (e.g., days of the week, numbers).",
            "1": "This is a novel and interesting addition to the field of mechanistic interpretability.",
            "2": "**Mechanistic Interpretability**: The authors provide a detailed mechanistic explanation of how successor heads function, which is a significant contribution to understanding the internal workings of LLMs.",
            "3": "This is particularly valuable given the complexity and opacity of these models.",
            "4": "**Universality Hypothesis**: The paper provides evidence supporting a weak form of the universality hypothesis, showing that successor heads appear across different model architectures and scales.",
            "5": "This is an important finding that could influence future research directions in interpretability.",
            "6": "**Compositional Numeric Representations**: The discovery of compositional numeric representations, such as the \"mod-10\" features, is a significant contribution.",
            "7": "This finding enhances our understanding of how LLMs handle numeric data and ordinal sequences.",
            "8": "**Empirical Validation**: The authors validate their findings through extensive experiments across various models (e.g., GPT-2, Pythia, Llama-2) and tasks.",
            "9": "This thorough empirical approach strengthens the credibility of their claims.",
            "10": "**Interpretable Polysemanticity**: The identification of polysemantic behavior in successor heads, particularly the dual roles in successorship and acronym prediction, is a novel and intriguing finding.",
            "11": "This adds depth to our understanding of how attention heads can perform multiple functions.",
            "12": "**Practical Implications**: The paper demonstrates the practical importance of successor heads in natural language tasks, showing that these heads are crucial for achieving low loss in tasks involving succession.",
            "13": "This practical relevance is a strong point of the paper.",
            "14": "#### Weaknesses\n\n1.",
            "15": "**Complexity and Accessibility**: The paper is highly technical and may be challenging for readers who are not already familiar with the intricacies of LLMs and mechanistic interpretability.",
            "16": "While the detailed explanations are a strength, they also make the paper less accessible to a broader audience.",
            "17": "**Greater-Than Bias**: The paper identifies a \"greater-than bias\" in successor heads but does not fully explain the underlying cause of this bias.",
            "18": "Further exploration and explanation of this phenomenon would strengthen the paper.",
            "19": "**Limited Scope of Tasks**: While the paper covers a range of tasks, it primarily focuses on numeric and ordinal sequences.",
            "20": "It would be beneficial to explore whether successor heads play a role in other types of tasks or data representations.",
            "21": "**Generalization to Other Models**: Although the paper provides evidence for the presence of successor heads in several models, it would be useful to see a broader range of models and architectures tested to further validate the universality hypothesis.",
            "22": "**Acronym Behavior**: The identification of acronym behavior in successor heads is interesting, but the paper could benefit from a deeper exploration of this behavior.",
            "23": "Specifically, understanding how and why successor heads develop this capability would add valuable insights.",
            "24": "**Sparse Autoencoders**: The use of sparse autoencoders to identify mod-10 features is innovative, but the paper could provide more details on the limitations and potential biases of this method.",
            "25": "Additionally, exploring alternative methods for feature identification could strengthen the findings.",
            "26": "**Practical Applications**: While the paper demonstrates the importance of successor heads in natural language tasks, it could benefit from a discussion on potential practical applications and implications of these findings in real-world scenarios.",
            "27": "#### Conclusion\n\nOverall, the paper \"Successor Heads: Recurring, Interpretable Attention Heads in the Wild\" makes significant contributions to the field of mechanistic interpretability of LLMs.",
            "28": "The introduction of successor heads, the evidence supporting the universality hypothesis, and the discovery of compositional numeric representations are all valuable findings.",
            "29": "However, the paper could be improved by addressing the identified weaknesses, particularly in terms of accessibility, deeper exploration of identified behaviors, and broader validation across different models and tasks."
        },
        "3UmSZ4IwHn": {
            "0": "The empirical rigor of this work is high.",
            "1": "The authors provide several ablations to argue the existence of mod_$10$ features in transformers.",
            "2": "They moreover provide detailed additional information for experiments in the appendix.",
            "3": "The finding of successor heads is interesting and provides a good framework for understanding how transformers reason about incrementation.",
            "4": "The connection between incrementation and acronym prediction observed in successor heads is interesting.",
            "5": "The paper lacks a proper background section.",
            "6": "Terms like OV matrix are not introduced, and more generally, the notion of circuits, or what un enembedding is, are never properly defined.",
            "7": "This makes the paper very hard to digest without being familiar with the concepts of transformer circuits (Elhage '21).",
            "8": "Given that the paper is already quite dense, you can e.g.",
            "9": "move figure 7 to the appendix, and properly lay the appropriate terminology to understand this work."
        },
        "0rihrnLYVc": {
            "0": "Strengths:\n1.",
            "1": "The paper is clear and easy to read.",
            "2": "To show claims of a weak form of universality, the paper thoroughly tests for successor scores across several models for various numbers of parameters.",
            "3": "The experiments to find and verify the mod-10 features are also thoroughly performed.",
            "4": "These features were confirmed by several different methods: first by training a sparse autoencoder using reconstruction loss on the MLP0 outputs, and then by further comparisons to linear probing and ablation to reinforce these observations.",
            "5": "This is an interesting result uniting various tokens under mod-10 classes.",
            "6": "There are also interesting results with the natural language experiments that demonstrate interpretable polysemanticity.",
            "7": "Weaknesses:\n1.",
            "8": "In Section 4, direct effect mean ablation is used to show \"that when the successor head is contributing usefully, the prompts often required some kind of incrementation\".",
            "9": "Then in Appendix J, direct effect mean ablation is used again to show that the successor head is the most important head across 64 prompts.",
            "10": "Though this is stated with some evidence, not enough quantifiable evidence is shown here to justify the reach of these claims, such as the statement of \"mostly solved\".",
            "11": "More analysis can also be shown about the \"direct effect\" to separate it from \"indirect effects\".",
            "12": "The paper also did not clarify the details of the ablation, such as if it used resampling ablation (there may be issues if it used mean ablation from the same dataset, as there are known issues with mean ablation [1]), and/or path patching (to obtain direct effects).",
            "13": "[1] https://www.lesswrong.com/posts/kcZZAsEjwrbczxN2i/causal-scrubbing-appendix#3_Further_discussion_of_zero_and_mean_ablation\n\n2.",
            "14": "The paper mentions that vector addition was performed successfully for 89% of the cases for digits 20-29, and mentions how it was performed on token '35'.",
            "15": "It does not mention how this performed for other digits.",
            "16": "This is likewise the case for number words only showing ten to twenty.",
            "17": "Presumably, the performance is similar, but the paper should explicitly mention this to avoid criticism of cherry-picking.",
            "18": "In Figure 7, it's also unclear what \"target residues modulo 10\" means when referring to the column headers.",
            "19": "Presumably, this is stating something similar to how the vector arithmetic on MLP0(E('fifth') makes it \"behave more like MLP0(E('seventh')\".",
            "20": "The wording can be made clearer to avoid confusion that it means the number \"7\" rather than the word \"seventh\".",
            "21": "Additionally, the checkmarks are given when \"the max logits are on the successor token\".",
            "22": "This is an interesting result, but how big is the logit difference between logits for the successor token and other tokens?",
            "23": "Appendix D states that scaling was used on the additive feature terms.",
            "24": "A quick explanation of why a particular scaling factor was used would be helpful.",
            "25": "The paper states: \"to the best of our knowledge the presence of both successorship and acronym behavior in head L12H0 is the cleanest example of polysemantic behavior identified so far in an LLM.\"",
            "26": "Why is this the cleanest example of polysemantic behavior, compared to other studies on the topic such as in [2]?",
            "27": "Similarly for this statement, \"which to the best of our knowledge are the most closely studied components in LLMs that occur in both small and large models\", what other components are you comparing to that are not as closely studied?",
            "28": "[2] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas.",
            "29": "Finding neurons in a haystack: Case studies with sparse probing, 2023\n\n4.",
            "30": "This paper discovers novel and interesting observations, but it does not elaborate much on why this observation is impactful enough."
        },
        "6SwWiTBpXv": {
            "0": "Good informative figures such as Fig 1 and Fig 7, clear writing.",
            "1": "The use of OV circuits in the discovery and analysis seems smart and somewhat novel to standard methodology for these kinds of findings.",
            "2": "Interesting behavior and good multi-pronged analysis of it.",
            "3": "Somewhat overclaiming the contribution:\nFor example abstract says:  \"Existing research in this area has found interpretable language model components in small toy models.",
            "4": "However, results in toy models have not yet led to insights that explain the internals of frontier models and little is currently understood about the internal operations of large language models.\"",
            "5": "This makes it sound like existing work has only studied toy models which is not true, while also making it sound like this work would study frontier models which is not the case.",
            "6": "While they look at larger models than most related work, the wording makes it sound like difference is larger than it is.",
            "7": "Also the findings about mod 10 features are almost entirely based on the setting of incremental numbers which makes sense, while the writing makes it sound like they are behind successor head behavior on all tasks.",
            "8": "The only evidence of these being used on other task is a low success percentage on changing output month with vector arithmetic.",
            "9": "I would expect for tasks like months and days there would be other mod-12 or mod-7 features for example that could explain this behavior, was this studied?"
        },
        "XpaDH0136J": {
            "0": "The findings presented in this paper are significantly novel.",
            "1": "Authors have clearly described the functions of successor heads and designed multiple experiments to validate their hypothesis.",
            "2": "I especially appreciate Section 3.3 where the evidence in arithmetic is a strong proof that the activation of success attention indeed captures the natural ordering of words and is responsible for the LLM’s reasoning.",
            "3": "There are a few issues mostly in the presentation of the work.",
            "4": "I am getting really annoyed when the authors place all definitions, i.e.",
            "5": "the Glossary section, at the end of the appendix.",
            "6": "It is really inconvenient for the reader to go back and forth during the reading.",
            "7": "It has to have better ways to present the definitions in the context.",
            "8": "Please do not do this.",
            "9": "It lacks sufficient descriptions for the reader to understand the process that parses the original output of the attending heads to the sparse encoder’s output.",
            "10": "I understand that this is to make more room to present the findings; however, it makes the methodology part pretty unclear from reading the current version.",
            "11": "I have to go back and forth and spend a lot more time on Section 2 and 3 to make sure I understand the way each figure is plotted."
        }
    },
    "3d0OmYTNui": {
        "llm_review": {
            "0": "**Novelty and Relevance**: The paper addresses a highly relevant and timely issue in the field of machine learning, specifically the privacy concerns associated with aligning large language models (LLMs) using reinforcement learning (RL).",
            "1": "Given the increasing deployment of LLMs in various applications, ensuring privacy is crucial.",
            "2": "**Comprehensive Framework**: The authors propose a detailed and well-structured framework for achieving differential privacy (DP) in the alignment of LLMs.",
            "3": "The framework is divided into three main steps: supervised fine-tuning, learning a reward model, and fine-tuning a policy using a DP adaptation of Proximal Policy Optimization (PPO).",
            "4": "This structured approach makes the methodology clear and replicable.",
            "5": "**Theoretical Guarantees**: The paper provides strong mathematical guarantees for the privacy of the alignment process.",
            "6": "The use of differential privacy ensures that the final model does not leak sensitive information from the training data, which is a significant contribution to the field.",
            "7": "**Empirical Validation**: The authors validate their approach through extensive experiments on commonly studied tasks, such as sentiment generation and summarization.",
            "8": "The results demonstrate that it is possible to achieve competitive utility while ensuring strong privacy protections.",
            "9": "**Detailed Experimental Setup**: The paper provides a thorough description of the experimental setup, including the datasets used, model architectures, and hyperparameters.",
            "10": "This level of detail is beneficial for reproducibility and for other researchers looking to build on this work.",
            "11": "**Addressing Practical Concerns**: The authors discuss practical considerations, such as the use of LoRA for computational efficiency and stability in DP training.",
            "12": "This shows an understanding of the real-world challenges in implementing DP in large-scale models.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Complexity and Accessibility**: The proposed framework, while comprehensive, is quite complex.",
            "15": "The paper assumes a high level of familiarity with both reinforcement learning and differential privacy, which might make it less accessible to a broader audience.",
            "16": "Simplifying some of the explanations or providing more background information could help.",
            "17": "**Limited Scope of Evaluation**: The empirical evaluation, although thorough, is limited to two specific tasks: sentiment generation and summarization.",
            "18": "While these are important tasks, it would be beneficial to see the framework applied to a wider range of tasks to demonstrate its generalizability.",
            "19": "**Privacy-Utility Trade-off**: The paper discusses the privacy-utility trade-off but does not delve deeply into the potential limitations of this trade-off.",
            "20": "For instance, the performance gap between private and non-private models is noted, but more discussion on how to mitigate this gap or the implications of this trade-off in practical applications would be valuable.",
            "21": "**Scalability Concerns**: While the use of LoRA is a step towards addressing computational efficiency, the scalability of the proposed framework to even larger models or more complex tasks is not thoroughly explored.",
            "22": "Given the rapid growth in model sizes, this is an important consideration.",
            "23": "**Future Work and Open Questions**: The paper opens up several technical questions, such as improving DPPPO algorithms and adapting the framework to online settings.",
            "24": "However, it does not provide concrete directions or preliminary insights into how these challenges might be addressed.",
            "25": "Including a more detailed discussion on future work could strengthen the paper.",
            "26": "**Comparison with Other Privacy Methods**: The paper primarily focuses on differential privacy.",
            "27": "While this is a strong approach, it would be beneficial to compare the proposed method with other privacy-preserving techniques, such as federated learning or homomorphic encryption, to provide a more comprehensive view of the landscape.",
            "28": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the field by addressing the critical issue of privacy in the alignment of large language models using reinforcement learning.",
            "29": "The proposed framework is well-structured and theoretically sound, with empirical results that validate its effectiveness.",
            "30": "However, the complexity of the framework, limited scope of evaluation, and scalability concerns are areas that could be improved.",
            "31": "Future work should focus on expanding the range of tasks evaluated, exploring scalability, and providing more detailed directions for addressing open questions."
        },
        "dBfJTWyVRz": {
            "0": "The paper proposes a differentially private framework for aligning LLMs with RL, offering mathematical guarantees of privacy.",
            "1": "The paper empirically evaluates the framework on tasks like positive review generation and summarization, showing that it offers competitive utility while ensuring strong privacy protections.",
            "2": "The paper employs DPSGD to ensure privacy in the alignment of large language models through reinforcement learning.",
            "3": "While the use of DPSGD is well-established in the privacy literature.",
            "4": "Furthermore, the paper does not introduce significant modifications to the RLHF process.",
            "5": "The innovation seems to be more focused on engineering adjustments rather than novel theoretical contributions.",
            "6": "The paper discusses the trade-offs between privacy and utility but does not present these results in an intuitive manner.",
            "7": "A Pareto frontier could be more illustrative in showing how different levels of privacy (varying ε) impact the model's performance.",
            "8": "This would provide a clearer understanding of the trade-offs involved.",
            "9": "If the reward in step 2 is DP, is it necessary to use the DPPPO in step 3 as the learning reward is already DP?"
        },
        "ohPgFfwTif": {
            "0": "The paper combines two well-understood algorithms, DP-SGD and PPO in a well-motivated task of reinforcement with human feedback and supervised fine-tuning.",
            "1": "The experiments and the results are clear and the application of private model fine-tuning is reasonable.",
            "2": "The paper is well organized and the writing is clear.",
            "3": "The paper is well written and the algorithm is clearly explained.",
            "4": "Their claims are based on the GPT-2 family of models and run experiments on the ROUGE metrics and the TweetEval benchmark.",
            "5": "The authors offer a privacy-preserving technique to undertake reinforcement learning with human feedback.",
            "6": "They combine DP-SGD and PPO with a few adaptations and show utility benefits on NLP benchmarks.",
            "7": "While the examples are helpful, the overall motivation could be a bit stronger.",
            "8": "What are we protecting and why?",
            "9": "What is the threat model around incorporating human feedback?",
            "10": "Are their examples of memorization from human feedback?",
            "11": "The experiments in the main body do not include error or number of trials details.",
            "12": "It is unclear in Table 1 why models with less provacy should do worse than those with more privacy (GPT-2 Medium, eps 4->8, or GPT-2 Large eps 8->Inf).",
            "13": "Such results demand further study and/or ablations and are difficult to interpret without confidence intervals.",
            "14": "The use of corporate imagery (Reddit / OpenAI) weakens the overall presentation and the generality of the results.",
            "15": "Work in differential privacy and RL can be traced to differentially-private policy evaluation (Balle, Gomrockchi, Precup).",
            "16": "The paper touches on the privacy accounting implications when  $T_{\\text{PPO}} \\neq 1$\n, but does not offer evaluate the implication of fixing it to the default value of 4."
        },
        "HKjGsy9YeW": {
            "0": "(1) The approach proposed in this paper for aligning language models with PPO in a privacy-preserving way is original; (2) The paper is clearly written and well-organized.",
            "1": "(3) The paper gives a quite comprehensive analysis of the procedure and emphasize the difficult issues in the implementation.",
            "2": "(1)The DP part is too condensed to understand.",
            "3": "The authors used DP-SGD on several occasions but without a clear explanation of this algorithm.",
            "4": "And in the main text, I could not find a concrete DP algorithm and a clear procedure how it is combined with the alignment.",
            "5": "(2) Algorithm 1 is not original.",
            "6": "I don’t see the reason why it was presented in detail in the paper.",
            "7": "The PRIVATE alignment should be more interesting."
        }
    },
    "LZT9T57Bg0": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel method, LARK, which integrates logical reasoning over knowledge graphs (KGs) with the capabilities of large language models (LLMs).",
            "1": "This is a significant advancement over traditional methods that rely on geometric embeddings.",
            "2": "**Performance Improvement**: The experimental results demonstrate that LARK outperforms state-of-the-art KG reasoning methods by a substantial margin (35%-84% MRR improvement) across various logical query constructs.",
            "3": "This is a strong indicator of the effectiveness of the proposed approach.",
            "4": "**Scalability**: The paper shows that the performance of LARK improves with the increase in the size of the underlying LLM.",
            "5": "This scalability is crucial for leveraging future advancements in LLMs.",
            "6": "**Chain Decomposition**: The logical query chain decomposition mechanism is a key innovation.",
            "7": "It simplifies complex queries into simpler ones, which LLMs can handle more effectively.",
            "8": "This approach significantly enhances the reasoning capabilities of LLMs.",
            "9": "**Generalizability**: The query abstraction technique, which replaces entities and relations with unique IDs, ensures that the method is generalizable across different datasets.",
            "10": "This is a valuable feature for real-world applications where KGs can vary widely in structure and content.",
            "11": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of LARK on multiple standard benchmark datasets (FB15k, FB15k-237, NELL995) and various logical query types.",
            "12": "This comprehensive evaluation strengthens the validity of the results.",
            "13": "**Ethical Considerations**: The paper acknowledges the potential ethical issues related to bias in KGs and LLMs.",
            "14": "This is an important aspect that needs to be addressed in AI research.",
            "15": "#### Weaknesses\n\n1.",
            "16": "**Token Limitations**: The current implementation of LARK is limited by the token size of the underlying LLMs.",
            "17": "While the paper suggests that increasing token limits in future LLMs will resolve this issue, it remains a significant limitation for now.",
            "18": "**Complexity Handling**: Although the chain decomposition mechanism improves performance, the paper notes that LLMs are less effective at capturing depth over multiple relations.",
            "19": "This indicates that there is still room for improvement in handling highly complex queries.",
            "20": "**Dependency on LLM Quality**: The performance of LARK is heavily dependent on the quality and design of the underlying LLM.",
            "21": "This means that any limitations or biases in the LLM will directly affect the performance of LARK.",
            "22": "**Limited Analysis on Negation Queries**: While LARK shows significant improvement over BetaE for negation queries, the performance is still limited for highly complex negation queries (e.g., 3in and pni).",
            "23": "More analysis and potential solutions for these cases would strengthen the paper.",
            "24": "**Resource Intensive**: The implementation requires significant computational resources (eight Nvidia A100 GPUs).",
            "25": "This could limit the accessibility and practicality of the approach for some researchers and practitioners.",
            "26": "**Lack of Real-World Application Examples**: The paper could be strengthened by providing more concrete examples of how LARK can be applied to real-world problems, beyond the standard benchmark datasets.",
            "27": "**Detailed Methodology**: While the methodology is well-explained, some parts, such as the specific prompt templates and the exact configuration files, are relegated to the appendix.",
            "28": "Including more of these details in the main text could improve the clarity and reproducibility of the work.",
            "29": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of logical reasoning over knowledge graphs by leveraging the capabilities of large language models.",
            "30": "The novel approach, comprehensive evaluation, and substantial performance improvements make it a valuable contribution.",
            "31": "However, addressing the limitations related to token size, complexity handling, and resource requirements will be crucial for further enhancing the practicality and effectiveness of LARK."
        },
        "P06KaFZB2U": {
            "0": "The paper outlined an exciting application of LLMs to improve neural logic reasoning on knowledge graphs.",
            "1": "Figure 1 vividly illustrates the procedure of using query prompts to get single query results from LLM.",
            "2": "In most of the experiments, the performances indeed greatly outperform the SOTA level.",
            "3": "Authors assume that LLMs have the ability to reason and ascribe the improvement of their system's performances to this.",
            "4": "However, whether LLMs can reason is still an open question.",
            "5": "See below.",
            "6": "C. Biever, The easy intelligence tests that AI chatbots fails, Nature 619 (2023) 686–689\nM. Melanie, How do we know how smart AI systems are?, Science 381 (6654) (2023) adj5957\n\nIf we look carefully at the experiment results, it is not the case the proposed system consistently greatly outperforms SOTA level.",
            "7": "In the FB15K dataset, CQD system outperforms the authors' system in 4 out of 9 tasks.",
            "8": "In the NELL995 dataset (in ip task, CQD scores 70.0, while the authors' system scores 29.3), CQD system outperforms the authors' system in 2 out of 9 tasks.",
            "9": "In the case of the negation query, BetaE outperforms authors' system in 2 out of 5 tasks in the FB15K datasets."
        },
        "z65hRhU93p": {
            "0": "Impressive results on NELL dataset.",
            "1": "This approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs.",
            "2": "Integration of LLMs: The performance of the proposed approach improves proportionally to the increase in size of the underlying LLM, enabling the integration of the latest advancements in LLMs for logical reasoning over KGs.",
            "3": "The example query in Figure 1 equals to 3i.",
            "4": "It is not a simplest form.",
            "5": "Maybe another query structure is better.",
            "6": "RQ3.",
            "7": "Only two sizes of LLMs are not sufficient, though we do observe a significant performance improvement on 13B Llama2.",
            "8": "The experiment to answer RQ4 is not a good design.",
            "9": "GPT3.5 is different from Llama2-7B and Llama2-13B.",
            "10": "It’s hard to tell the importance of token limit.",
            "11": "Because GPT3.5 outperforms various open-source LLMs (including Llama2-7B and Llama2-13B) on many reasoning benchmarks.",
            "12": "A proper way to study how the token limit affects reasoning performance is to vary token limit on one LLM.",
            "13": "For example, we could set 100%, 75%, 50%, 25% on max token limit of Llama2.",
            "14": "Lack of comparison with other LLM-based approaches: The paper only compares the proposed approach with state-of-the-art KG reasoning methods and does not compare it with other LLM-based approaches for KG reasoning."
        },
        "BXD0d6MmEr": {
            "0": "The proposed method is simple yet effective.",
            "1": "The obtained improvement in performance is significant.",
            "2": "The presentations and figures are clear and easy to follow.",
            "3": "The limitations of the proposed method are preliminarily discussed.",
            "4": "The technical novelty is neutral, as the paper seems to use LLM for FOL queries directly.",
            "5": "The writing of the paper can be largely improved.",
            "6": "The running-time efficiency of LARK is not reported.",
            "7": "The paper is empirically driven and lacks in-depth analysis, whether from methodological or theoretical perspectives.",
            "8": "Besides, the paper does not provide satisfying insights or underlying properties of the LARK model.",
            "9": "As the paper uses entities and relations in queries to find pertinent subgraph contexts, it would be better to discuss some relevant subgraph sampling methods on KG, e.g., AStarNet (Zhaocheng Zhu et al., NeurIPS 2023) and AdaProp (Yongqi Zhang et al., KDD 2023)."
        },
        "HvebEZQzcg": {
            "0": "- The proposed approach based on LLMs only observers a subset of the full KG with anonymized entity and relation IDs.",
            "1": "This allows the system to:\n    - Easily generalize to new KGs\n    - Scale to larger KGs (since the LLM only observes a k-hop subgraph)\n    - Most existing approaches for KG reasoning need all or part (one-hop link prediction model) to be fine-tuned for each KG\n- Authors demonstrate that step-by-step query execution performs significantly better than directly attempting to execute the complex query\n- Authors show that other LLMs also seem to handle the task, demonstrating that the performance is not specific to Llama-2 - The presentation of the paper lacks details of the evaluation protocol\n    - The paper evaluates the system on the queries generated by [1].",
            "2": "[1] highlights 2 types of answer entities: trivial answers (query can be directly executed on the available KG to gather answers) and non-trivial answers (queries require reasoning about missing edges).",
            "3": "- It is unclear if this paper reports results on the trivial, non-trivial, or combined subsets of answers.",
            "4": "- While the paper reports strong results on all benchmarks, there is no indication of the mechanism by which an LLM \"could\" solve the task.",
            "5": "- The results are especially surprising given that the LLM only observes anonymized entity and relation names and cannot apply any semantic reasoning\n    - The LLM only sees a small subset of the KG, so it does not have sufficient information to learn the semantics of the anonymized relations \n- The paper misses some relevant work in query execution on KGs\n    - [2] is a strong baseline that uses learned one-hop link prediction models for each KG and uses them to execute complex queries.",
            "6": "The method provides guarantees that the trivial answers will be predicted exactly and shows strong performance on the non-trivial answer entities\n    - Other competitive baselines include [3] and [4].",
            "7": "[2] provides MRR results for these approaches on the same datasets\n    - The MRR for the CQD baseline in this paper do not match the corresponding results in [2]\n\nI have reframed these weaknesses as a series of questions in the next section.",
            "8": "---\n[1] Ren, H., Hu, W., and Leskovec, J. Query2box: reasoning over knowledge graphs in vector space using box embeddings.",
            "9": "In International Conference on Learning Representations, 2020.",
            "10": "[2] Yushi Bai, Xin Lv, Juanzi Li, and Lei Hou.",
            "11": "2023.",
            "12": "Answering complex logical queries on knowledge graphs via query computation tree optimization.",
            "13": "In Proceedings of the 40th International Conference on Machine Learning (ICML'23)\n\n[3] Chen, X., Hu, Z., and Sun, Y.",
            "14": "Fuzzy logic based logical query answering on knowledge graphs.",
            "15": "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp.",
            "16": "3939–3948, 2022.",
            "17": "[4] Zhu, Z., Galkin, M., Zhang, Z., and Tang, J. Neuralsymbolic models for logical queries on knowledge graphs.",
            "18": "In Proceedings of the 39th International Conference on Machine Learning, volume 162, pp.",
            "19": "27454–27478, 2022."
        }
    },
    "3KDbIWT26J": {
        "llm_review": {
            "0": "**Novel Investigation**: The paper addresses a significant and intriguing question about the translation capabilities of large language models (LLMs) trained on non-parallel data.",
            "1": "This is a novel area of research that has not been extensively explored, making the study both timely and relevant.",
            "2": "**Comprehensive Analysis**: The authors conduct a thorough investigation into various forms of bilingualism, including sentence alignment, word alignment, and code-switching.",
            "3": "This comprehensive approach allows for a deeper understanding of the factors contributing to LLMs' translation abilities.",
            "4": "**Methodological Rigor**: The paper employs both post-training and pre-training experiments to evaluate the impact of different types of bilingual data.",
            "5": "This dual approach adds robustness to the findings and helps in cross-verifying the results.",
            "6": "**Significant Findings**: The discovery that word alignment data can be as effective, or even more so, than sentence alignment data in imparting translation capabilities to LLMs is a significant contribution to the field.",
            "7": "This finding challenges existing assumptions and opens up new avenues for research.",
            "8": "**Practical Implications**: The insights gained from this study have practical implications for improving machine translation systems, especially in low-resource settings.",
            "9": "The potential for data augmentation and supervised fine-tuning techniques based on these findings is particularly noteworthy.",
            "10": "**Reproducibility**: The authors provide detailed information about their data collection and processing pipeline, as well as the implementation details and hyper-parameters used in their experiments.",
            "11": "This transparency enhances the reproducibility of the study.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Resource Constraints**: The pre-training experiments were limited by computational resources, resulting in models that were not fully converged.",
            "14": "This limitation might affect the generalizability of the findings, especially for larger-scale models.",
            "15": "**Limited Language Pairs**: While the study focuses on English and Chinese, it would have been beneficial to include a more diverse set of language pairs to validate the findings across different linguistic contexts.",
            "16": "The inclusion of more low-resource languages could provide additional insights.",
            "17": "**Evaluation Metrics**: The reliance on perplexity as a primary metric for evaluating translation capability in pre-trained models might not fully capture the nuances of translation quality.",
            "18": "While perplexity is a useful measure, it would be beneficial to complement it with other metrics that can provide a more holistic evaluation.",
            "19": "**Impact of Shared Tokens**: The study highlights the role of shared tokens (e.g., digits and symbols) in facilitating translation capabilities.",
            "20": "However, the experiments on digit substitution might not fully isolate the impact of shared tokens from other factors.",
            "21": "A more controlled experiment design could strengthen this finding.",
            "22": "**Parameter Sharing**: The investigation into the impact of parameter sharing between languages is insightful, but the study could benefit from a more detailed analysis of how different architectural choices (e.g., shared vs. separate layers) affect translation performance.",
            "23": "**Ethical Considerations**: While the paper includes an ethics statement, it would be beneficial to discuss potential ethical implications of using LLMs for translation, such as biases in training data and the impact on linguistic diversity.",
            "24": "#### Conclusion:\n\nOverall, the paper makes a valuable contribution to understanding the translation capabilities of LLMs trained on non-parallel data.",
            "25": "The strengths of the study lie in its novel investigation, comprehensive analysis, and significant findings.",
            "26": "However, there are areas for improvement, particularly in terms of resource constraints, evaluation metrics, and the diversity of language pairs.",
            "27": "Addressing these weaknesses in future research could further enhance the understanding and application of LLMs in machine translation."
        },
        "efI5Unca3A": {
            "0": "The paper innovatively delved into the emergent translation abilities of LLM by analyzing its training data composition.",
            "1": "The evaluation methods have its limitations.",
            "2": "There are flaws in the methodology, suggesting that the prompts might influence the results.",
            "3": "At its core, it's an ablation study at the data level."
        },
        "soiLX4dfXh": {
            "0": "The underlying question has been addressed before, suggesting that the presence of incidental sentence alignment in \"supposedly\" monolingual data plays a large role in LLM performance on MT.",
            "1": "This submission pushes the investigation by looking at word-level alignment and code-switched data which as far as I know is novel.",
            "2": "The positive impact of word alignment is demonstrated in a number of experiments showing improvements in either translation quality or perplexity.",
            "3": "The hypothesis that the larger amount of word alignment data (as compared to sentence-aligned) allows it to be as useful as sentence-aligned data is well supported by the experiments.",
            "4": "MT quality is evaluated using neural metrics rather than BLEU, which aligns with recent results from the WMT22 metrics task (https://aclanthology.org/2022.wmt-1.2/).",
            "5": "Some results are presented with significance tests of the differences, yay!",
            "6": "The UBD (sentence & word aligned + code switched) is extracted using an automated methods with arbitrary parameters that seem fairly ad hoc (eg 10 BLEU poins threshold, appdx A).",
            "7": "This raises the question of how good that data is... Are sentence-aligned segments even aligned sentences?",
            "8": "At 10 BLEU point, this is not very clear.",
            "9": "This data is at the core of the argument of the paper, better quality control would make a more convincing case.",
            "10": "Experiments use mostly smaller models, as well as surrogate methods, and some models are not even converged.",
            "11": "Clearly the amount of computation is significant.",
            "12": "However, would one make conclusion on a chemical process from a reaction that has not completed?",
            "13": "These imperfect or incomplete experimental conditions make the conclusions less convincing.",
            "14": "The switch to perplexity does not help -- the claims in Section 5.3 are only mildly convincing if it is not possible to extract minimally useful translations from the models.",
            "15": "Section 5.4 would benefit from a comparison with a model using UBD data somehow -- this would allow to gauge whether the performance in Table 8 is getting remotely close to acceptable translation quality."
        },
        "8VSwKLyzGm": {
            "0": "- Extensive experiments are conducted, to address the main question of \"Why can multilingual large language model can learn to translate without parallel data?\"",
            "1": "- The authors provide detailed empirical analyses to identify the underlying mechanism of translation capability in multilingual large language model.",
            "2": "They also discuss the cases where single monolingual data are available with different sizes.",
            "3": "This part might be helpful on how to improve low-resource language translation quality in the language model\n- The paper is well organized and clearly described in most parts.",
            "4": "- The authors focus on English and Chinese data to identify unintentional bilingualism types, and I was wondering if any other bilingualism types exist when you check the other languages.",
            "5": "- This paper looks interesting in light of the empirical analyses of bilingualism's role in translation capabilities; however, the question still remains on how to effectively enhance translation capability further against the supervised translation models."
        },
        "vkLY9MCt1u": {
            "0": "* The findings from the paper are interesting and may shed light on the amazing translation ability from LLM.",
            "1": "I enjoy reading the work a lot.",
            "2": "My biggest concern about the work is that while it is true that maybe the presence of sentence alignment, word alignment and code-switching data contribute an important role in explaining this amazing ability of multilingual LLMs.",
            "3": "We just don't know how important they are and is there any other reason (e.g.",
            "4": "the presence of other stuff) that is actually even way more important than the three.",
            "5": "I don't think the paper provide any data points on this.",
            "6": "Another smaller concern is that I see some paragraphs are just half-baked from curiosity perspective.",
            "7": "For instance, section 5.4 shows that after eliminating unintentional bilingual data, the translation ability is still there, apparently.",
            "8": "But there is no data points on why that happens, just some postulating about the reasons and that is it.",
            "9": "The final weakness point of the paper to me is the presentation in Section 5.5.",
            "10": "I could not follow exactly what \"the shared transformer layers in the BLOOM-560m model\".",
            "11": "To make the work self-contained I think the paper should at least present some high level details about the shared transfomer layers before presenting how this may influence the effect of unintentional bilingualism."
        }
    },
    "gLARhFLE0F": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces LUT-GEMM, a novel kernel for quantized matrix multiplication that eliminates the need for resource-intensive dequantization processes.",
            "1": "This is a significant advancement in the field of efficient inference for large-scale generative language models.",
            "2": "**Performance Improvement**: The experimental results demonstrate substantial improvements in token generation latency, achieving a 2.1x speedup compared to OPTQ.",
            "3": "This is a notable achievement, especially for large models like OPT-175B.",
            "4": "**Flexibility**: The proposed group-wise quantization offers a flexible trade-off between compression ratio and accuracy.",
            "5": "This flexibility is crucial for adapting the method to different models and requirements.",
            "6": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of LUT-GEMM, including kernel-level performance, end-to-end latency, and comparisons with existing methods.",
            "7": "This comprehensive evaluation strengthens the validity of the proposed method.",
            "8": "**Reproducibility**: The authors have made the code available on GitHub, which is a commendable step towards ensuring reproducibility and facilitating further research in this area.",
            "9": "**Detailed Methodology**: The paper provides a detailed explanation of the design methodology of LUT-GEMM, including the use of binary-coding quantization and the implementation of LUT-based operations on GPUs.",
            "10": "This detailed methodology helps in understanding the underlying principles and potential applications of the proposed method.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Limited Scope**: The paper primarily focuses on single-batch inference.",
            "13": "While this is a significant use case, the performance gains may diminish as the batch size increases.",
            "14": "This limitation should be addressed in future work to make the method more broadly applicable.",
            "15": "**Complexity of Implementation**: The implementation of LUT-GEMM involves several intricate steps, including the construction of LUTs and the use of binary-coding quantization.",
            "16": "This complexity might pose challenges for practitioners looking to adopt this method in real-world applications.",
            "17": "**Memory Bandwidth Constraints**: The paper mentions that the performance gains are limited by the memory bandwidth between the core and LUTs in the shared memory.",
            "18": "While this is a hardware constraint, it is an important consideration that might affect the scalability of the method.",
            "19": "**Quantization Accuracy**: The paper shows that reducing the group size can decrease perplexity, but this comes at the cost of increased latency.",
            "20": "Balancing accuracy and latency is a critical challenge, and more insights into this trade-off would be beneficial.",
            "21": "**Generalization to Other Models**: While the paper demonstrates the effectiveness of LUT-GEMM on the OPT-175B model, it would be valuable to see more experiments on a wider range of models to understand the generalizability of the method.",
            "22": "**Energy Consumption**: Although the paper discusses energy savings, a more detailed analysis of energy consumption across different configurations and models would provide a clearer picture of the method's efficiency.",
            "23": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of efficient inference for large-scale generative language models.",
            "24": "The introduction of LUT-GEMM and the elimination of the dequantization process are noteworthy contributions.",
            "25": "However, the method's complexity, limited scope, and potential scalability issues should be addressed in future work.",
            "26": "The comprehensive evaluation and availability of the code are commendable, making this paper a valuable resource for researchers and practitioners in the field."
        },
        "hWhffvaKQ8": {
            "0": "Clear ideas and descriptions of representing weights with scaling factor matrices and binary weight matrices.",
            "1": "The advantage of using LUT and grouped scaling factors is extensively evaluated.",
            "2": "The way of leveraging fine-grained scaling factors enabled a non-uniform quantization format.",
            "3": "Solid implementation.",
            "4": "The analysis and GPU implementation stand this idea out.",
            "5": "I understand the importance of having it work on GPUs but I still wonder how efficient is the implemented kernel as compared to the customized hardware accelerator.",
            "6": "Plausible ablation studies.",
            "7": "There are a lot of ablation studies to show the effectiveness of every proposed component.",
            "8": "One big issue in writing that prevents one from reimplementing the proposed method is how to construct the binary matrices and scaling factor matrices from the pre-trained weights.",
            "9": "The attached code assumes randomized matrices so that part cannot provide any useful information regarding this.",
            "10": "For more technical questions:\n\n1.",
            "11": "Regarding the LUT part, what is the repetition that we can exploit form?",
            "12": "The quantitative measurements or ablation studies on this will provide more insights into how many benefits we can get for such LUT-based GEMM instead of vanilla additions.",
            "13": "Any theoretical analysis on the upper bound of savings brought from LUT?",
            "14": "Regarding the scaling factor part, how to determine the group size?",
            "15": "In my understanding, if the group size is small, there are no benefits as you have as many matrices as used bits to represent weights.",
            "16": "At what group size, do the benefits compensate the cost of the scaling factor matrices?",
            "17": "Is there any formula to calculate this?",
            "18": "Regarding the experiments, why only consider the OPT model?",
            "19": "How about the speedups on other candidates like LLaMa at different scales?",
            "20": "In that way, we can also understand the scalability of the used kernel better."
        },
        "9ZSSNiVgpa": {
            "0": "The work of this paper is of great significance, especially for optimizing the inference performance of LLM, which is currently highly concerned.",
            "1": "Compared to the baseline cuBLAS implementation, the performance of this method has significantly improved.",
            "2": "The design, experiment, and analysis of this paper are relatively solid.",
            "3": "For the algorithm design of GPU, this paper provides detailed explanations and time complexity analysis.",
            "4": "This paper carefully considers the performance of different compression ratios under different bit and grouping parameters g. \n3.",
            "5": "CUDA kernel code and performance test code is provided.",
            "6": "The novelty of this paper is ordinary.",
            "7": "The main parts, including BCQ quantification and group-wise quantization optimization, both come from existing research.",
            "8": "This paper proposes to convert uniform quantization into BCQ format, which is not a complex transformation.",
            "9": "The author mentioned that the optimization here currently only focuses on the inference of a single batch, which may limit its use.",
            "10": "The LUT-Based method requires a significant amount of memory capacity in exchange for efficiency.",
            "11": "In Table 2, the 4-bit quantified LUT-GEMM storage footprint exceeds the 16 bit model of the cuBLAS baseline.",
            "12": "In fact, storage resources are also the main focus of quantization in large language models, not just performance.",
            "13": "This paper seems to focus mainly on computational efficiency, but lacks a comparison between memory resource usage.",
            "14": "The experimental data in this paper is not enough, and the main baseline for comparison is 16 bit cuBLAS, lacking a comprehensive comparison of other quantization methods.",
            "15": "The experiment only tested the quantization accuracy of PPL indicators using the OPT model on the LAMBADA dataset, which cannot fully demonstrate the effectiveness and universality of the method."
        },
        "Pk1RmtmQ32": {
            "0": "The core idea of the paper makes sense and in fact is more general and beyond the BCQ format used this paper.",
            "1": "The key question on rating the originality is the relation to prior works.",
            "2": "For example, how does this work relates to BiQGEMM (Jeon et al., 2020)?",
            "3": "Section 3.1 does acknowledge this and a few other prior works but it's unclear what are the distinctions.",
            "4": "If a satisfactory answer could be provided, then originality could be a big strength for this paper.",
            "5": "The experimental setup is good, particularly that perplexity test is included.",
            "6": "Including source code is a big plus, although I have not verified it.",
            "7": "As said above, the originality question is the key.",
            "8": "Please provide detailed narrative on the differentiation.",
            "9": "Including prior kernels in evaluations would be even stronger.",
            "10": "The last row of Table 2 suggests 4X speed up with 4-bit quantization at kernel level.",
            "11": "However this does not seems to translate to the 4-bit end2end latency in Tables 3 and 4, not anywhere close to 4X.",
            "12": "Can you explain why?",
            "13": "A critical property of the proposal is that the benefit gets larger as the matrix dimension increases.",
            "14": "Right now the results only show end2end latency for OPT 66B and 175B.",
            "15": "Could you add a plot of latency as a function of OPT model sizes, all the way from the smallest to 175B?"
        },
        "j3lGWF117I": {
            "0": "Extensive empirical evidence presented through various data visualizations confirms the efficiency of LUT-GEMM method.",
            "1": "The research goes further than most in addressing the real-world deployment challenges on standard hardware, which is a standout feature.",
            "2": "Impressive experiment section focusing on the largest open-source models and fitting them onto a single GPU.",
            "3": "Detailed latency insights provided in the paper lay a solid foundation for understanding the efficiency gains.",
            "4": "Well-written motivation and related work sections.",
            "5": "Discussions show a deep understanding of GPU programming.",
            "6": "The achievement of deploying huge models on a singular GPU is undermined by the lack of reported accuracy metrics at this scale.",
            "7": "The paper lacks a direct accuracy and latency comparison among different models, although the OPT model family is a good choice.",
            "8": "The paper does seem to have some limited technical novelty since it adapts and extends prior method in minor to moderate ways, but the analysis and implementation seem to make up for it."
        }
    },
    "BifeBRhikU": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method, Partially-Binarized LLM (PB-LLM), which addresses the challenge of extreme low-bit quantization for large language models (LLMs) while maintaining their linguistic reasoning capabilities.",
            "1": "This is a significant advancement in the field of model compression.",
            "2": "**Comprehensive Analysis**: The authors provide a thorough exploration of the ineffectiveness of existing binarization methods for LLMs and highlight the importance of salient weights.",
            "3": "This detailed analysis helps in understanding the limitations of current techniques and the necessity for a new approach.",
            "4": "**Methodological Rigor**: The paper presents a well-structured methodology for PB-LLM, including the identification and handling of salient weights, the use of post-training quantization (PTQ), and quantization-aware training (QAT).",
            "5": "The proposed methods are backed by solid theoretical foundations and practical implementations.",
            "6": "**Empirical Validation**: The authors validate their approach through extensive experiments on various LLMs, including OPT-1.3B and LLaMA-7B, and evaluate the performance on multiple zero-shot common sense reasoning tasks.",
            "7": "The results demonstrate the effectiveness of PB-LLM in achieving low-bit quantization without significant loss in performance.",
            "8": "**Efficiency**: The paper highlights the efficiency of PB-LLM in terms of training time and resource utilization.",
            "9": "The proposed method achieves comparable performance to full-precision models with significantly reduced memory and computational requirements.",
            "10": "**Open Source Code**: The availability of the code for PB-LLM enhances the reproducibility of the research and allows other researchers to build upon this work.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Complexity of Implementation**: While the paper provides a detailed methodology, the implementation of PB-LLM involves several complex steps, including the identification of salient weights, the use of Hessian matrices, and the application of optimal scaling factors.",
            "13": "This complexity might pose challenges for practitioners looking to adopt this approach.",
            "14": "**Limited Scope of Evaluation**: Although the paper evaluates PB-LLM on a variety of tasks and models, the experiments are primarily conducted on OPT-1.3B and LLaMA-7B.",
            "15": "It would be beneficial to see the performance of PB-LLM on a wider range of LLMs and tasks to further validate its generalizability.",
            "16": "**Salient Weight Selection Criteria**: The paper discusses the use of magnitude and Hessian-based criteria for selecting salient weights but ultimately chooses the magnitude criterion for simplicity.",
            "17": "While this choice is justified, a more in-depth comparison of the two criteria, including potential trade-offs, would strengthen the argument.",
            "18": "**Scalability Concerns**: The paper mentions the use of bitmap mechanisms and other storage optimization techniques for handling salient weights.",
            "19": "However, the scalability of these methods for extremely large models with billions of parameters is not thoroughly addressed.",
            "20": "**Impact on Downstream Applications**: The paper focuses on the performance of PB-LLM in terms of perplexity and accuracy on common sense reasoning tasks.",
            "21": "However, the impact of this quantization method on other downstream applications, such as machine translation or text generation, is not explored.",
            "22": "**Lack of Real-World Deployment Scenarios**: While the paper demonstrates the theoretical and empirical benefits of PB-LLM, it lacks discussion on real-world deployment scenarios.",
            "23": "Insights into how PB-LLM performs in practical settings, such as on edge devices or in production environments, would be valuable.",
            "24": "**Conclusion:**\n\nOverall, the paper presents a significant contribution to the field of LLM compression with the introduction of PB-LLM.",
            "25": "The strengths of the paper lie in its innovative approach, comprehensive analysis, and empirical validation.",
            "26": "However, the complexity of implementation, limited scope of evaluation, and scalability concerns are areas that could be further addressed.",
            "27": "Despite these weaknesses, PB-LLM represents a promising direction for achieving efficient and effective low-bit quantization of large language models."
        },
        "NqTeCSfDES": {
            "0": "+ Presents a practical approach that analytically identifies non-salient weights and applies binarization selectively for large models.",
            "1": "+ Achieves LLM binarization without sacrificing performance gains.",
            "2": "- The proposed approach section is comprehensive, but its complexity makes it challenging to navigate and comprehend throughout the entire section.",
            "3": "- The analysis of evaluation is limited to a single task.",
            "4": "It would be valuable to explore the potential limitations of PB-LLM in achieving comparable performance across various tasks."
        },
        "djmyVVuIaL": {
            "0": "- The paper is well-written and well-motivated.",
            "1": "- The proposed PB-LLM scheme is easy to follow and straightforward to understand.",
            "2": "- Exploring to improve the memory and/or storage efficiency using quantization (and algorithmic approximation in general) of LLMs is a promising research direction.",
            "3": "- Only Llama 7B is studied as the LLM for PB-LLM and all other baselines.",
            "4": "Thus, it's not clear how the PB-LLM method performs on larger-scale models.",
            "5": "- Only pre-trained base models are experimented with those models, however, are usually not deployed directly as applications.",
            "6": "- The LLM quantization scheme is motivated using an angle of GPU memory efficiency.",
            "7": "However, the actual GPU memory usage before and after binarization/quantization is not studied in this paper."
        },
        "0oBMzfPunQ": {
            "0": "- The paper provides a well-structured presentation of the preliminaries of binary quantization to the introduction of the proposed method, which made it easy to follow.",
            "1": "- Building on previous research such as AWQ and SparseGPT, this paper proposes a partial binarization technique through salient weight protection based on the hessian information and effectively demonstrates its efficacy in PTQ with ablation study (as shown in Table 1).",
            "2": "- The paper showcases empirical improvements in QAT optimization (higher accuracy with fewer training steps than LLM-QAT) across the CSQA tasks.",
            "3": "Major Concerns\n- Lack of novelty: The authors propose the optimal scaling factor as their primary contribution, but the core idea itself seems to have already been proposed in the previous work.",
            "4": "For example, [R1] proposed an optimal ternary function (eq.",
            "5": "(3) of [R1]), but it can be trivially reduced to the binary function when the threshold is zero; then the equation seems to be identical to the equation (8) and (9) proposed in this paper.",
            "6": "[R1] Li te tal., Ternary Weight Networks\n\n\n- Lack of Evaluation Task: This paper evaluates the reasoning capability of LLM only through the accuracy of the CSQA task.",
            "7": "In the CSQA task, tasks such as OBQA and ARC challenge were used for OPT-1.3B, where the FP performance did not reach even the random baseline (25%).",
            "8": "It raises questions about the suitability of these tasks for demonstrating the effectiveness of fine-tuning, and, hence, the superiority of PB-LLM.",
            "9": "To show the effects of fine-tuning more clearly, it would be advisable to carefully select reasoning tasks that are appropriate for the model capacity.",
            "10": "Reporting performance not just on CSQA, but also on multi-task accuracy like MMLU would be also beneficial for highlighting PB-LLM's efficacy.",
            "11": "- Inconsistent Salient Weight Methodology between PTQ and QAT: The absence of a consistent methodology for salient weight protection between PTQ and QAT is concerning.",
            "12": "While the effectiveness of using Hessian criteria for identifying salient weights in PTQ is demonstrated through performance comparisons, the rationale for using magnitude criteria to identify salient weights in QAT seems to be missing.",
            "13": "Understanding the disparity in the approach to salient weight protection across PTQ and QAT is crucial for a holistic appreciation of the proposed method.",
            "14": "- Insufficient evidence on PB-LLM efficiency: To claim that PB-LLM is more efficient in terms of training iteration number compared to LLM-QAT, a more thorough comparison seems necessary.",
            "15": "Specifically, it needs to be clear whether the LLM-QAT, being compared with PB-LLM, has been fine-tuned on the same dataset as PB-LLM.",
            "16": "Detailed experimental setup information regarding the LLM-QAT is required.",
            "17": "Moreover, verification is needed on whether the results through PB-LLM QAT have fully enhanced the reasoning capacity of the pre-trained model.",
            "18": "Essentially, it appears that the reasoning accuracy of the target model (OPT-1.3B) obtained through FP fine-tuning should be presented as the upper bound in Figure 7.",
            "19": "Additionally, there seems to be a lack of information in Table 2 regarding whether FP LLaMA-7B performance is pre-trained or fine-tuned.",
            "20": "Minor Concerns\n- Typo: Sec 3.3 bianrize -> binarize\n- Consistent notation should be used in Sec 4.1 -> LLaMA, LLaMa -> LLaMA\n- There may be an incorrect reference link in Sec 4.1, \"showing its fast convergence property (refer to 3.2)\" Should it possibly be corrected to \"refer to 3.4?\"",
            "21": "- There are spacing issues in the Figure 7 caption, \"LLMin\" should be \"LLM in\" and \"PM-LLMtriumphs\" should be \"PB-LLM triumphs\"."
        },
        "T5IFyC96el": {
            "0": "Compressing LLMs is an important question for today’s AI research, and the authors first introduce binarization into LLM compression pushing the quantized LLM into ultra-low bits.",
            "1": "The authors present a thorough exploration of network binarization techniques.",
            "2": "They effectively demonstrate the feasibility and potential of partially-binarized LLMs using post-training quantization and quantization-aware training methodologies.",
            "3": "The inclusion of source code with the submission is commendable, enabling reproducibility and verification of the reported results, which are impressive.",
            "4": "While the incorporation of Quantization-Aware Training (QAT) in LLM compression is an interesting proposal, its practicality is uncertain given the substantial costs associated with training LLMs.",
            "5": "Could the authors elaborate on the overhead implications of QAT for LLMs?",
            "6": "In regards to Table 2, it is unclear whether GPTQ-PB represents the method proposed by the authors.",
            "7": "Could you clarify the distinction between GPTQ-PB and PB-LLM within the context of your study?",
            "8": "The application of optimal scaling techniques appears to be confined to the specific case presented.",
            "9": "Could these techniques be generalized to other bit configurations, and if so, how might this affect the compression performance?"
        }
    },
    "EyDPfGy4Wh": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces Expert Projection Attention (EPA), a novel method that leverages Mixture-of-Experts (MoE) layers to reduce the number of attention matrices in Transformers.",
            "1": "This is a significant contribution as it addresses the computational and memory inefficiencies of traditional self-attention mechanisms.",
            "2": "**Performance and Efficiency**: The proposed method achieves comparable language modeling performance to baseline Transformers while significantly reducing computational and memory requirements.",
            "3": "This is demonstrated across various datasets and model sizes, showcasing the robustness and scalability of the approach.",
            "4": "**Comprehensive Experiments**: The authors conduct extensive experiments on multiple datasets, including Wikitext 103, C4, Enwik8, and peS2o, and compare their method against strong baselines.",
            "5": "This thorough evaluation provides strong evidence of the effectiveness of EPA.",
            "6": "**Combination with MoE MLP Layers**: The paper also explores the combination of EPA with MoE MLP layers, resulting in a \"Fast Transformer\" that further enhances efficiency.",
            "7": "This demonstrates the flexibility and potential of the proposed method to be integrated with other efficient architectures.",
            "8": "**Detailed Analysis**: The paper includes a detailed analysis of the resource usage of different methods, providing a clear understanding of the computational and memory savings achieved by EPA.",
            "9": "The analysis of attention maps and expert selections adds interpretability to the model's behavior.",
            "10": "**Practical Implications**: By reducing the resource requirements, EPA makes it feasible for researchers and institutions with limited computational resources to train and deploy large language models, democratizing access to advanced NLP technologies.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Complexity of Implementation**: While the paper claims that EPA does not require additional regularization or tricks for stable training, the implementation of MoE layers and the selection mechanism might still be complex for practitioners.",
            "13": "The paper could benefit from providing more implementation details or guidelines.",
            "14": "**Limited Comparison with Other Efficient Attention Mechanisms**: The paper primarily compares EPA with MoA and standard Transformers.",
            "15": "Including comparisons with other recent efficient attention mechanisms, such as Linformer or Performer, would provide a more comprehensive evaluation of EPA's relative performance.",
            "16": "**Scalability to Larger Models**: While the paper demonstrates the effectiveness of EPA on models up to 262M parameters, it would be valuable to see how the method scales to even larger models, such as those with billions of parameters, which are common in state-of-the-art language models.",
            "17": "**Generalization to Other Tasks**: The experiments focus on language modeling tasks.",
            "18": "It would strengthen the paper to include results on other NLP tasks, such as machine translation or text classification, to demonstrate the generalizability of EPA.",
            "19": "**Interpretability of Expert Selections**: The paper mentions that expert selections are often interpretable, but the provided examples are limited.",
            "20": "A more in-depth analysis of the interpretability of expert selections across different layers and tasks would be beneficial.",
            "21": "**Potential Overhead of Expert Selection**: The paper does not thoroughly discuss the potential overhead introduced by the expert selection mechanism.",
            "22": "While the overall computational savings are highlighted, understanding the trade-offs involved in the selection process would provide a more complete picture.",
            "23": "#### Conclusion\n\nOverall, \"Fewheads Are Enough\" presents a significant advancement in making Transformers more efficient through the novel use of Expert Projection Attention.",
            "24": "The method shows promise in reducing computational and memory requirements while maintaining performance, making it a valuable contribution to the field of NLP.",
            "25": "Addressing the mentioned weaknesses, particularly in terms of broader comparisons and implementation details, would further strengthen the paper and its impact."
        },
        "Y8SbFOYFze": {
            "0": "the method is clearly illustrated and the analysis in 2.3 is helpful for understanding the difference.",
            "1": "The author seems to misunderstand the position of flash-attention, see questions below.",
            "2": "2. the scale of experiment is small, how would this method generalize to larger models such as llama?",
            "3": "Some experiments have not been finished (Table 4)."
        },
        "ZdjmMUtzMD": {
            "0": "The idea at a high level looks decent.",
            "1": "However, the poor writing and underwhelming evaluation really makes it hard to appreciate it.",
            "2": "Please see the summary."
        },
        "HvwP4taCax": {
            "0": "The paper is well-written and effectively highlights the issues with the current attention architecture in terms of computational and memory demands.",
            "1": "The paper conducts experiments on various datasets and compares its results with existing baseline methods, including MOA.",
            "2": "The paper conducts a thorough analysis of attention maps to facilitate a qualitative study and comparisons with conventional attention matrices.",
            "3": "The paper refers to FlashAttention multiple times and compares against their CUDA kernel (SW designed to exploit HW efficiently) optimization vs algorithmic insight in this paper.",
            "4": "I am not sure if its an apple-to-apple comparison since there are tons of other literature for transformers which aim to reduce computation/memory cost (like quantization/sparsity methods) and the paper doesn’t compare against these.",
            "5": "While authors compare against FlashAttention custom kernel implementation and mention that as a drawback, EPA algorithm itself requires a custom CUDA kernel with its own set of restrictions (pointed in the results section).",
            "6": "For the EPA algorithm, the paper mentions that K/Q source experts are not necessary for good results and only output/value experts are required, which seems to contradict the disadvantages shown in 2.2 naive algorithm."
        }
    },
    "27YiINkhw3": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces TOOLDEC, a novel finite-state machine (FSM)-guided decoding algorithm that ensures syntactically correct tool calls for large language models (LLMs).",
            "1": "This approach is innovative and addresses a significant problem in the field of tool-augmented LLMs.",
            "2": "**Error Elimination**: TOOLDEC effectively eliminates tool-related syntax errors, which is a major advancement over existing methods.",
            "3": "The paper provides strong empirical evidence showing that TOOLDEC reduces syntactic errors to zero, which is a substantial improvement.",
            "4": "**Generalization**: The proposed method demonstrates superior generalization to unseen tools without the need for additional training data or in-context documentation.",
            "5": "This is a significant advantage, as it allows LLMs to adapt to new tools more efficiently.",
            "6": "**Comprehensive Evaluation**: The paper evaluates TOOLDEC on multiple benchmarks, including FuncQA, KAMEL, and RestBench, covering diverse domains such as math functions, knowledge graph relations, and real-world RESTful APIs.",
            "7": "The results consistently show that TOOLDEC outperforms existing methods in terms of accuracy and inference time.",
            "8": "**Detailed Methodology**: The paper provides a detailed explanation of the FSM construction and the decoding process, making it easier for readers to understand and potentially replicate the approach.",
            "9": "**Practical Implications**: By ensuring syntactically correct tool calls and enabling efficient generalization to new tools, TOOLDEC has practical implications for improving the robustness and versatility of LLMs in real-world applications.",
            "10": "#### Weaknesses:\n\n1.",
            "11": "**Complexity of Implementation**: While the paper provides a detailed methodology, the implementation of FSMs for various tools and the integration with LLMs might be complex and require significant effort.",
            "12": "This could be a barrier for practitioners looking to adopt TOOLDEC.",
            "13": "**Limited Discussion on Limitations**: The paper does not thoroughly discuss the potential limitations of TOOLDEC.",
            "14": "For instance, the reliance on meaningful tool names and the assumption that LLMs can predict plausible tool names might not always hold true in practice.",
            "15": "**Scalability Concerns**: Although the paper demonstrates the effectiveness of TOOLDEC on a variety of benchmarks, it does not address potential scalability issues when dealing with an extremely large number of tools or very complex tool signatures.",
            "16": "**Evaluation Metrics**: The paper primarily focuses on accuracy and inference time as evaluation metrics.",
            "17": "While these are important, additional metrics such as robustness to adversarial inputs or the impact on model interpretability could provide a more comprehensive evaluation of TOOLDEC.",
            "18": "**Comparative Baselines**: The paper compares TOOLDEC with a few recent baselines, but it would be beneficial to include a broader range of baselines, including more traditional methods and other state-of-the-art approaches, to provide a more thorough comparison.",
            "19": "**Real-World Application Examples**: While the paper includes benchmarks from real-world scenarios, it would be helpful to provide more detailed examples of how TOOLDEC can be applied in practical applications, including potential challenges and solutions.",
            "20": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of tool-augmented LLMs with the introduction of TOOLDEC.",
            "21": "The strengths of the paper lie in its innovative approach, comprehensive evaluation, and practical implications.",
            "22": "However, the complexity of implementation, limited discussion on limitations, and scalability concerns are areas that could be addressed in future work.",
            "23": "Despite these weaknesses, TOOLDEC represents a promising direction for improving the robustness and versatility of LLMs in using external tools."
        },
        "fUF6TEOv3u": {
            "0": "This paper shows that LLMs still require an external knowledge to constrain the search space for tool use and existing methods such as finetuning and in-context learning are not enough.",
            "1": "It shows that the type of errors (syntax errors) can be addressed by an adoption of a simple FSM.",
            "2": "It is shown that it is true for the settings the method was tested for.",
            "3": "Novelty\n\nIt is essentially about constraining the search space of a language model by a grammar.",
            "4": "It is definitely expected that the use of a grammar can reduce syntax errors if we know that the output needs to follow the grammar.",
            "5": "I feel that it is a known technique but not a novel finding although probably it has not been applied for LLMs yet.",
            "6": "It does not necessarily need to be theoretical, but I would probably at least want to see deeper discussions on why LLM has limitations without such external knowledge.",
            "7": "Does a much stronger LLM have the same problem?",
            "8": "Complexity\n\nA good thing about LLMs is that the input and output are both plain text and the mechanism is very simple.",
            "9": "This technique is against the simplicity.",
            "10": "It says the FSM can be automatically constructed, but I am not sure if it is always the case for more complex tools.",
            "11": "Defining FSM manually could be tedious and error-prone for complex ones.",
            "12": "Decoding with an external FSM will add additional complexity to the system although this could be standardized by for example open tools."
        },
        "WXy1b0neiK": {
            "0": "* Experimental validation that enforcing prefix-checkable constraints on generation can result in more effective tool use for a relatively large set of tools.",
            "1": "* The decoding approach is validated for several different LLM (ToolkenLLM, RestGPT, and ToolLLM), and appears to improve the in-context learning ability of the LLM (S4.3).",
            "2": "* Approach maintains levels of accuracy even with increasing numbers of unseen tools in the test set (Figure 5).",
            "3": "* Unclear why general machinery of FSM is necessary when the approach amounts to constrained decoding using prefix-checkable constraints on next token generation.",
            "4": "The paper states “Note that in practice, it’s not necessary to explicitly construct this FSM.",
            "5": "Any grammar checker that tells the set of valid next tokens suffice.” Perhaps there could be better motivation for using FSM?",
            "6": "* In some ways, the approach seems like a step backwards to expert-based AI, in that the improvements from the proposed approach appear largely to be the result of hand-crafting decoding constraints.",
            "7": "* Related to the above concern, it’s unclear how the proposed approach was validated.",
            "8": "Was the hand-crafted decoding approach tailored to perform on test data?"
        },
        "x2jWzW8eLW": {
            "0": "- The FSM guided decoding method is intuitive and suitable for solving the syntax errors.",
            "1": "- The proposed method is compatible with existing LLM tool-use schemes, i.e., both finetuning or in-context learning.",
            "2": "- The method has shown to be empirically effective in eliminating syntax error, and leads to performance improvements.",
            "3": "- The FSM construction may require careful curation.",
            "4": "For example, how does one decide what's the best naming for a tool?",
            "5": "Are LLMs robust to the name changes?",
            "6": "Also, what would the process be like for one to construct the FSMs for a large collection of tools?",
            "7": "Would it be done through parsing the tool documentations?",
            "8": "It'd be helpful if the authors provide more discussion here.",
            "9": "- It is not clear to me as to how ToolDec can enable generalization to new tools?",
            "10": "While adding new FSM (for the new tool) can ensure the LLM uses the new tool in a syntactically correct way, the FSM itself does not provide sufficient information on when the tool should be invoked.",
            "11": "Current generalization then seems to only depend on LLM's language prior, and thus related to above, it's tool use performance can largely depend on the proper naming of the tools.",
            "12": "- Following from above, it'd be interesting to see an experiment testing the robustness of ToolDec by assigning tool names that not are not semantically meaningful."
        },
        "vGMdDo1lcz": {
            "0": "Strengths:\nThis paper proposes the finite-state machine-guided decoding algorithm, which reduces the errors during calling tools.",
            "1": "It is a clear and simple method to restrict the decoding space.",
            "2": "The experimental results show that the method is effective in the tool learning task, which significantly reduces name errors.",
            "3": "Weaknesses:\nEven though the model achieves significant improvements, it is unclear the language and tool mechanism switching.",
            "4": "I think the switching effectiveness should be evaluated and whether the <T> token can be appropriately decoded.",
            "5": "The augment errors are zero.",
            "6": "However, this reason may lie in that the existing tool learning benchmark is a little easy.",
            "7": "If the input is a more complex problem and contains several numbers, the argument can also be wrong.",
            "8": "The zero error rate should be carefully claimed."
        }
    },
    "MNShbDSxKH": {
        "llm_review": {
            "0": "**Innovative Approach**:\n   - The paper introduces a novel framework, GENOME, which leverages large language models (LLMs) to generate and reuse modules for visual reasoning tasks.",
            "1": "This approach is inspired by human learning processes, where knowledge is accumulated and reused over time.",
            "2": "**Modular Design**:\n   - The model's design is divided into three stages: module initialization, module generation, and module execution.",
            "3": "This structured approach ensures that the model can efficiently handle new tasks by reusing existing modules and generating new ones when necessary.",
            "4": "**Performance**:\n   - The model demonstrates competitive performance on standard visual reasoning tasks such as visual question answering (GQA) and referring expression comprehension (RefCOCO).",
            "5": "The results indicate that GENOME can effectively handle these tasks while maintaining transparency and efficiency.",
            "6": "**Transfer Learning**:\n   - GENOME shows strong transfer learning capabilities.",
            "7": "Modules learned from one task can be seamlessly transferred to new tasks, such as image editing and knowledge tagging.",
            "8": "This adaptability is a significant advantage over traditional models that require extensive retraining for new tasks.",
            "9": "**Few-Shot Learning**:\n   - The model can adapt to new visual reasoning tasks by observing only a few training examples.",
            "10": "This few-shot learning capability is crucial for practical applications where large annotated datasets may not be available.",
            "11": "**Transparency and Interpretability**:\n   - By using neuro-symbolic methods, GENOME provides better model transparency and interpretability compared to pure neural network models.",
            "12": "The explicit Python scripts generated by the model make it easier to understand and debug the reasoning process.",
            "13": "**Comprehensive Evaluation**:\n   - The paper provides a thorough evaluation of the model's performance across various tasks and datasets.",
            "14": "The inclusion of qualitative examples and detailed comparisons with baseline models adds credibility to the results.",
            "15": "#### Weaknesses:\n\n1.",
            "16": "**Dependency on LLMs**:\n   - The model heavily relies on the capabilities of large language models.",
            "17": "The performance of GENOME is intrinsically tied to the quality and capacity of the LLM used.",
            "18": "This dependency might limit the model's applicability in scenarios where access to powerful LLMs is restricted.",
            "19": "**Prompt Engineering**:\n   - The framework requires task-specific prompts for each distinct reasoning task.",
            "20": "This need for manual prompt engineering can be labor-intensive and may limit the model's scalability.",
            "21": "Exploring the use of a universal prompt for all tasks could be a potential area for improvement.",
            "22": "**Limited Evaluation on Diverse Modalities**:\n   - While the model shows promising results on visual reasoning tasks, its applicability to other multi-modal reasoning tasks (e.g., audio, video, tactile information) is not explored.",
            "23": "Extending the framework to encompass a broader range of multi-modal inputs would enhance its versatility.",
            "24": "**Scalability Concerns**:\n   - The process of generating and testing new modules can be computationally expensive, especially when dealing with large datasets or complex tasks.",
            "25": "The paper does not provide a detailed analysis of the computational resources required for training and inference.",
            "26": "**Error Handling and Debugging**:\n   - Although the model includes a mechanism for debugging and refining generated code snippets, the paper does not discuss the efficiency and effectiveness of this process in detail.",
            "27": "Understanding how the model handles errors and iteratively improves its performance would be valuable.",
            "28": "**Generalization to Unseen Tasks**:\n   - While the model shows good performance on the tasks it was evaluated on, its ability to generalize to completely unseen tasks without any task-specific training examples remains uncertain.",
            "29": "Further experiments on a wider variety of tasks would strengthen the claims of generalizability.",
            "30": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of neuro-symbolic visual reasoning by introducing a generative approach that grows and reuses modules.",
            "31": "The strengths of the model, including its innovative design, strong performance, transfer learning capabilities, and transparency, make it a valuable contribution.",
            "32": "However, the dependency on LLMs, need for prompt engineering, and scalability concerns are areas that require further attention.",
            "33": "Future work could focus on addressing these limitations and extending the framework to a broader range of multi-modal reasoning tasks."
        },
        "lSWl1eYZmF": {
            "0": "I am appreciative of the idea of generating more modular and composable modules that are verified, to be used in a library of skills for future tasks.",
            "1": "I think the idea of continuously growing this library is innovative, and the method simple and elegant.",
            "2": "W1.",
            "3": "Is there anything to prevent overfitting to the small train set of a given task, and only creating specific modules tailored for that domain?",
            "4": "I can imagine that these overfitted modules may not be very useful in new tasks.",
            "5": "W2.",
            "6": "Isn’t it possible that the method creates a broad function signature, but during stage 2 verification with the small set of train examples, it overfits to a bad implementation that only works for those examples, and therefore actually harms performance from there on out?",
            "7": "I’m mainly concerned about the above two overfitting challenges.",
            "8": "W3.",
            "9": "There seems to be an assumption that the queries in the train set actually all require similar modules, for example, to verify the new modules, the selected set of test cases from the train set must actually use those modules.",
            "10": "I’m not sure if this is a reasonable assumption, and also, how is this sampling of train examples (both in stage 1 and stage 2) done?",
            "11": "W4.",
            "12": "In the experiments, are Visprog/Viper also given the same few-shot training set?",
            "13": "For example, I believe you can use the same method of error correction and correct both Visprog/Viper when it is incorrect.",
            "14": "In this way, you can include Visprog/Viper comparison in Tables 3 and 4.",
            "15": "Would be great to better disentangle what drives this improvement of performance -- modularity in the proposed modules, or the training examples given in the loop, etc."
        },
        "4iIhr5iJO1": {
            "0": "In general, I think the direction of the proposed method is sound and compelling; systems like VisProg and ViperGPT provide promising ways to push neuro-symbolic reasoning past toy-domains, but they are limited by fixed APIs.",
            "1": "Using LLMs to augment these APIs in a task-specific manner is an interesting idea with many potential avenues for future exploration.",
            "2": "From a methodological stand-point, I think the division of labor between the module initialization step and the module generation step is a neat insight.",
            "3": "This design decision allows the LLM to create it's own tests, that can then be used to help guarantee that LLM generated functions 'do what they are supposed to'.",
            "4": "The experimental results are largely positive for the proposed method, although I have some concerns about their design that I'll detail below.",
            "5": "That said, I think the experimental results do support the claim that the proposed system offers improvements over both VisProg and ViperGPT across tasks.",
            "6": "Perhaps the most compelling claim is that the modules that GNSVR finds from the GQA and RefCOCO tasks can \"transfer\" for related tasks of image editing and knowledge tagging, supported by the fact that GNSVR significantly outperforms VisProg in this setting.",
            "7": "The evaluations against fully-supervised methods on the Raven and MEWL tasks is also fairly impressive, considering GNSVR is operating in a few-shot learning paradigm.",
            "8": "# Main Concern\n\nFrom my perspective, the biggest current weakness of the paper is that from the experimental design its hard to parse out exactly how the discovered modules affect the system's performance.",
            "9": "Ostensibly, this can be gleaned from comparisons between GNSVR and VisProg/ViperGPT, but there are more differences between these systems beyond merging in discovered modules.",
            "10": "Specifically, GNSVR uses a \"base API\" that is a combination of VisProg and ViperGPT, so the \"fair\" comparison would be against an ablated version of GNSVR that removes steps 1 and 2, and just tries to solve test-problems with the original API functions.",
            "11": "This condition is considered in the ablation experiment (GNSVR w/o ML), but only a subset of the RefCOCO test-set.",
            "12": "To solidify the claim that the improvement GNSVR observes stems from its discovered modules, this base condition should be added to all of the experimental set-ups (tables 1-5), for example, from Table 2 its unclear how much of the delta improvement between VisProg and GNSVR can be attributed to improvements in the base API versus improvements to the API from the new modules.",
            "13": "Beyond this, I'm also slightly concerned about the design of the GNSVR w/o ML baseline.",
            "14": "At inference time, is this baseline allowed to invoke arbitrary python logic in the style of ViperGPT (e.g.",
            "15": "standard control flow constructs) or is it restricted to *only* using API function calls in the style of VisProg.",
            "16": "I would imagine that the first condition would be more fair to evaluate GNSVR.",
            "17": "Solving some tasks might require simple logic that the LLM knows how to express in python, but might not be directly expressible with a series of API calls.",
            "18": "In GNSVR, this logic is incorporated into modules, but in the baseline the LLM should still have the opportunity to invoke similar logic in its test-time solutions (otherwise its impossible to properly evaluate the usefulness of the discovered modules).",
            "19": "Please clarify which of these modes the baseline is operating in.",
            "20": "# Minor\n\nCompared to VisProg and ViperGPT this system seems to require more training data, as the I/O pairs are not only used to populate in-context examples, but also impact (i) what module concepts are proposed and (ii) how the correctness of each module concept is evaluated.",
            "21": "This point about reliance on training data is touched on in the ablation section, but it would be good to make this distinction explicit when comparing the pros/cons of the proposed system against past work."
        },
        "vlzGzJslie": {
            "0": "Firstly, the paper is very well written and the GNSVR method is very well explained, with a healthy split between using the main paper and supplementary for splitting the key information versus additional information.",
            "1": "The empirical performance of GNSVR for transfer tasks, as well as the examples of the modules generated is quite impressive - it would justify the overall approach to growing modules for reasoning and their effectiveness for few-shot  generalization.",
            "2": "There are several components to the GNSVR framework but the paper provides no detailed analysis in the main paper of the importance of each of the components of the GNSVR framework.",
            "3": "While I think the framework overall is useful, the components are not all equally important to solve the reasoning problem and hence it is important to understand for future research on modular reasoning to understand what works and what doesn't, and if so why not.",
            "4": "How big of a role does \"good initlialization\" of the neural module operators plays?How important is defining the correct input and output format for a new module?",
            "5": "How important is the selection of the few shot samples to evaluate a new module?",
            "6": "(the authors say \"We extracted 300 examples from GQA, 100 from RefCOCO, 10 from Raven, and 10 from MEWL based on experimental experience.\"",
            "7": "- is the experience here just cherry picking for results or something else?)",
            "8": "How important is the LLM capability to learn new modules?",
            "9": "What role does the prompt play for the LLM in evaluating existing modules and creating new ones?",
            "10": "Without detailed analysis to support answers to all these questions, the paper is limited in terms of explaining the method beyong just presenting a new method and showing empirical results.",
            "11": "Lastly, I would not suggest the authors not report results on the RAVEN dataset.",
            "12": "As shown independently in [1] and [2] the dataset contains flaws in choice design which enables models to learn shortcuts to solve the RPM reasoning task.",
            "13": "I would recommend the authors use the i-RAVEN dataset inroduced in [1] instead.",
            "14": "**References** \n\n1.",
            "15": "Hu, S., Ma, Y., Liu, X., Wei, Y. and Bai, S., 2021, May.",
            "16": "Stratified rule-aware network for abstract visual reasoning.",
            "17": "In Proceedings of the AAAI Conference on Artificial Intelligence (Vol.",
            "18": "35, No.",
            "19": "2, pp.",
            "20": "1567-1574).",
            "21": "Spratley, S., Ehinger, K. and Miller, T., 2020.",
            "22": "A closer look at generalisation in raven.",
            "23": "In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVII 16 (pp.",
            "24": "601-616).",
            "25": "Springer International Publishing."
        },
        "3Js79px0vh": {
            "0": "This paper makes a clear case for it's own contribution, and that contribution does appear to be valuable - visual learning is an important task, and the prohibitive cost of using SOTA large language models makes reuse of code appealing (although they explicitly don't use ChatGPT4 \"due to the prohibitive cost\", so maybe this is less of an argument than it would be otherwise).",
            "1": "The fact that they can show it's use on several domains and types of tasks is also appealing.",
            "2": "My main concern is that, based on the presentation, it seems that the authors took a lot of highly intricate API's for LLM's that large teams may have worked on and cobbled them together to solve a new task.",
            "3": "I refer to this section: \"The success of our GNSVR relies on a set of pre-defined modules and APIs as the starting point.",
            "4": "We utilize handcrafted modules from VisProg (Gupta & Kembhavi, 2022) as our initial components.",
            "5": "Additionally, we incorporate several new APIs from ViperGPT to enhance module creation.",
            "6": "We also include some new APIs from ViperGPT (Sur´ıs et al., 2023) for making new modules.\"",
            "7": "I appreciate their novelty in how they use these API's, but the ratio of insights of these authors vs of the authors of the API's appears insignificant.",
            "8": "I'm also not entirely convinced of the novelty of this paper.",
            "9": "I refer to \"Iterative Disambiguation: Towards LLM-Supported Programming and System Design\" (Pereira and Hartmann) and \"Self-planning Code Generation with Large Language Models\" (Jiang et al).",
            "10": "I don't think the fact that this is in the visual domain is enough to call it \"novel\", because there is virtually no engagement with visual modalities by the authors - as stated above, according to my understanding, they are using predefined modules which handle the interface between vision and language.",
            "11": "If I was given evidence against either of the two above claims - that all of the work (particularly the \"visual reasoning\" work) is being done by existing tools, or that the paper is not fundamentally using LLM's in a novel way - I would be happy to increase my score."
        }
    },
    "LIW88mwqgv": {
        "llm_review": {
            "0": "**Innovative Approach**: The introduction of Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts prompting) is a novel approach that addresses the common issues in CoT prompting, such as error-prone reasoning chains and inappropriate exemplars.",
            "1": "The iterative bootstrapping mechanism allows LLMs to autonomously rectify errors, which is a significant advancement.",
            "2": "**Comprehensive Evaluation**: The paper evaluates Iter-CoT on a wide range of datasets (ten datasets across three distinct reasoning tasks), demonstrating its effectiveness and generalizability.",
            "3": "This extensive evaluation provides strong evidence of the method's robustness and applicability.",
            "4": "**Detailed Analysis**: The paper provides a thorough analysis of the impact of different components of Iter-CoT, such as the bootstrapping and summarization phases.",
            "5": "This detailed breakdown helps in understanding the contribution of each component to the overall performance.",
            "6": "**Comparison with Baselines**: The paper compares Iter-CoT with several baseline methods, including Manual-CoT, Random-CoT, Complex-CoT, Auto-CoT, and Self-Consistency.",
            "7": "The results show that Iter-CoT consistently outperforms these baselines, highlighting its superiority.",
            "8": "**Flexibility**: Iter-CoT is shown to work effectively both with and without ground truth labels, making it versatile and applicable in various scenarios.",
            "9": "The use of GPT-4 as an evaluator in the absence of labels is a practical solution that maintains high performance.",
            "10": "**Self-Correction Capability**: The empirical evidence showing LLMs' ability to self-correct through iterative bootstrapping is compelling.",
            "11": "This capability is crucial for improving the accuracy and reliability of reasoning chains generated by LLMs.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Complexity and Computational Cost**: The iterative bootstrapping process, while effective, may introduce significant computational overhead.",
            "14": "The paper does not provide a detailed analysis of the computational cost associated with Iter-CoT, which could be a concern for practical deployment.",
            "15": "**Dependence on Evaluator Accuracy**: The performance of Iter-CoT(w/o label) is heavily dependent on the accuracy of the evaluator (e.g., GPT-4).",
            "16": "While the paper acknowledges this and provides some analysis, a more in-depth exploration of how to mitigate the impact of evaluator errors would strengthen the work.",
            "17": "**Limited Discussion on Failure Cases**: The paper primarily focuses on the successes of Iter-CoT but does not delve deeply into the failure cases or limitations of the approach.",
            "18": "Understanding the scenarios where Iter-CoT might not perform well would provide a more balanced view.",
            "19": "**Generalizability to Other Models**: While the paper demonstrates the effectiveness of Iter-CoT on several models (GPT-3.5-turbo, GPT-4, Llama-2-70B, and Llama-2-70B-Chat), it would be beneficial to see its performance on a broader range of models, especially smaller or less powerful ones, to fully understand its generalizability.",
            "20": "**User Study or Real-World Application**: The paper lacks a user study or real-world application to validate the practical benefits of Iter-CoT.",
            "21": "Including such a study would provide additional evidence of its utility and effectiveness in real-world scenarios.",
            "22": "**Clarity in Presentation**: Some sections of the paper, particularly the methodology and experimental setup, could benefit from clearer explanations and more structured presentation.",
            "23": "This would help readers better understand the implementation details and reproduce the results.",
            "24": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of chain-of-thought prompting for large language models.",
            "25": "The introduction of Iter-CoT and its demonstrated effectiveness across multiple datasets and tasks is commendable.",
            "26": "However, addressing the weaknesses, particularly in terms of computational cost, evaluator dependence, and clarity in presentation, would further strengthen the work."
        },
        "NXOoNofHP6": {
            "0": "This paper proposes iter-CoT, which uses the experiences with wrong cases to construct the demonstrations for CoT prompts.",
            "1": "It is a new CoT method that focuses on the demonstration selection to improve the in-context learning performance.",
            "2": "The evaluation is conducted on a wide range of benchmarks and various LLMs, showing advances in performance.",
            "3": "The idea of this paper is not very exciting.",
            "4": "a) The idea of self-correction is discussed in recent studies [1,2].",
            "5": "By constructing the correction demonstration pool, the samples that the LLM is prone to getting wrong are gathered and a correct CoT is prepared for each.",
            "6": "By sampling from these, the model can get stronger prompt.",
            "7": "b) The idea of ‘bootstrap’ is not very appealing.",
            "8": "First, LLM bootstrap has been proposed in [3], where the demos are totally generated by the proposed system.",
            "9": "However, in iter-CoT, both ‘w/ label’ setting and ‘wo label’ setting are under supervised, i.e., the golden label or another more powerful LLM.",
            "10": "(Minor) The analysis does not make this paper more convincing.",
            "11": "The settings of demo selection in the ablation can hardly uncover what the model learns from history errors and corrections.",
            "12": "The ‘rising and then falling’ trend in Figure 6 is not obvious.",
            "13": "Also, the comparisons are coarse-grained.",
            "14": "The demos, method, and reasoning steps are all different across all settings and may require in-depth analysis.",
            "15": "[1] Learning from Mistakes via Interactive Study Assistant for Large Language Models\n\n[2] Large Language Models Cannot Self-Correct Reasoning Yet\n\n[3] Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP"
        },
        "5xQmyzxdZf": {
            "0": "The motivation is strong and supported by two findings, i.e.",
            "1": "the self-correction ability of LLMs and the value of revised examples.",
            "2": "The Iter-CoT method is straightforward to follow.",
            "3": "Experimental results across three categories of reasoning tasks and ten datasets prove the effectiveness of this method.",
            "4": "A series of ablation experiments are conducted to investigate the two phases of the method.",
            "5": "The bootstrapping phase relies on GPT-4 to assess examples without labels, which does not address the issue of hallucinations and might lead to misjudgment of correct responses.",
            "6": "The summarization phase employs an LLM, which could also lead to hallucinations, especially when dealing with lengthy multi-turn conversation contexts, potentially impairing LLM performance and resulting in suboptimal summarizations.",
            "7": "Additionally, including the entire correction process in the summarization phase might lead to misleading summaries.",
            "8": "I found the innovativeness of the proposed method is limited, as the verify-and-correct approach has been previously explored in works such as ReAct and self-ask.",
            "9": "Additionally, more efficient methods for demonstration sampling, like k-nearest neighbor sampling, are available.",
            "10": "Reference:\nYao, Shunyu, et al.",
            "11": "\"React: Synergizing reasoning and acting in language models.\"",
            "12": "Press, Ofir, et al.",
            "13": "\"Measuring and narrowing the compositionality gap in language models.”\nLiu, Jiachang, et al.",
            "14": "\"What Makes Good In-Context Examples for GPT-3 ?.”"
        },
        "ekEFkkySeb": {
            "0": "The paper propose a new way of constructing few-shot chain-of-thought examples to use for in-context learning.",
            "1": "It is the first to apply self-refinement and LLM self-evaluation in construction demonstration examples, and the experimental results are strong with and without ground truth labels.",
            "2": "The authors evaluate the method with both proprietary GPT models and open source LLAMA models, and performs well in both cases.",
            "3": "The authors conduct thorough ablation to examine different stages of the construction pipeline and impact of LLM evaluator's performance on final accuracy.",
            "4": "These results are helpful for understanding the model performance and consistent with the main motivation to use iterative bootstrapping to improve exemplar quality.",
            "5": "It is mentioned that during the inference stage, a random N exemplars are samples as the fixed demonstrations for the entire test set.",
            "6": "I have a few questions on this choice:\n    1.",
            "7": "If only N random exemplars are used for the entire test set for inference, is it still necessary to construct the demonstration pool based on the entire training set?",
            "8": "Or what is the use of \"pool\" here since only N exemplars are used.",
            "9": "Is is possible to have comparison with the baselines use the same set of few-shot examples, just with different reasoning chain annotations.",
            "10": "Since the contribution of the paper is mainly on the construction of the reasoning chains, I feel it is better to have the selection of examples consistent for fair comparison.",
            "11": "Meanwhile it would be interesting to see if the example selection strategies used in Complex-CoT and Auto-CoT are helpful for Iter-CoT or not.",
            "12": "While the experiments are conducted over 10 different datasets, most of them are on the easier side and has some synthetic nature.",
            "13": "This also reflects on the results, where the LLMs generally achieve very high scores on these datasets already.",
            "14": "It would be better if the authors could evaluate on other more realistic and challenging datasets."
        },
        "lpmZITrm3E": {
            "0": "Overall, the writing is clear and easy to follow.",
            "1": "In addition, the organization of the main draft is well-established.",
            "2": "Improving the reasoning capability of LLMs is an interesting and important problem.",
            "3": "To this end, considering the quality of few-shot demonstrations is a reasonable and well-motivated direction.",
            "4": "The proposed method is simple and can be applicable regardless of the types of LLMs.",
            "5": "Also, it shows a significant improvement compared to the existing baselines, well-studied in the same problems.",
            "6": "While the improvements are significant, the explanation for such gains is not sufficient.",
            "7": "For example, the proposed Iter-CoT outperforms both Manual-CoT and Complex-CoT in Table 1, which use the ground-truth annotations of rationales.",
            "8": "Since Iter-CoT is a method to generate correct rationales of unannotated samples, both methods can be considered as the upper bound as they always use the ground-truth rationales.",
            "9": "Therefore, it’s not natural that Iter-CoT shows better performance than those ones.",
            "10": "What is the source of such improvement?",
            "11": "One of the major concerns is an increase in cost due to the iterative usage of LLMs, but there is no discussion regarding this.",
            "12": "How many costs are required for Iter-CoT, compared to other methods?",
            "13": "Also, it would be better if the authors could have a corresponding discussion in the draft.",
            "14": "While the proposed method is applied to generate rationales of the training examples for few-shot demonstrations, it can be applicable during the inference.",
            "15": "Also, the framework of bootstrapping the rationale has been widely explored [1,2].",
            "16": "Hence, I’m wondering if the improvement can be enlarged when Iter-CoT is also applied during inference.",
            "17": "### Minor\n\n1.",
            "18": "It would be better to change the order of the presented method in Table 3, with the decreasing order for better presentation.",
            "19": "Also, as the authors presented in the Appendix, STaR is highly relevant to the proposed Iter-CoT.",
            "20": "It seems to be better to add this baseline to the main table if possible.",
            "21": "Regarding Figure 6, the authors mention that “we utilize the best exemplars in this section”.",
            "22": "What is the meaning of best exemplars?",
            "23": "Also, how do you select them?",
            "24": "Do you utilize another validation set to choose them?",
            "25": "[1] Madaan et al., Self-Refine: Iterative Refinement with Self-Feedback., NeurIPS 23  \n[2] Shinn et al., Reflexion: Language Agents with Verbal Reinforcement Learning., NeurIPS23"
        }
    },
    "i5da6iedW8": {
        "llm_review": {
            "0": "**Innovative Approach**:\n   - The paper introduces FedBiOT, a novel method for federated learning that addresses both data privacy and intellectual property protection.",
            "1": "This dual focus is crucial in real-world applications where data sensitivity and proprietary models are common.",
            "2": "**Bi-Level Optimization**:\n   - The use of bi-level optimization to ensure the emulator distilled on a public dataset can aid in local fine-tuning on clients' private datasets is a sophisticated approach.",
            "3": "This method effectively handles distribution drifts between datasets, which is a significant challenge in federated learning.",
            "4": "**Extensive Experiments**:\n   - The paper conducts extensive experiments on LLaMA-7B for various federated learning tasks, demonstrating significant improvements over existing baselines.",
            "5": "This empirical validation strengthens the credibility of the proposed method.",
            "6": "**Detailed Problem Formulation**:\n   - The paper provides a thorough formulation of the problem, clearly outlining the challenges and the proposed solutions.",
            "7": "This clarity helps in understanding the complexities involved in federated learning with large language models.",
            "8": "**Comprehensive Evaluation**:\n   - The evaluation covers multiple tasks (code generation, math problem-solving, and question answering) and different settings (i.i.d.",
            "9": "and non-i.i.d.",
            "10": "data), providing a holistic view of the method's performance.",
            "11": "**Ablation Studies**:\n   - The inclusion of ablation studies to analyze the impact of different components and hyperparameters of FedBiOT is commendable.",
            "12": "This analysis provides insights into the effectiveness of each part of the proposed method.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Complexity and Practicality**:\n   - The proposed method, while innovative, is quite complex.",
            "15": "The bi-level optimization and the need for emulator alignment might be challenging to implement in practice, especially for organizations with limited computational resources.",
            "16": "**Limited Comparison with Other Methods**:\n   - The paper primarily compares FedBiOT with offsite-tuning and its federated version (FedOT).",
            "17": "It would be beneficial to include comparisons with other state-of-the-art federated learning methods to provide a broader context of its performance.",
            "18": "**Scalability Concerns**:\n   - The scalability of FedBiOT to a larger number of clients or more diverse datasets is not thoroughly explored.",
            "19": "Federated learning systems often involve numerous clients, and it is essential to understand how the method scales in such scenarios.",
            "20": "**Public Dataset Dependency**:\n   - The method relies on a public dataset for emulator distillation.",
            "21": "The availability and quality of such public datasets can vary, potentially affecting the performance of the method.",
            "22": "This dependency might limit the applicability of FedBiOT in certain domains.",
            "23": "**Evaluation Metrics**:\n   - While the paper uses standard evaluation metrics for the tasks, it would be helpful to include additional metrics that capture other aspects of model performance, such as robustness to adversarial attacks or fairness across different client data distributions.",
            "24": "**Communication Overhead**:\n   - The paper mentions the communication overhead but does not provide a detailed analysis of how FedBiOT mitigates this issue compared to other federated learning methods.",
            "25": "A more in-depth discussion on communication efficiency would be valuable.",
            "26": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in federated learning for large language models by addressing both data privacy and intellectual property protection.",
            "27": "The innovative use of bi-level optimization and the comprehensive experimental validation are notable strengths.",
            "28": "However, the complexity of the method, scalability concerns, and dependency on public datasets are areas that need further exploration.",
            "29": "Including more comparisons with other methods and a detailed analysis of communication overhead would strengthen the paper further."
        },
        "tDOzx90LnE": {
            "0": "The combination of LLM and federated learning is interesting.",
            "1": "The problem formulation is well presented.",
            "2": "The layer selection and dropout mechanism is interesting.",
            "3": "However, there are some improvements for the paper:\n1.",
            "4": "The number of clients is very small.",
            "5": "In section 4.1, the number of clients is 4, which is relatively very small compared with that in real FL settings.",
            "6": "The idea is straightforward, which is presented in existing works, e.g., Yosinski et al., 2014.",
            "7": "The selection of dropout rate is not well elaborated.",
            "8": "Tables 1 and 2 are not clear.",
            "9": "The first line is not explained.",
            "10": "The unit can be added, and the meaning of the numbers can be explained.",
            "11": "The experimentation show that the performance of FedBiOT may be inferior than baselines.",
            "12": "The classic FL approaches can be added as baselines."
        },
        "0BprflfTDA": {
            "0": "- This paper considers a relatively new setting in federated learning and large language models.",
            "1": "- This paper proposes a new FL algorithm FedBiOT, which trains adapter over emulator to achieve parameter-efficient tuning.",
            "2": "- Experiments show the effectiveness of FedBiOT by comparing with two baselines.",
            "3": "- The contributions need to be clarified.",
            "4": "For me, I think the topic of this paper is interesting and worth exploring.",
            "5": "However, it is not so clear what are the main contributions of this paper since previous work [1] has considered such setting and proposed FedOT (federated learning with offsite-tuning).",
            "6": "Are the main contributions lying on improving FedOT via a bi-level optimization approach?",
            "7": "- The motivations need to be further clarified.",
            "8": "This paper claims that the clients cannot obtain the full model due to intellectual property of LLM.",
            "9": "However, I wonder if such claim still holds after the release of Llama2.",
            "10": "- Some meaningful experiments are missing.",
            "11": "- Some experiments for reference.",
            "12": "It would be more helpful if the authors can provide the results when clients can obtain the full model, such that we could see how large the gap is.",
            "13": "- Computation resources comparisons.",
            "14": "This method requires more training resources (e.g., more training steps) compared to baselines.",
            "15": "However, this paper does not show such comparisons, which would promote readers' understanding.",
            "16": "- Some confusions:\n  - \"Improvement 1\" at page 5.",
            "17": "What are the definations of bottom / first / last layers.",
            "18": "Suggest consistent expressions like first / last.",
            "19": "Currently, my rating is between 5 and 6.",
            "20": "I would consider re-rating if the authors can address the above concerns.",
            "21": "[1] Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou.",
            "22": "Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning."
        },
        "Ta9TatQnmd": {
            "0": "The topic is timely and interesting.",
            "1": "The experimental evaluation was only implemented in LLaMA-7B.",
            "2": "How does it work on other mainstream models such as ChatGPT2?",
            "3": "In the experiment, federated learning only considered 8 clients.",
            "4": "There is a lack of experiments that vary the number of clients and the number of training samples each client own."
        },
        "TnQupyYQ8Y": {
            "0": "The paper is built upon a relatively recent work so that it may offer modern insights into the related research fields.",
            "1": "Experimental results support the proposed improvements in the paper.",
            "2": "The proposed improvements in the paper are general and should be easy to adopt.",
            "3": "From a technical perspective, the two improvements proposed in the article may be incremental.",
            "4": "One involves changing the index of the fine-tuning layers (based on observation), and the other relies on the traditional distillation method.",
            "5": "Both methods are essentially at the level of tricks and are insufficient to serve as contributions to the paper.",
            "6": "I have doubts about the \"intellectual property protection\" aspect of the paper.",
            "7": "In this framework, although local clients can only obtain a portion of the model instead of the entire model, this sub-model can still be fine-tuned and used for inference, which implies that the majority of the model's functionality has been preserved.",
            "8": "Essentially, malicious users can still steal this intellectual property.",
            "9": "This framework does not seem to provide significant protection, so I do not consider the \"intellectual property protection\" mentioned in the title appropriate.",
            "10": "The paper should provide a detailed algorithm to help readers follow."
        }
    },
    "H49g8rRIiF": {
        "llm_review": {
            "0": "**Innovative Cross-Modal Approach**:\n   - The paper introduces a novel approach by leveraging pretrained language models for 3D point cloud perception.",
            "1": "This cross-modal strategy is innovative and opens new avenues for 3D point cloud understanding, which traditionally relies on vision-based models.",
            "2": "**Effective Methodology**:\n   - The proposed LAMP (Language Models reading Point clouds) method is simple yet effective.",
            "3": "It aligns the data distribution of 3D point clouds with pretrained language models, requiring only a small portion of parameters to be trained.",
            "4": "This efficiency is a significant strength.",
            "5": "**Comprehensive Experiments**:\n   - The paper provides extensive experimental validation on both unimodal and multimodal tasks.",
            "6": "The results demonstrate the superiority of LAMP over existing methods in various benchmarks, including ModelNet-40, S3DIS, ShapeNetPart, and ScanRefer.",
            "7": "**Performance on Long-Tailed and OOD Recognition**:\n   - LAMP shows impressive performance in handling long-tailed and out-of-domain (OOD) recognition problems.",
            "8": "This robustness is crucial for real-world applications where data distributions are often imbalanced or different from training data.",
            "9": "**Multimodal Capabilities**:\n   - The extension of LAMP to multimodal tasks, such as 3D visual grounding, is well-executed.",
            "10": "The shared encoding between text and point clouds mitigates the modality gap, leading to improved performance in multimodal scenarios.",
            "11": "**Detailed Analysis**:\n   - The paper provides a thorough analysis of various design choices, including pretrained corpus, model scale, architecture, and learning schemes.",
            "12": "This detailed examination helps in understanding the impact of different factors on the model's performance.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Limited Discussion on Limitations**:\n   - The paper lacks a detailed discussion on the limitations of the proposed approach.",
            "15": "For instance, the potential challenges in scaling the method to more complex or larger datasets, or the computational overhead of using language models for 3D perception, are not addressed.",
            "16": "**Dependency on Pretrained Language Models**:\n   - While leveraging pretrained language models is innovative, it also introduces a dependency on the quality and nature of the language corpus used for pretraining.",
            "17": "The paper does not explore the potential impact of different pretraining corpora on the model's performance in depth.",
            "18": "**Generalization to Other Modalities**:\n   - The paper focuses on point clouds and text, but it does not discuss the potential for generalizing this approach to other modalities, such as images or videos.",
            "19": "Exploring this aspect could provide a more comprehensive understanding of the method's applicability.",
            "20": "**Complexity of Implementation**:\n   - The proposed method involves several components, such as point tokenization, cross-modal self-attention, and cross-modal cross-attention learning.",
            "21": "While effective, the complexity of implementing these components might be a barrier for practitioners.",
            "22": "**Evaluation Metrics**:\n   - The paper primarily uses standard metrics like overall accuracy (OA) and mean accuracy (mAcc) for evaluation.",
            "23": "Including additional metrics, such as computational efficiency, memory usage, and inference time, would provide a more holistic evaluation of the method.",
            "24": "**Comparison with More Baselines**:\n   - While the paper compares LAMP with several state-of-the-art methods, including more diverse baselines, especially those from different domains (e.g., image-based models adapted for 3D perception), would strengthen the comparative analysis.",
            "25": "#### Conclusion:\n\nOverall, the paper presents a novel and effective approach to adapting language models for 3D point cloud perception.",
            "26": "The extensive experimental validation and detailed analysis of design choices are commendable.",
            "27": "However, addressing the limitations, exploring generalization to other modalities, and providing a more comprehensive evaluation would further strengthen the work.",
            "28": "The proposed LAMP framework has the potential to significantly impact both 3D vision and multimodal research, making it a valuable contribution to the field."
        },
        "SfNwCOjE7r": {
            "0": "- The authors propose to adapt Language Models to tackle the point cloud perception problem, which has some originality.",
            "1": "- Their method outperforms the existing baseline approaches on several benchmarks, which demonstrates the effectiveness of their proposed method.",
            "2": "- The paper writing is clear and easy to follow.",
            "3": "This paper shares very similar spirits with many recent papers on Large (Vision) Language Models and Point Cloud Understanding:\n\n1.",
            "4": "Gao et al.",
            "5": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model.",
            "6": "Guo et al.",
            "7": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following.",
            "8": "Essentially, this paper and other relevant papers are trying to bind point cloud representations to Language Models via adaptation.",
            "9": "The attention-based adaptation has also been exploited in LLaMA-Adapter V2 (Note that this paper also supports point cloud inputs).",
            "10": "Hence, the authors need to discuss the differences with those works and also compare their method with those methods in the experiments.",
            "11": "From my understanding adapting Language Models to 3D point clouds with attention is straight-forward and not that novel considering the above-mentioned literature.",
            "12": "In addition to the 3D object datasets, the authors also need to evaluate their method on the more realistic indoor 3D datasets such as SUN-RGBD and ScanNet."
        },
        "jbkhHEKzwE": {
            "0": "The proposed LAMP approach demonstrates that by merely projecting point cloud features onto language models, while maintaining the language model in a frozen state, it is still possible for the model to process 3D data.",
            "1": "This underscores the versatility of language models as general-purpose functions, showcasing their capacity to handle data from unfamiliar modalities even without directly updating their parameters.",
            "2": "The experiments show that even with a few trainable parameters, the LAMP can still achieve reasonable performance.",
            "3": "The reviewer finds some performance in the paper somewhat unconvincing and also seems to lack a proper baseline to compare, particularly when referring to the results in Table 2.",
            "4": "For instance, when comparing point-MLP elite with LAMP, the performance appears quite similar, or even worse (considering Point-MLP elite has 90.9 as mAcc).",
            "5": "While the trainable parameters of the Point-MLP elite are also minimal at 0.68M, its inference speed is anticipated to be notably faster.",
            "6": "This is because, during inference, Point-MLP elite maintains its 0.68M parameters.",
            "7": "Conversely, LAMP, despite reducing only its trainable parameters, is expected to have a longer inference time, given it leverages a significantly larger frozen language model.",
            "8": "Thus, from a practical standpoint, the advantage of LAMP having a small number of trainable parameters but including an expansive language model seems to weaken its asserted advantages.",
            "9": "Additionally, given that ModelNet40 is somewhat of a saturated benchmark, it would enhance the paper's credibility if LAMP were evaluated on more challenging datasets, for example, the ScanObjectNN classification benchmark.",
            "10": "This would provide a clearer perspective on its efficacy and potential real-world applications.",
            "11": "Also, there are some other works like RepSurf[1], which is also lightweight (~1.5M) and exhibits very strong performance (94.7 OA) and at the same time fast at inference (3.1ms, 0.81GFLOPs, roughly 20 to 200 times faster).",
            "12": "[1] Ran, Haoxi, Jun Liu, and Chengjie Wang.",
            "13": "\"Surface representation for point clouds.\"",
            "14": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
            "15": "2022."
        },
        "IrXTupuHJX": {
            "0": "The starting point of the paper is very meaningful.",
            "1": "It uses the existing Language model to initialize the model and only trains a small part of the adapter, making the model have good performance capabilities.",
            "2": "The experimental results look pretty good.",
            "3": "Although using LLM (Language Learning Models) for 3D point analysis is a good starting point, I notice that the main experiments in the article still focus on pure point cloud experiments, such as 3D Object Classification and Part Segmentation, etc.",
            "4": "Tasks using Language Models typically focus on multimodal tasks (point cloud-text), like the 3D Visual Grounding mentioned in the paper.",
            "5": "However, it seems that most of the experiments in the article still conventionally utilize task-specific heads for 3D point cloud analysis.",
            "6": "The use of the LLM+adapter pipeline doesn't seem very suitable for traditional 3D point cloud analysis.",
            "7": "The reason why LLaVA and minigpt4 can effectively use the LLM+image adapter pipeline is that the final output space is still in the language space, so there's no issue with keeping the LLM fixed without further training.",
            "8": "In this article, the output space is a traditional perceptual space, such as 3D classification or segmentation.",
            "9": "To address these issues, one could either formulate traditional point analysis tasks as vision-language tasks with an output in the language space, or replace the LLM with a 2D image encoder to initialize parameters.",
            "10": "The approach in the article seems somewhat unreasonable and odd.",
            "11": "The author might want to reconsider it.",
            "12": "The experimental results don't seem to show a significant improvement.",
            "13": "The ModelNet dataset is too small.",
            "14": "Using a model the size of BERT might lead to overfitting?",
            "15": "It seems that even simpler models already achieve good results, such as the PointNet++ from six years ago.",
            "16": "Perhaps it's more appropriate to test on a larger dataset and then redefine all tasks as vision-language tasks.",
            "17": "The paper writing needs improvement; it looks a bit rushed."
        }
    },
    "1mjsP8RYAw": {
        "llm_review": {
            "0": "**Novelty and Innovation**:\n   - The paper introduces SFA VEL, a novel unsupervised pretraining framework for fact verification that leverages language model distillation.",
            "1": "This approach is innovative as it eliminates the need for annotated data, which is often time-consuming and costly to obtain.",
            "2": "**State-of-the-Art Performance**:\n   - The proposed method achieves significant improvements over previous state-of-the-art methods on the FEVER and FB15k-237 datasets.",
            "3": "Specifically, it improves label accuracy by +8% on FEVER and +5.3% Hits@1 on FB15k-237, demonstrating the effectiveness of the approach.",
            "4": "**Comprehensive Evaluation**:\n   - The paper provides extensive experimental results, including comparisons with both supervised and unsupervised methods.",
            "5": "The inclusion of low-data experiments and ablation studies further strengthens the evaluation, showcasing the robustness and generalizability of the proposed method.",
            "6": "**Detailed Methodology**:\n   - The paper thoroughly explains the proposed approach, including the data processing pipeline, pretraining methodology, and the novel contrastive loss function.",
            "7": "The clarity in the presentation of the method makes it easier for readers to understand and potentially replicate the work.",
            "8": "**Reproducibility**:\n   - The authors have made the source code available, which is a significant step towards ensuring reproducibility of the results.",
            "9": "This transparency is commendable and beneficial for the research community.",
            "10": "#### Weaknesses:\n\n1.",
            "11": "**Complexity of the Approach**:\n   - While the proposed method is innovative, it is also quite complex.",
            "12": "The combination of multiple components (e.g., claim-fact distillation, intra-sample contrastive loss, scoring loss) might make it challenging for practitioners to implement and understand fully.",
            "13": "Simplifying the approach or providing more intuitive explanations could help in this regard.",
            "14": "**Dependence on Pre-trained Language Models**:\n   - The performance of SFA VEL heavily relies on the quality of the pre-trained language models used.",
            "15": "This dependence might limit the applicability of the method in scenarios where such high-quality pre-trained models are not available or feasible to use due to computational constraints.",
            "16": "**Limited Analysis of Negative Samples**:\n   - The paper briefly discusses the generation of negative samples but does not provide a detailed analysis of their impact on the overall performance.",
            "17": "A more in-depth exploration of how different strategies for generating negative samples affect the results could provide valuable insights.",
            "18": "**Scalability Concerns**:\n   - The method involves processing large knowledge bases and generating embeddings for numerous facts, which might raise scalability concerns.",
            "19": "The paper could benefit from a discussion on the computational requirements and potential optimizations to handle large-scale datasets efficiently.",
            "20": "**Generalization to Other Domains**:\n   - While the method shows impressive results on the FEVER and FB15k-237 datasets, it is not clear how well it generalizes to other domains or types of fact verification tasks.",
            "21": "Including experiments on a more diverse set of datasets could strengthen the claims of generalizability.",
            "22": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of fact verification by introducing a novel unsupervised pretraining framework that leverages language model distillation.",
            "23": "The method achieves state-of-the-art performance and is thoroughly evaluated through extensive experiments and ablation studies.",
            "24": "However, the complexity of the approach, dependence on pre-trained models, and scalability concerns are potential areas for improvement.",
            "25": "Addressing these weaknesses could further enhance the applicability and impact of the proposed method."
        },
        "IUmLC4Exat": {
            "0": "Experimental results on the FEVER dataset show the advantage of the proposed method.",
            "1": "The improvement seems very significant.",
            "2": "The unsupervised manner of conducting fact verification is encouraged and useful.",
            "3": "Though the experiments show the effectiveness of the method, I do not get how the framework solves the cold starting.",
            "4": "For the scoring module, the claim embeddings from the LM are very different from those of Knowledge from the knowledge model.",
            "5": "Then how does the framework pick the top-k evidence at the beginning?",
            "6": "How does $L_{distill}$ work at the early iterations?",
            "7": "This is an important prerequisite that should be clearly stated in the paper.",
            "8": "Why does the paper only evaluate the effect of each loss on a smaller T5 model?",
            "9": "Considering the best performance reported in the work is based on Transformer-XL, ablation studies based on it are desired.",
            "10": "The annotation in the methodology section makes me really confused.",
            "11": "For example but not limited to:\n\n    (1) In Section 3.1, what does the V and each $v_i$ mean?",
            "12": "I cannot get it until I read through the whole methodology section.",
            "13": "(2) The use of subscript and superscript is messed up.",
            "14": "I think embedding is presented as $X_F$ in Section 3.2, but it becomes $X^F$ in Section 3.3.",
            "15": "(3) Some annotations are not really necessary, e.g., $N_i$ in Equation 3 are not used anywhere else in the paper.",
            "16": "I encourage the authors to make Section 3 more concise and clear by better formula presenting."
        },
        "doSiX2dZiB": {
            "0": "The experimental result of SFEVEL on the FEVER dataset is remarkable.",
            "1": "First of all, the paper is confusing in using the term \"unsupervised\".",
            "2": "The proposed method SFAVEL is unsupervised because it is for learning a knowledge model.",
            "3": "However, the fact-verification model reported in section 4 is supervised.",
            "4": "The model uses SFAVEL for mapping fact / claim to vectors and then uses a classifier trained in a supervised learning manner.",
            "5": "Secondly, it is unclear about what is the used \"linear probe\".",
            "6": "Fig 4b shows that the linear probe takes top evidence as input.",
            "7": "But then how can we verify the input claim if we use only evidence (and their scores)?",
            "8": "E.g.",
            "9": "how knowing \"Obama was born in Hawaii\" and \"Hawaii is in the US\"  can reject a claim without knowing what the claim is?",
            "10": "Thirdly, although the performance of the proposed model is remarkable, it is unclear why there's such a big gap between it and the existing models in the literature.",
            "11": "What are cases that the proposed model can solve but the others can?",
            "12": "Does the model find some crucial factors that the others miss?",
            "13": "Last but not least, the proposed SFAVEL is for learning a knowledge model.",
            "14": "But it is unclear whether that knowledge model is useful for other fact-verification cases (like on other FEVER dataset -- FEVER 2.0 for example).",
            "15": "Also, whether that knowledge model is also useful for other downstream task requiring fact?"
        },
        "cqTCmzm6YX": {
            "0": "The paper describes a novel approach for fact-verification\n2.",
            "1": "Results show significant gains in comparison to state of the art approaches 1.",
            "2": "Generalizability of the approach given other knowledge bases\n2.",
            "3": "Self-supervision is an ambitious claim\n3.",
            "4": "Fever is the only dataset used"
        },
        "liaKzzBmaT": {
            "0": "* The paper is very well written and well motivated.",
            "1": "* The results presented in the paper are impressive, outperforming FEVER SOTA even for supervised approaches.",
            "2": "* The authors compare the approach on 7 different models, including a variety of small to medium size models.",
            "3": "* The paper contains good ablation experiments, in particular analysing the different components of the loss on a small model.",
            "4": "* No large models were included, the biggest model tested has 250M parameters.",
            "5": "There is no strict definition of LLM, but the authors may overpromise in their title/intro when no model with more than 1B parameters is included.",
            "6": "* The increase over the SOTA may be exaggerated, given that most of the systems the paper compares to are several years old, and do not include the latest generation of models.",
            "7": "(This is not strictly a weakness, but context worth mentioning.)"
        },
        "PKkxHxsx5P": {
            "0": "The author's idea of leveraging a knowledge graph to produce positive and negative examples of unlabeled claims to train a scoring model is creative.",
            "1": "The paper is very well-structured, and easy to follow.",
            "2": "The experiments presented promising results on FEVER (~8% improvement on accuracy), and the method can work on a broad set of language models.",
            "3": "The technique proposed in the paper does not seem to be generalizable.",
            "4": "Specifically, the positive and negative examples constructed through triples from knowledge graph are too simple, which makes this method difficult to generalize to more complicated claims.",
            "5": "Specifically, triples can only represent who did what, while in reality, a claim can be who did what at where on when for why.",
            "6": "Any wrong information about these factors can make a claim false.",
            "7": "While it is not clear to me how the current method can learn a model that can be effectively aware of some more fine-grained factual differences.",
            "8": "The experimental setup is limited.",
            "9": "The evaluations are only based on FEVER, which is not convincing.",
            "10": "FEVER is created through Wikipedia, and Wikipedia information is closer to triples, which is bias to author's method and training process.",
            "11": "At least, an experiment to show the effectiveness of this method on other fact verification dataset would be very helpful.",
            "12": "Ranking may not be the best problem formulation for fact verification.",
            "13": "For claim verification, it is important to help people decide whether they should believe the claim or not.",
            "14": "Now the author formulates this problem as a ranking problem, which is not very useful from a fact verification perspective.",
            "15": "it is not clear what does it mean to the user that a claim can find a piece of evidence with 0.9 score."
        }
    },
    "YH5w12OUuU": {
        "llm_review": {
            "0": "**Innovative Approach**:\n   - The paper introduces a novel framework, TEMPO, which leverages the strengths of Generative Pre-trained Transformers (GPT) for time series forecasting.",
            "1": "This is a significant step forward in exploring the applicability of GPT-type architectures beyond natural language processing.",
            "2": "**Incorporation of Inductive Biases**:\n   - TEMPO effectively incorporates two essential inductive biases for time series tasks: decomposition of trend, seasonal, and residual components, and the design of prompts to facilitate distribution adaptation.",
            "3": "This dual focus is well-justified and theoretically sound, enhancing the model's ability to capture complex temporal dynamics.",
            "4": "**Comprehensive Experiments**:\n   - The experiments are extensive and cover a wide range of benchmark datasets, including zero-shot settings and multimodal inputs.",
            "5": "The results demonstrate the superior performance of TEMPO over state-of-the-art methods, highlighting its robustness and generalizability.",
            "6": "**Theoretical Insights**:\n   - The paper provides a formal analysis bridging the time series domain with the frequency domain, underscoring the necessity of decomposing components for effective time series analysis.",
            "7": "This theoretical foundation strengthens the validity of the proposed approach.",
            "8": "**Interpretable Framework**:\n   - By leveraging the three key additive components of time series data, TEMPO offers an interpretable framework for understanding the interactions among input components.",
            "9": "This is a valuable feature for practical applications where interpretability is crucial.",
            "10": "**Multimodal Capability**:\n   - The inclusion of multimodal datasets and the demonstration of TEMPO's ability to handle textual information alongside time series data is a significant advancement.",
            "11": "This capability opens up new avenues for research and application in diverse domains.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Complexity and Computational Overhead**:\n   - The proposed method involves several complex steps, including decomposition, normalization, and prompt design, which may introduce significant computational overhead.",
            "14": "The paper could benefit from a more detailed discussion on the computational efficiency and scalability of TEMPO.",
            "15": "**Limited Comparison with Non-Transformer Models**:\n   - While the paper compares TEMPO with several state-of-the-art transformer-based models, it lacks a thorough comparison with non-transformer models, such as traditional statistical methods or other deep learning architectures like CNNs and RNNs.",
            "16": "Including these comparisons would provide a more comprehensive evaluation of TEMPO's performance.",
            "17": "**Ablation Study Scope**:\n   - The ablation study, although insightful, is somewhat limited in scope.",
            "18": "It primarily focuses on the impact of prompt and decomposition components.",
            "19": "A more detailed ablation study exploring other aspects of the model, such as the effect of different prompt designs or the choice of pre-trained transformer backbone, would be beneficial.",
            "20": "**Generalization to Other Domains**:\n   - While the paper demonstrates TEMPO's effectiveness on a variety of time series datasets, it would be helpful to see more discussion on its generalization to other domains or types of time series data.",
            "21": "For instance, how well does TEMPO perform on highly irregular or sparse time series data?",
            "22": "**Interpretability and Practical Application**:\n   - Although the paper emphasizes the interpretability of the model, it could provide more concrete examples or case studies demonstrating how this interpretability can be leveraged in practical applications.",
            "23": "This would help bridge the gap between theoretical contributions and real-world utility.",
            "24": "**Potential Overfitting in Zero-Shot Settings**:\n   - The zero-shot setting is a challenging scenario, and while TEMPO shows promising results, there is a risk of overfitting to the pre-training datasets.",
            "25": "The paper could discuss strategies to mitigate this risk and ensure robust generalization to truly unseen data.",
            "26": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of time series forecasting by introducing TEMPO, a prompt-based generative pre-trained transformer.",
            "27": "The strengths of the paper lie in its innovative approach, comprehensive experiments, and theoretical insights.",
            "28": "However, addressing the weaknesses related to computational complexity, broader comparisons, and practical applications would further strengthen the contribution.",
            "29": "The paper is a valuable addition to the literature and opens up new directions for research in time series forecasting using pre-trained transformers."
        },
        "G9AdPXMHXn": {
            "0": "Writing: The paper is well-written for the most part.",
            "1": "Interpretability: The authors aim to shed some light on the time-series predictions made by the model.",
            "2": "Modeling time-series and text together: I really liked this key insight of the paper.",
            "3": "I think it is under-explored and valuable.",
            "4": "Theory: The authors provide some theoretical insight into the design decisions behind their model.",
            "5": "Claims:\n1.",
            "6": "While achieving strong prediction performance, the previous works on timeseries mostly benefit ... that captures temporal dependencies but overlooks a series of intricate patterns within timeseries data, such as seasonality, trend, and residual.\"",
            "7": "-- I do not agree with the authors, multiple recent approaches using deep learning for time-series forecasting have decomposed model inputs into trend and seasonal components.",
            "8": "See N-BEATS, N-HITS (stacked MLP based models), AutoFormer, as an example.",
            "9": "Theorem 3.1 -- I do not fully understand the implications of Theorem 3.1 and how that affects the design choices of the authors.",
            "10": "Prompt pool captures seasonal and trend components: The authors provide an example of 3 time-series from 1 dataset to demonstrate that the prompts capture season and trend components, but I am not sure this is sufficient evidence.",
            "11": "It would be interesting to look at the distribution of prompts for multiple (or all) time-series in one or more datasets, as time-series are clustered based on their trend and/or seasonality components.",
            "12": "I believe this would give a more, dataset level evidence for the authors' claims.",
            "13": "Interpretability: I am not sure how the GAM and SHAP provide interpretability, beyond confirming what is expected from these models, i.e.",
            "14": "the residuals do not have any pattern.",
            "15": "Experimentation: \n1.",
            "16": "\"Large-scale experiments/benchmarks\": The authors omit several benchmarks, and therefore I would argue that the experiments are not large-scale.",
            "17": "For e.g., for long horizon datasets, the authors do not use the Influenza-like Illnesses and Exchange-rate datasets which PatchTST and TimesNet, and other recent studies.",
            "18": "Secondly, there are multiple short-horizon benchmarks, like M3 or the M4 datasets, and the much larger and comprehensive Monash Forecasting archive, yet the authors do not confirm their methods on these datasets.",
            "19": "Multiple methods are omitted from the evaluation, for e.g.",
            "20": "statistical methods such as Auto-ARIMA, Auto-THETA, Auto-ETS etc., and deep learning methods such as N-HITS and N-BEATS.",
            "21": "Also the authors cite PromptCast but do not compare their method to this particular baseline.",
            "22": "The value of prompt pool-- The authors demonstrate in Table 9 that the prompt pool helps model prediction.",
            "23": "How would they explain the methods without prompt pooling doing better on some datasets for some forecasting horizons?",
            "24": "Clarity:\n1.",
            "25": "Insufficient details in model design and experimentation.",
            "26": "See Questions.",
            "27": "Minor: \n1.",
            "28": "Typos: inclduing, outpemforms ... etc.",
            "29": "References: I would encourage the authors to find references to accepted papers, instead of citing their ArXiv versions."
        },
        "I4gu39uxqe": {
            "0": "1) The paper is well-written.",
            "1": "2) The improvement of forecasting performance over benchmark methods is significant and consistent across datasets.",
            "2": "1) The idea of decomposition of trend, seasonality, and residuals is not that novel and has been used for time-series forecasting.",
            "3": "2) The theorem 3.1 does not directly prove the point “more importance in current transformer-based methods as the attention mechanism, in theory, may not disentangle the disorthogonal trend and season signals automatically\".",
            "4": "3) The result shown in Figure 2 seems to be obvious since the trend is easier to learn and may take a large portion of the data."
        },
        "tk1gvink0K": {
            "0": "This paper’s writing is clear and easy to follow.",
            "1": "For example, the methodology part gives a clear description on how to build the time series input representation and the design of the prompt pool.",
            "2": "The experiments on long-term time series forecasting, short-term forecasting and towards foundation model’s training are well organized to prove the model’s power from different aspects.",
            "3": "The proposed solution is well motivated: the motivation of decomposition is supported by both empirically and theoretically and the introduce of retrieval-based prompt selection can help the large pre-trained model handle complex non-stationary time series data with distribution shifts.",
            "4": "Utilizing the pre-trained transformer backbone, TEMPO give a state-of-the-art results on the popular time series research dataset.",
            "5": "The prompt pool’s improvement is limited: the prompt pool is supposed to have more contribution to the accuracy as the intuition is clear and convincing.",
            "6": "The collection of TETS dataset is not clear: a clear but simple discription in the main paper is necessary.",
            "7": "It seems only decoder-based pretrain model is considered in this paper.",
            "8": "The encoder based backbone (like Bert) and encoder-decoder based backbone (like T5) is also recommend in this stage."
        }
    },
    "64kSvC4iPg": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method for compressing context memory in Transformer language models, which is crucial for online scenarios where the context continually expands.",
            "1": "This is a significant contribution as it addresses the growing memory and computational demands in such settings.",
            "2": "**Efficiency**: The proposed method, Compressed Context Memory (CCM), effectively reduces the memory and computational overhead by compressing the attention key/value pairs into a compact memory space.",
            "3": "This is particularly beneficial for environments with limited memory resources.",
            "4": "**Conditional LoRA**: The integration of a lightweight conditional LoRA into the language model’s forward pass during inference is a clever approach.",
            "5": "It allows for efficient training without the need for fine-tuning the entire set of model weights, which is a significant advantage.",
            "6": "**Parallelized Training Strategy**: The paper proposes an efficient training strategy that models the recursive compression process as a single parallelized forward computation.",
            "7": "This innovation addresses the training inefficiencies typically associated with recursive context compression procedures.",
            "8": "**Comprehensive Evaluation**: The authors provide thorough evaluations across various applications, including conversation, personalization, and multi-task learning.",
            "9": "The results demonstrate that the proposed method achieves performance levels comparable to a full context model with significantly smaller context memory size.",
            "10": "**Practical Applicability**: The paper demonstrates the applicability of the approach in a streaming setting with unlimited context length, outperforming the sliding window approach.",
            "11": "This shows the method's potential for real-world applications.",
            "12": "**Open Source Code**: The availability of the code on GitHub enhances the paper's impact by allowing other researchers to replicate and build upon the work.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Complexity Analysis**: While the paper provides a complexity analysis, it could benefit from a more detailed discussion on the trade-offs between different memory update functions (e.g., CCM-concat vs. CCM-merge).",
            "15": "A deeper exploration of the scenarios where one might be preferred over the other would be valuable.",
            "16": "**Generalization**: Although the paper demonstrates the generalization ability of the method across different tasks, the performance drop when using a unified compression adapter suggests that there might be room for improvement.",
            "17": "Further investigation into how to mitigate this performance drop would strengthen the paper.",
            "18": "**Comparison with More Baselines**: The paper compares the proposed method with several established baselines, but it could include more recent and advanced methods for a more comprehensive evaluation.",
            "19": "For instance, a comparison with other state-of-the-art context compression techniques could provide a clearer picture of the method's relative performance.",
            "20": "**Detailed Ablation Studies**: While the paper includes some ablation studies, more detailed experiments isolating the impact of each component (e.g., the conditional LoRA, the parallelized training strategy) would provide deeper insights into the contributions of each part of the proposed method.",
            "21": "**Scalability**: The paper primarily focuses on the LLaMA-7B model.",
            "22": "It would be beneficial to see how the method scales with even larger models, as well as its performance on different hardware configurations beyond the GPUs mentioned.",
            "23": "**Real-World Applications**: Although the paper demonstrates the method's applicability in various scenarios, more real-world application examples and case studies would help illustrate the practical benefits and potential limitations of the approach in live systems.",
            "24": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of online language model interaction by introducing a compressed context memory system.",
            "25": "The innovative use of conditional LoRA and the parallelized training strategy are noteworthy contributions.",
            "26": "While there are areas for further exploration and improvement, the strengths of the paper, particularly its efficiency and practical applicability, make it a valuable addition to the literature."
        },
        "s2cM2Ff25b": {
            "0": "The exact method proposed is novel.",
            "1": "On the three presented datasets, it seems to work pretty well, judging from the small gap between compressed and uncompressed setups.",
            "2": "The problem this paper is trying to address is very important to the field, and I believe this work has significant contributions.",
            "3": "Finally, the paper is generally easy to read.",
            "4": "This method could be evaluated on more diverse datasets, such as those used for long context, by utilizing a sliding window for example.",
            "5": "From my perspective, this work has the potential to be applied more broadly beyond ICL, or dialog.",
            "6": "It'll also be nice to have the comparison with RMT and AutoCompressor in the main text, as they are very relevant for this problem."
        },
        "fR3Rsgvg3g": {
            "0": "- A interesting method to compress contexts in the few-shot learning setting.",
            "1": "- The results evaluated in the few-shot learning tasks show the effectiveness and superiority over the conventional approaches like RMT and Gist.",
            "2": "While this paper presents a seemingly promising solution to long contexts, I have significant concerns about several limitations.",
            "3": "Firstly, one of the main focuses of this paper is handling dynamic context for interaction.",
            "4": "Judging from its experimental design, it mainly conducts experiments with a fine-tuned LLM for few-shot learning scenarios, which are generally simpler tasks, all being multi-choice, or classification tasks.",
            "5": "The methods primarily compared in this paper are general context compression or long context handling methods (general and generative tasks).",
            "6": "This involves 3 issues.",
            "7": "The first issue: for simpler tasks like classification, compression is relatively easy (this is why previous models were easily distilled but GPT was not; I believe it's not because GPT is hard to distill, but because GPT is not for a specific task, but a general model.",
            "8": "The authors may better know what I said if reading the paper about information bottleneck: https://arxiv.org/pdf/1503.02406.pdf).",
            "9": "Therefore, it's not surprising that this method achieves good compression results (model size compression and context compression are similar, both reducing model capacity), but I believe it's hard for this paper's method and results to scale to general scenarios.",
            "10": "At least in this paper, I didn't see any general tests to prove its compression effect.",
            "11": "The second issue: for the few-shot learning setting, adding compression tokens after the demonstration makes sense, but for general scenarios, there is no definite boundary to limit compression tokens, making this method hard to generalize.",
            "12": "Even if a compression token can be added every K tokens, this approach would lead to inefficient training, as a large amount of sampling is required to ensure the model learns well for each position.",
            "13": "The third issue: the main setting of this paper is few-shot learning, and its main claim is online interaction.",
            "14": "But for few-shot learning, there doesn't seem to be any online interaction.",
            "15": "Users usually provide all demonstrations at once for the model to give an answer, and it's hard for me to imagine a setting where users incrementally provide demonstrations to the model.",
            "16": "Secondly, for dialogue tasks, this is a context compression for a specific task (more preciesely, for a specific dataset).",
            "17": "Similar to what I mentioned above, if it's for a specific task/benchmark, there is actually a lot of compression space, which has been discussed in many previous works, such as: https://arxiv.org/pdf/2301.12726.pdf.",
            "18": "For context compression of a specific task, even without this method, other methods should also achieve good compression results.",
            "19": "Thirdly, I am not entirely convinced by the results presented in the paper.",
            "20": "For example, in Table 15, the performance of RMT is almost the same as that of No context, which is hard to believe.",
            "21": "Moreover, Table 15 is an experiment conducted on OPT-2.7b.",
            "22": "The few-shot learning ability of the 2.7b OPT model, as far as I understand, should be very weak, and changes in the order of sample arrangement will significantly affect the results.",
            "23": "For a method like this paper's, which is similar to a recurrent method, it should be very easily influenced by later samples.",
            "24": "Unfortunately, I didn't see any discussion about this.",
            "25": "Some questions:\n1.",
            "26": "Figure 4 is a little confusing.",
            "27": "In (a), what do the blocks in different colors mean?",
            "28": "It seems that this work uses 1 single 80GB A100 for training.",
            "29": "Could the authors provide more details about the fine-tuning process?"
        },
        "720yspkERI": {
            "0": "The paper is overall sound.",
            "1": "The method design is concise, effective, and efficient.",
            "2": "Compared with retrieval-based method to re-compute the sentence embedding, the CCM can directly adopt the KV cache of introduced <COMP> token as the memory vector for one utterance and utilize them in further inference.",
            "3": "To engage the LLM to utilize such CCM, the parallel training and LoRA adapter are designed well for efficient adaptation.",
            "4": "The CCM is efficient in both training and inference.",
            "5": "Firstly, there is no need to re-compute the sentence embedding and instead caching the attention keys and values.",
            "6": "Secondly, the memory storage cost, the compression ratio, and the algorithmic complexity all demonstrate the efficiency of the method.",
            "7": "Thirdly, LoRA based adapter tuning brings a lot of efficiency in memory-engaged adaptation.",
            "8": "The evaluation is comprehensive and diverse.",
            "9": "Three important benchmarks, MetaICL, LaMP, and DailyDialog are selected for evaluation and MetaICL covers 26 tasks with high-diversity.",
            "10": "The CCM method is not that novel and has been explored well in some important early milestones before the creation of Transformer, i.e., Memory Networks, Fast Weights to Attend Recent Past.",
            "11": "The author should mention and discuss the relation with these methods.",
            "12": "Additionally, the Compress Transformer should be briefly introduced as it is not a universally known preliminary for readers.",
            "13": "In terms of the baselines, in the main tables, CCM is only compared with “no context\" and \"full context\" baselines on accuracy, which lacks sufficient comparison for demonstrating the effectiveness of the method.",
            "14": "As least, the retrieval-based method should be considered as an important baseline and it is now the universally-adopted method for memory compression.",
            "15": "If I understand the paper correctly, the token embedding produced by <COMP> token is the same as a sentence embedding.",
            "16": "The author can follow MemoryBank for adopting retrieval-based method to compress the long-context during online interactions.",
            "17": "Other baselines like LongMem and UnlimitedFormer can be also considered but not necessary."
        },
        "5tIU8XBC9Z": {
            "0": "- The problem of efficiently handling expanding contexts is highly relevant given the online nature of systems like ChatGPT.",
            "1": "The paper addresses an important open challenge.",
            "2": "- The method is flexible and broadly applicable to diverse online inference scenarios like multi-task learning, personalization and conversation.",
            "3": "- Empirical evaluations across three datasets substantiate the memory and computation advantages over baselines.",
            "4": "The method achieves slightly lower performance than the full context with 5x smaller memory.",
            "5": "- The parallel training strategy is effective in enabling large model optimization.",
            "6": "The conditional adapter improves compression capability.",
            "7": "- The complexity analysis clearly articulates the efficiency benefits, and ablation studies validate the design choices.",
            "8": "- The main limitation of the proposed compression framework is that it is task-specific.",
            "9": "The compression module must be trained for each task, which requires additional data, computation, and cannot generalize to new tasks.",
            "10": "This is a significant drawback in the context of foundation models which are trained on large datasets for general-purpose use.",
            "11": "- There is still a obvious gap in performance between the compressed and full context models.",
            "12": "The paper does not provide a clear explanation for this gap.",
            "13": "The authors should provide more analysis into why the compressed context is less effective.",
            "14": "- While the compression framework is novel, the proposed memory update functions are basic.",
            "15": "More sophisticated memory update mechanisms could further enhance capability.",
            "16": "- The comparison is primarily with simple adaptations of fixed-context compression methods.",
            "17": "A direct comparison to recurrent memory approaches, such as linear Transformers, would be more informative."
        }
    },
    "UX9lljSZqX": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel Unified Static and Dynamic Network (UniSDNet) inspired by human visual perception biology.",
            "1": "This approach is innovative and provides a fresh perspective on video grounding tasks by integrating static and dynamic modeling.",
            "2": "**Comprehensive Modeling**: The combination of a Static Semantic Supplement Network (S3Net) and a Dynamic Temporal Filtering Network (DTFNet) allows for a comprehensive understanding of video content.",
            "3": "The static module captures global interactions, while the dynamic module models fine-grained temporal context, which is a significant strength.",
            "4": "**State-of-the-Art Performance**: The proposed UniSDNet achieves state-of-the-art (SOTA) performance on multiple datasets for both Natural Language Video Grounding (NLVG) and Spoken Language Video Grounding (SLVG).",
            "5": "The results on ActivityNet Captions, TACoS, and Charades-STA datasets are particularly impressive.",
            "6": "**Efficiency**: The model demonstrates a good trade-off between performance and complexity.",
            "7": "It achieves faster inference speeds compared to strong multi-query benchmarks while maintaining high accuracy.",
            "8": "**New Datasets**: The authors contribute two new datasets (Charades-STA Speech and TACoS Speech) for the SLVG task, which is a valuable addition to the research community and will facilitate further research in this area.",
            "9": "**Detailed Experiments**: The paper provides extensive experimental results and ablation studies, which help in understanding the contributions of different components of the model.",
            "10": "The qualitative results further illustrate the effectiveness of the proposed approach.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Complexity of the Model**: While the model achieves a good trade-off between performance and complexity, the overall architecture is quite complex.",
            "13": "This complexity might make it challenging for practitioners to implement and extend the model without significant effort.",
            "14": "**Limited Discussion on Limitations**: The paper does not provide a detailed discussion on the limitations of the proposed approach.",
            "15": "Understanding the potential weaknesses or scenarios where the model might not perform well is crucial for a comprehensive evaluation.",
            "16": "**Generalization to Other Tasks**: Although the model performs well on NLVG and SLVG tasks, it is not clear how well the approach would generalize to other video-related tasks.",
            "17": "A discussion or preliminary experiments on other tasks could strengthen the paper.",
            "18": "**Dependency on Pre-trained Models**: The model relies on various pre-trained models for feature extraction (e.g., C3D, GloVe, Data2vec).",
            "19": "This dependency might limit the applicability of the approach in scenarios where such pre-trained models are not available or suitable.",
            "20": "**Scalability**: The scalability of the model to very large datasets or real-time applications is not thoroughly discussed.",
            "21": "Given the complexity of the model, it would be beneficial to understand its performance in such scenarios.",
            "22": "**Clarity in Presentation**: Some parts of the methodology, particularly the dynamic temporal filtering graph network, are quite dense and may be difficult for readers to follow.",
            "23": "Simplifying the explanations or providing more intuitive descriptions could improve the readability of the paper.",
            "24": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of video grounding by introducing a biologically inspired unified static and dynamic network.",
            "25": "The innovative approach, state-of-the-art performance, and contribution of new datasets are notable strengths.",
            "26": "However, the complexity of the model, limited discussion on limitations, and potential challenges in generalization and scalability are areas that could be addressed in future work.",
            "27": "Despite these weaknesses, the paper makes a valuable contribution to the research community and opens up new avenues for exploring video grounding tasks."
        },
        "1L3ZND615P": {
            "0": "+ The manuscript is overall well-organized and easy to follow.",
            "1": "+ The motivation behind the static and dynamic interactions based on the brain activity is clear and compelling.",
            "2": "+ The two-stage information aggregation methods are shown to be effective, where each component is appropriately designed.",
            "3": "+ The experimental results are very strong, clearly outperforming the existing approaches for both grounding tasks.",
            "4": "+ The collected spoken language grounding datasets will significantly benefit the research community.",
            "5": "The authors are encouraged to publish the code and data after the review process.",
            "6": "I did not find major weaknesses in this paper, yet summarize some questions about the method below.",
            "7": "- What is the motivation behind the implementation of Static Semantic Supplement Network?",
            "8": "I am wondering how the cross-modal interaction is performed through the MLP layers.",
            "9": "To my understanding, the shared weights across different modalities would extract some common features spanning different modalities.",
            "10": "Some analytical experiments on this would be beneficial.",
            "11": "Also, the architecture design seems similar to that of Transformer blocks except for the self-attention.",
            "12": "What happens if we use the conventional Transformer layers?",
            "13": "- The proposed architecture exploits multiple queries at once, to facilitate the model learning.",
            "14": "However, how the number of queries affects the performance is not diagnosed.",
            "15": "An ablative study on the number of queries regarding performance and cost would be helpful.",
            "16": "- In Figure 5, the effectiveness of the proposed filtering GCN is clearly verified.",
            "17": "On the other hand, there are some interesting tendency differences between NLVG and SLVG.",
            "18": "That is, the graph convolution layer itself is important, yet different layer modeling brings insignificant performance gaps on NLVG.",
            "19": "In contrast, on SLVG, the graph modeling brings negligible gains alone, but the proposed filtering mechanism shows substantial improvements.",
            "20": "How can one interpret this phenomenon?",
            "21": "If you have, please share some insights.",
            "22": "- The proposed method is well validated in the datasets with one-to-one matching between queries and moments.",
            "23": "How would it perform for one-to-many matching datasets, such as QVHighlights [1]?",
            "24": "[1] Lei et al.",
            "25": "\"QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries\", Neurips, 2021.",
            "26": "(Minor)\n\nThe manuscript contains some formatting errors due to the excessively small margins between captions and the main text.",
            "27": "They should be handled appropriately to raise the quality of the paper."
        },
        "pi86s7onrw": {
            "0": "Good performance on both NLVG and SLVG benchmarks.",
            "1": "It is nice to see an extension from NLVG to SLVG with a newly proposed benchmark.",
            "2": "The proposed method proves effective on both tasks.",
            "3": "Detailed implementation details and prediction analysis in the appendix.",
            "4": "The inspiration from human visual perception biology is not very motivating.",
            "5": "Specifically, it is hard to see why a MLP with residual connection is the way to achieve the “global broadcast communication” of the brain.",
            "6": "Either bridge the gap or Simply drop the bio-inspiration and go straight into the technical method.",
            "7": "When expanding a single gaussian kernel to multi-kernel Gaussian, it seems that only the bias $z_i$ is sweeping?",
            "8": "Have you tried different $\\gamma$?",
            "9": "Ablation in Fig 5 shows mostly similar results especially on NLVG, indicating that the designs in Dynamic Filter Graph actually do not quite matter."
        },
        "OFRQYAs5FY": {
            "0": "Most parts of the paper is well-written, clearly demonstrating the motivation,  methodology and experiments.",
            "1": "The methodology part is kind of easy to follow.",
            "2": "The idea is motivated from the human visual perception biology, which formulates an interesting story for this paper.",
            "3": "Extensive experiments successfully demonstrate the effectiveness of each proposed component of this work, which is good.",
            "4": "The visualization and figures are plus to show more intuitions.",
            "5": "The final results of this paper achieves the state-of-the-art from both efficiency and effectiveness perspectives.",
            "6": "The introduction reads like a related work.",
            "7": "It will be great to make more comparison between this work and previous work.",
            "8": "Answering what is wrong with previous works?",
            "9": "and where the efficiency and performance gain come from in this paper?",
            "10": "This paper introduces some new/confusing terminologies with their own definition, which hurts the reading experience.",
            "11": "For example, 'static semantic supplement network' and 'activity-silent mechanism' are actually the global context interaction.",
            "12": "Although the motivation of static and dynamic network is demonstrated, the justification of specific design is not enough.",
            "13": "For example, in the static network, transformer architecture or the recent S4[1] architecture can also be used as long-range filter.",
            "14": "Some ablation studies regarding either the performance or efficiency would be great to include.",
            "15": "In the dynamic network, not sure why use Gaussian filter on the distance (d_{ij}).",
            "16": "Can you provide more insights?",
            "17": "why not directly use the distance.",
            "18": "No notation for the 'FNN'.",
            "19": "Is this the feedforward network?",
            "20": "In the Figure 5, no notation/description for 'D'.",
            "21": "[1]  Efficiently modeling long sequences with structured state spaces.",
            "22": "ICLR 2021"
        },
        "q6tRFCz9qI": {
            "0": "The proposed Dynamic Temporal Filter Network captures more fine-grained context correlations between video clips based on a well-desgined graph network.",
            "1": "The proposed method achieves state-of-the-art performance on NLVG and SLVG tasks.",
            "2": "In this work, two new SLVG datasets are collected based on existing NLVG datasets.",
            "3": "Compared with previous multi-queried methods, the proposed UniSDNet has less model parameters and is more efficient according to the average inference time per query.",
            "4": "In ResMLP, visual features and multiple query features are concatenated and fed into the network, largely leveraging the information leakage between different queries (because the features incorporate more accurate textual information that describes the video content).",
            "5": "If each query is individually input into the network, would this method exhibit a significant performance degradation?",
            "6": "In the ablation study, individually employing the static network and DTFNet yields significant improvements compared to the baseline.",
            "7": "However, the combination of both modules does not exhibit a notably large improvement compared to using either single module.",
            "8": "Is there a specific explanation for this phenomenon?",
            "9": "The authors should provide more details about the baseline models."
        }
    },
    "50P9TDPEsh": {
        "llm_review": {
            "0": "**Novelty and Relevance**: The paper addresses a relatively unexplored area in the evaluation of large language models (LLMs) — their ability to critique responses.",
            "1": "This is a significant step forward as it not only assesses the models' performance but also their potential for self-improvement through self-critique.",
            "2": "**Comprehensive Benchmark**: The introduction of CRITIC BENCH, a benchmark comprising 3K high-quality natural language queries and corresponding model responses, is a valuable contribution.",
            "3": "The benchmark covers diverse tasks such as math problem-solving, code completion, and question answering, providing a broad evaluation framework.",
            "4": "**Detailed Analysis**: The paper provides a thorough analysis of the critique abilities of various LLMs, including insights into how these abilities scale with model size and the challenges associated with self-critique.",
            "5": "This detailed examination helps in understanding the limitations and potential of current LLMs.",
            "6": "**Scalability and Generalizability**: The data collection and selection methods are designed to be scalable and generalizable across different tasks and domains.",
            "7": "This ensures that the benchmark can be extended and adapted for future research.",
            "8": "**Practical Application**: The introduction of the self-check method demonstrates a practical application of critique abilities to improve model performance.",
            "9": "This method shows promise in enhancing the accuracy of LLMs on tasks like math word problems.",
            "10": "**Clear Metrics and Methodology**: The paper introduces clear metrics such as certainty score and correctness rate to evaluate the difficulty of queries and the models' confidence in their responses.",
            "11": "These metrics are well-defined and provide a solid basis for the analysis.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Scope of Tasks**: While the benchmark covers a range of tasks, it is still limited to three specific areas: math problem-solving, code completion, and question answering.",
            "14": "Including more diverse tasks such as natural language understanding, summarization, and translation could provide a more comprehensive evaluation of critique abilities.",
            "15": "**Self-Critique Challenges**: The paper acknowledges that self-critique is particularly challenging for LLMs, but it does not delve deeply into potential solutions or improvements.",
            "16": "More discussion on how to enhance self-critique capabilities would be beneficial.",
            "17": "**Evaluation of Intermediate Analysis**: The paper focuses on the final judgment accuracy of the critique without evaluating the correctness of the intermediate chain-of-thought analysis.",
            "18": "Assessing the intermediate steps could provide more insights into the models' reasoning processes and areas for improvement.",
            "19": "**Dependence on Model Size**: The findings suggest that critique ability is an emergent property that only appears in sufficiently large models.",
            "20": "This raises concerns about the accessibility and practicality of deploying such large models in real-world applications.",
            "21": "Exploring ways to enhance critique abilities in smaller models would be valuable.",
            "22": "**Tool Use for Code Evaluation**: The paper notes that evaluating the correctness of code snippets without execution is challenging.",
            "23": "While this is acknowledged, the paper does not propose concrete methods to address this issue, such as integrating code execution tools into the evaluation process.",
            "24": "**Limited Discussion on Ethical Implications**: The paper briefly mentions the potential risks of reduced human oversight with autonomous self-critique.",
            "25": "However, it does not provide a detailed discussion on the ethical implications and safeguards needed to mitigate these risks.",
            "26": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the field by introducing a new benchmark and providing a detailed analysis of the critique abilities of LLMs.",
            "27": "The strengths of the paper lie in its novelty, comprehensive benchmark, and practical application of critique abilities.",
            "28": "However, there are areas for improvement, such as expanding the scope of tasks, addressing self-critique challenges, and discussing ethical implications in more detail.",
            "29": "Despite these weaknesses, the paper provides a solid foundation for future research on the critique abilities of LLMs and their potential for self-improvement."
        },
        "NW5KsbCn3m": {
            "0": "- To explore the critique ability of LLMs is interesting, and timely at this point.",
            "1": "- This paper provides a standardized way to evaluate the critique ability of LLMs on diverse tasks, \n- The paper offers several noteworthy insights, such as the challenges associated with self-critique in LLMs.",
            "2": "These findings can guide future research and model development.",
            "3": "- The evaluation is not comprehensive.",
            "4": "While it claims to evaluate the critique ability, it only evaluates this across three tasks: math, code, and commonsense.",
            "5": "A broader range of tasks should be tested.",
            "6": "- The paper does not discuss potential biases.",
            "7": "Without discussing these biases, it's unclear how they might influence the evaluation results, which could affect the validity of the findings.",
            "8": "- Authors could offer a more in-depth analysis of the utility of self-critique.",
            "9": "Understanding why self-critique could be better and its influence on critique capabilities would strengthen the paper's arguments.",
            "10": "- The paper's presentation appears disjointed.",
            "11": "The content seems pieced together without careful review.",
            "12": "Consistency in terminology is essential for clarity.",
            "13": "- The paper does not define key terms like the policy model and critic model.",
            "14": "- Lack of related work.",
            "15": "- Despite introducing a benchmark, the authors do not release it, limiting its utility and reproducibility for the research community."
        },
        "8EcckOIEUs": {
            "0": "The paper addresses an important and under-explored aspect of LLMs, which is their ability to critique their own outputs.",
            "1": "This is a valuable contribution as it moves beyond traditional evaluation metrics and looks at a model's ability to self-improve.",
            "2": "The paper presents a clear definition of critique ability and distinguishes between critique and self-critique, which helps in setting the scope and understanding the objectives of the study.",
            "3": "The paper could benefit from a more detailed discussion on the limitations of the current approach, particularly regarding the scalability of the self-check method and its applicability to real-world scenarios [1,2,3].",
            "4": "The study is limited to a few tasks and datasets.",
            "5": "Expanding the benchmark to include more diverse tasks and domains would make the findings more generalizable.",
            "6": "The evaluation of self-critique abilities shows that models struggle with certain tasks, but the paper does not delve deeply into why this is the case or propose potential solutions to improve self-critique performance.",
            "7": "The paper does not address the potential ethical implications of models that can self-critique and self-improve, especially in terms of reduced human oversight.",
            "8": "References \n\n[1] Madaan, Aman, et al.",
            "9": "\"Self-refine: Iterative refinement with self-feedback.\"",
            "10": "arXiv preprint arXiv:2303.17651 (2023).",
            "11": "[2] Krishna, Satyapriya.",
            "12": "“On the Intersection of Self-Correction and Trust in Language Models.” (2023).",
            "13": "[3] Huang, Jie, et al.",
            "14": "\"Large language models cannot self-correct reasoning yet.\"",
            "15": "arXiv preprint arXiv:2310.01798 (2023)."
        },
        "86snFxKYr8": {
            "0": "The paper is well-written and easy to follow.",
            "1": "The authors are very clear about all details in the data collection process and provided good motivation for the various design choices.",
            "2": "The evaluation is thorough and covers a wide range of models.",
            "3": "The proposed new heuristic is not particularly novel, but achieves solid improvement on the new benchmark.",
            "4": "A critique in this paper is defined as a language model assessment of another language model output on some underlying task.",
            "5": "A good critique model should be effective at identifying flaws in language model outputs.",
            "6": "The challenging examples to the task of critique are nuanced flaws, which would also require a detailed explanation by the critique model.",
            "7": "But the benchmark proposed by this paper use a simplistic quantitative metric that reduces the quality of a critique to a binary decision, which assumes that it’s appropriate to use a binary metric for the underlying task as well.",
            "8": "The benchmark offers very limited granularity.",
            "9": "Using a granular quantitative measure means that the qualitative questions that the benchmark can answer are also limited.",
            "10": "Outside of developing and evaluating self-refinement heuristics like the one proposed by the authors, the benchmark provides limited information for other uses of model-generated critique, such as informing human oversight.",
            "11": "Since the benchmark requires tasks with well-defined, fully-automated metrics for the underlying task, the problem of developing self-refinement critiques does not in fact depend on such a benchmark: even if the model critique doesn’t make sense to a human, as long as it improves subsequent prediction accuracy, it’s a good critique."
        }
    },
    "4qFIkOhq24": {
        "llm_review": {
            "0": "**Theoretical Framework**: The introduction of the Behavior Expectation Bounds (BEB) framework is a significant contribution.",
            "1": "It provides a formal and probabilistic approach to understanding the alignment of large language models (LLMs).",
            "2": "This framework could serve as a foundation for future research in AI alignment.",
            "3": "**Comprehensive Analysis**: The paper thoroughly investigates the limitations of current alignment techniques, particularly focusing on adversarial prompting attacks.",
            "4": "The theoretical results are well-supported by empirical evidence, which strengthens the validity of the claims.",
            "5": "**Novel Insights**: The paper offers novel insights into the vulnerabilities of LLMs, especially regarding the ease with which adversarial prompts can misalign even well-aligned models.",
            "6": "This is a critical area of concern for AI safety and ethics.",
            "7": "**Empirical Validation**: The empirical results, particularly those involving the LLaMA LLM family, provide concrete evidence supporting the theoretical claims.",
            "8": "The experiments are well-designed and demonstrate the practical implications of the theoretical findings.",
            "9": "**Clarity and Structure**: The paper is well-structured and clearly written.",
            "10": "The definitions, theorems, and proofs are presented in a logical and understandable manner, making the complex theoretical concepts accessible.",
            "11": "**Relevance and Timeliness**: Given the increasing reliance on LLMs in various applications, the paper addresses a highly relevant and timely issue.",
            "12": "The discussion on the limitations of reinforcement learning from human feedback (RLHF) and the need for external alignment methods is particularly pertinent.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Assumptions and Generalizability**: The BEB framework relies on several assumptions, such as the decomposability of LLM distributions into well-behaved and ill-behaved components.",
            "15": "While these assumptions are intuitive and supported by some empirical evidence, they may not fully capture the complexity of real-world LLM behaviors.",
            "16": "Further validation and refinement of these assumptions are needed.",
            "17": "**Scope of Empirical Validation**: The empirical validation is primarily conducted on the LLaMA LLM family.",
            "18": "While this provides valuable insights, it would be beneficial to see similar experiments conducted on a wider range of LLMs, including those from different architectures and training paradigms.",
            "19": "**Behavior Scoring Complexity**: The paper assumes ground truth behavior scores for sentences, which may oversimplify the complexity of behavior evaluation in practice.",
            "20": "Real-world behavior scoring can be more nuanced and context-dependent, and this aspect is not fully addressed in the paper.",
            "21": "**Practical Implications**: While the theoretical results are compelling, the paper could benefit from a more detailed discussion on the practical implications of these findings.",
            "22": "For instance, how can the insights from the BEB framework be translated into actionable strategies for improving LLM alignment in real-world applications?",
            "23": "**Future Work and Open Questions**: The paper leaves several important questions open for future research, such as the deeper investigation of superposition and decomposability in LLM distributions.",
            "24": "While this is a natural aspect of ongoing research, a more detailed roadmap for addressing these open questions would enhance the paper's contribution.",
            "25": "**External Alignment Methods**: The paper briefly mentions the importance of external alignment methods, such as filters and controllers, but does not delve deeply into how these methods can be effectively implemented and integrated with current LLMs.",
            "26": "A more thorough exploration of these alternatives would be valuable.",
            "27": "#### Conclusion\n\nOverall, the paper \"Fundamental Limitations of Alignment in Large Language Models\" makes a significant contribution to the field of AI alignment.",
            "28": "The introduction of the BEB framework and the comprehensive analysis of alignment limitations provide valuable insights.",
            "29": "However, the paper could benefit from further validation of its assumptions, a broader scope of empirical validation, and a more detailed discussion on practical implications and future research directions.",
            "30": "Despite these weaknesses, the paper is a strong and timely contribution to the ongoing discourse on AI safety and alignment."
        },
        "C4g9zxTNlK": {
            "0": "This paper introduces and examines a framework for the theoretical study of LLM alignment.",
            "1": "While acknowledging potential limitations within the framework and its underlying assumptions, it presents an original perspective for the theoretical analysis of a complex empirical phenomenon.",
            "2": "The writing of this paper is clear and easy to follow, with most definitions and assumptions followed by high-level intuition.",
            "3": "My main comments are focused on three topics:\n\nThe mixture model seems to be a very strong assumption on what the models entail after pretraining.",
            "4": "Details are discussed in Questions.",
            "5": "Although empirical values for problem parameters are provided in the experiments, it is still hard to comprehend each assumption and their overall importance to the derived results.",
            "6": "Details are discussed in Questions.",
            "7": "Some experiment details are lacking.",
            "8": "See below."
        },
        "jT3gYLYaFc": {
            "0": "The paper attempts to offer a much needed theoretical base to the problem of aligning of LLMs.",
            "1": "The paper has a solid theoretical analysis that shows that under certain conditions, adversarial prompting can result in very low probability behaviours being exhibited with high probability.",
            "2": "The authors study these behaviours also in real-world models and show that adversarial prefixing can indeed be used to misalign a model.",
            "3": "The definition of γ-prompt-misalignment is extremely conservative: The existence of a single prompt resulting in misaligned behaviour is sufficient to label the whole model misaligned.",
            "4": "This makes this is a binary condition and it is not that surprising that there exists at least one prompt that will result in an undesirable behaviour.",
            "5": "However, this is not a realistic setting and in practice more nuanced measures of “misalignment” are needed.",
            "6": "The definition for β-distinguishability is very strict and, contrary to the claims in the paper, it is not clear to me whether $\\mathbb P_{-}$ and $\\mathbb P_{+}$ would be at all distinguishable in practice.",
            "7": "That is, because the definition requires that bound (5) holds *for any prefix* $s_0$.",
            "8": "However, while the components can be polar opposite in one sense, e.g.",
            "9": "“agreeableness”, the models are likely similar in many other ways.",
            "10": "E.g., “Which is the capital of France” is probably going to be completed with “Paris”, by both $\\mathbb P_{-}$ and $\\mathbb P_{+}$.",
            "11": "If that’s the case, then $\\beta=0$ and that’d invalidate the paper’s results.",
            "12": "The same issue seems to appear in the experimental estimation of $\\beta$.",
            "13": "It seem that the authors are not actually estimating $\\beta$.",
            "14": "The KL divergence is computed only for prefixes sampled from the unconditional negative distribution $\\mathbb P_{-}$ which of course has a bias.",
            "15": "This results in over-approximating $\\beta$, possibly by a lot.",
            "16": "However, if one considers all sentences $s_0$, there would be many for which the completion would be the same (e.g.",
            "17": "the Paris example), hence $\\beta$ would be 0.",
            "18": "Overall, Section 2.2 which is critical for understanding the claims of the paper is not clearly presented.",
            "19": "I would strongly recommend the authors to add examples of, e.g.",
            "20": "β-distinguishable and non-β-distinguishable distributions, as well as α,β,γ-negatively-distinguishable and non-α,β,γ-negatively-distinguishable factorizations.",
            "21": "The paper also fails to discuss the limitations of the analysis and the conditions under which it holds.",
            "22": "While the plausibility of the factorisation of the distribution is mentioned, I am missing the discussion on the other technical assumptions, as mentioned above."
        },
        "sgJMkMj5LQ": {
            "0": "Originality: The Behavior Expectation Bounds (BEB) framework offers a novel theoretical perspective on the alignment issues of LLMs.",
            "1": "Quality: The paper effectively combines theoretical insights with empirical results to support its claims.",
            "2": "The formalisms and theorems provide a solid foundation for the study.",
            "3": "Clarity: The paper is well-structured and the distinction between theoretical and empirical sections ensures the reader can follow the progression of ideas.",
            "4": "Significance: The problem of LLM alignment is pressing, and the paper's findings can influence future practices and methodologies in training and deploying these models.",
            "5": "Assumption Limitations: The framework is based on some strong assumptions, such as the decomposition of LLMs into distinct behavioral components.",
            "6": "This could be overly simplified or not universally applicable.",
            "7": "Overemphasis on Theoretical Aspects: While the theorems and formalizations are valuable, the balance between theoretical and practical aspects could be adjusted to appeal to a broader audience."
        },
        "m3GEG92kff": {
            "0": "+ The theory is presented clearly: the assumptions are presented well and the theorems explained nicely.",
            "1": "+ The potential impact of the work is quite large: this work presents fundamental limits on the ability of models to be correctly aligned.",
            "2": "If current trends continue and large models continue to increase in capability, this points towards important implications of an inability to avoid potentially very hazardous misalignment.",
            "3": "+ Experimental results go some way towards backing up the theoretical claims.",
            "4": "+ The analysis of the conversational and aligning prompt cases are interesting, and appropriate given the focus on conversational agents in the previous year.",
            "5": "The result that conversations can require longer adversarial input is counter-intuitive at first, but makes sense upon reading the proof and analysis.",
            "6": "+ The fact that all the results are asymptotic seems to be a limitation to the results.",
            "7": "Of course, developing finite-sample bounds is likely much harder than asymptotic results.",
            "8": "In principle, the results could be vacuous if the constants were large enough.",
            "9": "Given recent work on finding adversarial prompt injections, I don't think the results are actually vacuous, but I think a brief discussion of this is warranted in the paper.",
            "10": "+ The relevance of the experimental results is debatable, as investigating the fine-tuned models is not the same as investigating the different modes $P_-$, $P_+$.",
            "11": "I understand that direct examination of the modes is perhaps impossible, but I would like to see more discussion of the feasibility of this.",
            "12": "+ There is no discussion about the computational feasibility of finding adversarial prompts.",
            "13": "In light of the combinatorially large search space of all possible contexts of length $n$ of size $V^n$ for vocab size $n$, the main result is less impressive unless it is computationally tractable to find these adversarial injections.",
            "14": "Again, I think a discussion of recent injection techniques should address this concern in the paper."
        }
    },
    "3EWTEy9MTM": {
        "llm_review": {
            "0": "**Theoretical Contribution**: The paper provides a robust theoretical framework to understand the power of Chain of Thought (CoT) in enhancing the expressiveness of transformer models.",
            "1": "By leveraging concepts from circuit complexity, the authors offer a clear and rigorous explanation of how CoT enables transformers to perform inherently serial computations, which is a significant advancement in the field.",
            "2": "**Novel Insights**: The paper introduces the concept of CoT and demonstrates its impact on the expressiveness of transformers.",
            "3": "The distinction between the capabilities of transformers with and without CoT is well-articulated, providing new insights into the limitations and potential of these models.",
            "4": "**Empirical Validation**: The authors complement their theoretical findings with empirical experiments on various tasks, including modular addition, permutation composition, iterated squaring, and circuit value problems.",
            "5": "These experiments convincingly show that CoT significantly improves the performance of transformers on tasks that require serial computations.",
            "6": "**Comprehensive Analysis**: The paper covers a wide range of related work and situates its contributions within the broader context of transformer research.",
            "7": "The discussion on the limitations of previous works and how CoT addresses these limitations is thorough and well-argued.",
            "8": "**Clarity and Rigor**: The paper is well-written, with clear definitions, theorems, and proofs.",
            "9": "The use of formal language and detailed explanations ensures that the theoretical contributions are accessible to readers with a background in computational complexity and machine learning.",
            "10": "#### Weaknesses\n\n1.",
            "11": "**Complexity and Accessibility**: While the theoretical contributions are significant, the paper's complexity might make it less accessible to practitioners who are not well-versed in circuit complexity and formal methods.",
            "12": "Simplifying some of the technical details or providing more intuitive explanations could help broaden the paper's audience.",
            "13": "**Empirical Scope**: Although the empirical validation is strong, it is limited to synthetic tasks.",
            "14": "It would be beneficial to see how CoT performs on more diverse and real-world tasks, such as natural language processing or other practical applications, to better understand its practical implications.",
            "15": "**Non-Uniformity Assumption**: The paper assumes non-uniformity in its theoretical framework, which might not align with practical scenarios where a single model is expected to handle varying input lengths.",
            "16": "Addressing this limitation or discussing potential solutions for uniform settings could strengthen the paper's applicability.",
            "17": "**Scalability Concerns**: The paper does not thoroughly address the scalability of CoT in terms of computational resources and training time.",
            "18": "Given the additional steps involved in generating intermediate tokens, it would be useful to discuss the trade-offs between improved expressiveness and increased computational cost.",
            "19": "**Comparison with Other Methods**: While the paper focuses on CoT, it would be valuable to compare its effectiveness with other recent advancements in transformer architectures, such as memory-augmented models or models with recurrence.",
            "20": "This comparison could provide a more comprehensive understanding of where CoT stands relative to other approaches.",
            "21": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the understanding of transformer models and their capabilities.",
            "22": "The introduction of Chain of Thought as a mechanism to enhance the expressiveness of transformers is both novel and impactful.",
            "23": "The theoretical insights are well-supported by empirical evidence, making a strong case for the potential of CoT in solving inherently serial problems.",
            "24": "Addressing the identified weaknesses could further enhance the paper's impact and applicability in both theoretical and practical domains."
        },
        "ofubcIbvxL": {
            "0": "The first contribution (power of hard attention) may not be the most realistic model of practical transformers (see Weaknesses), but it is still potentially valuable for filling out the theory of transformers of different types.",
            "1": "The second contribution showing log-precision transformers with rounding are still in TC0 is quite interesting and solves an technical problem unresolved in prior work.",
            "2": "Even though the result is not fully general (requires zero exponent), the progress here is quite exciting and perhaps could be extended in future work.",
            "3": "I would like some of the assumptions for the P/poly result to be better discussed (see Weaknesses), but it is a valid and potentially useful result for formalizing the power of CoT.",
            "4": "The paper is generally well-written and organized, and I appreciate the inclusion of a neat empirical study.",
            "5": "## Limitations of Constant Precision\n\nConstant precision is not necessarily a realistic setting, since it means transformers cannot attend based on positional encodings or compute uniform attention (which require $\\log n$ bits).",
            "6": "In the practical regime, transformers have enough precision to express uniform attention over their input length and can use uniform attention to recognize majority ([Merrill et al., 2021](https://aclanthology.org/2022.tacl-1.49/)), which is outside your upper bound of AC0.",
            "7": "Presumably, if we wanted to apply transformers on very long input lengths, we would scale up the precision of attention logarithmically so that uniform attention and positional embeddings would remain expressible.",
            "8": "For this reason, [Merrill & Sabharwal (2023)](https://neurips.cc/virtual/2023/poster/70153) propose studying the log-precision model instead of the constant precision one.",
            "9": "To put it another way, let's say you ran the same experimental setup as Figure 3 but with Majority instead of Iterated Squaring.",
            "10": "If transformers are in AC0, we'd expect a similar qualitative pattern where models without CoT struggle without sufficient depth but models with CoT can succeed.",
            "11": "But I think that's not what you would find: instead, even models of one layer could do well using a single uniform attention head.",
            "12": "To be clear, even though I think log-precision is more realistic, I think it is still potentially interesting to analyze the constant-precision case to fill out the overall theory and understand the value of log precision.",
            "13": "However, the authors should discuss the differences between constant-precision and log-precision and specifically mention or respond to the argument for log-precision from [Merrill & Sabharwal (2023)](https://neurips.cc/virtual/2023/poster/70153).",
            "14": "It could also be helpful to run the experiment I described above with Majority and potentially include the results in the appendix.",
            "15": "## Nonuniform Embeddings\n\nThe paper characterizes transformers with T steps as P/poly.",
            "16": "There is something weird about this result, in that it characterizes transformers by a nonuniform complexity class that contains undecidable problems (e.g., the unary encoding of the halting problem)!",
            "17": "In contrast, we know that transformers cannot compute undecidable problems since they can be implemented on standard computers.",
            "18": "This disconnect comes from the fact that the embeddings are assumed to be **nonuniform**: i.e., they can be any sequence of $O(\\log n)$ bits on inputs of size $n$.",
            "19": "This enables the embeddings to be able to encode advice for solving undecidable problems, which standard positional transformers cannot do because they are computable.",
            "20": "This assumption of nonuniform embeddings should be better highlighted in Section 3.4: right now it's not even visible in the theorem statement.",
            "21": "It would also be good to add some discussion of the assumption in the introduction or other high-level parts of the paper so readers don't miss it."
        },
        "2WoJs9OwlX": {
            "0": "The main strengths of this paper are:\n\n1. its rigorousness in making clear, concise statements of its findings (except for one important \"detail\" discuss under weaknesses);\n\n2. a formal characterization of the power of transformers with CoT, for which results have come out only very, very recently (after the ICLR submission deadline);\n\n3. carefully crafted arguments and proofs around rounding of numbers when performing addition of $n$ numbers, a key step used in multiple prior papers unless less realistic assumptions; and\n\n4. empirical evaluation to support the theory, which is often missing in similar theoretical characterizations of transformer variants in the past.",
            "1": "I have not read the proofs in detail (esp.",
            "2": "the material in the appendix), but the results and approach intuitively appear plausible.",
            "3": "I really have only one, albeit big, concern about the paper, namely **the formal model of transformers being studied is different not only from all prior theoretical works but also from practical use of transformers**.",
            "4": "This is made worse by the lack of a discussion of this difference.",
            "5": "Consequently, while the results appear to tighten prior upper bounds and provide novel lower bounds, they really are applicable to a different model.",
            "6": "Specifically, the authors assume a model of transformers that is **non-uniform** (in the sense it is used in circuit complexity), namely, for each $n$, there is a **different** transformer.",
            "7": "As they state in Defn 3.4, \"for every ... $n$, there is a $L$-layer ... transformer\".",
            "8": "This means that one needs a *family* of transformers, one for each input size $n$, to solve a given problem, and the weights of the transformer for input length $n$ may have nothing to do with that of the transformer for input lengths $n+1$.",
            "9": "To specify such a family, one thus needs to specify an infinite family of unrelated weights, which obviously is unrealistic.",
            "10": "In contrast, in practice, one trains a transformer on inputs of certain lengths, freezes those weights, and then uses the same frozen-weights transformers for inputs (and chains-of-thought) of arbitrary lengths.",
            "11": "In fact, inspired by this, the theoretical research on the representation power of transformers in the past 2-3 years has gone in the opposite direction---from non-uniform upper bounds, to tighter and tigher uniform bounds (e.g., log-space uniform, log-time uniform, FO-uniform, etc.).",
            "12": "Importantly, all along, the model of transformers used was uniform.",
            "13": "The *non-uniformity* of the model assumed here has other, well-known undesirable consequences also seen in non-uniform circuits---it allows transformers to **trivially solve certain undecidable problems**, namely any undecidable unary problem, over the alphabet $\\{1\\}$.",
            "14": "E.g., it can solve the halting problem expressed in unary, just like all non-uniform circuit classes (including P/poly) can.",
            "15": "Besides being unrealistic, this raises a question about the meaningfulness of the technique used to prove the main result (Theorem 3.3).",
            "16": "In order to simulate a circuit of size $T(n)$, one must somehow *embed* the circuit into the transformer as the transformer needs to know what circuit to compute.",
            "17": "As seen from the proof of it in the appendix, the authors have a creative solution: they put the description of the circuit in the *positional embedding* of the transformer.",
            "18": "While interesting and unique, this has two undesirable implications:\n\n1.",
            "19": "The positional embedding for inputs of length $n$ is allowed to be arbitrarily different from the positional embeding for inputs of length, say, $n+1$ (because there is no uniformity constraint between the circuits for the two respective sizes, $n$ and $n+1$), which departs heavily from practice.",
            "20": "It means that the proposed construction must include some **uncomputable / undecidable positional embeddings**!",
            "21": "To see this, consider any undecidable unary problem $P$ in P/poly.",
            "22": "The typical polysize circuit construction for $P$ is to have, for each $n$, a trivial circuit $C_n$ that outputs a $1$ if and only if $1^n$ is in that undecidable language (e.g., $1^n$ is the unary encoding of the $n$-th Halting problem).",
            "23": "Thus, by the authors' construction, there is a trivial transformer that decides the same language---and its embedding of the first position computes membership in that undecidable language!",
            "24": "In other words, the embedding itself in not computable by any reasonable model of computation.",
            "25": "To summarize this, while the construction is correct to my understanding, it is for a formal model that departs from practice and assumes a lot of power (e.g., that of having access to potentially uncomputable embeddings).",
            "26": "At the very least, these limitations and their implications should be clearly discussed in the paper.",
            "27": "**Minor points**\n\n* In the abstract, the statement \"with $T$ steps of CoT, constant-depth transformers ... can solve ...\" should be qualified with T being at most a polynomial in $n$.",
            "28": "* In the 2nd last line of page one, I think you mean \"encoder-only\" rather than \"decoder-only\"; or single-step decoder.",
            "29": "* In line 3 of section 2, do you mean $\\phi(bin_k(x)) = x$ rather than $\\ldots = 0$?",
            "30": "* In the 2nd paragraph of section 2, where is the input length limited to $n_\\max$?",
            "31": "* page 3, two lines before defn 3: \"over more two\" => \"over more than two\""
        },
        "BHBs4nGhdi": {
            "0": "Overall i felt it was a quite well written paper and the case was made well.",
            "1": "The paper formalizes the problem precisely, which is not only critical for answering the question considered in the paper, but is also of independent interest.",
            "2": "I think the parameterisation of CoT  in terms of embedding size and depth, is novel and interesting.",
            "3": "There are several well chosen examples that made comprehension easier.",
            "4": "The empirical evaluation is fairly convincing but it does not really reveal anything new insights not already covered by the proofs.",
            "5": "Minor  errors:\nSection 1: poewrful -> powerful"
        },
        "dCCNzcot8V": {
            "0": "To my mind, there were three main strengths to the paper.",
            "1": "Its theoretical approach: I greatly appreciated the theoretical bend to the paper.",
            "2": "Phrasing things in terms of problem classes that a particular model can solve is the kind of I'd like to see more of.",
            "3": "Interesting examples: The problem classes they used were interesting, novel, and nicely targeted to the theoretical results\n\n3.",
            "4": "A detailed and rigorous approach to a intuitive idea.",
            "5": "The supporting information section that includes very detailed proofs and ample details for the interested reader.",
            "6": "In terms of the primary dimensions, this paper was quite strong in two of them: Originality and Significance\n\nOriginality: As previously noted, phrasing the CoT problem in terms of specific problem classes for particular families of models is a fantastic idea, something we don't see nearly enough of in the field.",
            "7": "I also thought the experiments performed were quite original - finding problems in particular problem classes takes a great deal of effort and creativity, the problems they used were not brand new, but were new to the area of CoT and transformer models\n\nQuality: The experiments performed were well chosen and did a good job supporting the theory\n\nSignificance: This paper does make a significant claim and provides good evidence to back it up.",
            "8": "Understanding the power of transformer models, where they face challenges, and what problem classes they excel at, is a very significant result and exactly the kind of result that should be featured at an ICLR tier conference.",
            "9": "For all the strengths of this paper, there were a number of weaknesses as well.",
            "10": "The performance evaluation - data was trained and evaluated using \"freshly sampled synthetic data\".",
            "11": "I think this is problematic at several levels.",
            "12": "First, without a carefully though out test/train split, the model is at risk of overfitting.",
            "13": "I think that is fine here - because overfitting is still telling you information about the expressive power of your model class, however this goes against the grain of standard model training practice, and at least deserves comment.",
            "14": "Second, the problems used were all discrete problems.",
            "15": "Discrete problems are great test beds for theoretical arguments - but in most of these examples, the problem space is finite for a fixed size, and randomly sampling from a discrete space gives the artificial impressing of having more data than is actually available.",
            "16": "Finally, how you sample from these discrete spaces seems like it would make a big difference on the model performance\n\nThe model discussion was also significantly lacking.",
            "17": "At the very least, a discussion of what the class of transformer models looks like and how you are bounding it belong in the main body of the text, not banished to appendix G on the final page of the supporting information.",
            "18": "The details that were present, were in the form of Algorithm 1, defining an implementation, but not the class itsself.",
            "19": "That made it difficult to tell what was the result of the model class and what was the result of the training process.",
            "20": "Finally, this paper would be stronger if the presentation was as crisp and focused as the data classes and results.",
            "21": "In particular, there was an odd mix of too much detail, too little detail, and superfluous detail.",
            "22": "For example, Definitions 3.1 and 3.2 were primarily used in appendix C so their presentation on page 3 distracted from the main arguments of the paper."
        },
        "jTocaS1d05": {
            "0": "(+) Important result regarding the expressiveness of transformers.",
            "1": "The finding seems very obvious intuitively, but the novelty seems to be in a rigorous proof.",
            "2": "(+) Realistic modeling of finite precision computations\n\n(+) The theoretical finding is matched with empirical results on four tasks in arithmetic.",
            "3": "(1) I'm not sure I grasp the significance of the finding.",
            "4": "It seems obvious that any model can perform serial computations if it is allowed to store intermediate results somewhere (in this case, in the output sequence, which can be written to and subsequently access with self-attention).",
            "5": "Therefore, producing such intermediate output seems as expressive as a chain of multiple instances of the model (this does not even specific to transformers).",
            "6": "Is this intuition misguided?",
            "7": "Perhaps the important finding is rather that a transformer *without* CoT could not solve serial problems?",
            "8": "(but this was covered in previous work).",
            "9": "Note that this topic is not exactly my area of expertise (I was called as an emergency reviewer), but this should be addressed since it is likely something that other readers will wonder about.",
            "10": "---------------------\n\n(2) Presentation could be improved.",
            "11": "These are details though that are easy to fix.",
            "12": "The abstract could be clearer about what form of CoT is studied in the paper (whether it's about the training data, the test phase, the prompting, ...).",
            "13": "The very first sentence seemed clear in retrospect to me, not on the first read.",
            "14": "So it's probably good to be even more explicit and specify that it's just about instructing the model to generate intermediate steps.",
            "15": "There are some informal shortcuts in the technical language that should be fixed.",
            "16": "Examples:\n\"transformers with polynomial intermediate steps\" -> \"with a number of intermediate steps polynomial in ...\"\n\"transformers with linear intermediate steps\"\n\"poly width and log precision\"\netc.",
            "17": "Typos: poewrful, circuit valuation, \"because every regular languages\" (should be singular)\n\nSec.",
            "18": "4\"we find that cotis always\" (missing space, also in multiple other places in this section)\n\nWeird grammar in the abstract: \"previous works have ...\"\nI suppose you mean \"previous works show that ...\""
        },
        "GbA6LcPI3e": {
            "0": "- Analyzing the effect of the presence or absence of CoT on model expressivity from a theoretical perspective.",
            "1": "- Proposing a tighter upper bound for constant depth transformers' expressive power.",
            "2": "- The motivation of this paper is clear.",
            "3": "- Symbols are not clearly described.",
            "4": "The authors  don't make clear instructions when using symbols like NC^1, AC^0, etc.",
            "5": "It is recommended that these symbols be explained when they are first mentioned.",
            "6": "- The conclusions of Theorem 3.8 and Theorem 3.9 presented in the paper seem to be contradictory.",
            "7": "Theorem 3.8 indicates that enlarging the embedding size to poly(n) doesn't improve the model's expressiveness of T(n) = poly(n) step CoT.",
            "8": "On the other hand, Theorem 3.9 demonstrates that broadening the embedding width strengthens the model's power for any particular polynomial T(n) = n^k step CoT.",
            "9": "The conclusions of these two theorems are confusing and require further clarification."
        }
    },
    "ARP0xaE6od": {
        "llm_review": {
            "0": "**Relevance and Timeliness**: The paper addresses a highly relevant and timely issue in the field of content moderation, leveraging the advancements in Large Language Models (LLMs).",
            "1": "Given the increasing volume of online content and the necessity for effective moderation, this research is of significant practical importance.",
            "2": "**Comprehensive Approach**: The authors provide a detailed exploration of the process of fine-tuning LLMs for content moderation.",
            "3": "They cover various aspects, including data engineering, the incorporation of reasoning processes, and the use of weak supervision.",
            "4": "This comprehensive approach is valuable for researchers and practitioners looking to implement similar systems.",
            "5": "**Empirical Validation**: The paper includes extensive experiments and empirical validation, comparing different fine-tuning strategies and models.",
            "6": "The use of both in-distribution and out-of-distribution data for evaluation adds robustness to the findings.",
            "7": "**Weak Supervision**: The introduction of weak supervision to improve the quality of the training data is a notable contribution.",
            "8": "This approach helps in filtering out low-quality samples and enhancing the performance of the fine-tuned models.",
            "9": "**Detailed Analysis**: The authors provide a thorough analysis of the results, discussing the impact of different design choices on the performance of the models.",
            "10": "This level of detail is beneficial for understanding the nuances of fine-tuning LLMs for specific tasks.",
            "11": "**Practical Insights**: The paper offers practical insights and recommendations for fine-tuning LLMs in domain-specific scenarios.",
            "12": "This is particularly useful for organizations looking to deploy custom content moderation solutions.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Clarity and Readability**: The paper is dense and contains a lot of technical details, which might be overwhelming for some readers.",
            "15": "The clarity and readability could be improved by breaking down complex sections and providing more intuitive explanations.",
            "16": "**Limited Scope of Evaluation**: While the paper evaluates the models on Chinese content moderation tasks, it would be beneficial to see how the proposed methods generalize to other languages and contexts.",
            "17": "This would enhance the generalizability of the findings.",
            "18": "**Comparison with Baselines**: The paper could benefit from a more detailed comparison with existing baseline methods for content moderation.",
            "19": "While the authors compare their models with GPT-3.5, additional comparisons with other state-of-the-art content moderation systems would provide a clearer picture of the improvements achieved.",
            "20": "**Ethical Considerations**: The paper briefly mentions the potential for harmful content but does not delve deeply into the ethical considerations of using LLMs for content moderation.",
            "21": "A more thorough discussion on the ethical implications and potential biases in the models would be valuable.",
            "22": "**Scalability and Deployment**: The paper focuses on the fine-tuning process but does not provide much detail on the scalability and deployment of the fine-tuned models in real-world scenarios.",
            "23": "Addressing these aspects would make the research more applicable to practical implementations.",
            "24": "**Limited Discussion on Hallucinations**: While the paper mentions hallucinations in LLMs, it does not provide a detailed analysis of how these hallucinations impact the content moderation task and what specific measures can be taken to mitigate them.",
            "25": "#### Conclusion:\n\nOverall, the paper makes a significant contribution to the field of content moderation by exploring the adaptation of LLMs through fine-tuning and weak supervision.",
            "26": "The comprehensive approach and detailed analysis are commendable, though there are areas for improvement in terms of clarity, generalizability, and practical deployment considerations.",
            "27": "The research provides valuable insights and practical recommendations that can guide future work in this domain."
        },
        "9hOTxLJk2E": {
            "0": "clear implementation details - the first point I like the paper most is it provide details to the extent that it is easy for others to reimplement their method.",
            "1": "The prompts, the process, and fine tuning methods, make it a hands on guide to fine tuning domain specific task using llm.",
            "2": "improvement over baseline model - from the experiment I find the purposed reasoning prompt method has significantly improved the two baseline llm's performance.",
            "3": "this is useful in that not all scenarios is suitable for cloud llm providers, and it is reasonable self trained or open source models could perform bad on a domain specific task.",
            "4": "in general I think the authors of the papers are resolving a real world issue\n\nmiscs - ablation studies, discussions, etc are meaningful 1. from the comparison the purposed methods do not seem to beat gpt 3.5 model by a large margin, and in OOD test CInsult dataset seems the improvement is trivial.",
            "5": "although this does not negates the usefulness of the reasoning, it seems to me cloud llm provider still has advantage in most of the tasks\n\n2. some of the tables/figures are not annotated/referenced in the paper like table 2, which is kind of confusing at first glimpse (the first row of table 2 is the setting A in table 1)\n\n3. table 3 is kind of confusing - what do you want to express with the figure?",
            "6": "training set performance is similar for reasoning and expansion?",
            "7": "how is it related to the main claim of the paper?"
        },
        "k8boqC9Wkc": {
            "0": "The paper discusses the suitability of LLMs to serve as content moderation tools, specifically using open-source models.",
            "1": "This is an important area of research since it provides insights into how open-source models can be used in that context and how they compare to commercial models (and hence has potential widespread applicability).",
            "2": "* The paper claims that their approach results in performances which are \"not weaker than\" those of GPT-3.5.",
            "3": "However, the results in Table 1 do not necessarily support that claim, since performance results are still lower than GPT-3.5 across most of the investigated categories.",
            "4": "* No details on datasets provided: the paper uses a privately collected and annotated dataset for experimentation (in-distribution), however no details on the number of examples and label distributions are provided.",
            "5": "* Related to the above, it would also be helpful to show how many examples have been incorrectly labeled by GPT-3.5 in Setting B and Setting C. How reliably can GPT-3.5 be used in that context?",
            "6": "* The explanations generated by GPT-3.5 to enhance the training datasets have not been manually verified.",
            "7": "While I acknowledge that an increase in performance hints at their usefulness, it would be interesting to see how such reasoning samples look like, and whether they truly represent what they are intended to.",
            "8": "Furthermore, which prompt formulations were used to obtain the reasoning and label?",
            "9": "Did you ask the model to first label and then generate reasoning, or vice versa?",
            "10": "Analyzing such details can have a notable impact on performance and should be discussed in more detail.",
            "11": "* Table 2 in the paper is not referred to in the manuscript?",
            "12": "* In Table 3, there are no details on which metrics are being reported, making it difficult to understand the shown results.",
            "13": "* Re.",
            "14": "Table 4: The paper assumes that length serves as an indicator for detailedness, which would need to be verified with manual annotations."
        },
        "g4CsnLldOF": {
            "0": "+ content moderation is an important research topic\n+ exploiting large language models for content moderation is a promising direction - I wonder if the authors could better clarify which part of the methodology corresponds to addressing the three challenges proposed in the introduction.",
            "1": "In addition, how does the proposed approach alleviate the need for quality annotated data?",
            "2": "- What is the unique technical contribution of the proposed pipeline?",
            "3": "Data deduplication is not new, and fine-tuning with chain-or-thought paths neither.",
            "4": "I'm also not exactly sure if the \"weak supervision\" part claimed in the methodology is very accurate.",
            "5": "It would be great to better highlight in the methodology the parts taken from existing works and the parts uniquely proposed by this work.",
            "6": "- How is self-recheck actually performed in step 2.b?",
            "7": "Is this a prompting-based approach?",
            "8": "Please provide more details as there is much unclear about this important step.",
            "9": "- I'm not sure about the contributions of Table 1 and Section 3.2.",
            "10": "Yes, fine-tuning a smaller model for a specific task outperforms ChatGPT, which might be considered already established.",
            "11": "I wonder if the authors could provide more analysis/insights as to this gigantic table.",
            "12": "- One of the major concerns is that there isn't any supervised baseline in this work.",
            "13": "While the authors proposed an approach based on supervised fine-tuning and claims for performance gains, it would be better to support it with supervised baselines proposed in recent literature.",
            "14": "- Maybe this is a minor point, but the taxonomy of \"content moderation\" could be better supported by related literature.",
            "15": "There is much research and discussion on what constitutes \"content that should be moderated\" and the ethical considerations associated with it.",
            "16": "- The related work on content moderation is inadequate.",
            "17": "It mostly discusses 2023 works with large language models, potentially overlooking a decade of research on related topics such as misinformation, hate speech, Twitter bots, and more.",
            "18": "It is suggested to greatly enrich related work discussions and baselines to better position this work in the context of the research landscape."
        }
    },
    "MiPacpmtmy": {
        "llm_review": {
            "0": "**Novel Dataset (COMPACT)**: The introduction of the COMPACT dataset is a significant contribution.",
            "1": "It is meticulously curated from the EK-100 dataset, ensuring that individual concepts are consistently distributed across training and evaluation sets while their compositions are novel in the evaluation set.",
            "2": "This setup is crucial for studying compositional generalization in a multimodal context.",
            "3": "**Comprehensive Evaluation**: The paper conducts a thorough evaluation of several unimodal and multimodal models, including both encoder-only and large language models.",
            "4": "This comprehensive assessment provides valuable insights into the performance of different models in the context of multimodal sequential compositional generalization.",
            "5": "**Multimodal Approach**: The study highlights the importance of multimodality in achieving better generalization.",
            "6": "The findings that bi-modal and tri-modal models exhibit a clear edge over their text-only counterparts underscore the potential of leveraging multiple modalities for improved performance.",
            "7": "**Task Design**: The design of the next utterance prediction and atom classification tasks is well thought out.",
            "8": "These tasks are relevant for evaluating the models' ability to understand and generate predictions about novel compositions of primitive elements derived from sequential multimodal inputs.",
            "9": "**Detailed Analysis**: The paper provides a detailed analysis of the results, including both quantitative and qualitative comparisons.",
            "10": "The qualitative examples effectively illustrate the challenges and the performance of different models, adding depth to the evaluation.",
            "11": "**Discussion on Pretrained Models**: The inclusion of pretrained models like LLaMA2, IDEFICS, MERLOT Reserve, and ImageBind adds a valuable dimension to the study.",
            "12": "The discussion on the performance of these models, especially in the context of their pretraining, is insightful.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Domain-Specific Dataset**: While the COMPACT dataset is a significant contribution, it is domain-specific, focusing on kitchen activities.",
            "15": "This limitation may affect the generalizability of the findings to other domains.",
            "16": "Future work could explore more diverse datasets to validate the conclusions.",
            "17": "**Limited Fusion Strategies**: The paper primarily focuses on a specific strategy for fusing multimodal data.",
            "18": "Given that multimodal learning is an open research problem with various proposed fusion strategies, the study could benefit from exploring and comparing different fusion techniques to identify the most effective approach.",
            "19": "**Simplistic Textual Utterances**: The textual utterances used in the study are inherently simplistic and may not capture the complexities of natural language.",
            "20": "Extending the work to more complex and natural language data could provide additional insights and improve the robustness of the findings.",
            "21": "**Pretraining Distributional Consistency**: The paper acknowledges that pretrained models might violate the distributional consistency between train and test splits during their pretraining.",
            "22": "This issue complicates the interpretation of the performance of these models.",
            "23": "A more detailed discussion on how to address this limitation would be beneficial.",
            "24": "**Limited Discussion on Limitations**: While the paper does mention some limitations, a more in-depth discussion on the potential impact of these limitations on the findings and future research directions would strengthen the study.",
            "25": "**Evaluation Metrics**: The study uses a range of evaluation metrics, but it could benefit from a more detailed explanation of why these specific metrics were chosen and how they relate to the goals of the study.",
            "26": "Additionally, exploring other relevant metrics could provide a more comprehensive evaluation.",
            "27": "#### Conclusion:\n\nOverall, the paper makes a significant contribution to the field of multimodal learning by introducing the COMPACT dataset and conducting a comprehensive evaluation of various models.",
            "28": "The findings highlight the importance of multimodality in achieving better compositional generalization.",
            "29": "However, the study could be further strengthened by addressing the limitations related to the domain-specific dataset, exploring different fusion strategies, and extending the work to more complex language data.",
            "30": "Despite these weaknesses, the paper provides valuable insights and sets a solid foundation for future research in multimodal sequential compositional generalization."
        },
        "7ZaSdQcnLR": {
            "0": "The paper is clear and well-written.",
            "1": "The study of multimodal models on natural video datasets is quite timely.",
            "2": "The idea of curating a subset of an existing dataset, rather than creating one from scratch, is a sound idea, especially since there are many similar datasets out there.",
            "3": "The paper is essentially an evaluation of well-defined tasks using an existing dataset and pre-trained models.",
            "4": "Out of the three tasks, the problem of action classification is only slightly different from next-utterance noun prediction and verb prediction (i.e., action is a combination of noun + verb, and the other two are predicted separately.)",
            "5": "Various pre-trained models are run in a zero-shot manner to predict the next entity in the sequence.",
            "6": "While the evaluation results could be of interest to someone who is looking to build these models, the paper main content offers little beyond this evaluation."
        },
        "VunhYePF1j": {
            "0": "The data distributions between training and evaluation in the CompAct benchmark are carefully controlled, allowing for the diagnosis of models' compositional generalization capabilities, which could be useful to the research community.",
            "1": "The authors present experimental results for approximately ten different unimodal or multimodal models.",
            "2": "Some of the results are intriguing; for example, the language-only method outperforms the multimodal method in noun classification.",
            "3": "However, for verb classification or next utterance prediction, the multimodal methods demonstrate superior performance.",
            "4": "The proposed method for curating train/eval splits to diagnose compositional generalization appears to be applicable to many other existing video datasets.",
            "5": "The authors have overlooked several works and benchmarks that are highly similar (see Questions below).",
            "6": "Compared to these existing works, the contribution of this paper does not seem to be very significant.",
            "7": "Additionally, the conclusions drawn from the experiments (e.g., recognition that compositional generalization is an area requiring improvement or that multi-modality could be more important than single-modality for certain challenging tasks) lack depth and insight."
        },
        "UrA4si1dpH": {
            "0": "The detailed strengths are as follows:\n1.",
            "1": "This paper is interesting because it is trying to understand the compositional generalization capabilities of foundation models.",
            "2": "This is a crucial skill for intelligent agents and yet there are limited work and benchmarks proposed to investigate the question.",
            "3": "Paper in this topic should be encouraged.",
            "4": "It investigates the important topic of compositional generalization capabilities in foundational models.",
            "5": "This is a crucial skill for intelligent agents and yet there are limited research and benchmarks in this domain.",
            "6": "Studies like this should be encouraged.",
            "7": "- However, the paper appears to have limitations in addressing this issue for large-scale pre-trained foundational models.",
            "8": "See weaknesses for details.",
            "9": "To answer this question, the paper presents a carefully curated novel dataset from real-world videos which could be much useful for future studies.",
            "10": "The paper also designs a set of multimodal models use different combinations of modalities (including unimodal) and different ways of fusing the multi-modal information.",
            "11": "This investigation provides valuable insight on how multi-modality inputs could influence the performance of models' compositional generalization ability.",
            "12": "The paper does not sufficiently investigate the compositional generalization ability of **foundation** models.",
            "13": "Addressing this is challenging due to the potential distributional discrepancies between training and testing splits during their pretraining, as noted in the paper.",
            "14": "Consequently, emphasizing \"foundation models\" in the title may be somewhat overstated.",
            "15": "- Could incorporating domain-specific fine-tuning offer additional insights?",
            "16": "The dataset's domain-specific nature results in text descriptions that lack diversity.",
            "17": "As a result, unlike foundation LLM, language models trained on these specific tests might be prone to overfitting and lack reasoning skills.",
            "18": "On the other hand, other modalities, such as the vision input processed by a pretrained ResNet model, inherently resist overfitting, potentially leading to enhanced generalization.",
            "19": "Thus, the conclusion that multi-modality contributes to improvements and that visual features consistently enhance results could potentially be invalid."
        },
        "M2jEVfY3qn": {
            "0": "The paper focus on studying the composition of foundation models on many variants, on Epic kitchen dataset that is tailored for composition evaluation.",
            "1": "The paper draws a conclusion that multimodal helps composition, yet from Table 1, the trend is not very clear."
        }
    },
    "AL1fq05o7H": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel class of selective state space models (SSMs) that address the limitations of traditional SSMs in handling discrete and information-dense data such as text.",
            "1": "By making SSM parameters functions of the input, the model can selectively propagate or forget information, which is a significant improvement over existing models.",
            "2": "**Efficiency**: Mamba achieves linear scaling in sequence length and demonstrates a 5x higher inference throughput compared to Transformers.",
            "3": "This is a substantial advancement in terms of computational efficiency, especially for long sequences.",
            "4": "**Empirical Validation**: The paper provides extensive empirical validation across various modalities, including language, audio, and genomics.",
            "5": "The results show that Mamba outperforms state-of-the-art models in these domains, which is a strong testament to its effectiveness.",
            "6": "**Hardware-Aware Algorithm**: The design of a hardware-aware parallel algorithm that computes the model recurrently without materializing the expanded state is a notable contribution.",
            "7": "This approach leverages modern hardware capabilities to achieve significant speedups.",
            "8": "**Simplified Architecture**: The Mamba architecture is simplified by combining the design of prior SSM architectures with the MLP block of Transformers into a single block.",
            "9": "This homogenous design is elegant and reduces the complexity of the model.",
            "10": "**Scalability**: The paper demonstrates that Mamba scales well with model size and sequence length, showing improvements in performance with longer contexts up to million-length sequences.",
            "11": "This scalability is crucial for practical applications in various domains.",
            "12": "**Comprehensive Evaluation**: The paper includes a thorough evaluation on synthetic tasks, language modeling, DNA modeling, and audio modeling.",
            "13": "The results are detailed and provide a clear comparison with existing models, highlighting the strengths of Mamba.",
            "14": "#### Weaknesses\n\n1.",
            "15": "**Complexity of Implementation**: While the paper introduces an efficient hardware-aware algorithm, the implementation details might be complex for practitioners to reproduce.",
            "16": "The reliance on specific hardware optimizations could limit the accessibility of the model.",
            "17": "**Limited Discussion on Limitations**: The paper does not extensively discuss the potential limitations or failure modes of the Mamba architecture.",
            "18": "Understanding the scenarios where Mamba might not perform well is important for a comprehensive evaluation.",
            "19": "**Ablation Studies**: Although the paper includes some ablation studies, more detailed ablations on the impact of different components of the selective SSMs and the Mamba architecture would strengthen the understanding of the model's performance.",
            "20": "**Generalization to Other Modalities**: While the paper shows strong results in language, audio, and genomics, it would be beneficial to see evaluations on other modalities such as video or time-series data to further validate the generalizability of the model.",
            "21": "**Comparison with More Baselines**: The paper primarily compares Mamba with Transformers and a few other models.",
            "22": "Including a broader range of baselines, especially recent advancements in efficient sequence modeling, would provide a more comprehensive comparison.",
            "23": "**Theoretical Insights**: The paper focuses heavily on empirical results, but additional theoretical insights into why the selective mechanism works so well would be valuable.",
            "24": "A deeper theoretical analysis could provide a stronger foundation for the proposed approach.",
            "25": "#### Conclusion\n\nOverall, the paper presents a significant advancement in sequence modeling with the introduction of selective state space models and the Mamba architecture.",
            "26": "The strengths of the paper lie in its innovative approach, efficiency, empirical validation, and scalability.",
            "27": "However, the complexity of implementation, limited discussion on limitations, and the need for more comprehensive ablation studies and comparisons are areas that could be improved.",
            "28": "Despite these weaknesses, the contributions of the paper are substantial and have the potential to impact various domains requiring efficient and effective sequence modeling."
        },
        "ZJCXkIMKQY": {
            "0": "S1: The paper addresses very efficiently and effectively pressing problems in sequential modeling.",
            "1": "S2: The authors have identified simple toy tasks, such as selective copying and associative recall, that enable them to make design choices which state-of-the-art impact on real-world data.",
            "2": "S3: The connection to the role of gating mechanisms in RNNs is well-appreciated.",
            "3": "S4: The empirical part of the paper is very thorough, and the results are strong.",
            "4": "I do not identify any major weaknesses of the paper."
        },
        "t7w5AlzJfq": {
            "0": "The paper is written in a clear and understandable manner, with a well-defined approach and simple yet effective improvement strategies.",
            "1": "The paper lacks references to some relevant works, such as [1], [2], [3], [4] which discusses some Linear Attention methods, and [5], which is also a LongConv method.",
            "2": "However, these references are completely absent in the paper.",
            "3": "I suggest that the authors consider adding these citations to provide a more comprehensive review of related work.",
            "4": "[1] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong.",
            "5": "cosformer: Rethinking softmax in attention.",
            "6": "In ICLR, 2022.",
            "7": "[2] Efficient Attention via Control Variates, Lin Zheng, Jianbo Yuan, Chong Wang, and Lingpeng Kong In International Conference on Learning Representations (ICLR), 2023\n\n[3] Linear Complexity Randomized Self-attention Mechanism, Lin Zheng, Chong Wang, and Lingpeng Kong In International Conference on Machine Learning (ICML), 2022\n\n[4] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong.",
            "8": "The devil in linear transformer.",
            "9": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7025–7041, Abu Dhabi, United Arab Emirates, Dec. 2022.",
            "10": "Association for Computational Linguistics.",
            "11": "[5] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong.",
            "12": "Toeplitz neural network for sequence modeling.",
            "13": "In The Eleventh International Conference on Learning Representations (ICLR), 2023."
        },
        "bq8XeiBhwY": {
            "0": "+ A key limitation of prior SSMs is the inability to efficiently select data in an input-dependent manner.",
            "1": "The paper introduces a key mechanism by parameterizing the SSM parameters based on the input, allowing the model to filter out irrelevant information and remember relevant information indefinitely.",
            "2": "+ The results as compared to Pythia, and Transforms on many benchmarks are impressive.",
            "3": "- The model still has a quadratic memory requirement during training like Transformers."
        },
        "eIarHE3vGs": {
            "0": "* The proposed Mamba method includes a simple modification to the conventional SSM model: add additional models to make SSM models dependent on the inputs.",
            "1": "SSMs are known for their computational difficulties, and the authors address this issue by several performance optimization techniques.",
            "2": "* The authors pre-train several variants of Mamba, ranging from 130M parameters to 1.4B parameters.",
            "3": "These pre-trained models show performance improvements compared with the baselines in the paper.",
            "4": "Concerns about model design:\n\n* The motivation of Mamba is to address the drawbacks of recurrent models while improving the efficiency of attention-based models.",
            "5": "There are many works following the same direction: S4-diagonal [1], SGConv [2], MEGA [3], SPADE [4], and many efficient Transformer models (e.g., [5]).",
            "6": "All of these models achieve near linear complexity, and the authors need to compare Mamba with these works in terms of both model performance and efficiency.",
            "7": "For model performance, some simple experiments such as language modeling on Wikitext-103 should suffice.",
            "8": "* Many attention-based Transformer models show length generalization ability, i.e., models can be trained on a shorter sequence length and tested on a longer sequence length.",
            "9": "Some examples include relative positional encoding (T5) and Alibi [6].",
            "10": "Because SSMs are in general sequential, does Mamba have this length generalization ability?",
            "11": "Concerns about experiments:\n\n* The authors need to compare with stronger baselines.",
            "12": "The authors acknowledge that H3 was used as a motivation for the model architecture.",
            "13": "However, they did not compare with H3 in the experiments.",
            "14": "From Table 4 in [7], ppl of H3 is 8.8 (125M), 7.1 (355M), and 6.0 (1.3B) on the Pile dataset, which are considerably better than Mamba.",
            "15": "The authors need to show comparisons with H3.",
            "16": "* For the pre-trained models, the authors only show results on zero-shot inference.",
            "17": "This setting is quite limited and the results cannot support the effectiveness of Mamba well.",
            "18": "I suggest the authors run more long-sequence experiments such as document summarization, where the input sequence is naturally long (e.g., the average sequence length of the arXiv dataset is greater than 8k).",
            "19": "* One of the main contributions that the authors claim is long sequence modeling.",
            "20": "The authors should compare with more baselines on LRA (Long Range Arena), which is essentially the standard benchmark for long sequence understanding.",
            "21": "* Memory benchmarking is missing.",
            "22": "Even though Section 4.5 is titled “speed and memory benchmark”, only speed comparisons are presented.",
            "23": "Also, the authors should provide more detailed setups of Figure 8 left, e.g., model layers, model sizes, details of the convolution, etc.",
            "24": "Could the authors provide some intuitions why FlashAttention is the slowest when the sequence length is very large (Figure 8 left)?",
            "25": "[1] https://arxiv.org/pdf/2203.14343.pdf \\\n[2] https://arxiv.org/pdf/2210.09298.pdf \\\n[3] https://arxiv.org/pdf/2209.10655.pdf \\\n[4] https://arxiv.org/pdf/2212.08136.pdf \\\n[5] https://arxiv.org/pdf/2202.10447.pdf \\\n[6] https://arxiv.org/pdf/2108.12409.pdf \\\n[7] https://arxiv.org/pdf/2212.14052.pdf"
        }
    },
    "6PmJoRfdaK": {
        "llm_review": {
            "0": "**Innovative Approach**: The introduction of LongLoRA, which combines shifted sparse attention (S2-Attn) and an improved version of LoRA, is a novel and efficient method for extending the context size of large language models (LLMs).",
            "1": "This approach addresses the computational challenges associated with training LLMs on long contexts.",
            "2": "**Efficiency**: LongLoRA significantly reduces the computational cost and memory requirements compared to full fine-tuning.",
            "3": "The paper demonstrates that LongLoRA can extend the context length of Llama2 models up to 100k tokens for the 7B model and 32k tokens for the 70B model on a single 8 × A100 machine, which is a substantial improvement.",
            "4": "**Empirical Results**: The paper provides strong empirical results, showing that LongLoRA achieves comparable performance to full fine-tuning with much lower computational costs.",
            "5": "The results on various tasks and datasets, including PG19 and proof-pile, are convincing.",
            "6": "**Compatibility**: LongLoRA retains the original architecture of the models during inference, making it compatible with existing techniques like Flash-Attention2.",
            "7": "This ensures that the method can be easily integrated into existing workflows without significant modifications.",
            "8": "**Implementation Simplicity**: The proposed S2-Attn can be implemented with only two lines of code, making it accessible and easy to adopt for researchers and practitioners.",
            "9": "**Comprehensive Evaluation**: The paper includes a thorough evaluation of LongLoRA, including ablation studies on fine-tuning steps, attention patterns, and efficiency analysis.",
            "10": "This comprehensive evaluation helps in understanding the strengths and limitations of the proposed method.",
            "11": "**Open Source**: The authors have made their code, models, dataset, and demo available on GitHub, promoting transparency and reproducibility in research.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Limited Comparison with Other Methods**: While the paper compares LongLoRA with some existing methods, it could benefit from a more extensive comparison with a broader range of state-of-the-art techniques for long-context LLMs.",
            "14": "This would provide a clearer picture of where LongLoRA stands in the landscape of long-context adaptation methods.",
            "15": "**Scalability to Larger Models**: Although the paper demonstrates the effectiveness of LongLoRA on Llama2 models up to 70B parameters, it would be valuable to see how the method scales to even larger models, such as those with hundreds of billions of parameters.",
            "16": "This would help in understanding the scalability and limitations of LongLoRA in more extreme settings.",
            "17": "**Generalization to Other Architectures**: The paper primarily focuses on Llama2 models.",
            "18": "It would be beneficial to explore the generalization of LongLoRA to other LLM architectures, such as GPT-3 or T5, to demonstrate its broader applicability.",
            "19": "**Impact on Downstream Tasks**: While the paper shows improvements in language modeling tasks, it would be interesting to see the impact of LongLoRA on a wider range of downstream tasks, such as question answering, summarization, and dialogue systems.",
            "20": "This would provide a more comprehensive understanding of the practical benefits of the method.",
            "21": "**Detailed Analysis of S2-Attn**: The paper introduces S2-Attn as a key component of LongLoRA, but a more detailed analysis of its behavior and impact on model performance would be helpful.",
            "22": "For instance, exploring how different configurations of S2-Attn affect the trade-off between efficiency and accuracy could provide deeper insights.",
            "23": "**LongAlpaca Dataset**: The paper mentions the creation of the LongAlpaca dataset for supervised fine-tuning but provides limited details about the dataset.",
            "24": "A more detailed description of the dataset, including its size, diversity, and examples, would be useful for understanding its role in the experiments.",
            "25": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the efficient fine-tuning of long-context LLMs.",
            "26": "The combination of S2-Attn and improved LoRA in LongLoRA offers a practical solution to the computational challenges associated with extending context sizes.",
            "27": "The empirical results are strong, and the method's compatibility with existing architectures and techniques makes it a valuable contribution to the field.",
            "28": "Addressing the identified weaknesses in future work could further strengthen the impact and applicability of LongLoRA."
        },
        "nxauXgEcUM": {
            "0": "1. the paper is well written and easy to follow.",
            "1": "the proposed approach is a simple method that can adapt LLM for longer context without too much compute.",
            "2": "2. the paper has good ablation to show that LoRA on embedding and normalization is important for long-context adaptation.",
            "3": "1. the paper only evaluated on retrieval and perplexity.",
            "4": "It would be good to evaluate on other generative tasks that require longer context.",
            "5": "2. the improvement on perplexity doesn't seem super consistent in Table.",
            "6": "4"
        },
        "Ke4BKU68Fr": {
            "0": "(1) The method seems useful and impactful, and the evaluation is thorough with strong results.",
            "1": "(2) The authors perform very thorough ablations and isolate key design decisions (attention shift, modifying the norm & embedding layers) that enable the method to match full fine-tuning.",
            "2": "(3) The paper is well-written.",
            "3": "No major weaknesses."
        },
        "LhmjbCk8CO": {
            "0": "- The proposed method builds on previous work and shows strong empirical results on long lange language modelling and a retrieval task\n- The proposed approach is conceptually simple and can be implemented in a few lines of code (as demonstrated by the authors)\n- The proposed approach can be combined with existing approaches for context extension such as positional interpolation \n- The authors provide a detailed discussion of related work - The efficiency aspect of the could could be more prominently discussed in the main body of the paper\n- The presentation of the work could be improved.",
            "1": "See below for suggestions"
        },
        "yHUD09zCLP": {
            "0": "- The authors propose an extremely simple method, that performs well and is applicable to existing pretrained models - The authors only evaluate perplexity and retrieval setting"
        }
    },
    "c8McWs4Av0": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method, explicit code-based self-verification (CSV), which leverages the code generation and execution capabilities of GPT-4 Code Interpreter to enhance its problem-solving accuracy.",
            "1": "This method is a significant advancement over previous approaches that relied solely on natural language reasoning.",
            "2": "**Systematic Analysis**: The authors provide a thorough analysis of the impact of code usage on the performance of GPT-4 Code Interpreter.",
            "3": "By introducing the concept of Code Usage Frequency, they offer a quantitative measure to evaluate the effectiveness of different prompting strategies.",
            "4": "**Empirical Validation**: The proposed method is empirically validated on multiple challenging mathematical problem-solving benchmarks, including MATH, GSM8K, and MMLU-Math datasets.",
            "5": "The results demonstrate substantial improvements in accuracy, showcasing the effectiveness of the CSV method.",
            "6": "**Detailed Experiments**: The paper includes detailed experiments and comparisons with various baselines, including state-of-the-art methods.",
            "7": "The inclusion of ablation studies and hyper-parameter tuning further strengthens the validity of the proposed approach.",
            "8": "**Clear Presentation**: The paper is well-structured and clearly presents the methodology, experiments, and results.",
            "9": "The use of figures and tables to illustrate the findings enhances the readability and comprehension of the paper.",
            "10": "**Practical Implications**: The proposed CSV method has practical implications for improving the reliability and accuracy of large language models in solving complex mathematical problems.",
            "11": "This can be particularly useful in educational and professional settings where accurate problem-solving is critical.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Limited Scope of Verification**: While the CSV method significantly improves accuracy, it primarily focuses on code-based verification.",
            "14": "The paper does not explore the potential benefits of combining code-based and natural language verification, which could further enhance the robustness of the model.",
            "15": "**Dependence on Code Execution**: The approach heavily relies on the model's ability to generate and execute code correctly.",
            "16": "In scenarios where the code execution environment is limited or unavailable, the effectiveness of the CSV method may be compromised.",
            "17": "**Complexity of Implementation**: The proposed method involves multiple stages of code generation, execution, and verification, which may increase the complexity of implementation.",
            "18": "This could be a barrier for practitioners who seek simpler solutions.",
            "19": "**Generalization to Other Domains**: The paper focuses on mathematical problem-solving, and it is unclear how well the CSV method would generalize to other domains that require different types of reasoning and verification.",
            "20": "**Evaluation Metrics**: The paper primarily uses accuracy as the evaluation metric.",
            "21": "While accuracy is important, additional metrics such as computational efficiency, robustness to different types of errors, and user satisfaction could provide a more comprehensive evaluation of the method.",
            "22": "**Limited Discussion on Failure Cases**: The paper does not provide an in-depth analysis of the failure cases where the CSV method does not lead to correct solutions.",
            "23": "Understanding these limitations could help in further refining the approach.",
            "24": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of mathematical problem-solving using large language models.",
            "25": "The introduction of the explicit code-based self-verification method and the systematic analysis of code usage provide valuable insights and practical improvements.",
            "26": "While there are some limitations and areas for further exploration, the strengths of the paper outweigh the weaknesses, making it a valuable contribution to the field."
        },
        "Q933vlNNOH": {
            "0": "A good demonstration of OpenAI's new GPT4 with Python Interpreter version.",
            "1": "The experiments show the great improvement of the new model comparing w/ the traditional GPT4.",
            "2": "It also shows the potential of using code interpreter as an external tool to enhance performance on math reasoning tasks.",
            "3": "They authors push the SoTA of MATH to 84.3%, which is a very high number considering its complexity.",
            "4": "Most of the methods discussed in the paper are proposed by existing works and the main contribution of this paper is to try them out using the new OpenAI model.",
            "5": "The contribution and novelty could be a weakness of the paper.",
            "6": "Most credit of the huge improvement on MATH should be given to the better capability of GPT4-code itself.",
            "7": "The authors seem to over claim their own contribution throughout the paper.",
            "8": "For example in Page 6, the authors said \"Before the advent of GPT4-Code, prior frameworks (Lightman et al., 2023; Cobbe et al., 2021) depended on an external LLM to use natural language for verification and well-designed fewshot example prompts.",
            "9": "In contrast, our approach simplifies the process by relying solely on a straightforward prompt for GPT4-Code, all in a **zero-shot** manner.\"",
            "10": "But the root reason that zero-shot is applicable is because you are using OpenAI's instruction-tuned model.",
            "11": "In addition, it is very likely that OpenAI has already used PRM (Lightman et al., 2023) to RL tune it's new model, therefore the API you used could be based on top of Lightman et al., 2023."
        },
        "7uD78Lo0gM": {
            "0": "This paper is timely.",
            "1": "It's a novel approach of leveraging code self-verification to improve math reasoning ability.",
            "2": "It's simplicity means it can be easily used by GPT-4 users, and the thorough study provides solid evidence of adopting this technique.",
            "3": "Concretely, the paper's strengths include:\n1.",
            "4": "Novel technique that combines self-debugging and test generation for math reasoning tasks.",
            "5": "Quite complete study that highlights effectiveness of the technique.",
            "6": "Weighted voting technique is quite unique, and it can possibly used in other self-debugging techniques beyond Math reasoning tasks.",
            "7": "The paper lacks some insights into the quality of self-verification results, and how that matters for model performance.",
            "8": "The paper can potentially dive deeper into analysis of consistency between verification process and NL reasoning process as well as output correctness.",
            "9": "As shown in prior work like CodeT, some self-generated test-cases or verification code can either be wrong or inconsistent, but it may or may not affect model output quality.",
            "10": "Guessing from the paper's results on weighted voting versus simple voting, such inconsistency exists and they could benefit model performance (or affect model performance if we simply reject such answers).",
            "11": "I would suggest the authors perform some qualitative analysis to dive into this problem.",
            "12": "1b.",
            "13": "Some deeper qualitative analysis into what types of verification code are generated would also be helpful.",
            "14": "2.The paper considers both sampling and sequential self-repair.",
            "15": "It would be great if the authors can analyze the tradeoff between the depths of self-repair (e.g., if self-verification continues to fail, how much can the model benefit from continuing self-repair until verification succeed) verse breaths of self-verification (e.g., simply repair once, but generate multiple samples to do weighted sampling).",
            "16": "The paper plots are visually misleading: Figure 2 accuracy numbers should all start from 0 as opposed to 60 in figure 2a and 40 in figure 2b.",
            "17": "Otherwise the improvement looks like 10x as opposed to 6% comparing prompts 1 and 2.",
            "18": "Similarly for other figures."
        },
        "LAj677FfFc": {
            "0": "- Like many recent works, this work provides strong evidence that prompting LLMs in a proper way could significantly influence the performance\n- This work also makes another natural but still interesting finding -- the frequency of code usage has a strong correlation with the accuracy of the final answer.",
            "1": "The rationality is that executing code is more accurate/reliable than performing reasoning through natural language text.",
            "2": "- This work presents the new state-of-the-art results on three datasets (i.e., MATH, GSM8K, MMLU-Math).",
            "3": "- In terms of improving LLMs, the ideas of code writing, self-validating, and majority voting have been already explored in recent literature.",
            "4": "The novelty of this work seems a simple combination of all three ideas together.",
            "5": "- Only one particular and proprietary LLM (i.e., GPT-4) is used for evaluation.",
            "6": "Whether the results and findings reported in this work may generalize to other publicly available LLMs (e.g., Llama 2) or relatively smaller language models is unclear.",
            "7": "- Another similar concern is that only one specific kind of benchmark (i.e., grade school math problems) is used, thus the proposed prompt might be overfitting to simple math problems.",
            "8": "For the chosen benchmarks, improvements due to coding and CSV on GSM8K and MMLU-Math are already fairly small (i.e., 2-4%).",
            "9": "- The improvement seems largely due to the superior capability of GPT-4.",
            "10": "For instance, GPT-4 itself can outperform the state-of-the-art by a large margin without any sophisticated prompting and majority voting.",
            "11": "It is already well-known that even a simple prompt could dramatically influence performance (see ).",
            "12": "The particular finding of GPT-4 on the MATH benchmark is not very surprising."
        },
        "vlsGuwEwdw": {
            "0": "- The results on correlations between code usage frequency and accuracy are a nice analysis, and breaking it down by difficulty level of problem is useful (e.g.",
            "1": "at higher difficulty levels amount of code usage seems to matter more)\n- The CSV setup itself is quite simple, which is a good thing – \"Solve the problem using code interpreter step by step, even in every sub-step.",
            "2": "And following your answer, please verify it using code interpreter by yourself.\"",
            "3": "A simple way of getting a decent boost in performance from zero-shot prompting is a fairly useful contribution.",
            "4": "The boost of 3.85% accuracy on the MATH dataset from using CSV over the base prompt is solid – that's a reasonable gain.",
            "5": "- The Verification-guided weighted majority voting setup is also quite simple (just a weighted majority vote with human-picked parameters) which is good.",
            "6": "The weighting seems to provide a boost of around +1% accuracy over a standard majority voting approach (at 16 samples, MATH dataset, per Fig 6b).",
            "7": "- The precision-recall-accuracy analysis and also analysis of weighted majority vs naive majority are good to see, I was preparing to suggest that, and I'm glad it was done.",
            "8": "Overall I'm marginally below acceptance on this paper, but would certainly consider raising my score if the following points are well addressed in revisions/rebuttal.",
            "9": "- As-is the current paper has only one row in Table 1 that uses 16 samples with GPT4-Code, so the +14.63% improvement is a comparison between a 1 sample and 16 sample method.",
            "10": "This can be a bit misleading to readers (or at least it was to me) since much of that improvement comes from using additional samples, which is something that could also be done through prior work via naive majority voting (which is actually evaluated later in the paper, in Fig 6b).",
            "11": "Therefore, Table 1 should include a line for **GPT4-Code + CSV + Majority Voting** (which does about 1% worse than weighted voting, per Fig 6b).",
            "12": "- For the same reasons as in the previous comment, adding a line to Table 1 with **GPT4-Code + Majority Voting (no CSV)**, an ablation that done with neither CSV nor weighted voting, would be an informative baseline to include.",
            "13": "These two baselines would considerably strengthen the analysis.",
            "14": "- There doesn't seem to be a **GPT4-Code + CSV** row in Table 2, nor a **GPT4-Code + CSV + Voting** row in Table 3.",
            "15": "This use of different setups on different datasets is confusing – an explanation should be given, or preferably both should be added.",
            "16": "Having both of these novel methods from the paper evaluated on all 3 datasets would strengthen the paper.",
            "17": "- Figure 6a is quite confusing to me:\n    - Which dataset is used?",
            "18": "I believe it's MATH overall dataset, but this should be in the caption and optionally also main text\n    - I don't understand why there are 5 separate paths graphed, given that there's nothing special differentiating path 0 from path 1, etc.",
            "19": "The different path indices are just IID samples, so of course when you calculate precision/accuracy/recall averaged over a whole dataset of problems, all the histogram bars will look the same for different sample path indices.",
            "20": "So having 5 separate paths doesn't seem to add anything – why not just collapse them all into a single graph with three bars (accuracy, precision, recall)?",
            "21": "minor:\n- Consider weakening the statement \"Each line in Fig.",
            "22": "5 has an obvious trend of going upwards, proving that the increase of Code Usage Frequency induces a general improvement in accuracy\": *Proves* is a strong word there, when it's just a nice correlation (not causal – there are many ways of including meaningless code in an output that wouldn't not induce an improvement in accuracy).",
            "23": "Just state it as a correlation and that's good."
        }
    },
    "Pj52xO5ysY": {
        "llm_review": {
            "0": "**Interpretability**:\n   - The primary strength of the paper lies in its focus on interpretability.",
            "1": "The proposed AMIC model provides word-level sentiment analysis, which is more interpretable compared to traditional document-level sentiment analysis models.",
            "2": "This is crucial for applications where understanding the reasoning behind the model's predictions is as important as the predictions themselves.",
            "3": "**Innovative Use of Multiple Instance Classification (MIC)**:\n   - The use of MIC in the context of sentiment analysis is innovative.",
            "4": "By treating each document as a bag of words and each word as an instance, the model can effectively identify sentiment words and their contributions to the overall sentiment of the document.",
            "5": "**Attention Mechanism with Relative Positional Representations**:\n   - The incorporation of self-attention with relative positional representations is a significant strength.",
            "6": "This allows the model to capture both global and local dependencies, which is essential for handling complex linguistic phenomena such as negation and sentiment intensity.",
            "7": "**Performance**:\n   - The model achieves competitive performance on two datasets (wine reviews and Twitter Sentiment140), demonstrating its effectiveness.",
            "8": "It performs particularly well on the wine review dataset, achieving accuracy close to that of BERT but with far fewer parameters.",
            "9": "**Comprehensive Evaluation**:\n   - The paper provides a thorough evaluation of the model, including ablation studies that highlight the importance of different components of the model.",
            "10": "This helps in understanding the contribution of each component to the overall performance.",
            "11": "**Detailed Examples**:\n   - The paper includes detailed examples that illustrate how the model handles various linguistic complexities, such as negation and implicit sentiment.",
            "12": "These examples effectively demonstrate the interpretability and fine-grained analysis capabilities of the model.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Complexity and Scalability**:\n   - While the model is more interpretable than traditional transformer models, it still involves a complex architecture with multiple components (e.g., context-independent sentiment scores, global and local sentiment shifters).",
            "15": "This complexity might pose challenges in terms of scalability and computational efficiency, especially for very large datasets.",
            "16": "**Limited Dataset Variety**:\n   - The evaluation is conducted on only two datasets.",
            "17": "While these datasets are diverse (formal language in wine reviews and informal language in tweets), additional datasets from different domains could provide a more comprehensive evaluation of the model's robustness and generalizability.",
            "18": "**Dependence on Pre-trained Embeddings**:\n   - The model relies on pre-trained word embeddings (Glove-300-Wiki and word2vec).",
            "19": "The performance of the model might be sensitive to the quality and domain-specificity of these embeddings.",
            "20": "Exploring the impact of different embeddings or fine-tuning them for specific tasks could be beneficial.",
            "21": "**Interpretability vs.",
            "22": "Performance Trade-off**:\n   - Although the model is interpretable, it still lags behind BERT in terms of performance on the Twitter Sentiment140 dataset.",
            "23": "This trade-off between interpretability and performance is a common challenge, but it would be useful to explore ways to further close this gap.",
            "24": "**Penalty Terms and Hyperparameters**:\n   - The model includes several penalty terms and hyperparameters (e.g., c1, c2, c3) that need to be tuned.",
            "25": "The process of tuning these parameters can be time-consuming and might require domain-specific knowledge.",
            "26": "Providing more guidance or automated methods for tuning these parameters could enhance the usability of the model.",
            "27": "**Generalization to Multiclass Classification**:\n   - The paper mentions that the model can be extended to multiclass classification, but this is not demonstrated.",
            "28": "Providing empirical results for multiclass sentiment analysis would strengthen the claims about the model's versatility.",
            "29": "#### Conclusion:\n\nOverall, the paper presents a novel and interpretable approach to sentiment analysis that effectively combines the strengths of multiple instance classification and transformer-based models.",
            "30": "The focus on word-level sentiment analysis and the incorporation of relative positional representations are significant contributions.",
            "31": "However, the complexity of the model, limited dataset variety, and the need for extensive hyperparameter tuning are areas that could be further addressed.",
            "32": "Despite these weaknesses, the model shows promise in providing interpretable and competitive sentiment analysis, making it a valuable contribution to the field."
        },
        "ibqCN1mgRD": {
            "0": "The proposed model improves the interpretability from the word-level perspective and uses the MIC module.",
            "1": "The overall structure is well organized, and the experimental results illustrate the effectiveness of the proposed model on two public SA datasets.",
            "2": "The novelty and contributions are limited.",
            "3": "There are some existing methods using the MIC and self-attention mechanism on SA tasks.",
            "4": "The authors should introduce them and explain the main differences as well as advantages.",
            "5": "Chapter 4 in \"Sentiment Lexicon Induction and Interpretable Multiple-instance Learning in Financial Markets\".",
            "6": "In addition, for the word level interpretability of the SA model, does the author consider combining other level information in the model as illustrated in \"A Multi-Grained Self-Interpretable Symbolic-Neural Model For Single/Multi-Labeled Text Classification\" -- \"there is only word-level attribution but no high-level attribution such as those over phrases and clauses.",
            "7": "Take sentiment analysis as an example, in addition to the ability to recognize the sentiment of sentences, an ideal interpretable model should be able to identify the sentiment and polarity reversal at the levels of words, phrases, and clauses.\"",
            "8": "For the AMIC architecture, the reason why using a three-layer feedforward operation should be introduced.",
            "9": "And why not use a Transformer-based model and only use a self-attention mechanism?",
            "10": "For the word embedding, what are the differences between Glove-300-Wiki  and  word2vec?",
            "11": "The author should add more comparative experiments to show the different performances and BERT embedding.",
            "12": "For Table 2, it is better to add more ablation studies on different datasets.",
            "13": "Moreover, sentiment analysis is a common NLP task, so the authors should add other baselines.",
            "14": "For interpretability, do the authors consider an attention map to show the performance of the attention mechanism?",
            "15": "to illustrate the effectiveness of the proposed model?",
            "16": "More sentence case studies could be added in the appendix."
        },
        "r7e9hspBA9": {
            "0": "-\tReasonable method.",
            "1": "-\tEvaluation of two datasets with relatively detailed experimental analysis.",
            "2": "-\tThe techniques employed in this study are rather conventional, and in terms of text interpretability, while they do offer some assistance, their significance is limited.",
            "3": "Furthermore, for longer sentences or sentence-level tasks (commonly addressed using BERT-based models for text encoding), they lack scalability.",
            "4": "-\tThe work presented in this paper is quite mundane, and employing large models like ChatGPT for word-level interpretability might even yield better results, enhancing its overall scalability as well.",
            "5": "**Text:** \\\nThe service of the restaurant is good, the overall experience is not bad\n**Prompt:** \\\nWhat is the emotion of this sentence, analyzed at the word level\n**ChatGPT 3.5:** \\\nAt the word level, the emotion of this sentence can be broken down as follows:\n\n\"good\" implies a positive emotion.",
            "6": "\"not bad\" implies a somewhat positive or neutral emotion.",
            "7": "While it contains a negation (\"not\"), it's a double negative, and when used colloquially, it often means \"quite good\" or \"satisfactory.\"",
            "8": "So, the overall emotion at the word level is generally positive, with a touch of reservation or neutrality.",
            "9": "-\tThe related work should be updated with more recent related works.",
            "10": "-\tFigure 1 appears quite blurry.",
            "11": "I recommend redrawing the figure to ensure it meets the required resolution of 300 DPI.",
            "12": "-\tThe formulas in the article appear quite unusual and are not conducive to understanding, especially when there are three formulas in a single line.",
            "13": "It's not recommended.",
            "14": "-\tThe format of the references in the paper needs to be consistent.",
            "15": "If a referenced paper has already been published, it should not be cited in the arXiv format.",
            "16": "This should be updated to reflect the appropriate citation style for the published version.",
            "17": "-\tWriting errors are common across the overall paper.",
            "18": "Examples could be found in “Typos, Grammar, Style, and Presentation Improvements”.",
            "19": "**Typos, Grammar, Style, and Presentation Improvements:**\n\n-\tOn page 1, “… weighting, indication of …” => “… weighting, an indication of …”\n-\tOn page 1, “… insights on how context influences …” => “… insights into how context influences …”\n-\tOn page 1, “… multiple instance classification model …” => “… multiple instance classification models …”\n-\tOn page 1, “… methods which focus …” => “… methods that focus …”\n-\tOn page 2, “… support vector machine …” => “… support vector machines …”\n-\tOn page 2 “… long-short term …” => “… long-short-term …”\n-\tOn page 2 “… shown BERT …” => “… shown that BERT …”\n-\tOn page 2 “… in text can …” => “… in the text can …”\n-\tOn page 3 “… sentiment score at …” => “… sentiment scores at …”\n-\tOn page 3 “… help a SA model …” => “… help an SA model …”\n-\tOn page 3 “… its ignorance to the …” => “… its ignorance of the …”\n-\tOn page 6 “… the sentiment of a wine …” => “… the sentiment of wine …”\n-\tOn page 6 “… sentiment and the other …” => “… sentiment, and the other …”\n-\tOn page 6 “… follows a 18:1:1 ratio …” => “… follows an 18:1:1 ratio …”\n-\tOn page 7 “… has similar performance …” => “… has a similar performance …”\n-\tOn page 7 “… is able to also capture …” => “… is able to capture …”\n-\tOn page 7 “… various impact of …” => “… various impacts of …”\n-\tOn page 7 “… local patterns recognition.” => “… local pattern recognition.”\n-\tOn page 7 “… aspect to the …” => “… aspect of the …”\n-\tOn page 7 “… delicate lingistic complexities …” => “… delicate linguistic complexities …”\n-\tOn page 7 “… clauses seperated by …” => “… clauses separated by …”\n-\tOn page 8 “… the capability AMIC in provid …” => “… has a similar \n-\tOn page 9 “… providing detailed interpretation …” => “… providing a detailed interpretation …”\n-\tOn page 9 “… of analysis process …” => “… of the analysis process …”"
        },
        "Qh9ZldEtAj": {
            "0": "The model's integration of the MIC module enhances word-level interpretability, and its efficacy is demonstrated through experiments conducted on two public sentiment analysis datasets.",
            "1": "The critique highlights the perceived lack of innovation and the limited contributions of the study, suggesting that the methods used are standard and offer minimal advancements in text interpretability.",
            "2": "It points out a scalability issue with longer text sequences.",
            "3": "It suggests that utilizing more advanced language models for sentence-level tasks or even LLMs (like ChatGPT or LLaMa) for word-level interpretability could improve performance and scalability significantly.",
            "4": "In addressing the use of different word embeddings, the question is about the rationale behind using GloVe-300-Wiki for the wine dataset and word2vec for the Sentiment140 dataset.",
            "5": "The author is encouraged to explain the reasons for this choice and to provide a comparison between the two embeddings to elucidate their differences and justify their specific applications within the study.",
            "6": "The related work section is weak and does not cover SOTA models.",
            "7": "It needs to be updated with more recent studies.",
            "8": "Figure 1 is noticeably unclear and requires enhancement for better visibility.",
            "9": "The article has many writing errors throughout and needs to be thoroughly corrected."
        }
    },
    "hgDDyoWQt3": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method, Feasibility with Language Model (FLM), which leverages large language models (LLMs) to predict the feasibility of state-object pairs in open-world compositional zero-shot learning (OW-CZSL).",
            "1": "This is a creative application of LLMs, demonstrating their versatility beyond traditional NLP tasks.",
            "2": "**Comprehensive Evaluation**: The authors conduct extensive experiments across three benchmark datasets (MIT-States, UT-Zappos, and C-GQA), comparing their method with existing approaches like GloVe and ConceptNet.",
            "3": "The results consistently show that FLM outperforms these baselines, particularly in challenging open-world settings.",
            "4": "**Detailed Analysis**: The paper provides a thorough analysis of the feasibility prediction in isolation, demonstrating the effectiveness of FLM in distinguishing between feasible and infeasible state-object pairs.",
            "5": "This is supported by both quantitative metrics and qualitative examples.",
            "6": "**Ablation Studies**: The authors perform detailed ablation studies to investigate the impact of different prompt components and the number of in-context examples.",
            "7": "This helps in understanding the key factors contributing to the success of their approach.",
            "8": "**Versatility of FLM**: The proposed method is shown to be versatile, as it can be integrated with any existing vision-language model (VLM) to improve performance in OW-CZSL tasks.",
            "9": "This makes FLM a valuable addition to the toolkit for researchers working on compositional zero-shot learning.",
            "10": "**Use of Multiple LLMs**: The paper evaluates the performance of FLM using various LLMs, including both open-source models (Vicuna, LLaMa-2) and proprietary models (ChatGPT, GPT-4, PaLM-2, Claude-2).",
            "11": "This comprehensive comparison highlights the robustness of the proposed method across different LLMs.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Limited Access to Proprietary Models**: While the paper evaluates several proprietary LLMs, the authors note that they can only obtain binary feasibility scores from these models due to API constraints.",
            "14": "This limits the ability to fully leverage the capabilities of these advanced models, potentially affecting the comparison.",
            "15": "**Dependence on In-Context Learning**: The success of FLM heavily relies on the in-context learning capabilities of LLMs.",
            "16": "While the authors demonstrate the effectiveness of this approach, it may not be as straightforward to implement for other tasks or datasets where relevant in-context examples are not readily available.",
            "17": "**Scalability Concerns**: The paper shows that increasing the number of in-context examples improves performance.",
            "18": "However, this raises questions about the scalability of the approach, especially for datasets with a large number of state-object pairs.",
            "19": "The computational cost and practicality of providing numerous in-context examples need further exploration.",
            "20": "**Generalization to Other Tasks**: While the paper focuses on OW-CZSL, it would be beneficial to see how the proposed method generalizes to other related tasks, such as generalized zero-shot learning or few-shot learning.",
            "21": "This would help in understanding the broader applicability of FLM.",
            "22": "**Threshold Selection**: The method involves selecting a threshold (τ) to determine the feasibility of state-object pairs.",
            "23": "The process of choosing this threshold is not fully detailed, and it may require careful tuning for different datasets, which could be a limitation in practical applications.",
            "24": "**Potential Bias in LLMs**: The reliance on LLMs for feasibility prediction introduces the risk of biases inherent in these models.",
            "25": "The paper does not address how such biases might affect the feasibility scores and the overall performance of the OW-CZSL task.",
            "26": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of open-world compositional zero-shot learning by leveraging the capabilities of large language models.",
            "27": "The proposed FLM method demonstrates strong performance improvements over existing approaches and provides valuable insights through comprehensive experiments and ablation studies.",
            "28": "However, there are some limitations related to the dependence on in-context learning, scalability, and potential biases in LLMs that need to be addressed in future work.",
            "29": "Despite these weaknesses, the paper makes a substantial contribution to the field and opens up new avenues for research in leveraging LLMs for complex vision-language tasks."
        },
        "cs3fKTCUdZ": {
            "0": "1.The paper is well-written and easy to follow.",
            "1": "2.According to the experiments, FLM achieves noteworthy improvement in performance.",
            "2": "1.Time costs need to be taken into account.",
            "3": "As is known to all, the open-word setting will produce a large number of virtual compositions, which will bring a huge amount of calculation to the model (the proposed model process all possible pairs once and predict the score).",
            "4": "2.According to the paper, the In-context Learning seems not to be trained in the process, which means that the performance relies on the LLMs.",
            "5": "However, there existing some objects and states that are totally unknown to LLMs.",
            "6": "In this situation, the proposed model cannot transfer the knowledge to the unknown compositions.",
            "7": "3.The proposed method relies much on the quality of LLMs, and the transferability of the model is not reflected in the paper."
        },
        "VK7PeSRalx": {
            "0": "To the best of the reviewer’s knowledge, this method proposed in this paper is novel.",
            "1": "This paper is clearly motivated and the intuition behind the proposed methods are also very clear.",
            "2": "The idea of using LLMs for solving feasibility conflicts is simple yet quite effective.",
            "3": "The authors also show that as an orthogonal component to existing compositional zero shot learning methods, LLM-guided feasibility calibration can clearly boost the performance for most of the scenarios.",
            "4": "Despite the work’s obvious merit, the idea itself is very simple.",
            "5": "Within the ablations, it would be helpful if the authors are to thoroughly examine more variants of prompts since LLMs output can vary a lot.",
            "6": "The performance variations under such scenarios would be very informative to the community."
        },
        "Mb6pFxVPVl": {
            "0": "Figure 1 is well designed and helps the reader to understand the content.",
            "1": "The proposed method is simple and easy to understand.",
            "2": "The main concern of this work is its contribution.",
            "3": "The paper basically uses the existing LLM to determine the feasibility of a state-object combination.",
            "4": "This only shows that the existing LLM is able to determine the feasibility of a state-object combination, but what is the author’s contribution throughout the process?",
            "5": "Since different threshold will affect the binary classification performance, wouldn’t a metric like ROC curve suits the tasks better?",
            "6": "For Figure 2, it seems that both green and red block only show the feasible (s,o) pairs.",
            "7": "The author is suggested to show some infeasible (s,o) pairs and the model prediction on those infeasible (s,o) pairs.",
            "8": "It is challenging to tell whether GloVe or the proposed FLM separates the feasible and infeasible better by only looking at the figures.",
            "9": "The author is suggested to show some numerical results to support the claim.",
            "10": "In the evaluation metric, the author mentioned that the calibration bias is varied.",
            "11": "Does it mean that different calibration bias is used for different metric?",
            "12": "Typo: such “at” ChatGPT"
        },
        "aQBQ35PlOu": {
            "0": "S1: The studied problem about open-world compositional zero-shot learning is significant important and can apply to the real-world scene.",
            "1": "S2: The large-language models are used to reduce the gap between machines and humans.",
            "2": "S3: Extensive experiments on many prompt variants and six LLMs shows the best performence.",
            "3": "W1: Is this the first paper to solve the CZSL problem by using  the LLMs?",
            "4": "If yes, I am curious about the motivation or some motivation experiments to demonstrate the effectiveness of LLMs?",
            "5": "If no, I tend to see some differents compared with other published related works.",
            "6": "W2: This method in this paper is not novel and performance improvement depends entirely on the language model.",
            "7": "If the language model introduces biases, such as racial discrimination, during training, will this also affect downstream tasks?",
            "8": "W3: Does a more powerful language model perform best in this paper?",
            "9": "In my opinion, simply introducing a language model to solve downstream tasks does not reach the upper limit of ICLR acceptance."
        }
    },
    "71mqtQdKB9": {
        "llm_review": {
            "0": "**Novelty and Innovation**:\n   - The paper introduces a novel discrete score matching loss called score entropy, which is a significant contribution to the field of discrete diffusion models.",
            "1": "This loss is designed to be more stable than existing methods and forms an ELBO for maximum likelihood training, which is a notable theoretical advancement.",
            "2": "**Empirical Performance**:\n   - The proposed Score Entropy Discrete Diffusion models (SEDD) achieve competitive perplexity scores compared to GPT-2, which is a strong baseline in language modeling.",
            "3": "The results show that SEDD models can attain comparable perplexities, often within +10% of GPT-2, and sometimes even outperform it.",
            "4": "**Algorithmic Advantages**:\n   - SEDD models offer several distinct advantages, such as learning a more faithful sequence distribution, the ability to trade off compute for generation quality, and enabling arbitrary infilling beyond the standard left-to-right prompting.",
            "5": "These features provide practical benefits for various applications in natural language processing.",
            "6": "**Theoretical Contributions**:\n   - The paper provides a thorough theoretical foundation for the score entropy loss, including its consistency, gradient behavior, and equivalence to implicit and denoising score matching losses.",
            "7": "The introduction of a discrete Tweedie’s theorem and the derivation of an ELBO for likelihood-based training and evaluation are significant theoretical contributions.",
            "8": "**Scalability**:\n   - The authors demonstrate the scalability of their approach by training SEDD models to GPT-2 model sizes and achieving competitive performance.",
            "9": "The practical implementation details, such as the use of structured transition matrices and efficient computation of transition probabilities, are well-explained and show the feasibility of the approach for large-scale language modeling tasks.",
            "10": "**Comprehensive Experiments**:\n   - The paper includes a comprehensive set of experiments, comparing SEDD models with GPT-2 on various datasets and tasks.",
            "11": "The results are presented clearly, and the authors provide additional samples and ablation studies in the appendix, which adds to the robustness of their findings.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Complexity of the Loss Function**:\n   - The score entropy loss is more complex than standard score matching variants, which might make it harder to understand and implement for practitioners.",
            "14": "While the paper provides a detailed explanation, the complexity could be a barrier to wider adoption.",
            "15": "**Limited Exploration of Noise Schedules**:\n   - The authors mention that they did not systematically explore noise schedules or alternative loss weighting, which could potentially improve sample perplexity and generation quality.",
            "16": "A more thorough investigation of these aspects could strengthen the empirical results.",
            "17": "**Comparison with Other Diffusion Models**:\n   - While the paper compares SEDD with GPT-2 and mentions prior discrete diffusion models, a more detailed comparison with other state-of-the-art diffusion models, especially those that embed tokens into Euclidean space, would provide a clearer picture of the relative strengths and weaknesses of the proposed approach.",
            "18": "**Generative Perplexity Evaluation**:\n   - The evaluation of generative perplexity using GPT-2 Large as a reference model might introduce some bias, as it assumes that GPT-2 Large is a perfect evaluator of language model quality.",
            "19": "Exploring alternative evaluation metrics or human evaluations could provide a more comprehensive assessment of the generated text quality.",
            "20": "**Practical Implementation Details**:\n   - While the paper provides practical implementation details, some aspects, such as the choice of structured transition matrices and the specific architectural improvements, could be elaborated further to help practitioners replicate the results more easily.",
            "21": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in discrete diffusion language modeling with the introduction of the score entropy loss and the development of SEDD models.",
            "22": "The theoretical contributions, empirical performance, and practical advantages make this work a valuable addition to the field.",
            "23": "Addressing the identified weaknesses, such as exploring noise schedules and providing more detailed comparisons with other diffusion models, could further enhance the impact and applicability of the proposed approach."
        },
        "YxrqzpwA9i": {
            "0": "**[S1]** The proposed loss seems intuitively reasonable.",
            "1": "The authors motivate it based on limitations of previous score-network-inspired discrete diffusion methods, and argue why their approach should work better.",
            "2": "~~(Although, a lot of this argument seems to rely on unproven theorems; see [W1] below.",
            "3": ")~~\n\n**[S2]** The generative perplexity results are quite impressive.",
            "4": "It seems that the proposed sampling strategy is Pareto-optimal relative to the fixed GPT-2-small model, when evaluating based on GPT-2-large's perplexity v.s.",
            "5": "number of sampling iterations.",
            "6": "**[S3]** The problem of building better non-autoregressive probabilistically-sound generative models is an important one, and the claimed improvements represent an important step in this direction ~~(although due to [W1], [W3], and [W4] below I'm not convinced they've justified their claims sufficiently in this regard)~~\n\n**[S4]** The authors do a good job connecting this work to previous work on diffusion models, and in particular on drawing connections between their score-matching objective and previous work on continuous score-matching diffusion models.",
            "7": "~~**[W1]**~~ *(addressed in current revision)* The central theoretical claims of this work are incomplete and unsupported, and I am not convinced they are correct.",
            "8": "In particular, although much of the paper is devoted to statements about the new \"score entropy\" and its properties, the proofs are either omitted, incorrect, or only provided in a sketch form.",
            "9": "- Proposition 3.2, which states that their score entropy loss has the right minimum, is never proven.",
            "10": "- Proposition 3.3 and Theorem 3.4, which give alternative forms of the score entropy, have \"proofs\" that are very handwavey and informal.",
            "11": "And I believe these proofs are also incorrect!",
            "12": "The derivations ignore the weights $w_{xy}$ and thus end up proving something different than the intended proposition/theorem.",
            "13": "- The central result, Theorem 3.6, is justified only with a sketch which says to apply the (likely incorrect) trick from the \"proof\" of 3.3 to some unstated result of Campbell et al.",
            "14": "(2022).",
            "15": "This is nowhere near enough detail to reconstruct an adequate proof.",
            "16": "- Theorem 4.2's proof is also a sketch which does not include enough detail for me to verify its correctness.",
            "17": "Additionally, although the introduction claims that one contribution of the work is a \"Langevin corrector framework\", this never appears in the paper.",
            "18": "*Edit: The authors have corrected some small errors in their theorems and added detailed proofs for all of them.",
            "19": "I believe the theoretical claims are justified now, although I'm not familiar enough with stochastic processes to check everything in detail.",
            "20": "*\n\n---\n\n**[W2]** The provided experiments appear to be only preliminary results.",
            "21": "For their SEDD-small model, they \"emphasize that it is still improving\", and for their SEDD-medium model, they state that it \"has not started converging\".",
            "22": "The authors say they will \"update our model results as training progresses\".",
            "23": "My understanding is that work submitted to ICLR is supposed to be feature-complete at the time of submission.",
            "24": "I'm not sure it's appropriate to plan on updating the central results of the submission during the review process.",
            "25": "*Edit: The authors have explained their reasoning below (they meant to pre-emptively reassure reviewers that they could add more comparisons if asked, not to do so unasked).",
            "26": "It still seems a bit strange to include comments directed at the reviewers in a paper submission, especially with results that the paper calls \"preliminary\", since presumably these would always be removed in the final version.",
            "27": "On the other hand, if these statements had simply not been added in the first place, I think the initial results would have still supported the main empirical claims, so perhaps this isn't a big deal.",
            "28": "*\n\n---\n\n~~**[W3]**~~ *(addressed in current revision)* I found the evaluation criteria to be somewhat imprecise, especially in regards to the authors claims that the demonstrate \"for the first time, a non-autoregressive modeling technique that is able to achieve similar perplexity scores as autoregressive modeling\".",
            "29": "The authors claim performance is \"competitive\" with GPT-2-small, but this seems like a subjective statement; the perplexity of their SEDD-small models seems to be a few points higher for everything except the PTB dataset.",
            "30": "They also present results for SEDD-medium, a larger model, which outperform the smaller GPT-2 model.",
            "31": "However, it's not clear that comparing perplexity across model sizes is fair without controlling for the amount of training compute.",
            "32": "The authors additionally reference the Plaid 1B model from Gulrajani & Hashimoto (2023), which had previously shown strong non-autoregressive performance relative to GPT-2-small (albeit with a larger model and more training compute than GPT-2-medium).",
            "33": "That seems to contradict the claim that this work is the \"first time\" non-autoregressive modeling has been competitive with autoregressive modeling.",
            "34": "I would have hoped for a more rigorous set of experimental results here.",
            "35": "For instance, Gulrajani & Hashimoto (2023) give a thorough study of different model scaling law behavior while controlling for training compute; this kind of thing seems necessary to fairly compare with autoregressive methods.",
            "36": "(Perhaps much of the performance of the SEDD models here is due to them being overtrained relative to the GPT-2 models.)",
            "37": "*Edit: The authors have added context for their 10% perplexity gap based on existing continuous diffusion results, added comparisions between their medium model and GPT-2 medium, and clarified that their experimental results are not overtrained with respect to GPT-2.",
            "38": "The new baselines also provide additional supporting evidence.",
            "39": "*\n\n---\n\n~~**[W4]**~~ *(addressed in current revision)* Although motivated as a way to improve upon previously-proposed discrete diffusion approaches, the experiments do not include any discrete diffusion model baselines.",
            "40": "Additionally, the perplexity experiments use different evaluation splits and different evaluation methods from previous works, so the numbers cannot be directly compared to previous works.",
            "41": "The GPT-2-small comparisons may also be confounded by differences in the dataset or number of training iterations used for GPT-2-small.",
            "42": "It is thus difficult to tell how much of the observed gains are due to the new contributions in this work, rather than being due to the training procedure, base model architecture, or evaluation method.",
            "43": "*Edit: Diffusion baselines have been added, using consistent training, architecture, and evaluation setups.",
            "44": "*\n\n---\n\n~~**[W5]**~~ *(addressed in current revision)* The generated samples still seem somewhat incoherent in a qualitative sense.",
            "45": "In particular, I found the \"infilling\" samples in Table 2 to be unimpressive; none of them appear to be meaningful or consistent with the provided prompt tokens.",
            "46": "*Edit: The newer SEDD-medium samples and long-form samples are much more coherent than those in the initial submission.",
            "47": "*"
        },
        "hjuengPMYp": {
            "0": "Discrete diffusion models and especially text-diffusion models are difficult but exiting research topics: as mentioned by the authors much work remains to be done before discrete diffusion models can truly rival state-of-the-art autoregressive models on text generation.",
            "1": "The main weakness of text-diffusion models is their extremely slow training time when compared to (equivalent) autoregressive models.",
            "2": "However their future potential is huge, especially regarding the ability of control they provide.",
            "3": "- I found the paper easy to follow and interesting\n- I find the idea of trying a better -- numerically more stable -- score-matching criterion as proposed in this article interesting\n- The authors also extend the study of (Meng et al.",
            "4": "2022) and provide an ELBO and a denoising variant of their criterion\n- This article may provides a real step toward an improvement of discrete diffusion models - There is undoubtedly a lot of work in this article, but I felt that the scientific impact of this contribution is unclear: the main contribution is to propose a new score-matching loss but I see no theoretical evidence and no experiment, be it on a toy example, showing that a simple \"quadratic score-matching loss\" as in (Meng et al.",
            "5": "2022) would be less efficient than its new \"score entropy loss\" counterpart.",
            "6": "- The paper lacks of a proper ablation study (be it on small datasets)\n- The experiments are only provided on text generation and seem unfinished at submission time (due, I guess, to the huge amount of compute time required to train a medium-size GPT2-like diffusion model)\n- On Table 1, the SEDD-medium results are provided, but the equivalent results for medium-size GPT2 must be provided as well otherwise it could be misleading (I hope this will be fixed at the rebuttal time).",
            "7": "Minor remarks:\n- typo on page 3 equation 7 : \"k\\neq i\" -> \"z \\neq x\"\n- the indices used to write score functions can be confusing to the reader e.g.",
            "8": "$s_\\theta(x)_y$, $s_\\theta(x,t)_j$"
        },
        "GrTzVksQGz": {
            "0": "The proposed criterion is extremely well justified from a theoretical point of view.",
            "1": "The simplified criteria for model scaling are well justified.",
            "2": "The derivations were fun to read and follow.",
            "3": "The experimental results are compelling.",
            "4": "The only weakness is that, while presenting so much detail about the scaling properties of the proposed criterion, the paper omits to explain the unusually complicated form of the criterion itself.",
            "5": "The derivations give wonderful consequences of Eq.",
            "6": "(9), but don't really explain where Eq.",
            "7": "(9) comes from!",
            "8": "This might be relevant because I think there might be a small typo in Eq.",
            "9": "(9).",
            "10": "I am almost able to derive Eq.",
            "11": "(9) by making the assumption that it is a Bregman divergence between s(x,y) and p(y)/p(x), using -log as the convex function, which would totally make sense, because it would guarantee that your score divergence is non-negative, reflexive, and convex in s(x,y); these properties are stated in the paper, but are not proven in the paper, perhaps because they follow naturally from the Bregman divergence.",
            "12": "However, if I derive it in that way, I find one typo in the equation: by that derivation, the last term should not be  (p(y)/p(x))\\log(p(y)/p(x)-1), it should be (p(y)/p(x))(log(p(y)/p(x))-1).",
            "13": "Indeed, my correction seems necessary, because log(p(y)/p(x)-1) will often be taking the logarithm of a negative number, which would be avoided if you instead calculated log(p(y)/p(x))-1.",
            "14": "Notably, this last term in Eq.",
            "15": "(9) is ignored for most of the rest of the paper, since it does not involve s_\\theta(x); it seems to be necessary only for the purpose of shifting the criterion upward so that it is strictly non-negative."
        },
        "NmacgIB5lf": {
            "0": "* The paper is well-motivated and addresses an important area of research that is of interest to the larger community.",
            "1": "* SEDD generalizes score to the discrete domain and improves upon CSM by addressing its limitations (i.e., infinite KL divergence) and satisfies a number of desirable properties that make it suitable for score matching.",
            "2": "* SEDD models achieve competitive metrics compared to GPT-2 on a variety of standard datasets, which suggests the robustness and generalizability of the method.",
            "3": "* The experiment lacks good baselines.",
            "4": "Although the paper claims to improve over concrete score matching, they do not consider CSM in their baseline and only compare the proposed SEDD with an autoregressive model (GPT-2 small).",
            "5": "Moreover, SEDD-medium is compared with GPT-2 small.",
            "6": "* The experiment appears inconclusive or incomplete.",
            "7": "The model is still being trained, and the authors claim that it has not converged yet; the experiment on the 1 billion-word dataset is said to have encountered unexpected errors, without elaboration.",
            "8": "* Certain design choices lack justification.",
            "9": "The SEDD model uses rotary embeddings instead of learned positional embeddings, as in the GPT-2 baseline.",
            "10": "In the absence of ablations, it is unclear how much this decision impacted obtained results.",
            "11": "* Not all variables and notations are clearly specified, making the paper difficult to follow at times."
        },
        "WVXpQ1TFFA": {
            "0": "- Thorough theoretical analysis about the diffusion weighted denoising score entropy.",
            "1": "- Better generation quality than same-scaled GPT-2 - Evaluation is a little bit weak.",
            "2": "Like, lack of comparision between previous discrete or continous diffusion mdoels, including the sampling speed and generation quality.",
            "3": "No specific numbers of  sampling speed (only the caption of Fig 2 mentioned once).",
            "4": "No quantitative evaluation for infilling tasks, just showing some examples.",
            "5": "- Some motivations are not clear.",
            "6": "Section 4 is not well presented.",
            "7": "Why the design of this denoising scheme is needed?",
            "8": "If it is designed for speedup, you need to explain two things: (1) no detailed experiments or ablation study about this strategy (2) some discrete diffusion models can sampling within several steps (like~10), and in such condition, discrete diffusion models already have the advantages over generation speed, so what's the difference between theirs and yours?",
            "9": "- Writing: cictation format (citep and citet) is mixed up."
        }
    },
    "RTL8fWgJaS": {
        "llm_review": {
            "0": "**Innovative Approach**: The concept of self-specialization is a novel extension of self-alignment, focusing on domain-specific expertise rather than general instruction-following.",
            "1": "This is a significant contribution to the field of NLP, particularly in the context of large language models (LLMs).",
            "2": "**Empirical Validation**: The paper provides comprehensive experimental results demonstrating the effectiveness of self-specialization in both biomedical and financial domains.",
            "3": "The results show substantial improvements in zero-shot and few-shot performance, which is impressive.",
            "4": "**Efficiency**: The approach is data and parameter efficient.",
            "5": "The use of QLoRA for fine-tuning with minimal additional parameters is a practical solution that makes the method accessible and scalable.",
            "6": "**Benchmarking**: The paper includes a thorough benchmarking of existing aligned models within specialized domains, highlighting the limitations of general instruction-following models in expert domains.",
            "7": "This sets a clear motivation for the proposed approach.",
            "8": "**Iterative Improvement**: The concept of iterative self-specialization, where the model undergoes multiple rounds of self-alignment to refine its expertise, is a thoughtful addition that shows the potential for continuous improvement.",
            "9": "**Retrieval Component**: Incorporating a retrieval mechanism to enhance response generation with domain-specific knowledge is a smart way to reduce hallucinations and improve the accuracy of the generated data.",
            "10": "**Generalizability**: The method is shown to be applicable to different models and scales, including MPT-30B and Falcon-40B, which demonstrates its versatility.",
            "11": "**Practical Implications**: The approach has practical implications for creating specialized models that can be efficiently deployed in memory, which is valuable for real-world applications.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Scope of Evaluation**: While the paper demonstrates the effectiveness of self-specialization in biomedical and financial domains, it would be beneficial to see results in a broader range of domains to further validate the generalizability of the approach.",
            "14": "**Dependence on Seed Quality**: The quality of the initial seed instructions is crucial for the success of the self-specialization process.",
            "15": "The paper does not thoroughly discuss the potential impact of poor-quality seeds or how to mitigate this issue.",
            "16": "**Retrieval Mechanism Simplicity**: The use of BM25 as the retrieval mechanism is a practical choice, but it is relatively simple.",
            "17": "Exploring more sophisticated retrieval methods could potentially yield better results, especially in more complex domains.",
            "18": "**Iterative Process Complexity**: The iterative self-specialization process, while promising, adds complexity to the training procedure.",
            "19": "The paper could benefit from a more detailed discussion on the computational costs and practical challenges associated with this iterative approach.",
            "20": "**Comparison with Human-Labeled Data**: The comparison with models trained on human-labeled data is limited.",
            "21": "A more extensive comparison could provide a clearer picture of the trade-offs between self-specialization and traditional supervised learning.",
            "22": "**Scalability Concerns**: While the paper shows promising results with models up to 40B parameters, it does not address potential scalability issues when applying self-specialization to even larger models (e.g., 175B or more).",
            "23": "**Lack of Qualitative Analysis**: The paper could benefit from more qualitative analysis of the generated instructions and responses to provide deeper insights into the nature of the improvements brought by self-specialization.",
            "24": "**Potential Overfitting**: There is a risk of overfitting to the domain-specific data, especially with a small number of seeds.",
            "25": "The paper does not discuss strategies to prevent overfitting or ensure that the model retains its generalization capabilities.",
            "26": "#### Conclusion\n\nOverall, the paper presents a compelling and innovative approach to uncovering latent expertise within large language models through self-specialization.",
            "27": "The strengths of the paper lie in its novel methodology, empirical validation, and practical implications.",
            "28": "However, there are areas for improvement, particularly in terms of broader evaluation, addressing potential limitations, and providing more qualitative insights.",
            "29": "Despite these weaknesses, the paper makes a significant contribution to the field and opens up exciting avenues for future research."
        },
        "hu3hs512uX": {
            "0": "- Methodology: The self-specialization technique introduced is both straightforward and powerful.",
            "1": "The authors effectively demonstrate that, by refining the instruction dataset for a target domain, one can substantially elevate the performance of an LLM.",
            "2": "- Results: The outcomes are compelling.",
            "3": "The self-specialized technique not only outpaces the baseline LLM with general instruction tuning across various NLP tasks, but even a 30B self-specialized model can occasionally surpass larger models of 65B capacity.",
            "4": "- Structured Presentation & Clear Communication: This research is characterized by its lucid motivation and coherent narrative, seamlessly bridging the gap between identified issues and the proposed solutions.",
            "5": "The results are presented with clarity, reinforcing the dominance of the introduced methods.",
            "6": "At its core, the proposed method is somewhat an amalgamation of existing concepts.",
            "7": "While termed \"self-specialization\", it essentially contrasts with \"self-alignment\" (for instance, as seen in Alpaca) and incorporates \"domain-specific knowledge-guided generation\".",
            "8": "The enhancements observed are not surprising since the retrieval-augmented generation, grounded on specific knowledge retrieval, naturally offers a more thorough and targeted knowledge base for instruction tuning.",
            "9": "Thus, one could argue that self-specialization is essentially self-alignment augmented with RAG, somewhat constraining its novelty.",
            "10": "Furthermore, rather than relying on RAG to generate the instruction dataset with the foundational model, the authors might have delved deeper into refining the dataset construction process.",
            "11": "Several uncharted avenues remain: (1) devising superior methods for seed generation, considering aspects like topic ratios or instruction diversity; (2) optimizing answer generation for the instructions, with focus on enhancing and validating answer quality.",
            "12": "Regrettably, this research offers only a cursory exploration in these dimensions."
        },
        "TYt3sx7sOB": {
            "0": "The paper is well-motivated, as specializing the language model in a specific domain is an area of interest.",
            "1": "Additionally, the paper is well-structured and easy to follow.",
            "2": "Despite the promising results reported in this paper within the biomedical domain, I still feel uncertain about its contribution for the following reasons:\n1) The experimental results are not convincing.",
            "3": "Table 1 reports the comparative results of the base model and the self-specialized model, with the authors stating \"the scores (F1) witness a rise from 25.15 to 36.63 in a zero-shot setting.\"",
            "4": "However, this is the average result, and the datasets \"BioASQ-Yesno\" and \"Medical Drugs\" contribute the most differences, with a difference of around 60-70 in \"BioASQ-Yesno.",
            "5": "I highly doubt the correctness of the results for this dataset.",
            "6": "This is a binary classification dataset, and the base model performs much worse than random guessing (50.0 if labels are balanced), which is the first odd point.",
            "7": "Secondly, as the number of demonstrations increases, the F1 score of the base model surprisingly decreases.",
            "8": "This implies that the base model must learn information from the prompted demonstrations.",
            "9": "If the model is not performing well on a dataset, the F1 score should remain almost the same or should not exhibit such consistent declines.",
            "10": "I am considering if the authors mistakenly reversed the labels of \"yes\" and \"no,\" leading to these counterintuitive results.",
            "11": "If we exclude the \"BioASQ-Yesno\" and \"Medical Drugs\" datasets, the average F1 score improvement is down to around 2.5, far less than the reported 11.48 (36.63 - 25.15).",
            "12": "This makes doubt not only on the reported results in Table 1 but also on Fig.",
            "13": "4 and its claims of the model's superiority over all 65B models despite its ≈2.2x smaller size.",
            "14": "2) The Gouge-L scores for the \"DDI\" and \"Medical\" datasets are identical to their respective F1 scores.",
            "15": "I haven't investigated if this is possible, but please check the reported results carefully.",
            "16": "3) During the seed generation phase, the proposed method requires the NLP benchmarks in the target domain.",
            "17": "This limits the proposed approach to extend to other domains, as not all domains have this NLP benchmark accessible.",
            "18": "With the paper using 80 seeds, one possible solution could be manually generating them; however, the paper didn't mention this point.",
            "19": "4) How about the results if applying the proposed scheme to a larger model, such as LLaMA-65B?",
            "20": "The base model used in the paper initially shows not good results (20-30 F1 scores), which are relatively easy to improve.",
            "21": "If a larger model with better initial results is used, is it still feasible to have improvement through the proposed scheme?",
            "22": "5) Some places lack professional writing.",
            "23": "For example, in Eq.",
            "24": "(3), $p_{lm}$ is introduced without prior definition.",
            "25": "While the intended meaning may be inferred, a scientific paper should maintain consistency and rigor in its notation and explanations."
        },
        "msMvr8IOkK": {
            "0": "The topic of specialization is important for deploying LLM to a specified domain.",
            "1": "The paper is well-written and easy to follow.",
            "2": "The idea of self-specialization is interesting, which utilizes both seed instructions and generated ones together.",
            "3": "Several experiments are conducted to evaluate the proposed method.",
            "4": "The proposed self-specialization method outperforms the base model and larger LLaMA variants.",
            "5": "The benchmark only includes two aligned models.",
            "6": "Better to include more aligned models for comprehensive benchmarking.",
            "7": "It claims that 'we hypothesize that the model expertise in different domains resides in “superposition” in the model’s parameters and hidden states'.",
            "8": "But there are no further theoretical explanations or experimental results to support this claim.",
            "9": "If the \"superposition\" could be explained in details, the application scope of the proposed method may be more clear.",
            "10": "The metrics of the y-axes should be added in figure 2 and 5."
        },
        "ANdAjxWhid": {
            "0": "- The paper is well-written and easy to follow, providing sufficient technical details.",
            "1": "The authors also mention that the data, code, and trained model will be open source.",
            "2": "- The empirical findings demonstrate that incorporating unlabeled data positively impacts the model's ability to effectively respond to queries within a specialized domain, particularly in the challenging context of biomedical research.",
            "3": "- The proposed method, self-specialization, is an extension of the self-instruct [1] work to recover certain expertise in the LLMs.",
            "4": "However, the method is only designed and evaluated for one specific domain without showcasing its generalization ability to other domains.",
            "5": "- The compared methods in the paper are based on zero-shot/few-shot settings, while the proposed method uses LoRA for finetuning.",
            "6": "It would be beneficial to include stronger task-specific comparison methods to better illustrate the effectiveness of the proposed approach.",
            "7": "- Since the method has multiple components, it would be helpful to show the contribution of each component to the overall performance.",
            "8": "For example, how does domain-specific response generation impact the final results?",
            "9": "- The performance in the knowledge sparse domain is uncertain.",
            "10": "It is understood that domain response generation involving harnessing knowledge is one of the main contributors to the specification, but it is important to understand the method's bottleneck when existing knowledge is limited for certain domains.",
            "11": "- The paper should discuss potential data contamination and address how the authors ensure that the data for downstream testing does not overlap with the generated data.",
            "12": "[1] Wang, Yizhong, et al.",
            "13": "\"Self-instruct: Aligning language model with self generated instructions.\"",
            "14": "arXiv preprint arXiv:2212.10560 (2022)."
        }
    },
    "S1hTxWE7cJ": {
        "llm_review": {
            "0": "**Novelty and Relevance**:\n   - The paper introduces a novel classification challenge, X-Shot, which reflects real-world scenarios where label frequencies vary widely.",
            "1": "This is a significant step forward as it addresses the practical need for systems that can handle frequent, few-shot, and zero-shot labels simultaneously.",
            "2": "**Unified System**:\n   - The proposed system leverages Instruction Learning and data generated by pre-trained Language Models (PLMs) to create a unified framework.",
            "3": "This approach is innovative and shows promise in handling diverse label scenarios effectively.",
            "4": "**Comprehensive Evaluation**:\n   - The authors evaluate their system on three benchmark datasets (FewRel, UFET, and Situation) across different domains and classification paradigms.",
            "5": "This thorough evaluation demonstrates the system's versatility and robustness.",
            "6": "**Performance**:\n   - The X-Shot system consistently outperforms state-of-the-art techniques in both single-label and multi-label classifications.",
            "7": "This is a strong indicator of the system's effectiveness and potential for practical deployment.",
            "8": "**Detailed Methodology**:\n   - The paper provides a clear and detailed explanation of the methodology, including the transformation of classification problems into a unified binary classification framework and the acquisition of supervision for low-shot labels.",
            "9": "This transparency is beneficial for reproducibility and understanding the approach.",
            "10": "**Error Analysis and Insights**:\n   - The authors conduct a detailed error analysis, identifying common error patterns and providing insights into the system's performance.",
            "11": "This analysis is valuable for understanding the limitations and potential areas for improvement.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Complexity and Implementation**:\n   - The proposed system, while innovative, is complex and may be challenging to implement and deploy in real-world applications.",
            "14": "The reliance on instruction tuning and PLMs may require significant computational resources and expertise.",
            "15": "**Dependence on Indirect Supervision**:\n   - The system heavily relies on indirect supervision from a diverse set of NLP tasks.",
            "16": "While this approach is effective, it may not always be feasible to obtain such a wide range of tasks, especially in specialized domains.",
            "17": "**Generalization to Other Domains**:\n   - Although the system is evaluated on three diverse datasets, it is unclear how well it would generalize to other domains or tasks not covered in the evaluation.",
            "18": "Additional experiments on more varied datasets would strengthen the claims of generalizability.",
            "19": "**Handling of Zero-Shot Labels**:\n   - The performance on zero-shot labels, while better than baselines, still shows room for improvement.",
            "20": "The use of GPT-3.5 for generating weakly labeled examples is a clever approach, but it may not always produce high-quality instances, potentially affecting the system's performance.",
            "21": "**Threshold Selection**:\n   - The need for a threshold parameter for label prediction introduces an additional layer of complexity.",
            "22": "The process of selecting the optimal threshold is not fully detailed, and it may vary across different datasets and tasks.",
            "23": "**Comparison with More Baselines**:\n   - While the paper compares the proposed system with several baselines, including state-of-the-art methods, it would be beneficial to include more recent and diverse baselines to provide a more comprehensive comparison.",
            "24": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of text classification by introducing the X-Shot challenge and proposing a unified system to handle labels with varying frequencies.",
            "25": "The strengths of the paper lie in its novelty, comprehensive evaluation, and detailed methodology.",
            "26": "However, the complexity of the system, dependence on indirect supervision, and handling of zero-shot labels are areas that could be further improved.",
            "27": "Despite these weaknesses, the paper makes a valuable contribution to the field and opens up new avenues for research and practical applications."
        },
        "UBkHKcocSx": {
            "0": "-- The problem X-shot learning is a novel one and it is practically useful.",
            "1": "-- The presentation is generally good with comprehensive introduction of background and related works.",
            "2": "-- In section 4.1, it is unclear what the instruction term stand for.",
            "3": "It would be helpful to give some examples in the text or Figure 1 for clarity.",
            "4": "-- It is also unclear how the \"indirect supervision\" works.",
            "5": "Again, including concrete examples in Figure 2 would be helpful.",
            "6": "-- The experimental results do not provide convincing proof that the proposed approach outperform its counterparts.",
            "7": "The comparison may be unfair since varying resources have been used in different methods."
        },
        "t7bJGld9oB": {
            "0": "Originality: The paper introduces a new problem setting for text classification, X-SHOT, which can handle any label occurrence, whether it be frequent-shot, few-shot, or zero-shot.",
            "1": "This is a novel problem formulation according to the authors.",
            "2": "But since I am not well-versed in this line of work, I am not confident about the authors's claim on the originality.",
            "3": "I encourage other reviewers comment on this point.",
            "4": "The paper reframes text classification tasks into a binary classification task, adaptable to any number of labels and occurrences.",
            "5": "I think it is a neat approach that unifies various text classification tasks into a single framework.",
            "6": "It is something I have not read about before, but again I am not an expert in this area, so please refer to other reviewers' comments on this point.",
            "7": "Quality: The paper provides a thorough evaluation of X-SHOT on three benchmark datasets across diverse domains in both single-label and multi-label classifications.",
            "8": "The evaluation demonstrates that X-SHOT outperforms preceding state-of-the-art techniques in FewRel and UFET.",
            "9": "Clarity: The paper is well-organized and easy to follow, with clear section headings and subheadings.",
            "10": "The paper provides detailed explanations of technical terms and concepts, making it accessible to readers with varying levels of expertise.",
            "11": "Significance: The paper's contributions seems to have significant implications for the field of text classification, as X-SHOT provides a unified system that can handle any label occurrence, making it a versatile solution for real-world applications.",
            "12": "* May be over claiming contributions: I have to admit that I have not read a lot of papers in few-shot/zero-shot/one-shot domain.",
            "13": "It seems quite unexpected to me that no one had ever attempted to develop a system that can utilize labels of all frequencies.",
            "14": "After reading this paper, I did some simple google search.",
            "15": "Without going into too much details, I found some articles discussing models that can handle \"N-shot\" labels with N ranging from 0 to some N, albeit N is not claimed to be set to infinity, which seems to be very similar to the \"X-shot\" problem formation.",
            "16": "Since I am no expert in this topic and I certainly do not intend to become on for the purpose of this review, I encourage other reviewers who have more domain knowledge than me to think about this point.",
            "17": "* The fact that the proposed model does not do well on the Situation dataset can be problematic: From table 2 we can see that the proposed model does not do well on the Situation dataset and it certainly did not \"still exceed[s] all other baselines significantly.\"",
            "18": "In particular, I find it hard to be persuaded that the proposed model exceeds NLI (Li et al., 2022) **significantly**.",
            "19": "Can authors elaborate on this?",
            "20": "* Other aspect might worth considerations yet unexplored: For example, for the Situation dataset, GPT3.5 is able to outperform all other models.",
            "21": "However, the authors could have argue that GPT3.5 might be less advantageous because of computational efficiency.",
            "22": "The paper does not provide a detailed comparison of X-SHOT with other state-of-the-art techniques in terms of computational efficiency.",
            "23": "It is unclear how X-SHOT compares in terms of training time and computational resources required."
        },
        "m6JmKANL4g": {
            "0": "The authors propose an interesting challenge that the zero-shot, few-shot and frequent-shot tasks can be trained within one unified framework, which has never been studied before.",
            "1": "The way to leverage the indirect supervision by enabling the model to distinguish the positive and negative classes from marginally tweaked inputs is worth thinking.",
            "2": "The proposed method lacks novelty.",
            "3": "It is unclear which part is more important.",
            "4": "If the pretraining stage on Super-NaturalInstruction Dataset is more critical, it seems unfair to directly compare with those competitor methods which adopts no pretraining stage.",
            "5": "The results seems not good enough, especially on the Situation benchmark.",
            "6": "In order to prove the practical value of the proposed X-shot that the zero-shot, few-shot, and frequent-shot problems can be well-solved by the unified training, I think the final results should be compared with the additional results retrieved by disjoint training.",
            "7": "For example, the zero-shot results retrieved by unify training should be better than that retrieved by only training on all zero-shot tasks."
        },
        "ftPbtDBKKN": {
            "0": "The paper is well-written and easy to follow.",
            "1": "The paper unifies freq-shot, few-shot, and zero-shot labels in the proposed *X-shot* setting.",
            "2": "The ideas of indirect supervision and GPT-supervision are reasonable.",
            "3": "The solution for *X-shot* setting is not new (Sec 4.1).",
            "4": "Converting multi-label classification into binary classifications is a conventional approach in ML algorithms.",
            "5": "While GPT supervision is proposed for zero-shot labels (Sec 4.2), there is no ablation study on the influence of generated instances on freq-shot, few-shot, and zero-shot labels.",
            "6": "The improvement looks marginal compared to previous works (Tab.",
            "7": "2)."
        }
    },
    "4L0xnS4GQM": {
        "llm_review": {
            "0": "**Innovative Approach**:\n   - The paper introduces a novel framework, CHAIN-OF-TABLE, which leverages the tabular structure to express intermediate thoughts for table-based reasoning.",
            "1": "This is a significant advancement over traditional methods that rely on free-form text or code for intermediate steps.",
            "2": "**Dynamic Planning**:\n   - The framework's ability to dynamically plan a chain of operations based on the input table and question is a notable strength.",
            "3": "This allows for a more flexible and adaptive approach to table understanding, which is crucial for handling complex tables and questions.",
            "4": "**State-of-the-Art Performance**:\n   - The proposed method achieves new state-of-the-art performance on multiple benchmarks (WikiTQ, FeTaQA, and TabFact) across different LLMs (PaLM 2 and GPT-3.5).",
            "5": "This demonstrates the effectiveness and generalizability of the approach.",
            "6": "**Detailed Experimental Analysis**:\n   - The paper provides a comprehensive evaluation of the proposed method, including comparisons with various baselines, performance analysis under different operation chain lengths, and table sizes.",
            "7": "This thorough analysis helps in understanding the strengths and limitations of the approach.",
            "8": "**Efficiency**:\n   - The CHAIN-OF-TABLE framework is shown to be more efficient in terms of the number of generated samples required for a single question compared to other methods like Binder and Dater.",
            "9": "This efficiency is crucial for practical applications where computational resources and time are limited.",
            "10": "**Reproducibility**:\n   - The authors have made efforts to ensure the reproducibility of their results by providing detailed descriptions of their methods, including prompt examples and links to the official open-sourced code for baseline methods.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Complexity of Implementation**:\n   - While the dynamic planning and iterative process of CHAIN-OF-TABLE are strengths, they also add complexity to the implementation.",
            "13": "This might make it challenging for practitioners to adopt and integrate the framework into existing systems without significant effort.",
            "14": "**Dependence on LLMs**:\n   - The framework heavily relies on the capabilities of large language models (LLMs) like PaLM 2 and GPT-3.5.",
            "15": "This dependence might limit its applicability in scenarios where access to such powerful models is restricted due to cost or other constraints.",
            "16": "**Scalability Concerns**:\n   - Although the paper demonstrates the framework's effectiveness on large tables, the iterative nature of the approach might raise concerns about scalability when dealing with extremely large datasets or real-time applications.",
            "17": "**Limited Discussion on Failure Cases**:\n   - The paper could benefit from a more detailed discussion on the failure cases or limitations of the CHAIN-OF-TABLE framework.",
            "18": "Understanding where and why the method fails can provide valuable insights for future improvements.",
            "19": "**Generalization to Other Domains**:\n   - While the framework shows impressive results on table-based reasoning tasks, it is not clear how well it would generalize to other domains or types of data.",
            "20": "Further experiments on diverse datasets could strengthen the claims of generalizability.",
            "21": "**User Study**:\n   - A user study or qualitative analysis involving domain experts could provide additional validation of the framework's practical utility and ease of use.",
            "22": "This aspect is currently missing from the paper.",
            "23": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of table-based reasoning with LLMs through the introduction of the CHAIN-OF-TABLE framework.",
            "24": "The innovative approach, strong experimental results, and detailed analysis are commendable.",
            "25": "However, the complexity of implementation, dependence on powerful LLMs, and scalability concerns are areas that need to be addressed in future work.",
            "26": "Additionally, a more thorough discussion on failure cases and generalization to other domains would further strengthen the paper."
        },
        "t9AbVC8TSd": {
            "0": "The proposed method is new in that it adapts the previous chain-of-thought prompting to the table-related tasks, and it improves the understanding of intermediate reasoning results.",
            "1": "Experiments on three benchmarks show the advantages of proposed method over a broad range of baselines including end-to-end methods and other program-aided methods.",
            "2": "The proposed method requires significantly more number of queries to LLMs compared to baselines such as chain-of-thought and Binder, since the method needs to query LLMs twice (one for operation generation and one for arguments generation) for each operation.",
            "3": "However, no comparison is provided for performance under the same number of queries.",
            "4": "For example, what is the performance of baselines when they adopt the self-consistency idea (which is shown to be beneficial in the Binder paper)?",
            "5": "Does the proposed method still have advantage when using the same number of queries?",
            "6": "The pre-defined five atomic operations seem to significantly limit the available operations.",
            "7": "For example, aggregations such as SUM, MAX, MIN, etc.",
            "8": "can be easily done in SQL.",
            "9": "How are these aggregations done in the proposed method?",
            "10": "No explanation is provided for why these five operations are used in the paper and why they cover the complete set of operations on table.",
            "11": "The presentation of argument generation process is not clear.",
            "12": "Based on the prompts in the Appendix, it seems add_column operation directly uses LLMs to generate the new column, whereas other four operations only prompt LLMs to generate arguments that will be fed to the programming language."
        },
        "dTZc7RsELr": {
            "0": "The investigated problem of leveraging and understanding structural data like tables is important and practical while existing LLMs can not solve it well.",
            "1": "The proposed method does not require training (or fine-tuning) of the existing LLMs.",
            "2": "The design of atomic operations is novel and also reasonable.",
            "3": "The overall reasoning procedure of chain-of-table is step-by-step, explainable, and effective.",
            "4": "The paper is solid from a technical perspective, and extensive experiments are conducted.",
            "5": "The presentation and drawn figures are generally clear and easy to understand.",
            "6": "Several case studies are also elaborated on in the Appendix.",
            "7": "The proposed method only achieves marginal improvements in some cases, e.g., TabFact and ROUGE-1/2/L datasets.",
            "8": "I would suggest the paper discuss the potential reasons.",
            "9": "The observations in Figure 4 are quite interesting.",
            "10": "It seems that a longer chain does not consistently bring more accurate results.",
            "11": "What are the underlying reasons for this?",
            "12": "Dater (Ye et al., 2023) should be the most important baseline for comparison.",
            "13": "I would suggest the paper make a further comparison with in-depth analysis from the perspective of methodology, e.g., a comparison between one-step and multi-step reasoning on tabular data.",
            "14": "Besides, how efficient is chain-of-tables when dealing with large-scale data?",
            "15": "It seems that the running-time efficiency is known from the current draft."
        },
        "xyegwnMopa": {
            "0": "The proposed chain-of-table is simple and effective.",
            "1": "This highlights the value of decomposing reasoning in tabular tasks, as opposed to employing single-step table reasoning methods.",
            "2": "The good performance underscores its effectiveness compared to baseline methods across multiple tabular reasoning datasets.",
            "3": "The proposed method is an extension of chain-of-thought to tabular data.",
            "4": "Each reasoning step is constrained by predefined operations on the table.",
            "5": "However, it raises questions about the adaptability of the chain-of-table framework to incorporate new operations or external knowledge, such as contextual information related to the table.",
            "6": "Chain-of-table requires a table, which could be large, in each reasoning step.",
            "7": "This may significantly increase the computational cost.",
            "8": "(See my questions below.)",
            "9": "While it is not necessarily a weakness, it would be beneficial to evaluate the proposed method with an open-sourced model (e.g.",
            "10": "Llama-2) to understand whether the framework can be easily adopted by other models.",
            "11": "One concern is that chain-of-table relies on LLMs' ability to comprehend the defined operations and reasoning chains, and it is uncertain whether other LLMs can seamlessly adapt to these requirements."
        },
        "7z70rzmbzR": {
            "0": "Recently, there has been a lot of interest in developing methods to improve the performance of LLMs on tabular data.",
            "1": "- The paper addresses one of the key stumbling blocks in improving the performance of LLMs on reasoning over tables.",
            "2": "- The paper is well-written and easy to follow.",
            "3": "- Empirical results show that the proposed approach outperforms the baselines on real-world datasets.",
            "4": "One of the issues with the paper is how they evaluate, especially the choice of benchmarks.",
            "5": "I encourage the author to evaluate their approach over a diverse class of tasks, especially tasks like Table summarization, Column type annotation, Row augmentation, etc.",
            "6": "In addition, I also encourage the authors to include open models too."
        }
    },
    "2ov9RiAkxE": {
        "llm_review": {
            "0": "**Novelty and Relevance**: The paper addresses a timely and critical issue in the deployment of large language models (LLMs) within integrated applications.",
            "1": "As LLMs become more prevalent in various domains, understanding and mitigating their vulnerabilities is essential.",
            "2": "**Comprehensive Threat Analysis**: The authors provide a detailed analysis of potential vulnerabilities in LLM-integrated applications, distinguishing between insider and outsider threats.",
            "3": "This thorough characterization of the attack surface is a significant contribution to the field.",
            "4": "**Empirical Validation**: The paper includes extensive empirical evaluations using OpenAI's GPT-3.5 and GPT-4, demonstrating the practical implications of the identified threats.",
            "5": "The use of real-world applications, such as online shopping chatbots, adds credibility to the findings.",
            "6": "**Proposed Defense Mechanism**: The introduction of the Shield API as a defense mechanism is a notable contribution.",
            "7": "The authors outline the properties required for an effective defense and provide both theoretical and empirical validation of Shield's effectiveness.",
            "8": "**Clarity and Structure**: The paper is well-organized, with clear sections that guide the reader through the problem statement, threat models, empirical evaluations, and proposed solutions.",
            "9": "The use of figures and tables enhances the readability and understanding of the content.",
            "10": "#### Weaknesses:\n\n1.",
            "11": "**Limited Scope of Applications**: While the paper focuses on online shopping applications, it would benefit from a broader evaluation across more diverse application domains.",
            "12": "Although the authors mention additional applications in the appendix, a more detailed analysis in the main text would strengthen the generalizability of the findings.",
            "13": "**Assumption of Non-Malicious Users and LLMs**: The paper assumes that both users and LLMs are non-malicious.",
            "14": "This assumption may not hold in all real-world scenarios, where users or LLMs could be compromised.",
            "15": "Addressing potential threats from malicious users or compromised LLMs would provide a more comprehensive security analysis.",
            "16": "**Evaluation Metrics**: The paper uses targeted attack success rate (TSR) as a primary metric for evaluating the effectiveness of attacks.",
            "17": "While TSR is a useful measure, additional metrics such as the impact on user experience, response time, and computational overhead would provide a more holistic assessment of the proposed defense mechanism.",
            "18": "**Cost Analysis**: The cost analysis focuses primarily on token usage.",
            "19": "While this is an important aspect, other costs such as computational resources, latency, and potential impacts on scalability should also be considered.",
            "20": "A more detailed cost-benefit analysis would enhance the practical relevance of the proposed solution.",
            "21": "**Ethical Considerations**: The paper briefly mentions ethical concerns but does not delve deeply into the potential misuse of the identified threats and the proposed defense mechanism.",
            "22": "A more thorough discussion on the ethical implications and guidelines for responsible use would be beneficial.",
            "23": "#### Conclusion:\n\nOverall, the paper makes significant contributions to the understanding and mitigation of vulnerabilities in LLM-integrated applications.",
            "24": "The detailed threat analysis, empirical validation, and introduction of the Shield API are commendable.",
            "25": "However, addressing the identified weaknesses, such as expanding the scope of applications, considering malicious users and LLMs, and providing a more comprehensive cost analysis, would further strengthen the paper.",
            "26": "The work is a valuable addition to the field and provides a solid foundation for future research on securing LLM-integrated applications."
        },
        "OFXG6uXvwH": {
            "0": "+ The study focuses on an interesting and important topic, the potential vulnerabilities in LLM-integrated applications.",
            "1": "+ The service scheme of LLM-integrated applications is clear presented.",
            "2": "- Lack of real-world case analysis\n\nMy first concern is related to threat evaluation.",
            "3": "In my opinion, it would be better and necessary to provide a set of real-world cases for this threat evaluation, rather than simply mentioning \"consider an online shopping application whose chatbot uses GPT-3.5 and GPT-4 from OpenAI\".",
            "4": "Since there is no detailed information about this shopping application provided, I doubt whether it represents a real-world application.",
            "5": "Even if it is, to present the potential threats more effectively, it would be beneficial to involve multiple real-world applications in the evaluation.",
            "6": "- Sending message directly to LLM may break the business model\n\nIn the proposed mitigation, it is mentioned that \"queries from users are also sent to an LLM along with queries refined by the application\".",
            "7": "If I understand this correctly, this approach may break the business model of LLM-integrated applications, as illustrated in Figure 1.",
            "8": "Additionally, it would be helpful to clarify how directly sending messages to the LLM model can prevent the attacks discussed in the threat model, as transmitting more information may increase the attack surface.",
            "9": "- Not clear what is verified in the proposed Shield\n\nDespite the security concerns that may arise with the proposed Shield, it is not clear what exactly the Shield verifies in the proposed defense.",
            "10": "It appears that the Shield only verifies whether the message originates from a user, rather than conducting semantic analysis.",
            "11": "As described in the threat model and shown in Figure 4, an attacker can manipulate the output of the LLM by sending a malicious system prompt, rather than altering the information in the user's message.",
            "12": "Please clarify how such signature verification can effectively address the potential threats described in Figure 4."
        },
        "2vcihgWc09": {
            "0": "The paper proposes an analysis over vulnerability of LLMs 1.",
            "1": "Assessing the vulnerability of LLMs is an important topic.",
            "2": "However, the analysis presented in the paper and the results obtained from those analysis are already  widely known.",
            "3": "The paper is poorly written.",
            "4": "It is extremely difficult to follow.",
            "5": "The problem setting and the proposed attack surfaces are not  well-defined and it is not clear how these attacks are different from the existing attacks proposed for LLMs (e.g., [1]) .",
            "6": "At the very end of the paper, it proposes a defense mechanism which is not talked about at all throughout the paper.",
            "7": "It is also not clear how the proposed defense mechanism is different form existing defenses proposed for LLMs.",
            "8": "[1]Wei, Alexander, Nika Haghtalab, and Jacob Steinhardt.",
            "9": "\"Jailbroken: How does llm safety training fail?.\"",
            "10": "arXiv preprint arXiv:2307.02483 (2023)."
        },
        "mJairL26iQ": {
            "0": "Paper discusses a relevant area of research which might become very important in the near future.",
            "1": "Because of the recent success of LLMs there is a keen interest in integrating all sorts of applications (including chatbots) with LLMs using APIs.",
            "2": "However most people in the industry are still unaware of the potential risks and security threats involved in doing this although they fear that if they are not doing this they might fall behind.",
            "3": "This work can help identify some of these risks and the mitigation steps and as such will be very useful for the industry practitioners to read and implement.",
            "4": "The contribution of the paper is very well articulated.",
            "5": "For example, it is clear that the authors are not focused on the typical risks like hallucination, unwanted content, privacy and bias associated with the LLM response.",
            "6": "These risks have been well studied and also the industry is more aware of these kind of risks.",
            "7": "The authors here are instead focused on insider and outsider threats associated with LLM integration by which  restrictions and policies imposed by OpenAI can be bypassed to achieve an undesired objective.",
            "8": "The paper proposes a simple yet effective method for guarding against upstream and downstream manipulation of user queries using a signing an verification process which ensures that the correct user query is used for prompting and the correct response is received at the user end.",
            "9": "Any semantic perturbations of the user query or LLM response are detected by the Shield system.",
            "10": "This appears to be a novel contribution and can be easily adopted in the industry.",
            "11": "The scientific contribution of this paper is limited except for the defence detection strategy.",
            "12": "However this method also does not involve any ML/DL and uses cryptographic techniques (RSA based).",
            "13": "Having said that, the overall contribution is valuable as it exposes the weakest of an AI based system and helps in defending against attacks on such systems by malicious users.",
            "14": "2.Some of the contributions of the paper like cost analysis are not mentioned in the paper and is available only in the supplemental information.",
            "15": "Not sure if this can be used in the evaluation of the paper as then the paper itself will exceed the content limit.",
            "16": "However a lot of questions which I had after reading the paper was actually answered satisfactorily by the supplemental material."
        },
        "uvLYKMSwIF": {
            "0": "- This paper provides extensive experimental results on various vulnerabilities in LLM-intergrated applcations.",
            "1": "Considering the rapid expansion of such applications, this work focuses on an important problem.",
            "2": "These results could be valuable for the community for building more secure applications using LLMs.",
            "3": "- It characterizes key properties required for reducing vulnerabilties in LLM-integrated applications.",
            "4": "This characterization could potentially be useful for developing solutions in this domain.",
            "5": "- Experimental results shows that the proposed API,  Sheild, provides effective defense to counter the presented threat models in LLM-integrated applications that use GPT-based models.",
            "6": "- While this work provides extensive empirical results on potential vulnerabilities, the novelty of this work on showing the risks in the query-response protocol with LLM compared to existing works on prompt injection is not clear.",
            "7": "- For attack detection, Shield relies on LLM's capability in detecting maliciousness.",
            "8": "It would be interesting to see how this dependency impacts the overall effectiveness of Shield.",
            "9": "Results from different LLMs may provide some insights."
        }
    },
    "3ucOvX8WVu": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel method, Local Fine-Tuning (LoFT), which fine-tunes proxy models on queries similar to harmful ones.",
            "1": "This approach is innovative and addresses the challenge of improving the transferability of adversarial attacks from proxy models to target models.",
            "2": "**Comprehensive Evaluation**: The authors conduct extensive experiments to validate their hypothesis.",
            "3": "They demonstrate that LoFT significantly improves the attack success rate on various target models, including ChatGPT, GPT-4, and Claude.",
            "4": "The results are compelling, showing substantial improvements in attack success rates.",
            "5": "**Detailed Methodology**: The paper provides a thorough explanation of the methodology, including the generation of similar queries, the fine-tuning process, and the evaluation of attack success.",
            "6": "This level of detail enhances the reproducibility of the research.",
            "7": "**Human Evaluation**: The inclusion of human evaluation to assess the true harmfulness of the responses is a significant strength.",
            "8": "It highlights the inadequacy of existing automated metrics and provides a more accurate measure of the attack's success.",
            "9": "**Ethical Considerations**: The authors acknowledge the potential misuse of their research and emphasize their commitment to ethical guidelines.",
            "10": "They also plan to inform the developers of the target LLMs about the nature of harmful responses obtained, which is a responsible approach.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Limited Scope of Target Models**: While the paper evaluates the effectiveness of LoFT on several target models, it primarily focuses on ChatGPT, GPT-4, and Claude.",
            "13": "It would be beneficial to see the approach tested on a broader range of models to generalize the findings.",
            "14": "**Dependency on Target Model Responses**: The success of LoFT relies heavily on the ability to generate similar queries and obtain responses from the target models.",
            "15": "This dependency might limit the applicability of the approach in scenarios where access to target model responses is restricted or costly.",
            "16": "**Evaluation Metrics**: Although the paper introduces human evaluation to assess the attack success rate, the reliance on response rate as a primary metric might still be insufficient.",
            "17": "The paper could benefit from exploring additional automated metrics that better correlate with human judgments.",
            "18": "**Scalability Concerns**: The process of generating similar queries and fine-tuning proxy models might be computationally expensive, especially for large-scale applications.",
            "19": "The paper does not provide a detailed analysis of the computational costs and scalability of the proposed approach.",
            "20": "**Potential for Misuse**: Despite the ethical considerations mentioned, the research inherently poses a risk of being misused to develop more effective adversarial attacks.",
            "21": "The paper could discuss more concrete measures to mitigate this risk, such as controlled access to the fine-tuning data and models.",
            "22": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of adversarial attacks on large language models.",
            "23": "The introduction of Local Fine-Tuning (LoFT) is a novel and effective approach to improving the transferability of attacks.",
            "24": "The comprehensive evaluation and inclusion of human assessments add robustness to the findings.",
            "25": "However, the paper could benefit from addressing the limitations related to the scope of target models, dependency on target model responses, evaluation metrics, scalability, and potential for misuse.",
            "26": "Despite these weaknesses, the research makes a valuable contribution to understanding and improving the robustness of large language models against adversarial attacks."
        },
        "8jptr6UQoP": {
            "0": "- Trendy topic\n- Interesting idea\n- Good performance on specific models\n- Convinced evaluation method (human evaluation) - Limited technical contribution\n- Unclear description\n- More explanations are needed"
        },
        "P3rbUeNCOJ": {
            "0": "This paper studies an important research topic of how to attack LLM without knowing the characterization.",
            "1": "The idea of fine-tuning a proxy model to approximate the target model is easy to understand and technically sound.",
            "2": "The authors provide multiple options for similar query generation and conduct a study of the performance of each method.",
            "3": "Some design details of LOFT are not clearly explained.",
            "4": "For example, what is the attack algorithm in Fig.2 and how would it affect the performance of attack transferability?",
            "5": "Some claims are not supported with convincing results and solutions.",
            "6": "For example, the authors claim they “discover that even when an adversarial attack induces the target LLM to respond to a harmful query, the response does not necessarily contain harmful content”.",
            "7": "However, they don’t provide enough results to support the claim.",
            "8": "Also, it would be nice to dive deep and provide a solution (e.g., a new metric) for the observation.",
            "9": "The performance of LOFT is inconsistent across different models (e.g., the improvement on Claude is only 0.5%), which indicates that the utility of LOFT might be limited.",
            "10": "There seems to be negative impact of fine-tuning the proxy model.",
            "11": "For example, in Table 3, the response rate of Claude and GPT-3.5 generated data on GPT-4 are even worse than the baseline without fine-tuning."
        },
        "TusvvCpKar": {
            "0": "- Jailbreak attacks represent a major threat to the safety of LLMs.",
            "1": "The paper studies an important and challenging problem.",
            "2": "- By assuming a black-box setting, the paper considers a more practical threat model than prior work (e.g., Zou et al.",
            "3": "2023).",
            "4": "- The paper considers and evaluates various designs, including the ways of generating semantically similar prompts and transferring prompts generated based on one LLM to another.",
            "5": "- The proposed attack requires fine-tuning the surrogate LLM for each harmful prompt.",
            "6": "It is unclear how this approach scales up to a large number of prompts.",
            "7": "- The evaluation mainly compares LoFT with the case of no fine-tuning, which seems not very meaningful: as the initial prompts are not optimized, they are likely to have low response rate and attack success rate.",
            "8": "I think a more meaningful comparison would be to compare LoFT with other blackbox jailbreak attacks, and show the number of queries needed to generate successful adversarial prompts.",
            "9": "- The number of harmful prompts used in the evaluation is fairly small (25).",
            "10": "It is suggested to conduct a larger-scale experiments using more diverse prompts."
        },
        "vJlH3VoUX9": {
            "0": "==*== Strengths\n\n+ An efficient Local Fine-Tuning (LoFT) method is proposed to effectively adversarially attack large closed-source commercial models.",
            "1": "+ The research problem is well defined and valuable to the research community.",
            "2": "==*== Weaknesses\n\n- The outcomes of the experiment need to be made more convincing.",
            "3": "- Limited in-depth comparison with state-of-the-art solutions."
        }
    },
    "GDdxmymrwL": {
        "llm_review": {
            "0": "**Innovative Approach**:\n   - The introduction of Corex, which leverages multi-model collaboration, is a novel approach to enhancing the reasoning capabilities of Large Language Models (LLMs).",
            "1": "This method is inspired by human behaviors and incorporates Debate, Review, and Retrieve modes, which is a fresh perspective in the field of NLP.",
            "2": "**Comprehensive Evaluation**:\n   - The paper provides extensive experimental results across four different types of reasoning tasks: mathematical reasoning, commonsense reasoning, symbolic reasoning, and semi-structured reasoning.",
            "3": "This thorough evaluation demonstrates the robustness and versatility of the proposed method.",
            "4": "**Task-Agnostic Framework**:\n   - Corex is designed to be task-agnostic, which means it does not require task-specific designs or iterative processes during the reasoning phase.",
            "5": "This broad applicability is a significant advantage over other methods that may require tailored approaches for different tasks.",
            "6": "**Performance Improvements**:\n   - The results show that Corex consistently outperforms existing methods, including Chain-of-Thought (CoT) prompting, self-consistency decoding, and program-aided language models (PAL).",
            "7": "The performance gains are particularly notable in complex reasoning tasks.",
            "8": "**Cost-Effectiveness**:\n   - The paper highlights the cost-effectiveness of Corex, showing that it achieves superior performance with significantly lower computational costs compared to majority voting-based methods.",
            "9": "This is an important consideration for practical applications.",
            "10": "**Detailed Analysis**:\n   - The paper provides an in-depth analysis of the interaction rounds in the Debate mode, the performance enhancement per review cycle, and the synergies between different LLMs.",
            "11": "This detailed examination helps in understanding the internal workings and benefits of the proposed collaboration strategies.",
            "12": "**Code and Data Availability**:\n   - The authors have made the code and data available, which promotes transparency and allows other researchers to replicate and build upon their work.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Complexity of Implementation**:\n   - While the multi-model collaboration approach is innovative, it may introduce additional complexity in implementation and management.",
            "15": "Coordinating multiple LLMs and ensuring effective communication between them could be challenging in practice.",
            "16": "**Scalability Concerns**:\n   - The paper does not thoroughly address the scalability of Corex when applied to very large datasets or in real-time applications.",
            "17": "The overhead of managing multiple models and their interactions might become a bottleneck in such scenarios.",
            "18": "**Limited Discussion on Limitations**:\n   - The paper could benefit from a more detailed discussion on the limitations and potential drawbacks of the proposed method.",
            "19": "For instance, the impact of model selection on performance and the potential biases introduced by the collaboration strategies are not extensively covered.",
            "20": "**Generalization to Other Domains**:\n   - While the task-agnostic nature of Corex is a strength, the paper does not provide sufficient evidence on how well the method generalizes to domains outside of the evaluated tasks.",
            "21": "Additional experiments on a wider range of tasks and domains would strengthen the claims of broad applicability.",
            "22": "**Dependency on LLM Capabilities**:\n   - The effectiveness of Corex heavily relies on the capabilities of the underlying LLMs.",
            "23": "The paper shows that the performance varies with different LLMs as judges or retrievers, indicating that the method's success is contingent on having access to high-quality models.",
            "24": "**Evaluation Metrics**:\n   - The paper primarily focuses on accuracy as the evaluation metric.",
            "25": "Including other metrics such as precision, recall, and F1-score, especially for tasks like commonsense reasoning, would provide a more comprehensive evaluation of the method's performance.",
            "26": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of NLP by introducing Corex, a suite of strategies for multi-model collaboration to enhance complex reasoning.",
            "27": "The innovative approach, comprehensive evaluation, and demonstrated performance improvements are commendable.",
            "28": "However, the paper could be strengthened by addressing the implementation complexity, scalability concerns, and providing a more detailed discussion on limitations and generalization to other domains.",
            "29": "Despite these weaknesses, Corex represents a promising direction for future research in leveraging collective intelligence and autonomous agents for complex reasoning tasks."
        },
        "2LlrFhQ7PN": {
            "0": "- This paper proposes multi-model collaborations for solving complex reasoning problems, which is an interesting and attractive direction for the current community.",
            "1": "- A fairly diverse set of results across several benchmarks are provided, with several LLMs explored.",
            "2": "The analysis is detailed and provides some insights into the proposed method.",
            "3": "I especially like the analysis for cost-effectiveness.",
            "4": "Code is provided for reproducibility.",
            "5": "- The paper is generally well-written and organized, and most of the content is clear to me.",
            "6": "Code is also provided for reproducibility.",
            "7": "- The proposed three modes are not that novel to me.",
            "8": "There are many existing works that have already explored or at least share similar ideas to the three modes.",
            "9": "I only list several representative works here.",
            "10": "For Debate mode, [1] already explored this setting.",
            "11": "For Review mode, [2] has a similar method.",
            "12": "And finally for Retrieve mode, [3] also shares similar ideas.",
            "13": "I don't think Section 2 Related Works is well-written either.",
            "14": "It only lists related works without thoroughly discussing the relatedness and differences.",
            "15": "Also, I don't see why \"External knowledge and tool utilization\" is that related to this work.",
            "16": "- The performance improvement is not that consistent.",
            "17": "It seems that in most cases, *Corex-Debate* and *Corex-Review-NL* do not perform that well.",
            "18": "Instead, *Corex-Review-Code* and *Corex-Retrieve* seems to be better.",
            "19": "I think it demonstrates the advantage of using PL, which is a well-acknowledged fact in the community.",
            "20": "The paper also does not provide explanations or understandings of why these methods work well or not.",
            "21": "I think it is important to have [1,2,3] as baselines and better explain why *Corex* works as a paradigm of multi-model collaboration.",
            "22": "[1] Du, Yilun, et al.",
            "23": "\"Improving Factuality and Reasoning in Language Models through Multiagent Debate.\"",
            "24": "arXiv preprint arXiv:2305.14325 (2023).",
            "25": "[2] Madaan, Aman, et al.",
            "26": "\"Self-refine: Iterative refinement with self-feedback.\"",
            "27": "arXiv preprint arXiv:2303.17651 (2023).",
            "28": "[3] Yang, Chengrun, et al.",
            "29": "\"Large language models as optimizers.\"",
            "30": "arXiv preprint arXiv:2309.03409 (2023)."
        },
        "Yqcmh3ZuTU": {
            "0": "S1) The coverage of the related work in the paper is very extensive and seems complete.",
            "1": "S2) The paper lists a lot of challenges, which are indeed relevant, including misunderstanding the question, or generating a faulty reasoning process.",
            "2": "Using multiple LLMs, treated as agents, is an exciting direction to explore to address these challenges.",
            "3": "S3) The paper is overall relatively well-written and easy to follow.",
            "4": "S4) The method should be relatively easy to reproduce based on the details provided in the paper.",
            "5": "S5) The results are informative as they cover a large set of tasks and categories.",
            "6": "The additional analysis sheds light on the method's behavior and computational efficiency.",
            "7": "W1) Contribution unclear - It is clear that the Corex variants fare better against the baselines, but it is less clear what is the main contribution of Corex.",
            "8": "All three components are based on ideas that are present in prior work, and Corex does not integrate the components.",
            "9": "So methodologically, it is indeed only a suite of what has been done before.",
            "10": "To make the contribution further complicated, the abstract and the introduction mention (often vaguely) a number of issues, including \"the limitations of its internal representation\", \"limitations in solving reasoning tasks\", \"unreliable answers\", \"think outside the box\", \"prevalent obstacles\".",
            "11": "The three issues that are specifically listed in Figure 1: wrong calculation, misunderstanding the relationship between variables, and codes fail to accurately reflect the problem statement - are actionable, but there is no analysis on whether the improved performance of Corex has a qualitative impact on any of these issues.",
            "12": "The desired features reviewed in Table 1 (e.g., reference free, multiple LLMs) are again different from what the introduction was arguing.",
            "13": "In the other sections, aspects like factuality, task-agnosticity, and reliability are argued for, but again, there was no experiment to validate these claims.",
            "14": "W2) Comparison to baselines - the paper compares against one set of baselines in Table 1, then another set of baselines is referred to in the method section (e.g., Du et al.",
            "15": "2023 for the Debate module), and then the results and the analysis focus on general approaches like CoT and SC.",
            "16": "It is unclear why the other baselines from Table 1 and prior works that designed the individual components are not included in the evaluation.",
            "17": "This is even more important because Corex diverges from prior works to design these components differently (e.g., the Debate component), and it is important to know if this different approach fares better or worse, and why.",
            "18": "W3) Originality - the proposed method reads like a more complete combination of heuristics compared to prior work, but these heuristics are already present in recent methods.",
            "19": "In that sense, it is unclear what is the methodological delta between this method and prior work.",
            "20": "This novelty gap is further blurred by the absence of a clear problem statement in section 1, and the lack of direct comparison to related work in section 2.",
            "21": "W4) Premise - The overall premise of Corex is also confusing.",
            "22": "If LLMs cannot reason reliably (as stated in section 1), then what makes these same LLMs suddenly able to reason in Corex?",
            "23": "Moreover, in some of the Corex variants, like the Debate module, it is unclear what reasoning means exactly - because the design here explicitly opts for a majority voting to suppress reasoning.",
            "24": "The Retriever compares different chains of thought, but whether these are scored based on their reasoning soundness, is not clear.",
            "25": "W5) Result takeaways - The results often say \"our method\" but in fact Corex is a multitude of methods, whose probability of outperforming the baselines is generally around 50% (e.g., table 5 has 4 Corex variants and three baselines).",
            "26": "Moreover, the best Corex variant is largely unstable over the tasks, though the Debate one is typically the weakest, while the Retrieve and Review-code are usually performing better.",
            "27": "The results need a discussion that dives deeper into these distinctions in performance across tasks.",
            "28": "Minor:\n* Retrieve is not a paradigm\n* Footnote 3 - not clear what is meant by the \"nature of commonsense tasks\" - prior works have used code representations to also address these"
        },
        "i2ncMSJAPg": {
            "0": "The idea of using multi-agent collaboration to solve complex tasks is well-motivated and is a promising direction.",
            "1": "The paper is in general well written and easy to follow.",
            "2": "The experimental results show some improvement upon the compared baselines.",
            "3": "The paper lacks technical novelty.",
            "4": "Multi-agent debate for reasoning and complex task-solving, the \"review\" method is also quite similar to recent work on self-reflection and self-refinement, the \"retrieve\" method is intuitively very similar to RAG (retrieval-augmented generation) and the main difference is incorporating it in the multi-agent framework using one agent as the retriever.",
            "5": "The proposed components including debate / review / retrieve are not conceptually very much related.",
            "6": "Instead they seem to be distinct methods.",
            "7": "The authors also use them separately in the experiments without combining or integrating them into a single framework.",
            "8": "The performance improvement over stronger baselines such as CoT-SC(10) is not very significant.",
            "9": "Also, It is unshown whether variants such as CoT-SC(20/30) will lead to different conclusions.",
            "10": "Maybe it's because the CoT-SC(10) baseline consumes similar number of tokens with OpenAI's API?",
            "11": "But the authors did not show the total tokens consumed by different methods.",
            "12": "And if so the comparison with CoT would not be fair enough.",
            "13": "The manuscript lacks analysis of when (or for which kind of tasks) one of the methods among debate / review / retrieve outperform others and why it is the case.",
            "14": "Adding some analysis about this question will bring more insights for the manuscript."
        },
        "eLOyNp1T9B": {
            "0": "--The idea of collaborative language models is very interesting and fairly novel.",
            "1": "--Using the debate structure to guide the interactions is novel and effective.",
            "2": "--The experiments are conducted on many and a variety of benchmarks with different tasks.",
            "3": "--The analysis is interesting and insightful.",
            "4": "--The notation is a bit unclear in some places.",
            "5": "In the very beginning explaining the debate,  what is k?",
            "6": "Please denote this when you explain c^i_t.",
            "7": "c_i is the viewpoint or one step of the reasoning chain?",
            "8": "Or both?",
            "9": "Please make it explicit and use one term consistently.",
            "10": "--I expected the collaboration of multiple models to be based on 5 different LLMs.",
            "11": "The paper uses GPT 3 and 4 and Claude.",
            "12": "It was not clear in the paper how the 5 different opinions were solicited.",
            "13": "Are you using different temperatures or obtaining multiple samples from one LLM and looking at them as different heterogeneous agents?",
            "14": "I see in the experiments that you use different LLMs to play the judge roles but was not sure if that is enough to have a real heterogeneous setting with multiple agents."
        }
    },
    "RIbH5ekQpr": {
        "llm_review": {
            "0": "**Novel Dataset Introduction**:\n   - The paper introduces a novel dataset, IMP, specifically designed to challenge and evaluate vision-language models on the concept of image polysemy.",
            "1": "This is a significant contribution as it addresses a gap in existing vision-language research, which predominantly focuses on descriptive captions.",
            "2": "**Comprehensive Evaluation**:\n   - The authors conduct a thorough evaluation of various state-of-the-art vision-language models on the IMP dataset.",
            "3": "This includes both zero-shot and fine-tuning settings, providing a detailed analysis of the models' performance.",
            "4": "**Diverse Captions**:\n   - The dataset includes a range of captions from descriptive to conceptual, which is crucial for testing the models' ability to understand the multiple semantic dimensions of images.",
            "5": "This diversity is well-illustrated in the examples provided in the paper.",
            "6": "**Detailed Methodology**:\n   - The paper provides a clear and detailed description of the data curation pipeline, including the use of CLIP for image encoding and Google Vision API for web entity search.",
            "7": "This transparency is valuable for reproducibility and understanding the dataset's construction.",
            "8": "**Quantitative and Qualitative Analysis**:\n   - The paper includes both quantitative metrics (e.g., Recall@K, RSUM) and qualitative analysis (e.g., examples of hard captions) to evaluate the models' performance.",
            "9": "This comprehensive approach helps in understanding the strengths and limitations of the models in dealing with image polysemy.",
            "10": "**Insightful Observations**:\n   - The authors make several insightful observations, such as the impact of model size and pre-training data on performance, and the challenges posed by highly conceptual captions.",
            "11": "These observations are valuable for guiding future research in this area.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Limited Scope of Models**:\n   - While the paper evaluates a wide range of models, it does not include some state-of-the-art models like Florence and FILIP due to unavailability.",
            "14": "Including these models could provide a more comprehensive evaluation.",
            "15": "**Dataset Size**:\n   - The IMP dataset, while novel, is relatively small compared to other large-scale datasets used for vision-language pre-training.",
            "16": "This might limit its utility for training large models from scratch and could affect the generalizability of the findings.",
            "17": "**Focus on Retrieval Tasks**:\n   - The evaluation primarily focuses on cross-modal retrieval tasks.",
            "18": "While this is a valid approach, it would be beneficial to include other tasks such as image captioning or visual question answering to provide a more holistic evaluation of the models' capabilities.",
            "19": "**Limited Discussion on Future Directions**:\n   - The paper highlights the limitations of existing models in dealing with image polysemy but provides limited discussion on potential future directions or approaches to address these challenges.",
            "20": "Including more concrete suggestions or preliminary experiments with novel approaches could strengthen the paper.",
            "21": "**Annotation Quality**:\n   - The paper mentions manual annotation and cleaning of captions but does not provide detailed information on the annotation process, such as the number of annotators, their expertise, and the guidelines followed.",
            "22": "This information is crucial for assessing the quality and reliability of the dataset.",
            "23": "**Evaluation Metrics**:\n   - The paper primarily uses Recall@K and RSUM as evaluation metrics.",
            "24": "While these are standard metrics, including additional metrics such as Mean Reciprocal Rank (MRR) or Normalized Discounted Cumulative Gain (NDCG) could provide a more nuanced evaluation of the models' performance.",
            "25": "#### Conclusion:\n\nOverall, the paper makes a significant contribution by introducing the IMP dataset and highlighting the challenges of image polysemy in vision-language models.",
            "26": "The comprehensive evaluation and detailed analysis provide valuable insights into the limitations of existing models.",
            "27": "However, the paper could be strengthened by including a broader range of models, additional evaluation tasks, and more detailed information on the annotation process.",
            "28": "Despite these weaknesses, the paper is a valuable addition to the field and sets the stage for future research on image polysemy in vision-language models."
        },
        "7UjeP7NiIq": {
            "0": "The dataset is novel, and is the first to study image polysemy in the context of image-text matching / retrieval.",
            "1": "The experiment evaluation covers a wide range of models and is sufficient.",
            "2": "I am concerned about the difficulty of the task.",
            "3": "\"Vanilla\" image-text retrieval itself is not straightforward to evaluate, given the high rate of false negative caused by plausible but unrecorded matches [1,2].",
            "4": "The subjectivity of image polysemy amplifies all these difficulties.",
            "5": "I wonder to what degree this task is even _possible_ to evaluate objectively.",
            "6": "For example, it was not obvious to me that the caption and images in Fig 4.",
            "7": "Col1 & Col 4 go together.",
            "8": "Similarly for many of pairs in Fig 3.",
            "9": "The subjectiveness of this benchmark calls for a human accuracy check, given that this benchmark is so heavily based on human judgements of image meaning.",
            "10": "The human judgements of similarity can then be used to create similarity judgements similar to [1,2] or correct false negatives.",
            "11": "Given the dataset construction technique, I'm skeptical whether the evaluations are meaningful given that the captions are abstract enough that many of the might match plausibly match to other images.",
            "12": "This could be corrected by verifying human accuracy on the dataset and modifying the benchmark so that correlation with human accuracy is being assessed instead.",
            "13": "Note that the annotation process _does not_ guarantee this, since the annotation procedure does not exclude the possibility that there are 10-15 other captions somewhere in the dataset that describe the image equally well, and vice versa for any image.",
            "14": "You could do this by using CLIP to rank _all_ the captions in the dataset w.r.t to a particular image (and vice versa), and having humans rerank the top 50 from each cluster.",
            "15": "This could be used to measure both human accuracy, and capture more plausible measurements of human similarity.",
            "16": "[1] ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO\n[2]  Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for ms-coco"
        },
        "2s6D7OA5jw": {
            "0": "- Tackles an important barrier in higher level image-text comprehension\n- Contributes another fairly large scale dataset that can be of use to community - The claim for having higher diversity in captions in this work is slightly problematic because diversity is never clearly defined.",
            "1": "Based on their method, it seems to refer specifically to whatever euclidean distance can be associated with in sentenceBERT's embedding representation, which I am not entirely sure how to interpret.",
            "2": "- Some concerns regarding methodology which I elaborate on in the next section.",
            "3": "- While this work certainly provides a useful benchmark for polysemy, I don't think the contributions of this work sufficiently shed light on any new aspects of the problem that the community was already aware of."
        },
        "qesICnTjcU": {
            "0": "Extensive zero-shot experiments across a large collection of models.",
            "1": "Should prove to be a useful resource for evaluating VLMs.",
            "2": "I didn't feel like both Figure 1 and Figure 3 needed to exist in the main body of the paper because it feels like they are duplicating information.",
            "3": "Not entirely clear why the dataset needed to be based on \"high-quality stock photography from Unsplash\"."
        },
        "QJ7gqzGpuB": {
            "0": "- S1: I think that the research question studied in this paper is significant and interesting.",
            "1": "This benchmark would be really beneficial to the community.",
            "2": "As far as I can tell, no benchmark like this exists.",
            "3": "Previous benchmarks also suffer from losing images over time.",
            "4": "- S2: Overall, both the approach for collecting the dataset and experimental design are sound.",
            "5": "- W1: Analysis of this dataset can be improved significantly from what is reported in Table 1.",
            "6": "My major concern is in the characterization of polysemy, which is not much beyond “non-descriptive”.",
            "7": "For instance, does any kind of image-text correspondence ontology emerge?",
            "8": "Perhaps this needs human classification of (a subset) captions into a pre-defined categories, or automatically clustering captions, etc.",
            "9": "This is especially important since quantitatively MPL2D cannot distinguish noise from diversity.",
            "10": "Finally, I would also like to see further statistics beyond word lengths (word clouds, token/type ratio, etc.",
            "11": "that would lead us to have a clearer picture of how this dataset is more diverse than existing ones.",
            "12": "- W2: The tasks that perform on this dataset are standard image- and text- retrieval tasks with zero-shot and fine-tuning evaluation.",
            "13": "Further, an adaptation of models that address image polysemy does not lead to improved performance (see, e.g., SE models in Table 4).",
            "14": "These experiments are a good start but the paper would be stronger with additional tasks that focus on measuring model’s capability in addressing polysemy such as image captioning generation given a “caption sense”.",
            "15": "- W3: Besides improving the analysis in W1 above, the discussion of image-text datasets can also be further expanded to include additional datasets in the analysis, including RedCaps and SBU captions.",
            "16": "- W4: Clarity on the data collection process is quite obscure (under Table 4).",
            "17": "IMO, it is important to include the details on the database used to retrieve captions (CC3M and CC12M) in the main text so the reader is well-informed about the bias of these captions.",
            "18": "In addition, it is important to include the details on how the authors optimize for diversity and quality control."
        },
        "pf1gBanuVp": {
            "0": "Most prior work seeks to have cleaner and more descriptive captions; it is interesting to see efforts on including more conceptual captions.",
            "1": "The evaluation shows that current models struggle with retrieving more abstract and conceptual captions, which raises an interesting and challenging problem.",
            "2": "**A.",
            "3": "** It seems that the “image polysemy” considered in this paper mainly means: the model should have the ability to match images to both “descriptive” and “conceptual” captions; previous datasets such as COCO contains mainly descriptive captions.",
            "4": "However, I am not fully convinced that the collected datasets have more “conceptual” captions than the web image-text data such as CC3M, since many of the captions in the dataset come from web data.",
            "5": "The MPL2D also does not indicate that the collected dataset is more “conceptual”.",
            "6": "The only advantage of the dataset seems to be that it has multiple captions per image while CC3M/12M does not.",
            "7": "But if the final goal is to teach a model to match an image to descriptive / conceptual captions, then it is not clear why it is necessary to have multiple captions per image for training; as long as CC3M has a lot of conceptual & descriptive captions, then the model can learn to retrieve both types captions.",
            "8": "E.g., say there is a dataset A with 1K images each with 5 captions; suppose a dataset B with 5K images each with 1 caption.",
            "9": "The captions in A and B are identical.",
            "10": "Then I do not see the necessity of training on A if we have dataset B.",
            "11": "In sum, it would be better if the paper could illustrate either a) why / how the dataset has more diverse / conceptual captions than CC3M or b) the importance of having multiple captions per image.",
            "12": "**B.",
            "13": "** The experiments are not very insightful.",
            "14": "While the problem of image polysemy is interesting, the paper simply evaluates/fine-tunes current models with image-text retrieval on the collected datasets.",
            "15": "The take-away conclusion seems to be that the task is hard and larger models perform better.",
            "16": "I would expect more analysis and discussions on why and how studying image polysemy could benefit future vision-language models and how to model such a phenomena.",
            "17": "For example, does explicitly modeling image polysemy benefit other tasks that could require high-level conceptual understanding (e.g., understanding actions, events, memes, etc)?",
            "18": "**C.** For the image-to-text retrieval evaluation, how does this test model’s ability to handle image polysemy?",
            "19": "If I understand it correctly, as each image has 5 matching captions, as long as the model retrieves 1 of the matching captions, then it is counted as correctly retrieved?",
            "20": "Then the evaluation protocol does not test whether the model handles polysemy; if for most images, at least 1 of the captions is “descriptive”, then a model that only “understands” descriptive captions will still score high on image-to-text retrieval.",
            "21": "**D.** For comparing the dataset with CC3M/CC12M on MPL2D, why not treat the collected dataset as a single caption dataset (either downsample to one caption per image or just “duplicate” the images)?",
            "22": "In addition, I am not sure about the takeaway message by comparing MPL2D: is higher MPL2D score better or lower score better?",
            "23": "On the one hand, if there are more diverse / conceptual captions, the score is higher; on the other hand, if the captions are noisier, the score is also higher.",
            "24": "Thus, a lower/higher score could be attributed to these two possible factors and we cannot make a conclusion."
        }
    },
    "dONpC9GL1o": {
        "llm_review": {
            "0": "**Theoretical Insights:**\n   - The paper provides a solid theoretical foundation for understanding why truncation sampling methods, such as nucleus sampling, are effective.",
            "1": "The authors prove that truncation methods can guarantee that all sampled tokens have nonzero true probability, which is a significant contribution to the field.",
            "2": "**Novel Approach:**\n   - The introduction of Basis-Aware Threshold (BAT) sampling is innovative.",
            "3": "By leveraging the softmax bottleneck, the authors propose a more nuanced sampling strategy that does not rely on a fixed threshold, allowing for the rejection of high-probability but low-quality tokens and the acceptance of low-probability but high-quality tokens.",
            "4": "**Empirical Validation:**\n   - The paper includes comprehensive pilot experiments that demonstrate the effectiveness of BAT sampling.",
            "5": "The results show that BAT sampling outperforms traditional threshold-based methods in low-entropy generation tasks, which is a promising finding.",
            "6": "**Practical Implementation:**\n   - The authors address the computational challenges of implementing BAT sampling by proposing an approximation method that reduces the size of the linear program, making the approach more feasible in practice.",
            "7": "**Human and Automatic Evaluation:**\n   - The paper employs both automatic metrics (MAUVE) and human evaluations to assess the quality of generated text, providing a well-rounded evaluation of the proposed methods.",
            "8": "**Weaknesses:**\n\n1.",
            "9": "**Computational Complexity:**\n   - Despite the proposed optimizations, the computational cost of BAT sampling remains high.",
            "10": "The need for solving large linear programs can be a significant barrier to practical implementation, especially for real-time applications.",
            "11": "**Limited Scope of Evaluation:**\n   - The experiments are primarily conducted on GPT-2 models.",
            "12": "While the results are promising, it would be beneficial to see how BAT sampling performs on other state-of-the-art models, such as GPT-3 or PaLM, to generalize the findings.",
            "13": "**Parameter Sensitivity:**\n   - The performance of BAT sampling is highly dependent on the choice of the parameter δ.",
            "14": "The paper does not provide a clear guideline for selecting this parameter, which could limit the method's usability.",
            "15": "**Fallback Mechanism:**\n   - The paper mentions that for certain prefixes and low-entropy sampling parameters, BAT sampling may accept no tokens, leading to computational inefficiencies.",
            "16": "A more robust fallback mechanism or a clearer strategy for handling such cases would strengthen the approach.",
            "17": "**Human Evaluation Bias:**\n   - The human evaluation results show a narrow preference for BAT sampling.",
            "18": "The instructions and context provided to human annotators could influence their judgments.",
            "19": "A more detailed analysis of the human evaluation process and potential biases would add credibility to the findings.",
            "20": "**Conclusion:**\n\nOverall, \"Closing the Curious Case of Neural Text Degeneration\" makes significant contributions to the understanding and improvement of text generation methods.",
            "21": "The theoretical insights and the novel BAT sampling approach are valuable additions to the field.",
            "22": "However, the practical implementation challenges and the need for further validation on diverse models and settings highlight areas for future work.",
            "23": "The paper is a commendable effort that opens new avenues for research in neural text generation."
        },
        "U1ZdwK842i": {
            "0": "The proposed approach is innovative and theoretically-grounded.",
            "1": "This paper brings new insights to the community.",
            "2": "The theoretical concepts discussed in this paper are previously ignored but seems to be important.",
            "3": "Basis-aware sampling is specifically designed for models trained using cross-entropy loss.",
            "4": "However, not all language models meet this criterion.",
            "5": "For instance, LLMs fine-tuned with RLHF do not adhere to this condition.",
            "6": "Theorem 2 and Corollary 2 provide sufficient but unnecessary conditions for proving tokens are in the true support.",
            "7": "Therefore, the induced sampling algorithm may also discard tokens in the support of true distribution, leading to biased sampling.",
            "8": "In Corollary 1, the statement \"threshold-based truncation sampling correctly discards all tokens that are not in the support of p*\" is not precise, as it may also incorrectly discard tokens that are in the support of p*.",
            "9": "A more precise phrasing would be: \"all tokens that are not in the support of p* will be discarded by threshold-based truncation sampling.\"",
            "10": "The relationship between the softmax bottleneck and text degeneration phenomena has not been verified.",
            "11": "It remains unclear whether text degeneration is directly caused by the softmax bottleneck, or if increasing the dimensionality (d) beyond the vocabulary size (v) would effectively resolve the issue."
        },
        "0EYFL0hNoA": {
            "0": "This is a great analysis paper, providing an interesting explanation for why truncation sampling works so well in language model decoding.",
            "1": "The paper's motivation is clear and well-written.",
            "2": "The fact that BAT can determine that some tokens have nonzero true support, even though they are assigned less probability than others which are not in the support of the true distribution, is a surprising and compelling result.",
            "3": "Leveraging the softmax bottleneck is a clever trick here and one that will be unexpected to most readers in NLP.",
            "4": "I expected BAT to be computationally infeasible to run in practice due to its dependence on an LP-solver at each tilmestep of decoding.",
            "5": "However, the speedups in the \"Basis-aware threshold sampling in practice\" (namely, using a decomposition of the softmax matrix and only relying on BAT when a token under the threshold probability is chosen) seem reasonable and compelling, and the amortized cost of 0.1s/token, while slow, is not infeasible for certain classes of applications.",
            "6": "The experiments, although not particularly compelling as a reason to start using BAT sampling in practice, seem reasonable and sufficiently thorough.",
            "7": "In particular, the analysis of performance as more constraints are added back (after the SVD) is very clear.",
            "8": "In contrast, I did not find the \"BAT outperforms all other methods for GPT-2-Large\" paragraph very compelling given that BAT is not the best-performing model on any other model size.",
            "9": "The primary weakness seems to be the performance of BAT compared to other methods.",
            "10": "Despite its theoretical justification, it does not clearly outperform other sampling approaches (Figure 5).",
            "11": "Although there is a preference for BAT to eta-sampling shown in Figure 6 and Table 1, this preference is very slight and the comparison is only between two sampling methods.",
            "12": "However, I do not see this weakness as a legitimate reason to reject the paper, since its main contribution seems to be analysis and theoretical understanding of existing decoding algorithms."
        },
        "rrgW7V8GfO": {
            "0": "- Nice idea and analysis\n- Well written / clear\n- Shines new insights on a well-studied problem and could lead to more promising sampling methods - Results are rather weak, efficacy of the method still remains to be demonstrated (minor)\n- An pseudo-code / algorithm box with the practical implementation of BA is needed in the main paper (minor)\n- Unclear whether the method will help for larger models or for models where the approximation errors (under-estimation / over-estimation) are small (kinda major)."
        },
        "K1uEotYN5O": {
            "0": "* The work offers a theoretical explanation for why certain ad-hoc methods used during language generator decoding work well.",
            "1": "This is a valuable insight to the NLG community\n* The work then develops a sampling algorithm based on this theoretical explanation * The theoretical portion of the paper is at times difficult to understand due to notational choices and lack of specificity (for example, switching between individual token probabilities ).",
            "2": "This is particularly important since the theoretical portion is the main contribution of the work \n* The method does not appear to have empirical performance benefits and is computationally expensive, making it impractical\n* There lacks robust empirical justification of the hypothesis.",
            "3": "Figure 7, which is intended to show that including more of the original optimization problem constraints lead to better results, only consists of 3 points, which hardly feels like enough evidence to claim a “trend”.",
            "4": "* The terminology of the “true” distribution is perhaps misleading.",
            "5": "I personally think that something like the “aggregate” distribution or the “data-generating” distribution would be more accurate\n* A small point: the intro of section 4 has some grammatical errors"
        }
    },
    "6J7WG1jyog": {
        "llm_review": {
            "0": "**Cultural Sensitivity and Localization**: The paper addresses a significant gap in the current landscape of large language models (LLMs) by focusing on the Arabic language and its unique cultural context.",
            "1": "This is a crucial step towards making AI more inclusive and effective for non-English speaking communities.",
            "2": "**Comprehensive Methodology**: The authors present a well-rounded approach that includes incremental pre-training with Arabic texts, supervised fine-tuning (SFT) with native Arabic instructions, and reinforcement learning with AI feedback (RLAIF) using a reward model attuned to local culture and values.",
            "3": "This multi-faceted approach ensures that the model is not only linguistically proficient but also culturally aware.",
            "4": "**State-of-the-Art Performance**: The model, AceGPT, achieves state-of-the-art performance across various benchmarks, including instruction-following, knowledge, and cultural and value alignment benchmarks.",
            "5": "This demonstrates the effectiveness of the proposed methodology.",
            "6": "**Evaluation and Benchmarking**: The paper provides comprehensive evaluations using multiple benchmarks such as Arabic Vicuna-80, Arabic AlpacaEval, Arabic MMLU, EXAMs, and the newly introduced Arabic Cultural and Value Alignment (ACVA) benchmark.",
            "7": "This thorough evaluation adds credibility to the claims of the paper.",
            "8": "**Open-Source Contribution**: The authors contribute to the community by releasing AceGPT and the reward model, which can be beneficial for further research and development in the field of localized LLMs.",
            "9": "**Human Evaluation**: The inclusion of human evaluations, particularly from native Arabic speakers, adds an extra layer of validation to the model's performance, ensuring that it aligns well with human expectations and cultural nuances.",
            "10": "#### Weaknesses\n\n1.",
            "11": "**Limited Vocabulary Expansion**: The paper mentions that the vocabulary used is derived from LLaMA2 and is not expanded further.",
            "12": "This could limit the model's efficiency in encoding Arabic text, potentially affecting its performance in more complex or nuanced tasks.",
            "13": "**Resource Constraints**: The pre-training phase was limited by machine resources, resulting in a relatively small number of tokens being used.",
            "14": "This suggests that the model's potential might not be fully realized, and there could be room for improvement with more extensive training.",
            "15": "**Evaluation Gaps**: The paper does not conduct reasoning, misinformation, and bias testing.",
            "16": "These are critical aspects of evaluating LLMs, especially when considering their deployment in real-world applications.",
            "17": "The absence of these evaluations leaves some questions unanswered about the model's robustness and reliability.",
            "18": "**Safety and Alignment Concerns**: The authors acknowledge that the model is not yet suitable for online deployment due to safety alignment issues.",
            "19": "This is a significant limitation, as it restricts the model's practical application and adoption.",
            "20": "**Quality and Quantity of Cultural Dataset**: While the paper introduces a new cultural and value alignment benchmark, the authors note that there is room for improvement in both the quality and quantity of the questions.",
            "21": "This could impact the model's ability to generalize and perform well in diverse real-world scenarios.",
            "22": "**Dependence on GPT-4 for Responses**: The methodology relies heavily on GPT-4 for generating localized responses.",
            "23": "While this ensures high-quality outputs, it also raises questions about the model's independence and the potential biases introduced by GPT-4.",
            "24": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the localization of large language models for the Arabic language.",
            "25": "The comprehensive methodology, state-of-the-art performance, and thorough evaluation make it a valuable contribution to the field.",
            "26": "However, there are notable limitations, particularly in terms of vocabulary expansion, resource constraints, and the absence of certain evaluations.",
            "27": "Addressing these weaknesses in future work could further enhance the model's effectiveness and applicability."
        },
        "cm8ykwwI0y": {
            "0": "The authors organize the paper clearly and describe in details on how to tune an LLM to work better on a specific language Arabic.",
            "1": "From the evaluation, tuning improves metrics on both human and auto evaluations.",
            "2": "I think the main weakness is novelty.",
            "3": "Though novel application should be also considered an contribution, I do not think the paper provides many insights on LLM in Arabic.",
            "4": "It seems to just follow the common techniques continue training/SFT/RLAIF."
        },
        "qzS5qBWX8e": {
            "0": "This paper is well-written and easy to follow.",
            "1": "Localization on specific culture is an important topic in LLMs.",
            "2": "This paper mentioned Arabic-related dataset and models, which can be useful for the people in related fields.",
            "3": "The theoretical and technical contributions are poor.",
            "4": "This paper is more like a engineering report to introduce how to localize a public LLM on Arabic, illustrating the operation and dataset during pre-training, instruction tunning and RLHF stage.",
            "5": "All the methods are well-known.",
            "6": "The findings are intuitive, using localize data to pre-train, instruction tuning and training RLHF can be helpful for better localization.",
            "7": "It seems more suitable for an empirical NLP conference rather than a learning conference.",
            "8": "The newly introduced Arabic Cultural and Value Alignment benchmark is not novel.",
            "9": "Previous work [1] also introduce a benchmark to measure the culture bias in Arabic LLMs.",
            "10": "It seems the authors didn’t mention and cite this paper.",
            "11": "The analyses are insufficient.",
            "12": "I will be more excited to see a comprehensive and systematic evaluation on the contribution of different operation, i.e., pre-training, instruction tuning and RLHF, on different evaluation perspective, such as instruction-following, knowledge accuracy, culture alignment, etc.",
            "13": "[1] Having Beer after Prayer?",
            "14": "Measuring Cultural Bias in Large Language Models, https://arxiv.org/abs/2305.14456"
        },
        "2pdheArqie": {
            "0": "* Pre-training with Arabic data seems to improve performance on MMLU and ACVA benchmarks, proving the utility of native language data.",
            "1": "* An interesting analysis of preference for certain cultural contexts in Language model responses.",
            "2": "A dataset for studying cultural alignment the same has been created, which is a novel and useful contribution.",
            "3": "* Better performance than JAIS, comparable to ChaptGPT.",
            "4": "* Ablation studies show the utility of pre-training with Arabic data and RLAIF (which improves both instruction following and localization) * While the paper makes an interesting contribution to an improved Arabic LLM, it does little to advance the study of building/adapting LLMs for non-English languages.",
            "5": "Most of the methods are well known.",
            "6": "A few studies can help draw broader lessons on the localization of LLMs: \n    * How much pre-training data is required?",
            "7": "What is the best data balance between English and other languages?",
            "8": "* Does the English performance get impacted due to the finetuning?",
            "9": "* How objective is the ACVA benchmark?",
            "10": "Different annotators (even amongst Arabic annotators) themselves might have different viewpoints on Arabic culture.",
            "11": "On  subjective questions, what is the inter-annotator agreement on the answers?",
            "12": "* Evaluation using automatically translated datasets can be problematic (MMLU and EXAM).",
            "13": "* Some human judgment on translation quality should be provided to understand the quality of the benchmark.",
            "14": "* You could consider other translation models: GPT4 vs NLLB or any available Arabic-specific NMT model that might be better than GPT4."
        }
    },
    "3TO3TtnOFl": {
        "llm_review": {
            "0": "**Innovative Approach**: The introduction of binary token representations (BTR) is a novel approach to address the inefficiencies in retrieval-augmented language models.",
            "1": "By using 1-bit vectors, the authors significantly reduce the computational load during inference, which is a critical bottleneck in current systems.",
            "2": "**Comprehensive Evaluation**: The paper evaluates BTR on five diverse knowledge-intensive NLP tasks, demonstrating its effectiveness across different domains.",
            "3": "This comprehensive evaluation strengthens the validity of the proposed method.",
            "4": "**Efficiency Gains**: BTR achieves impressive efficiency gains, with up to 4x speedup in inference and over 100x reduction in storage requirements.",
            "5": "These improvements are substantial and can have a significant impact on the deployment of large language models in real-world applications.",
            "6": "**Mitigation of Accuracy Loss**: Despite the potential loss of accuracy due to binarization, the authors introduce calibration techniques and training objectives that help restore performance.",
            "7": "The reported results show that BTR maintains over 95% of the original task performance, which is a commendable achievement.",
            "8": "**Detailed Methodology**: The paper provides a detailed description of the methodology, including the calibration techniques, training objectives, and compression methods.",
            "9": "This level of detail allows for reproducibility and a clear understanding of the proposed approach.",
            "10": "**Scalability**: The authors discuss the scalability of BTR, particularly its application to larger corpora and models.",
            "11": "This forward-looking perspective is valuable for future research and practical implementations.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Complexity of Implementation**: While the paper provides a detailed methodology, the implementation of BTR involves several complex steps, including calibrated binarization, offline compression, and runtime compression.",
            "14": "This complexity might pose challenges for practitioners looking to adopt the method.",
            "15": "**Limited Comparison with State-of-the-Art**: Although the paper compares BTR with several baselines, it would benefit from a more extensive comparison with other state-of-the-art methods in retrieval-augmented language models.",
            "16": "This would provide a clearer picture of where BTR stands in the broader landscape of NLP research.",
            "17": "**Focus on Encoder-Decoder Models**: The paper primarily focuses on encoder-decoder models and briefly mentions the challenges of extending BTR to decoder-only models.",
            "18": "A more in-depth exploration of these challenges and potential solutions would be beneficial for a broader application of BTR.",
            "19": "**Impact on Long Queries**: The paper notes that BTR's speedup is relatively smaller for longer queries.",
            "20": "This limitation is acknowledged but not thoroughly explored.",
            "21": "Further analysis and potential solutions for handling long queries would strengthen the paper.",
            "22": "**Storage Overhead for Variance Information**: While the storage overhead for variance information is described as negligible, the paper does not provide quantitative details on this aspect.",
            "23": "Including specific numbers would help in understanding the overall storage requirements more comprehensively.",
            "24": "**Potential for Further Optimization**: The paper mentions that BTR can be further optimized for larger models and extremely long input queries.",
            "25": "However, it does not provide concrete strategies or preliminary results in these areas.",
            "26": "Including such information would make the paper more robust and forward-looking.",
            "27": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of retrieval-augmented language models with the introduction of BTR.",
            "28": "The method's ability to drastically reduce computation and storage requirements while maintaining high task performance is impressive.",
            "29": "However, the complexity of implementation and the need for further exploration in certain areas are notable weaknesses.",
            "30": "Addressing these issues in future work could enhance the applicability and impact of BTR in the NLP community."
        },
        "V3uNytBy7T": {
            "0": "For the first time, the authors construct binary token representations to improve the efficiency of retrieval augmentation models, an approach that has never been explored before.",
            "1": "The paper is very well-written and well-organized and provides clear motivation, background, and some technical details for the proposed model, including model quantization and binarization.",
            "2": "Experiments are conducted on 5 datasets and provide meaningful comparisons with existing retrieval-enhanced language models such as Atlas and DensePhrase and large language models such as LLaMA2-7B.",
            "3": "The results show that the proposed model can maintain 95% of the above task performance while improving the inference speed and greatly reducing the storage space.",
            "4": "The paper also includes ablation studies to analyze the performance further.",
            "5": "As the paper mentions, BTR is difficult to apply to decoder-only models, and most current large language models utilize a decoder-only structure.",
            "6": "The paper only discusses FiD, which is the SOTA model in the KiLT ranking.",
            "7": "However, it is also important to discuss other commonly used retrieval-augmented structures, such as RAG, particularly when dealing with black-box LLM.",
            "8": "For retrieval augmented language models, the number of retrieved passages is an important factor for performance and efficiency.",
            "9": "While \nauthors only set this factor to a fixed number (40 or 30 for different datasets), thus additional experiments should be involved."
        },
        "RWNoVwdv9g": {
            "0": "Storage savings of 100x.",
            "1": "This will make retrieval more available to many, since storage can often become a bottleneck in retrieval-augmented ML.",
            "2": "Speed savings of 2-4x.",
            "3": "Although this is similar speed improvement as Deformer (Cao et al), it leads to 5 percent accuracy loss which is worse than Deformer's 1 percent accuracy loss.",
            "4": "Extensive experiments and analysis.",
            "5": "There are some confusing or missing details, but these can be probably be easily fixed.",
            "6": "There are many steps required to make BTR work well.",
            "7": "Of course, that is also why this is a valuable paper since it outlines what these steps are.",
            "8": "There is a substantial (about 5 percent) drop in model accuracy.",
            "9": "It's not clear whether 2-4x speed boost is enough value to make up for this, although the storage improvements are definitely very valuable.",
            "10": "The paper is not very self-contained.",
            "11": "It seems like the reader is expected to have read Cao et al and Atlas papers very closely.",
            "12": "This is not ideal.",
            "13": "For example, it is hard to understand what it means by \"decomposed model\" or \"decomposed reader\" in sec 3.2.",
            "14": "Also, the reader is left to infer that retrieval is done at the passage level, but passages are incorporated at the token-level.",
            "15": "(minor) The paper is hard to read.",
            "16": "In more than one instance, a method is used but is \"defined below\", so we must constantly revisit parts of the reading to get a full understanding.",
            "17": "(medium) \"BTR is more accurate and efficient than other baseline methods\" This claim can easily be interpreted as \"all other baselines\", which is not accurate.",
            "18": "There are baselines that are more accurate or more efficient.",
            "19": "This should probably be revised to be more accurate/specific.",
            "20": "Also, I am not sure why BTR base is constantly bolded in Sp column when it is not the best value.",
            "21": "There is not a good breakdown of efficiency between retrieval + inference.",
            "22": "Perhaps alternative methods akin to DistilBERT will give a substantial speed boost plus keep good performance.",
            "23": "(minor) Similarly, given the emphasis on storage it would have been helpful to see some basic baselines to improve storage.",
            "24": "Although I am not sure what else can easily achieve 100x savings without larger tradeoff in performance.",
            "25": "(very minor) It was confusing whether the token distillation is taken directly from Cao et al, or is something new."
        },
        "9ZlXTDc5zn": {
            "0": "* Importance and relevance of topic: LLMs are everywhere and retrieval augmentation LMs addresses critical problems in LLMs such as hallucination, staleness, and privacy leaks, but suffer from low inference speed and huge storage requirements.",
            "1": "* BTR has much lower storage footprint than other approaches, is more scalable and has lower inference speed at the expense of a \"modest\" loss in performance.",
            "2": "* The paper reports numbers for actual throughput for a more realistic comparison of efficiency across different systems.",
            "3": "* The code will be publicly available.",
            "4": "* \"BTR is more accurate and efficient than other baseline methods.",
            "5": "\": According to Figure 3, the proposed BTR appears to be on the Pareto front.",
            "6": "I.e., it isn't substantially better than existing methods but provides a different tradeoff between speed and accuracy.",
            "7": "This makes it hard to assess the merits of BTR.",
            "8": "To make the result less sensitive to a specific operating point and better comparability, it would be interesting to show, for example, how the tradeoff changes with the resolution of the representation (1-bit vs b-bit representations).",
            "9": "Furthermore, adding sub-optimal points to the plots would give a more comprehensive picture (for example, LLaMA at different size/speed/accuracy).",
            "10": "* The benefits at runtime come with a clearly more complicated training pipeline and increased training time.",
            "11": "How much?",
            "12": "* Figure 3: Why do the plots include Atlas-Q but not the original model (Atlas)?",
            "13": "Also, I can't see any small points for Atlas-Q and LLaMA2-7B.",
            "14": "* Different spelling in title and text: \"retrieval augmented language model\" vs \"retrieval-augmented language model\".",
            "15": "* Funsion-in-Decoder → Fusion-in-Decoder"
        },
        "LOpL1iFVYo": {
            "0": "The strengths are as follows:\n* Retrieval augment language models are becoming increasingly popular.",
            "1": "Increasing the inference speed and reducing the memory footprint of such methods will be quite useful.",
            "2": "The authors demonstrate that they can achieve a 4x speedup in inference speed and a 100x reduction in storage space.",
            "3": "* The authors present results across multiple datasets.",
            "4": "* The authors ablate all of their modifications and show how each one affects performance, speed and memory.",
            "5": "The weaknesses are as follows:\n* Some things like passage representation regularization are mentioned in passing and it would be helpful if the authors added a couple sentences providing context and explaining what this is.",
            "6": "* No motivation or explanation is given for why distillation is required/helpful.",
            "7": "* It is not clear if the linear projection layer for passage representation recovery is a learnable layer.",
            "8": "Also, the $L_{recovery}$ term is confusing; $b_i$ is the binary passage representation and $h_i$ is the original passage representation, where is the projection layer used?",
            "9": "* No comparison to newer methods like Lumen."
        }
    },
    "KzMMv0OygD": {
        "llm_review": {
            "0": "**Innovative Approach**:\n   - The paper introduces a novel method for generating high-quality instruction-tuning data by leveraging human-written texts.",
            "1": "This approach addresses the common issues of hallucination and noise in LLM-generated data, which is a significant advancement over existing methods.",
            "2": "**Comprehensive Methodology**:\n   - The proposed method involves a two-phase process: creating a meta-training set using ChatGPT and then training a task generator and discriminator using LLaMA 2-7B.",
            "3": "This dual-phase approach ensures the generation of coherent and aligned tasks, which is a notable improvement over previous methods.",
            "4": "**Quality Control**:\n   - The inclusion of a task discriminator to filter out invalid tasks is a strong point.",
            "5": "This step helps in maintaining the quality of the generated data by reducing hallucinations and ensuring logical consistency.",
            "6": "**Extensive Evaluation**:\n   - The paper provides both automated and manual evaluations to validate the quality of the generated dataset.",
            "7": "The results show that the TEG-INSTRUCT dataset performs competitively in both in-domain and out-of-domain tasks, which is a testament to its robustness.",
            "8": "**Scalability**:\n   - The method is designed to be scalable, with the potential to generate vast amounts of instruction-tuning data.",
            "9": "This scalability is crucial for practical applications and further research.",
            "10": "**Detailed Analysis**:\n   - The paper includes a thorough analysis of the generated data, including instruction diversity and relevance to the original documents.",
            "11": "This level of detail helps in understanding the strengths and limitations of the dataset.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Complexity**:\n   - The proposed method is quite complex, involving multiple stages and components (e.g., meta-training set creation, task generator, task discriminator).",
            "14": "This complexity might make it challenging for other researchers to replicate the results without significant resources.",
            "15": "**Dependence on ChatGPT**:\n   - The initial phase of creating the meta-training set relies heavily on ChatGPT.",
            "16": "While this reduces labor costs, it also introduces a dependency on a proprietary model, which might not be accessible to all researchers.",
            "17": "**Evaluation Metrics**:\n   - While the paper uses a variety of evaluation metrics, some of them (e.g., Bert-Score) might not fully capture the nuances of instruction-following capabilities.",
            "18": "Including more diverse and task-specific metrics could provide a more comprehensive evaluation.",
            "19": "**Generalization to Other Domains**:\n   - The paper demonstrates the effectiveness of the method on specific datasets and domains.",
            "20": "However, it remains to be seen how well the method generalizes to entirely different domains or more complex tasks that require deeper understanding and reasoning.",
            "21": "**Manual Evaluation**:\n   - The manual evaluation process, although thorough, is time-consuming and might not be scalable for larger datasets.",
            "22": "Automating parts of this process or developing more efficient evaluation methods could be beneficial.",
            "23": "**Potential Biases**:\n   - The method might inherit biases from the human-written texts and the LLMs used in the process.",
            "24": "Addressing these biases and ensuring fairness in the generated data is an important aspect that could be explored further.",
            "25": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of instruction-tuning data collection.",
            "26": "The proposed TEG-INSTRUCT method effectively addresses common issues in existing approaches and demonstrates strong performance in both in-domain and out-of-domain tasks.",
            "27": "However, the complexity of the method, dependence on proprietary models, and potential biases are areas that could be improved.",
            "28": "Future work could focus on simplifying the methodology, ensuring broader accessibility, and addressing biases to further enhance the quality and applicability of the generated data."
        },
        "DFn5hpKkwq": {
            "0": "- The proposed method is scalable and the released dataset is large-scale.",
            "1": "- The paper is generally well-written and organized.",
            "2": "Almost everything is clear to me.",
            "3": "- Thorough automatic and human evaluations over a range of instruction-tuning datasets on various downstream tasks.",
            "4": "The analysis for the curated *Teg-Instruct* is also comprehensive.",
            "5": "- I'm not so convinced by the experiment results.",
            "6": "It is not surprising that *Teg-Instruct* achieves the best performance for in-domain tests.",
            "7": "For out-of-domain experiment results, the performance is not that satisfactory.",
            "8": "It.",
            "9": "is understandable for different output distributions.",
            "10": "Instead, what is the performance on some general benchmarks aside from the in-domain and out-domain evaluations, such as LongForm and Alpaca Eval?",
            "11": "- Not a strict weakness, just wondering why it is called \"text-grounded task design\"?",
            "12": "I'm a little confused by the term \"task design\".",
            "13": "In my view, \"task\" usually refers to well-defined task formats, such as QA, IE, etc."
        },
        "J8IzE2Eer6": {
            "0": "- This paper proposes a text-grounded instruction-tuning dataset with up to 270k examples, and the method is scalable.",
            "1": "- The method trains a discriminator to filter out low-quality data, which can alleviate the problem of hallucination.",
            "2": "- The motivation of this paper is intuitive.",
            "3": "- The contribution of this paper is a little bit limited, and the differences compared to [1] and [2] are not very distinct.",
            "4": "- There are some concerns regarding the evaluation of the proposed method and dataset.",
            "5": "In terms of automated evaluation, using the test set generated in this paper, TEG-Instruct, as the testing dataset might be somewhat unfair.",
            "6": "Additionally, including evaluation results on more benchmarks can enhance the robustness of the article.",
            "7": "[1] \"Self-Alignment with Instruction Backtranslation\".",
            "8": "Li et al.",
            "9": "[2] \"Longform: Optimizing instruction tuning for long text generation with corpus extraction.\"",
            "10": "Köksal et al."
        },
        "HF4KaKw9R3": {
            "0": "+ The paper provides extensive and thorough experimentation on the data quality.",
            "1": "Particularly, The human evaluation on the intrinsic quality of the instruction tuning datasets is valuable.",
            "2": "+ The instruction tuning datasets will be released and will be valuable to the community.",
            "3": "- The improvement from the dataset seems incremental compared with previous datasets and methods especially in terms of downstream evaluation with out-of-domain data.",
            "4": "- The data creation process is complicated with many rules not clearly specified."
        },
        "BI0UVD8N77": {
            "0": "The introduced method can generate more Instruction tuning samples at a lower cost.",
            "1": "The paper states in Figure 1 that human-written text contains noise.",
            "2": "However, this is a rare case.",
            "3": "If the authors think this is a common problem, then all the human-annotated tasks are pointless.",
            "4": "The author uses examples generated by ChatGPT as training samples to train a Llama-2-7B model to generate new training samples.",
            "5": "I think the training samples generated in this case will have more noise.",
            "6": "The paper states in Figure 1 that the lack of input field is detrimental to LLM’s generalization capability in diversity tasks.",
            "7": "There is no verification of it, no explanation in other papers, and no ablation experiments.",
            "8": "This method has almost no improvement in out-of-domain datasets compared to other baselines, and the in-domain experiment is almost meaningless because other baselines are all in the setting of out-of-domain when dealing with the dataset proposed in this paper.",
            "9": "This is an unfair comparison.",
            "10": "In addition, the article lacks the number of training data for other baselines."
        }
    },
    "rhp5PDNOgf": {
        "llm_review": {
            "0": "**Novelty and Innovation**: The paper introduces a novel adaptive scheduling strategy called Spaced Scheduling Training (SST), inspired by the spaced repetition learning method used by humans.",
            "1": "This approach is innovative in the context of instruction tuning for large language models (LLMs) and addresses the issue of performance degradation often observed with instruction tuning.",
            "2": "**Empirical Validation**: The authors provide extensive empirical evaluation and statistical analysis to demonstrate the effectiveness of SST.",
            "3": "The results show that SST yields better performance than random sampling and other pruning and scheduling baselines, particularly in reasoning tasks.",
            "4": "**Balanced Performance**: One of the key contributions of SST is its ability to yield more balanced performance across all subcategories of the tested benchmarks.",
            "5": "This is a significant improvement over existing methods that often excel in specific areas but perform poorly in others.",
            "6": "**Reduction of Catastrophic Forgetting**: The paper highlights that SST reduces catastrophic forgetting, a common issue in instruction tuning where the tuning process erases the knowledge acquired during pre-training.",
            "7": "This is an important contribution as it ensures that the model retains valuable pre-trained knowledge while improving its instruction-following capabilities.",
            "8": "**Comprehensive Evaluation**: The authors evaluate their method on a wide range of capabilities and benchmarks, including code, commonsense reasoning, word knowledge, reading comprehension, math, MMLU, and BBH.",
            "9": "This comprehensive evaluation provides a robust validation of the proposed method.",
            "10": "**Ablation Studies**: The paper includes ablation studies to show the benefit of each main component of the SST algorithm.",
            "11": "This helps in understanding the contribution of each component to the overall performance improvement.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Compute Overhead**: One of the limitations of the proposed approach is the compute overhead induced by the evaluation process performed before every learning epoch.",
            "14": "While the authors argue that this overhead is offset by the number of training samples dropped, it remains a potential drawback, especially in scenarios where the training data is of high quality and most examples are useful.",
            "15": "**Warm-up Phase and Learning Rate Scheduling**: The paper mentions an interplay between the warm-up phase of SST and learning rate scheduling, which led the authors to disable the latter for their experiments.",
            "16": "This could be seen as a limitation as it may not generalize well to other training setups where learning rate scheduling is crucial.",
            "17": "**Scalability to Larger Models**: The experiments are conducted on models with at most 13B parameters.",
            "18": "It is not clear if the proposed approach would scale effectively to larger models such as LLaMA-2 70B.",
            "19": "This is an important consideration given the trend towards increasingly larger models in the field.",
            "20": "**Handling of Negative Impact Datasets**: The paper acknowledges the issue of the negative impact of some datasets when dealing with a large set of IFT datasets.",
            "21": "While the authors suggest that including an evaluation signal in the dropping mechanism might alleviate this issue, this remains an area for future work and is not addressed in the current study.",
            "22": "**Complexity of Implementation**: The proposed method involves several components, including spaced repetition algorithms, dynamic data pruning, and stratified sampling.",
            "23": "This complexity might make it challenging to implement and tune in practice, especially for practitioners who are not experts in these areas.",
            "24": "**Generalization to Other Modalities**: While the authors suggest that SST can be applied to other modalities and model architectures, the current study is limited to LLMs and text data.",
            "25": "Further experimentation is needed to validate the effectiveness of SST in other contexts.",
            "26": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of instruction tuning for large language models through the introduction of Spaced Scheduling Training.",
            "27": "The method shows promising results in improving reasoning capabilities and reducing catastrophic forgetting.",
            "28": "However, the compute overhead, scalability to larger models, and complexity of implementation are areas that need further exploration.",
            "29": "The paper provides a solid foundation for future work in adaptive scheduling strategies for deep learning training."
        },
        "QhelRsXvHS": {
            "0": "Motivating the technique with relevant psychology literature.",
            "1": "Applying the technique to instruction tuning LLMs, which is a research\n  topic that is attracting considerable attention.",
            "2": "Conducting an ablation analysis on the components of the proposed algorithm.",
            "3": "* Discussion of related work.",
            "4": "For example, spaced scheduling for deep\n  learning has been considered in **Hadi Amiri et.",
            "5": "al**:\n  *Repeat before Forgetting: Spaced Repetition for Efficient and Effective\n  Training of Neural Networks (ACL 2017, see page 2404)*\n\n* The proposed method does not appear to be motivated with a memory model; compare\n  **Amiri et al.",
            "6": "** or **https://arxiv.org/pdf/1602.07032.pdf**, both works seem to motivate\n  their proposals based on a memory model.",
            "7": "* In my opinion, the empirical part should have at least a comparison to\n  another spaced scheduling method (compare **Amiri et al.**).",
            "8": "* In my opinion, it is hard to conclude if one should use the proposed method\n  or some other online scheduling approach.",
            "9": "For example, there is prior\n  relevant work on automated curriculum learning, see for example **Kreutzer et.",
            "10": "al**: *Bandits Don’t Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits (ACL 2021)*.",
            "11": "While I am **not necessarily** advocating direct comparison\n  to the algorithm of **Kreutzer et al.",
            "12": "**, I think the empirical part would be\n  more solid by having a comparison to one or two additional approaches\n  that schedule the data dynamically."
        },
        "IGUKQw8dfa": {
            "0": "- The introduction seems interesting.",
            "1": "The analogy drawn to human learning processes is quite inspiring.",
            "2": "- The author performed an ablation study to show the benefit of each main component of the proposed algorithm.",
            "3": "- Some parts of the writing are ambiguous.",
            "4": "I think it is better to provide a representative concept figure.",
            "5": "- The experimental results are not so good.",
            "6": "Stating that there is an overall performance improvement seems risky because there was a performance gain in four out of seven evaluations.",
            "7": "- While there are numerous hyperparameters within the proposed algorithm, the impact of their variations on performance has not been analyzed.",
            "8": "Some ablation studies on hyperparameters, e.g., $s_t$ and $\\rho_0$, seem necessary.",
            "9": "This is necessary to determine whether this method is insensitive to hyperparameters.",
            "10": "- There are often instances where explanations are missing.",
            "11": "For instance, it would be advisable for the authors to explain why the formula for the minimum score threshold based on competency is $z_t \\leftarrow z_{max} - \\kappa -1$."
        },
        "nrQrogXnoS": {
            "0": "- The paper is well-written and clearly presented; \n- The paper tackled the important problem of data mixture & curriculum  learning of instruction tuning for large language model and proposed a novel method, \"space scheduling\", quantitive results on 4 benchmark suite show the effectiveness of the proposed methods versus baseline or random mixture methods; \n- Detailed ablation and qualitative examples w.r.t other scheduling variants have been presented to show the effectiveness of the proposed \n- It is great to show the proposed methods are based on LoRA, which offers extra accessibility to large research community; - There is a missing comparison in Table 1 versus Tulu as mentioned in 4.2 for the effectiveness of MERCURY versus original Tulu paper.",
            "1": "Besides, MT-Bench or other human-involved evaluations might also be good to show the comprehensive effectiveness of the proposed methods; \n- Another concern of the proposed method is that it seems the benefits are enlarged for Math/Code with both MERCURY / Space Scheduling.",
            "2": "However, when adding OpenOrca dataset only will contribute to that effect should be ablated; \n- The scalability of the proposed method is questionable but it is great to show the purposed methods are based on LoRA;"
        },
        "pf64plMlP5": {
            "0": "Spaced repetition consistently improves accuracy across all 5 benchmarks and 2 models, demonstrating robust gains.",
            "1": "The paper provides an ablation study for three components of the algorithm, and it seems that all 3 improve performance\n3.",
            "2": "The paper spends time clearly outlining their precise algorithm I overall would appreciate better contextualization/analysis of the method to prove that it's improving upon our current understanding/practice of fine-tuning.",
            "3": "Baselines: Though it is encouraging that this model improves over vanilla fine-tuning, this is not the only work in improving data quality/curriculum for fine-tuning.",
            "4": "In terms of static pruning methods, one naive baseline is to filter sentences with too high or low perplexity (similar to high/low difficulty), as done [for pretraining](https://arxiv.org/abs/2309.04564).",
            "5": "In terms of active learning methods, [Data diets](https://arxiv.org/abs/2306.03208) performs a very similar algorithm to the one in this work, dynamically pruning based on a notion of sample importance (the related work of this paper also provides other references).",
            "6": "I believe the paper should reflect this in two ways.",
            "7": "- The related work and contextualization of the current paper make it seem that this problem has not been studied before, which can be misleading.",
            "8": "Better contextualizing the work in terms of prior research in this area will help highlight the novel contributions by this paper.",
            "9": "- Though the results provide an ablation study, it is unclear where prior work lies in this spectrum.",
            "10": "Regardless of the motivation, this paper is providing a new method, and its important to contextualize its gains with respect to prior work.",
            "11": "Though there are too many baselines to evaluate all, I would appreciate seeing a reasonable baseline to confirm that along some axis, this work pushes along various fine-tuning tradeoffs.",
            "12": "Overhead: This algorithm should induce extra time overhead since examples have to be scored, and \"currently difficult\" examples may have to go through the model multiple times.",
            "13": "The authors should report the time taken by both algorithms to evaluate this slowdown so one can evaluate whether this accuracy improvement is worth the additional cost.",
            "14": "Connection to spaced repetition in psychology: From reading the paper, it is not clear to me how connected the algorithm is to spaced repetition learning for humans.",
            "15": "According to the paper, spaced repetition says that \"brains retain information more effectively when we learn in multiple, spread-out sessions\".",
            "16": "However, the actual algorithm proposed does not do this, and frames example selection under dynamic filtering based on example difficulty.",
            "17": "Even if the analysis in Section 5.3 implies that the method is implicitly setting a curriculum, it is not clear to me how this connects to spaced repetition.",
            "18": "I wonder what value the psychological motivation provides in the context of this work, and if there is a connection I'm missing in this regard.",
            "19": "I am happy to adjust my score provided some further analysis along these axes."
        }
    },
    "NddKiWtdUm": {
        "llm_review": {
            "0": "**Novel Training Paradigm**: The paper introduces a novel training paradigm that allows language models (LMs) to learn from simulated social interactions.",
            "1": "This approach is innovative and addresses the limitations of current training methodologies that rely on static datasets and lack social context.",
            "2": "**Scalability and Efficiency**: The proposed method, Stable Alignment, is highlighted as being more scalable and efficient compared to existing methodologies like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF).",
            "3": "This is a significant advantage, especially for large-scale applications.",
            "4": "**Comprehensive Evaluation**: The authors provide a thorough evaluation of their method across multiple benchmarks, including Anthropic HH, Moral Stories, MIC, ETHICS-Deontology, and TruthfulQA.",
            "5": "This comprehensive evaluation demonstrates the robustness and generalizability of the proposed approach.",
            "6": "**Human Evaluation**: The inclusion of human evaluations to assess the alignment of the generated responses adds credibility to the results.",
            "7": "The use of human annotators to compare the performance of Stable Alignment with other methods provides a practical perspective on the model's effectiveness.",
            "8": "**Detailed Methodology**: The paper provides a detailed description of the SANDBOX simulation environment and the Back-Scatter protocol used for simulating social interactions.",
            "9": "This level of detail is beneficial for reproducibility and understanding the underlying mechanics of the proposed approach.",
            "10": "**Ablation Studies**: The authors conduct ablation studies to assess the contributions of different training stages in Stable Alignment.",
            "11": "This helps in understanding the importance of each component and the overall effectiveness of the training framework.",
            "12": "**Ethical Considerations**: The paper addresses ethical considerations related to the representation of societal norms and the potential biases in the simulated interactions.",
            "13": "This is an important aspect, given the impact of AI systems on society.",
            "14": "#### Weaknesses\n\n1.",
            "15": "**Limited Real-World Validation**: While the simulated social interactions provide a controlled environment for training, the lack of real-world validation is a limitation.",
            "16": "The effectiveness of the model in real-world scenarios, where social norms and interactions are more complex and dynamic, remains uncertain.",
            "17": "**Language and Cultural Bias**: The experiments and analyses are conducted primarily in English, which limits the generalizability of the findings.",
            "18": "The paper acknowledges this limitation but does not provide a concrete plan for extending the approach to other languages and cultural contexts.",
            "19": "**Static View of Societal Norms**: The SANDBOX simulation assumes a static view of societal norms, which may not accurately reflect the evolving nature of social values.",
            "20": "This could limit the model's ability to adapt to changing societal expectations over time.",
            "21": "**Complexity of Implementation**: The proposed approach involves multiple stages and a sophisticated simulation environment, which may be challenging to implement and replicate.",
            "22": "The complexity of the methodology could be a barrier for wider adoption.",
            "23": "**Dependence on Pre-trained Models**: The approach relies on pre-trained language models, which may already contain inherent biases.",
            "24": "While the simulation aims to mitigate these biases, the initial quality and alignment of the pre-trained models could influence the final outcomes.",
            "25": "**Evaluation Metrics**: The paper uses a variety of evaluation metrics, but the alignment scores and accuracy measures may not fully capture the nuanced aspects of social alignment.",
            "26": "More qualitative assessments and user studies could provide additional insights into the model's performance.",
            "27": "**Potential for Overfitting**: The use of simulated interactions for training raises the risk of overfitting to the specific scenarios and feedback mechanisms used in the simulation.",
            "28": "The model's ability to generalize to unseen and diverse real-world interactions needs further investigation.",
            "29": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the training of socially aligned language models through simulated social interactions.",
            "30": "The novel approach, comprehensive evaluation, and detailed methodology are commendable strengths.",
            "31": "However, the limitations related to real-world validation, language and cultural bias, and the complexity of implementation need to be addressed in future work.",
            "32": "The proposed framework has the potential to improve the social alignment of AI systems, but further research is required to ensure its robustness and applicability in diverse real-world contexts."
        },
        "ESYYxy8Xlf": {
            "0": "The paper proposes a quite original approach to alignment, using social interactions between multiple LMs as a data-generating process for finetuning.",
            "1": "It’s ambitious and inspiring.",
            "2": "I think multi-agent experiments with LMs studying norm establishing, norm following and social learning are interesting on their own and the paper (specifically, the Sandbox environment) is a great contribution to this area.",
            "3": "The paper is well-written.",
            "4": "Despite complexity of the method, it’s relatively easy to understand.",
            "5": "The experiments are quite extensive and well-designed.",
            "6": "The results look promising: significant improvements can be seen on multiple relevant benchmarks.",
            "7": "I don’t think the authors have enough evidence to claim that “their approach is considerably more scalable” (from the abstract).",
            "8": "I’m a bit concerned that the method does not scale with model size (page 4) and I didn’t see experiments showing scaling wrt society size.",
            "9": "If it doesn’t scale, Stable Alignment is significantly less promising as an alignment method.",
            "10": "(It’s still interesting as an LLM capabilities analysis though.)",
            "11": "I’m not entirely convinced how important is the social aspect of the data-generating process.",
            "12": "It seems to be just an extension of [language model cascades](https://arxiv.org/abs/2207.10342) line of work, including [imitational learning from language feedback](https://arxiv.org/abs/2303.16755) (ILF) or [constitutional AI](https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback) (CAI), where LMs give feedback on LMs’ initial responses which are then refined.",
            "13": "The agents are weakly individuated in the proposes framework, i.e.",
            "14": "(as far as I understand) they start the same and and only differ in their conversation histories.",
            "15": "I wonder how important is treating multiple calls to the same LM as simulating different agents.",
            "16": "Does it improve alignment compared with a single-agent society, more similar to ILF or CAI?",
            "17": "Is there a scaling plot?",
            "18": "(I might’ve missed this, I’m happy to corrected by the authors.)",
            "19": "Relatedly, I think [language model cascades](https://arxiv.org/abs/2207.10342) line of work (including [imitational learning from language feedback](https://arxiv.org/abs/2303.16755) (ILF), [constitutional AI](https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback) (CAI), [critiques](https://arxiv.org/abs/2206.05802), [Reflexion](https://arxiv.org/abs/2303.11366), [STaR](https://arxiv.org/abs/2203.14465)) could be discussed briefly in Related work.",
            "20": "I’m also happy to be corrected if the authors think I’m wrong seeing their paper as a multi-agent extension of this line of work."
        },
        "of63zCaAER": {
            "0": "The proposed method is interesting and conceptually novel, especially the sandbox idea.",
            "1": "The experiment is extensive (although contains some issues that I am concerned with, which I will discuss later)\n\n3.",
            "2": "The writing is clear.",
            "3": "The main issue that I am concerned with is the relation between this work and RLAIF[1].",
            "4": "It is also trying to use AI feedbacks to align the model.",
            "5": "The main difference between the proposed method and RLAIF is that RLAIF still follows the paradigm of RLHF and only replace human feedback with AI feedback.",
            "6": "And the proposed method is building up a sandbox of multiple AI.",
            "7": "I am wondering how much difference in performance will be brought by this difference.",
            "8": "[1]Lee, Harrison, et al.",
            "9": "\"Rlaif: Scaling reinforcement learning from human feedback with ai feedback.\"",
            "10": "arXiv preprint arXiv:2309.00267 (2023)."
        },
        "RszNx5d8Re": {
            "0": "-The proposed alignment method is novel, and takes inspiration from recently published research.",
            "1": "The human evaluation results confirm experimental results on relevant benchmarks.",
            "2": "Relevant related work is cited.",
            "3": "The paper is well written, and graphics help with understanding.",
            "4": "-The proposed methodology is complicated compared to existing alternatives, but ablation shows that each component is critical in the effectiveness.",
            "5": "I would assume the proposed method generalizes to other (larger) LMs, but additional experimental results would be helpful.",
            "6": "-I might overlook this but an analysis of the results on chatgpt will be helpful to identify future improvements.",
            "7": "-Experiments can be extended to more human value relevant datasets such as mmlu and civil comments.",
            "8": "-See questions below."
        },
        "AyJOmWekX0": {
            "0": "The proposed Contrastive Preference Optimization is new, and is shown to be able to effectively train LMs to learn from both positive samples (well-aligned responses) and negative ones.",
            "1": "This hints at how data collection procedure in the future may not always need to focus on collecting high-quality, socially aligned data only, which can be costly and time-consuming.",
            "2": "The authors provided extensive analysis across six benchmark tasks with seven related baselines to show that Stable Alignment can generate/choose more socially aligned responses.",
            "3": "This is helpful especially because the pipeline is autonomous: it does not require human involvement for additional annotation.",
            "4": "The authors claim the proposed Stable Alignment addresses limitations from (e.g.)",
            "5": "RL from a reward model, which \"may be inherently imperfect and not fully capture nuances of human judgment\".",
            "6": "However, since the data collection process (i.e., the sandbox) mainly prompts LLMs to gather feedback and ratings, how are these not imperfect or can be guaranteed to fully capture nuances of human judgement?",
            "7": "There is a lack of analysis or discussion on the noises that could come from gathering data from LLMs, which is arguably critical for the proposed method to succeed and can be seen as limitation of how far this approach can go.",
            "8": "2. the sandbox data creation procedure appears to be an extension to prompt *multiple* LLM agents to provide rating and feedback for a single agent, instead of just prompting a single LLM teacher to gather data.",
            "9": "There is a lack of experiments showing the advantage of using \"multiple\" LLM agents, and whether if this is even necessary.",
            "10": "If not, many relevant work ([1]-[3]), which also did learning to generate feedback/critiques/improvement from a single teacher model, and is not discussed in this work.",
            "11": "This limits the novelty and usefulness of the proposed \"sandbox\" construction and the following supervised training procedure."
        },
        "D6fIS8mYzn": {
            "0": "The language in the paper is mostly clear and easy to read.",
            "1": "The general concept of being inspired by social interactions is interesting, and there is the possibility of being able to train an aligned model without the need to host an online reward model.",
            "2": "However, there are many details in the execution that cast doubt on the learning method’s efficacy.",
            "3": "Additionally, the claim of simulating human society with no supporting references from the social science literature is too grandiose.",
            "4": "There are extensive experiments on multiple datasets.",
            "5": "Human annotators were used for evaluating the final output rather than simply using a language model as a shortcut.",
            "6": "> However, this method often yields models susceptible to adversarial attacks, like “jailbreaking prompting” (Subhash, 2023; Xu et al., 2021), due to limited exposure to misaligned data during training (Amodei et al., 2016).",
            "7": "To address this, a more advanced technique, “reward modeling” has been proposed (Leike et al., 2018; Christiano et al., 2017).",
            "8": "Subhash, 2023 uses ChatGPT, a model that has likely been trained with a reward model, as an example of a vulnerable model and appears to contradict the second sentence stating that reward modeling addresses the vulnerabilities of SFT-trained models.",
            "9": "The claim of simulating human society with no supporting references from the social science literature is way too grandiose.",
            "10": "The authors should improve the accuracy of their claim.",
            "11": "It is somewhat unclear how the OpenAI models fit into the picture; are 99 of the 100 models all simply one of the 3 OpenAI models and one of them the Stable Alignment model?",
            "12": "If so, this seems like a form of distillation of the preferences/values from the OpenAI model, which was purportedly trained with SFT and RLHF.",
            "13": "Given that this method claims to do away with the need for RLHF/RMs, experiments showing its efficacy should not rely on models trained with such methods.",
            "14": "In other words, the current experiments do not convince me that Stable Alignment can replace RLHF on its own.",
            "15": "For example, I would be much more convinced if all the agents were initialized from non-RM based models.",
            "16": "Relatedly, while convenient, using the OpenAI API models for critical parts of the experiments makes them essentially non-replicable and rather non-reproducible, given the constantly changing model in the backend.",
            "17": "An additional nitpick is that the method claims to deviate from SFT-like approaches but uses an SFT-trained model as the starting point.",
            "18": "I would like to see this addressed as well; what is the performance of Stable Alignment without using an SFT-trained model as the initialization?",
            "19": "Regarding CPO, since it is a new technique that claims to improve the efficacy of the overall method, there should be an ablation where it is replaced with the regular SFT loss or other alignment algorithms, such as perhaps DPO.",
            "20": "More details on the procedure and exactly which models and types of social agents were used to generate data for each learning stage are needed; this is still unclear to me after reading through the details and appendices multiple times.",
            "21": "Are the numbers of examples for each stage in Fig 2 the total over all iterations, or per iteration?",
            "22": "How many iterations were needed to arrive at the final model?",
            "23": "Was the model trained on data from all three societies at each stage or were 3 separate models trained?",
            "24": "If the latter, which model was used in the final evaluation?",
            "25": "Use of HH-A as an estimate of adversarial prompt robustness:\nI went through the examples in the HHH dataset (there are typos where the last H is missing throughout the paper) and disagree with the use of the appended “misaligned” response as a dataset good enough for adversarial robustness evaluation.",
            "26": "A better evaluation of adversarial prompt robustness would be to use a library such as [garak](https://github.com/leondz/garak) with known adversarial prompts used by the community."
        }
    },
    "ygxTuVz9eU": {
        "llm_review": {
            "0": "**Innovative Approach**:\n   - The paper introduces a novel method for detecting dirty samples in datasets by leveraging visual-linguistic inconsistencies.",
            "1": "This approach is innovative and leverages the capabilities of multimodal large language models (MLLMs) to address a critical issue in data-centric AI.",
            "2": "**Comprehensive Framework**:\n   - The proposed Versatile Data Cleanser (VDC) framework is well-structured, consisting of three consecutive modules: visual question generation (VQG), visual question answering (VQA), and visual answer evaluation (VAE).",
            "3": "This modular approach ensures a thorough examination of the data.",
            "4": "**Generalization Capability**:\n   - VDC demonstrates strong generalization capabilities across various types of dirty samples, including poisoned samples, noisy labels, and hybrids of both.",
            "5": "This is a significant improvement over existing methods that often struggle with generalization.",
            "6": "**Extensive Experiments**:\n   - The paper provides extensive experimental results on multiple datasets (CIFAR-10, ImageNet-100, and ImageNet-Dog) and various types of dirty samples.",
            "7": "The results consistently show that VDC outperforms existing methods in terms of true positive rate (TPR) and false positive rate (FPR).",
            "8": "**Detailed Analysis**:\n   - The paper includes a detailed analysis of the impact of different types and numbers of visual questions, as well as the effect of using different MLLMs.",
            "9": "This thorough analysis helps in understanding the robustness and limitations of the proposed method.",
            "10": "**Open Source Code**:\n   - The authors have made the code available on GitHub, which promotes transparency and allows other researchers to replicate and build upon their work.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Dependence on LLM and MLLM**:\n   - The effectiveness of VDC heavily relies on the performance of large language models (LLMs) and multimodal large language models (MLLMs).",
            "13": "Any limitations or biases in these models could affect the performance of VDC.",
            "14": "The paper acknowledges this but does not provide a detailed mitigation strategy.",
            "15": "**Computational Complexity**:\n   - The computational complexity of VDC is relatively high due to the inference requirements of LLMs and MLLMs.",
            "16": "While the paper discusses this, it would benefit from a more detailed analysis of the computational costs and potential optimizations.",
            "17": "**Limited Scope of Dirty Samples**:\n   - VDC is designed to detect dirty samples with corrupted labels.",
            "18": "However, it may not be effective for detecting other types of data issues, such as clean-label backdoor attacks or subtle data manipulations that do not involve label corruption.",
            "19": "**Evaluation on Real-World Datasets**:\n   - While the paper provides extensive experiments on benchmark datasets, it would be beneficial to see evaluations on more diverse and real-world datasets to further validate the generalization capabilities of VDC.",
            "20": "**Threshold Selection**:\n   - The paper sets a fixed threshold (α = 0.5) for detecting dirty samples.",
            "21": "However, the choice of this threshold could significantly impact the performance.",
            "22": "A more detailed discussion on how to select and potentially adapt this threshold in different scenarios would be useful.",
            "23": "**Potential for Incorrect Responses**:\n   - The paper acknowledges that LLMs and MLLMs may yield incorrect responses, which could affect the performance of VDC.",
            "24": "While ensembling techniques are used to mitigate this risk, the paper could benefit from a more detailed discussion on how to handle such cases effectively.",
            "25": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of data-centric AI by introducing a versatile and effective method for detecting dirty samples using visual-linguistic inconsistencies.",
            "26": "The proposed VDC framework demonstrates strong performance and generalization capabilities across various types of dirty samples and datasets.",
            "27": "However, the reliance on LLMs and MLLMs, computational complexity, and limited scope of dirty samples are areas that could be further explored and addressed in future work.",
            "28": "The open-source code and detailed analysis provided in the paper are commendable and contribute to the transparency and reproducibility of the research."
        },
        "oa4JbDyZ22": {
            "0": "- The proposed method leverages the recent trend of MLLM to the label error detection literature\n- The proposed method is training-free - The empirical comparison is not fair.",
            "1": "- The propose approach is using instruct-BLIP (larger network trained on larger dataset), while the baseline is usually using less-expressive network trained on smaller datasets, e.g., CL is using ResNet-18 and trained on CIFAR dataset.",
            "2": "- The claim that the proposed method mitigates all three types of label noises is too strong.",
            "3": "- The proposed approach is general, but so do other baselines.",
            "4": "For example, SimiFeat-V leverages the feature similarity to detect noisy labels, which is also applicable to the scenario of “poisoned samples” as long as the feature extractor is trained on a poison-free dataset."
        },
        "LpFKkIcfAp": {
            "0": "+ The paper's approach is innovative, using popular multimodal large models to replace manual data cleaning work.",
            "1": "+ The paper unified the treatment of noisy data and poisoned data from backdoor attacks, which is relatively rare in previous research.",
            "2": "+ The paper's extensive experiments demonstrate the effectiveness of VDC in handling dirty data.",
            "3": "+ The overall content of the paper seems to be more about using out-of-distribution methods to filter data, and perhaps this should be reflected in the accuracy of sample selection.",
            "4": "+ While the paper effectively addresses the issue of dirty data using MLLM, it seems to have a bias towards reporting the application of MLLM."
        },
        "VV87JWnLKE": {
            "0": "Noisy or dirty data detection and cleaning is an important research topic.",
            "1": "It is becoming even more critical for recent machine learning research since the data scale is always getting larger.",
            "2": "The proposed framework wisely utilize the advantage of current large-scale model to benefit the dirty data detection task.",
            "3": "Comprehensive empirical results show the framework superiority compared with other baselines.",
            "4": "The whole draft is in a good format for readers.",
            "5": "I mainly concern about the technical contribution in this draft.",
            "6": "The wise combination of prompting and dirty data detection is interesting.",
            "7": "However, it still based on the visual-language understanding from large-scale pretrained model.",
            "8": "Only based on such powerful tools relatively diminish this paper novelty.",
            "9": "In addition, the key point of this paper is proposing an integrated detection pipeline instead of only focusing on sample or label.",
            "10": "This point looks like a trivial combination which is incremental compared with previous settings.",
            "11": "Is this setting practical and necessary for real-world scenarios?",
            "12": "Adding more recent published works for comparison may further help to support the paper contribution.",
            "13": "Currently, only one or two baselines are published within past one year."
        }
    },
    "farT6XXntP": {
        "llm_review": {
            "0": "**Novel Fine-Tuning Approach**: The paper introduces a two-stage fine-tuning approach that significantly enhances the translation performance of LLMs without relying on large amounts of parallel data.",
            "1": "This is a notable advancement in the field of machine translation.",
            "2": "**Empirical Validation**: The authors provide extensive empirical evidence demonstrating the effectiveness of their approach.",
            "3": "The results show substantial improvements in BLEU and COMET scores across multiple language pairs, outperforming previous state-of-the-art models.",
            "4": "**Efficiency**: The proposed method is computationally efficient.",
            "5": "The authors highlight that fine-tuning on 1B monolingual tokens can be completed in around 18 hours using 16 MI200 GPUs, making it accessible for practical applications.",
            "6": "**Comprehensive Analysis**: The paper includes a thorough analysis of the impact of monolingual data and parallel data quality on translation performance.",
            "7": "This provides valuable insights into the factors contributing to the success of the proposed method.",
            "8": "**Comparison with State-of-the-Art**: The authors compare their models with several state-of-the-art models, including NLLB-54B and GPT-3.5, demonstrating that their approach achieves competitive or superior performance.",
            "9": "**Open Source Contribution**: The authors have made their code and models available on GitHub, promoting transparency and enabling further research and development in the field.",
            "10": "#### Weaknesses:\n\n1.",
            "11": "**Limited Language Pairs**: While the paper demonstrates significant improvements across 10 translation directions, it would be beneficial to see the approach tested on a broader range of languages, especially low-resource languages, to fully understand its generalizability.",
            "12": "**Dependency on High-Quality Data**: The approach relies on high-quality monolingual and parallel data.",
            "13": "In real-world scenarios, such high-quality data may not always be available, which could limit the applicability of the method.",
            "14": "**Evaluation Metrics**: The paper primarily relies on BLEU and COMET scores for evaluation.",
            "15": "While these are standard metrics, incorporating additional evaluation methods, such as human evaluations, could provide a more comprehensive assessment of translation quality.",
            "16": "**Model Size and Resources**: Although the method is efficient, it still requires substantial computational resources for fine-tuning.",
            "17": "This might be a barrier for smaller research groups or organizations with limited access to high-performance computing resources.",
            "18": "**Potential Overfitting**: The paper does not discuss the potential risk of overfitting to the high-quality parallel data used in the second stage of fine-tuning.",
            "19": "It would be useful to include an analysis of how well the model generalizes to unseen data.",
            "20": "**Lack of Qualitative Analysis**: The paper focuses heavily on quantitative results.",
            "21": "Including qualitative examples of translations before and after fine-tuning could provide more intuitive insights into the improvements achieved by the proposed method.",
            "22": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of machine translation by introducing a novel fine-tuning approach that enhances the performance of LLMs.",
            "23": "The empirical results are compelling, and the method is computationally efficient.",
            "24": "However, the approach's dependency on high-quality data and the need for substantial computational resources are potential limitations.",
            "25": "Future work could address these issues by exploring the method's applicability to a wider range of languages and evaluating its performance with more diverse data sources."
        },
        "VezMUEs1o4": {
            "0": "The results of the paper indicated that smaller models could achieve SOTA translation levels through specialized fine-tuning, suggesting that there might not be a continuous need to expand datasets and models for better performance.",
            "1": "Through compact, specialized fine-tuning, smaller LLMs could achieve SOTA translation quality without billions of parameters.",
            "2": "The focus of this research was on tailored fine-tuning methods that unleashed the potential of LLM's multilingual capabilities on a broader scale.",
            "3": "The paper demonstrated that instead of increasing data scale, intentional fine-tuning targeting key language capabilities might be the key to maximizing LLM performance.",
            "4": "By revealing the potential of smaller LLMs for efficient and accurate machine translation, this work laid the foundation for developing more user-friendly and scalable machine translation systems.",
            "5": "This training approach offered more possibilities for deploying capable multilingual LLMs in real-world applications.",
            "6": "There were certain flaws in the method, and prompts affected the results.",
            "7": "The evaluation methods had its limitations.",
            "8": "The stability of the proposed method was not verified."
        },
        "RzDrUF628F": {
            "0": "They show large improvements in the translation capabilities of the most useful size of models (7B,13B) with very affordable limited fine-tuning and data.",
            "1": "This is a useful paper for people working in machine translation to see what works in fine-tuning large language models for the translation task.",
            "2": "There is not a lot of novelty in the approach - either in training or modelling.",
            "3": "I am not sure that the \"New paradigm\" title is justified.",
            "4": "I have not learned much from reading the paper - it is still not clear what the contribution of the monolingual vs parallel training data is.",
            "5": "It is also not clear whether the good performance of the trained models is due to the reduced number of non English languages (5) vs other models (NLLB, GPT3.5,4).",
            "6": "I am also not sure these results (improvement over LLaMa7B with fine-tuning) would hold if you used few-shot - and it would have been a very easy experiment to conduct.",
            "7": "The paper writing is not particularly clear (see questions for details)."
        },
        "XozJtG57x1": {
            "0": "1) Propose a simple training recipe for LLMs for translation tasks: finetuning first on monolingual data and then on small high-quality parallel data.",
            "1": "2) Demonstrate impressive performance across 5 language pairs with LLAMA-2.",
            "2": "1) The statement of \"paradigm shift\" is somehow overestimated.",
            "3": "2) The few-shot prompting results are highly undervalued.",
            "4": "3) The proposed recipe might not apply to other LLMs and languages."
        },
        "Ls28hhc4xO": {
            "0": "The paper is clearly written and provides many insights.",
            "1": "Using LLM to boost the translation quality is an interesting and important topic.",
            "2": "It proposes a novel fine-tuning paradigm to let the moderate size LLMs better at translation.",
            "3": "Many analyses should be very helpful to the NLP and ML community.",
            "4": "The paper is mainly focusing on improve the translation quality of LLMs.",
            "5": "It'd be better to compare more with the encoder-decoder translation models and shed light on the best practice of translation itself."
        }
    },
    "oWENFj7583": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel 2-stage low-rank adaptation method (2LoRA) and its improved version (pLoRA) to adapt large-scale pre-trained image generation models for non-visible light domains, specifically Synthetic Aperture Radar (SAR) imagery.",
            "1": "This is a significant contribution as it addresses the challenge of data scarcity in SAR datasets.",
            "2": "**Comprehensive Methodology**: The authors provide a detailed explanation of their methodology, including the two stages of adaptation (view and modality adaptation) and the use of prototype LoRA to address class imbalance.",
            "3": "The use of ControlNet for structure-conditional image generation is also well-explained.",
            "4": "**Empirical Validation**: The paper includes extensive experiments to validate the proposed methods.",
            "5": "The use of both quantitative (FID and F1 scores) and qualitative evaluations provides a robust assessment of the performance improvements achieved by 2LoRA and pLoRA.",
            "6": "**Practical Application**: The focus on SAR imagery, which has wide applications in marine safety, environmental protection, and climate studies, highlights the practical relevance of the research.",
            "7": "The proposed methods have the potential to significantly improve SAR data augmentation and recognition models.",
            "8": "**Use of Large Language Models (LLMs)**: The innovative use of GPT-4 for prompt construction in the ORS dataset is a noteworthy contribution.",
            "9": "This automation of prompt engineering can be beneficial for other applications as well.",
            "10": "**Open-Sourced Dataset**: The introduction of the FU-SRS dataset for fine-grained classification of SAR ships and its open-sourcing for review is a commendable effort.",
            "11": "This can facilitate further research and benchmarking in the field.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Complexity and Reproducibility**: The proposed methods, especially pLoRA, involve multiple stages and components, which might be complex to implement and reproduce.",
            "14": "The paper could benefit from a more detailed step-by-step guide or supplementary materials to aid reproducibility.",
            "15": "**Limited Comparison with Baselines**: While the paper compares 2LoRA and pLoRA with fine-tuning and single LoRA methods, it lacks a comparison with other state-of-the-art data augmentation techniques for SAR imagery.",
            "16": "Including such comparisons would strengthen the validation of the proposed methods.",
            "17": "**Generalization to Other Modalities**: The paper focuses on SAR imagery, but it would be beneficial to discuss the potential generalization of the proposed methods to other non-visible light domains, such as infrared or medical imaging, in more detail.",
            "18": "This would broaden the impact of the research.",
            "19": "**Evaluation Metrics**: The use of FID and F1 scores is appropriate, but additional metrics such as precision-recall curves, confusion matrices, or user studies (for qualitative assessment) could provide a more comprehensive evaluation of the generated images and their utility in downstream tasks.",
            "20": "**Scalability and Efficiency**: The paper does not discuss the computational requirements and scalability of the proposed methods.",
            "21": "Given the complexity of the models and the adaptation process, it would be useful to include an analysis of the computational cost and efficiency.",
            "22": "**Clarity in Presentation**: Some sections of the paper, particularly the methodology, are dense and could benefit from clearer explanations and visual aids.",
            "23": "For instance, the process of clustering SAR data and the calculation of bias scores for pLoRA could be elaborated with more intuitive diagrams or examples.",
            "24": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of non-visible light data synthesis, specifically for SAR imagery.",
            "25": "The proposed 2LoRA and pLoRA methods are innovative and show promising results in addressing data scarcity and class imbalance issues.",
            "26": "However, the complexity of the methods and the need for more comprehensive comparisons and evaluations are areas that could be improved.",
            "27": "With these enhancements, the research has the potential to make a substantial impact on SAR data augmentation and recognition models, as well as other non-visible light domains."
        },
        "CC75OFaXA3": {
            "0": "The paper addresses an important problem in the context of using deep learning in remote sensing, which is the scarcity of annotated data, especially for certain acquisition modalities like SAR.",
            "1": "Using synthetically generated images can be a sensible way of performing data augmentation, that has already been shown effective in other domains.",
            "2": "The overall novelty of work is limited.",
            "3": "The methodology is based on well-known concepts such as LoRA and ControlNet.",
            "4": "The two-stage approach to adaptation, while important for problem, is minor and of interest only for a niche audience.",
            "5": "Overall, the clarity of the paper can be improved.",
            "6": "For example, the presentation of the method is often intertwined with experimental details.",
            "7": "Experimental evaluation is also problematic.",
            "8": "The FID metric is not suitable for SAR images, since it is a distance in the latent space of an ImageNet-trained neural network, thus having a large domain gap (some works point out that the ImageNet prior affects evaluation even of RGB images, nevermind SAR, see Kynkäänniemi et al.",
            "9": "\"The Role of ImageNet Classes in Fréchet Inception Distance\").",
            "10": "The F1 score of the downstream classification task is a relevant metric but the results are difficult to understand.",
            "11": "For instance, Table 2 does not report the F1 performance of the baseline without augmentations."
        },
        "InXRMdfcrX": {
            "0": "The strengths of this research lie in its innovative approach to addressing the challenges of adapting semantic knowledge from regular imagery to synthetic aperture radar (SAR) imagery.",
            "1": "The 2LoRA method and its enhanced version, pLoRA, offer a novel solution to overcome class imbalance issues in SAR datasets, particularly for minor classes.",
            "2": "The paper's pioneering use of large-scale pre-trained generation models for synthesizing non-visible light images is a notable technical contribution, allowing the transfer of pre-learned semantic knowledge from regular images to SAR data despite significant domain gaps.",
            "3": "A comprehensive comparison is essential to substantiate the efficacy of the proposed approach.",
            "4": "Additionally, the authors' reliance on a custom dataset limits the ability to assess the method's effectiveness without validation on publicly available datasets.",
            "5": "Furthermore, there are concerns regarding stability, including potential dependencies in prompt construction and the generation of detailed visual descriptions by GPT-4."
        },
        "ps88JVnIHT": {
            "0": "Data synthesis is important and valuable for non-visible light data, which is hard to collect large-scale dataset.",
            "1": "The proposed methods do not show significant performance improvement when using the full traning dataset.",
            "2": "In practical, we always will employ all data to train a model rather than only employ 10% data.",
            "3": "However, when employing the full training data, the resample strategy is simple and comparable with the proposed method.",
            "4": "The pLoRA is not effective.",
            "5": "According to the experiments, pLoRA performs worse than 2LoRA in most cases."
        },
        "ugWtjB1yrH": {
            "0": "The paper is well written and the background sound.",
            "1": "A large amount of content has been presented in a short number of pages very well.",
            "2": "I find no plagiarism either.",
            "3": "The paper may not be easily accessible to non-experts as the paper is written very high-level.",
            "4": "Due to space limitations, however, this is ok."
        }
    },
    "tcFcKyJgRM": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel framework, Hierarchical Policies for Web Actions using LLMs (HeaP), which leverages large language models (LLMs) to decompose complex web tasks into modular sub-tasks.",
            "1": "This hierarchical approach is innovative and addresses the challenges of combinatorially large open-world tasks and variations across web interfaces effectively.",
            "2": "**Efficiency**: HeaP demonstrates significant efficiency in terms of data usage.",
            "3": "The framework achieves high success rates with orders of magnitude fewer training examples compared to prior works.",
            "4": "This is a substantial improvement, showcasing the potential of hierarchical prompting and in-context learning.",
            "5": "**Comprehensive Evaluation**: The paper evaluates HeaP across a diverse set of environments, including MiniWoB++, WebArena, a mock airline CRM, and live website interactions.",
            "6": "This comprehensive evaluation provides a robust validation of the framework's effectiveness and generalizability.",
            "7": "**Detailed Methodology**: The paper provides a detailed description of the methodology, including the problem formulation, inference time procedure, and the process of generating task and policy prompts from demonstrations.",
            "8": "This clarity in presentation helps in understanding the framework and its implementation.",
            "9": "**Comparison with Baselines**: The paper includes a thorough comparison with various baselines and prior state-of-the-art methods.",
            "10": "The results show that HeaP outperforms or matches these methods with significantly fewer demonstrations, highlighting the effectiveness of the proposed approach.",
            "11": "**Qualitative Analysis**: The inclusion of qualitative examples and detailed reasoning for specific tasks helps in understanding the practical application and advantages of HeaP.",
            "12": "This analysis provides insights into how the hierarchical approach improves task decomposition and execution.",
            "13": "**Weaknesses:**\n\n1.",
            "14": "**Complexity of Implementation**: While the hierarchical approach is innovative, it adds complexity to the implementation.",
            "15": "The need to design and manage multiple low-level policies and their interactions can be challenging and may require significant effort in practice.",
            "16": "**Dependence on LLMs**: The framework heavily relies on the capabilities of LLMs, which may not always be accessible or cost-effective for all users.",
            "17": "The performance of HeaP is tied to the quality and scale of the LLMs used, which could be a limitation for broader adoption.",
            "18": "**Handling Visual Information**: The paper acknowledges that HeaP currently struggles with web pages that contain visual-only components.",
            "19": "This limitation is significant as many real-world web tasks involve visual elements that are not captured in the HTML DOM.",
            "20": "Future work needs to address this gap, possibly by integrating multi-modal models.",
            "21": "**Error Recovery**: The framework's ability to recover from errors is limited.",
            "22": "If HeaP makes a mistake, such as clicking the wrong link, it may not know how to recover and continue the task.",
            "23": "This lack of robust error recovery mechanisms could impact the reliability of the framework in dynamic web environments.",
            "24": "**Scalability to Complex Webpages**: While HeaP shows promise in handling various web tasks, its scalability to more complex webpages with extensive content and interactions remains uncertain.",
            "25": "The paper mentions the need for advanced compression techniques and saliency models, but these are not fully explored or implemented in the current work.",
            "26": "**Security and Misuse Concerns**: The paper briefly mentions the potential for misuse of action LLMs in open-domain environments.",
            "27": "This is an important consideration, and the framework would benefit from a more detailed discussion on security measures and ethical considerations to prevent misuse.",
            "28": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of web task automation using LLMs.",
            "29": "The hierarchical approach of HeaP is innovative and demonstrates impressive efficiency and generalizability across various web environments.",
            "30": "However, the complexity of implementation, dependence on LLMs, handling of visual information, error recovery, scalability, and security concerns are areas that need further exploration and improvement.",
            "31": "Despite these weaknesses, the strengths of the paper make it a valuable contribution to the field, providing a solid foundation for future research and development."
        },
        "xmk9BvbTI7": {
            "0": "The design of this method is sound and reasonable.",
            "1": "Exhaustive details of the prompt and results analysis are presented.",
            "2": "The evaluation is based on too few samples: 45 tasks on MIniWob++, 125 examples of two domains on WebArena, 5 distinct tasks with 20 scenarios on Ariline CRM, and 3 website with 10 searches per site on Live Websites.",
            "3": "Given the fact that human demonstration is collected to form the prompts to the LLM in HEAP, it should be actually evaluated on more diverse websites instead of fewer websites."
        },
        "xEIR7Ij9Wt": {
            "0": "The approach introduces a novel hierarchical approach to prompt LLMs to perform web tasks.",
            "1": "Experimental results on various complex web benchmarking datasets show the superiority of the proposed approach.",
            "2": "I recommend moving some implementation details like prompts into the main body to help the reader better understand the work."
        },
        "fgGmSXNhF6": {
            "0": "- The paper is overall easy to read, although some important methodological details like autolabeling and prompt construction are in appendix which makes it hard to read.",
            "1": "- The experiments are extensive over 4 datasets with many tasks.",
            "2": "The gain demonstrated is substantial.",
            "3": "- The idea of hierarchical planning with a high-level planner and low-level policies using LLM has been explored by many previous robotics works e.g.",
            "4": "LLM-planner (https://arxiv.org/pdf/2212.04088.pdf and the line of works they cited).",
            "5": "Additionally, PaP (https://aclanthology.org/2022.suki-1.8.pdf) and Parsel (https://arxiv.org/pdf/2212.10561.pdf) have also explored similar ideas of prompting LLM to generate a hierarchical plan but implementing low-level planners with programs.",
            "6": "Implementing both high-level and low-level planners with LLM prompting has been explored in Decomposed Prompting (https://openreview.net/pdf?id=_nGgzQjzaRy).",
            "7": "Considering these previous works, the novelty of this paper is limited to applying existing ideas to web datasets and potentially the technical details of autolabeling from human demonstrations.",
            "8": "- The low-level policies are manually defined during autolabeling, making the framework limited in flexibility comparing to previous works that allow LLM to generate decompositions freely.",
            "9": "- The only LLM prompting baseline compared against is ReAct, which demonstrates the benefits of hierarchical planning.",
            "10": "However, such benefits have been demonstrated with the prior works mentioned above."
        },
        "ZMHbTgCDhJ": {
            "0": "-Originality: The idea is interesting in the way HeaP leverages hierarchical policies to decompose complex web tasks using a high-level task planner \n into modular  low-level web policies.",
            "1": "-Quality: The paper is quite thorough in its experimental setup as it tests on 4 interesting datasets, including simulated and live websites, to assess the performance of the proposed approach.",
            "2": "-Clarity: The paper is well-written and structured, making it easy for readers to follow and understand the proposed approach\n\n-Significance: The paper addresses a significant challenge in the field of natural language processing and machine learning, which is teaching LLMs to perform web-based tasks which can lead to a huge set of applications - The tasks are not that challenging and the results are very weak relative to how powerful the LLM model used here which is GPT-3.5.",
            "3": "For example, it seems that the proposed method struggles with book-flight which is a basic constrained task and therefore this method is very far from being deployed in the real world\n\n- Using closed source methods like GPT-3.5 is expensive.",
            "4": "I'd be curious to see how this method would perform with open source methods like Llama and Mistral.",
            "5": "- No code was provided to asses and verify the results as well as understand the low level details of how the method is implemented"
        }
    },
    "uLOFyiruin": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel benchmark, Babel-ImageNet, which provides translations of 1000 ImageNet labels into 92 languages.",
            "1": "This is achieved without relying on machine translation or manual annotation, which is a significant advancement in the field of multilingual vision-and-language (VL) models.",
            "2": "**Comprehensive Evaluation**: The authors evaluate eight different multilingual CLIP models on zero-shot image classification (ZS-IC) for each of the 92 languages.",
            "3": "This extensive evaluation provides valuable insights into the performance of these models across a wide range of languages, highlighting the gaps between high-resource and low-resource languages.",
            "4": "**Correlation with Image-Text Retrieval**: The paper demonstrates that the ZS-IC performance on Babel-ImageNet highly correlates with the performance in image-text retrieval tasks.",
            "5": "This validation is crucial as it establishes Babel-ImageNet as a reliable benchmark for evaluating multilingual VL representation spaces.",
            "6": "**Parameter-Efficient Training**: The authors propose a computationally efficient approach for improving the performance of multilingual CLIP models for low-resource languages.",
            "7": "This method shows significant performance gains, making it a practical solution for enhancing multilingual VL models.",
            "8": "**Public Availability**: The authors make their code and data publicly available, which promotes transparency and allows other researchers to build upon their work.",
            "9": "#### Weaknesses:\n\n1.",
            "10": "**Anglo-Centric Bias**: The paper acknowledges that the set of classes in ImageNet-1k tends to be Anglo-centric due to inherited biases from WordNet.",
            "11": "This limitation means that the benchmark may not fully capture the cultural and geographical diversity of the languages evaluated, potentially overestimating the usability of VL models in certain contexts.",
            "12": "**Language Selection**: The selection of the 92 languages used in XLM-R pretraining, while a reasonable tradeoff, reinforces research focus on these languages to the detriment of other extremely low-resource languages.",
            "13": "This selection bias could limit the generalizability of the findings to a broader set of languages.",
            "14": "**Error Rate in Label Translation**: Although the authors estimate that less than 1% of labels are incorrect, this error rate, combined with the inherent noise in the original ImageNet labels, could still impact the overall evaluation results.",
            "15": "A more detailed analysis of the impact of these errors on the benchmark's reliability would be beneficial.",
            "16": "**Limited Scope of Evaluation Tasks**: While the paper focuses on ZS-IC and image-text retrieval, it would be valuable to see how the models perform on other VL tasks, such as visual question answering or cross-modal reasoning, to provide a more comprehensive evaluation of their capabilities.",
            "17": "**Dependency on XLM-R**: The reliance on XLM-R for text encoder initialization in many of the evaluated models may limit the exploration of other potential multilingual text encoders that could offer different strengths and weaknesses.",
            "18": "#### Conclusion:\n\nOverall, the paper presents a significant contribution to the field of multilingual vision-and-language models by introducing Babel-ImageNet, a massively multilingual benchmark.",
            "19": "The comprehensive evaluation and validation of the benchmark, along with the proposed parameter-efficient training method, provide valuable insights and practical solutions for improving multilingual VL models.",
            "20": "However, the paper could benefit from addressing the Anglo-centric bias, expanding the scope of evaluation tasks, and exploring a broader range of multilingual text encoders."
        },
        "tz7HIXzzdC": {
            "0": "- The study goes beyond traditional monolingual evaluation, offering a comprehensive analysis of 8 multilingual CLIP models across 92 languages.",
            "1": "- The paper is well-motivated and, in general, clear enough to follow through;\n- It provides a practical and parameter-efficient approach that significantly improves model performance, making multilingual models more relevant and accessible for underrepresented linguistic communities;\n- The dataset/benchmark contribution targets a relevant issue (the overall imbalance between high and low-resourced languages);\n- The authors already provide the code for reproducibility purposes; - BabelNet reliance.",
            "2": "This work relies entirely on BabelNet and assumes that the mapping between WordNet and other resources is high quality.",
            "3": "However, BabelNet is automated and has a known percentage of error, potentially affecting the label mapping [1];\n- Using WordNet synsets for translations may introduce limitations, as not all concepts or words have direct equivalents in WordNet or BabelNet, potentially impacting the completeness of translations for some languages.",
            "4": "- While the paper emphasizes the creation of the benchmark and model evaluation, it could benefit from a deeper analysis of why certain languages perform poorly according to the chosen metrics and explore potential solutions to address these disparities;\n- Considering that the paper belongs to the \"datasets and benchmarks\" area, the methodology employed (mapping from ImageNet to WordNet and then to BabelNet) is expectedly straightforward.",
            "5": "However, I think there's also some weakness in the data cleaning and validation since the obtained multilingual data is used to evaluate models, but those same benchmarks cannot be used to assess the quality of the data itself;\n- The paper's process of removing words with identical English counterparts in the class label translation and cleaning may not be fully justified, as there can be legitimate shared words between the English and language-specific vocabulary;\n\n[1] Ten Years of BabelNet: A Survey.",
            "6": "Roberto Navigli, Michele Bevilacqua, Simone Conia, Dario Montagnini, Francesco Cecconi.",
            "7": "IJCAI 2021"
        },
        "CgAA4ceifX": {
            "0": "This paper introduces an extensive image-text evaluation benchmark on a large set of languages which motivates research in the largely unexplored multilingual VL representation learning space.",
            "1": "Also, the technique is free from any machine translation or similar techniques that can introduce errors in the evaluation data.",
            "2": "This makes it more robust and suitable for adoption.",
            "3": "This evaluation corpus should be extremely helpful for furthering research in this area.",
            "4": "None."
        },
        "90Fx0BorTV": {
            "0": "- The proposed translation method is robust and the claimed error rate from manual inspection is low.",
            "1": "- The translation covers 92 languages, including many medium and low resource languages.",
            "2": "- When evaluating multilingual models, the performance on Babel-ImageNet correlates well with the text to image retrieval performance on multilingual image-text datasets, suggesting the usefulness of this dataset as an alternative evaluation method for multilingual models From a significance and usefulness perspective, the unique advantage of this dataset over the multilingual image-text datasets for model evaluation is unclear.",
            "3": "It is not surprising that the performance of models on multilingual ImageNet classification is correlated with multilingual text to image retrieval.",
            "4": "My concern is that Babel-ImageNet might not be as good as the multilingual image-text datasets as the former contains much less detailed description for the image, and that other image-text datasets support image-to-text retrieval as well for which Babel-ImageNet could not cover.",
            "5": "The section 6 discussion might be a good opportunity to set up such a comparison if the models there could be evaluated on the multilingual image-text datasets as well.",
            "6": "If the authors can show that Babel-ImageNet better reflects the model quality improvement, that would make a strong argument."
        },
        "cQqPKvNbKj": {
            "0": "1) The multilingual ImageNet benchmark, which supports 92 languages, serves as an excellent platform for evaluating multilingual CLIP models, particularly for those languages that are under-resourced.",
            "1": "2) The assessment of eight different multilingual CLIP models also provides valuable insights.",
            "2": "My concern is about the simplicity of the method, which merely translates English ImageNet labels using BabelNet.",
            "3": "While the resulting benchmark proves useful, the method's contribution appears to be limited."
        }
    },
    "V8aD5pUcVX": {
        "llm_review": {
            "0": "#### Strengths:\n\n1.",
            "1": "**Comprehensive Benchmarking**: The paper introduces a new benchmark, GVTBench, which is specifically designed to evaluate both semantic understanding and fine-grained visual perception capabilities of Multimodal Large Language Models (MLLMs).",
            "2": "This is a significant contribution as it provides a more holistic evaluation framework compared to existing benchmarks.",
            "3": "**Empirical Insights**: The study provides valuable empirical insights into the performance of different visual tokenizers pre-trained with various methods (e.g., DeiT, CLIP, MAE, DINO, and DINOv2).",
            "4": "The observations regarding the trade-offs between fully/weakly supervised and self-supervised models are particularly insightful.",
            "5": "**Novel Visual Tokenizer (GVT)**: The paper proposes a new visual tokenizer, GVT, which combines the strengths of semantic understanding and fine-grained visual perception.",
            "6": "The approach of using feature distillation without masking to preserve rich semantics while enhancing fine-grained perception is innovative.",
            "7": "**Detailed Experimental Setup**: The paper provides a thorough experimental setup, including the datasets used, training protocols, and evaluation metrics.",
            "8": "This transparency allows for reproducibility and a clear understanding of the experimental design.",
            "9": "**Strong Performance**: The proposed GVT model demonstrates superior performance on a variety of vision-language tasks, including visual question answering, image captioning, object counting, and multi-class identification.",
            "10": "This highlights the effectiveness of the proposed visual tokenizer.",
            "11": "**Attention to Fine-Grained Perception**: The paper emphasizes the importance of fine-grained visual perception, which is often overlooked in existing MLLM evaluations.",
            "12": "The inclusion of tasks like Object Counting (OC) and Multi-Class Identification (MCI) in GVTBench is a notable strength.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Limited Scope of Visual Tokenizers**: While the paper evaluates several visual tokenizers, it primarily focuses on those based on the Vision Transformer (ViT) architecture.",
            "15": "Exploring a broader range of architectures, including convolutional neural networks (CNNs) or hybrid models, could provide a more comprehensive understanding.",
            "16": "**Instruction-Tuning Dataset Size**: The paper notes that the instruction-tuning dataset is relatively small (~5M image-text pairs) compared to the pre-training datasets.",
            "17": "This limitation might affect the generalizability of the findings, especially regarding the impact of joint tuning on semantic loss.",
            "18": "**Comparison with State-of-the-Art Models**: Although the paper compares GVT with several recent MLLMs, it would benefit from a more detailed comparison with state-of-the-art models on additional benchmarks beyond GVTBench.",
            "19": "This would provide a clearer picture of GVT's relative performance.",
            "20": "**Ablation Studies**: While the paper includes some ablation studies, more detailed analyses could be provided.",
            "21": "For instance, exploring the impact of different hyperparameters in the feature distillation process or the effect of varying the size of the instruction-tuning dataset could offer deeper insights.",
            "22": "**Qualitative Analysis**: The paper includes some qualitative results, but a more extensive qualitative analysis, including failure cases, could help in understanding the limitations and areas for improvement of the proposed GVT model.",
            "23": "**Future Work Directions**: The paper briefly mentions future work but could benefit from a more detailed discussion on potential research directions.",
            "24": "For example, exploring how the proposed visual tokenizer can be adapted for other modalities or more complex visual understanding tasks would be valuable.",
            "25": "#### Conclusion:\n\nOverall, the paper makes significant contributions to the field of multimodal large language models by providing a comprehensive evaluation framework and proposing a novel visual tokenizer that excels in both semantic understanding and fine-grained visual perception.",
            "26": "The empirical insights and strong performance of the proposed GVT model are commendable.",
            "27": "However, the paper could be further strengthened by exploring a broader range of visual tokenizers, providing more detailed ablation studies, and offering a more extensive qualitative analysis."
        },
        "aYRvrqJMqA": {
            "0": "This paper shows the following strengths:\n\n1.",
            "1": "The authors have conveyed their design choices and insights via extensive experiments.",
            "2": "For example, the insights of the visual tokenizers are derived from the experiments of various models and datasets.",
            "3": "The benchmark constructed could be useful for future studies on multi-modal models with LLMs.",
            "4": "The final approach of feature distillation is straightforward and clearly improves the performance on the additional datasets of object counting and multi-class identification.",
            "5": "I think the authors may want to pay attention to the following weaknesses.",
            "6": "I will include the specific questions in the next section.",
            "7": "I think the authors need to **better define the important notions** and consequently **clarify the difference to previous works**.",
            "8": "* Describing the difference between \"overall semantics\" and \"fine-grained perception\" is especially critical.",
            "9": "Both terms seem vague to the readers but are the foundation of the evaluation benchmarks and insights in the paper.",
            "10": "* I understand the intuition of adding object counting and multi-class identification.",
            "11": "However, their separation from VQA is unclear.",
            "12": "I suggest improving the current clarification in Sec.",
            "13": "2.1 according to my questions below.",
            "14": "The method of this paper is quite simple (in a good way) by combining the techniques of feature distillation with patch-level supervision.",
            "15": "However, I found the following things need improvement:\n* Writing.",
            "16": "Since the method is simple, it is always better to clearly explain the method.",
            "17": "Sadly, I could hardly find a grounded description of the method in both Sec.",
            "18": "2.1 and Sec.",
            "19": "3.2.",
            "20": "Adding proper equations and detailed clarifications will help the reader to fully understand the approach.",
            "21": "I personally think that the title of the paper is a little **overclaiming**.",
            "22": "I will give two perspectives:\n* I know that it is hard to define \"good,\" but what the paper achieves is only **making MLLMs better at object counting multi-class identification** using patch-level feature distillation, without improving the overall semantic understanding.",
            "23": "So concentrating on what is achieved is more precise, in my own opinion.",
            "24": "* When we talk about visual tokenizers, a lot of variables also need to be covered, such as the architecture, training techniques, etc.",
            "25": "The paper mainly focuses on the types of supervision without investigating the other variables, which might not be sufficient for a grand title or claim.",
            "26": "For example, will switching the claim of the paper to concentrate on \"supervision\" be better?"
        },
        "FQLw00f2UA": {
            "0": "The analysis of different visual tokenizers (e.g., Table 1) is very informative.",
            "1": "The writing is clear and easy to follow.",
            "2": "Compared to the baselines, there are consistent improvements on OC and MCI tasks.",
            "3": "The main concern is about the experimental setup.",
            "4": "Besides CC3M and SBU, authors use Visual Genome, MS-COCO, Object365 and OpenImages V6 to train the visual tokenizer.",
            "5": "Based on the descriptions in Section 2.1, questions in OC and MCI tasks are object-centric.",
            "6": "Such pretraining datasets provide a privilege for the proposed model.",
            "7": "As shown in table 4, most of the improvements come from OC and MCI.",
            "8": "In table 1 and 2, authors use CLIP for ablation study but switch to EVA-CLIP for the remaining experiments.",
            "9": "Why is there a inconsistency?",
            "10": "Based on table 5 and 6, it seems that there are consistent improvements by explicitly utilizing object-level annotations during pretraining.",
            "11": "This does not necessarily mean that the proposed tokenizer is a good visual tokenizer, especially considering the performance drop on VQA and image captioning tasks."
        },
        "EOMw1H1eJy": {
            "0": "I appreciate the authors for conducting a good experimental study on the effect of different visual tokenizers for large multimodal models.",
            "1": "It has been a routine that almost all LMMs use CLIP vision encoder as the visual tokenizer.",
            "2": "Few people pay enough attention to the vision encoder itself, though it is a very important factor in LMMs.",
            "3": "The study on different visual tokenizers suggests that CLIP is still one of the best visual tokenizers for LMMs, despite it is not necessarily able to provide fine-grained representations.",
            "4": "This is somehow surprising but also expected because all of the other used visual tokenizers were trained either with much fewer image-text pairs or purely image data at similar scales.",
            "5": "The authors further proposed a new benchmark and a simple distillation method to investigate and enhance the fine-grained visual understanding capability for LMMs.",
            "6": "The experimental results demonstrate the superiority of the proposed method to previous ones.",
            "7": "As mentioned earlier, I like the first part of this work studying the effectiveness of different visual tokenizers for LMMs.",
            "8": "The argument of LMMs needing both semantic-rich and fine-grained representations is valid and shared by many but has never been demonstrated experimentally before.",
            "9": "However, I do not see a clear motivation to propose a distillation method to address the problem derived from the observation at the first part.",
            "10": "The authors simply distill the visual representations from EVA-CLIP to another randomly initialized vision encoder and presume that the new model has better semantic and fine-grained representations.",
            "11": "I do not buy that the proposed method learns better fine-grained representations in that the input images for distillation are still 224x224, and the distillation is simply copying one network to another based on a limited number of images from ImageNet-1K.",
            "12": "Despite the better performance in Table 4, I can see the unfairness when comparing the proposed method with previous ones -- e.g.",
            "13": "different models have different settings and pretraining data.",
            "14": "In the ablation study, the authors did show some improvements after distillation on the new benchmarks.",
            "15": "Two question marks arise.",
            "16": "First, why the authors use EVA-CLIP which proved to be defective in Table 3.",
            "17": "Second, is it because the distillation pulls the feature space closer to the target domain, e.g., web images->imagenet->COCO?",
            "18": "In Table 6, the gap actually is very marginal (seems the averaged gap does not match the individual gaps for Flant5-xxl).",
            "19": "The experimental study is not enough to demonstrate the effectiveness of the distilled visual tokenizer.",
            "20": "The authors should examine the visual tokenizers in a wider range of vision and vision-language tasks.",
            "21": "That being said, the authors attempt to find a more generic visual tokenizer that can provide semantic-rich and fine-grained representations but fail to demonstrate the generality of the proposed visual tokenizer by itself."
        }
    },
    "bRLed9prWC": {
        "llm_review": {
            "0": "**Novelty and Contribution**: The paper introduces a novel task of future language modeling, which is a significant contribution to the field of natural language processing (NLP).",
            "1": "This task involves predicting future textual content based on a temporal history of documents, which has not been formally addressed in prior research.",
            "2": "**Comprehensive Approach**: The authors propose three distinct methods for future language modeling: the word frequency model, the temporal contextual model, and the doubly contextualized model.",
            "3": "This variety of approaches demonstrates a thorough exploration of the problem space.",
            "4": "**Evaluation Metrics**: The paper employs a robust set of evaluation metrics, including perplexity, content perplexity, and content meteor, to assess the performance of the proposed models.",
            "5": "These metrics provide a comprehensive evaluation of both the fluency and the adequacy of the generated content.",
            "6": "**Human Evaluation**: In addition to automatic metrics, the paper includes a human evaluation component, which is crucial for assessing the quality and relevance of the generated abstracts.",
            "7": "The criteria used for human evaluation are well-defined and relevant to the task.",
            "8": "**Experimental Results**: The experimental results show that the proposed models outperform baseline models on both automatic and human evaluation metrics.",
            "9": "This demonstrates the effectiveness of incorporating temporal information into language models for predicting future text.",
            "10": "**Case Study**: The inclusion of a case study on generating future abstracts for NLP papers provides a concrete example of the application of the proposed models.",
            "11": "This helps in understanding the practical implications and potential of the research.",
            "12": "**Code Availability**: The authors have made their code available on GitHub, which promotes transparency and reproducibility of the research.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Complexity and Scalability**: The proposed models, especially the doubly contextualized model, are complex and may require significant computational resources.",
            "15": "The paper does not provide a detailed analysis of the computational cost and scalability of these models.",
            "16": "**Generalization to Other Domains**: While the paper focuses on generating future abstracts for NLP papers, it is not clear how well the proposed models would generalize to other domains or types of textual data.",
            "17": "A broader evaluation across different domains would strengthen the claims of the paper.",
            "18": "**Temporal Granularity**: The paper uses years as the temporal granularity for updating the language model.",
            "19": "It would be interesting to see how the models perform with different temporal granularities, such as months or days, especially for domains where changes occur more rapidly.",
            "20": "**Handling of Random Events**: The authors acknowledge that their models cannot predict random events, new named entities, or serendipitous discoveries.",
            "21": "However, the paper does not discuss potential strategies for mitigating the impact of such unpredictable elements on the model's performance.",
            "22": "**Evaluation of Novelty**: The human evaluation criteria include assessing whether the topic, problem, and method are new.",
            "23": "However, the paper does not provide a clear definition of what constitutes \"new\" in this context.",
            "24": "This could lead to subjective interpretations and variability in the evaluation results.",
            "25": "**Limited Baseline Comparisons**: The paper compares the proposed models primarily with GPT-2 based baselines.",
            "26": "Including comparisons with other state-of-the-art language models or temporal models would provide a more comprehensive evaluation of the proposed approaches.",
            "27": "**Detailed Analysis of Failures**: While the paper presents successful examples of generated abstracts, it lacks a detailed analysis of failure cases.",
            "28": "Understanding where and why the models fail would provide valuable insights for further improvements.",
            "29": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the field of NLP by introducing and addressing the novel task of future language modeling.",
            "30": "The proposed models and comprehensive evaluation demonstrate the potential of incorporating temporal information into language models.",
            "31": "However, the paper could benefit from a more detailed analysis of computational costs, generalization to other domains, and handling of unpredictable events.",
            "32": "Additionally, providing a clearer definition of novelty and including more baseline comparisons would strengthen the evaluation.",
            "33": "Despite these weaknesses, the paper presents a promising direction for future research in predictive language modeling."
        },
        "4ulqHTN5pb": {
            "0": "To the best of my knowledge, the application and task are novel and interesting to study.",
            "1": "Three architectures are introduced, each building on each other.",
            "2": "The effects of each component are studied and give us a sense of what is working and what is needed to improve performance.",
            "3": "More motivation and applications for this task could be provided or speculated.",
            "4": "The paper uses GPT-2 as the base model for all experiments.",
            "5": "It would be better for testing robustness to potentially include a few more base models, perhaps with larger sizes or an encoder-decoder architecture for diversity.",
            "6": "Additional experiments and evaluation on downstream tasks can be performed using conditional generation or prompting on existing data sets with temporal dimensions e.g.",
            "7": "the temporal NER data set (Rijhwani & Preotiuc-Pietro, ACL 2019; Luu et al NAACL 2022) or generating hashtags in future tweets using just the tweet text (similar to Preotiuc-Pietro & Cohn, EMNLP 2013).",
            "8": "These would avoid the issues associated with evaluating generations.",
            "9": "Another interesting experiment to conduct would be to study prediction more into the future and quantify the expected degradation as the time window increases."
        },
        "akEfqFBZp8": {
            "0": "Originality:\n- First to formalize future textual data prediction using temporal information.",
            "1": "Develops novel methods for measuring temporal dynamics in language modeling.",
            "2": "Quality:\n- Presents a thorough structure, comparing three new models against multiple baselines.",
            "3": "- Demonstrates model effectiveness through careful data handling, especially distinguishing between content and non-content words during evaluation.",
            "4": "Clarity:\n- Clearly articulates research motivations, background literature, methodology, and findings.",
            "5": "Significance:\n- Offers significant research outcomes with implications for various applications.",
            "6": "- Discusses potential future applications and necessary adaptations.",
            "7": "To improve readability, you can align the organization of tables and figures more closely with their corresponding text."
        },
        "0Rk7OGoR6p": {
            "0": "Modeling of temporal patterns in LLM has not captured much attention from the community.",
            "1": "Yet it is an important problem to look into.",
            "2": "The paper is very easy to follow.",
            "3": "The authors did a good job of describing their ideas and approaches in simple yet accurate terms and notations.",
            "4": "The proposed models look reasonable and are proven to be effective in generating future text based on historical documents.",
            "5": "The proposed models are relatively simple and don't leverage the power of the most advanced LLM.",
            "6": "Some problems the authors tried to solve, such as the gating problem in Sec 3.4 look like sth that would not be an issue to GPT-3 or other recently developed LLM as they are very effective in generating readable and coherent text.",
            "7": "Finetuning a more powerful LLM with the latest text data seems to be a very effective way to model temporal patterns.",
            "8": "Some details and questions from the experiment were not well discussed.",
            "9": "For example, how many raters participated in the human evaluation, what are their agreements, and how subjective are their ratings?",
            "10": "Besides, the results in Table 4. are worth more analysis and discussion.",
            "11": "Why do the baselines not perform well in Problem and Method?",
            "12": "Intuitively, they should be good at generating coherent and readable content."
        }
    },
    "2PKZtPMyvI": {
        "llm_review": {
            "0": "**Novel Perspective**: The paper introduces a fresh perspective on understanding Transformers by focusing on the intrinsic dimension (ID) of data representations.",
            "1": "This approach is innovative and provides new insights into the geometrical characteristics of data representations learned by Transformers.",
            "2": "**Comprehensive Experiments**: The authors conducted a series of well-designed experiments across multiple datasets (IMDB, AG, and SST2) and various hyper-parameter settings.",
            "3": "This thorough experimental setup strengthens the validity of their findings.",
            "4": "**Clear Observations**: The paper presents clear and significant observations, such as the correlation between higher terminal feature ID and lower classification error rates, and the trend of decreasing ID with increasing layer depth.",
            "5": "These insights are valuable for understanding the inner workings of Transformers.",
            "6": "**Practical Implications**: The study provides practical guidance for choosing hyper-parameters and applying dimension/data reduction when using Transformers for text classification tasks.",
            "7": "This can be particularly useful for practitioners in the field.",
            "8": "**Detailed Analysis**: The paper includes a detailed analysis of the relationship between embedding dimensions, intrinsic dimensions, and task performance.",
            "9": "This comprehensive analysis helps in understanding the impact of these factors on the performance of Transformers.",
            "10": "**PCA Comparison**: The comparison with Principal Component Analysis (PCA) provides additional depth to the study, highlighting the non-linear nature of the data representations learned by Transformers.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Limited Scope**: While the paper focuses on text classification tasks, it would have been beneficial to include experiments on other types of sequential modeling tasks, such as machine translation or text generation, to generalize the findings.",
            "13": "**Generative Models**: The paper mentions the potential for future work in generative domains but does not provide any preliminary results or insights in this area.",
            "14": "Including some initial experiments or discussions on generative models would have added more depth to the study.",
            "15": "**Complexity of ID Estimation**: The paper relies heavily on the TwoNN method for estimating intrinsic dimensions.",
            "16": "While this method is computationally efficient, it would have been useful to discuss its limitations and compare it with other ID estimation techniques in more detail.",
            "17": "**Impact of Hyper-parameters**: Although the paper explores the impact of embedding dimensions and model depth, it does not delve deeply into other hyper-parameters, such as learning rate, batch size, or dropout rates, which could also influence the intrinsic dimensions and performance.",
            "18": "**Real-world Applications**: The practical implications of the findings are discussed, but the paper could benefit from more concrete examples or case studies demonstrating how these insights can be applied in real-world scenarios.",
            "19": "**Theoretical Foundations**: The paper provides empirical evidence for the observed phenomena but lacks a strong theoretical foundation to explain why these patterns occur.",
            "20": "A deeper theoretical analysis would enhance the understanding of the intrinsic dimension's role in Transformers.",
            "21": "#### Conclusion:\n\nOverall, the paper \"An Intrinsic Dimension Perspective of Transformers for Sequential Modeling\" offers valuable insights into the geometrical characteristics of data representations learned by Transformers.",
            "22": "It introduces a novel perspective and provides practical guidance for improving model performance.",
            "23": "However, the study could be strengthened by expanding its scope to other sequential modeling tasks, providing more theoretical explanations, and exploring additional hyper-parameters.",
            "24": "Despite these weaknesses, the paper makes a significant contribution to the field and opens up new avenues for future research."
        },
        "VdynIaqIXr": {
            "0": "Studying the internal representation of deep networks, and in particular of transformers, is a very timely and important research line, and the ID is one of the tools which allows addressing this task.",
            "1": "The monotonic dependence of the ID on the depth is at odds with what observed in convolutional NN for image classification, and also with a similar analysis performed on transformers trained by self-supervision (https://arxiv.org/abs/2302.00294).",
            "2": "Also the focus on the impact of sequence length on learning  (section 3.6 and 3.7) is interesting.",
            "3": "The observation that the ID grows with the embedding dimension  (ED), while the generalisation error decreases (bullet point number 3) seems to me pretty trivial.",
            "4": "Of course by enlarging the ED one recovers a richer presentation, whose ID will be larger, and which will provide better models.",
            "5": "I was not able to understand if the ID is computed by performing a prior average pooling over the sequence, as done in other works on transformers.",
            "6": "If this pooling is not performed, the dimension of the representation is equal to the number of tokens times ED, and not to ED.",
            "7": "The analysis performed in 3.6 is in principle interesting, but inconclusive (the results in table 5 seem to show that the ID cannot be used as a quality proxy to decide if the learning set can be reduced)\nThe analysis presented in 3.4 is very similar to the one presented in [Ansuini 2019]"
        },
        "0F1WTVzYZq": {
            "0": "1.The understanding of Transformer is crucial in field of natural language process, the paper supplied a viewpoint from intrinsic dimension analysis to uncover the behavior of Transformer in sequential classification.",
            "1": "1.Several important related works on analysis of Transformers are missed (such as Revisiting over-smoothing in BERT from the perspective of graph), and the observations of decreasing intrinsic dimension can be viewed as a showcase of over-smoothing.",
            "2": "2.The experiments should be further improved since the paper resorts to experimental study.",
            "3": "For example, more sequential learning tasks should be includes, only text classification seems insufficient, such as machine translation which is a typical sequential learning task for Transformer understanding (see more in question part).",
            "4": "3.Lack of deep understanding of the observation, for example, increasing the embedding dimension will increase (intrinsic dimension).",
            "5": "However, large embedding dimension may suffer from over-fitting even with fixed depth (see more in question part)."
        },
        "BHRasoPBbo": {
            "0": "The analysis of the intrinsic dimension of the hidden representations in transformers trained on NLP tasks is an interesting and relevant topic that has only recently started to be addressed by some studies.",
            "1": "The experimental tests in support of some claims are not solid enough (see my concern regarding the ID vs. accuracy analysis).",
            "2": "Some parts contain technical flaws (see the concern about the PCA-ID computation), and the sentences are phrased in a way that is sometimes hard to follow."
        },
        "IsCaiV5VIS": {
            "0": "originality: Although the intrinsic dimension (ID) is an established concept and the codes have been provided in prior work [1], this paper under review is the first to carefully examine ID (under this particular definition) in Transformers in the text classification setting.",
            "1": "clarity: The methodology and findings are clearly described.",
            "2": "[1] Intrinsic Dimension of Data Representations in Deep Neural Networks, NeurIPS 2019, https://github.com/ansuini/IntrinsicDimDeep quality: a key underlying assumption is questionable, making the observations less convincing\n\nIntrinsic dimension (ID) is defined in the following way: assuming a set of vectors “locally uniformly” lie on a d-dimensional subspace of the full D-dimensional vector space (d <= D), then the ratios of close pairwise distances follow the Pareto distribution parametrized by d. Then, the intrinsic dimension (ID) is the max likelihood estimation of d. (For a formal definition, see Section 2, paragraph “TwoNN Method” on page 3.)",
            "3": "Thus, the ID estimation is only meaningful if the following Assumption 1 holds: \n\nAssumption 1: The ratios of close pairwise distances of Transformer representations (approximately) follow the Pareto distribution.",
            "4": "However, without further justification, the validity of Assumption 1 is questionable.",
            "5": "According to [2], a sufficient condition for Assumption 1 is that the representations are “locally uniform” in density, where \"locally\" means “within the range of the second neighbor for each data point”.",
            "6": "It is unclear whether this sufficient condition holds for Transformer representations.",
            "7": "Nevertheless, note that this is likely not a necessary condition, so there could be other ways to achieve Assumption 1.",
            "8": "However, the authors did not check (or even explicitly mention) Assumption 1, which calls into question the validity of the method and the results.",
            "9": "Moreover, [1] states that for ID estimation for convolutional neural networks (CNNs) representations on image data,  \n\n“the estimated values remain very close to the ground truth ID, when this is smaller than ~20.",
            "10": "For larger IDs and finite sample size, the approach moderately underestimates the correct value, especially if the density of data is non-uniform.",
            "11": "Therefore, the values reported in the following figures, when larger ~20, should be considered as lower bounds.”\n\nNote that in the current paper under review, most of the estimated IDs are greater than 20 (Figures 1-2, Tables 1-5).",
            "12": "Can we trust the estimated IDs by this approach?",
            "13": "Do we have evidence that the estimated values remain very close to the ground truth ID for a larger range of IDs (> 20) in the setting of this paper?",
            "14": "originality: it seems that the current paper under review mostly used the approach of [1] (which was for convolutional neural networks (CNNs) representations on image data), applying it to Transformers on text classification data.",
            "15": "While some observations could potentially be interesting, the main technical contribution of the methodology should be primarily attributed to prior works.",
            "16": "[1] Intrinsic Dimension of Data Representations in Deep Neural Networks.",
            "17": "NeurIPS 2019\n\n[2] Estimating the intrinsic dimension of datasets by a minimal neighborhood information.",
            "18": "Scientific reports, vol.",
            "19": "7, no.",
            "20": "1, p. 12140, 2017"
        },
        "EPJSuPePcr": {
            "0": "None This paper seems to be a study.",
            "1": "And an incomplete one.",
            "2": "It goes from related work where they describe the TwoNN method proposed by another paper, to experiment results.",
            "3": "There is no technical section?",
            "4": "It is also written kind of strangely."
        }
    },
    "VpCqrMMGVm": {
        "llm_review": {
            "0": "**Novelty and Contribution**:\n   - The paper addresses a significant gap in the understanding of the internal mechanisms of large language models (LLMs), specifically focusing on mathematical addition.",
            "1": "This is a novel and important contribution to the field of interpretability in machine learning.",
            "2": "**Methodological Rigor**:\n   - The use of path patching and knockout techniques to identify and validate the importance of specific attention heads is methodologically sound.",
            "3": "These techniques provide a robust framework for understanding the causal relationships within the model.",
            "4": "**Comprehensive Experiments**:\n   - The experiments are thorough and well-designed, covering multiple LLMs (LLaMA2-7B, Qwen-7B, and chatGLM2-6B).",
            "5": "This adds robustness to the findings and ensures that the results are not model-specific.",
            "6": "**Generalizability**:\n   - The paper demonstrates that the identified key heads are not only crucial for addition tasks but also generalize to subtraction tasks.",
            "7": "This suggests that the discovered mechanisms are fundamental to the model's mathematical reasoning capabilities.",
            "8": "**Clear Presentation**:\n   - The paper is well-structured and clearly presents the methodology, experiments, and results.",
            "9": "The use of visual aids, such as attention pattern visualizations and performance graphs, helps in understanding the findings.",
            "10": "**Impact on Future Research**:\n   - By identifying specific components within LLMs that are responsible for mathematical reasoning, this work lays a foundation for future research aimed at improving the interpretability and transparency of these models.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Limited Scope**:\n   - While the focus on mathematical addition is a good starting point, the scope of the study is somewhat limited.",
            "13": "The paper could be strengthened by including other basic arithmetic operations like multiplication and division to provide a more comprehensive understanding of the model's mathematical capabilities.",
            "14": "**Dataset Construction**:\n   - The paper mentions the creation of datasets based on templates but does not provide detailed information on the diversity and complexity of these datasets.",
            "15": "More details on the dataset construction and the rationale behind the chosen templates would enhance the reproducibility and credibility of the experiments.",
            "16": "**Attention to Earlier Layers**:\n   - The paper primarily focuses on the middle layers of the models, where the key heads are located.",
            "17": "However, it acknowledges that earlier layers might still have an indirect influence.",
            "18": "A more detailed investigation into the role of earlier layers could provide a more holistic understanding of the model's internal mechanisms.",
            "19": "**Interpretation of Attention Patterns**:\n   - While the paper provides insights into the attention patterns of key heads, the interpretation of these patterns could be more detailed.",
            "20": "For instance, discussing why certain heads focus on specific tokens and how this relates to the model's overall reasoning process would add depth to the analysis.",
            "21": "**Validation on More Tasks**:\n   - The paper validates the findings on addition and subtraction tasks.",
            "22": "However, extending the validation to more complex mathematical tasks or other types of reasoning tasks would strengthen the claim that the identified mechanisms are fundamental to the model's reasoning capabilities.",
            "23": "**Potential Overfitting to Specific Templates**:\n   - There is a risk that the identified key heads might be overfitting to the specific templates used in the experiments.",
            "24": "Including a wider variety of templates and ensuring that the findings generalize across different types of input would address this concern.",
            "25": "#### Conclusion:\n\nOverall, the paper makes a significant contribution to the field of interpretability in large language models by identifying and validating the importance of specific attention heads in mathematical addition tasks.",
            "26": "The methodological rigor and comprehensive experiments are commendable, and the findings have the potential to impact future research in this area.",
            "27": "However, the paper could be strengthened by expanding the scope of the study, providing more details on dataset construction, and extending the validation to more tasks and templates."
        },
        "GQN85sAbm5": {
            "0": "The analysis is thorough and rigorous.",
            "1": "The paper is clearly written, and the presentation is well-organised and presented.",
            "2": "On top of identifying the “key heads” being focused on addition, the paper shows that the same heads are also involved in subtraction.",
            "3": "While this might be intuitive, considering that one of these tasks is the opposite of the other, it is not obvious that an LLM would discover and utilize this duality.",
            "4": "However, it is unclear whether that is because the heads focus only on numbers or because they are utilising the duality of summation and subtraction.",
            "5": "The paper recognises that later heads depend on earlier ones and attempts to analyse these dependencies (although it appears there are none).",
            "6": "The paper identifies attention heads that take part in the processing of summation but does not look into or explain what each of the “key heads” actually does and what is the mechanism through which it contributes to summation.",
            "7": "Therefore, the paper focuses on _localization_ of the heads that partake in summation, rather than _interpreting_ them.",
            "8": "The paper does not look at alternative representations of numbers.",
            "9": "For example, in words (“two” instead of 2), Roman numerals (II instead of 2), and other languages (二 or ٢ instead of 2).",
            "10": "The lack of such analysis leaves the question open whether these heads simply attend to numerical tokens or whether they are involved in higher-order reasoning about numbers and arithmetic.",
            "11": "Related to the above, the paper seems to focus only on single-digit summation.",
            "12": "It is unclear whether the results would translate to the summation of larger numbers (or more than two numbers).",
            "13": "This is important as prior works have shown that the ability of LLMs to do arithmetic quickly decreases with the increase of the number of digits.",
            "14": "It would be interesting to see if your analysis would be able to provide insights into this phenomenon.",
            "15": "I am not sure how to read the attention patterns in Fig.",
            "16": "How can the attention be negative?",
            "17": "In fact, it does not seem that these heads attend to all numbers.",
            "18": "The first head seems to attend to the completion of “or” with “anges” and the full stop.",
            "19": "Both heads seem to attend only to 3 while solving the task would also require attention to 5.",
            "20": "Therefore, it is not clear how these heads participate in performing summation.",
            "21": "The paper looks predominantly at attention heads.",
            "22": "However, it is well known that a lot of the computation and processing happens in the MLPs.",
            "23": "Hence, a full picture of the interoperation of the mechanisms for summation should also include the MLPs"
        },
        "1bdsUgaWIC": {
            "0": "Authors are tackling an important problem by aiming to understand the inner workings of LLMs.",
            "1": "With the increased pace of advancements happening in the field, it is imperative to gain this understanding.",
            "2": "Authors tackle the problem in a clear manner, by coming up with a clean task (involving addition of 2 integers) and testing their hypothesis systematically.",
            "3": "Their findings indicate that a limited number of attention heads suffice for achieving strong performance across a range of addition tasks.",
            "4": "Importantly, the methodology they introduce can prove valuable for conducting sensitivity analyses in other areas of interest and even facilitate model sparsification.",
            "5": "They validate their hypothesis on several LLMs and a few addition tasks.",
            "6": "Additionally, their preliminary investigations reveal that the attention heads vital for addition tasks also exert a substantial influence on subtraction.",
            "7": "While the authors have indeed posed a clear problem and approached it systematically, I find the setup to be somewhat restrictive.",
            "8": "- Although the authors make a great effort to tackle the task of addition, their focus remains solely on the addition of two integers.",
            "9": "It would be intriguing to see whether their findings extend to addition of multiple integers and rational numbers, as well as their applicability to problems involving multiple addition operations.",
            "10": "- The robustness of this study could be significantly enhanced if the authors were to conduct analogous experiments on subtraction, multiplication, and division.",
            "11": "Such investigations would shed light on whether a select group of attention heads can consistently influence performance across all four mathematical operations."
        },
        "WHjm3zElaq": {
            "0": "Strengths:\n\n- The language of the paper is concise and clear.",
            "1": "- The breadth and depth of the paper is excellent - specifically the use of 3 LLMs\n(LLaMA2-7B, Qwen-7B and chatGLM2-6B), 20 question formats and 10K sample\nquestions.",
            "2": "- The rigorous nature of the paper is excellent - the claims re addition are confirmed via\ndetailed experimentation.",
            "3": "- The most significant finding is that a small number of attention heads are consistently\nused by each model to perform one-digit addition across the various question formats.",
            "4": "Weaknesses:\n\n- The paper (seems to) limit itself to one-digit addition and subtraction - reducing its scope\nto a subset of addition and subtraction.",
            "5": "The abstract should explicitly say that the scope\nis one-digit integer addition.",
            "6": "-  The paper (seems to) limit itself to simple one-digit addition and subtraction (without\n“carry over one” or “borrow one” examples - reducing its scope to a subset of addition\nand subtraction.",
            "7": "The abstract should explicitly say that the scope is simple one-digit\ninteger addition.",
            "8": "-  The paper does not explain how the attention heads (&/or MLP layer) actually perform\nthe addition calculation.",
            "9": "This explanation is left for future work.",
            "10": "-  The paper touches on subtraction, showing similarities, but a detailed analysis is left for\nfuture work.",
            "11": "-  A discussion of the differences in how each of the LLMs implement one-digit addition\nwould have been interesting e.g.",
            "12": "do all the models use roughly the same number of attention heads to implement addition?",
            "13": "If no differences were found, then this would be\nan interesting finding in itself.",
            "14": "-  The small scope of this paper limits the reusability of this work."
        },
        "ax1aEHFP6r": {
            "0": "- a timely topic is treated, how models that are used in practice perform mathematical addition and subtraction\n- a large number of figures that show how attention heads are activated on concrete examples help to make the paper readable - The authors didn't include, as related work, some publications that also deal with mathematical reasoning, such as [1]\n- studying only mathematical addition and subtraction seems restrictive.",
            "1": "I do note that the authors state at the end however: \"_A more thorough study on the subtraction task as well as the validation on more computation tasks (e.g., multiplication and division, etc.)",
            "2": "is left for future work._\"\n\n[1] https://arxiv.org/pdf/2305.08809.pdf"
        }
    },
    "ZDGKPbF0VQ": {
        "llm_review": {
            "0": "**Novelty and Innovation**:\n   - The introduction of Advantage-Leftover Lunch RL (A-L OL) is a significant contribution to the field of offline reinforcement learning (RL) for language models.",
            "1": "The method's ability to use pre-existing data and focus on positive advantage data points is innovative and addresses key challenges in RLHF, such as instability and data hunger.",
            "2": "**Robustness to Noisy Data**:\n   - A-L OL's resilience to noisy or suboptimal training data is a notable strength.",
            "3": "The method's ability to discard data points with negative advantages ensures that the model focuses on beneficial training instances, which is particularly useful in real-world scenarios where data quality can vary.",
            "4": "**Comprehensive Evaluation**:\n   - The paper provides a thorough evaluation of A-L OL across multiple tasks, including the Helpful and Harmless Assistant (HHA) task and Reddit response generation.",
            "5": "The comparison with various baselines, including preference-based and reward-based offline RL methods, as well as PPO, demonstrates the effectiveness and stability of A-L OL.",
            "6": "**Diversity and Quality of Outputs**:\n   - A-L OL not only achieves high average rewards but also generates diverse and high-quality responses.",
            "7": "This is particularly evident in the HHA task, where A-L OL variants produce responses that are rated as more helpful and safe by both GPT-4 and human evaluators.",
            "8": "**Flexibility and Variants**:\n   - The paper explores several variants of A-L OL, such as A-L OL (ref.",
            "9": "free), A-L OL seq., and A-L OL KL, showcasing the flexibility of the approach.",
            "10": "The per-token importance weight variant (A-L OL seq.)",
            "11": "is particularly noteworthy for its ability to improve both performance and diversity.",
            "12": "**Reproducibility**:\n   - The authors provide detailed implementation details, hyperparameters, and a generalized pseudocode for A-L OL, which enhances the reproducibility of the research.",
            "13": "The inclusion of multiple random seeds for experiments further strengthens the reliability of the results.",
            "14": "#### Weaknesses:\n\n1.",
            "15": "**Complexity of Implementation**:\n   - While the paper claims that A-L OL is easy to implement, the introduction of multiple variants and the need for a value estimate layer might add complexity for practitioners who are not deeply familiar with RL techniques.",
            "16": "A more detailed step-by-step guide or code examples could help mitigate this issue.",
            "17": "**Limited Comparison with Online RL Methods**:\n   - The paper primarily focuses on offline RL methods and provides limited comparison with online RL methods like PPO.",
            "18": "Although PPO is included in the HHA task, a more extensive comparison across different tasks could provide a clearer picture of the trade-offs between offline and online RL approaches.",
            "19": "**Dependence on Reward Models**:\n   - The effectiveness of A-L OL heavily relies on the quality of the reward models used.",
            "20": "In scenarios where high-quality reward models are not available, the performance of A-L OL might be compromised.",
            "21": "The paper could benefit from a discussion on how to handle situations with suboptimal or biased reward models.",
            "22": "**Scalability to Larger Models**:\n   - The experiments are conducted on models with up to 7B parameters.",
            "23": "While this is substantial, the scalability of A-L OL to even larger models (e.g., 52B parameters) is not explored.",
            "24": "Given the trend towards increasingly large language models, an analysis of A-L OL's scalability would be valuable.",
            "25": "**Human Evaluation Sample Size**:\n   - The human evaluation is conducted on a relatively small sample size (200 instances).",
            "26": "While this provides useful insights, a larger sample size could offer more robust conclusions about the model's performance in real-world applications.",
            "27": "**Potential Bias in GPT-4 Evaluation**:\n   - The use of GPT-4 for evaluation introduces potential biases, as the model might have inherent preferences or limitations.",
            "28": "While the authors attempt to mitigate positional bias, a more detailed analysis of GPT-4's evaluation reliability and potential biases would strengthen the findings.",
            "29": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in offline RL for language models with the introduction of A-L OL.",
            "30": "The method's robustness to noisy data, comprehensive evaluation, and flexibility are commendable strengths.",
            "31": "However, addressing the complexities of implementation, expanding comparisons with online RL methods, and exploring scalability to larger models would further enhance the impact and applicability of this research."
        },
        "S4SuC0Y8KJ": {
            "0": "The proposed method is clear and easy to implement, with relatively few assumptions.",
            "1": "Therefore, it may have practical merits.",
            "2": "The experiments are relatively throughout and the results are promising.",
            "3": "Human study helps demonstrating the efficacy of the proposed method.",
            "4": "The paper is generally easy to follow.",
            "5": "Unclear method contribution: the proposed method is nearly, if not exactly, a special case of TRPO/PPO method in the bandit setting.",
            "6": "Such a special bandit instantiation has been widely considered in classical RLHF works, such as [1,2].",
            "7": "Advantage-weighted policy optimization is also a well-studied method in offline RL.",
            "8": "For example, Eq.",
            "9": "5 in this paper is very similar to Eq.",
            "10": "4 in AWAC [3], except for the importance weighting that basically comes from TRPO/PPO.",
            "11": "The formulation of considering the entire output sequence as a single action step may suffer from exponentially large action space, which may make policy training harder and less stable.",
            "12": "See for example [4, 5].",
            "13": "As an aside, recent works have already tried to learn a per-token reward function that incorporates arbitrary human-designed scoring function(s), which may better cope with the large action space in NLG problem, see, e.g., [6].",
            "14": "Weighted behavior cloning has been quite extensively used in prior NLP papers, e.g., [6,7,8,9,10,11].",
            "15": "It will make the algorithmic contribution of this paper more clear if the authors can have a paragraph discussing and comparing with such related works, instead of only citing the CRR paper from offline RL.",
            "16": "In Table 2, the comparison with PPO may not be fair, because the reward for PPO is a good-or-bad classifier.",
            "17": "The offline RL methods, on the other hand, are fitted towards the original responses that themselves show high linguistic diversity, which would implicitly guide the offline RL methods, especially A-LoL, towards generating longer and more diverse sequences.",
            "18": "In short, there is no guiding signal for PPO to generate such sequences, while the offline RL methods implicitly have the guidance.",
            "19": "There are several well-established exogenous components in the proposed method, such as (1) importance clipping, (2) discarding negative advantage datapoints, (3) prioritized sampling.",
            "20": "It is unclear how each of those exogenous components contribute to the overall performance.",
            "21": "It is also unclear if the baselines can also benefit from such exogenous components, e.g., (2) and (3).",
            "22": "This again muds the algorithmic contribution of the proposed method and make the experiment results less convincing.",
            "23": "[1] Stiennon, Nisan, et al.",
            "24": "\"Learning to summarize with human feedback.\"",
            "25": "Advances in Neural Information Processing Systems 33 (2020): 3008-3021.",
            "26": "[2] Ouyang, Long, et al.",
            "27": "\"Training language models to follow instructions with human feedback.\"",
            "28": "Advances in Neural Information Processing Systems 35 (2022): 27730-27744.",
            "29": "[3] Peng, Xue Bin, et al.",
            "30": "\"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.\"",
            "31": "arXiv preprint arXiv:1910.00177 (2019).",
            "32": "[4] Guo, Han, et al.",
            "33": "\"Text Generation with Efficient (Soft) $ Q $-Learning.\"",
            "34": "(2021).",
            "35": "[5] Snell, Charlie, et al.",
            "36": "\"Offline rl for natural language generation with implicit language q learning.\"",
            "37": "arXiv preprint arXiv:2206.11871 (2022).",
            "38": "[6] Yang, Shentao, et al.",
            "39": "\"Preference-grounded Token-level Guidance for Language Model Fine-tuning.\"",
            "40": "arXiv preprint arXiv:2306.00398 (2023).",
            "41": "[7] Govardana Sachithanandam Ramachandran, Kazuma Hashimoto, and Caiming Xiong.",
            "42": "2022.",
            "43": "[CASPI] Causal-aware Safe Policy Improvement for Task-oriented Dialogue.",
            "44": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 92–102, Dublin, Ireland.",
            "45": "Association for Computational Linguistics.",
            "46": "[8] Feng, Y., Yang, S., Zhang, S., Zhang, J., Xiong, C., Zhou, M., & Wang, H. (2023).",
            "47": "Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems.",
            "48": "arXiv preprint arXiv:2302.10342.",
            "49": "[9] Norouzi, Mohammad, et al.",
            "50": "\"Reward augmented maximum likelihood for neural structured prediction.\"",
            "51": "Advances In Neural Information Processing Systems 29 (2016).",
            "52": "[10] Sayan Ghosh, Zheng Qi, Snigdha Chaturvedi, and Shashank Srivastava.",
            "53": "How helpful is inverse reinforcement learning for table-to-text generation?",
            "54": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 71–79, 2021.",
            "55": "[11] Marcin Junczys-Dowmunt, Roman Grundkiewicz, Shubha Guha, and Kenneth Heafield.",
            "56": "Approaching neural grammatical error correction as a low-resource machine translation task.",
            "57": "arXiv preprint arXiv:1804.05940, 2018."
        },
        "bIEMVkp07E": {
            "0": "I appreciate the direction of exploring objectives that are not on-policy RL, which is the hot topic these days in fine-tuning LLMs.",
            "1": "I’m glad the discussion of comparison with GOLD exists (Section 2.4), because the motivation and derivation are extremely similar to GOLD (except for a few differences like the per-token action vs. single action distinction, the different treatment of importance weights, etc., as described in Section 2.4).",
            "2": "I think it’s totally fine even if it’s similar to GOLD – there are design differences, a few tricks are used, and more experiments are done.",
            "3": "Related to the above point: for experiments, I especially appreciate the experiments on the helpful-harmless assistant task.",
            "4": "Approximations don’t seem justified mathematically.",
            "5": "Maybe it’s alright given RL+NLP research has too many approximations in general – I’ll need to see what other reviewers think.",
            "6": "- For the “ref free” variant: Is it justified to use 1 as importance weights, in the reference free variant of the algorithm?",
            "7": "I can’t wrap my head around whether that’s an acceptable approximation, or whether that's making the derived Equation (3) or Equation (4) simply incorrect.",
            "8": "- For the “sequence” variant: The approximation of importance rule is a bit strange.",
            "9": "See line 6 of the “variants with alternative importance weight” paragraph on page 4. it’s essentially saying a1 * a2 * … * aT * (b1 + b2 + … + bT) = a1 * b1 + a2 * b2 + … + aT * bT.",
            "10": "But this seems wrong?",
            "11": "Am I understanding this correctly?",
            "12": "Perhaps an explanation of why this is a good approximation will be helpful.",
            "13": "But at the same time, the empirical results aren’t really impacted much, so I’m conflicted on how much I should treat this approximation seriously.",
            "14": "A major issue: did the authors train PPO methods for more training steps (more than 1.3 times the training steps of offline methods)?",
            "15": "If for more training steps, PPO results improve but your results stay stable, then we can’t say PPO is worse.",
            "16": "Phrasing of the main question in paragraph 1 – the main question seems to be the italicized sentence at the end of the first paragraph: “can we perform rewarded learning, similar to PPO, while only using pre-existing data” but the answer is already yes given the literature in the past few years.",
            "17": "- Cringe loss (https://aclanthology.org/2023.acl-long.493.pdf), as well as the older director loss (https://arxiv.org/pdf/2206.07694.pdf) and unlikelihood loss are relevant.",
            "18": "The other algorithms the authors cited are also examples where we can learn from rewards while only using pre-existing data.",
            "19": "I think the authors’ research question can be more specific & take prior work into account.",
            "20": "- In addition, I’m also confused about what “similar to PPO” means: do the authors mean that PPO is a form of “rewarded learning” or do the authors mean “can we perform rewarded learning such that the results on X benchmark is similar to PPO performance?”\n\nNo on-policy RL performance on Reddit generation task.",
            "21": "Is PPO helpful here (given that it’s so popular)?"
        },
        "HvaC6wmQGY": {
            "0": "The single-action assumption made here is very reasonable in the RLHF setting because the transition kernel in natural language generation is deterministic and trivial, such that the standard RLHF is in effect a contextual bandit problem instead of an RL problem.",
            "1": "RL algorithms like PPO are unnecessarily complicated in the standard RLHF setting, so it's nice to see a more stable contextual bandit problem algorithm.",
            "2": "The proposed method is different enough from other alternatives.",
            "3": "In the common HHA benchmark, A-LOL beats other recent preference-based offline RL baselines such as DPO and PRO and other common baselines such as weighted behavior cloning and PPO.",
            "4": "Experiments on reddit generation task also shows the advantage of the proposed method and its flexibility in terms of optimizing for versatile rewards.",
            "5": "A-LoL does not seem to perform better than DPO on the HHA benchmark with the common reward function.",
            "6": "In particular, A-LoL seems to be less \"Helpful\" compared to DPO.",
            "7": "Is there any explanation on that?",
            "8": "The paper seems to be suggesting that there is an issue of reward hacking with the common reward function, is there a concrete example supporting this claim?",
            "9": "In the offline setting where all the data comes from existing offline datasets, the best that we can do seems only to be as good as the best trajectories in the offline datasets.",
            "10": "Is it possible to modify A-LoL such that it can continue to improve itself with online data generated by itself and labeled by the reward model?",
            "11": "minor - The single-action assumption might not always hold especially when the dialogue involves multi-step interaction with the users."
        },
        "esu8qcqbw1": {
            "0": "Strengths:\n- The motivation of the paper and the technical contributions are clear.",
            "1": "- The authors perform a thorough empirical investigation of their proposed technique across several tasks.",
            "2": "The authors conducted various ablation experiments of the proposed idea to show why the algorithm performed well (e.g., R-LOL versus A-LOL).",
            "3": "- The author studies an important question of comparing online and offline policy gradient algorithms.",
            "4": "- The authors also discuss an interesting issue around a subset of RLHF techniques needing human preference data, whereas other techniques do not.",
            "5": "Weaknesses:\n- Given that in language, the transition function is trivial - it is unclear why offline algorithms are more sample-efficient than online algorithms.",
            "6": "Offline algorithms assume access to a lot of quality data, while online algorithms can work with small amounts of data and well-designed reward functions.",
            "7": "- The paper relies on the assumption that each token is not an action but instead a sequence is an action, but it is unclear why this assumption matters.",
            "8": "Most RLHF techniques optimize policies on sequence-level losses, not token-level losses because the reward functions are defined on a sequence.",
            "9": "- For the A-LOL algorithms to work, there is a set of assumptions that are not explicitly mentioned in the paper that could have a big impact on performance.",
            "10": "In particular, if you don't have good data coverage and a good initial policy, then A-LOL will fail, which means that A-LOL and PPO in RLHF have the same assumptions.",
            "11": "- The experiments did not include PPO due to a seed collapsing, but there has been evidence in the literature that this does not happen, which means the authors did not tune this baseline algorithm properly [1].",
            "12": "- The authors claim that their proposed approach is more data-efficient because they filtered out 33% of good responses, but the same procedure can be done for other techniques.",
            "13": "However, a similar procedure was not conducted for the baseline algorithms to show that with less data, their proposed approach performs better.",
            "14": "[1] PAIRWISE PROXIMAL POLICY OPTIMIZATION: HARNESSING RELATIVE FEEDBACK FOR LLM ALIGN- MENT  https://arxiv.org/pdf/2310.00212.pdf"
        }
    },
    "d94x0gWTUX": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel framework, Themis, which integrates external tools into reward modeling.",
            "1": "This approach addresses significant limitations of conventional reward models, such as difficulties in arithmetic computation, code execution, and factual lookup.",
            "2": "**Comprehensive Evaluation**: The authors validate their approach across a wide range of domains and tasks, demonstrating a noteworthy improvement of 17.7% in preference ranking and outperforming Gopher 280B by 7.3% on the TruthfulQA task.",
            "3": "This extensive evaluation provides strong evidence of the effectiveness of the proposed method.",
            "4": "**Transparency and Interpretability**: Themis offers a transparent and sequential account of actions and reasoning traces, enhancing human interpretability and trustworthiness.",
            "5": "This is a significant improvement over the \"blackbox\" nature of conventional reward models.",
            "6": "**Public Availability of Resources**: The authors have made the code, data, and model checkpoints publicly available.",
            "7": "This openness facilitates further research and replication of their results, contributing to the scientific community.",
            "8": "**Detailed Dataset Creation**: The creation of the Tool-Augmented Reward Dataset (TARA) is well-documented, involving a multi-step process that includes human and agent interactions.",
            "9": "This dataset is a valuable resource for future research in tool-augmented reward modeling.",
            "10": "**Scalability**: The paper demonstrates that Themis scales well with model size, showing improved performance with larger models.",
            "11": "This scalability is crucial for practical applications in real-world scenarios.",
            "12": "**Human Preference Evaluation**: The human evaluation results, showing a 32% average win rate over baselines, provide additional validation of the practical utility of Themis in real-world applications.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Complexity of Implementation**: The integration of multiple external tools and the sequential reasoning process add significant complexity to the implementation of Themis.",
            "15": "This complexity might pose challenges for researchers and practitioners looking to adopt this approach.",
            "16": "**Dependence on External Tools**: The reliance on external tools for certain tasks could introduce dependencies and potential points of failure.",
            "17": "For instance, if an external tool is unavailable or provides incorrect information, it could negatively impact the performance of Themis.",
            "18": "**Limited Analysis of Failure Cases**: While the paper demonstrates the effectiveness of Themis, it lacks a detailed analysis of failure cases.",
            "19": "Understanding the scenarios where Themis does not perform well would provide valuable insights for further improvement.",
            "20": "**Generalization to Other Domains**: Although the paper shows promising results in the evaluated tasks, it remains to be seen how well Themis generalizes to other domains and tasks not covered in the study.",
            "21": "Additional experiments on a broader range of tasks would strengthen the claims of generalizability.",
            "22": "**Resource Intensity**: The approach requires significant computational resources, especially when scaling to larger models.",
            "23": "This resource intensity might limit the accessibility of Themis to researchers with limited computational capabilities.",
            "24": "**Evaluation Metrics**: The paper primarily uses accuracy and human preference win rates as evaluation metrics.",
            "25": "While these metrics are useful, additional metrics such as robustness, efficiency, and real-time performance would provide a more comprehensive evaluation of Themis.",
            "26": "**Potential Bias in Tool Selection**: The selection of tools and the design of the toolbank might introduce biases that could affect the performance of Themis.",
            "27": "A more detailed discussion on the criteria for tool selection and potential biases would be beneficial.",
            "28": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of reward modeling by introducing Themis, a tool-augmented framework that enhances the capabilities of reward models.",
            "29": "The comprehensive evaluation and public availability of resources are commendable.",
            "30": "However, the complexity of implementation, dependence on external tools, and limited analysis of failure cases are areas that need further attention.",
            "31": "Addressing these weaknesses in future work would further solidify the contributions of this research."
        },
        "Rr8S0Vp25s": {
            "0": "**Strength 1**: The idea of augmenting reward models with tools is very interesting, novel, and timely.",
            "1": "**Strength 2**: The proposed method provides a nice and logical way for tools to be included in the reward design process.",
            "2": "**Strength 3**: This paper provides some interesting experiments such as application to RLHF and scaling experiments.",
            "3": "**Weakness 1**: One of my main concerns is lack of experiments on standard reward modeling datasets.",
            "4": "There are many datasets not included in the paper such as the Anthropic HH dataset, Stack Overflow, OpenAI WebGPT, and ChatGPT comparisons datasets.",
            "5": "They do conduct analysis on a small portion of the HH dataset, but not on the provided testing set.",
            "6": "In addition, they show worst test accuracy than is reported in some other papers that only use conventional reward modeling [1].",
            "7": "Since the main claim of the paper is that by using tools they can improve the accuracy of reward models, I think their method should be validated on these popular datasets.",
            "8": "[1] Dong, Hanze, et al.",
            "9": "\"Raft: Reward ranked finetuning for generative foundation model alignment.\"",
            "10": "arXiv preprint arXiv:2304.06767 (2023).",
            "11": "**Weakness 2**: Little Hyperparameter study.",
            "12": "THEMIS introduces various hyperparameters, but the sensitivity of model performance to these hyperparameters is not discussed.",
            "13": "**Weakness 3**: This paper does not discuss a significant limitation of this method: the difficulty of creating the dataset.",
            "14": "The dataset creation process consists of various complex steps, involves tool selection and design of heuristics.",
            "15": "This seems to be difficult to scale to large scale preference datasets.",
            "16": "**Weakness 4**: I think that this paper could use a more in depth discussion of related works.",
            "17": "In particular, various works have attempted to use similar tools such as a compiler in the reward design process [2,3,4,5] and [4] use it to guide the reward model training.",
            "18": "Discussing these works could help better frame the contribution of this work.",
            "19": "[2] Le, Hung, et al.",
            "20": "\"Coderl: Mastering code generation through pretrained models and deep reinforcement learning.\"",
            "21": "Advances in Neural Information Processing Systems 35 (2022): 21314-21328.",
            "22": "[3] Shen, Bo, et al.",
            "23": "\"Pangu-coder2: Boosting large language models for code with ranking feedback.\"",
            "24": "arXiv preprint arXiv:2307.14936 (2023).",
            "25": "[4] Bukharin, Alexander, et al.",
            "26": "\"Deep Reinforcement Learning from Hierarchical Weak Preference Feedback.\"",
            "27": "arXiv preprint arXiv:2309.02632 (2023).",
            "28": "[5] Shojaee, Parshin, et al.",
            "29": "\"Execution-based code generation using deep reinforcement learning.\"",
            "30": "arXiv preprint arXiv:2301.13816 (2023)."
        },
        "Drpkvb5ubb": {
            "0": "- Having a level of reasoning and interpretability is great feature to have for reward models\n- The experiments and provided implementation details look comprehensive 1- How to trust tools is an important aspect to consider here.",
            "1": "At least in the examples, it looks like there is a risk of biasing the reward model and generative model to outputs of specific tools being used.",
            "2": "This could be concerning as tools are not necessarily unbiased.",
            "3": "2- It is not entirely clear how GPT-4 is used to generate RM training data.",
            "4": "Note that GPT-4 itself is a system if the proposal is to use GPT-4 to train RM, one can argue why not directly train RM on GPT-4 data or use GPT-4 directly as reward model.",
            "5": "3- When alpha in eq.",
            "6": "2 is set to zero, we converge to typical tool use via simple prompting right?",
            "7": "the RM is still a pretrained model and can be prompted to use tools even without explicit loss terms on tool use.",
            "8": "Is this understanding correct?",
            "9": "4- Model size of 7B is quite small to capture knowledge to compete with the tools used in this paper.",
            "10": "I think a more realistic setup would be to take a larger model and the gap between say Wiki tool and that result could look very different.",
            "11": "5- I think the write-up could improve, especially for Sec 3.1, I had difficulty understanding exactly how GPT-4 was used and how training data was prepared"
        },
        "NpKwNl7kbO": {
            "0": "(1) The paper addresses an important issue in reward modeling by introducing a tool-augmented approach to enhance the effectiveness of RMs.",
            "1": "(2) The proposed methodology of integrating external tools into RMs is innovative and practical, allowing for dynamic decision-making and reasoning processes.",
            "2": "(3) The experimental results demonstrate significant improvements in preference ranking and outperformance of Themis compared to baseline RMs, validating the effectiveness of the approach.",
            "3": "The description of the method is not very clear.",
            "4": "My understanding is that the reward model first generates some explanations based on the inputted question and answer, and then connects a fully connected layer to the final hidden state to produce a scalar reward."
        }
    },
    "tUM39YTRxH": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel framework, TEXT2REWARD, which leverages large language models (LLMs) to generate dense reward functions for reinforcement learning (RL).",
            "1": "This approach is innovative as it automates the traditionally manual and expertise-intensive process of reward shaping.",
            "2": "**Data-Free Framework**: TEXT2REWARD is data-free, meaning it does not require specialized knowledge or domain data to generate reward functions.",
            "3": "This significantly reduces the cost and effort associated with developing reward functions for RL tasks.",
            "4": "**Interpretable and Flexible Reward Codes**: The generated reward codes are interpretable and can cover a wide range of tasks.",
            "5": "They utilize existing packages and allow for iterative refinement with human feedback, enhancing their flexibility and applicability.",
            "6": "**Comprehensive Evaluation**: The framework is evaluated on multiple benchmarks, including robotic manipulation (MANISKILL 2, METAWORLD) and locomotion environments (MUJOCO).",
            "7": "The results show that policies trained with TEXT2REWARD-generated reward codes achieve similar or better performance compared to expert-written codes.",
            "8": "**Human Feedback Integration**: The framework includes a mechanism for incorporating human feedback to iteratively refine the reward functions.",
            "9": "This human-in-the-loop approach addresses the issue of language ambiguity and improves the success rates of learned policies.",
            "10": "**Real-World Applicability**: The paper demonstrates the real-world applicability of the framework by deploying policies trained in a simulator on a real robot.",
            "11": "This highlights the practical potential of TEXT2REWARD in real-world scenarios.",
            "12": "**Detailed Experimental Setup**: The paper provides a detailed description of the experimental setup, including the environments, tasks, and evaluation metrics.",
            "13": "This transparency enhances the reproducibility of the results.",
            "14": "**Qualitative Analysis**: The paper includes a qualitative analysis of the differences between generated reward functions and expert-written ones.",
            "15": "This analysis provides insights into the strengths and limitations of the generated codes.",
            "16": "#### Weaknesses:\n\n1.",
            "17": "**Dependence on LLM Quality**: The effectiveness of TEXT2REWARD heavily relies on the quality and capabilities of the underlying LLMs.",
            "18": "As LLMs evolve, the performance of the framework may vary, and it may require updates to leverage the latest advancements in LLMs.",
            "19": "**Limited Task Coverage**: While the framework shows promising results on a variety of tasks, the evaluation is limited to specific benchmarks.",
            "20": "It would be beneficial to see how TEXT2REWARD performs on a broader range of RL tasks and environments.",
            "21": "**Human Feedback Quality**: The quality of human feedback plays a crucial role in refining the reward functions.",
            "22": "The paper does not provide a detailed analysis of how different types of feedback (e.g., from experts vs. non-experts) impact the performance of the framework.",
            "23": "**Scalability Concerns**: The iterative process of refining reward functions with human feedback may raise scalability concerns, especially for complex tasks that require multiple rounds of feedback.",
            "24": "The paper does not address how the framework scales with increasing task complexity and feedback iterations.",
            "25": "**Sparse-to-Dense Reward Generation**: The paper mentions the potential of a sparse-to-dense reward generation paradigm but does not explore it in detail.",
            "26": "Investigating this approach could provide additional insights and improvements to the framework.",
            "27": "**Baseline Comparisons**: While the paper compares TEXT2REWARD with expert-written reward functions and a baseline adapted from Yu et al.",
            "28": "(2023), it would be beneficial to include comparisons with other state-of-the-art reward shaping methods to provide a more comprehensive evaluation.",
            "29": "**Error Handling and Debugging**: The paper briefly mentions the use of code execution feedback to reduce errors in generated reward codes.",
            "30": "However, it does not provide a detailed discussion on the types of errors encountered and the effectiveness of the debugging process.",
            "31": "**User Study**: Conducting a user study to evaluate the ease of use and effectiveness of the human-in-the-loop feedback mechanism would provide valuable insights into the practical usability of the framework.",
            "32": "#### Conclusion:\n\nOverall, TEXT2REWARD presents a significant advancement in the field of reinforcement learning by automating the generation of dense reward functions using large language models.",
            "33": "The framework's ability to generate interpretable and flexible reward codes, combined with its integration of human feedback, makes it a promising tool for a wide range of RL tasks.",
            "34": "However, there are areas for improvement, such as expanding task coverage, addressing scalability concerns, and providing more detailed analyses of feedback quality and error handling.",
            "35": "Despite these weaknesses, the paper makes a substantial contribution to the intersection of reinforcement learning and code generation, and it opens up new avenues for future research."
        },
        "C81YNNAhLY": {
            "0": "The paper addresses the important problem of dense reward specification in reinforcement learning, using a method based on zero-shot and few-shot prompting of a language model, something not shown before except for concurrent work **[1]**.",
            "1": "The authors show that their method is competitive with human expert reward specification in a broad series of experiments.",
            "2": "They moreover demonstrate one such policy to transfer successfully to real world execution.",
            "3": "The authors also include an analysis of the failure modes of reward specification, specifically for the cases in which the generated reward function code leads to a python runtime error.",
            "4": "**[1]** Ma et al., Eureka: Human-Level Reward Design via Coding Large Language Models, https://eureka-research.github.io/ The paper essentially proposes prompting techniques for code generation with pre-trained language models accessible via API, specifically GPT-4.",
            "5": "Despite its interesting conclusion and results, it consists at most of an interesting observational study over the capabilities of GPT-4, as the prompting techniques appear straightforward, fundamentally easy to execute manually by any user of the GPT-4 online api platform in a process of trial and error for reward design.",
            "6": "I would not consider this a problem worthy of rejection per se, as similar “prompting techniques” results have been published before in machine learning venues, such as the famous “chain of thought” prompting technique.",
            "7": "Still, it is my opinion that this limits the significance of the work, as GPT-4 simply fills in the shoes of an expert reinforcement learning reward coder on well known simulation environments (while “chain of thought” prompting was an innovation in how to query a model for more general purpose NLP benchmarks).",
            "8": "More importantly, to my understanding, no truly novel environments are tackled in this paper.",
            "9": "For most of the shown tasks, GPT-4 can reasonably be expected to draw from training data containing countless reward function specifications from those environments, if not reward functions for the desired task itself.",
            "10": "Moreover, the reliance of the paper on GPT-4 makes the results inherently irreproducible, as OpenAI does not have a policy of indefinitely supporting api access to specific snapshot versions of their models (specifically, GPT-4-0314 will be deprecated on June 13th 2024 at the earliest)."
        },
        "DrwOPh39nW": {
            "0": "************************************************************Evaluation Comprehensiveness:************************************************************ The authors utilized 17 tasks in ManiSkill2 and Metaworld, 6 in Mujoco, and also a few real world experiments — this is quite comprehensive compared to the average ICLR RL paper.",
            "1": "****************************Experiments:**************************** \n\n- Real world experiments are always nice!",
            "2": "- Human feedback experiments are also great and demonstrate that on certain tasks the method can improve the policy learning performance — something that isn’t always straightforward even with humans redesigning reward functions to try to improve polichy learning\n\n******************Results:****************** I think results are solid compared to the oracle.",
            "3": "**********Motivation:********** The paper motivation is nice — reducing human effort in scaling policy training by using LLMs for python/numpy reward function design.",
            "4": "************************************************************Contribution over prior work:************************************************************ I’m not too convinced on the major **********technical********** contribution over Yu et al.",
            "5": "2023 (Language to Rewards for Skill Synthesis).",
            "6": "Compared to that paper, the main claimed novelty is dense vs sparse reward and the use of standard python code: “Different from recent work….our free-form dense reward code has a wider coverage of tasks and can use established coding packages.” But that paper also uses very pythonic code, furthermore utilizes sparse reward mainly due to using MPC.",
            "7": "I’m not too convinced that the pythonic → python and MPC → RL are large technical contributions on their own.",
            "8": "This should be clarified more specifically/clearly in the paper if there is another technical contribution over Yu 2023, and if not, then is one of the main reasons for my score.",
            "9": "************************Experiments:************************\n\n- The authors should compare against Yu 2023, especially if claiming their dense free-form reward + use of established coding packages can result in superior performance.",
            "10": "The comparison isn’t exactly 1-1 given the claims, but currently there is no comparison to any baseline to contextualize the performance of the method.",
            "11": "In fact, I think a comparison with Yu 2023 + RL would be fairest, as Yu 2023 likely can use RL instead of MPC without change.",
            "12": "- Open-source LLMs: Utilizing closed-source LLMs has obvious downsides, e.g., reproducibility (API backend can change at any time) and access to academic researchers (cost per token vs able to be used on a standard GPU setup).",
            "13": "It would be beneficial to the community to demonstrate some results with some smaller open source models like LLaMa-2.",
            "14": "**************************Minor Issues:**************************\n\n- 4.1: Appendex → Appendix\n- I think it’d be nice to have a few small examples in the **********main paper********** of generated reward functions (not full things, just a few lines).",
            "15": "This makes the experiment section more readable without needing to jump around to the appendix."
        },
        "pCZXpMBSgf": {
            "0": "This paper studies the pertinent problem of automated reward design using LLMs.",
            "1": "Given that reward design is a fundamental challenge in RL and that LLMs for decision making have largely been limited to high-level planning tasks, this paper offers a fresh perspective and a nice solution to the growing literature of LLMs for problem design and low-level robotic control.",
            "2": "This paper's method is novel and more flexible than a prior work (Yu et al., 2023) in that it does not require extensive manual templating per robotic embodiment or task and is capable of generating free-form reward functions.",
            "3": "It incorporates a compact representation of the environment, background knowledge (e.g., available function APIs), and/or few-shot examples to successfully do so.",
            "4": "Finally, this paper demonstrates interesting use case of the proposed method, such as real-world evaluation as well as learning from human feedback.",
            "5": "The paper is well-written and free of grammatic errors.",
            "6": "The primary weakness of the paper is that most evaluation tasks are from benchmarks that have been released before GPT-4's knowledge cutoff date (September, 2021).",
            "7": "Mujoco and Metaworld tasks have been extensively studied in the reinforcement learning literature; ManiSkill2, though released recently, have many overlapping tasks with ManiSkill, which was released in mid 2021; in particular, most of the tasks, to the best of my knowledge, were in the original ManiSkill benchmark.",
            "8": "Given this, it is not clear whether the reward design capability of T2R can readily transfer to an unseen task in a new simulator.",
            "9": "Relatedly, the \"novel\" behavior on the Mujoco locomtoin tasks have appeared in prior literature; for example, Hopper back flip is shown in Christiano et al., 2017.",
            "10": "It's unclear whether T2R has benefited from that knowledge possibly being in the training set of the backbone LLM.",
            "11": "Most manipulation tasks are of \"pick-and-place\" or opening/closing/pushing nature.",
            "12": "These are also the most common types of manipulation tasks that the RL literature has studied.",
            "13": "It is possible that GPT-4 is generally adept at writing reward functions for those task types.",
            "14": "T2R appears to still work best with few-shot examples.",
            "15": "In many tasks that do not belong to a family of tasks introduced by a benchmark, providing few-shot examples can still be difficult.",
            "16": "For each task, only one reward function is reported.",
            "17": "It is not clear whether T2R is robust to stochasticity in LLM generation."
        },
        "DHH1jLUgfp": {
            "0": "- Interesting application of a new tool!",
            "1": "Using LLMs to generate reward code seems like an easy way to simplify problems we may not already have solutions for, but can describe in language, and is a completely different way around the sparse reward problem\n- Zero-shot results seem pretty strong across all environments\n- Nice results on new tasks that (as far as I know) we don't have expert reward for (e.g.",
            "2": "Hopper flipping) - There is no qualitative analysis/discussion of what the source of the improvement is:\n  - Why does Zero-shot outperform Few-shot on Turn Faucet, Open Cabinet Door, Open Cabinet Drawer?",
            "3": "- Why does Few-shot fail to outperform Oracle even though Oracle is in context (Lift Cube, Turn Faucet, Push Chair, Open Cabinet Drawer)?",
            "4": "- In the cases that few-shot improves on zero-shot, what is the source of this improvement?",
            "5": "- There are a lot of missing details in the experiments:\n  - There is only one example of human feedback (Figure 6) and it is in a schematic diagram for a task that does not have ground truth, I would like to see a few traces of the whole round (generation, feedback, generation, feedback, generation) in order to understand what exactly is happening\n  - For results in Table 1 and Figure 4 on novel behaviors, the standards for human evaluation and who conducted the evaluation (the authors, peers from the same institution, crowdsourced to a platform) are missing\n  - The experiments in Table 1 and Figure 4 are supposedly conducted in a zero-shot setting (caption in Figure 4), yet Figure 6 gives a schematic for ambiguity resolution which would imply a few-shot result for novel behaviors, an experiment which I do not see in the paper and appendices.",
            "6": "- I do not see the choice of $k$ for the number of neighbors that appear in the few-shot context\n  - For generated rewards in Appendix D.1 on Pick Cube, the few-shot vs. oracle code is almost indistinguishable except for 2 constants (cube placed at goal reward, grasped reward).",
            "7": "Given this difference is so small, it seems important to know what the human feedback was: are we just getting lucky?",
            "8": "- As before, given that the few-shot vs. oracle code is so close on Pick Cube (the only example we have to judge), why is it the case that the few-shot generation is underperforming oracle generation in other settings (Lift Cube, Turn Faucet, Open Cabinet Drawer, Push Chair)?",
            "9": "- Is it always the case that the Oracle code for the task is put into the context for Few-shot?",
            "10": "- Section 4.2 is about code errors that occur before any RL happens, and this seems like a necessary filtering step, but I think having an example of the generation/feedback process is much more important in the main body than Table 2.",
            "11": "- How long does the iteration loop take?",
            "12": "Each iteration requires training policies, so it is quite expensive, and it may be nice to think about early evaluation\n- It would be nice to include code examples for the novel behavior tasks to see what is happening.",
            "13": "Given there is no baseline in this case, simply presenting a quantitative evaluation without any analysis is a little sparse..."
        }
    },
    "EE75tyB5Ay": {
        "llm_review": {
            "0": "**Comprehensive Dataset Collection**: The authors have created a new dataset, HC-Var, which includes a variety of prompts, text lengths, topics, and language tasks.",
            "1": "This dataset is a significant contribution as it allows for a more thorough investigation of the generalization capabilities of ChatGPT detection methods.",
            "2": "**Detailed Analysis**: The paper provides an in-depth analysis of the generalization behavior of training-based ChatGPT detection methods.",
            "3": "The authors explore various factors that can cause distribution shifts, such as prompts, text lengths, topics, and language tasks.",
            "4": "This comprehensive approach helps in understanding the limitations and strengths of current detection methods.",
            "5": "**Theoretical Insights**: The theoretical analysis provided in Section 4.3 is a strong point.",
            "6": "It offers a deeper understanding of why models trained on certain datasets may fail to generalize well.",
            "7": "The concept of \"Human-ChatGPT Alignment\" and the distinction between \"ChatGPT Direction\" and \"Irrelevant Direction\" are particularly insightful.",
            "8": "**Practical Recommendations**: The paper offers practical insights and recommendations for data collection and model training.",
            "9": "For instance, the importance of collecting ChatGPT texts that are similar in length to human texts to avoid overfitting to irrelevant features is a valuable takeaway.",
            "10": "**Transfer Learning**: The exploration of transfer learning as a method to improve generalization across different tasks and topics is well-executed.",
            "11": "The results show that models can indeed benefit from transfer learning, which is a practical approach for real-world applications.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Limited Scope of Language Models**: The paper primarily focuses on ChatGPT and does not explore the generalization of detection methods to texts generated by other language models like LLaMA2.",
            "14": "This limits the applicability of the findings to a broader range of language models.",
            "15": "**Partial Text Generation**: The paper does not address scenarios where a text is partially written by ChatGPT and partially by a human or another language model.",
            "16": "This is a common real-world scenario that could affect the performance of detection methods.",
            "17": "**Programming Code Writing**: While the paper covers various natural language tasks, it does not include tasks like programming code writing, which is another area where ChatGPT is widely used.",
            "18": "Including such tasks could provide a more comprehensive understanding of the generalization capabilities of detection methods.",
            "19": "**Evaluation Metrics**: The paper primarily uses F1 score, TPR, and 1-FPR as evaluation metrics.",
            "20": "While these are standard metrics, including additional metrics like precision, recall, and confusion matrices could provide a more nuanced understanding of model performance.",
            "21": "**Visualization and Interpretability**: While the paper includes some visualizations, more detailed and interpretable visualizations of model performance and feature importance could enhance the understanding of why certain models generalize better than others.",
            "22": "**Real-World Application**: The paper could benefit from a discussion on the practical implications of the findings.",
            "23": "For instance, how can organizations implement these insights to improve their ChatGPT detection systems?",
            "24": "What are the potential challenges in deploying these models in real-world settings?",
            "25": "#### Conclusion:\n\nOverall, the paper makes a significant contribution to the field of ChatGPT detection by providing a comprehensive analysis of the generalization capabilities of training-based methods.",
            "26": "The creation of the HC-Var dataset and the detailed empirical and theoretical studies are commendable.",
            "27": "However, the paper could be strengthened by addressing the limitations related to the scope of language models, partial text generation, and including more diverse evaluation metrics and visualizations.",
            "28": "Additionally, a discussion on the practical implications of the findings would make the paper more relevant to real-world applications."
        },
        "mzihAVfCN9": {
            "0": "This work studies an important and timely problem for detecting LLM-generated content.",
            "1": "Experiments have conducted for in-distributed settings as well as OOD settings involving content length shift and topic/domain shift.",
            "2": "The feature attribution analysis is an interesting and novel angle of study in the context of LLM-generated content detection.",
            "3": "The study seeks for detecting content generated by ChatGPT, which is just an interface where the backend model keeps evolving.",
            "4": "Hence, it is hard to say if the experimental results and analysis are reproducible and sustainable.",
            "5": "In my opinion, this type of study should be conducted for a static LLM.",
            "6": "Length shift and domain/topic shift represent limited types of distribution shift that is easily detectable.",
            "7": "The authors could have considered more implicit shift where content is paraphrased with syntax-controlled paraphrasing or style transfer, like those used in recent approaches for data pollution attack / defense.",
            "8": "Typos:\n\n3.1:\ndon't -> do not"
        },
        "KUMpz1d3yx": {
            "0": "* The authors present a novel data set.",
            "1": "* The analysis is detailed and comprehensive.",
            "2": "* They provide insights on the data collection and domain adaption strategy.",
            "3": "* ChatGPT Direction seems to be not well motivated.",
            "4": "It needs a why, not just a  what and a how.",
            "5": "* This work exclusively discusses the train-based methods, which are smaller in scope."
        },
        "SZhaUPcWsw": {
            "0": "- **Comprehensive Investigation**: The paper conducts a thorough analysis of the generalization behaviors of existing methods under distribution shifts caused by various factors like prompts, text lengths, topics, and language tasks.",
            "1": "- **New Dataset**: The authors contribute to the field by collecting a new dataset containing both human and ChatGPT-generated texts, facilitating in-depth studies on the detection methods.",
            "2": "- **Insightful Findings**: The research uncovers insightful findings, providing valuable guidance for the development of future methodologies and data collection strategies for ChatGPT detection.",
            "3": "- The authors used three prompts in Figure 1; but in general, users might use various prompts.",
            "4": "This is not aligned with real user usage,\n- The experiments are limited to CHATGPT, we do not know whether these conclusions still hold in GPT4 or other open-source LLMs.",
            "5": "- Most findinds seem obvious."
        }
    },
    "vwSxJEq8VO": {
        "llm_review": {
            "0": "**Novel Framework**: The paper introduces a novel framework for transforming natural language descriptions into executable code, which is a significant step forward in automating code generation.",
            "1": "This approach can potentially streamline the development process in machine learning applications.",
            "2": "**Fine-Tuning of Llama Model**: The use of the Llama model for fine-tuning and ranking different solutions is a strong point.",
            "3": "This method leverages the extensive pre-training of Llama and adapts it to specific tasks, which can lead to more accurate and contextually appropriate code generation.",
            "4": "**Smart-GPT Technique**: The integration of the Smart-GPT technique to refine the instructions generated by Llama is innovative.",
            "5": "This additional layer of processing helps in identifying and correcting logical errors, thereby improving the quality of the generated code.",
            "6": "**Empirical Validation**: The paper provides empirical validation of the proposed framework through experiments on Kaggle competitions.",
            "7": "The results demonstrate that the generated code is not only compilable but also performs well according to the evaluation metrics, which is a strong indicator of the framework's effectiveness.",
            "8": "**Comprehensive Dataset**: The use of the Code4ML dataset, which includes Python code snippets, contest summaries, and data descriptions from Kaggle competitions, is a significant strength.",
            "9": "This dataset provides a rich resource for training and validating the proposed framework.",
            "10": "**Detailed Methodology**: The paper provides a detailed description of the methodology, including the prompts used for GPT-3 and Llama, the fine-tuning process, and the sequential pipeline for instruction-to-code transformation.",
            "11": "This level of detail enhances the reproducibility of the research.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Scope of Evaluation**: While the paper presents results from five Kaggle competitions, a broader evaluation across more diverse tasks and datasets would strengthen the claims of generalizability and robustness of the proposed framework.",
            "14": "**Dependence on Pre-trained Models**: The framework heavily relies on pre-trained models like GPT-3 and Llama.",
            "15": "While this is a strength in terms of leveraging existing knowledge, it also means that the framework's performance is limited by the capabilities and biases of these models.",
            "16": "**Smart-GPT Performance Variability**: The paper notes that Smart-GPT does not consistently outperform unprocessed Llama instructions.",
            "17": "This inconsistency suggests that the refinement process may need further optimization to ensure reliable improvements in instruction quality.",
            "18": "**Complexity of Implementation**: The multi-stage process involving fine-tuning, inference, and refinement adds complexity to the implementation.",
            "19": "This complexity might pose challenges for practitioners looking to adopt the framework in real-world applications.",
            "20": "**Lack of User Study**: The paper does not include a user study to evaluate the usability and effectiveness of the generated code from the perspective of end-users.",
            "21": "Such a study could provide valuable insights into the practical utility of the framework.",
            "22": "**Potential Overfitting**: The fine-tuning process on a specific dataset (Code4ML) raises the possibility of overfitting to the characteristics of that dataset.",
            "23": "Evaluating the framework on entirely different datasets would help in assessing its generalization capabilities.",
            "24": "**Conclusion:**\n\nOverall, the paper presents a promising approach to automating the transformation of natural language ML task descriptions into executable code.",
            "25": "The novel framework, fine-tuning of the Llama model, and the use of Smart-GPT for instruction refinement are notable contributions.",
            "26": "However, the framework's dependence on pre-trained models, the variability in Smart-GPT's performance, and the limited scope of evaluation are areas that could be improved.",
            "27": "Addressing these weaknesses in future work could further enhance the robustness and applicability of the proposed approach."
        },
        "QmhPVCvjSw": {
            "0": "- I found the decomposition of broad task descriptions into concrete subsection instructions interesting and it seems like a decent idea to explore.",
            "1": "- I don't think this work is anywhere close to publication.",
            "2": "Honestly, it feels more like a project report submitted for publication, and it does not hold the rigor required for publication.",
            "3": "- There are some major issues with the paper, some of which I list below:\n  - Related work: The entire related work section needs to be more thorough and relevant to the domain.",
            "4": "Authors have cited works like the Sparks of AGI, Codex, and Palm papers, but do not mention numerous other works such as CodeT5, StarCoder etc.",
            "5": "- Seemingly random papers were mentioned in the Related work, such as [1].",
            "6": "I could not figure out how this work explored zero-shot learning and how zero-shot learning extends the capabilities of LLMs to generate code in unseen languages!",
            "7": "- The entire field of AutoML has not been mentioned, even though they do something similar to what this work aims to do.",
            "8": "- Approach: The entire paper is NL to ML code using GPT.",
            "9": "Llama is used for prompt generation given natural language instructions, but the training data for it comes from the GPT model.",
            "10": "Why can't the GPT model be used to decompose the description for the test data itself, especially when generating code from detailed instructions is done by the GPT model?",
            "11": "What is the utility of the fine-tuned Llama model?",
            "12": "- Baselines: There are no baselines provided.",
            "13": "How does the GPT model do without any prompt decomposition?",
            "14": "How do other models perform?",
            "15": "- Test set size: Results are presented for 5 competitions.",
            "16": "That is not enough sample size to judge the performance.",
            "17": "- Overall, the paper text does not do a good job of describing the approach well and it needs to be improved.",
            "18": "References:\n\n[1] - Thirunavukarasu, Arun James.",
            "19": "\"Large language models will not replace healthcare professionals: curbing popular fears and hype.\"",
            "20": "Journal of the Royal Society of Medicine (2023): 01410768231173123."
        },
        "H8Dd2vgGKa": {
            "0": "The topic is very interesting.",
            "1": "The authors try to design a framework that can solve ML tasks with large language models.",
            "2": "Several parts look good, for example, first create high-level code instructions, then transform these instructions into Python code.",
            "3": "In Table 1, the authors showed reasonable results in five competitions.",
            "4": "The presentation is bad.",
            "5": "Several parts are unclear or missing.",
            "6": "For example, what are the inputs for prompts in Figure 3 and Figure 4?",
            "7": "What is \"**Kaggle code**\"?",
            "8": "What is your training data from?",
            "9": "In Section 4.1, the authors say that GPT-3 is used to generate high-level instructions.",
            "10": "However, in Figure 1, it looks like OpenAI models are used.",
            "11": "It is important that the authors have a clear description.",
            "12": "There are no detailed performance results: the performance of high-level code instruction creation or the quality of code generation.",
            "13": "It is not enough to have sample results on 5 competitions ( shown in Table 1).",
            "14": "There is no baseline in this submission."
        },
        "EhQvj6lB8M": {
            "0": "* Two-phase approach: The two-step approach of instructions followed by code generation provides flexibility and control lacking in other text-to-code techniques.",
            "1": "* Modular design: The overall pipeline is modular with clear interfaces between the stages.",
            "2": "Limited evaluation: The framework was only evaluated on a small set of unseen Kaggle competitions.",
            "3": "More comprehensive evaluation on diverse ML tasks could strengthen the results.",
            "4": "No comparison to other text-to-code methods: The paper does not provide any quantitative comparison to other natural language to code generation techniques like PaLM-Coder, Code Llama, StarCoder etc.",
            "5": "This could have demonstrated superiority.",
            "6": "Lack of ablation studies: Ablation studies could have isolated the contributions of different components like SmartGPT, inference with Llama, etc.",
            "7": "This would give more insights.",
            "8": "No analysis of generated code quality: Beyond correctness, metrics analyzing code quality like modularity, comments, naming conventions could have provided more insights.",
            "9": "No discussion of limitations: The conclusion does not discuss any limitations of the current approach or challenges that need to be addressed in future work.",
            "10": "Discussing limitations would have provided a balanced perspective.",
            "11": "Overall, while the paper introduces a novel framework, more rigorous evaluation and comparisons to other techniques could have strengthened the results.",
            "12": "Providing more analyses of the generated code quality and limitations of the approach could have added valuable insights as well.",
            "13": "Expanding the evaluation to more diverse tasks remains an area of future work."
        },
        "tq6nxcRybK": {
            "0": "The paper presents a novel framework that addresses the challenge of transforming natural language machine learning task descriptions into executable code, leveraging large language models.",
            "1": "This approach marks a significant contribution to the field of code generation and machine learning.",
            "2": "The paper demonstrates a well-designed framework, incorporating fine-tuning of the Llama model and instruction-based sequential generation.",
            "3": "The experimental evaluations showcase the effectiveness of the approach, with promising results in the machine learning domain.",
            "4": "The paper is well-structured and easy to understand, providing a clear explanation of the proposed framework, its components, and the underlying methodology.",
            "5": "The use of figures and tables effectively illustrates the approach and the results.",
            "6": "The proposed framework has the potential to advance machine learning applications across diverse domains by automating code generation and bridging the gap between task descriptions and executable code.",
            "7": "This work holds promise for improving the efficiency and accessibility of machine learning development.",
            "8": "One potential weakness of the paper is the occasional suboptimal performance of SmartGPT compared to unprocessed Llama instructions.",
            "9": "This inconsistency indicates the need for further refinement and evaluation of the algorithmic approach employed by SmartGPT to ensure consistently high-quality solutions.",
            "10": "Another limitation might be the generalizability of the framework.",
            "11": "While the paper demonstrates promising results in the machine learning domain, it would be valuable to explore its applicability and effectiveness across a wider range of tasks and domains.",
            "12": "Lastly, the paper could benefit from a more comprehensive comparison with existing approaches in code generation and text-to-code conversion.",
            "13": "Providing a deeper analysis of the framework's strengths and weaknesses in relation to other methods would help establish its position in the field and identify areas for further improvement."
        }
    },
    "tmBKIecDE9": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method, Motif, which leverages Large Language Models (LLMs) to generate intrinsic rewards for reinforcement learning (RL) agents.",
            "1": "This approach is innovative as it bridges the gap between high-level knowledge encoded in LLMs and low-level decision-making required in RL.",
            "2": "**Performance**: The results are impressive, particularly in the NetHack Learning Environment.",
            "3": "Motif not only outperforms traditional RL methods but also achieves higher scores using intrinsic rewards alone compared to agents trained directly on extrinsic rewards.",
            "4": "This is a significant achievement in the field of RL.",
            "5": "**Human-Aligned Behaviors**: The paper demonstrates that Motif generates behaviors that are more aligned with human intuition.",
            "6": "This is a crucial aspect for developing AI systems that can operate in complex, real-world environments.",
            "7": "**Scalability**: The method shows promising scalability with the size of the LLM.",
            "8": "Larger models lead to better performance, indicating that Motif can benefit from future advancements in LLMs.",
            "9": "**Steerability**: The ability to steer the agent's behavior through prompt modifications is a valuable feature.",
            "10": "This allows for the generation of diverse policies and can be used to tailor the agent's behavior to specific tasks or preferences.",
            "11": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of Motif across various tasks in the NetHack environment.",
            "12": "It includes both quantitative performance metrics and qualitative behavior analysis, offering a well-rounded assessment of the method's capabilities.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Dependency on LLMs**: The method heavily relies on the capabilities of LLMs.",
            "15": "While this is a strength in terms of leveraging existing knowledge, it also means that the performance of Motif is tied to the quality and size of the LLM.",
            "16": "This could be a limitation in scenarios where access to large, high-quality LLMs is restricted.",
            "17": "**Prompt Sensitivity**: The paper highlights that the performance and behavior of the agent can be highly sensitive to the wording of the prompt.",
            "18": "This sensitivity could pose challenges in ensuring consistent and reliable performance across different tasks and environments.",
            "19": "**Misalignment by Composition**: The phenomenon of misalignment by composition, where the combination of intrinsic and extrinsic rewards leads to unexpected behaviors, is a notable issue.",
            "20": "While the paper identifies this problem, it does not provide a concrete solution, leaving room for further research to address this challenge.",
            "21": "**Generalization to Other Environments**: While the results in the NetHack environment are impressive, it remains to be seen how well Motif generalizes to other environments, especially those that do not have a built-in captioning mechanism.",
            "22": "The paper suggests that the method can be generalized, but empirical evidence in diverse environments would strengthen this claim.",
            "23": "**Complexity of Implementation**: The method involves multiple phases, including dataset annotation, reward training, and RL training.",
            "24": "This multi-step process could be complex to implement and may require significant computational resources, particularly for training large LLMs and RL agents.",
            "25": "**Ethical Considerations**: The paper briefly touches on the alignment properties of Motif but does not delve deeply into the ethical implications of using LLMs to generate intrinsic rewards.",
            "26": "As LLMs can sometimes produce biased or inappropriate outputs, it is important to consider the potential ethical risks and develop safeguards to mitigate them.",
            "27": "#### Conclusion\n\nOverall, the paper presents a compelling and innovative approach to leveraging LLMs for intrinsic motivation in RL.",
            "28": "The strengths of the method, particularly its performance and human-aligned behaviors, are significant contributions to the field.",
            "29": "However, there are some limitations and challenges, such as dependency on LLMs, prompt sensitivity, and the complexity of implementation, that need to be addressed in future work.",
            "30": "The paper provides a solid foundation for further research and development in this area, and its findings have the potential to influence the design of more intelligent and adaptable RL agents."
        },
        "ybgpiwvG1G": {
            "0": "### quality and clarity\n- This paper is well-written and easy to follow.",
            "1": "### significance\n- The empirical results are strong.",
            "2": "It would be notable to solve the difficult, sparse-reward NetHack environments that previous intrinsic-motivation methods cannot solve by leveraging preference-based reward.",
            "3": "- I think the point of this paper is that \"joint optimization of preference-based and extrinsic reward helps resolve the sparse reward problems\".",
            "4": "As the source of feedback, either humans or LLMs are OK.",
            "5": "I think describing this as LLM's contribution might be an overstatement.",
            "6": "- As a preference-based RL method, I guess there are no differences from the original paper [1].",
            "7": "In the LLM literature, [2] leverages GPT-4 to solve game environments, and [3] incorporates LLM-based rewards for RL pretraining.",
            "8": "- Terminology: I'm not sure if a preference-based reward should be treated as an \"intrinsic\" reward.",
            "9": "I think it is extrinsic knowledge (from humans or LLM).",
            "10": "[1] https://arxiv.org/abs/1706.03741\n\n[2] https://arxiv.org/abs/2305.16291\n\n[3] https://arxiv.org/abs/2302.06692"
        },
        "QAyb3tGtSA": {
            "0": "- S1.",
            "1": "First of all, this paper is well-written and well-organized.",
            "2": "- S2.",
            "3": "The idea of using a LLM as a preference annotator for preference-based RL is interesting and promising.",
            "4": "- S3.",
            "5": "This paper provides a loss function (equation 1) to train an intrinsic reward model.",
            "6": "- S4.",
            "7": "This paper shows that training agents with intrinsic rewards is very effective.",
            "8": "- W1.",
            "9": "One of my main questions is whether Motif can be generally applied to other environments.",
            "10": "Even though the NetHack Learning Environment (NLE) is a very challenging environment, it seems that the NLE may be one of environments that a LLM can easily annotate preferences."
        },
        "YmGIe1M2XC": {
            "0": "The paper is clear and well presented.",
            "1": "The idea of using intrinsic rewards generated from an LLM's preferences is both innovative and practically useful, potentially paving the way for more human-aligned agents.",
            "2": "The method scales well with the size of the LLM and is sensitive to prompt modifications, offering flexibility and adaptability.",
            "3": "The paper provides a comprehensive analysis, covering not just the quantitative but also the qualitative behaviors of the agents.",
            "4": "The paper could benefit from a more extensive comparison to other methods, especially those that also attempt to integrate LLMs into decision-making agents.",
            "5": "There is a lack of discussion on the computational cost and efficiency aspects of implementing Motif.",
            "6": "While the paper makes a strong case for Motif, it doesn't delve deeply into the limitations or potential drawbacks of relying on LLMs for intrinsic reward generation."
        },
        "VZz21uJuD5": {
            "0": "- Figure 1 provides a nice clean bird's eye view of the overall approach and helps with readability.",
            "1": "- The evaluation of the agent for not just the game score but also other dimensions provides a helpful qualitative assessment of the proposed approach and baselines through the spider graph in Figure 4.",
            "2": "- The ablation experiments for the approach are quite exhaustive, covering scaling laws, prior v/s zero knowledge, rewordings of the prompts, etc.",
            "3": "- Using a 70-billion LLM to generate a preference dataset from given captions is quite expensive; while I understand this is out of the scope of the paper, perhaps using a large VLM to annotate frames without captions might have been more economical?",
            "4": "- Given that one of the key contributions of the paper is the intrinsic reward function that is learnt from preferences extracted from the LLM, it might be worthwhile having a baseline that gives preferences using a simpler model (say sentiment analysis) and learn the RL policy using this intrinsic reward model."
        }
    },
    "LAEd3kHao9": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method, PLID, which leverages large language models (LLMs) to generate diverse and informative text descriptions for compositional classes.",
            "1": "This approach is innovative and addresses the limitations of existing methods that lack diversity and informativeness in prompts.",
            "2": "**Comprehensive Evaluation**: The authors conduct extensive experiments on three benchmark datasets (MIT-States, UT-Zappos, and C-GQA) and demonstrate that PLID outperforms existing state-of-the-art methods in both closed-world and open-world settings.",
            "3": "The results are robust and show significant improvements in key metrics such as harmonic mean (H) and area under the curve (AUC).",
            "4": "**Detailed Analysis**: The paper provides a thorough analysis of the proposed method, including ablation studies and the impact of various hyperparameters.",
            "5": "This detailed analysis helps in understanding the contributions of different components of the PLID model and their effects on performance.",
            "6": "**Visual and Language Decomposition**: The introduction of the Visual-Language Primitive Decomposition (VLPD) module and the Stochastic Logit Mixup (SLM) strategy is a significant contribution.",
            "7": "These components enhance the model's ability to decompose visual features into simple primitives and fuse decisions from compositional and primitive logit spaces, leading to better generalization.",
            "8": "**Qualitative Analysis**: The paper includes qualitative results and visualizations that provide insights into the model's behavior.",
            "9": "The tSNE visualizations of text embeddings and learned DSPs effectively illustrate the compactness and separability of class embeddings achieved by PLID.",
            "10": "**Parameter Efficiency**: The proposed method is parameter-efficient, as it only requires learning a single soft prompt, unlike other methods that need to optimize a large collection of prompts.",
            "11": "This efficiency is particularly important for the CZSL task, which involves a large number of compositional classes.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Complexity and Clarity**: The paper is dense and complex, with many components and technical details.",
            "14": "While the comprehensive analysis is a strength, it can also make the paper challenging to follow for readers who are not familiar with the domain.",
            "15": "Simplifying the presentation or providing a more concise summary of key points could improve readability.",
            "16": "**Limited Comparison with Non-CLIP Methods**: The paper primarily compares PLID with other CLIP-based methods.",
            "17": "While this is relevant, it would be beneficial to include comparisons with non-CLIP-based CZSL methods to provide a broader perspective on the performance improvements achieved by PLID.",
            "18": "**Generalization to Other Tasks**: The paper focuses on the CZSL task, but it would be interesting to see how the proposed method generalizes to other zero-shot learning tasks or different domains.",
            "19": "Including experiments or discussions on the applicability of PLID to other tasks could strengthen the paper.",
            "20": "**Impact of LLM Choice**: The paper shows the impact of using different LLMs (T5 and OPT), but the analysis is somewhat limited.",
            "21": "A more detailed discussion on why certain LLMs perform better and how the choice of LLM affects the overall performance would be valuable.",
            "22": "**Failure Cases**: While the paper includes some failure cases, a more in-depth analysis of these cases could provide insights into the limitations of the proposed method.",
            "23": "Understanding why the model fails in certain scenarios could help in identifying areas for further improvement.",
            "24": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of compositional zero-shot learning by introducing the PLID method.",
            "25": "The innovative use of LLMs to generate diverse and informative text descriptions, combined with the VLPD and SLM strategies, leads to substantial performance improvements.",
            "26": "Despite some complexity in presentation and limited comparisons with non-CLIP methods, the paper's strengths in innovation, comprehensive evaluation, and detailed analysis make it a valuable contribution to the field."
        },
        "A3oky3xA9M": {
            "0": "1.The paper is organized and clearly written.",
            "1": "2.The proposed method seems to be intuitively reasonable.",
            "2": "1.The proposed method relies much on the quality of LLMs, and the transferability of the model is not reflected in the paper.",
            "3": "2.According to the Ablation study, the experiment w/o VLPD does not change much (even the H_cw value decreases).",
            "4": "3.The proposed languageinformed distributions (LID) can effectively avoid the issue of intra-class variety.",
            "5": "However, the authors would better also intepret how to solve the issue of inter-class correlation."
        },
        "Gfs4cWMJm4": {
            "0": "The motivations of suggested primitive decomposition and stochastic logit mixup are okay, which have positive effects on zero-shot compositional visual recognition.",
            "1": "The paper is well presented and the suggested method slightly exceeds the comparison method.",
            "2": "The authors did not mention whether the comparison methods also used N-view augmentation.",
            "3": "If the answer is no, I think the comparison is unfair.",
            "4": "Please indicate which method uses the same augmentation.",
            "5": "The author mentioned in the ablation study that LID significantly improves performance compared to the baseline is confusing.",
            "6": "According to Table 2, TFE and VFE seem to have little effect, while LID appears to be effective, and its effectiveness should come from LLM, which in this paper belongs to incremental engineering.",
            "7": "Figure 2 should be consistent with its caption.",
            "8": "For example, Figure 2 should provide the corresponding parts of LID, which should be more helpful for understanding."
        },
        "6jZPRIVAsx": {
            "0": "Despite the method having several building blocks, the idea and motivation behind each introduced component are well-described in the text: e.g.",
            "1": "LLM is used to create a pool of sentences describing the compositions, which in turn is used to estimate a distribution that can be in turn used to estimate pairwise-margins between compositions.",
            "2": "Fig.",
            "3": "1 also helps the reader understand the starting idea for the model (i.e.",
            "4": "composition ambiguities), and why a distribution over the language space may help deal with such uncertainty.",
            "5": "The experiments show that the proposed approach (PLID) surpasses by a margin all competitors in all settings, especially in the closed-world scenario where unseen compositions are known (Table 1).",
            "6": "In this latter setting, the gap in AUC is remarkable, with 1.5 points improvement on MIT-states, 2.7 on UT-Zappos, and 0.5 on C-GQA.",
            "7": "Sections 2, 3, and 4 give credit to the approaches the method builds on, presenting a detailed overview of the literature and the proposed contributions.",
            "8": "I have two main concerns regarding the experimental analysis and the presentation.",
            "9": "For the former, the proposed method contains several components (i.e.",
            "10": "LID, TFE, VFE, VLPD, SLM).",
            "11": "While all components have motivations justifying their use, each of them has specific design choices whose impact is not fully clear from Section 5.2.",
            "12": "Examples are:\n1.",
            "13": "The gap between considering and not considering distributional-based margins in Eq.",
            "14": "(1) and Eq.",
            "15": "(4) is mild accordingly is mild according to Table 4 (i.e.",
            "16": "gap lower than 0.5 points but for harmonic mean OW).",
            "17": "Fig.",
            "18": "5.a shows that indeed going from 4 to 64 LLM-based sentences improves the overall results but less than 1 point and less in the more challenging OW setting.",
            "19": "Given that querying LLM is costly in this setting (i.e.",
            "20": "the number C of compositions might be in the order of thousands) and can be noisy (as per Appendix A), it is questionable whether LLMs and distributional semantics are crucial for the approach (as suggested by the title).",
            "21": "The ablations are also conducted on the MIT-states dataset which is known to be noisy (Atzmon et al.",
            "22": "2020), thus it should be verified if the findings hold across datasets.",
            "23": "Related to the previous points, TFE and VFE are modules that refine visual/textual embeddings.",
            "24": "The article does not contain ablations on their number of parameters/design choices and the improvement from the added views is mild (i.e.",
            "25": "less than 0.5 points on Fig.",
            "26": "5b).",
            "27": "Ablating variants of these modules, potentially taking out the set of text embeddings/views and focusing on their parameters (e.g.",
            "28": "even via MLPs, etc.)",
            "29": "would strengthen the need for their implementation as cross-modal blocks and also of their specific input choices.",
            "30": "This applies also to the specific implementations of the prediction modules $f_s$ and $f_o$ in Eq.",
            "31": "(2).",
            "32": "The SLM module should provide flexibility to the model regarding which predictions to trust.",
            "33": "However, SLM is not compared to other aggregation strategies (e.g.",
            "34": "average, max, product) and on the single scoring mechanism (compositional vs primitive-based).",
            "35": "Thus, it is hard to assess the need for this module.",
            "36": "Regarding the presentation: from the abstract and introduction it is unclear what is the role of LLMs/how they are used.",
            "37": "It is implied that a language-informed class distribution is used, but not how this is achieved (or kept this information generic in Section 1).",
            "38": "This is not a major weakness per-se, but given that the title focuses on this distribution, it would be helpful to give hints on how this distribution is estimated already at the beginning of the manuscript, clarifying the methodological idea to the reader.",
            "39": "This is a purpose that Fig.",
            "40": "1 serves well, but the text does not stress.",
            "41": "The notation of Section 3 is not straightforward to follow.",
            "42": "The main reason is not the lack of explanations (each term is properly defined) but the number of terms defined that the reader should remember to fully appreciate the method.",
            "43": "While I understand that it is not easy to make the notation simpler given the presence of multiple components, in some cases, the notation could be simplified.",
            "44": "For instance, the name of the text embeddings is detached from their inputs (e.g.",
            "45": "S becomes D, [p:..] becomes q, x and X become v, etc.).",
            "46": "The end of the VLPD part introduces $\\mathbf{h}$ elements and $\\mathbf{H}$, that could be replaced by simply stating that $h^{rc}_y = h_s + h_o$ (even directly on Eq.",
            "47": "(4)).",
            "48": "These are (arguable and probably subjective) examples on how some of the elements could be not defined and/or the notation simplified.",
            "49": "Minors:\n- The qualitative results in Fig.",
            "50": "8 do not provide specific insights on the model as the predictions are only compared with the ground truth.",
            "51": "As in Fig.",
            "52": "7, it would have been more helpful to investigate how predictions are affected by different design choices.",
            "53": "- Fig.",
            "54": "2, caption, \"VEF\" vs \"VFE\"."
        },
        "YvJGnLKC2U": {
            "0": "The overall organization is reasonable, and the writing is good.",
            "1": "The class-wise distribution modeling and its afterward alignment with the image modal is novel.",
            "2": "Sufficient experiments and ablation studies are performed.",
            "3": "The motivation for modeling class distribution is not new, i.e., it has been proposed and used in the vision-language model.",
            "4": "Also, the augmentation to image input seems like a test-time adaptation strategy that is also explored in the community.",
            "5": "These aspects degrade the contribution to the community.",
            "6": "The framework incrementally follows ProDA by introducing D^{(y)} and introduces VLPD by compositing v which is also widely used in CZSL.",
            "7": "The framework seems like a combination of many existing techniques.",
            "8": "This degrades its novelty.",
            "9": "Why A in Eq.",
            "10": "1 is defined like that?",
            "11": "Is the dimension shape of A correct by defining it as A_k,y?",
            "12": "What’s the shape of A_k,y?",
            "13": "Some parameter analysis is shown, however, the results w.r.t.",
            "14": "the value of N = 0 is not shown.",
            "15": "Also, how to balance the tradeoff between different losses.",
            "16": "The final training loss is not shown."
        }
    },
    "LY3ukUANko": {
        "llm_review": {
            "0": "**Comprehensive Analysis**: The paper provides a thorough analysis of the performance gap between attention-based and gated-convolution language models.",
            "1": "The authors pretrain and evaluate 17 different models, which adds robustness to their findings.",
            "2": "**Identification of Key Issue**: The identification of associative recall (AR) as a primary factor contributing to the performance gap is insightful.",
            "3": "The authors' fine-grained analysis shows that AR errors account for 82% of the perplexity gap, which is a significant finding.",
            "4": "**Introduction of MQAR**: The introduction of the multi-query associative recall (MQAR) task is a valuable contribution.",
            "5": "It better reflects the complexities of real language modeling compared to previous synthetic tasks.",
            "6": "**Theoretical and Empirical Validation**: The paper combines theoretical analysis with empirical validation.",
            "7": "The theoretical results, such as the equivalency between BASECONV and arithmetic circuits, provide a solid foundation for the empirical findings.",
            "8": "**Practical Solutions**: The authors propose practical solutions to close the AR gap, such as hybrid models that combine BASECONV with sparse attention.",
            "9": "These solutions are shown to be effective in reducing the perplexity gap while maintaining efficiency.",
            "10": "**Open Source Code**: The availability of the code on GitHub enhances the reproducibility of the research and allows other researchers to build upon the work.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Complexity of Theoretical Analysis**: While the theoretical analysis is rigorous, it may be challenging for readers who are not well-versed in computational complexity and arithmetic circuits.",
            "13": "The paper could benefit from more intuitive explanations or visual aids to make these concepts more accessible.",
            "14": "**Limited Scope of Evaluation**: The evaluation is primarily focused on the Pile dataset.",
            "15": "While this is a comprehensive dataset, additional evaluations on other datasets or tasks could strengthen the generalizability of the findings.",
            "16": "**Sparse Attention Implementation**: The implementation details of the sparse attention mechanisms, especially the learned selection function, are somewhat brief.",
            "17": "More detailed explanations or pseudocode could help readers better understand and replicate these methods.",
            "18": "**Scalability Concerns**: The proposed solutions, such as hybrid models with sparse attention, may still face scalability issues for extremely long sequences.",
            "19": "The paper could discuss potential limitations and future directions for handling such cases.",
            "20": "**Comparison with Other Efficient Models**: The paper focuses on gated-convolution models and attention-based models.",
            "21": "A comparison with other efficient architectures, such as recurrent neural networks (RNNs) or memory-augmented networks, could provide a more comprehensive view of the landscape.",
            "22": "**Real-World Applicability**: While the paper demonstrates improvements in perplexity, it would be beneficial to see more discussion on the real-world applicability of these models.",
            "23": "For instance, how do these improvements translate to downstream tasks like machine translation or text summarization?",
            "24": "#### Conclusion\n\nOverall, the paper \"ZOOLOGY: Measuring and Improving Recall in Efficient Language Models\" makes significant contributions to the understanding and improvement of efficient language models.",
            "25": "The identification of associative recall as a key factor and the introduction of MQAR are particularly noteworthy.",
            "26": "While there are some areas for improvement, such as the complexity of the theoretical analysis and the scope of evaluation, the paper provides valuable insights and practical solutions that can guide future research in this area."
        },
        "tffThhYAox": {
            "0": "The studied problem is important and may have a big impact.",
            "1": "The pinpointed failure model (i.e., associative recall) is novel and reasonable.",
            "2": "Both empirical and theoretical studies are conducted to support the argument.",
            "3": "To further demonstrate the impact of the analyses, the authors examine two alternative strategies, which support the intuition of the author.",
            "4": "The proposed attention hybrid method seems to perform well in the experiment.",
            "5": "However, it is not clear how it would perform on a larger scale."
        },
        "luL4VyJwP5": {
            "0": "Using the bigram frequency and the test perplexity of real data was insightful.",
            "1": "There's a lot of typos and confusing writing.",
            "2": "It's hard to properly understand all the theoretical claims of the work.",
            "3": "For starters, Proposition 4.3 should say `u \\in \\{ 0, 1 \\}^{N \\times \\log c}`.",
            "4": "The description of definition 3.1 could be improved; a simple example would be quite helpful.",
            "5": "In appendix, `N` sometimes means the entire sequence length or the number of triplets in MQAR; this confusion is exacerbated by the fact that the meaning changes in the same theorem/proof.",
            "6": "Page 6 states \"[e]ach BaseConv layer uses O(Nd) parameters and ... O(Nd) operations\".",
            "7": "I believe it uses `O(Nd +d^2)` parameters and `O(d N log N + Nd^2)` FLOPs?",
            "8": "This makes me believe that the rest of the theoretical results may have to be carefully revisited by the authors.",
            "9": "Page 26.",
            "10": "Proof of C.19.",
            "11": "It's unclear how `Q[i, :]` can be set to a zero vector when `i \\notin Q` (also boldface 0 suffices to express a vector; no need for a superscript d; also, d is defined as `log(C)` in the same page but `C` is a set.",
            "12": "`d = log(c)` small c since `c := |C|`.)",
            "13": "and similarly for `K` and `V`, because QKV is actually a linear projection of the input.",
            "14": "Linear projection can not implement this non-linear operation of masking some of the activations out.",
            "15": "There's a sentence that reads `... where Q, K, V ... are positional embeddings ...`.",
            "16": "They are different projections of u.",
            "17": "The calculation of the percentage of gap due to AR hits could be better motivated and justified in appendix.",
            "18": "Other minor typos:\n - `u * k = FFT^{-1} (FFT(u) FFT(k))` (should drop the convolution operation in frequency domain).",
            "19": "- Page 5: Other tokens: ... `1,000` => `1250`."
        },
        "LL4kNOsUMV": {
            "0": "The paper provides a comprehensive study of the associative recall capability of neural language models with different neural architectures, and examines its impact on the next token prediction performance of the models on real world data.",
            "1": "The authors empirically demonstrate that boosting the associative recall capability of GCMs can mostly bridge its performance gap with the attention-based model under the scale of 360M parameters.",
            "2": "The authors derive a theoretical scaling bound for data-independent GCMs to solve AR, and validate it with synthetic data.",
            "3": "The novelty of the paper is limited.",
            "4": "The lack of AR ability of State Space Models (which is a special kind of long convolution model with embedded recurrency) has been analyzed in the H3 paper [1] through synthetic data.",
            "5": "The proposed architectural modification of hybridization is a simple replication of the Hybrid-H3.",
            "6": "The scale of the experiments is limited.",
            "7": "The authors only empirically examine their hypothesis for models with the size up to 360M number of parameters.",
            "8": "It is not clear whether their claims still holds empirically given the shrinking trend of the performance gap that can already be observed under the current setting.",
            "9": "The paper does not provide important technical details for reproducibility.",
            "10": "The implementation details of the proposed modification of selectively look-up is missing.",
            "11": "The research problem that the authors are trying to solve has been alleviated with existing [1,2] or emerging solutions [3,4].",
            "12": "The authors do not examine the empirical AR ability of data-dependant convolutions by claiming technical difficulties, but there does exist data-dependant SSMs, such as Liquid-S4 [2], that support causal language modeling.",
            "13": "Not to mention the latest GCM, Monarch Mixer [3], that also supports causality.",
            "14": "On hybridization, a previous work [4] on dynamic input subset selection for attention modules has also been proposed for efficiently combining SSMs with attention.",
            "15": "The authors should consider comparing the proposed architectural modifications with these works to avoid being outdated upon publication.",
            "16": "---\n[1] Hungry Hungry Hippos: Towards Language Modeling with State Space Models (ICLR 2023)\n\n[2] Liquid Structural State-Space Models (ICLR 2023)\n\n[3] Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture (NeurIPS 2023)\n\n[4] Sparse Modular Activation for Efficient Sequence Modeling (NeurIPS 2023)"
        }
    },
    "hESD2NJFg8": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel pipeline, LLM-GNN, which leverages the strengths of both Graph Neural Networks (GNNs) and Large Language Models (LLMs) for label-free node classification.",
            "1": "This hybrid approach is innovative and addresses the limitations of both GNNs and LLMs effectively.",
            "2": "**Comprehensive Evaluation**: The authors conduct extensive experiments on multiple datasets, including CORA, CITESEER, PUBMED, OGBN-ARXIV, OGBN-PRODUCTS, and WIKICS.",
            "3": "This thorough evaluation demonstrates the robustness and generalizability of the proposed method.",
            "4": "**Cost-Effectiveness**: The proposed method achieves high accuracy with minimal cost.",
            "5": "For instance, LLM-GNN achieves 74.9% accuracy on the OGBN-PRODUCTS dataset with a cost of less than 1 dollar, which is significantly lower than other methods like LLMs-as-Predictors.",
            "6": "**Active Node Selection and Post-Filtering**: The introduction of difficulty-aware active node selection and confidence-aware post-filtering is a significant contribution.",
            "7": "These techniques ensure that the selected nodes for annotation are both informative and easy to annotate, thereby improving the overall performance of the GNN.",
            "8": "**Flexibility**: The LLM-GNN pipeline is designed to be flexible, allowing for different designs in each component.",
            "9": "This adaptability makes it suitable for various real-world scenarios and datasets.",
            "10": "**Detailed Analysis**: The paper provides a detailed analysis of the impact of different active selection strategies, loss functions, and budgets on the performance of the pipeline.",
            "11": "This thorough analysis helps in understanding the strengths and limitations of the proposed method.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Complexity**: The proposed pipeline involves multiple steps, including active node selection, LLM annotation, post-filtering, and GNN training.",
            "14": "This complexity might make it challenging to implement and tune in practice, especially for practitioners who are not experts in both GNNs and LLMs.",
            "15": "**Scalability Concerns**: While the method is shown to be cost-effective, the scalability of the approach, especially for very large graphs, might be a concern.",
            "16": "The paper does not provide a detailed analysis of the time complexity and computational resources required for very large-scale datasets.",
            "17": "**Dependency on LLMs**: The reliance on LLMs for generating annotations might limit the applicability of the method in scenarios where access to powerful LLMs is restricted due to cost or computational constraints.",
            "18": "**Limited Comparison with State-of-the-Art**: Although the paper compares LLM-GNN with several label-free node classification methods, it would be beneficial to include more state-of-the-art methods in the comparison to provide a more comprehensive evaluation of the proposed method's performance.",
            "19": "**Annotation Quality**: While the paper discusses the quality of annotations generated by LLMs, it would be helpful to provide more insights into the types of errors made by LLMs and how these errors impact the overall performance of the GNN.",
            "20": "**Generalization to Other Types of Graphs**: The paper primarily focuses on text-attributed graphs.",
            "21": "It would be interesting to see how the proposed method generalizes to other types of graphs, such as those with numerical or categorical attributes.",
            "22": "#### Conclusion:\n\nOverall, the paper presents a novel and effective approach for label-free node classification by combining the strengths of GNNs and LLMs.",
            "23": "The proposed LLM-GNN pipeline is innovative, cost-effective, and demonstrates strong performance across multiple datasets.",
            "24": "However, the complexity of the method, scalability concerns, and dependency on LLMs are potential limitations that need to be addressed in future work.",
            "25": "The paper provides a solid foundation for further research in this area and opens up new possibilities for leveraging LLMs in graph-based tasks."
        },
        "c2ESuNg2pC": {
            "0": "LLM-GNN presents an innovative approach to node classification on graphs by harnessing the complementary strengths of GNNs and LLMs.",
            "1": "It acknowledges the challenges of obtaining high-quality labels and proposes a label-free solution, which is a significant contribution to the field of machine learning.",
            "2": "The paper demonstrates the cost-effectiveness of LLM-GNN by achieving high accuracy on a large dataset with annotation costs under 1 dollar.",
            "3": "This cost-efficient approach is particularly relevant for real-world applications with resource constraints.",
            "4": "LLM-GNN offers a comprehensive methodology that not only utilizes LLMs for annotations but also considers active node selection, confidence-aware annotations, and post-filtering.",
            "5": "This approach ensures the quality, representativeness, and diversity of annotations, addressing key challenges in label-free node classification.",
            "6": "Since LLMs generate annotations without access to ground truth labels, there is a risk of noisy annotations.",
            "7": "It would be better to investigate the robustness of LLM-GNN to noisy annotations and potential strategies for mitigating their effects.",
            "8": "LLM-GNN's performance is demonstrated on a specific dataset (OGBN-PRODUCTS), and while it achieves impressive results, its generalizability to other datasets or domains is not thoroughly explored in the paper.",
            "9": "The effectiveness of the approach in different scenarios and with various types of graphs should be investigated to assess its broader applicability.",
            "10": "There could be better with a detired comparison on the economic perspective."
        },
        "7SQadl2a8U": {
            "0": "The proposed method is among the first trials of combining LLMs with GNNs to solve a novel problem, i.e., label-free node classification.",
            "1": "The proposed method is clearly described and the paper is easy to follow in general.",
            "2": "The authors compare with various heuristic baselines and conduct analyses to demonstrate the efficacy of the proposed method.",
            "3": "Though I acknowledge that the proposed method is a valid solution, the technical contribution of the paper is somewhat limited, especially considering that the three major components are largely based on heuristic observations, and the rest are based on existing LLMs and GNNs.",
            "4": "It would make the paper stronger if some theoretical analyses could be provided for the proposed components.",
            "5": "The authors should more explicitly mention that their proposed method only works for text-attributed graphs rather than any general graph, e.g., in the abstract and introduction.",
            "6": "Otherwise, the paper may have overclaiming issues.",
            "7": "In generating the initial node labels using LLMs, it seems that only the feature information is utilized and no structure is considered.",
            "8": "Since it is well-known in the graph machine learning literature that both features and structures greatly affect the node labels, there exists a large room for improvement.",
            "9": "There are some missing related works regarding zero-shot node classification such as [1-2], which should be added.",
            "10": "I also wonder how different LLMs affect the model (the reported results are all based on GPT-3.5-turbo).",
            "11": "[1] Zero-shot Node Classification with Decomposed Graph Prototype Network, KDD’21  \n[2] Dual Bidirectional Graph Convolutional Networks for Zero-shot Node Classification, KDD’22"
        },
        "qpFCDvj1cs": {
            "0": "- The idea of label-free annotation using LLMs on text attributed graphs is an interesting research direction introduced by the paper.",
            "1": "- The paper has experimented with different datasets and incorporated various existing techniques to come up with a cost-effective model.",
            "2": "- This paper appropriately balanced traditional graph active selection criteria with annotation quality by incorporating difficulty-aware active selection with post filtering to obtain training nodes from LLM.",
            "3": "- Difficulty aware (DA) selection: According to the paper, LLMs annotation quality degrades when they have to annotate nodes which are away from the centers.",
            "4": "It implies that the LLMs annotation quality would suffer in case of the diverse nodes (away from center).",
            "5": "However, GNNs accuracy will only improve if the nodes are diverse.",
            "6": "Hence, difficulty aware selection i.e.",
            "7": "use of c-density might not always help and, in fact, it may hinder in some cases.",
            "8": "This is also evident from the results shown in Table 2 : Active_Selection_Methods and the corresponding  DA-Active_Selection_Methods show similar performance on average across different techniques (i.e., selection methods).",
            "9": "Moreover, for at least 50% of the cases, the DA-method (row 2) performs poorly compared to the corresponding active learning method (row 1).",
            "10": "- Though the accuracy from the LLM-GNN model is good (and of course, the model is efficient), it couldn't outperform LLM as a predictor (Table 3).",
            "11": "- The methods are heuristics and do not have theoretical evidence."
        },
        "6f2Z4aNkfX": {
            "0": "- The proposed method is well-motivated.",
            "1": "- Each component supports motivation reasonably.",
            "2": "- Well written paper, it is easy to follow.",
            "3": "- The performance gain is incremental, especially for Difficulty-aware active node selection (DA).",
            "4": "- Explanations about experiments are not enough, and some parts are unclear.",
            "5": "Please refer to questions for details."
        }
    },
    "YKK1jXEWja": {
        "llm_review": {
            "0": "**Innovative Approach**: The introduction of Self-Asking and Trajectory Ranking is a novel approach to enhance the performance of LLM agents in decision-making tasks.",
            "1": "These methods address the limitations of existing few-shot prompting techniques by incorporating intermediate steps and leveraging the stochastic nature of LLMs.",
            "2": "**Empirical Validation**: The paper provides comprehensive empirical evidence demonstrating the effectiveness of Prospector.",
            "3": "The experiments on ALFWorld and WebShop benchmarks show significant improvements in success rates compared to state-of-the-art methods like ReAct and Reflexion.",
            "4": "**Detailed Analysis**: The paper includes a thorough analysis of the impact of different components of Prospector, such as the number of generated trajectories and the performance of LLM Critics.",
            "5": "This detailed evaluation helps in understanding the contribution of each component to the overall performance.",
            "6": "**Generalizability**: The proposed methods are designed to be generalizable to any LLM and decision-making task, given few-shot demonstrations.",
            "7": "This makes Prospector a versatile approach that can be applied to a wide range of interactive decision-making problems.",
            "8": "**Comparison with Baselines**: The paper provides a clear comparison with existing methods, including traditional IL and RL approaches, as well as recent advancements like ReAct and Reflexion.",
            "9": "This helps in positioning Prospector within the current state of research and highlights its advantages.",
            "10": "**Weaknesses:**\n\n1.",
            "11": "**Complexity and Efficiency**: While Prospector shows improved performance, it introduces additional complexity and computational overhead due to the generation and evaluation of multiple trajectories.",
            "12": "This might limit its applicability in real-time or resource-constrained environments.",
            "13": "**Dependence on Reward Models**: The success of Trajectory Ranking heavily relies on the accuracy of the reward prediction models.",
            "14": "The paper shows that fine-tuning LLM Critics significantly improves performance, but this also introduces additional training costs and complexity.",
            "15": "**Limited Scope of Evaluation**: The experiments are conducted on two specific benchmarks, ALFWorld and WebShop.",
            "16": "While these are representative environments, it would be beneficial to see the performance of Prospector on a broader range of tasks to further validate its generalizability.",
            "17": "**Scalability Concerns**: The paper does not address the scalability of Prospector when applied to tasks with a larger action space or more complex environments.",
            "18": "It would be useful to understand how the approach scales with increasing task complexity and whether any modifications are needed to maintain performance.",
            "19": "**Human Comparison**: The comparison with human performance is limited to the WebShop environment.",
            "20": "Including human performance benchmarks for ALFWorld or other tasks would provide a more comprehensive understanding of how close Prospector is to human-level decision-making.",
            "21": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of LLM agents for decision-making tasks.",
            "22": "The introduction of Self-Asking and Trajectory Ranking addresses key limitations of existing methods and demonstrates substantial improvements in performance.",
            "23": "However, the added complexity and dependence on accurate reward models are important considerations for practical applications.",
            "24": "Future work could focus on optimizing the efficiency of Prospector and expanding its evaluation to a wider range of tasks and environments."
        },
        "RqgyEZ01AU": {
            "0": "- The proposed method is both simple and intuitive, using LLMs for both planning and critiquing of possible trajectories.",
            "1": "The paper is well-written and clear.",
            "2": "- The experiments seem thorough, with comparisons against state-of-the-art methods in the same task domains, and ablations of each component of the proposed Prospector method (removing the trajectory ranking, evaluating the accuracy of the different LLM critics, comparing few-shot and finetuned LLM critics).",
            "3": "In particular, studying the choice of either a fine-tuned or ICL-based critic is interesting and seems novel.",
            "4": "While the method is straightforward and intuitive with impressive experimental results, my main concern is that the two main components of the methods seem to lack novelty in themselves.",
            "5": "This can maybe be clarified with further experimentation: \n\n-  It’s not clear how much of the overall performance improvement is just due to giving the LLM multiple attempts at a single question with the trajectory ranking process.",
            "6": "Further experiments disentangling this would be helpful: for example, if we used the same LLM critics and trajectory ranking process with the ReAct prompt, would it perform on par with Prospector (these experiments seem to be present for ALFWorld but not WebShop)?",
            "7": "Would majority voting at every step, which also allows multiple trajectory attempts but without an explicit LLM critic, be less useful than using the LLM-based critic as in Prospector?",
            "8": "- The AskAct process is does not seem like a novel contribution in and of itself, as it was proposed in Measuring and Narrowing the Compositionality Gap in Language Models (Press et al., 2022).",
            "9": "While the authors note that that Self-Ask work was developed for QA tasks specifically, applying the same general technique of prompting the LLM to ask itself a limited set of questions to reason is a limited contribution.",
            "10": "In particular, it seems like AskAct was not applied to the ALFWorld benchmark for the Prospector agent, and only tested in WebShop as a single fixed question that asks \"which observed object is most proper to select\" (shown in Figure 2 and Table 13 and 16).",
            "11": "Further experiments eliciting different types of self-asked questions across all the tasks would strengthen this contribution."
        },
        "2SqlrxhfRb": {
            "0": "The paper addresses a gap in current LLM-based decision-making methods by integrating feedback from the environment and incorporating stochasticity in trajectory generation.",
            "1": "The proposed method shows empirical success, outperforming existing state-of-the-art methods on standard benchmarks.",
            "2": "Prospector offers an approach that avoids costly fine-tuning, making it more generalizable and efficient.",
            "3": "Both the critic and the generator are LLMs.",
            "4": "This could amplify any existing issues inherent to LLMs.",
            "5": "Limited discussion on the limitations of the reward prediction models used for Trajectory Ranking.",
            "6": "The paper could benefit from a more comprehensive analysis comparing the computational overhead introduced by the Self-Asking and Trajectory Ranking components."
        },
        "qBMxjidHW4": {
            "0": "The paper has a few strong points, such as:\n1.",
            "1": "A comprehensive evaluation across different language models used as critics.",
            "2": "On different parameters of the experiments, a proper experimentation schedule was used, such as few-shot reward prediction accuracy.",
            "3": "The success rate on the evaluated benchmarks show marked improvement over previous work, however, I am not familiar with the benchmarks in the field enough to know if this is sufficient.",
            "4": "The positive impact of the paper is beset by several downsides.",
            "5": "Here are these in the order of importance:\n1.",
            "6": "I am not certain about the magnitude of the impact of the method introduced in this paper.",
            "7": "The method of self-asking itself does not seem significant enough in and of itself without the trajectory ranking, and is quite similar to many different previous methods such as thinking step by step.",
            "8": "Trajectory ranking is definitely the more interesting of the two components, but I am not sure it is a novel and significant enough contribution to merit a place in this venue.",
            "9": "Following up on this, the work is beset by the fact that the new methods are only evaluated in two benchmarks only.",
            "10": "While they perform well on the benchmarks, the question of how easy they will be to scale to a variety of other tasks remain unanswered from the paper itself.",
            "11": "While there is a comprehensive study run on LLM critic and which language model is best for that task, it does not extend to the LLM actor itself.",
            "12": "Rather, only two models of incredibly large sizes are used, which keeps the evaluation quite one-sided.",
            "13": "Overall, this paper shows promise in a few direction, but does not make a noteworthy contribution in any of the directions in my opinion.",
            "14": "However, given my limited experience in such works, I am happy to reconsider my take at the word of the area chair."
        },
        "WkvztltWTx": {
            "0": "The paper is well-written and easy to follow\n2.",
            "1": "Extensive ablations and analysis presented is nicely done -- it shows how the two components of the prospector framework work and improve the performance of the baseline react models.",
            "2": "Limited novelty: While it is good to see how two simple ideas when put together in the prospector framework can lead to good task performance in interactive decision making scenarios, the two ideas themselves are very close to existing work.",
            "3": "Consequently, the novelty seems a bit limited, IMO.",
            "4": "Broader baselines: I liked the authors ablations and comparison with React and its variants given the closeness of the approach (prospector) to react.",
            "5": "These were helpful in understanding how prospector's individual components improve performance.",
            "6": "However, it would have been also useful to see how prospector's performance compares to other llm planning approaches e.g., the ones that combine llms + tree search/classical planning approaches such as https://arxiv.org/pdf/2307.08962.pdf to see how far does prospector push the performance.",
            "7": "Lastly, given that prospector does some training for critics using example trajectories, I am wondering how the performance of prospector would compare to finetuned LLM planner/policy e.g., with LIMA (https://arxiv.org/abs/2305.11206) which can be used to finetune LLM with limited data.",
            "8": "Without these, right now, it is unclear whether prospector should be the goto planning approach for interactive decision making problems or is it really just a better version of react?"
        }
    },
    "qiOqgphnVL": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel methodology for comparing different Diffusion Language Models (DLMs) by focusing on the changes that occur in the samples during the generation process.",
            "1": "This approach is innovative and addresses the challenge of comparing models with unique diffusion frameworks.",
            "2": "**Early Exiting Mechanism**: One of the significant contributions of this paper is the identification and implementation of an adaptive early exit mechanism for DLMs.",
            "3": "This feature can significantly accelerate the text generation process without compromising the quality of the generated text, which is a valuable contribution to the field.",
            "4": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of the proposed Democratized Diffusion Language Model (DDLM) and compares it with other models like SSD and Plaid.",
            "5": "The use of various metrics such as AR-NLL, MAUVE, distinct N-grams, and Zipf’s coefficient ensures a comprehensive assessment of the models' performance.",
            "6": "**Reproducibility and Open Access**: By releasing the DDLM model to the public, the authors promote further research in Diffusion Models for text generation.",
            "7": "This democratization of research tools is commendable and aligns with the open science movement.",
            "8": "**Detailed Analysis**: The paper includes a detailed analysis of the generation process, including the behavior of token switches, entropy, and L2 norms of embeddings.",
            "9": "This in-depth analysis provides valuable insights into the inner workings of DLMs and the potential for early exiting.",
            "10": "**Adaptive Criteria for Early Exiting**: The introduction of three adaptive criteria for early exiting (Entropy, Patience, and KL) and their empirical evaluation is a strong point.",
            "11": "The results show that these criteria can effectively reduce the number of steps required for text generation, enhancing the efficiency of DLMs.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Scope of Models**: The paper focuses on a limited set of DLMs (DDLM, SSD, and Plaid) and omits other models like GENIE or DiffuSeq.",
            "14": "While the authors justify this by the lack of evidence for unconditional text generation in these models, including a broader range of models could have provided a more comprehensive comparison.",
            "15": "**Reproduction of CDCD**: The DDLM model is a reproduction of the CDCD framework, but it is not a precise reproduction due to the lack of available source code.",
            "16": "This limitation could affect the validity of the comparisons made in the paper.",
            "17": "The authors acknowledge this but do not provide a detailed discussion on how these discrepancies might impact the results.",
            "18": "**Unclear Generalizability**: While the paper demonstrates the effectiveness of early exiting for the models studied, it remains unclear whether this behavior is a feature of well-trained models or a sign of underlying issues.",
            "19": "The paper suggests future research to explore this, but a more in-depth discussion on the potential causes and implications of early exiting would have strengthened the paper.",
            "20": "**Evaluation Metrics**: Although the paper uses a variety of metrics to evaluate the models, it primarily focuses on AR-NLL and other quantitative measures.",
            "21": "Including qualitative assessments of the generated text, such as human evaluations, could provide a more holistic view of the models' performance.",
            "22": "**Training Details**: The paper provides some details on the training process and hyperparameters used for DDLM, but more information on the training dynamics, such as learning curves and convergence behavior, would be beneficial.",
            "23": "This additional information could help in understanding the training stability and efficiency of the proposed model.",
            "24": "**Complexity of Analysis**: The detailed analysis of the generation process, while thorough, might be overwhelming for readers who are not deeply familiar with the technical aspects of DLMs.",
            "25": "Simplifying some of the explanations or providing more intuitive summaries could make the paper more accessible.",
            "26": "#### Conclusion\n\nOverall, the paper presents a significant contribution to the field of text generation using Diffusion Language Models.",
            "27": "The innovative approach to comparing DLMs, the introduction of adaptive early exiting mechanisms, and the comprehensive evaluation of the proposed DDLM model are notable strengths.",
            "28": "However, the limited scope of models, the reproduction limitations, and the need for more qualitative assessments are areas that could be improved.",
            "29": "Despite these weaknesses, the paper provides valuable insights and opens up new avenues for research in efficient text generation using DLMs."
        },
        "CWwKPtMqAp": {
            "0": "The paper studies a new field for text generation using diffusion models.",
            "1": "Given the inherent complexities and resource-intensive nature of running diffusion models continuously during generation, the research investigates the feasibility of early exiting by monitoring token switches across various pre-training checkpoints.",
            "2": "The methodology of evaluating Cos between the score function and L2 norm the sample embeddings, and subsequently observing score angle changes, provides a novel insights to assess diffusion models.",
            "3": "The paper focuses on the concept of early stopping in diffusion models, which is an idea that has been previously explored, as noted in \"Accelerating Diffusion Models via Early Stop of the Diffusion Process\" as an example.",
            "4": "The contribution to extend to text generation needs to be assessed.",
            "5": "The technique of early stopping is a recognized practice during the inference stage of diffusion models.",
            "6": "While the current paper's examination of token switches across different pre-training checkpoints offers a fresh angle, the approach's broader implications and significance in comparison to established methodologies could be further elucidated.",
            "7": "From Table 1 main results, we can see the choice of steps also provides very marginal impact to the final performance.",
            "8": "It might be beneficial for the research to delve deeper into how this method stands out from or builds upon existing techniques in the field of diffusion models."
        },
        "Mk69bc3ec4": {
            "0": "This paper reimplemented the CDCD framework.",
            "1": "If the code and checkpoint can be open-sourced, it can provide support for the research of DLMs.",
            "2": "This paper makes sufficient experiments and analysis on the existing DLMs, and obtains the early stopping strategy of DLMs by observing the AR-NLL curve.",
            "3": "The innovation of the paper is insufficient.",
            "4": "The main contribution is to reproduce the CDCD structure and analyze the existing DLMs, without proposing new models or methods.",
            "5": "The length of the trained model is limited to 64, and it is not clear whether there will be different conclusions for longer lengths.",
            "6": "The length of 64 is still a bit far from actual application.",
            "7": "We still care about the performance of pre-trained models on downstream tasks, and the paper did not select some downstream tasks for evaluation.",
            "8": "Writing issues:\n\n    (1) The main contribution of the paper, such as the analysis of DLMs, is not given in the title.",
            "9": "The writing style is a bit messy.",
            "10": "(2) It is recommended to add the model parameter quantity to the comparison in Table 1."
        },
        "Kyv735YEON": {
            "0": "- Re-implementation benifts the community.",
            "1": "- The step-by-step analysis could help us understand the generation process of diffusion models.",
            "2": "- It is good to see analysis of sampling between different diffusion models, however, no further explanation about the deep reason to cause these differences.",
            "3": "- The advantage of DDLM is to early exit and speedup the generation process.",
            "4": "However, compared with some faster ODE solvers (e.g.",
            "5": "DPM-solver[1]), early exit of DDLM maybe not superior than them.",
            "6": "- Early exit leads to the downgrade of generation diversity."
        }
    },
    "ndR8Ytrzhh": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces Early-Stopping Self-Consistency (ESC), a novel method to reduce the computational cost of self-consistency (SC) in multi-step reasoning tasks.",
            "1": "This is a significant contribution as it addresses the high cost associated with SC without compromising performance.",
            "2": "**Comprehensive Evaluation**: The authors conduct extensive experiments across various reasoning tasks (arithmetic, commonsense, and symbolic) and multiple language models (GPT-4, GPT-3.5-Turbo, and Llama-2 7B).",
            "3": "This thorough evaluation demonstrates the robustness and effectiveness of ESC.",
            "4": "**Significant Cost Reduction**: The empirical results show that ESC can reduce the average number of samples by a substantial margin (e.g., -33.8% on MATH, -80.1% on GSM8K) while maintaining comparable performance to SC.",
            "5": "This is a notable achievement, especially for resource-constrained environments.",
            "6": "**Control Scheme**: The paper proposes a control scheme for ESC, allowing dynamic adjustment of the performance-cost balance based on specific task requirements.",
            "7": "This adds flexibility and practicality to the method, making it adaptable to various scenarios.",
            "8": "**Theoretical Analysis**: The authors provide a solid theoretical foundation for ESC, including a detailed analysis of the probability of inconsistent results and the expected sampling cost.",
            "9": "This enhances the credibility and understanding of the proposed method.",
            "10": "**Robustness**: The paper demonstrates the robustness of ESC across different sampling parameters, prompts, and even in zero-shot settings.",
            "11": "This indicates that ESC is a versatile and reliable method.",
            "12": "**Open-Ended Generations**: The extension of ESC to open-ended generation tasks, such as code generation and text summarization, shows the method's applicability beyond fixed-answer problems.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Complexity of Implementation**: While the paper provides a detailed description of ESC, the implementation might be complex for practitioners who are not well-versed in the underlying concepts.",
            "15": "Additional practical guidance or code examples could help in this regard.",
            "16": "**Limited Discussion on Limitations**: The paper primarily focuses on the strengths and positive results of ESC.",
            "17": "A more balanced discussion that includes potential limitations or scenarios where ESC might not perform as well would provide a more comprehensive view.",
            "18": "**Dependency on Initial Window Size**: The control scheme relies on the initial window size (w0) to predict the performance-cost balance.",
            "19": "The choice of w0 might influence the results, and the paper does not provide a detailed analysis of how to optimally select this parameter.",
            "20": "**Generalization to Other Models**: While the paper evaluates ESC on three language models, it would be beneficial to see how the method generalizes to other models, especially those with different architectures or training paradigms.",
            "21": "**Comparison with Other Cost-Reduction Methods**: The paper could benefit from a comparison with other existing methods aimed at reducing the computational cost of reasoning tasks.",
            "22": "This would help position ESC within the broader landscape of cost-reduction techniques.",
            "23": "**Empirical Results Presentation**: The presentation of empirical results, while comprehensive, could be more concise.",
            "24": "Some tables and figures are dense and might benefit from a more streamlined presentation to enhance readability.",
            "25": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in reducing the computational cost of self-consistency in multi-step reasoning tasks through the introduction of Early-Stopping Self-Consistency (ESC).",
            "26": "The method is well-supported by theoretical analysis and extensive empirical evaluation, demonstrating its effectiveness and robustness.",
            "27": "However, the paper could be improved by addressing the complexity of implementation, providing a more balanced discussion of limitations, and comparing ESC with other cost-reduction methods.",
            "28": "Despite these minor weaknesses, the paper makes a valuable contribution to the field and offers a practical solution to a pressing problem."
        },
        "KuKtlTzhnY": {
            "0": "(1) The method is simple and effective.",
            "1": "(2) It is backed by a solid theoretical foundation.",
            "2": "(3) Extensive experiments have been conducted to confirm its effectiveness and reliability.",
            "3": "(1) A related paper with a similar idea, called \"Let’s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs\" (https://arxiv.org/pdf/2305.11860.pdf), was not referenced.",
            "4": "(2) In Table 1, there appear to be inaccuracies in some of the results highlighted in green.",
            "5": "For instance, in the row labeled \"Lˆ-SC (GPT4)\" and the column labeled \"SQA,\" the value \"(-0.27)\" should actually be \"(+0.87)\" because the correct difference is 0.78 (81.42 - 80.55 = 0.78).",
            "6": "Similar issues can be found in the \"SQA\" column.",
            "7": "Additionally, it's puzzling that in the \"SQA\" dataset, Lˆ-SC outperforms SC, even though SC has a larger sample size.",
            "8": "This phenomenon requires further explanation."
        },
        "2E5rjpfmQJ": {
            "0": "Originality: The introduction of ESC offers a fresh perspective in the realm of efficient machine learning algorithms.",
            "1": "Quality: The experimental setup, including testing on six benchmarks, demonstrates the thoroughness of the research.",
            "2": "Clarity: The paper, for the most part, is well-written and concepts are explained clearly.",
            "3": "Comparison with State-of-the-art: It would be helpful to see direct comparisons with current state-of-the-art methods in terms of efficiency and performance.",
            "4": "Generalizability: The paper could discuss potential limitations or scenarios where ESC might not be the optimal solution."
        },
        "A8jimmXNql": {
            "0": "LLMs are a popular topic currently and their execution is costly, either in monetary terms or computationally.",
            "1": "Therefore, it is a good approach to reduce the number of calls necessary, as is proposed in the paper.",
            "2": "It is also a positive thing that existing proven techniques and statistical approaches are re-visited and used in these settings, such as early stopping or using answer entropy as a cut-off criterion.",
            "3": "The experimental evaluation confirms the suitability of the approach over the more exhaustive standard SC technique.",
            "4": "Experiments are extensive and consider many facets of the proposed approach.",
            "5": "The contribution is not particularly strong.",
            "6": "Early stopping or using the confidence respectively the variation in multiple answers in an ensemble of answers is a well known technique.",
            "7": "While we have (maybe, I'm not sure) not seen this in LLM sampling, it is not a particularly strong contribution in the context of an ICLR paper.",
            "8": "I'm also not sure we actually need the notion of the window in the method or if other statistical measurements of the confidence resp.",
            "9": "variability  could be used to determine the cut-off point.",
            "10": "Unfortunately, this has not been discussed."
        },
        "C5gU1B0zgi": {
            "0": "ESC is a simple yet effective adaptation of the original self-consistency to reduce the sampling cost.",
            "1": "The ablation studies and theoretical analysis show that ESC is generally applicable to different benchmarks, and stays effective with different setups.",
            "2": "The novelty of this work is unclear.",
            "3": "[1] already proposed an adaptation of self-consistency to reduce the sampling cost, but this work did not cite and discuss this prior work.",
            "4": "Without a thorough discussion and direct comparison, it is unclear whether ESC is more effective.",
            "5": "In Table 1, when comparing ESC and L-SC, the performance difference is generally small.",
            "6": "The reason can be that the improvement of SC saturates when the sample size increases, thus reducing the sampling size also does not drastically degrade the performance for SC.",
            "7": "It is helpful to show this comparison for smaller sampling sizes, e.g., those in Table 2, and see if the performance improvement achieved by ESC can be more significant.",
            "8": "There are some issues in Table 1.",
            "9": "For example, the SQA results of L-SC are generally much higher than SC, which look problematic.",
            "10": "Also, it is confusing to list L in the table without additional notes, as L represents the sample size, while all other rows represent the task accuracies.",
            "11": "[1] Aggarwal et al., Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs, EMNLP 2023."
        }
    },
    "sY5N0zY5Od": {
        "llm_review": {
            "0": "**Innovative Programming Model**: The introduction of DSPy as a programming model that abstracts LM pipelines into text transformation graphs is a significant advancement.",
            "1": "This model allows for the creation of modular, parameterized, and self-improving pipelines, which can be a game-changer for the ML community.",
            "2": "**Parameterization and Optimization**: The ability to parameterize modules and optimize them through a compiler is a notable strength.",
            "3": "This approach reduces the reliance on hard-coded prompt templates and allows for more systematic and scalable development of LM pipelines.",
            "4": "**Empirical Validation**: The paper provides thorough empirical validation through two case studies: math word problems (GSM8K) and complex question answering (HotPotQA).",
            "5": "The results demonstrate that DSPy can produce pipelines that outperform few-shot prompting and expert-created demonstrations, even with smaller LMs.",
            "6": "**Modularity and Composability**: DSPy’s modular approach, inspired by neural network abstractions, allows for the composition of various modules to build complex pipelines.",
            "7": "This modularity is crucial for exploring and optimizing different pipeline configurations.",
            "8": "**Open Source Availability**: The availability of DSPy as an open-source framework on GitHub is a significant strength.",
            "9": "It allows the broader community to experiment with and build upon the work presented in the paper.",
            "10": "**Reduction of Hand-Crafted Prompts**: The paper successfully demonstrates that DSPy can reduce or even eliminate the need for hand-crafted prompts, which are often brittle and unscalable.",
            "11": "This is a substantial contribution to the field of NLP.",
            "12": "**Comprehensive Related Work**: The paper provides a comprehensive review of related work, situating DSPy within the broader context of neural network abstractions, in-context learning, and LM pipelines.",
            "13": "This helps in understanding the novelty and significance of the proposed approach.",
            "14": "#### Weaknesses\n\n1.",
            "15": "**Complexity of Implementation**: While the paper introduces a powerful and flexible framework, the complexity of implementing and understanding DSPy might be a barrier for some practitioners.",
            "16": "The learning curve associated with adopting this new programming model could be steep.",
            "17": "**Limited Scope of Case Studies**: Although the case studies on GSM8K and HotPotQA are thorough, the paper could benefit from additional case studies across a wider range of tasks.",
            "18": "This would help in demonstrating the generalizability and robustness of DSPy across different domains.",
            "19": "**Evaluation Metrics**: The paper primarily focuses on accuracy metrics for evaluation.",
            "20": "While this is standard, additional metrics such as computational efficiency, memory usage, and inference time could provide a more holistic evaluation of the proposed approach.",
            "21": "**Dependency on Pretrained LMs**: DSPy’s reliance on pretrained LMs like GPT-3.5 and Llama2-13b-chat might limit its applicability in scenarios where access to such models is restricted due to cost or other constraints.",
            "22": "Exploring the performance of DSPy with smaller, more accessible models could be beneficial.",
            "23": "**Scalability Concerns**: The paper mentions that compiling generally runs on the order of minutes or tens of minutes.",
            "24": "However, for very large datasets or more complex pipelines, the scalability of this approach might be a concern.",
            "25": "More detailed analysis on the scalability and performance trade-offs would be valuable.",
            "26": "**User Control and Customization**: While the modularity of DSPy is a strength, it also requires users to have a deep understanding of the modules and their interactions.",
            "27": "Providing more user-friendly interfaces or higher-level abstractions could make DSPy more accessible to a broader audience.",
            "28": "**Future Work and Extensions**: The paper briefly mentions future work but could benefit from a more detailed discussion on potential extensions and improvements.",
            "29": "For example, exploring dynamic test-time bootstrapping, more sophisticated optimization techniques, and integration with other ML frameworks could be interesting directions.",
            "30": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the development and optimization of LM pipelines through the introduction of DSPy.",
            "31": "The strengths of the proposed approach, including its modularity, parameterization, and empirical validation, outweigh the weaknesses.",
            "32": "Addressing the identified weaknesses, particularly in terms of complexity, scalability, and broader applicability, could further enhance the impact and adoption of DSPy in the ML community."
        },
        "qIhxcYk7zs": {
            "0": "Novel Approach.",
            "1": "This paper introduces a novel approach to systematically build LM pipelines and compile modules into a set of prompts (or fine-tunes) optimized for specific tasks.",
            "2": "The approach is promising, showcasing its ability to reduce the human effort required to develop prompt pipelines.",
            "3": "Well-Written.",
            "4": "The paper is clearly written and presents the core concepts of DSPy in a straightforward manner.",
            "5": "Lack of details: DSPy stands out from other frameworks, like LangChain, due to its capacity for automatic prompt generation and optimization.",
            "6": "However, the introduction of the DSPy Compiler lacks sufficient specificity.",
            "7": "It would be beneficial to provide more comprehensive details on how DSPy generates candidate values for module parameters (instructions, field description, and example input/output) based on signatures."
        },
        "uzjRMISkL7": {
            "0": "* The paper introduces a novel programming paradigm for defining and optimizing parameterized LLM-based text transformations.",
            "1": "* Though I have not used DSPy myself, my impression is that there is a focus on expressiveness and usability in the programming model, which enhances its usefulness for the ICLR community.",
            "2": "One piece of evidence for this expressiveness is the ability showcased to write CoT, reflection, ReAct, and other prompting strategies concisely within the DSPy model.",
            "3": "Another is the composibility e.g.",
            "4": "of different programs with different optimization (aka \"compilation\") approaches.",
            "5": "* DSPy reduces the reliance of creating LLM-based text transformations on manual prompting.",
            "6": "This can be seen as a strength, reducing the need for expertise in prompt writing.",
            "7": "(It can also on some occasions be a weakness if it increases the amount of examples required e.g.",
            "8": "from none.)",
            "9": "* My impression is that the value-add of DSPy for any of the listed strategies is somewhat small.",
            "10": "E.g.",
            "11": "implementing any of CoT, Reflection, or ReAct without DSPy does not require much code.",
            "12": "The same is true, I think, for the optimization approaches / compilers.",
            "13": "I expect this value-add grows when working with many such strategies at once, and additionally that DSPy provides organizational value both as the programs and compilers grow in complexity, and as the set of people using them grows.",
            "14": "Being able to compose different strategies and compiler techniques is also one of the key beneficial properties of the system.",
            "15": "* The evaluations performed do not provide a measure of compute usage or time (both for compilation as well as inference of the compiled programs), which makes comparisons across programs and compilers less meaningful.",
            "16": "* There are no examples of compiled programs provided in the paper or appendices.",
            "17": "I think an analysis of compiled programs would benefit the paper meaningfully.",
            "18": "In particular, some unanswered questions about the compiled programs include: how do they differ from the types of prompt programs that people write by hand or using other frameworks?",
            "19": "how do programs compiled for small LMs differ from those compiled for large LMs?",
            "20": "are there any obvious patterns in the compiled programs?",
            "21": "how about obvious shortcomings or irregularities, where additional hand-optimization would be easy?",
            "22": "any evidence that the optimization techniques overfit to the validation set?"
        },
        "Q00ZqwA9Bd": {
            "0": "DSPy is a major improvement over manually composing complex LM pipelines by hand.",
            "1": "The paper also demonstrates the possibility of automatically optimizing parts of the pipeline once written in the DSPy framework.",
            "2": "Finally, the experimental results are very impressive, particularly given the simplicity of user experience.",
            "3": "There is a major missing related work [1], which takes a similar approach of expressing LM pipelines as programs, and also comes with built-in optimizations.",
            "4": "I would be happy to increase my score if the paper is revised to include a discussion comparing the two approaches.",
            "5": "A more minor concern is that the optimization techniques demonstrated here are relatively limited in scope.",
            "6": "From a conceptual standpoint, I would have liked to see more than ensembling or bootstrapping few shot examples.",
            "7": "[1] Luca Beurer-Kellner, Marc Fischer, and Martin Vechev.",
            "8": "2023.",
            "9": "Prompting Is Programming: A Query Language for Large Language Models.",
            "10": "Proc.",
            "11": "ACM Program.",
            "12": "Lang.",
            "13": "7, PLDI, Article 186 (June 2023), 24 pages.",
            "14": "https://doi.org/10.1145/3591300\n\n====\n\nThe rebuttal has addressed my concerns and I have increased my score from a 6 to 8."
        }
    },
    "wHlYlC2TCM": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel method, DITO (Detection-Oriented Image-Text Pretraining for Open-vocabulary detection), which bridges the gap between image-level pretraining and open-vocabulary object detection.",
            "1": "This is achieved by using a detector architecture during pretraining, which is a significant departure from the traditional classification-based pretraining.",
            "2": "**Shifted-Window Learning**: The introduction of the Shifted-Window Learning (SWL) approach to enhance the robustness and translation-invariance of the backbone representation is innovative.",
            "3": "This method addresses the bias introduced by window attention patterns in vision transformers.",
            "4": "**Performance**: The proposed method achieves state-of-the-art results on the LVIS open-vocabulary detection benchmark, with a significant improvement of +6.5 mask AP over the best existing approach.",
            "5": "It also shows competitive performance on the COCO benchmark and in transfer detection setups.",
            "6": "**Simplicity and Effectiveness**: The approach is simple yet effective, extending the contrastive learning method to learn emergent object-semantic cues without the need for pseudo-labeling or weak supervision.",
            "7": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of the proposed method across multiple benchmarks (LVIS, COCO) and settings (standard, with external box annotations, transfer detection).",
            "8": "This comprehensive evaluation demonstrates the robustness and generalizability of the approach.",
            "9": "**Visualization and Analysis**: The paper includes visualizations that show the effectiveness of the detection-oriented pretraining in capturing localized semantic information.",
            "10": "This helps in understanding the qualitative improvements brought by the proposed method.",
            "11": "**Reproducibility**: The authors plan to release the code and models, which is crucial for reproducibility and further research in the community.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Complexity of Implementation**: While the method is conceptually simple, the implementation details, especially the integration of detection heads during pretraining and the shifted-window learning, might be complex and require significant computational resources.",
            "14": "This could limit the accessibility of the approach to researchers with less computational power.",
            "15": "**Limited Comparison with Pseudo-Labeling Methods**: Although the paper claims to outperform methods that use pseudo-labeling, a more detailed comparison with these methods, including an analysis of the trade-offs, would strengthen the argument.",
            "16": "It would be beneficial to understand the specific scenarios where DITO outperforms pseudo-labeling approaches and where it might fall short.",
            "17": "**Generalization to Other Tasks**: The paper focuses on open-vocabulary object detection.",
            "18": "It would be interesting to see how the proposed pretraining method generalizes to other related tasks, such as instance segmentation or panoptic segmentation.",
            "19": "Including such evaluations could provide a broader perspective on the applicability of the method.",
            "20": "**Bias and Fairness Considerations**: The paper briefly mentions the potential for reinforcing biases present in the raw web data used for pretraining.",
            "21": "However, a more in-depth analysis of these biases and their impact on the model's performance and fairness would be valuable.",
            "22": "This is especially important for applications in real-world scenarios where fairness and bias mitigation are critical.",
            "23": "**Ablation Studies**: While the paper includes several ablation studies, some aspects, such as the impact of different pretraining datasets or the effect of varying the number of RoIs sampled during pretraining, could be explored in more detail.",
            "24": "This would provide a deeper understanding of the factors contributing to the model's performance.",
            "25": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of open-vocabulary object detection by introducing a detection-oriented pretraining approach.",
            "26": "The method shows impressive performance improvements and introduces innovative techniques like shifted-window learning.",
            "27": "However, the complexity of implementation, limited comparison with pseudo-labeling methods, and the need for more detailed bias analysis are areas that could be further addressed.",
            "28": "The planned release of code and models is a positive step towards reproducibility and further research in this area."
        },
        "FEXRgDsRQk": {
            "0": "- The paper is well-written and easy to follow.",
            "1": "- The motivation of the method is clear and the proposed strategy is straightforward yet effective.",
            "2": "My primary concern pertains to the novelty of the proposed method.",
            "3": "Firstly, the detection-oriented image-text pertaining has been explored extensively since RegionCLIP[1].",
            "4": "Subsequently, there have been endeavors to employ randomly selected proposals for augmentation [2] and align multiple regions [3].",
            "5": "Given the foundation laid by the aforementioned works, the contrastive pretraining methodology introduced in this article, employing a detector architecture, may appear to lack a sufficient level of novelty.",
            "6": "Secondly,  I think the shifted-window learning technique is not enough to support the second technical contribution.",
            "7": "Overall, I agree with the practical values of this paper, but more scientific values from a paper in ICLR are expected.",
            "8": "[1] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao.",
            "9": "Regionclip: Region-based languageimage pretraining.",
            "10": "In CVPR, 2022.",
            "11": "[2] Wei, Fangyun, et al.",
            "12": "\"Aligning pretraining for detection via object-level contrastive learning.\"",
            "13": "Advances in Neural Information Processing Systems 34 (2021): 22682-22694.",
            "14": "[3] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy.",
            "15": "Aligning bag of regions for open-vocabulary object detection.",
            "16": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.",
            "17": "15254–15264, 2023a."
        },
        "W6Z427OuKh": {
            "0": "+ The paper is easy to follow.",
            "1": "+ The method demonstrates good performance on rare categories.",
            "2": "- The proposed contrastive pre-training method is not novel.",
            "3": "Incorporate region information in contrastive learning has been explored in many works [1][2][3].",
            "4": "- Using a private data on pre-training is not fair and not convincing.",
            "5": "- The baseline method is already better than previous works.",
            "6": "It makes the effectiveness of proposed components questionable.",
            "7": "- Figure 2 is misleading.",
            "8": "The similarity map using the backbone features in the middle should be comparing with CLIP based method, not your own.",
            "9": "- In Table 6 (c), stride 4 clearly performs better.",
            "10": "The paper doesn't explain why it chooses a suboptimal one.",
            "11": "- Since the method also involves external box annotations, it should compare with previous methods on pure zero-shot OVD benchmark.",
            "12": "[1] RegionCLIP: Region-based Language-Image Pretraining.",
            "13": "CVPR 2022\n\n[2] Aligning Pretraining for Detection via Object-Level Contrastive Learning.",
            "14": "NIPS 2021\n\n[3] Point-Level Region Contrast for Object Detection Pre-Training.",
            "15": "CVPR 2022"
        },
        "Q6RO939rgb": {
            "0": "- VL pretraining for detection seems a valuable direction of research and\nto my knowledge, this work is one of the first attempts along this direction.",
            "1": "- The overall frame is kept simple without using extra annotation or region proposals, \nwhich may inspire the following researches for extensions or applications for new tasks.",
            "2": "- Clear SOTA performance in the large-scale LVIS OVD benchmark.",
            "3": "- Justification of pretraining with random boxes: I am concerned that the random boxes may produce noisy supervisory signals.",
            "4": "For example, multi-object captions like \"a bird and a dog\"  may be assigned both for bird regions and dog regions.",
            "5": "I think that some discussions around here are needed.",
            "6": "Did the Authors test other choices here, for example, using regular grids?",
            "7": "- Basic comparisons with CLIP: DITO looks like a VL pretraining method, in essence, rather than an OVD method,\nbecause a significant part of the proposed modifications are in the pretraining phase.",
            "8": "It would be natural to compare it with CLIP in the tasks that it was designed for, \nfor example, text-sentence matching or zero-shot classification.",
            "9": "Or is it impossible due to architectural changes?"
        },
        "bxjnijrmoD": {
            "0": "Overall, the performance of the proposed framework is very good, and the motivation is reasonable.",
            "1": "* I also believe that aligning semantics at the region-level is very important and meaningful.",
            "2": "Now many methods are exploring how to achieve region-level pre-training.",
            "3": "The core point of open-vocabulary lies in region-language alignment, thereby generalizing to novel classes.",
            "4": "* Due to the framework being based on detection pre-training, it eliminates the need for distillation or pseudo-labeling, which is a strength of this work.",
            "5": "* The comparison experiments are very sufficient, and the proposed method achieves state-of-the-art results on various benchmarks.",
            "6": "I have several points that I find confusing in this article: \n\n* 1.",
            "7": "The entire framework relies on two backbones during inference.",
            "8": "The article mentions (there is a tendency to lose the pretrained image-text knowledge, Inspired by previous work (Kim et al., 2023a), we use a separate frozen ViT backbone as an open-vocabulary region classiﬁer at inference time to compute the VLM score z).",
            "9": "This indicates that the entire framework does not maintain semantic information during detection pre-training, which seems strange to me and makes the framework appear bulky.",
            "10": "If the image-text semantics could be preserved during detection pre-training, this would make the work more reasonable.",
            "11": "* 2.",
            "12": "This method uses the aligned dataset, while previous methods follow the clip pretrain setting, making it difficult to compare this method with previous methods.",
            "13": "It gives me the feeling that the good results are due to the large amount of data pre-training, rather than Detection-Oriented Pretraining.",
            "14": "* 3.",
            "15": "The design of shifted-window learning is strange.",
            "16": "It is more of a general network design rather than a solution for open vocabulary detection.",
            "17": "Furthermore, shifted-window and open vocabulary seem disconnected."
        }
    },
    "vLJg4wgBPu": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method called Iteration by Regimenting Self-Attention (IRSA) to trigger iterative behaviors in GPT-3, enabling it to execute algorithms rather than just generate or recall them.",
            "1": "This is a significant advancement in the capabilities of large language models (LLMs).",
            "2": "**Comprehensive Evaluation**: The authors provide a thorough evaluation of IRSA across a variety of tasks, including classical algorithms like Bubble Sort, logical deduction puzzles, and dynamic programming problems.",
            "3": "This demonstrates the versatility and robustness of the proposed method.",
            "4": "**Educational Applications**: The paper highlights the potential applications of IRSA in education, particularly in teaching data structures and algorithms.",
            "5": "The structured prompts and responses resemble student assignments, making it a valuable tool for educators.",
            "6": "**Prompt Design Insights**: The study emphasizes the critical role of prompt design in LLM performance.",
            "7": "By showing that even partial prompts can trigger algorithmic behavior, the paper provides valuable insights into how to effectively design prompts for complex tasks.",
            "8": "**Comparison with Baselines**: The authors compare IRSA with existing prompting methods, including few-shot learning and Chain-of-Thought (CoT) prompting.",
            "9": "The results show that IRSA significantly improves performance on tasks that require iterative reasoning.",
            "10": "**Skip-to-State Attention**: The introduction of skip-to-state attention to manage the token limit in LLMs is a practical and innovative solution.",
            "11": "This approach ensures that the model can handle long iterative algorithms without running out of tokens.",
            "12": "**Potential for Broader Applications**: The paper discusses the broader implications of IRSA for software engineering and other fields, suggesting that LLMs can be programmed to execute specific tasks, which could revolutionize how these models are used.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Complexity of Prompt Design**: While the paper demonstrates the effectiveness of IRSA, the process of designing such highly structured prompts can be complex and time-consuming.",
            "15": "This may limit the practical applicability of the method for users who are not experts in prompt engineering.",
            "16": "**Dependence on Specific Models**: The experiments are primarily conducted using GPT-3 CODE-DAVINCI-002.",
            "17": "While the authors briefly mention experiments with other models, the results may not generalize to all LLMs.",
            "18": "Further evaluation on a wider range of models would strengthen the findings.",
            "19": "**Limited Discussion on Limitations**: The paper could benefit from a more detailed discussion of the limitations of IRSA.",
            "20": "For example, the potential for infinite loops in logical deduction puzzles is mentioned but not thoroughly explored.",
            "21": "Understanding the boundaries of the method is crucial for its practical application.",
            "22": "**Scalability Concerns**: The skip-to-state attention mechanism is a clever solution to the token limit problem, but it may not be scalable for very large or complex tasks.",
            "23": "The paper does not address how this approach would handle tasks that require significantly more tokens than the current limits allow.",
            "24": "**Evaluation Metrics**: The paper primarily uses accuracy as the evaluation metric.",
            "25": "While this is appropriate for many tasks, additional metrics such as execution time, computational efficiency, and robustness to prompt variations would provide a more comprehensive assessment of IRSA's performance.",
            "26": "**Reproducibility**: The paper provides detailed prompts and examples, but it would benefit from a more explicit discussion on reproducibility.",
            "27": "Providing code and datasets used in the experiments would help other researchers validate and build upon the work.",
            "28": "**Comparison with GPT-4**: The paper mentions that GPT-4 cannot consistently execute programs without IRSA, but the comparison is not deeply explored.",
            "29": "A more detailed analysis of how GPT-4 performs with and without IRSA would provide valuable insights into the method's effectiveness across different model versions.",
            "30": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the capabilities of LLMs by introducing IRSA, a method that enables GPT-3 to execute iterative algorithms.",
            "31": "The comprehensive evaluation and potential applications in education and software engineering are notable strengths.",
            "32": "However, the complexity of prompt design, scalability concerns, and limited discussion on limitations are areas that could be improved.",
            "33": "Addressing these weaknesses would further enhance the impact and applicability of the proposed method."
        },
        "rP7bGyBAP8": {
            "0": "Importance of contribution: The proposed solution can achieve outperformance than state-of-the-art approaches.",
            "1": "Meanwhile, it also highlights the significance of prompting engineering as GPT-3 applied IRSA can generate more accurate results than GPT-4 without IRSA.",
            "2": "Soundness: The author(s) explain the approach in detail, and conduct evaluation via comparative analysis regarding different questions.",
            "3": "Quality of presentation: The paper is well-organized, and the language is technical yet understandable for readers with domain knowledge.",
            "4": "Comparison with related works: The author(s) introduce extant studies on large language model prompting.",
            "5": "- The methodology can be elaborated for better clarity.",
            "6": "- The overall structure of this paper can be adjusted.",
            "7": "- The research gaps can be further highlighted and discussed."
        },
        "AQPBJQORMX": {
            "0": "This is an interesting work.",
            "1": "The proposed approach seems to work given the evaluated programs.",
            "2": "The proposed work may point out an interesting direction for using LLMs to execute programs, but its current form and results are premature and it’s not ready to be published.",
            "3": "The presentation of IRSA has been majorly illustrated by examples.",
            "4": "While such examples are useful, there still lacks a formulation of IRSA.",
            "5": "The IRSA prompting for these examples look ad-hoc, and it is not clear how IRSA can be automatically applied to execute general programs, without significant manual efforts.",
            "6": "The example and its execution path play the critical role in IRSA.",
            "7": "Isn’t the availability of an execution path too strong assumption for enabling GPT to execute a program?",
            "8": "There are many things not discussed, including how to select the example and how to achieve the execution path.",
            "9": "Is there one example or multiple examples used in IRSA?",
            "10": "It is not clear why IRSA is not used together with GPT4, which casts doubts on the applicability of IRSA approach.",
            "11": "The evaluation is neither comprehensive nor systematic.",
            "12": "The example programs in the evaluation look simplistic."
        },
        "YPbYKwHyxx": {
            "0": "## potentially significant\n\nThis paper outlines several good prompting strategies, which are useful if you want to have the LLM to reason programmatically, with a rigid syntactic structure in its execution trace.",
            "1": "The paper explains each strategy, irsa, skip attention, fragmented prompting with examples, and show the proposed prompts can achieve better results paired with a weaker model (gpt3.5) than a naive prompt with a more advanced model (gpt4)\n\nThe analogy of GPT as a turing machine is good too.",
            "2": "## poor quality and clarity\n\nIt is unclear if the proposed method can be reliably replicated to other domains, given that it is only evaluated on a handful of problems.",
            "3": "I believe this work can be made substantially better if an automated method could be derived turning an existing complex program into a prompt, and evaluated on a larger set of problems, rather than the simplistic 100 python arithmetic problems.",
            "4": "## less than ideal novelty\nIt is also unclear how the proposed method is significantly different from Nye (2021)'s work on scratchpad, as both leverages trace information extensively.",
            "5": "It would be good to have a related work section to spell out the exact differences.",
            "6": "The paper note that the proposed technique may be beneficial in the education domain.",
            "7": "However, wouldn't having a LLM to simply mark up an existing execution trade of actually running the program be a better (and more correct) alternative?"
        },
        "xbmG2Tmjx1": {
            "0": "+ The paper explores an important question of broad interest to the community.",
            "1": "The exact capabilities and limits of LLMs remain unclear.",
            "2": "This work shows that highly structured prompts can be used to better control LLM output on tasks requiring precise state control (memory) and iterative execution (loops).",
            "3": "+ The proposed prompts (IRSA) are intuitively clear.",
            "4": "They seem novel, to my knowledge.",
            "5": "+ The experiments demonstrate that the IRSA prompts do indeed help the LLM correctly simulate algorithms requiring loops over the distribution of inputs considered.",
            "6": "There is a good amount of detail included in the main paper and appendices.",
            "7": "- The use of a trace in the prompt raises a few questions, which are not addressed.",
            "8": "A classic sorting algorithm can correctly sort very long lists using a relatively short specification of the algorithm.",
            "9": "Can IRSA do the same (sort long input lists with a short trace)?",
            "10": "The experiments seem restricted to short inputs length 5 in sorting, for example).",
            "11": "The scaling, generalization and robustness of the IRSA prompt to different inputs aren't well explored in the paper.",
            "12": "- The paper could better highlight its algorithmic and empirical contributions relative to a rapidly growing body of literature on how to improve a LLM's instruction-following abilities.",
            "13": "At the moment, I'm not sure if the experiments conclusively demonstrate that IRSA improves the instruction-following (via algorithm execution) abilities of a LLM.",
            "14": "- The terminology used can sometimes be a bit loose.",
            "15": "For example, \"this strategy hardens the attention\", \"skipping unnecessary attention saves computation\", etc.",
            "16": "More formal descriptions of these important ideas would increase the technical rigor of the paper.",
            "17": "Alternatively, the paper could simplify the description to emphasize the empirical aspects (i.e., prompt engineering), which are also valuable."
        }
    },
    "x6u2BQ7xcq": {
        "llm_review": {
            "0": "**Innovative Approach**: The introduction of image tagging into vision-language models is a novel approach that provides strong semantic guidance.",
            "1": "This method leverages tags parsed from image-paired text, which is a significant improvement over previous methods that relied on manually labeled or automatically detected object tags.",
            "2": "**Annotation-Free Tags**: The use of large-scale annotation-free image tags is a major strength.",
            "3": "This approach allows the model to utilize a vast amount of data without the need for expensive and time-consuming manual annotations.",
            "4": "**Comprehensive Tag Categories**: The model goes beyond object tags to include scenes, attributes, and actions.",
            "5": "This comprehensive approach provides a richer and more diverse set of tags, which enhances the model's ability to understand and describe images.",
            "6": "**Superior Performance**: The paper demonstrates that Tag2Text achieves state-of-the-art results across a wide range of downstream benchmarks, including image captioning, image-text retrieval, and multi-label image recognition.",
            "7": "The model's performance is comparable to fully supervised models, which is impressive given that it uses annotation-free tags.",
            "8": "**Efficiency**: The model is designed to be efficient, with fewer parameters and improved running time compared to previous detector-based models.",
            "9": "This efficiency is achieved by adding a simple tagging head to the image encoder, which allows for end-to-end pre-training.",
            "10": "**Flexibility and Control**: The ability to input desired tags to generate corresponding captions provides flexibility and control over the generated text.",
            "11": "This feature is particularly useful for applications that require specific descriptions or emphasis on certain aspects of an image.",
            "12": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of the model's performance on multiple benchmarks and tasks.",
            "13": "The inclusion of both zero-shot and fine-tuning results adds depth to the evaluation.",
            "14": "**Weaknesses:**\n\n1.",
            "15": "**Complexity of Implementation**: While the approach is innovative, it may be complex to implement in practice.",
            "16": "The process of parsing text to obtain tags and integrating these tags into the model requires careful design and tuning.",
            "17": "**Dependence on Text Quality**: The quality of the parsed tags depends heavily on the quality of the text descriptions.",
            "18": "If the text descriptions are noisy or inaccurate, the tags may not provide the desired semantic guidance.",
            "19": "**Scalability of Tag Parsing**: The text semantic parser used to obtain tags may face challenges in scaling to very large datasets or handling diverse and complex text descriptions.",
            "20": "The paper does not provide detailed information on the scalability and robustness of the tag parsing process.",
            "21": "**Evaluation on Diverse Datasets**: While the model is evaluated on several benchmarks, it would be beneficial to see its performance on more diverse and challenging datasets.",
            "22": "This would provide a better understanding of the model's generalization capabilities.",
            "23": "**Comparison with More Baselines**: The paper compares Tag2Text with several state-of-the-art models, but additional comparisons with more recent and diverse baselines would strengthen the evaluation.",
            "24": "This includes models that may use different architectures or training paradigms.",
            "25": "**Interpretability of Tags**: The paper does not discuss the interpretability of the tags generated by the model.",
            "26": "Understanding how the model generates and uses tags could provide insights into its decision-making process and help identify potential biases or errors.",
            "27": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in vision-language pre-training by introducing image tagging as a guiding mechanism.",
            "28": "The approach is innovative, efficient, and demonstrates superior performance across multiple tasks.",
            "29": "However, there are some challenges related to implementation complexity, dependence on text quality, and scalability of tag parsing that need to be addressed.",
            "30": "Further evaluation on diverse datasets and comparison with more baselines would provide a more comprehensive understanding of the model's capabilities."
        },
        "RETplhNAvl": {
            "0": "I really like the paper.",
            "1": "It is clearly motivated, well written, technically well executed, has clear novelty, and has good experimental results.",
            "2": "But I want to highlight one thing that is surprisingly hard to find: it seems useful.",
            "3": "The reasons: 1) Current V&L models are trained with long captions, which means small tag-like queries are not well supported.",
            "4": "There are indeed some object detection methods a-la CLIP (owl-vit and co) but they offer different functionalities and have different requirements.",
            "5": "So this is a good addition to the V&L toolbox.",
            "6": "2) it offers a way of controlling caption generation through the use of input tags.",
            "7": "Minor suggestions (up to the authors and no reply needed):\nFig.",
            "8": "1 might get a bit confusing as both the \"prior work strategy\" and the current strategy are included in the same flow graph.",
            "9": "Table 2 shows the last 3 methods seem apart from the rest but it's unclear why they are separated.",
            "10": "The method is based on some relatively standard techniques that are however well executed and put together.",
            "11": "While not much of a minus, but maybe a reason for an accept vs strong accept."
        },
        "V7u8zGLppk": {
            "0": "The idea of the utilization of image tags parsing from large-scale image-text pairs is interesting, efficient and effective from improving the performance of vision-language models.",
            "1": "The framework of Tag2Text employs a multi-task pretraining approach, including Tagging, Generation, and Alignment.",
            "2": "These tasks are reasonable and share the same visual features obtained from the image encoder, guaranteeing the efficiency of the framework.",
            "3": "A large number of experimental results over image tagging, image captioning, and image-text retrieval tasks prove the effective of the proposed methods.",
            "4": "The detail of text semantic parser is not clear.",
            "5": "Although it is based on existing work of [Wu et al.",
            "6": "2019], it should make clear how to obtain the corresponding tags in the paper.",
            "7": "In Fig.",
            "8": "2, it is difficult to understand which are users’ input desired tags, since they share the same forms of the recognized image tags.",
            "9": "In the Image-Tag-Text Generation paragraph, the introduction of this task is not clear.",
            "10": "According to Fig.",
            "11": "4(c), it seems the text embedding should not be used as input, which is conflicting with the introduction in the corresponding paragraph.",
            "12": "In the experiment on controllability analysis, it is not clear how the threshold of tagging head to control the tagging guidance.",
            "13": "There are some typos in the manuscript."
        },
        "ekIi9di4rb": {
            "0": "- The idea is straightforward and easy to grasp.",
            "1": "- Given the high cost of human labeling and the limited diversity in generating captions for Image Captioning models, such as BLIP, finding new methods for generating comprehensive captions is both challenging and valuable.",
            "2": "- This work offers a potential benefit in the context of person re-identification (REID).",
            "3": "- The motivation behind this approach is also quite appealing.",
            "4": "- Notably, the Tag method diverges from the commonly used Faster R-CNN-based object detectors and demonstrates significantly improved speed.",
            "5": "- Data and Pre-training Settings: The author of the paper used a 4M setting, which includes training data from COCO (Common Objects in Context) and Visual Genome.",
            "6": "NoCaps data is sourced from OpenImages and COCO.",
            "7": "Importantly, this work does not incorporate any out-of-distribution data.",
            "8": "The success of this approach on COCO-related tasks is attributed to the similarity between the pre-training data and COCO style, and it's noted that many works beyond BLIP face similar challenges.",
            "9": "- TagEval Task: TagEval is mentioned as a task, but it is not considered popular, and its persuasiveness is limited.",
            "10": "Models trained on tags are noted to perform well in this case.",
            "11": "- Tag Introduction in Pre-training: The introduction of tags in pre-training is not a novel idea.",
            "12": "There are existing works, like OSCAR, that have explored the concept of using tags to improve vision-language pre-training."
        },
        "MtYflFGaJd": {
            "0": "The paper is well written and easy to follow.",
            "1": "The method is simple and is shown to work well.",
            "2": "The idea of using image tags to aid VL pre-training makes a lot of sense.",
            "3": "The results are not always SOTA but are very good.",
            "4": "Results on multiple tasks/datasets are provided.",
            "5": "I believe the results are sufficient to show that the main idea behind the paper works as well as expected.",
            "6": "I think the main problem with the paper is that all of its components have been proposed before so the paper looks more like a re-implementation of known ideas with more recent architectures and pipelines which is of course expected to work better.",
            "7": "Specifically the main idea of using tags to aid VL pre-training appears in many works including OSCAR or more recently in DiHT (Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training) while other losses used like I2T are very commonly used in most works in VL pre-training.",
            "8": "But the method for sure can serve as strong baseline.",
            "9": "Somewhat less important concern: as most experiments are on COCO/Flick which are datasets very close to the ones used for training I am wondering whether the authors could carry out an experiment on out-of-domain data ."
        },
        "VlsCVYBflf": {
            "0": "Including image tagging task to vision language pretraining seems to be promising, which particularly benefit image captioning.",
            "1": "The proposed design that combines multi-tasks for vision language pretraining is interesting.",
            "2": "The experimental results are promising.",
            "3": "The paper is well-written in general and easy to comprehend.",
            "4": "This paper discusses VL pretraining methods that are either based on 1) object detection or 2) image tagging (the proposed method) and argues that image tagging is faster and introduces significantly fewer parameters.",
            "5": "However, this may not be a compelling motivation for choosing image tagging, as object detectors are fixed (with no additional learnable parameters) and only need to be executed once before training, incurring marginal computational cost compared to the training phase.",
            "6": "It would be beneficial for this paper to incorporate specific mathematical formulations to provide a more comprehensive description and discussion of the fundamental problems that require resolution.",
            "7": "There is room for improvement in the typesetting.",
            "8": "Table 1 appears to be disorganized, making it unclear which numbers to focus on and compare.",
            "9": "It is advisable to separate results for different tasks into distinct tables.",
            "10": "The \"SOTA\" comparison for image tagging (multi-label classification) seems to omit a substantial portion of recent works.",
            "11": "As a result, the reviewer maintains a skeptical stance concerning the associated claims and conclusions.",
            "12": "Metrics such as Precision, Recall, and F1 score, which are commonly used for image tagging and multi-label classification, are notably absent from the results.",
            "13": "The construction of the tag category system appears to involve human annotation in the process (Section 3.2), which contradicts the earlier claim of being an \"automatic\" approach (Section 1).",
            "14": "Conducting an ablation study on the choice of vocabulary set (tag set) size to be mined could offer valuable insights into the proposed method."
        }
    },
    "KIPJKST4gw": {
        "llm_review": {
            "0": "#### Strengths:\n\n1.",
            "1": "**Clear Research Question**: The paper addresses a specific and relevant research question: at which training stage does code data help LLMs' reasoning capabilities?",
            "2": "This focus provides a clear direction for the study and its contributions.",
            "3": "**Comprehensive Evaluation**: The authors conduct a thorough evaluation of the impact of code data on LLMs across different training stages (pre-training and instruction-tuning) and on various reasoning tasks.",
            "4": "This comprehensive approach ensures that the findings are robust and applicable to multiple domains.",
            "5": "**Diverse Reasoning Tasks**: The paper evaluates the models on six reasoning tasks across five domains, including logical reasoning, legal reasoning, scientific reasoning, analogical reasoning, and code reasoning.",
            "6": "This diversity in tasks helps in understanding the generalizability of the findings.",
            "7": "**Insightful Findings**: The paper provides several key insights, such as the significant enhancement of general reasoning capabilities when code data is introduced during pre-training, and the task-specific reasoning improvements during instruction-tuning.",
            "8": "These insights are valuable for the development and training of future LLMs.",
            "9": "**Dynamic Mixing Strategy**: The exploration of different data mixing strategies (uniform sampling, stepwise increase, and stepwise decrease) during instruction-tuning is a novel approach.",
            "10": "The findings suggest that a stepwise decrease strategy is particularly effective for code-related tasks.",
            "11": "**Open-Source Contribution**: The authors contribute to the community by releasing the model implementation and trained model parameters.",
            "12": "This openness facilitates further research and development in the field.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Limited Model Sizes**: The study primarily focuses on models with 2.6B and 13B parameters.",
            "15": "While these are substantial, the findings might not fully generalize to much larger models like GPT-4 or PaLM, which exhibit emergent capabilities.",
            "16": "Future work should explore the impact of code data on larger models.",
            "17": "**Negative Impact on Other Tasks**: The paper notes that adding code data can negatively impact performance on some non-reasoning tasks, such as reading comprehension.",
            "18": "This trade-off is not deeply explored, and further investigation is needed to understand and mitigate these negative effects.",
            "19": "**Resource Constraints**: The paper mentions that validating the impact of code data in the pre-training phase requires significant resources, which were not fully addressed in this study.",
            "20": "This limitation suggests that the findings might be preliminary and need further validation with more extensive resources.",
            "21": "**Instruction-Tuning Data Quality**: The quality and diversity of the instruction-tuning data, especially the code-related instructions, could significantly influence the results.",
            "22": "The paper does not provide a detailed analysis of the data quality, which could be a potential area for improvement.",
            "23": "**Statistical Significance**: While the paper reports p-values to indicate statistical significance, it would benefit from a more detailed statistical analysis, including confidence intervals and effect sizes, to better understand the practical significance of the findings.",
            "24": "**Chain-of-Thought (CoT) Analysis**: The paper briefly explores the impact of CoT prompts on reasoning tasks.",
            "25": "However, a more in-depth analysis of how CoT interacts with code data and its potential to enhance reasoning capabilities would strengthen the study.",
            "26": "#### Conclusion:\n\nOverall, the paper makes significant contributions to understanding the role of code data in enhancing the reasoning capabilities of LLMs.",
            "27": "The comprehensive evaluation, insightful findings, and open-source contributions are commendable.",
            "28": "However, the study could be improved by addressing the limitations related to model sizes, negative impacts on other tasks, and a more detailed statistical analysis.",
            "29": "Future work should also explore the impact of code data on larger models and further investigate the trade-offs involved in using code data during training."
        },
        "uvtMuxvWvx": {
            "0": "- This paper is well-motivated.",
            "1": "The impact of code data in LLMs is a hot research question.",
            "2": "This paper answers this issue from the reasoning capability aspect.",
            "3": "- The experiments are comprehensive, and the insights are remarkable.",
            "4": "The reasoning capability of LLMs is evaluated via six tasks in five domains.",
            "5": "The authors provide critical analyses and significant insights on training LLMs and the reasoning capability of LLMs.",
            "6": "- The idea of dynamic mixed strategy is easy to follow yet effective.",
            "7": "It helps LLMs learn reasoning skills progressively during training.",
            "8": "- The authors provide comprehensive open-source resources, demonstrating the reproducibility of the models.",
            "9": "These resources are valuable for the LLM community.",
            "10": "- Missing discussion on the applications.",
            "11": "Although the authors conduct experiments and provide insights on training LLMs and improving their reasoning capability, this paper does not discuss how to apply the insights to enhance the LLM products in different domains.",
            "12": "- Unclear construction of training corpus.",
            "13": "The author should provide more details about data collection, data cleaning, and training data construction.",
            "14": "The authors use fuzzy data deduplication, but they have not explained the tools of fuzzy.",
            "15": "They should open-source the data for reproducibility.",
            "16": "Besides, the detailed model architecture is missing.",
            "17": "- Table 7 is confusing.",
            "18": "The results in Table 7 show the code data will lead to a performance drop on four out of five datasets.",
            "19": "It indicates that the code data may not help to improve the reasoning capability of LLMs.",
            "20": "The authors should provide valid reasons.",
            "21": "- The related work is limited.",
            "22": "Recently, there have been various papers discussing the reasoning capability of LLMs.",
            "23": "Therefore, the authors should survey more related papers and compare with them.",
            "24": "- Fix the grammar errors and improve the presentation.",
            "25": "On page 8, “The experiment found that…’’ -> “The experiment showed that’’.",
            "26": "- Missing future work.",
            "27": "The authors should provide the potential future work on LLMs based on the experimental results and insights provided in this paper.",
            "28": "[1] Roziere B, Gehring J, Gloeckle F, et al.",
            "29": "Code llama: Open foundation models for code[J].",
            "30": "arXiv preprint arXiv:2308.12950, 2023."
        },
        "dXTv0waMHt": {
            "0": "Valuable research question.",
            "1": "The paper raises a meaningful research question: at which training stage introducing code data can really help the reasoning capabilities of LLM?",
            "2": "This question is of critical significance for understanding the training and application of LLM.",
            "3": "Comprehensive experimental design.",
            "4": "This paper provides a comprehensive and fair evaluation of the reasoning capabilities of LLMs on six reasoning tasks covering five domains.",
            "5": "This broad experimental scope ensures the generalizability and reliability of the conclusions.",
            "6": "Additionally, the authors compare models with different sizes to verify the generalization of the conclusion.",
            "7": "In-depth analyses and insights.",
            "8": "The paper not only provides experimental results but also performs in-depth analysis, providing insights into mixing code and text data to enhance the general reasoning capabilities and code reasoning capabilities of LLM.",
            "9": "Specifically, in the pre-training stage, mixed code data helps LLM improve general reasoning capabilities, and in the SFT stage, mixed code data helps LLM improve specific code reasoning capabilities.",
            "10": "Experimental details are insufficient.",
            "11": "The paper may not provide enough details on experimental settings and parameter selection in some parts (such as data mixing strategies, decoding strategies, etc.).",
            "12": "This might challenge researchers attempting to replicate or extend this work.",
            "13": "Recently, various code foundation models, such as CodeLlama [1], have been opened.",
            "14": "However, the authors do not conduct any discussions or experiments on them.",
            "15": "In my opinion, the reasoning capability of code foundation models is also an essential part of the interest scope of this work.",
            "16": "Quality of code data.",
            "17": "The quality of code data can have a significant impact on the reasoning capabilities of LLM.",
            "18": "How the author ensures the high quality of code data is not discussed in depth in the article.",
            "19": "The related work part is weak.",
            "20": "Missing important papers, such as [1,2,3].",
            "21": "[1] Roziere B, Gehring J, Gloeckle F, et al.",
            "22": "Code llama: Open foundation models for code[J].",
            "23": "arXiv preprint arXiv:2308.12950, 2023.",
            "24": "[2] Yang A, Xiao B, Wang B, et al.",
            "25": "Baichuan 2: Open large-scale language models[J].",
            "26": "arXiv preprint arXiv:2309.10305, 2023.",
            "27": "[3] Li P, Sun T, Tang Q, et al.",
            "28": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors[J].",
            "29": "arXiv preprint arXiv:2305.05711, 2023."
        },
        "87M2Eq4k3D": {
            "0": "- **Clear Hypothesis:** The paper asks a clear question and provides a clear setup for testing various hypothesis about that question.",
            "1": "- **Clear message:** The results show a clear message about the research question, albiet only on smaller-sized models.",
            "2": "- **Clarity of the writing:** The writing was mostly clear and easy to follow - **Datasets:** I was not familiar with some of the datasets used in this work and after looking into some of them, I could not get a sense of how general and challenging they are.",
            "3": "The majority of the computation cost for this project seems to be on the training stage, so I believe reporting results on a few more datasets (maybe only for Tables 2 and 3) can strengthen the main arguments of the paper.",
            "4": "That could include datasets from other reasoning domains (e.g., math might be an important one that does not appear in the results) or from the same domains but on datasets that are more established.",
            "5": "- **Mixture experiment:** The experiment on exploring ways to mix code and text data is interesting, but given that adding code at the instruction-tuning stage was already shown to be not that effective, I wonder why it was tested for the instruction-tuning stage.",
            "6": "I understand the high computation cost of pre-training, but it seems to me that given the previous set of results, this experiment makes sense mostly at the pre-training stage where we have seen that code data can be effective.",
            "7": "- **Language Inconsistency:** - For the results in Section 3.3.5, the authors conclude that training with code data *has little negative impact* on the performance of other tasks.",
            "8": "But the numbers in Table 7 don't seem to show little negative impact.",
            "9": "The large impact on the DuReader dataset has been already pointed out by the authors.",
            "10": "Moreover, on CMNLI the performance decreases from 45.07 to 43.49 which is almost equal in magnitude to some of the gains reported in Table 2.",
            "11": "I believe the language should be more consistent on the amount of improvement/decrement that can be considered a significant amount for the datasets and the experimental setup of the paper.",
            "12": "- **Minor suggestions:** 1- In Table 3, there are multiple equal numbers but only one of them is in bold face.",
            "13": "2- In Table 4, I suggest reversing the rows and columns to make it consistent with the other tables."
        },
        "ndP5xZp2gJ": {
            "0": "* The problem studied is interesting: the relationship between code data and reasoning and the paper aims to somehow tackle this issue.",
            "1": "* The paper is well-written and the results are well-presented.",
            "2": "* From my understanding (and correct me If I'm wrong), the code model is trained on more overall tokens than the NL model.",
            "3": "I would expect a study like that to control for the number of pre-training tokens while changing their nature i.e., text vs. code.",
            "4": "If the code model is trained on as many natural text tokens as the baseline model in addition to having code in the pre-training data, then the code model should be expected to perform better because it was trained on more data.",
            "5": "No surprise there.",
            "6": "* It's hard to say whether the results reported have statistical significance.",
            "7": "For example, in Table 2, the code model is only 0.13 points better than the NL model on ScienceQA.",
            "8": "Are these results significant?",
            "9": "And are these enough to conclude that code reasoning?",
            "10": "I would expect the authors to run a statistical significance test to support their results.",
            "11": "* The proposed dynamic mixing strategy produces very marginal improvements (except over logical reasoning) and has a negative effect on the performance over three tasks.",
            "12": "The paper does not thoroughly investigate why this is the case.",
            "13": "Also, the design of the mixing strategy seems rather arbitrary.",
            "14": "* The evaluation does not cover mathematical reasoning, although it's one type of reasoning where we should expect great improvements since code data is roughly similar to math data."
        }
    },
    "XTJ0YVBM10": {
        "llm_review": {
            "0": "TASK-ORIENTED ASKING WITH FACT-LEVEL MASKING\"\n\n#### Strengths\n\n1.",
            "1": "**Novelty and Relevance**: The paper introduces a novel concept of Task-Oriented Asking (TOA) and presents a framework for evaluating it.",
            "2": "This is a significant contribution as it addresses a gap in the current capabilities of large language models (LLMs) which are typically not designed to ask follow-up questions to gather missing information.",
            "3": "**Fact-Level Masking (FLM)**: The introduction of FLM as a method to create self-supervised TOA datasets is innovative.",
            "4": "This method allows for the generation of datasets that can be used to train and evaluate TOA models without the need for extensive manual annotation.",
            "5": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of several zero-shot language models on the newly created FLM-HotpotQA dataset.",
            "6": "The comparison with human performance highlights the current limitations of LLMs in generating useful task-oriented questions.",
            "7": "**Detailed Methodology**: The paper offers a clear and detailed description of the methodology, including the creation of the FLM-HotpotQA dataset, the evaluation pipeline, and the specific models used.",
            "8": "This transparency allows for reproducibility and further research in this area.",
            "9": "**Insightful Analysis**: The analysis of failure modes and the comparison of different models' performance provide valuable insights into the challenges and potential areas for improvement in TOA.",
            "10": "#### Weaknesses\n\n1.",
            "11": "**Limited Scope of Evaluation**: While the paper provides a comprehensive evaluation on the FLM-HotpotQA dataset, it would have been beneficial to see the application of FLM and TOA on a wider range of tasks and datasets.",
            "12": "This would help in understanding the generalizability of the proposed methods.",
            "13": "**Oracle Model Limitations**: The use of Flan-T5-Base as the oracle model, while justified for accessibility, may not fully capture the complexity of human responses.",
            "14": "Exploring more sophisticated or diverse oracle models could provide a more robust evaluation of TOA models.",
            "15": "**Human Annotation Comparison**: The comparison with human performance is based on a relatively small test subset (n=400).",
            "16": "A larger sample size would provide more robust conclusions about the gap between human and model performance.",
            "17": "**Iterative Asking Not Explored**: The paper mentions the potential for iterative task-oriented asking but does not explore this in the experiments.",
            "18": "Iterative asking could be a significant aspect of real-world applications and warrants investigation.",
            "19": "**Primary Model Bias**: The paper notes that primary models like GPT-4 may have memorized large portions of Wikipedia, which could affect the evaluation.",
            "20": "This bias needs to be addressed more thoroughly to ensure that the evaluation of TOA models is fair and accurate.",
            "21": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the field of natural language processing by introducing the concept of Task-Oriented Asking and providing a framework for its evaluation.",
            "22": "The Fact-Level Masking method and the creation of the FLM-HotpotQA dataset are innovative and valuable additions.",
            "23": "However, the scope of the evaluation could be broadened, and the limitations of the oracle model and primary model biases need to be addressed in future work.",
            "24": "The insights provided by the analysis of failure modes and the comparison with human performance are particularly valuable for guiding future research in this area."
        },
        "MiB8m7PqbC": {
            "0": "It is interesting to formulate the follow-up question generation task as a problem of task-oriented asking (TOA).",
            "1": "For the TOA task, this paper presents a fact-level masking method to generate TOA dataset, which can be used for training and evaluation.",
            "2": "Multiple zero-shot large language models are evaluated on the proposed FLM dataset and show that they struggle to ask a proper question for the given task.",
            "3": "A significant point of contention revolves around the definition of the term \"task-oriented.\"",
            "4": "Utilizing a question from HotpotQA as the primary task, denoted as $t$, may not be the most reasonable approach.",
            "5": "The task itself could encompass a repetitive and abstract procedure, and task-oriented questions primarily serve the purpose of populating missing values.",
            "6": "One noteworthy contribution of this paper pertains to the introduction of the TOA dataset.",
            "7": "However, the exposition of this dataset remains somewhat unclear.",
            "8": "Key details, such as the dataset's fields and the methodology employed for its measurements, are either absent or challenging to comprehend.",
            "9": "In section 4.2, the pipeline directly employs established models like GPT4 and other large language models as M2, with Flan-T5-base serving as $\\Phi$.",
            "10": "Given that all these models are readily available and widely known, it raises the question of what unique contribution this paper brings to the field.",
            "11": "Merely establishing a dataset and making what could be construed as a minor modification to an existing one may not be considered particularly groundbreaking for a conference like ICLR.",
            "12": "Furthermore, solely assessing this dataset using existing models might fall short of the standards expected for this conference."
        },
        "ywzIWsNQbb": {
            "0": "The task of TOA is an interesting testbed, especially for LLMs.",
            "1": "Although similar to slot-filling for dialogue systems, the proposed TOA task is more flexible and is potentially better suited for LLMs.",
            "2": "The introduced FLM-HotpotQA dataset is useful for evaluating future LLM performance in question generation.",
            "3": "Extensive ablation studies on the primary QA model and the oracle model provide insights for understanding the task.",
            "4": "Key implementation details are missing in the paper, which harms the understanding of the paper.",
            "5": "It is unclear how exactly different Oracle models select facts based on the questions, and what the prompt for the QA model is.",
            "6": "Although the idea of TOA is interesting, the implementation of the task in this paper does not fully reflect its motivation, which is \"(TOA) models’ ability to ask for missing information\".",
            "7": "For example, the involvement of the oracle model in this task makes the actual evaluation of the TOA model tricky.",
            "8": "According to Figure 7, the tested oracle models have a low MFRR even with human-generated questions.",
            "9": "The low performance of the oracle model is a serious bottleneck in the evaluation, which makes the model potentially impossible to achieve a high performance on the task.",
            "10": "The lack of Oracle models' implementation details along with the low performance of the Oracle models harm the soundness of this paper.",
            "11": "The results in Figure 6 are doubtful.",
            "12": "With supporting facts, the task for the primary model is degenerated to the original HotpotQA.",
            "13": "Seeing FLAN-T5-Base on par with GPT-3.5-Turbo and FLAN-T5-Large outperforming GPT-4 on HotpotQA is surprising.",
            "14": "The lack of primary models' implementation details along with the surprising results in Figure 6 also harm the soundness of this paper.",
            "15": "The related work section could be updated.",
            "16": "The area of Asking Clarification Questions (ACQ) is active in dialogue systems research.",
            "17": "Please refer to [1].",
            "18": "[1] Rahmani, Hossein A., et al.",
            "19": "\"A Survey on Asking Clarification Questions Datasets in Conversational Systems.\"",
            "20": "arXiv preprint arXiv:2305.15933 (2023)."
        },
        "t4hev3A975": {
            "0": "This work addresses an (understudied) problem of asking follow-up/clarification questions, and contributes a novel framework for addressing the problem, within the realm of QA.",
            "1": "Overall, this paper is also quite well-written, and very clearly lays out its method.",
            "2": "The paper very comprehensively articulates its scope and limitations, making note of any caveats where they may arise, and also makes the right comparisons and baselines to try and address any limitations in the evaluation (e.g.",
            "3": "comparing against the repeater baseline to address limitations laid out in bullets on page 5).",
            "4": "It seems like the authors have worked hard to address any spurious correlation or potential biases that may affect the evaluation.",
            "5": "Consequently, the results are quite convincing.",
            "6": "The paper also provided very comprehensive ablation studies in section 5, and provided concrete examples of failure cases.",
            "7": "The paper explores its TOA method with lots of different models, including open-source models.",
            "8": "The current setup in the paper seems somewhat dataset-specific, and the evaluation is also currently only focused on a single task & dataset.",
            "9": "In the introduction, the paper frames TOA as a more generic technique for all tasks (e.g.",
            "10": "intro makes reference to legal tasks and states “we propose task-oriented asking (TOA) as a benchmark for language model question generation”, without narrowing the scope to just QA tasks with a single missing supporting fact.)",
            "11": "Thus, either the claims in the introduction need to be tempered, or the current evaluation scheme should be broadened to give a sense of how well TOA generalizes to other tasks.",
            "12": "More specifically, the answer model is currently restricted to picking between a limited set of facts (one of them being the masked supporting fact necessary to answer the original question, and the others being distractor facts), which likely overestimates performance compared to an answering \n    1.",
            "13": "While understandably the authors were trying to simulate an oracle answer model, note that this does not necessarily tell us how well the question-asking model is, and does necessarily simulate a “perfect answerer”.",
            "14": "In particular, the task for the question-asking model shifts from “ask a question seeking, specifically, the missing information” to “ask a question that privileges the masked supporting fact as an answer over any of the provided distractor facts”.",
            "15": "In the latter case, we don’t even need to guarantee that the question being asked is comprehensible, or would organically be answered with the supporting fact, but simply that the supporting fact seems like a marginally better response than any of the distractors.",
            "16": "For example, it could be the case that the input itself carries information about what information is missing and the question was unnecessary, or the question isn’t actually asking for the missing information / only asks for part of the missing information but the masked fact is still the most pertinent answer.",
            "17": "While comparing against the Repeater baseline takes care of some of these concerns, this still does not take away from the fact that there are factors that aren’t explored due to the answer setup.",
            "18": "For example, how comprehensible & easy to answer are questions?",
            "19": "Would they naturally lead to an answer that contains the correct fact, supposing when didn’t have a constrained set of possible answers?",
            "20": "Answering these questions are important if we’d want to generalize beyond the setup here, as generally we do not have access to a set of possible answers.",
            "21": "Indeed, one of the key challenging considerations of asking questions is that the model needs to ask not just a question that recovers the missing information — but recovers the minimal unit of information that is necessary to perform the end-task.",
            "22": "Otherwise, we can imagine a question like “tell me everything you know” being maximally useful under the recovery metric for all inputs.",
            "23": "The current task setup is unable to measure whether the questions \n\n3.",
            "24": "Related to the above, the paper claims that TOA is able to “generate plausible-sounding questions”.",
            "25": "It would be great to get empirical concrete evidence of this — perhaps through some evaluation of the wellformedness of the resulting questions.",
            "26": "There were some places in the description of the evaluation framework that were unclear / missing important details (see questions).",
            "27": "Can you report error bars for Figure 4?"
        }
    },
    "lwtaEhDx9x": {
        "llm_review": {
            "0": "**Timely and Relevant Topic**: The paper addresses a critical issue in the application of Large Language Models (LLMs) to tabular data, specifically focusing on data contamination and memorization.",
            "1": "This is a timely and relevant topic given the increasing use of LLMs in various domains.",
            "2": "**Comprehensive Testing Framework**: The authors introduce a variety of techniques to assess the degrees of contamination and memorization in LLMs.",
            "3": "These include statistical tests for conditional distribution modeling and four specific tests to identify memorization.",
            "4": "This comprehensive approach provides a robust framework for evaluating LLMs.",
            "5": "**Clear Distinction Between Concepts**: The paper makes a clear distinction between knowledge, learning, and memorization.",
            "6": "This helps in understanding the different ways in which LLMs can interact with training data and the implications for downstream tasks.",
            "7": "**Empirical Validation**: The authors validate their proposed tests on multiple datasets, including both publicly available and less well-known datasets.",
            "8": "This empirical validation demonstrates the efficacy of their methods and provides concrete evidence of the issues discussed.",
            "9": "**Open-Source Tool**: The release of an open-source tool to perform the proposed tests is a significant contribution.",
            "10": "It facilitates future research and practical applications by providing a ready-to-use resource for assessing data contamination and memorization in LLMs.",
            "11": "**Insightful Findings**: The paper provides several insightful findings, such as the observation that LLMs often memorize the initial rows of datasets more than random rows.",
            "12": "These findings contribute to a deeper understanding of how LLMs process and memorize data.",
            "13": "**Discussion on Downstream Implications**: The discussion on the implications of learning and memorization for downstream prediction tasks is valuable.",
            "14": "It highlights the potential risks of using LLMs without proper checks for data contamination and memorization.",
            "15": "#### Weaknesses:\n\n1.",
            "16": "**Limited Access to Training Data**: A significant limitation of the study is the lack of access to the training data of the LLMs (GPT-3.5 and GPT-4).",
            "17": "This restricts the ability to draw definitive conclusions about the extent and nature of memorization.",
            "18": "The authors acknowledge this limitation, but it remains a significant constraint.",
            "19": "**Assumptions in Tests**: Some of the proposed tests rely on specific assumptions, such as the existence of a canonical CSV file or the presence of highly unique feature values.",
            "20": "These assumptions may not hold for all datasets, potentially limiting the generalizability of the tests.",
            "21": "**Dependence on Current LLM Capabilities**: The experiments and findings are tied to the capabilities of current LLMs (GPT-3.5 and GPT-4).",
            "22": "As LLMs evolve, the results and conclusions may change.",
            "23": "This dependence on current technology limits the long-term applicability of the findings.",
            "24": "**Potential for Overfitting in Tests**: The use of zero-knowledge prompting and few-shot learning in the tests could potentially lead to overfitting to the specific datasets used in the study.",
            "25": "This might affect the generalizability of the results to other datasets and LLMs.",
            "26": "**Lack of Detailed Analysis on Negative Results**: While the paper provides detailed analysis on positive results (evidence of memorization), it lacks a thorough discussion on negative results (absence of memorization).",
            "27": "Understanding why certain datasets do not show evidence of memorization could provide additional insights.",
            "28": "**Complexity of Statistical Tests**: Some of the statistical tests proposed for conditional distribution modeling and memorization detection are complex and may require significant expertise to implement and interpret.",
            "29": "This could limit their accessibility to a broader audience.",
            "30": "**Limited Discussion on Mitigation Strategies**: The paper focuses on detecting memorization and contamination but provides limited discussion on strategies to mitigate these issues.",
            "31": "Including recommendations for preventing or reducing memorization in LLMs would enhance the practical value of the study.",
            "32": "#### Conclusion:\n\nOverall, the paper makes a significant contribution to the understanding of data contamination and memorization in LLMs, particularly in the context of tabular data.",
            "33": "The comprehensive testing framework, empirical validation, and release of an open-source tool are notable strengths.",
            "34": "However, the study is limited by the lack of access to training data, reliance on specific assumptions, and dependence on current LLM capabilities.",
            "35": "Addressing these limitations and providing more detailed analysis on negative results and mitigation strategies would further strengthen the paper."
        },
        "WzTKlCpFu8": {
            "0": "- They propose several methods to test whether the language models were trained with those datasets.",
            "1": "- The idea of comparing the distribution generated by the model and the distribution in the datasets (Sec 3.3) is novel and interesting.",
            "2": "- The claim that they show that some language models are pretrained with some tabular datasets is somewhat convincing and interesting.",
            "3": "Main concerns:\n\n1.",
            "4": "This work does not provide strong evidence supporting the validity of their proposed approaches.",
            "5": "I think one main takeaway of this paper is that some models are pretrained with some datasets, so their performance is not indicative of.",
            "6": "But this takeaway is based on the validity of their proposed approaches.",
            "7": "I think the authors need to address this more.",
            "8": "I can’t understand the purpose of having these many different testing approaches, probably because the structure of this paper is hard to follow.",
            "9": "The authors propose many approaches, some of them are interesting, but they do not provide a holistic interpretation of the results from these many approaches.",
            "10": "The descriptions of the testing approaches are vague and not rigorous.",
            "11": "Writing down the testing approaches with simple math equations could help.",
            "12": "For example, in page 6, I can’t understand what it means by “we can perform a t-test between the similarity of model completions with actual vs. random rows.",
            "13": "Knowledge, learning, memorization should be defined more specifically.",
            "14": "The authors (claims to) show data contamination exists in some datasets.",
            "15": "However, I am not sure whether those datasets are commonly used to benchmark the language model.",
            "16": "Thus I am not sure whether the findings are important (if they are valid).",
            "17": "More specific (writing) issues:\n\n\n2.",
            "18": "The second paragraph in Sec 3.2: Here 4 possible causes are provided, but I don’t see how they are discussed in the following experimental designs.",
            "19": "The last sentence in page 5: “Empirically, we find … a very intuitive test …”.",
            "20": "I don’t understand how your empirical results support this.",
            "21": "Page 8: “It might be that this learning task is relatively simple, that our memorization test are not sensitive enough”.",
            "22": "I can’t understand why it is the case.",
            "23": "Grammar:\n\nThere are many grammar errors.",
            "24": "I suggest that the authors do some proofreading."
        },
        "qZSAjmfZao": {
            "0": "The paper presents several novel methods to evaluate memorization of tabular data in LLMs, and evaluation results on a series of datasets correlate well with the publication time and availability of the data, confirming the effectiveness of the proposed methods in identifying memorization.",
            "1": "The different evaluation methods also complement each other, elucidating the different aspects of memorization of tabular data.",
            "2": "The paper is overall well-written and very easy to read, the visualizations present the main findings nicely.",
            "3": "The contamination and memorization of training data by LLMs is a critical issue.",
            "4": "The findings provoke essential discussions on the evaluation of LLMs on tabular data, which is likely to become more relevant given the rising usage of LLMs in diverse tasks.",
            "5": "The introduced tools and code potentially provide easy and accessible ways to evaluate memorization of tabular data, reusable in future research.",
            "6": "Some important details in the experiment design may be missing or incomplete: \n\n* Evaluation metric for knowledge, learning, and memorization is unclear.",
            "7": "In Table 1, the evaluation results are categorized into three categories (✓,X, and ?",
            "8": "), but the metric for the categorization is not given.",
            "9": "It is probably a better idea to show the raw values (e.g., accuracy) than using categories to give the reader a direct comprehension of the degree of memorization on each dataset.",
            "10": "Notations such as \"✓\" could be misleading as it may be confused as perfect memorization.",
            "11": "The appendix gives raw accuracies for Row Completion Test, Feature Completion Test, and First Token Test, why raw accuracies for Feature Names, Feature Values, and Header Test are not provided as well?",
            "12": "* The differentiation between learning and memorization is not clear: the authors use feature distributions to examine learning, but memorization can also result in a high similarity of the generated data's feature distributions to the original data.",
            "13": "Learning is defined as the model's ability to perform tasks in the current paper, but task performance is heavily affected by memorization and may fail to reflect true learning.",
            "14": "Even with considerable discussion, the paper does not seem to arrive at a conclusion about how learning can be clearly assessed.",
            "15": "* Evalulation of memorization needs to take the nature of data fields into consideration.",
            "16": "Some data fields in the tabular dataset are considerably harder to memorize verbatim or to predict exactly (such as measurement values) than other simpler fields (categorical values such as sex, occupation, nation).",
            "17": "For numerical values, it may be more reasonable to measure the relative distance from the predicted value to the true value than using exact match (perhaps in a similar vein as the \"first token test\" in the paper but more principled).",
            "18": "Under the current evaluation protocol, it is likely that datasets containing more easy fields are more likely to be judged as memorized.",
            "19": "To compare the degree of memorization across datasets, it seems necessary to perform some kind of \"normalization\" before measuring memorization, for example, selecting a fixed number of categorical and numerical fields from each dataset.",
            "20": "Results in Table 3 could suffer from this limitation as well.",
            "21": "* Evaluation of memorization needs to be evaluated separately for the training and test split.",
            "22": "It may be possible that the training sets are memorized more than the test set due to more exposure on the internet.",
            "23": "Memorizing the test set definitely compromises evaluation, but memorizing the training set may not always compromise evaluation.",
            "24": "* Connection between memorization and downstream performance is not reliably established.",
            "25": "The main observation from Section 5 is that for datasets with a high degree of memorization, LLM performs better than decision tree and logistic regression, while for datasets with a low degree of memorization the reverse is true.",
            "26": "Such observation alone may not be sufficient to conclude that memorization compromises evaluation, because there is no evidence that LLM cannot perform better than decision tree and logistic regression under no memorization.",
            "27": "It would be much better to solicit new test sets for the tasks to use in evaluation, which can be used to show exactly how much performance gap is caused by memorization.",
            "28": "In case finding new examples is difficult, perhaps one can modify the values of the fields known to be irrelevant to the label in existing examples, and that may break the reliance on memorization in LLMs.",
            "29": "Some main conclusions of the paper are compromised because of the above limitations:\n\n* \"We emphasize the importance of verifying data contamination before applying LLMs\": the implication of data contamination is not reliably demonstrated in Section 5.",
            "30": "Also, from the current discussion, it is not very clear how to interpret the test results on knowledge, learning, and memorization together.",
            "31": "For example, if knowledge and learning show positive results and memorization show negative results, should we conclude that there is data contamination or not?",
            "32": "And could the performance on downstream tasks be trusted in this situation?",
            "33": "It can be argued that knowledge and learning will not directly compromise evaluation on downstream tasks, so there may not be as much need to evaluate them compared to memorization.",
            "34": "I would suggest allocating more space in the paper for extended experiments and discussions on memorization, which is the ultimate reason why people are concerned about data contamination.",
            "35": "* \"... and propose practical methods to do so\": the proposed method verifies memorization of data, but does not give a definite metric to judge when memorization is severe enough to compromise evaluation.",
            "36": "* \"We offer a principled distinction between learning and memorization in LLMs\": the distinction is not given clearly enough.",
            "37": "One can tell whether there is memorization from the proposed test, but it is not clear how to tell whether learning exists (especially when memorization is present)."
        },
        "MAjYHtyiHz": {
            "0": "* I believe this work (a) raises an important overlooked question, (b) addresses it, (c) by proposing an original technique.",
            "1": "I particularly like the zero-shot prompting technique that allows sampling from a dataset w/o leaking information in the prompt.",
            "2": "* The paper disentangles a few levels of training data contamination and comprehensively tests for those.",
            "3": "* The paper showcases the potential impact of the contamination on the downstream comparisons, hence proving a strong motivation to the work.",
            "4": "* The text and the story are clear.",
            "5": "* The code is made public.",
            "6": "* The paper only studies ChatGPT-3.5 and 4.",
            "7": "Those are very likely to be strongly correlated in terms of the data used, which harms the representativeness of the study.",
            "8": "* As there is no ground-truth knowledge on whether a particular dataset was seen at training, it is impossible to strictly verify the findings.",
            "9": "Including an LM trained on a known dataset would allow us to verify the used methods.",
            "10": "* Another related issue: the work is mostly relevant when we consider closed-data models w/ a black-box API access.",
            "11": "This scenario reflects a dominant situation at the moment, but it is not given that this will not/should not change.",
            "12": "Minor:\n* Table 1 is mentioned on page 3, yet only appears on page 6.",
            "13": "Is there a way to bring it closer?",
            "14": "* Would it make sense to consider swapping sections 5 and 6?",
            "15": "I feel the S6 is more connected to the S3-4 than S5."
        },
        "2QzbpIB3Le": {
            "0": "* LLMs are pervasive currently, and it's important to understand and control their behavior.",
            "1": "The authors emphasize the importance of verifying data contamination before applying LLM.",
            "2": "* Their setup based on tabular data is an elegant way to test “knowledge”, “learning”, and “memorization” of an LLM.",
            "3": "* Moreover, they assume only blackbox API access, without assuming access to the probability distributioin over tokens or the ability to re-train the model.",
            "4": "* Release of an open-source tool that can perform various tests for memorization.",
            "5": "* My main point of criticism is that the paper feels a bit like a collection of remarkable examples and the analysis largely confirms known concerns/behavior of LLMs.",
            "6": "* \"we also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim\": It's not clear to me what this statement means.",
            "7": "See also Question 1 below.",
            "8": "* Just echoing the authors: \"A limitation of our work is that we do not have access to the training data of GPT-3.5 and GPT4.\"",
            "9": "I.e., the interpretation of results often remains speculative.",
            "10": "* Figure 3: Why are some results with gpt-3.5 and some with gpt-4?",
            "11": "* Typo: \"two publicly available dataset that are highly memorized\"\n* Typo: \"UCI repository athttps://\""
        }
    },
    "iTrd5xyHLP": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method, LLMatic, which combines the code-generating capabilities of Large Language Models (LLMs) with Quality-Diversity (QD) optimization for Neural Architecture Search (NAS).",
            "1": "This is a creative and promising approach that leverages the strengths of both LLMs and QD algorithms.",
            "2": "**Empirical Validation**: The authors provide empirical evidence of the effectiveness of LLMatic on the CIFAR-10 dataset and the NAS-bench-201 benchmark.",
            "3": "The results show that LLMatic can produce competitive networks with just 2,000 evaluations, which is a significant reduction compared to traditional NAS methods.",
            "4": "**Comprehensive Ablation Study**: The paper includes a thorough ablation study that demonstrates the necessity of each component of LLMatic.",
            "5": "This adds credibility to the proposed method and helps in understanding the contribution of each part of the algorithm.",
            "6": "**Diversity in Solutions**: By using QD optimization, LLMatic is able to generate a diverse set of high-performing neural network architectures.",
            "7": "This is beneficial for understanding trade-offs between different architectures and for applications requiring a range of solutions.",
            "8": "**Open Source Code**: The authors have made their code available on GitHub, which promotes transparency and allows other researchers to reproduce and build upon their work.",
            "9": "#### Weaknesses:\n\n1.",
            "10": "**Limited Benchmarking**: While the paper demonstrates the effectiveness of LLMatic on CIFAR-10 and NAS-bench-201, it would be more convincing if the method were tested on a wider range of datasets and tasks, including more complex datasets and different domains such as natural language processing.",
            "11": "**Comparison with State-of-the-Art**: The paper compares LLMatic with a few existing methods, but it lacks a comprehensive comparison with a broader set of state-of-the-art NAS algorithms.",
            "12": "Including more comparisons would provide a clearer picture of where LLMatic stands in the current landscape of NAS research.",
            "13": "**Scalability and Efficiency**: Although the paper claims computational efficiency, it does not provide detailed information on the computational resources required for LLMatic compared to other NAS methods.",
            "14": "A more in-depth analysis of the computational cost and scalability of the approach would be beneficial.",
            "15": "**LLM Dependency**: The method relies heavily on the capabilities of the LLM used (CodeGen-6.1B).",
            "16": "It would be interesting to see how the performance of LLMatic varies with different LLMs, especially larger models, and whether the improvements justify the increased computational cost.",
            "17": "**Clarity and Presentation**: The paper is dense with technical details, which might be overwhelming for readers not familiar with NAS or QD optimization.",
            "18": "Improving the clarity of the presentation, perhaps with more visual aids and a clearer explanation of the key concepts, would make the paper more accessible.",
            "19": "#### Conclusion:\n\nOverall, the paper presents a novel and promising approach to Neural Architecture Search by combining Large Language Models with Quality-Diversity optimization.",
            "20": "The empirical results are encouraging, and the comprehensive ablation study adds robustness to the findings.",
            "21": "However, the paper would benefit from a broader benchmarking, more detailed comparisons with state-of-the-art methods, and a clearer presentation of the computational efficiency and scalability of the approach.",
            "22": "Despite these weaknesses, LLMatic represents a significant step forward in the field of NAS and opens up new avenues for future research."
        },
        "4XwBmmlHEd": {
            "0": "After reading the manuscript, I summarize the below strengths.",
            "1": "- The topic of the paper is new and interesting.",
            "2": "Leveraging generative models may produce impacts into the NAS field.",
            "3": "- The designed algorithm makes sense that should produce some high-performing DNNs.",
            "4": "Meanwhile, I have following concerns.",
            "5": "- The presentation of the paper is not satisfactory.",
            "6": "Especially the pseudocode, which lacks proper mathematical annotations.",
            "7": "Meanwhile, there exist a lot of typos, e.g., 'ta simple' on page 4.",
            "8": "The legends in Figures are invisible, etc.",
            "9": "- Again the presentation, the key components in the algorithm such as mutation operators, crossover operators and temperature mutation lacks proper description and explanations.",
            "10": "It makes the algorithm unclear.",
            "11": "- This paper looks more like an investigation paper to explore how to use LLM on generating new DNNs, yet lacks sufficiently novel algorithm to guide LLMs generating DNNs of higher fidelity.",
            "12": "The rank on ImageNet is significantly lower than CIFARs, which increases my concern regarding the effectiveness of the proposed algorithms on general tasks."
        },
        "vFMe6YyWJY": {
            "0": "- I like the idea of carrying out NAS directly on the code-level instead of an abstract representation such as a DAG, as it makes the search flexible.",
            "1": "This also removes the need for a compiler from the abstract model description to the code which can save time and hence lower the barrier to actually use NAS.",
            "2": "Joint NAS and HPO may also be expressed in a very natural way in this paradigm.",
            "3": "- Using quality diversity in this paradigm makes sense to me, not just for diverse hardware requirements, but also for good ensembling.",
            "4": "- The ablation experiment nicely illustrates the contribution of each component.",
            "5": "- Could you run additional experiments on Nas-Bench-101?",
            "6": "As the search space is constrained in a different way than 201, I would be interested to see whether your approach can remain within the search space at all times.",
            "7": "- Could you provide optimization trajectories for Nas-Bench-201 along with a baseline such as random search?",
            "8": "I am curious how your approach compares with more traditional approaches in terms of any-time performance.",
            "9": "- Overall the visual quality of the paper may be improved.",
            "10": "In Figures 2 and 3 the white space around each Figure should reduced.",
            "11": "Figure 4 should be changed to a jpg or contain a reduced number of points because it causes rendering issues.",
            "12": "Also please increase the fontsize to be at least footnotesize for all figures.",
            "13": "- I find Algorithm 1 to be hard to read.",
            "14": "As you don't define each function you use in the pseudo code, you could consider writing short sentences instead."
        },
        "z1rc3MMmXc": {
            "0": "- The use of large models' knowledge for network structure generation provides an example of applying large models in practice.",
            "1": "- While utilizing the code generation capability of large models for network structure generation is a good application, previous works such as GENIUS have already proposed very similar methods and demonstrated prompts.",
            "2": "Therefore, the novelty and contribution of this paper appear insufficient.",
            "3": "- The experimental description is not detailed, and the analysis is not sufficiently deep.",
            "4": "For example, Table 1 does not report the search cost of this paper compared to related works.",
            "5": "- There are relatively few experimental comparisons, and the comparison with related works is not comprehensive enough.",
            "6": "- After reading this paper, I did not get much insight.",
            "7": "Importantly, the assistance of prior knowledge from LLMs in network structure design remains unclear."
        },
        "nxh59jW2zv": {
            "0": "- The application of code-generating LLMs effectively for NAS is very novel.",
            "1": "Defining architecture generation as a language/code modelling task is a new way of formulating the neural architecture search problem.",
            "2": "- The presentation of the paper is mostly clear except in some parts (refer to suggestions and questions in the weakness and questions section)\n- Evaluation on the NB201 dataset and CIFAR10 dataset fairly exhaustive and well ablated.",
            "3": "- The authors release their code and additional details on the prompts used - Evaluation is limited: Currently NAS focuses a lot of transformer-based spaces [1], mobilenet-spaces [2] which are more practical and realistic compared to cell-based spaces.",
            "4": "A lot of thee approaches release a surrogate predictor or the supernet itself, to save training times ie the architecture training part in algorithm 1.",
            "5": "I recommend the authors evaluate the method on these search spaces too.",
            "6": "This is also in my opinion very important to study how llmatic scales across larger architecture definitions (code) and datasets eg: ImageNet.",
            "7": "- Comparison to black box approaches and generative approaches: Since llmatic requires training a lot of architectures (or querying a benchmark multiple times), its search time is more comparable to black-box approaches instead of approaches like lambda-darts.",
            "8": "It would be great to add other black-box methods (in addition to random search) to Table 1.",
            "9": "Furthermore since the work very much falls in the line of generative NAS a comparison with DiffusionNAG [3] would also be great.",
            "10": "- Clarity: In the current version of the paper in figure 4, I couldn't see the green points referring to llmatic architectures, am I missing something?",
            "11": "- Minor : Page 4 2nd paragraph \"In the first generation , ta simple neural network with one convolutional and one fully connected\nlayer initiates the evolution\" -> \"In the first generation , take simple neural network with one convolutional and one fully connected\nlayer initiates the evolution\"\n\n[1] Chen, M., Peng, H., Fu, J. and Ling, H., 2021.",
            "12": "Autoformer: Searching transformers for visual recognition.",
            "13": "In Proceedings of the IEEE/CVF international conference on computer vision (pp.",
            "14": "12270-12280).",
            "15": "[2] Cai, H., Gan, C., Wang, T., Zhang, Z. and Han, S., 2019.",
            "16": "Once-for-all: Train one network and specialize it for efficient deployment.",
            "17": "arXiv preprint arXiv:1908.09791.",
            "18": "[3] An, S., Lee, H., Jo, J., Lee, S. and Hwang, S.J., 2023.",
            "19": "DiffusionNAG: Task-guided Neural Architecture Generation with Diffusion Models.",
            "20": "arXiv preprint arXiv:2305.16943."
        },
        "fbIQrEiv6s": {
            "0": "- the idea of employing LLMs for NAS is novel and aligns with the emerging trend of leveraging these models in diverse fields, considering that LLMs have been applied in domains with structured data formats such as drug design and materials science, - LLMatic's performance heavily depends on the LLM's prior exposure to relevant coding patterns and neural network architectures during its training.",
            "1": "This dependence could be a potential weakness, as the model might not generalize well to novel or highly specialized architectural search spaces.",
            "2": "- The paper focuses on CIFAR-10 and NAS-bench-201 for performance evaluation.",
            "3": "While these are standard benchmarks, the scope of testing could be broadened to include more diverse and challenging datasets.",
            "4": "This would provide a more comprehensive understanding of LLMatic's capabilities and potential limitations, especially in real-world scenarios or more complex tasks.",
            "5": "- The paper could benefit from ablation studies to understand the contribution of each component of LLMatic to its overall performance.",
            "6": "- The writing should improve."
        }
    },
    "gCjeBKuDlc": {
        "llm_review": {
            "0": "**Clear Problem Definition and Categorization**:\n   - The paper effectively categorizes the problem of Visual Commonsense Reasoning (VCR) into two sub-problems: Visual Commonsense Understanding (VCU) and Visual Commonsense Inference (VCI).",
            "1": "This categorization helps in understanding the distinct challenges associated with each type of problem and provides a structured approach to tackle them.",
            "2": "**Innovative Collaborative Approach**:\n   - The proposed ViCor framework leverages the strengths of both Vision-and-Language Models (VLMs) and Large Language Models (LLMs).",
            "3": "The idea of using LLMs to guide VLMs in gathering relevant visual elements when uncertain about their reasoning is innovative and addresses the issue of passive perception in VLMs.",
            "4": "**Comprehensive Evaluation**:\n   - The paper evaluates the proposed framework on two benchmark datasets, VCR and A-OKVQA, and demonstrates that ViCor outperforms other methods that do not require in-domain supervised fine-tuning.",
            "5": "This provides strong evidence of the effectiveness of the proposed approach.",
            "6": "**Detailed Methodology**:\n   - The paper provides a detailed explanation of the ViCor framework, including the roles of LLMs as problem classifiers, VLM commanders, and visual commonsense reasoners.",
            "7": "The step-by-step process of how the framework operates is clearly outlined, making it easier to understand and replicate.",
            "8": "**Qualitative Examples**:\n   - The inclusion of qualitative examples helps in illustrating how the proposed framework works in practice.",
            "9": "These examples provide insights into the reasoning process and the effectiveness of the collaborative approach between LLMs and VLMs.",
            "10": "#### Weaknesses\n\n1.",
            "11": "**Dependence on Pre-trained Models**:\n   - The framework heavily relies on pre-trained models (both VLMs and LLMs).",
            "12": "While this allows for zero-shot or few-shot learning, it also means that the performance is limited by the capabilities and pre-training data of these models.",
            "13": "The paper does not explore the potential benefits of fine-tuning these models on specific VCR tasks.",
            "14": "**Limited Comparison with Supervised Methods**:\n   - Although the paper compares ViCor with other methods that do not require supervised fine-tuning, it does not provide a detailed comparison with state-of-the-art supervised methods.",
            "15": "This makes it difficult to gauge how close the proposed framework is to the best-performing methods in the field.",
            "16": "**Loss of Visual Details**:\n   - The paper acknowledges that using text as the communication medium between LLMs and VLMs may lead to a loss of visual details.",
            "17": "However, it does not explore alternative communication mediums, such as visual embeddings, which could potentially mitigate this issue.",
            "18": "**Scalability and Efficiency**:\n   - The framework involves multiple steps and interactions between LLMs and VLMs, which could be computationally expensive and time-consuming.",
            "19": "The paper does not provide a detailed analysis of the computational efficiency and scalability of the proposed approach.",
            "20": "**Generalization to Other Tasks**:\n   - While the framework is evaluated on VCR and A-OKVQA datasets, it is not clear how well it would generalize to other vision-and-language tasks.",
            "21": "The paper could benefit from a broader evaluation on a wider range of tasks to demonstrate the versatility of the proposed approach.",
            "22": "#### Conclusion\n\nOverall, the paper presents a novel and well-structured approach to bridging visual understanding and commonsense reasoning using large language models.",
            "23": "The categorization of VCR into VCU and VCI, along with the collaborative approach between LLMs and VLMs, is innovative and shows promising results.",
            "24": "However, the dependence on pre-trained models, limited comparison with supervised methods, and potential issues with scalability and loss of visual details are areas that could be further explored and addressed in future work."
        },
        "gEKSftVrAy": {
            "0": "- The studied problem - visual common sense reasoning, is useful and practical for evaluating large models' reasoning capabilities.",
            "1": "- The finding that the captions cannot be used for answering questions is interesting to know.",
            "2": "- The re-definition of visual common sense reasoning is not convincing at all.",
            "3": "There is a large overlap between the two sub-problems of visual common sense understanding and visual common sense inference.",
            "4": "Moreover, reasoning and understanding are also very close.",
            "5": "- Even with this new definition, the authors still perform their experiments on the VCR dataset, which is extremely confusing.",
            "6": "- There is no explicit definition of VCI.",
            "7": "- In fact, the definition of VCU is no different from that of image text retrieval.",
            "8": "- Fig.4, I think, should be changed with a table rather than drawing a table."
        },
        "q1IfDKSKiV": {
            "0": "This paper provides important insights about the comparative advantages of VLMs and LLMs, and introduces an effective framework where they can collaborate.",
            "1": "Concretely, VLMs are better at recognizing literal visual content.",
            "2": "Their contrastive pretraining has equipped them with strong image-text alignment capabilities.",
            "3": "However, VLMs lack commonsense or world knowledge.",
            "4": "Thus VLMs could be benefit from LLMs providing texts including meaningful visual clues to compute alignment scores.",
            "5": "On the other hand, LLMs posses a wealth of  commonsense and world knowledge, and are better at expanding or decomposing problems.",
            "6": "But LLMs do not have direct access to visual information, thus would require a VLM to act at their commands.",
            "7": "Studies in this paper have attempted to revealed that:\n- VLM as the decision model is suboptimal due to the lack of overall reasoning ability, unless the task is as simple as recognizing the literal visual content.",
            "8": "- LLM as the decision model works only when it partners with a VLM and queries the VLM with **visual-clue**-rich texts rewritten from the original textual question.",
            "9": "- The collaborative paradigm between VLM and LLM mitigates mistakes originated from a) easily overlooked visual clues in the surroundings, b) lack of explicit mentions of relevant visual factors in the question, c) misdirecting objects in the foreground.",
            "10": "- Judging from Table1, the number of examples studied is very small.",
            "11": "It is unclear if the evidence derived from the results is robust.",
            "12": "- No multiple runs across decoding configs (e.g.",
            "13": "temperature, selection of in-context examples).",
            "14": "This limits the robustness and generality of the findings.",
            "15": "- Judging from Table1's LLM section: I'm having a hard time drawing conclusive insights from these results.",
            "16": "LLM+Caption wins in two columns, LLM+Caption+VQA wins in two columns, while LLM+Caption+LLMclue wins in 4 columns.",
            "17": "None of the settings is consistently stronger.",
            "18": "Nor do the results suggest a consistent way to choose settings based on the problem category.",
            "19": "I believe that demonstrating how LLM and VLM can wisely collaborate in reasoning tasks is a direction worth pursuing.",
            "20": "Therefore, I don't question the motivation of this paper.",
            "21": "However, this paper only produced preliminary results on small validation sets and with a single set of decoding configurations.",
            "22": "So, the results might lack generality and comprehensiveness.",
            "23": "More comprehensive experiments across larger datasets and multiple seeds/decoding configurations would significantly strengthen the arguments the authors have sought to put forth."
        },
        "AncuRU2D59": {
            "0": "The proposed method is well-motivated.",
            "1": "Current pre-trained VLMs do not extract visual context based on the input questions.",
            "2": "The proposed method can address this problem.",
            "3": "Existing methods, such as BLIP2, instructBLIP, and mini-GPT4, align the visual context with the LLM inputs embedding instead of input words and achieve better performance.",
            "4": "It is not clear why the proposed method uses words(caption or VQA result) to transfer information from the VLM to LLM.",
            "5": "The paper lacks implementation details of the proposed model and compared methods.",
            "6": "BLIP2 has a different setting to obtain the answer to the original one."
        },
        "Gmzr5WF5sU": {
            "0": "This paper provides strong empirical results when combining proprietary language models such as GPT with vision-language models for solving VCR problems.",
            "1": "I wouldn't want to dismiss the paper as \"combination of multiple proprietary blackboxes\" (although it probably is that) -- the paper does demonstrate that there are novel ways to leverage these tools for solving challenging problems in vision.",
            "2": "The paper is well written and well explained to someone who is already familiar with the advances in this domain.",
            "3": "See Weakness 4 for the flip side.",
            "4": "Experiments could be more exhaustive -- for instance, why not expand the experiments into more VCR datasets such as OKVQA (Marino et al.",
            "5": "), VisualComet (Park et al.",
            "6": "), V2C (Fang et al)?",
            "7": "The pipeline doesn't seem to be specific to VCR and could be used for any VQA dataset (eg.",
            "8": "VQAv2, GQA, CLEVR, etc.)",
            "9": "-- it's not clear whether the proposed method also improve performance on these datasets.",
            "10": "In practice, questions to a real-time system could be of any type (those about commonsense or those about simple perception) -- so it would be important to improve performance on both.",
            "11": "In Table 1, it is unclear how each dataset is divided into two parts VCU and VCI for evaluation.",
            "12": "The paper is well written and well explained to someone who is already familiar with the advances in this domain, but this assumption could have limiting effects on who learns from the paper -- one of the advantages of publishing NLP papers in ICLR is a wider reach to the broad ML community (and a large part of this community does not work on NLP or LLMs)."
        }
    }
}