{
    "0tEed0ZiFX": {
        "llm_review": {
            "0": "**Innovative Joint Modeling Approach**: The paper introduces a novel framework that jointly models VerbNet and PropBank labels as one sequence.",
            "1": "This approach addresses the issue of generating structurally inconsistent label sequences, which is a common problem in multitask learning setups.",
            "2": "**Use of SEMLINK Constraints**: By enforcing SEMLINK constraints during decoding, the authors demonstrate a consistent improvement in overall F1 scores.",
            "3": "This method ensures that the decoded labels do not violate structural constraints, leading to more accurate and reliable results.",
            "4": "**High Performance**: The proposed models achieve state-of-the-art F1 scores on the CoNLL05 benchmark, outperforming previous best models by significant margins.",
            "5": "The improvements are notable both in-domain and out-of-domain, showcasing the robustness of the approach.",
            "6": "**Effective Use of PropBank-only Data**: The paper proposes a constrained marginal model that leverages the large amounts of PropBank-only data.",
            "7": "This semi-supervised learning approach further enhances the model's performance, particularly in out-of-domain generalization.",
            "8": "**Detailed Analysis and Ablation Studies**: The authors provide a thorough analysis of their models, including the impact of SEMLINK constraints during both training and inference.",
            "9": "The ablation studies and statistical significance tests add credibility to their findings.",
            "10": "**Practical Application for VerbNet Completion**: The paper addresses a realistic use case of inferring VerbNet arguments from given PropBank arguments, achieving over 99 F1.",
            "11": "This demonstrates the practical utility of the proposed approach in real-world scenarios.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Reliance on Gold Predicate Labels**: The models assume that gold predicate positions and attributes (VerbNet class and PropBank sense) are given.",
            "14": "This reliance on gold labels may limit the applicability of the models in fully end-to-end SRL systems where such information is not readily available.",
            "15": "**Limited Exploration of Document Context**: The paper does not explore the use of document context, which has been shown to positively impact SRL performance in previous work.",
            "16": "Incorporating document-level information could potentially further improve the model's accuracy.",
            "17": "**Marginal Model's Capacity for Constraints**: While the marginal model effectively handles SEMLINK constraints, it may not be as efficient in dealing with other types of SRL constraints, such as unique core role constraints.",
            "18": "This limitation could affect the model's performance in more complex SRL tasks.",
            "19": "**Potential Trade-offs in Semi-supervised Learning**: The semi-supervised models show mixed results, with some improvements in out-of-domain performance but potential trade-offs in in-domain accuracy.",
            "20": "This indicates that the models may not always use PropBank-only data efficiently.",
            "21": "**Variance in Out-of-domain Performance**: The models exhibit higher variance in F1 scores on the out-of-domain Brown test set.",
            "22": "This suggests that the models may be less stable when generalizing to new domains, which could be a concern for practical deployment.",
            "23": "**Complexity of Input Construction**: The proposed input construction for the completion mode (xCOMP) is less efficient compared to the shared input construction (xWP).",
            "24": "This complexity could impact the scalability of the approach when dealing with large datasets or multiple predicates.",
            "25": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of semantic role labeling by introducing a joint modeling approach that leverages SEMLINK constraints.",
            "26": "The proposed models achieve state-of-the-art performance and demonstrate strong generalization capabilities.",
            "27": "However, there are some limitations related to the reliance on gold predicate labels, the handling of document context, and the efficiency of semi-supervised learning.",
            "28": "Addressing these weaknesses in future work could further enhance the applicability and robustness of the proposed approach."
        },
        "ufHVWDcPw4": {
            "0": "- I think the direction explored in this work is interesting and important for the future development of unifying semantic resources.",
            "1": "- The paper is well-written and easy to follow, and the experiments are well-conducted.",
            "2": "- The experiments only involve joint prediction of PB and VN, where there are abundant joint data and high-coverage mapping resources.",
            "3": "The paper could be much stronger and more interesting if it could be extended to scenarios where there are less such joint resources (like PropBank/VerbNet & FrameNet).",
            "4": "- The performance gaps between different methods seem small.",
            "5": "Though this is not quite surprising giving strong neural models, it would be more interesting to explore scenarios where the proposed methods could be more helpful (such as low-resource cases and less-frequent predicates)."
        },
        "s1sxbnlOPT": {
            "0": "The results are state of the art, the methods sensible and the discussion useful.",
            "1": "The existence of compatible verbnet and PropBank labels is an accident of the history of the field There are not many situations parallel to this, so the result is probably of interest only to a small subset of ACL researchers.",
            "2": "The authors make little attempt to draw general lessons from their work."
        },
        "gpvduTkDqT": {
            "0": "Experiments conducted within this study are comprehensive, accompanied by a thorough analysis.",
            "1": "The proposed approach can be employed to facilitate mutual transformation between SRL data annotated under different standards, which holds significance for the automated construction of annotated datasets.",
            "2": "The writing of this paper requires a little improvement, as certain sections are difficult to comprehend.",
            "3": "The foundational aspect of SEMLINK is inadequately introduced.",
            "4": "For instance, in section 5, readers less familiar with SEMLINK might struggle to grasp concepts like \"SEML(u)\" and the associated inferences.",
            "5": "I believe the authors could enhance understanding by incorporating more illustrative examples.",
            "6": "Additionally, as mentioned by the authors in the Limitation section, the model in this study relies on gold predicate positions and predicate attributes.",
            "7": "This limitation diminishes the practicality of the model in real-world scenarios."
        }
    },
    "dbRZyDxYlL": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method, FuseST, which effectively combines speech and text modalities to improve speech translation.",
            "1": "This approach leverages the complementary strengths of both modalities, addressing the limitations of each.",
            "2": "**Comprehensive Analysis**: The authors provide a thorough analysis of the modality gap between speech and text, identifying three key aspects that impede their integration.",
            "3": "This detailed examination helps in understanding the challenges and the proposed solutions.",
            "4": "**Effective Techniques**: The paper employs multiple techniques for cross-modal alignment, including Cross-modal Contrastive Learning (CCL), Cross-Attentive Regularization (CAR), and Cross-Modal Regularization (CMR).",
            "5": "These techniques are well-explained and contribute to the overall effectiveness of the model.",
            "6": "**Strong Experimental Results**: The experiments conducted on MuST-C, GigaST, and newstest benchmarks demonstrate significant improvements in BLEU scores.",
            "7": "The FuseST model achieves an average of 34.0 BLEU on MuST-C En→De/Es/Fr, outperforming the state-of-the-art by 1.1 BLEU.",
            "8": "**Robustness to ASR Errors**: The model shows robustness to different ASR errors, which is crucial for practical applications.",
            "9": "The empirical study on the impact of ASR transcript quality further validates the model's effectiveness.",
            "10": "**Detailed Ablation Study**: The ablation study provides insights into the contributions of different components of the model, highlighting the importance of each auxiliary training objective.",
            "11": "**Case Studies**: The inclusion of case studies helps in understanding the practical implications of the model and its ability to handle real-world scenarios.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Reliance on ASR System**: Although the model improves speech translation by fusing speech and text, it still relies on an ASR system to transcribe speech into text.",
            "14": "This reliance does not address the issue of high latency in cascade systems, which is a significant limitation.",
            "15": "**Data Requirements**: The model requires labeled data for training, specifically <speech, transcript, target> pairs.",
            "16": "Obtaining such data, especially for low-resource languages, can be challenging and limits the model's applicability.",
            "17": "**Complexity of Implementation**: The proposed model involves multiple components and techniques, making it complex to implement and potentially difficult to reproduce.",
            "18": "This complexity might hinder its adoption in practical applications.",
            "19": "**Performance on High-Resource Languages**: While the model shows improvements on medium-resource languages, the gains on high-resource languages like En→Zh are less pronounced.",
            "20": "This suggests that the model's effectiveness might be limited in scenarios with abundant training data.",
            "21": "**Explicit Guidance Fusion**: The exploration of explicit guidance fusion using Align-Mask did not yield better results, indicating that the method might not be as effective as implicit guidance.",
            "22": "The reliance on external tools like MFA for alignment introduces errors, affecting the overall performance.",
            "23": "**Marginal Gains with Advanced Speech Models**: As speech pre-trained models improve, the marginal gains from fusing speech and text decrease.",
            "24": "This trend suggests that the benefits of the proposed approach might diminish with further advancements in speech pre-trained models.",
            "25": "**Conclusion:**\n\nThe paper presents a significant contribution to the field of speech translation by proposing a novel method that fuses speech and text modalities.",
            "26": "The comprehensive analysis, effective techniques, and strong experimental results highlight the potential of the FuseST model.",
            "27": "However, the reliance on ASR systems, data requirements, and complexity of implementation are notable limitations.",
            "28": "Future research could focus on addressing these limitations and exploring more effective methods for explicit guidance fusion.",
            "29": "Overall, the paper provides valuable insights and a promising direction for improving speech translation."
        },
        "8RCgMDKDnh": {
            "0": "The motivation is clear.",
            "1": "For speech translation, speech contains some signal to help translation but if ASR doesn't capture it then error would propagate to the final translation quality.",
            "2": "(1) The proposed method is complicated.",
            "3": "It combines pretrained audio, text modular with 8 different losses.",
            "4": "This make it hard to conclude the improvement is came from proposed model or just introduce many hyper-parameters to tune.",
            "5": "(2) The comparison missed one direction of fuse speech and text representation for speech translation.",
            "6": "For example, MAESTRO: Matched Speech Text Representations through Modality Matching (https://arxiv.org/abs/2204.03409).",
            "7": "That basically learn a joint encoder can encode both audio and text.",
            "8": "Can the author comment the pros and cons of this direction versus proposed method?",
            "9": "(3) Presentation is hard to follow.",
            "10": "For example, it mentioned all encoder decoder share the same param in section 3.2, but why figure 2 mark some of param shared?",
            "11": "There also lots of acronym in the paper, that make it hard to follow.",
            "12": "(4) It's hard to follow the comparison, since external modular been used.",
            "13": "In table 2, when mark as external speech/text data been used, does it comparable with other cited paper?",
            "14": "Maybe to make at least one data point that comparable, can the author provide their in-house cascade model, instead of cite other paper number?"
        },
        "VKyTgnA6ZG": {
            "0": "A) Experiments and Code: The experimental section is thorough and the paper achieves good results with improvements on machine translation as well.",
            "1": "The paper includes several baselines to compare against.",
            "2": "The code is also released, good job with this!",
            "3": "B) The figures in the paper are intuitive, color-coded and easy-to-understand.",
            "4": "C) The model framework section (3.2) is clearly explained, and nicely builds up to Equation 13 which has 8 loss terms.",
            "5": "D) The paper is also well-scoped in that the aim is clearly defined as speech translation, and the way to improve it is by improving cross-modal alignment.",
            "6": "The paper does not have more or less to offer than what it claims which is a great thing.",
            "7": "A) Writing: The paper might benefit with revisions in writing.",
            "8": "For example, L377 mentions that the \"only difference between our approach and others is the specific method for fusing speech and text\".",
            "9": "Accordingly, the related work section should talk about these works and discuss how the paper is different from them.",
            "10": "In its current state, its mostly a list of papers and their citations.",
            "11": "L096-101 are also unclear, since prompt tags are not introduced before this.",
            "12": "B) The overall framework optimizes for 8 objectives and it is unclear how each help and why they are included.",
            "13": "Although the paper has a lot of content to offer, a more intuitive approach for the inclusion of each regularization term even at the cost of a few experiments, would be beneficial, since a major contribution of the paper is the model.",
            "14": "For example, stating something like: \"analysing previous model outputs or systems reveal certain limitations which can be overcome by these additional objectives ...\".",
            "15": "The complexity of the framework and non-clarity on each objective makes me hesitant on the excitement front, but I find the study to be thorough."
        },
        "eeiprbWMlK": {
            "0": "The strategy is simple and straightforward.",
            "1": "The numerical results show significant improvements over the baseline.",
            "2": "My major concern is the novelty of paper.",
            "3": "Similar ideas on architecture, contrastive learning and cross attention regularization have been proposed by several other papers, e.g.",
            "4": "[1][2].",
            "5": "The design of the model is quite similar to [1] and [2] expect that the shared encoder takes concatenate input instead of separated input.",
            "6": "[1] Unified Speech-Text Pre-training for Speech Translation and Recognition\n[2] WACO: Word-Aligned Contrastive Learning for Speech Translation"
        }
    },
    "F1G7y94K02": {
        "llm_review": {
            "0": "**Comprehensive Analysis**: The paper provides a thorough investigation into how factual associations are stored and retrieved in auto-regressive language models.",
            "1": "The use of multiple methods, including attention knockout and gradient-based analysis, offers a detailed understanding of the internal mechanisms.",
            "2": "**Novel Insights**: The identification of a three-step internal mechanism for attribute extraction is a significant contribution.",
            "3": "The finding that early MLP sublayers play a crucial role in enriching subject representations with attributes is particularly noteworthy.",
            "4": "**Methodological Rigor**: The use of interventions on attention edges to trace information flow is a robust approach.",
            "5": "The experiments are well-designed, with careful consideration of different positions and layers within the models.",
            "6": "**Practical Implications**: The insights gained from this study have practical implications for knowledge localization and model editing.",
            "7": "Understanding how factual associations are stored and retrieved can inform the development of more transparent and editable language models.",
            "8": "**Reproducibility**: The authors have made their code publicly available, which enhances the reproducibility of their experiments and allows other researchers to build on their work.",
            "9": "**Clear Presentation**: The paper is well-written and clearly structured.",
            "10": "The use of figures and tables to illustrate key findings helps in understanding the complex processes being described.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Limited Scope of Models**: The study focuses on two specific models, GPT-2 and GPT-J.",
            "13": "While these are popular models, it would be beneficial to see if the findings generalize to other transformer-based models, such as BERT or T5.",
            "14": "**Dependence on Projections**: The reliance on projecting intermediate representations to the vocabulary space is a limitation.",
            "15": "This method provides an approximation and may not fully capture the nuances of the information encoded in these representations.",
            "16": "**First-Position Bias**: The paper acknowledges a positional bias in the first position of GPT-like models.",
            "17": "While this is addressed to some extent, it remains a confounding factor that could influence the interpretation of the results.",
            "18": "**Complexity of Interventions**: The intervention methods, while effective, are complex and may not be easily replicable by researchers without a deep understanding of the models' internals.",
            "19": "Simplifying these methods or providing more detailed guidelines could enhance accessibility.",
            "20": "**Evaluation Metrics**: The use of attributes rate as an evaluation metric is innovative, but it relies on the construction of candidate sets from Wikipedia.",
            "21": "This process may introduce biases or noise, and alternative metrics could be considered to validate the findings.",
            "22": "**Generalization to Other Tasks**: The study focuses on factual open-domain questions.",
            "23": "It would be interesting to see if the identified mechanisms apply to other types of tasks, such as sentiment analysis or machine translation.",
            "24": "#### Conclusion\n\nOverall, \"Dissecting Recall of Factual Associations in Auto-Regressive Language Models\" is a significant contribution to the field of NLP.",
            "25": "It provides deep insights into the internal workings of transformer-based language models and opens up new avenues for research in model transparency and editing.",
            "26": "While there are some limitations, the strengths of the paper far outweigh them, making it a valuable resource for researchers and practitioners alike."
        },
        "PD7VubKiWD": {
            "0": "Overall the paper is well written and easy to understand.",
            "1": "It presents interesting and useful findings for better understanding how a transformer-based LM predicts for subject-relation query.",
            "2": "For example, they found that early layers are important in propagating information for predicting the attribute.",
            "3": "Another example is the attribute information is aggregated gradually through layers.",
            "4": "These findings could not only help understand the models but also inspire new methods that better aggregate information for subject-relation queries.",
            "5": "The experiments are comprehensive and provide answers for questions at different levels.",
            "6": "The study is only on subject-relation query.",
            "7": "It might not be easy to apply the proposed methods to understand the information flow in other research tasks, where important words in query are not known beforehand (just as the subject and relation word in the subject-relation query task), such as reading comprehension."
        },
        "7wNpNpGd6M": {
            "0": "With the rise of the desire for explainability of LMs works which identify (or go some way towards identifying) important locations are of great potential value.",
            "1": "The paper uses figures to advantage, illustrating complex concepts and results.",
            "2": "This is a fairly technical paper, presumably a stepping stone on an incremental path."
        },
        "Qu2g97TNk9": {
            "0": "The study of internal representations of Transformer-based models is still an evolving domain, which needs more contributions.",
            "1": "Particularly, little is known about how factual predictions are built.",
            "2": "This paper proposes an automatic approximation of the subject-attribute relatedness, namely the attribute rate.",
            "3": "It is a reasonable and novel idea.",
            "4": "The experiments are well designed and the results are well presented.",
            "5": "There are some interesting findings.",
            "6": "Although we should start with simple experiments in order to study a complex system, it is often unclear whether conclusions drawn from simple experiments can extend to general, complicated cases.",
            "7": "It is difficult to tell whether other NLP tasks follow the same pattern.",
            "8": "The authors suggest that \"these findings open new research directions for knowledge localization and model editing\", but it is unclear whether the localization depends on training tasks, or model size, among other things."
        }
    },
    "ytQFU2XsBR": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method for combining two distinct reasoning models, Chain-of-Thought (CoT) and Program-Aided Language Models (PAL), using large language models (LLMs) for dynamic model selection.",
            "1": "This approach leverages the strengths of both models, potentially leading to improved performance in reasoning tasks.",
            "2": "**Theoretical Analysis**: The authors provide a detailed theoretical analysis to validate the feasibility of their method.",
            "3": "This analysis highlights the conditions under which the proposed model selection approach can be effective, offering a solid foundation for the empirical results.",
            "4": "**Empirical Validation**: The method is empirically validated across eight reasoning datasets using multiple LLMs, including Codex, ChatGPT, and GPT-4.",
            "5": "The results demonstrate significant performance improvements, with the method achieving new state-of-the-art accuracies on GSM8K and SVAMP.",
            "6": "**Complementary to Self-Consistency**: The proposed method is shown to be complementary to self-consistency, further enhancing performance while reducing computation costs.",
            "7": "This is a practical advantage, as it makes the method more efficient and scalable.",
            "8": "**Broad Applicability**: The method's applicability is demonstrated not only with proprietary LLMs but also with open-source models like Llama 2.",
            "9": "This broadens the potential use cases and accessibility of the approach.",
            "10": "**Detailed Analysis and Insights**: The paper provides a thorough analysis of the factors influencing the performance improvement, such as the difference between the two models and the success rate of the model selection.",
            "11": "This helps in understanding the underlying mechanisms and potential limitations of the method.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Dependence on LLMs**: The method relies heavily on the in-context learning capabilities of LLMs for model selection.",
            "14": "While this is effective with powerful models like GPT-4, it may not perform as well with less capable models, limiting its applicability in resource-constrained environments.",
            "15": "**Bias in Model Selection**: The paper acknowledges the inherent bias in LLMs, which can affect the model selection process.",
            "16": "The order of options in the multiple-choice format can influence the selection, potentially leading to suboptimal choices.",
            "17": "This bias needs to be addressed to ensure more reliable performance.",
            "18": "**Limited Exploration of Other Domains**: The focus of the paper is primarily on reasoning tasks.",
            "19": "While the method shows promise, its applicability to other domains remains unexplored.",
            "20": "Extending the approach to a wider range of tasks could provide more comprehensive insights into its effectiveness.",
            "21": "**Complexity of Implementation**: The method involves multiple stages, including solution generation and model selection, which may add complexity to the implementation.",
            "22": "Simplifying the process or providing more detailed guidelines for practical deployment could enhance its usability.",
            "23": "**Explanation Quality**: The qualitative analysis of explanations generated by the model selection process reveals that the quality of explanations can vary.",
            "24": "In some cases, the explanations provided by the LLMs are incorrect, which could undermine the reliability of the method.",
            "25": "**Scalability Concerns**: While the method reduces computation costs compared to self-consistency, it still involves generating multiple solutions and performing model selection, which may be computationally intensive.",
            "26": "Further optimization to improve scalability would be beneficial.",
            "27": "#### Conclusion\n\nOverall, the paper presents a compelling approach to improving reasoning performance by dynamically selecting between CoT and PAL using LLMs.",
            "28": "The combination of theoretical analysis and empirical validation strengthens the credibility of the method.",
            "29": "However, addressing the identified weaknesses, such as bias in model selection and exploring applicability to other domains, would further enhance the robustness and versatility of the approach."
        },
        "WVb7J9LT1Q": {
            "0": "- Proposed a simple, intuitive, and effective prompting method.",
            "1": "- The proposed method is complementary to the self-consistency (a prompting technique) and can be combined with it to further improve the performance.",
            "2": "- Achieved new state-of-the-art performance on GSM8K, an arithmetic reasoning task\n- Provided interesting insights:\n    - Even though CoT and PAL have similar performance, the performance difference in each sample is actually high, and there is room for performance improvement through combining them with the model selection.",
            "3": "- The combination of CoT and PAL is a more successful in the model selection than other combinations.",
            "4": "- The three LLMs used tended to prefer the first option in the model selection - Seems to be a lack of explanation about the experimental setup (Question A-C)"
        },
        "8YPqgAftnO": {
            "0": "+ The notion of utilizing large models for selection is innovative and effectively leverages the comprehension capabilities of these models.",
            "1": "+ Thorough theoretical analysis substantiates the method's advantages.",
            "2": "+ The paper is well-written and easily comprehensible.",
            "3": "- The improvements observed on GPT-4 are marginal, with most gains being minimal; it remains to be seen whether the slight benefits from the multiple reasoning in this approach justify the additional complexity compared to traditional single-pass methods(CoT/PAL).",
            "4": "- The authors have not conducted experiments or comprehensive analyses on the extra time and cost overhead introduced by their method.",
            "5": "- While experiments with proficient closed-source models like ChatGPT and CodeX are conducted, it is suggested to supplement the analysis with experiments on open-source models like Llama to assess the method's generalizability."
        },
        "nJY4c13dhB": {
            "0": "+ The paper provides atheoretical analysis to support the feasibility of the proposed model combination.",
            "1": "+ The empirical results are robust, showcasing performance improvements across multiple datasets and with various LLM backbones.",
            "2": "+ The paper is well written and easy to follow.",
            "3": "- The paper primarily utilizes existing models such as CoT and PAL without introducing notable innovations.",
            "4": "Its contribution appears to be an application of known methods rather than presenting a transformative approach or fresh insights.",
            "5": "- The paper heavily leans on existing models without introducing significant novel techniques or modifications.",
            "6": "This might position the paper as offering incremental improvements rather than groundbreaking contributions.",
            "7": "- The paper's reliance on in-context learning and self-evaluation abilities of LLMs might make the selector sensitive to prompts, potentially limiting its broader applicability.",
            "8": "- While the paper acknowledges LLM biases, it doesn't seem to propose any concrete solutions or mitigation strategies"
        },
        "m87rKxgoev": {
            "0": "- The paper presents a comprehensive theoretical analysis that quantifies the error rates of base methods and delineates the conditions under which the proposed methodology enhances performance effectively.",
            "1": "- The paper is articulately written, ensuring clarity and ease of understanding.",
            "2": "Furthermore, the empirical evidence robustly supports the theoretical claims, achieving state-of-the-art results on GSM8K and SVAMP.",
            "3": "- While the paper emphasizes \"model selection\", the experimental design primarily addresses \"prompting method selection\", with both base methods and the selection model leveraging the same LLM backbone.",
            "4": "Consequently, the proposed approach can be perceived as a novel prompting strategy, sequentially generating CoT and PAL results using the backbone LLM.",
            "5": "It would be beneficial to conduct further experiments with genuinely distinct models to truly harness their respective strengths, such as integrating CodeX and ChatGPT with GPT-4.",
            "6": "- The proposed method demonstrates effectiveness, but it necessitates thrice the computational and inferential resources for a single problem.",
            "7": "The \"model selection\" issue as addressed in this paper seems to be a variant of the self-consistency methods, a topic that has been extensively explored.",
            "8": "Prioritizing the selection of prompt strategies based on a specific problem might present a more significant research avenue.",
            "9": "- The motivation behind the paper stems from the notion that CoT and PAL excel in different task types.",
            "10": "However, a thorough analysis and explicit examples highlighting the disparities between these two methods are conspicuously absent.",
            "11": "Furthermore, there's a need for elucidation on why these methods showcase different correct distributions within the experimental section."
        }
    },
    "sCu26OfxxZ": {
        "llm_review": {
            "0": "**Novelty and Innovation:**\n   - The paper introduces a novel negotiation dialogue agent, INA, which is capable of negotiating not only on price but also on other factors such as the addition or removal of items from a deal bundle.",
            "1": "This integrative approach is a significant advancement over existing systems that primarily focus on price negotiation.",
            "2": "**Dataset Creation:**\n   - The creation of the Integrative Negotiation Dataset (IND) is a notable contribution.",
            "3": "The semi-automated data creation method, which combines defining negotiation intents, actions, and intent-action simulation, is innovative and helps in generating high-quality dialogue flows.",
            "4": "**Use of GPT-J:**\n   - Leveraging GPT-J for generating dialogues and incorporating a human-in-the-loop process for post-editing ensures high data quality.",
            "5": "This approach balances automation with human oversight, leading to a robust dataset.",
            "6": "**Reward-Based Training:**\n   - The use of a novel reward function tailored for the negotiation task is a significant strength.",
            "7": "The reward system incentivizes the chatbot to learn effective negotiation strategies that can adapt to various contextual requirements and price proposals.",
            "8": "**Comprehensive Evaluation:**\n   - The paper provides a thorough evaluation of the proposed system using both automatic and human evaluation metrics.",
            "9": "The results demonstrate the effectiveness of the INA in engaging in integrative negotiations and achieving win-win outcomes.",
            "10": "**Detailed Methodology:**\n   - The paper provides a detailed explanation of the methodology, including the dataset creation process, the design of the reward function, and the training process.",
            "11": "This transparency allows for reproducibility and further research in this area.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Scope of Products:**\n   - The dataset focuses on a limited range of electronic items.",
            "14": "While the methodology can be adapted to other products, the current scope may limit the generalizability of the findings.",
            "15": "**Dependency on GPT-J:**\n   - The reliance on GPT-J, which requires significant computational resources, may limit the accessibility of the approach for researchers with limited resources.",
            "16": "Additionally, the context window limitation of GPT-J constrains the prompting mechanism.",
            "17": "**Manual Effort in Data Correction:**\n   - Although the semi-automated data creation method reduces manual effort, the need for human experts to correct and filter the generated dialogues indicates that the process is not fully automated.",
            "18": "This reliance on human intervention may introduce biases and inconsistencies.",
            "19": "**Evaluation Metrics:**\n   - While the paper uses a variety of evaluation metrics, some of the metrics, such as METEOR and BERT Score, may not fully capture the nuances of negotiation dialogues.",
            "20": "More specialized metrics for negotiation effectiveness could provide a deeper understanding of the system's performance.",
            "21": "**Ethical Considerations:**\n   - The paper briefly mentions ethical considerations but does not delve deeply into the potential ethical implications of using negotiation agents in online marketplaces.",
            "22": "A more thorough discussion on how to ensure ethical negotiation practices would strengthen the paper.",
            "23": "**Future Work:**\n   - The paper suggests exploring the role of customer persona in future work but does not provide a detailed roadmap for this exploration.",
            "24": "More concrete suggestions for future research directions would be beneficial.",
            "25": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of negotiation dialogue systems with its integrative approach and reward-based training.",
            "26": "The creation of the IND dataset and the use of GPT-J for dialogue generation are notable contributions.",
            "27": "However, the paper could benefit from addressing the limitations related to the scope of products, dependency on computational resources, and ethical considerations.",
            "28": "Despite these weaknesses, the proposed INA system shows great promise in enhancing negotiation strategies in online marketplaces."
        },
        "povrqjyeVo": {
            "0": "1) The semi-automatic data collection technique is an interesting step and potentially useful in this space where collecting quality data is challenging.",
            "1": "This naturally has pros and cons (I discuss some of them below).",
            "2": "The process followed was also rigorous, involving post-processing and human evaluation to ensure quality.",
            "3": "2) In their approach, the authors defined useful rewards that are all relevant to negotiation and can potentially help future work.",
            "4": "3) The paper consists of a comprehensive evaluation, showing the superiority of the proposed approach on a variety of metrics based on both automatic and human evaluation.",
            "5": "1) The proposed approach is unclear.",
            "6": "RL formulation has not been stated properly.",
            "7": "It is unclear what the state and actions are and whether the rewards are defined at an utterance level or a dialogue level.",
            "8": "I suggest the authors to describe the approach in a more thorough manner.",
            "9": "2) The paper argues for win-win negotiations that “see” a product as a bundle rather than negotiating for a single issue of price.",
            "10": "However, neither the proposed approach nor the evaluation focuses on the other items in the bundle.",
            "11": "The rewards are based only on the price.",
            "12": "In other words, the paper set out with the motivation of handling a scenario with multiple items in a bundle, but the spirit of the approach seems no different than prior work by He et al 2018.",
            "13": "What am I missing here?",
            "14": "3) In addition, the pitch of the paper ignores a body of prior work on multi-issue negotiations that do capture multiple items rather than just one.",
            "15": "These datasets already involve integrative negotiations (along with distributive negotiations).",
            "16": "For instance, work on DealOrNoDeal task (Lewis et al.",
            "17": "2017) and more recent datasets like CaSiNo (Chawla et al.",
            "18": "2021) and JobInterview (Yamaguchi et al.",
            "19": "2021).",
            "20": "These should be discussed in related work.",
            "21": "One difference is that in this paper, the set of issues varies depending on the given context.",
            "22": "Thus, the claims in Lines 126-128 and 189-191 are misleading.",
            "23": "To fix these, I believe either the pitch of the paper should be changed or the experiments should be better designed in a way that captures the multiple items in a bundle (just like modeling work on multi-issue negotiation tasks).",
            "24": "One way to do this can be by defining a reward that captures the other items in the bundle.",
            "25": "4) The paper lacks additional critical analysis, such as comparing the intent accuracy of the final models and analyzing how the model deals with the other items in the bundle - this is important since this relates to how the paper has been motivated.",
            "26": "Hence, I suggest the authors to include this in the paper."
        },
        "XTiwyxsqhb": {
            "0": "This work provides substantial contents including:\n1.",
            "1": "A concrete pathway to collect data\n2.",
            "2": "A detailed training framework containing SFT and RL, as well as the delicate reward design.",
            "3": "Relatively positive experimental results both on automatic and human evaluation.",
            "4": "Figures in this paper should be polished up.",
            "5": "For example, several lines in Figure 2 are actually crooked."
        },
        "tIkVZRTB6b": {
            "0": "Authors present a novel new dataset of reasonable size, containing discussion on integrative negotiation dialogues.",
            "1": "The dialogue contain clarification intents, which is frankly useful in itself as a totally different task worth studying.",
            "2": "3,300 training dialogues is a respectable amount.",
            "3": "For quantitative metrics (METEOR, BERT-Score, WMD, Perplexity, and Length), the proposed INA model outperforms all baselines.",
            "4": "In human evaluation across five qualitative metrics, the INA model also does better than all ARDM and NegTOD baselines.",
            "5": "The quality of the dialogues is still fairly unrealistic.",
            "6": "To highlight a couple of areas:\na) The users don't really dive deep into the description or product details.",
            "7": "(ie.",
            "8": "\"The SnapDragon processor is 2 generations old now, you should lower the price.\")",
            "9": "b) There are no emotional appeals that would occur (ie.",
            "10": "\"Oh, please!",
            "11": "I really need it for my daughter's birthday gift!\")",
            "12": "c) Conversations are all roughly the same length, real life chats can go on for 100+ turns easily (or conversely, will end abruptly).",
            "13": "d) The prices are off by a factor of 100x?",
            "14": "(ie.",
            "15": "Tablet costs $92,800, rather than $928)\n\nThere are no ablations or extra analysis on the rewards used for RL-training.",
            "16": "In general, the modeling is not too novel, relying largely on pre-trained SL followed by RL, which is fairly common for dialogue system training.",
            "17": "This is not a huge deal though if we view this as resource paper, rather than a modeling paper."
        }
    },
    "mTiHLHu3sP": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces GPT-RE, a novel method that leverages task-aware representations and gold label-induced reasoning to enhance the performance of in-context learning (ICL) for relation extraction (RE).",
            "1": "This approach addresses the key shortcomings of existing ICL methods, such as low relevance in demonstration retrieval and lack of explanation in input-label mappings.",
            "2": "**Empirical Validation**: The authors provide comprehensive empirical validation of GPT-RE on four widely-used RE datasets: Semeval, TACRED, SciERC, and ACE05.",
            "3": "The results demonstrate that GPT-RE not only outperforms existing GPT-3 baselines but also achieves state-of-the-art (SOTA) performances on the Semeval and SciERC datasets, and competitive performances on TACRED and ACE05.",
            "4": "**Addressing Overprediction**: The paper highlights a critical issue of large language models (LLMs) in RE, which is the tendency to wrongly classify NULL examples into other predefined labels.",
            "5": "GPT-RE significantly alleviates this issue, showcasing its robustness and reliability in practical applications.",
            "6": "**Detailed Methodology**: The paper provides a detailed explanation of the methodology, including prompt construction, task-aware demonstration retrieval, and gold label-induced reasoning.",
            "7": "This thorough explanation allows for reproducibility and a clear understanding of the proposed approach.",
            "8": "**Ablation Studies**: The authors conduct ablation studies to analyze the impact of different components of GPT-RE, such as task-aware retrieval and reasoning enhancement.",
            "9": "These studies provide valuable insights into the effectiveness of each component and the overall robustness of the proposed method.",
            "10": "**Low-resource Scenario Analysis**: The paper includes an analysis of GPT-RE's performance in low-resource scenarios, demonstrating its effectiveness even with limited training data.",
            "11": "This is particularly important for real-world applications where annotated data may be scarce.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Reasoning Experiments**: While the paper demonstrates the effectiveness of gold label-induced reasoning, the experiments are limited to two datasets (Semeval and TACRED).",
            "14": "It would be beneficial to see the impact of reasoning on the other two datasets (SciERC and ACE05) to provide a more comprehensive evaluation.",
            "15": "**Subset Sampling**: For the TACRED and ACE05 datasets, the authors use a subset of the original test set due to the cost of running the model with GPT-3.",
            "16": "While the subset is designed to maintain the original label distribution, this approach may introduce biases and limit the generalizability of the results.",
            "17": "A more extensive evaluation on the full test sets would strengthen the findings.",
            "18": "**Overprediction Issue Not Fully Solved**: Although GPT-RE significantly alleviates the overprediction issue, it is not completely resolved.",
            "19": "The NULL recall still lags behind fully-supervised baselines, especially on datasets with a high proportion of NULL examples like ACE05.",
            "20": "Further research is needed to fully address this limitation.",
            "21": "**Dependence on Smaller PLMs**: The task-aware retriever in GPT-RE relies on representations generated by smaller pre-trained language models (PLMs) like SimCSE and BERT.",
            "22": "The authors acknowledge that LLMs can generate more robust representations, but due to access limitations, they could not utilize GPT-3's representations.",
            "23": "Future work should explore the potential of using LLM-generated representations for retrieval.",
            "24": "**Cost and Accessibility**: The reliance on GPT-3 for ICL poses challenges in terms of cost and accessibility.",
            "25": "Running experiments with GPT-3 can be expensive, and access to the model may be limited for some researchers.",
            "26": "Exploring more cost-effective and accessible alternatives would be beneficial.",
            "27": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of relation extraction using large language models.",
            "28": "GPT-RE's innovative approach, comprehensive empirical validation, and detailed methodology make it a valuable contribution to the literature.",
            "29": "However, there are areas for improvement, such as expanding reasoning experiments, addressing the overprediction issue more comprehensively, and exploring the use of LLM-generated representations for retrieval.",
            "30": "Despite these limitations, GPT-RE demonstrates the potential of in-context learning for achieving state-of-the-art performance in relation extraction tasks."
        },
        "ZHgq3xoJuR": {
            "0": "- The paper is well organized and mostly easy to follow.",
            "1": "- The study addresses a performance gap between fine-tuned models and in-context learning for relation extraction, and it proposes a solution which achieve SoTA or at least competitive results on multiple, widely used RE datasets.",
            "2": "- In additional ablation studies a deeper analysis of the performance demonstrates the potential of the approach in different settings.",
            "3": "- Overall the study is an interesting contribution to the relation extraction task Regarding the experiments done on TACRED and ACE05: due to the cost of running the model the authors sampled test subsets from that data, which is larger then SEMEVAL and SciERC.",
            "4": "The approach is understandable.",
            "5": "However, strangely the sampled subsets are smaller than the whole test data of the other two datasets; I would expect the subsets of TACRED and ACE05 to have at least the same size or even a bigger size as the other two datasets in order to reflect the whole test data better.",
            "6": "It does hamper the comparison between the models.",
            "7": "This is especially to relevant wrt TACRED, which contains a much larger number of relations than other datasets, and  many of them are probably not well represented in the sampled subset.",
            "8": "The authors plan to release the subsets, however the paper contains no information about the distribution of the relations in the test subsets."
        },
        "2jyjIez6vi": {
            "0": "The paper is well written with good survey of related works.",
            "1": "The authors showed that with task-aware representations, all the retrieval-based models have higher performance than the GPT-Random, while the gold label-induced reasoning helps to improve the performance.",
            "2": "The approach can be a good resource for future research of LLMs to resolve the RE problems.",
            "3": "In section 2.4.2, we define the relation representation as Rel = hi + hj.",
            "4": "Does the relation representation include the context in between, or does it include only the representations of two entities?",
            "5": "If it is the former, it is not surprising that the GPT-RE_FT model achieves good performance, the way the relation representation is obtained is similar to previous works, but instead of putting through a neural network, now it is put into a retriever.",
            "6": "The GPT-RE_SimCSE has the novelty in the entity prompts, which is still limited.",
            "7": "However, if it is the latter, as stated in section 2.4.2 \"representation Rel is naturally enriched with the entity information.",
            "8": "\", it is kind of weird that the GPT-RE_FT can achieves state-of-the-art with only entity information and without context."
        },
        "JRebPsRo6Q": {
            "0": "The paper adeptly highlighted a potential concern regarding GPT-3's utilization of in-context learning (ICL) and effectively provided a resolution.",
            "1": "The observed performance enhancements are indeed promising, as they notably elevate the performance beyond that of GPT-3 alone.",
            "2": "This achievement suggests that the strategies introduced in the paper hold the potential to offer benefits to a wide array of large language models (LLMs).",
            "3": "I think the paper could further strengthen its argument by providing additional evidence to substantiate the identified shortcomings of in-context learning (ICL).",
            "4": "By delving deeper into the reasons behind these shortcomings, the paper could enhance its overall credibility and contribute to a more comprehensive understanding of the challenges associated with ICL."
        }
    },
    "R4yb4m7Nus": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel method, Model-tuning Via Prompts (MVP), for adapting pre-trained language models to downstream tasks.",
            "1": "This method shows significant improvements in adversarial robustness compared to traditional fine-tuning methods.",
            "2": "**Comprehensive Evaluation**: The authors evaluate MVP across multiple datasets (AG News, SST-2, BoolQ, DBPedia14, and MRPC), adversarial attacks (TextFooler, TextBugger, BertAttack, and adversarial misspellings), and models (BERT, RoBERTa, and GPT-2).",
            "3": "This comprehensive evaluation strengthens the validity of their findings.",
            "4": "**Robustness Gains**: MVP demonstrates substantial gains in adversarial robustness, outperforming state-of-the-art adversarial training-based defenses by an average of 3.5%.",
            "5": "Combining MVP with adversarial training further boosts robustness without sacrificing performance on unperturbed examples.",
            "6": "**Sample Efficiency**: The paper shows that MVP is more sample-efficient than traditional fine-tuning methods, achieving higher clean and robust accuracy with fewer training samples.",
            "7": "**Effective Robustness**: MVP exhibits higher effective robustness, meaning it maintains higher robust accuracy for a given clean accuracy compared to traditional methods.",
            "8": "**Ablation Studies**: The authors conduct thorough ablation studies to investigate the mechanisms underlying the robustness gains of MVP.",
            "9": "They explore the impact of the number of prompt templates and candidate answers, providing valuable insights into the method's effectiveness.",
            "10": "**Human Study**: The inclusion of a human study to assess the validity of adversarial examples adds an extra layer of validation to the robustness claims.",
            "11": "The study shows that humans can detect a significant fraction of adversarial examples, highlighting the practical robustness of MVP.",
            "12": "**Broader Impact**: The paper discusses the broader impact of their work, emphasizing the importance of adversarial robustness for deployed NLP systems and contributing to the development of more reliable and safe models.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Limited Scope**: The paper focuses on models with fewer than 1 billion parameters.",
            "15": "While this is a practical choice given the computational constraints, it limits the generalizability of the findings to larger models, which are becoming increasingly popular in the NLP community.",
            "16": "**Latency Trade-off**: Using multiple prompt templates improves robustness but comes with a trade-off in latency.",
            "17": "The paper acknowledges this but does not provide a detailed analysis of the impact on real-world applications where latency is critical.",
            "18": "**Prompt Selection**: The paper does not perform dedicated prompt tuning, which could potentially lead to further improvements in robustness.",
            "19": "While the authors argue that prompt choice has a minimal impact, exploring more advanced prompt tuning techniques could provide additional insights.",
            "20": "**Evaluation Metrics**: The paper primarily focuses on accuracy metrics for evaluating robustness.",
            "21": "Including other metrics such as F1-score, precision, and recall could provide a more comprehensive evaluation of the model's performance, especially for imbalanced datasets.",
            "22": "**OOD Robustness**: While the paper investigates out-of-distribution (OOD) robustness, the evaluation is limited to sentiment analysis tasks.",
            "23": "Expanding the OOD evaluation to other types of tasks could strengthen the claims about the generalizability of MVP's robustness.",
            "24": "**Human Study Sample Size**: The human study involves a relatively small sample size of 250 examples.",
            "25": "Increasing the sample size and involving a more diverse group of annotators could provide more robust conclusions about the human perception of adversarial examples.",
            "26": "**Adversarial Training Complexity**: The paper mentions that MVP achieves robustness gains at a 2x computation cost of standard training, which is lower than other methods requiring 5-10x computation cost.",
            "27": "However, the complexity and feasibility of implementing MVP in large-scale systems are not thoroughly discussed.",
            "28": "#### Conclusion\n\nOverall, the paper presents a significant advancement in improving the adversarial robustness of NLP models through Model-tuning Via Prompts (MVP).",
            "29": "The comprehensive evaluation, robustness gains, and detailed ablation studies make a strong case for the effectiveness of MVP.",
            "30": "However, addressing the limitations related to scope, latency, prompt selection, evaluation metrics, OOD robustness, human study sample size, and adversarial training complexity could further strengthen the impact and applicability of the proposed method."
        },
        "WTPIMut1Me": {
            "0": "- The idea of prompt tuning for robustness is interesting.",
            "1": "The author states the motivation from the point of view of random parameter vulnerability.",
            "2": "- The explanation makes sense and the experimental results are strong.",
            "3": "- A key motivation for the paper is that vanilla fine tuning strategy by adding some random parameters can harm the model's performance and robustness .",
            "4": "However, some experimental results of the proposed method contradict motivation, making the idea less convincing.",
            "5": "By replacing candidates with dummy words such as Jack, John, Ann, the model is still robust.",
            "6": "In my opinion, such labels are no better than a random projection layer.",
            "7": "How can this avoid hurting the parameters of the model?",
            "8": "In pretrained models, many word embeddings are undertuned or even untuned.",
            "9": "For example, some dummy tokens are left for further use.",
            "10": "What if you use these words?",
            "11": "- Since the paper highlights the usefulness of prompts for robustness, further experimental study is needed.",
            "12": "For example, how do you choose the prompts?",
            "13": "How do different prompts affect the robustness of the model?",
            "14": "As a baseline, what if you use some dummy prompts, such as \"foo bar (mask)\"?",
            "15": "- It remains unclear whether robustness arises from the prompt-tuning strategy itself or from some tricky ensemble-style strategy, i.e.",
            "16": "by using different models (prompts) with different class logits whose signal may fool the attacker.",
            "17": "In Table 2, what is the adversarial accuracy of each prompt?",
            "18": "What is the result of \"1 prompt + 4 candidates\"?"
        },
        "wZGI5Py8Ob": {
            "0": "- Strong experiment results, especially wrt adversarial robustness.",
            "1": "- Human annotators validated efficacy of adversarial example set\n- Sound ablation studies and clear analysis that explains the results\n- MVP + Adv outperforms MLP-FT + Adv handily - Only tested on datasets that are simple classification tasks (BoolQ is 2 classes, AGNews contains 4 classes).",
            "2": "- No evaluation for datasets with larger number of classes, no evaluation for non-classification tasks."
        },
        "JNK2ekiizK": {
            "0": "Using prompts to avoid adding random parameters for adversarial robustness is a nice approach and it works on the datasets presented in the paper.",
            "1": "MVP shows better adversarial robustness compared to fine-tuning without sacrificing on accuracy.",
            "2": "The experiments confirm the issue with adding random features in the fine-tuning stage as raised by Kumar e. al 2022 etc.",
            "3": "fine-tuning using prompts is an alternate to the LPFT.",
            "4": "Lack of discussion and analysis on the inference latency as more templates are added for MVP.",
            "5": "This paper would have benefited from a direct comparison between LPFT and MVP on both OOD generalization and Adversarial robustness.",
            "6": "Both these approaches combat the \"brittleness\" of the fine-tuning process.",
            "7": "It would have been interesting to see LPFT vs MVP using different number of prompts.",
            "8": "A discussion on the relative advantages of each approach would have added to this work."
        }
    },
    "IksoHnq4rC": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel method, A3, which combines adversarial training and data augmentation to enhance the robustness of neural NLP models.",
            "1": "This dual approach is innovative and addresses the limitations of existing methods that typically focus on one aspect.",
            "2": "**End-to-End Framework**: The proposed end-to-end adversarial sample generation model is well-structured, consisting of a condition generator, a conditioned paraphrasing model, and a pretrained adversarial discriminator.",
            "3": "This comprehensive framework allows for efficient and effective adversarial sample generation.",
            "4": "**Efficiency**: One of the key strengths of the proposed method is its efficiency.",
            "5": "Unlike traditional adversarial sample generation methods that require extensive interaction with the target model, A3 generates samples independently, making the process faster and more scalable.",
            "6": "**Reusability**: The generated adversarial samples are reusable across different models, which is a significant advantage over methods that generate model-specific samples.",
            "7": "This reusability can save computational resources and time.",
            "8": "**Experimental Validation**: The paper provides extensive experimental results on multiple NLP tasks, demonstrating the effectiveness of the proposed method.",
            "9": "The results show improvements in both model performance and robustness against various attacking techniques.",
            "10": "**Detailed Analysis**: The paper includes a thorough analysis of the impact of different parameters, such as the perturbation ratio, on the effectiveness of the generated adversarial samples.",
            "11": "This analysis helps in understanding the behavior of the proposed method under different settings.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Complexity**: The proposed method, while innovative, is quite complex.",
            "14": "The integration of multiple components (condition generator, conditioned paraphrasing model, adversarial discriminator) and the use of various loss functions may make the implementation and tuning of the model challenging for practitioners.",
            "15": "**Limited Diversity in Conditions**: The paper mentions that the current form of condition limits the diversity and fine-grained control over the generation process.",
            "16": "This limitation could restrict the applicability of the method to a broader range of tasks and scenarios.",
            "17": "**Dependency on Pretrained Models**: The method relies on pretrained models, such as GloVe for word embeddings and a pretrained adversarial discriminator.",
            "18": "This dependency might limit the method's applicability in scenarios where such pretrained models are not available or suitable.",
            "19": "**Evaluation on Limited Tasks**: Although the paper evaluates the method on three different NLP tasks, it would be beneficial to see its performance on a wider range of tasks, including more complex ones like question answering or dialogue systems, to further validate its generalizability.",
            "20": "**Potential Overhead in Training**: While the method is efficient in generating adversarial samples, the training process itself might introduce overhead due to the need for pretraining the conditioned paraphrasing model and the adversarial discriminator.",
            "21": "This overhead should be quantified and compared with other methods.",
            "22": "**Lack of Real-World Application Scenarios**: The paper primarily focuses on standard datasets and tasks.",
            "23": "Including real-world application scenarios and demonstrating the method's effectiveness in practical settings would strengthen the paper's contributions.",
            "24": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of adversarial training and data augmentation for NLP models.",
            "25": "The proposed A3 method is innovative, efficient, and shows promising results in improving model robustness.",
            "26": "However, the complexity of the method, limited diversity in conditions, and dependency on pretrained models are areas that could be addressed in future work.",
            "27": "Expanding the evaluation to more tasks and real-world scenarios would further validate the method's effectiveness and applicability."
        },
        "c7x2v9RTL4": {
            "0": "The results show some improvements.",
            "1": "The motivation of adding each loss and module is clear.",
            "2": "TL;DR I appreciate the efforts and observations / merits found by the authors.",
            "3": "However, this paper poorly presents the methodology (both details and its key advantage), and it’s hard to validate the conclusion with such little hyperparameter analysis.",
            "4": "I would love to see more detailed results, but I could not accept this version to be accepted as an EMNLP paper.",
            "5": "1 There are too many missing details when presenting the methodology: e.g., what will be the effect if I remove one or two losses presented by the authors?",
            "6": "Though their motivations are clear, they do not validate the hypothesis clearly.",
            "7": "2 A lot of equations look like placeholders, such as equations (1, 2, 3, 5, 6).",
            "8": "3 Some of the pieces are simply using existing methods, such as  equation (12), the presentation of these methods are also vague (can only be understood after checking the original paper).",
            "9": "4 The pipeline misses a lot of details.",
            "10": "For example, how long does it take to pre-train each module?",
            "11": "How adding pre-training will benefit the performance?",
            "12": "How to schedule the training of the discriminator and the main module?",
            "13": "Not mentioning the detail design of the RNN network used.",
            "14": "5 Why do we need to focus on the four aspects?",
            "15": "They are just listed there.",
            "16": "Also, some of the results presentation does not seem to be thorough and valid.",
            "17": "For example, in table 2, the Quora datasets have the highest perturbation ratio, but the downgraded performance is the least among the three.",
            "18": "Is it really because the adversarial samples are effective instead of the task variance or dataset variance?",
            "19": "Also, we didn’t see the attack performance of other comparison methods.",
            "20": "And how is the test set generated?",
            "21": "What is the size of the adversarial test set and why is that a good benchmark?",
            "22": "6 In table 4, it’s actually hard to say which is better, A^3 or A^2T, if you count the number of winners for each row and column.",
            "23": "7 In table 5, is the computation time also considered the pre-training stage?",
            "24": "If not, why?",
            "25": "Does the pre-training stage can serve as a unified step which is agnostic to the dataset and tasks?",
            "26": "8 I don’t quite understand the point of section 4.6, and its relationship to the effectiveness of A^3.",
            "27": "This influence of /rho seems to be really obvious.",
            "28": "I would rather be more interested in changing the six hyperparameters mentioned in line 444 and test their effectiveness.",
            "29": "9 The related work section is also not very well-written.",
            "30": "I couldn’t understand what is the key difference and key advantage of A^3 compared to the other methods."
        },
        "IrEWaGcAup": {
            "0": "Outside of some rough grammar in places I found this paper to be very easy to follow and the motivations to create the end-to-end system well founded.",
            "1": "The adversarial + augmentation approach appears to greatly outperform the comparison methods in Table 4, of which only one other was also an adversarial + augmentation method (and it performed significantly slower).",
            "2": "The efficiency of the proposed method was significantly improved over the comparison methods, allowing for time and cost reduction over prior approaches in generating large datasets.",
            "3": "In the reported results tables there is no mention if the data is from a single run or best of many.",
            "4": "There is no given standard deviation or CI to frame the variation of quality in the generation between the compared methods.",
            "5": "Reproducibility would be hard if the authors will not release code, incomplete specifics on the BiRNNs used were given in the paper (were the RNNs identical for all stages?",
            "6": "optimizer?",
            "7": "learning rate?",
            "8": "etc).",
            "9": "If no code is to be released, an appendix would be welcome with further details on complete model construction."
        },
        "mW8IbzQaPT": {
            "0": "- Generate the conditions for paraphrasing model to make a suitable adversarial sample.",
            "1": "- Test on three different tasks to reveal the novelty of works.",
            "2": "- Simple but powerful performances regarding the experiments done by authors.",
            "3": "- Idea is very similar to Generative Adversarial Network.",
            "4": "The author needs to point out the difference to show the novelty of work in the Related work sections better.",
            "5": "Please see \"Missing References\" part.",
            "6": "- The algorithm is not clear, especially for discriminator since author sometimes argues that it is pre-trained model (line 82-85) but sometimes not (line 5 in Algorithm 1).",
            "7": "Please develop the algorithm part better to make it more understandable.",
            "8": "- The insights of authors choices (e.g.",
            "9": "model selection) and the investigation of experimental results are missing.",
            "10": "For example, why A2T in SNLI+DistillBERT is better than your approach (Table 3)?",
            "11": "- Please include the example of adversarial examples in each approach to understand better.",
            "12": "- Lack of ablation study regarding the selection of hyperparameters (e.g.",
            "13": "Section 4.1.3).",
            "14": "- Hard to reproduce the results since there is no offered source code.",
            "15": "- Please double-check whether authors follow the ACL format since the labelling of Table/Figure/Algorithm does not work."
        }
    },
    "ljsGKc8cVR": {
        "llm_review": {
            "0": "**Novel Approach to Long Document Retrieval:**\n   - The paper introduces Longtriever, a novel model specifically designed to address the challenges of long document retrieval.",
            "1": "This is a significant contribution as most existing models are optimized for short passages and struggle with long documents.",
            "2": "**Hierarchical Encoding:**\n   - Longtriever employs a hierarchical paradigm, splitting long documents into short blocks and using both intra-block and inter-block encoders.",
            "3": "This approach effectively balances local and global semantic modeling, ensuring comprehensive document understanding.",
            "4": "**Pre-training Phase:**\n   - The introduction of a pre-training phase with a novel task, Local Masked Autoencoder (LMAE), is a notable strength.",
            "5": "This task helps the model capture unsupervised semantic correlations, addressing the issue of scarce annotations in long document retrieval.",
            "6": "**Experimental Validation:**\n   - The paper provides extensive experimental results on two popular benchmark datasets, demonstrating the superiority of Longtriever over existing methods.",
            "7": "The model consistently outperforms state-of-the-art baselines in terms of MRR@100, Recall@100, and NDCG@10.",
            "8": "**Efficiency:**\n   - Longtriever maintains a desirable time complexity, making it more efficient than vanilla transformers.",
            "9": "The paper also provides a detailed analysis of time and memory costs, showing that Longtriever offers a good trade-off between performance and computational efficiency.",
            "10": "**Ablation Studies:**\n   - The paper includes thorough ablation studies, which help in understanding the contribution of different components of Longtriever.",
            "11": "This adds to the robustness and credibility of the proposed model.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Complexity of the Model:**\n   - While the hierarchical approach is effective, it adds complexity to the model.",
            "14": "The need for both intra-block and inter-block encoders, along with the pre-training phase, might make the model harder to implement and tune compared to simpler models.",
            "15": "**Limited Comparison with BERT-Document:**\n   - The paper mentions the limitation of not examining the performance of BERT when used as a document encoder (BERT-Document) due to substantial GPU memory requirements.",
            "16": "This comparison could have provided a more comprehensive evaluation of Longtriever's performance.",
            "17": "**Scalability Concerns:**\n   - Although the model is efficient, the scalability of Longtriever for extremely large datasets or real-time applications is not thoroughly discussed.",
            "18": "The paper could benefit from a more detailed analysis of how the model performs under different scalability scenarios.",
            "19": "**Pre-training Data:**\n   - The pre-training phase uses BookCorpus and English Wikipedia, which might not be representative of all types of long documents encountered in real-world applications.",
            "20": "The effectiveness of the pre-training phase on other types of documents remains uncertain.",
            "21": "**Generalization to Other Tasks:**\n   - The paper focuses on document retrieval, but it does not discuss the potential generalization of Longtriever to other NLP tasks involving long documents, such as summarization or question answering.",
            "22": "Exploring this aspect could enhance the impact of the proposed model.",
            "23": "**Hyperparameter Sensitivity:**\n   - The performance of Longtriever is shown to be sensitive to the block size and LMAE’s masking ratio.",
            "24": "This sensitivity might require careful tuning of hyperparameters for different datasets, which could be a limitation in practical applications.",
            "25": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of long document retrieval with the introduction of Longtriever.",
            "26": "The model's hierarchical encoding approach and the novel pre-training task (LMAE) are well-motivated and validated through extensive experiments.",
            "27": "However, the complexity of the model, limited comparison with BERT-Document, and scalability concerns are areas that could be addressed in future work.",
            "28": "Despite these weaknesses, Longtriever represents a promising direction for improving long document retrieval and sets a strong foundation for further research in this area."
        },
        "wHTFPFeSEu": {
            "0": "The proposed Longtriever architecture is intuitive and the paper is well written and easy to follow.",
            "1": "The fine-tuned Longtriever outperforms other existing retrievers on MS MARCO document ranking dataset.",
            "2": "It seems to me that the main contribution of the paper comes from LMAE pre-training, which is a revised version of MAE pre-training specific for modelling long document rather than the proposed architecture.",
            "3": "The second issue comes from the evaluation.",
            "4": "From the experiments, it seems the main factor for Longtriever to outperform other models comes from LMAE pre-training, which the other models do not experience, rather than the proposed Longtriever architecture (by comparing III in Table 4 with other long-document model in Table 2).",
            "5": "It is possible that if we apply the same LMAE pre-training strategy to other models, we can get similar ranking effectiveness.",
            "6": "The evaluation is only conducted on MS MARCO document ranking dataset.",
            "7": "However, there are many other document ranking datasets, such as Robust04, ClueWeb or Gov2."
        },
        "Y4U3y7LSiv": {
            "0": "- The work proposes a novel pre-training task, the Local Masked Autoencoder (LMAE), which enhances the model's ability to capture the underlying meaning of long texts by leveraging both local and global representations.",
            "1": "- The writing and organization of the paper are well, making it easy to follow and understand.",
            "2": "- The proposed LMAE approach bears a strong resemblance to LexMAE [1], yet the paper fails to provide a comparative analysis or discussion of the two methods.",
            "3": "- The experimental results presented in the paper do not include comparisons with other state-of-the-art models, such as LexMAE [1] and FDG [2].",
            "4": "- The description of the experimental setup is unclear, leaving readers uncertain about key experimental details.",
            "5": "For instance, it is not specified in Table 1 whether all methods, including the proposed approach, use a sequence length of 512 as input.",
            "6": "[1] Lexicon-Bottlenecked Pretraining for Large-Scale Retrieval.",
            "7": "ICLR 2023.\\\n[2] Fine-Grained Distillation for Long Document Retrieval."
        },
        "DmRhA3Zspm": {
            "0": "The paper is well written, providing a clear motivation and view of the comparative landscape.",
            "1": "The goals/challenges of document retrieval (e.g., computational cost, document understanding, scare annotations) are very valuable for readers.",
            "2": "It is noteworthy that the authors have effectively tackled each of these aspects within their work.",
            "3": "Longtriever introduces a novel framework and architectural design for dense retrieval.",
            "4": "While the concept of integrating document knowledge into dense retrieval is not new, the authors innovate by presenting a fresh paradigm for aggregating representations through the utilization of inter-intra mechanisms.",
            "5": "The authors provide a (very) comprehensive experimental setting, showing their model is competitive with SOTA methods.",
            "6": "The ablation studies shed some light on the importance of the author's choices along the way but do not provide any deep insight into the method's mechanism.",
            "7": "I encourage the authors to provide such analysis to help readers to understand the proposed method.",
            "8": "While Longtriever presents a novel contribution, it's important to note that the LMAE method may not be entirely novel.",
            "9": "The fundamental concept behind LMAE resembles a pre-training task found in autoencoders, such as BART, which involves sentence reconstruction through masking.",
            "10": "You don't state you will provide the code for your paper upon acceptance (if you had, and I missed it, please let me know), I believe one could reproduce your results without it, but I (strongly) encourage you to provide it for future researchers.",
            "11": "The organization of Section 5 is very hard to follow.",
            "12": "Notably, Tables 1 and 2 are positioned within Section 3, making it difficult for readers to follow the flow of experimental results seamlessly."
        }
    },
    "9r8WwpJv7M": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel method for understanding NLP models by incorporating linguistic complexity indices into the training process.",
            "1": "This approach is innovative and leverages existing linguistic knowledge to inform model training, which is a significant contribution to the field.",
            "2": "**Comprehensive Analysis**: The authors provide a thorough analysis of various linguistic complexity indices and their impact on NLP tasks.",
            "3": "This detailed examination helps in understanding the specific linguistic challenges associated with different tasks.",
            "4": "**Effective Curriculum Learning**: The proposed curriculum learning strategies, including time-varying sigmoid, negative-sigmoid, and Gaussian functions, are well-designed and show consistent performance improvements over baseline models.",
            "5": "The use of these strategies to gradually introduce more complex samples is a practical and effective approach.",
            "6": "**Evaluation Metrics**: The paper introduces a balanced accuracy metric based on linguistic indices, which provides a more nuanced evaluation of model performance.",
            "7": "This metric helps in identifying the strengths and weaknesses of models across different levels of linguistic complexity.",
            "8": "**Experimental Validation**: The authors validate their approach on multiple benchmark NLP datasets, demonstrating the generalizability and effectiveness of their methods.",
            "9": "The results show significant improvements in performance, particularly in tasks requiring substantial linguistic knowledge.",
            "10": "**Insightful Findings**: The paper provides valuable insights into the learning dynamics of NLP models and identifies key linguistic indices that contribute to model performance.",
            "11": "This information can guide future research and development in NLP.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Complexity and Computation Overhead**: The incorporation of linguistic complexity indices and the generation of data-driven curricula introduce additional computational overhead.",
            "14": "This complexity might limit the practical applicability of the approach, especially for large-scale datasets or real-time applications.",
            "15": "**Dependence on Linguistic Indices**: The approach relies heavily on the availability and accuracy of linguistic complexity indices.",
            "16": "In languages or domains where such indices are not well-defined or available, the applicability of the method might be limited.",
            "17": "**Limited Analysis of Interactions**: While the paper identifies important linguistic indices, it does not provide a detailed analysis of the interactions between these indices during training.",
            "18": "Understanding these interactions could further enhance the effectiveness of the proposed curricula.",
            "19": "**Generalization to Other Languages**: The paper primarily focuses on English, and it is not clear how well the approach would generalize to other languages with different linguistic structures.",
            "20": "Further research is needed to validate the method across diverse languages.",
            "21": "**Evaluation on More Diverse Tasks**: The paper evaluates the approach on a set of well-known NLP tasks, but it would be beneficial to see its performance on a broader range of tasks, including those with different linguistic and cognitive demands.",
            "22": "**Scalability**: The scalability of the proposed methods to very large datasets or models is not thoroughly discussed.",
            "23": "Given the additional computational requirements, it is important to understand how the approach scales with increasing data and model sizes.",
            "24": "#### Conclusion\n\nOverall, the paper presents a significant advancement in understanding and improving NLP models through the use of linguistic curricula.",
            "25": "The strengths of the paper lie in its novel approach, comprehensive analysis, and effective curriculum learning strategies.",
            "26": "However, the approach's complexity, dependence on linguistic indices, and limited analysis of interactions are areas that could be further explored.",
            "27": "Addressing these weaknesses in future work could enhance the applicability and impact of the proposed methods."
        },
        "ao6ORw67Dj": {
            "0": "The paper presents a novel approach to curriculum learned based on linguistic difficulty that goes beyond previous approaches with regard to the features used, and the way of modelling the curricula.",
            "1": "Furthermore, the paper provides an insightful analysis on the importance of different linguistic indices during different training stages.",
            "2": "Overall, the paper presents a very interesting idea, in combination with a meaningful evaluation.",
            "3": "There is not much information on the model and the experimental settings, only in the appendix.",
            "4": "I think it would be better to have some more information also in the main part of the paper.",
            "5": "A general downside of the approach is the dependency on linguistic knowledge generated from human experts, which will limit the amount of languages that can be modelled with this approach."
        },
        "eoee8dpdnS": {
            "0": "Several innovative methodologies are introduced in this work:\n(1) The incorporation of correlation and optimization methods to gauge the importance of diverse linguistic indices.",
            "1": "(2) Utilization of either the maximum or weighted average for aggregation.",
            "2": "(3) Introduction of curricula encompassing time-varying sigmoid, moving negative-sigmoid, time-varying Gaussian function, etc.",
            "3": "The paper demonstrates adept composition and a well-structured layout.",
            "4": "Readers can readily discern the research inquiries pursued and the corresponding actions taken by the authors.",
            "5": "The authors undertake a substantial array of experiments, bolstering the paper's foundation and substantiating its claims convincingly.",
            "6": "There are a series of methods proposed as mentioned before.",
            "7": "However, it's worth noting that not all of these methods have been examined within the conducted experiments.",
            "8": "For instance, the distinction between the correlation and optimization methods, as well as their respective impacts on system performance, is notably absent.",
            "9": "This paper could potentially benefit from additional pages to accommodate a more extensive array of experiments, thereby offering a more comprehensive and in-depth understanding.",
            "10": "This concern extends to the treatment of linguistic indices.",
            "11": "While multiple indices collaboratively contribute, the experimental focus narrows primarily to word rarity when evaluating different models.",
            "12": "Yet, word rarity is not the top three most important linguistic indices in each dataset, which casts a certain degree of uncertainty upon the conclusions drawn.",
            "13": "Furthermore, certain expressions within the paper lack clarity.",
            "14": "For instance, the term \"competence-based\" in line 350 and \"indices filtering\" in line 353 could benefit from a more explicit explanation.",
            "15": "While the intended meanings can be inferred to some extent, providing a clear and concise elucidation would enhance the overall understanding of the content."
        },
        "Xa1B7lMkCm": {
            "0": "- The paper nicely delineates how individual complexity measures contribute to performance on each task differently, and at what stage of the model training process; this may help others identify what impacts their model performance given what task they're working on.",
            "1": "- Well motivated approach of classifying data samples, then sequencing and weighting them during training - the insights how to use the data could be generalized to other methods \n- Appendix provides helpful context of what indices were used and follow up experiments \n - As the authors point out, linguistic complexity measures require expert knowledge to define and are not language agnostic.",
            "2": "This method then may not generate over to other languages until similar measures of complexity are defined.",
            "3": "- It was not all together clear if the described experiments should be carried out by others in their own work such as identifying complexity of data to better understand their model performance, or if this is a more self contained experiment"
        }
    },
    "J5FFUHZjNx": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel method, STEER LM, which provides an alternative to the widely used Reinforcement Learning from Human Feedback (RLHF) for aligning large language models (LLMs) with human preferences.",
            "1": "This approach leverages supervised fine-tuning (SFT) conditioned on multiple attributes, which is a significant innovation.",
            "2": "**User-Steerability**: One of the standout features of STEER LM is its ability to allow end-users to control the model's responses during inference by conditioning on a set of explicitly defined multi-dimensional attributes.",
            "3": "This flexibility is a major advantage over RLHF, which typically aligns models with implicit values that users cannot control at runtime.",
            "4": "**Simplified Training Process**: The paper highlights the complexity of RLHF training setups and presents STEER LM as a simpler alternative.",
            "5": "By using only the language modeling objective, STEER LM reduces the training complexity and makes it more accessible for public adoption.",
            "6": "**Empirical Validation**: The authors provide extensive empirical validation of STEER LM, showing that it outperforms state-of-the-art baselines, including models trained with RLHF, on the Vicuna benchmark.",
            "7": "This is supported by both automatic and human evaluations.",
            "8": "**Detailed Methodology**: The paper provides a comprehensive description of the methodology, including the steps involved in training the attribute prediction model, annotating datasets, and performing attribute-conditioned SFT.",
            "9": "This detailed explanation enhances the reproducibility of the work.",
            "10": "**Ablation Studies**: The inclusion of ablation studies helps in understanding the contribution of each component of the proposed method.",
            "11": "This adds to the robustness of the findings and demonstrates the effectiveness of the approach.",
            "12": "**Steerability Demonstration**: The paper includes a demonstration of the model's ability to control attributes like toxicity and humor, showcasing the practical utility of STEER LM in various applications.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Limited Evaluation Scope**: The evaluation is primarily focused on English language benchmarks.",
            "15": "The paper acknowledges this limitation but does not provide any evaluation on multilingual benchmarks.",
            "16": "This raises questions about the generalizability of the approach to other languages.",
            "17": "**Potential for Misuse**: The ability to control attributes like toxicity and violence at inference time, while useful in certain contexts, also opens up the potential for misuse.",
            "18": "The paper briefly addresses this concern but does not provide a comprehensive solution to mitigate such risks.",
            "19": "**Resource Intensive**: Although the training process is simplified compared to RLHF, the paper notes that the Attribute Prediction Model and Attribute-Conditioned SFT Models are fully supervised fine-tuned, which can be costly in terms of GPU hours and energy consumption.",
            "20": "This might limit the accessibility of the approach for smaller organizations or individual researchers.",
            "21": "**Dependence on High-Quality Annotations**: The effectiveness of the Attribute Prediction Model relies on the quality of the annotated datasets.",
            "22": "While the paper discusses the use of the model to denoise and calibrate annotations, the initial quality of human-annotated data remains a critical factor.",
            "23": "**Comparison with RLHF**: While the paper positions STEER LM as an alternative to RLHF, it would benefit from a more detailed comparison of the two approaches in terms of specific use cases, advantages, and limitations.",
            "24": "This would provide a clearer picture of when and why one might choose STEER LM over RLHF.",
            "25": "**Scalability**: The paper does not discuss the scalability of the approach in detail.",
            "26": "It would be useful to understand how well STEER LM performs with larger datasets and more complex attribute sets, as well as its performance on different hardware configurations.",
            "27": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of model alignment with human preferences.",
            "28": "STEER LM offers a promising alternative to RLHF, with the added benefit of user-steerability and a simplified training process.",
            "29": "The empirical results are compelling, and the detailed methodology enhances the reproducibility of the work.",
            "30": "However, the paper could benefit from addressing the potential for misuse, providing more comprehensive evaluations, and discussing the scalability and resource requirements in greater detail.",
            "31": "Despite these weaknesses, STEER LM represents a valuable contribution to the development of more flexible and user-friendly AI systems."
        },
        "IYy4GSbR3c": {
            "0": "Retraining using bootstrap high-quality data is sound and it makes user can decide the attributes like humor or toxicity\nIt outperforms ChatGPT 3.5 in auto evaluation and human evaluation on Vicuna test.",
            "1": "Detailed ablation studies proved its effective design.",
            "2": "See questions."
        },
        "LkoTQhlLH6": {
            "0": "(1) The paper proposes a novel approach, SteerLM, which offers a simpler and more user-controlled alternative to RLHF for aligning language models with human preferences.",
            "1": "(2) The methodology is well-structured and easy to understand.",
            "2": "The authors provide clear explanations of the attribute prediction model, attribute-conditioned SFT, and bootstrapping with high-quality samples.",
            "3": "(3) The paper includes comprehensive experimental evaluations, including automatic evaluation with GPT-4, human evaluation, and an ablation study, to demonstrate the effectiveness of STEER LM.",
            "4": "(1) More details are needed for the base model.",
            "5": "What is the base performance?",
            "6": "How much compute does it costs?",
            "7": "Are there any public access?",
            "8": "Because it is unclear about the base model, we do not know how much improvement has been made by the proposed method.",
            "9": "(2) There are 13 attributes in OASST, why do the authors choose 7 out of 13?",
            "10": "(3) The method is not new.",
            "11": "It is needed to discuss the differentiation on the method with RAFT [1] and RRHF [2].",
            "12": "Especially, an important and similar work is omitted [1].",
            "13": "[1] RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment https://arxiv.org/abs/2304.06767\n\n[2] Rrhf: Rank responses to align language models with human feed- back without tears https://arxiv.org/abs/2304.05302"
        },
        "abfvIbOmfP": {
            "0": "The paper proposes an alternative method for model alignment, providing a different approach to RLHF.",
            "1": "The proposed SteerLM approach demonstrates strong performance, surpassing existing models on the Vicuna benchmark.",
            "2": "The proposed method may introduce challenges in collecting alignment data, potentially increasing the difficulty of the alignment process.",
            "3": "It is important to verify the proposed method on other benchmarks or approaches to determine its generalizability.",
            "4": "It remains unclear whether the superior performance is primarily attributed to the 43B pretrained model or the attribute conditioned SFT.",
            "5": "The paper would benefit from additional evaluation results.",
            "6": "Existing benchmarks lack evaluations specifically related to toxicity and hallucinations, which are crucial aspects for assessing alignment effectiveness."
        }
    },
    "ouiQX2XWYc": {
        "llm_review": {
            "0": "**Novelty and Innovation:**\n   - The paper introduces a novel approach to watermarking large language models (LLMs) by leveraging the quantization process.",
            "1": "This is a fresh perspective compared to traditional watermarking methods that often rely on pre-defined triggers or decoding strategies.",
            "2": "**Practical Relevance:**\n   - The proposed method addresses a significant concern in the deployment of LLMs, which is the protection of model weights to prevent unauthorized or malicious usage.",
            "3": "This is particularly relevant given the rapid adoption and deployment of LLMs in various applications.",
            "4": "**Comprehensive Methodology:**\n   - The authors provide a detailed explanation of the quantization and de-quantization processes, and how these can be utilized to plant watermarks.",
            "5": "The introduction of interval optimization as a strategy to maintain the performance of quantized models while embedding watermarks is well-articulated.",
            "6": "**Experimental Validation:**\n   - The paper includes extensive experiments on two widely used LLMs, GPT-Neo and LLaMA, demonstrating the effectiveness of the proposed watermarking strategies.",
            "7": "The experiments cover various scenarios, including text-agnostic and text-related watermarking, and also explore the robustness of the watermarks against further pre-training.",
            "8": "**Clear Metrics for Evaluation:**\n   - The authors define clear metrics for evaluating the success of watermarking, including Watermark Plant Rate (WPR), Text Maintaining Rate (TMR), and Success Rate (SR).",
            "9": "This provides a transparent and quantifiable way to assess the performance of the proposed methods.",
            "10": "**Ethical Considerations:**\n   - The paper acknowledges the ethical implications of watermarking and aims to provide a protective measure for AI technologies, which is a positive aspect in the context of responsible AI deployment.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Watermark Erasing:**\n   - One of the major limitations highlighted by the authors is the ease with which the watermarks can be erased through further pre-training.",
            "13": "This undermines the robustness of the watermarking method and suggests that additional strategies may be needed to enhance the persistence of the watermarks.",
            "14": "**Generalization of Triggers:**\n   - In the text-related watermarking experiments, the generalization of triggers appears to be inconsistent.",
            "15": "While some triggers work well, others do not generalize effectively, which could limit the practical applicability of the method in real-world scenarios.",
            "16": "**Scalability and Performance Overhead:**\n   - The paper does not provide a detailed analysis of the computational overhead introduced by the watermarking process, especially in large-scale deployments.",
            "17": "Understanding the impact on model training and inference times would be crucial for practical adoption.",
            "18": "**Limited Scope of Experiments:**\n   - While the experiments are comprehensive, they are limited to two specific LLMs.",
            "19": "It would be beneficial to see how the proposed methods perform across a broader range of models and architectures to validate their generalizability.",
            "20": "**Lack of Real-World Application Scenarios:**\n   - The paper could benefit from more concrete examples of real-world application scenarios where the proposed watermarking methods would be particularly beneficial.",
            "21": "This would help in understanding the practical implications and potential use cases of the method.",
            "22": "**Complexity of Implementation:**\n   - The interval optimization strategy, while effective, may be complex to implement and integrate into existing workflows.",
            "23": "Providing more detailed guidelines or tools for implementation could help in wider adoption.",
            "24": "**Conclusion:**\n\nOverall, the paper presents a novel and promising approach to watermarking LLMs using weight quantization.",
            "25": "The strengths lie in the innovative methodology, practical relevance, and comprehensive experimental validation.",
            "26": "However, there are areas for improvement, particularly in enhancing the robustness of the watermarks, understanding the performance overhead, and providing more practical implementation guidelines.",
            "27": "Despite these weaknesses, the paper makes a significant contribution to the field and opens up new avenues for research in protecting LLMs from unauthorized usage."
        },
        "vZyf1oeeyl": {
            "0": "This paper works on a timely and important problem.",
            "1": "The watermarking could help protect intellectual property and prevent misuse of open-sourced large language models.",
            "2": "This paper proposes a watermarking technique that utilizes model quantization gaps to embed watermarks, avoiding the need for predefined triggers.",
            "3": "The proposed method is novel.",
            "4": "Experiments demonstrates that the proposed approach effectively work on major open-source models like GPT-Neo and LLaMA.",
            "5": "The paper has limited discussion of watermark robustness under finetuning.",
            "6": "Open-source LLMs like LLaMA-1 and LLaMA-2 are often finetuned for real-world usage.",
            "7": "The impact of finetuning on retaining the quantization watermarks is unclear.",
            "8": "This is a major concern for real usage.",
            "9": "There is limited analysis of the tradeoff between watermark capacity and model performance.",
            "10": "It would be great if the authors add more discussion of the evaluation of the watermarked models."
        },
        "5y4YD1PqAN": {
            "0": "- Very interesting connection with model watermark and quantization\n- Success demonstration that such method is applicable to LLaMA and GPT-Neo - I am highly interested in the intersection of quantization and model watermarking.",
            "1": "However, I am not convinced that the issue of watermark erasure should be disregarded.",
            "2": "A possible real-world scenario of model watermarking is when a user obtains a released model and fine-tunes on personalized/custom data.",
            "3": "Verifying the ownership of the finetuned model is very crucial to protect model IP (consider company x uses company y's model without following y's model license).",
            "4": "- Quantized model often results in performance degradation.",
            "5": "Since your work involved customized int8 quantization, it would be interesting to see if the performance of the quantized model is close to the original full-precision model.",
            "6": "I'd suggest testing the quantized model using LLM Harness benchmark [1].",
            "7": "- Other than these two issues I am fine with other experiments -- well-executed.",
            "8": "[1]: https://github.com/EleutherAI/lm-evaluation-harness"
        },
        "Of0x08fUb3": {
            "0": "- The idea of planting watermarks during the quantization process is quite novel and interesting\n- The proposed three testing scenarios and three metrics are good experimental setup designs and can be used by similar works along this line (though there are some experimental setup concerns mentioned in reject reasons) - Important comparison missing: there are only comparisons among different optimization methods or the watermarking based on the quantization idea in the paper, what would be the comparison result with traditional watermarking text-related watermarking?",
            "1": "I assume it’s feasible to obtain WPR, TMR and SR of the traditional trigger-based watermarking techniques.",
            "2": "- Important details missing: 1) what are concrete watermarks used, are they natural language sentences or something else?",
            "3": "2) How many watermarks are used in the experiments, when talking about WPR/SR, what is the total test case number (as it’s important for us to know whether the method is robust or not)?",
            "4": "- The application scenarios of the proposed watermarking technique are limited.",
            "5": "We have to assume the downstream model provides access to both quantized and full-precision copies of the new models to be able to check whether there is watermarking left.",
            "6": "If the downstream developer only provides API access to the model, we cannot detect the origin of the model.",
            "7": "- The experimental setting is relatively constrained.",
            "8": "The same set of datasets (around Line 354) is used to train the model with watermarks and to test the task success rate.",
            "9": "It’s hard to see whether the proposed method can yield similar performance when the training/testing data is changed."
        }
    },
    "ZVy8L79f5f": {
        "llm_review": {
            "0": "**Comprehensive Benchmark**: The paper introduces a novel benchmark, FaLB, which addresses multiple facets of the OIE-to-KG linking problem.",
            "1": "This includes transductive, inductive, polysemous, and out-of-KG detection, providing a thorough evaluation framework that reflects real-world scenarios.",
            "2": "**High-Quality Data**: The authors ensure high precision in aligning OIE triples with KG facts, verified through a human study with a 97% correctness rate.",
            "3": "This high-quality data is crucial for training and evaluating models effectively.",
            "4": "**Diverse OIE Methods**: The use of both rule-based and neural OIE methods (MinIE, StanfordOIE, MilIE, Multi2OIE) ensures a diverse set of OIE triples, which enhances the robustness of the benchmark.",
            "5": "**Entity Alias Augmentation**: The paper addresses the lack of diversity in entity mentions by augmenting the data with entity aliases.",
            "6": "This step is essential for training models that can handle the variability in natural language.",
            "7": "**Synthetic Data Utilization**: The authors demonstrate that training on synthetic data (SynthIE) can yield models that perform well on the OIE-to-KG linking task.",
            "8": "This finding is significant as it suggests that high-quality synthetic data can reduce the reliance on human-annotated data.",
            "9": "**Detailed Evaluation**: The paper provides a detailed evaluation of various models and methods, including the impact of KG size and the effectiveness of different training data.",
            "10": "This thorough analysis helps in understanding the strengths and limitations of each approach.",
            "11": "**Open-Source Resources**: The authors publicly release all resources, including data, benchmark, and code, which promotes transparency and facilitates further research in this area.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Language Scope**: The study is limited to the English language.",
            "14": "While the approach can be extended to other languages, the paper does not explore this aspect, which could limit the generalizability of the findings.",
            "15": "**Out-of-KG Detection Challenge**: The paper identifies out-of-KG detection, especially for relations, as an open research problem.",
            "16": "While this is a valuable insight, the paper could have explored more advanced techniques or provided a more in-depth analysis of potential solutions.",
            "17": "**Dependency on KG Quality**: The effectiveness of the proposed methods heavily relies on the quality and coverage of the reference KG (Wikidata).",
            "18": "In scenarios where the KG is incomplete or contains errors, the performance of the models might degrade.",
            "19": "**Complexity of Implementation**: The proposed methods, especially the combination of pre-ranking and re-ranking models, add complexity to the implementation.",
            "20": "This might pose challenges for practitioners looking to adopt these techniques without significant computational resources.",
            "21": "**Synthetic Data Limitations**: While synthetic data (SynthIE) shows promise, the paper acknowledges that it lacks the diversity found in real-world data.",
            "22": "The reliance on canonical forms in synthetic data might not fully capture the variability in natural language, potentially limiting the robustness of the trained models.",
            "23": "**Evaluation on Real-World Data**: The paper could benefit from additional evaluation on real-world datasets beyond REBEL and SynthIE to further validate the generalizability and robustness of the proposed methods.",
            "24": "#### Conclusion\n\nOverall, the paper makes significant contributions to the field of OIE-to-KG linking by introducing a comprehensive benchmark, ensuring high-quality data, and demonstrating the potential of synthetic data.",
            "25": "The detailed evaluation and open-source resources are commendable.",
            "26": "However, the study is limited to the English language, and the out-of-KG detection remains a challenging problem.",
            "27": "Future work could explore multilingual extensions, advanced techniques for out-of-KG detection, and further validation on diverse real-world datasets."
        },
        "qok8Wd2eIb": {
            "0": "- A novel benchmark for OIE-to-KG linking that covers multiple aspects of OIE to KG linking, as well as Out-of-KG detection.",
            "1": "- Evaluation with multiple baselines.",
            "2": "- Clear presentation.",
            "3": "- An artificial setup where only triple slots (subj, obj, pred) are used for linking without the use of the sentence in the context.",
            "4": "- Weak baselines (that don't use the context sentence)\n - A data augmentation (paraphrasing / using different aliases for entities) that is applied on the triple level and not on the sentence level, which wouldn't allow to use source sentence (because the model could use the mismatch information between the surface form in the triple and the sentence)"
        },
        "9sRDQSbxJS": {
            "0": "The paper gave a valuable dataset for OIE-to-KG linking.",
            "1": "It would also be beneficial for those who considering the OIE-to-KG linking problem.",
            "2": "The novelty is somewhat limited, making the model is a combination of existing works."
        },
        "02xR4U0RMm": {
            "0": "The proposed data set can be used as a new benchmark for KG fact extraction from text.",
            "1": "The dataset curation process considers a wide variety of settings which can serve a test bed for existing and future models for this task.",
            "2": "The paper proposes a solution that works well on the proposed data set.",
            "3": "No clear comparison is made between the proposed data set and other existing large scale datasets (e.g., T-Rex, ReVerb45K, etc.).",
            "4": "Such comparisons would make it clear why the community should adopt this data set instead of existing ones.",
            "5": "Lack of strong baselines.",
            "6": "The proposed method is overwhelmingly better than any of the baselines.",
            "7": "There are existing works of fact extraction from text (e.g., KBPearl, CESI) that could serve as a strong baseline."
        }
    },
    "4AiERjB5JD": {
        "llm_review": {
            "0": "**Innovative Use of Prefix-Tuning:**\n   - The paper introduces a novel approach to unsupervised text style transfer by leveraging prefix-tuning with pre-trained language models, specifically GPT-2.",
            "1": "This method is innovative as it diverges from traditional methods that rely on embeddings or latent representations, providing a richer and more expressive way to encode style and content information.",
            "2": "**Effective Content Preservation:**\n   - The proposed method demonstrates significant improvements in content preservation, as evidenced by the higher BLEU scores in both self-BLEU and ref-BLEU metrics.",
            "3": "This is attributed to the recursive use of the language model to generate content prefixes, ensuring better interaction between the input text and the language model.",
            "4": "**Fluency of Generated Text:**\n   - The use of GPT-2 as the backbone for both the generator and discriminator ensures that the generated text is fluent, as indicated by the lower perplexity (PPL) scores.",
            "5": "This is a crucial aspect of text style transfer, as maintaining the fluency of the output is essential for practical applications.",
            "6": "**Comprehensive Evaluation:**\n   - The paper provides a thorough evaluation of the proposed method, including both automatic and human evaluations.",
            "7": "The inclusion of human evaluation adds credibility to the results, showing that the method performs well not only in quantitative metrics but also in qualitative assessments.",
            "8": "**Adversarial Learning Framework:**\n   - The use of an adversarial learning framework with a generator and discriminator is well-justified and effectively implemented.",
            "9": "This approach helps in achieving better style transfer by ensuring that the generated text aligns with the target style while preserving content.",
            "10": "**Efficient Training:**\n   - The method requires training only 2% of the parameters of the pre-trained language model, making it computationally efficient compared to full fine-tuning.",
            "11": "This is a significant advantage in terms of resource utilization and scalability.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Scope of Evaluation:**\n   - The evaluation is conducted on only two datasets (Yelp and IMDb), both of which are sentiment-based.",
            "14": "While these are commonly used datasets, the method's applicability to other types of style transfer (e.g., formality, politeness) is not explored.",
            "15": "A broader evaluation would strengthen the claims of the paper.",
            "16": "**Dependence on Pre-trained Models:**\n   - The method heavily relies on the capabilities of pre-trained language models like GPT-2.",
            "17": "While this ensures high-quality text generation, it also means that the performance is contingent on the quality and size of the pre-trained model.",
            "18": "This could be a limitation in scenarios where such models are not available or feasible to use.",
            "19": "**Complexity of Implementation:**\n   - The proposed method involves multiple components, including shared, style, and content prefixes, as well as a recursive use of the language model.",
            "20": "This complexity might pose challenges in implementation and reproducibility, especially for practitioners who are not well-versed in advanced NLP techniques.",
            "21": "**Lack of Zero-Shot Evaluation:**\n   - The paper mentions the potential for extending the approach to zero-shot settings but does not provide any experiments or results in this regard.",
            "22": "Including zero-shot evaluation would demonstrate the versatility and robustness of the method.",
            "23": "**Potential Overfitting:**\n   - While the method shows competitive results, there is a risk of overfitting to the specific datasets used for training and evaluation.",
            "24": "The paper does not discuss measures taken to prevent overfitting or the generalizability of the model to unseen data.",
            "25": "**Human Evaluation Details:**\n   - The human evaluation section could benefit from more detailed reporting, such as inter-annotator agreement scores.",
            "26": "This would provide additional insights into the reliability and consistency of the human evaluation results.",
            "27": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of unsupervised text style transfer by introducing a prefix-tuning based approach.",
            "28": "The method shows promising results in terms of content preservation, fluency, and style transfer accuracy.",
            "29": "However, the scope of evaluation is somewhat limited, and the complexity of the method may pose challenges for practical implementation.",
            "30": "Future work could address these limitations by exploring a wider range of datasets, simplifying the implementation, and evaluating the method in zero-shot settings."
        },
        "wHvluZ2pQW": {
            "0": "- Proposing a new method based on GPT-2 for unsupervised text style transfer.",
            "1": "- Introducing three different types of prefixes to encode task-specific information, target style, and content details, thereby providing more comprehensive information to the model compared to the embeddings used in previous works.",
            "2": "- Adopting a recursive approach to improve the interactions between the input sentence and GPT-2.",
            "3": "- Providing comprehensive results, conducting ablation studies, and including subjective evaluations from human assessors to gain a deeper understanding of the proposed approach.",
            "4": "- This paper presents incremental work.",
            "5": "The proposed method is highly similar to the framework of (Dai et al., 2019) and incorporates prompt learning techniques that have been previously proposed.",
            "6": "- The evaluation of the proposed method is limited to sentiment transfer tasks.",
            "7": "It would be beneficial to explore the performance of the proposed method on other tasks (such as formality transfer) and datasets.",
            "8": "- The experiments in this paper only compare a small number of baselines.",
            "9": "It would be advantageous to include additional baselines, such as DelRetri [Li et al., 2018], Template [Li et al., 2018], UnsuperMT [Zhang et al., 2018], and DualRL [Luo et al., 2019]."
        },
        "URwVSgR3D7": {
            "0": "This work is clearly motivated to do text style transfer and proposes a quite new and interesting.",
            "1": "The adversarial framework of the methodology is reasonable.",
            "2": "The prefixes are quite significant according to the experimental results.",
            "3": "Moreover, the losses designed for the generator are quite thoroughly considered.",
            "4": "The statement of the methodology and the structure of the paper are good.",
            "5": "The implementation is comprehensive.",
            "6": "The results are solid.",
            "7": "Baselines in this paper are limited.",
            "8": "There are no baselines from 2022.",
            "9": "There a also no baselines of editing-based [1] or prompt-based methods [2].",
            "10": "There is no analysis of the three losses to train the generator.",
            "11": "[1] Machel Reid and Victor Zhong.",
            "12": "2021.",
            "13": "LEWIS: Leven- shtein editing for unsupervised text style transfer.",
            "14": "In Findings of ACL-IJCNLP, pages 3932–3944.",
            "15": "[2] Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky.",
            "16": "2022.",
            "17": "Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models.",
            "18": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2195–2222."
        },
        "UDFpZ39upl": {
            "0": "* The method provided improvements compared to three different models.",
            "1": "Compared to LaMDA, the improvement is mainly in BLEU score.",
            "2": "* The paper is written clearly (but there are parts that need to be explained better (e.g., why do we need the recursive exactly and what happens if we  do not do that?)",
            "3": "* The paper is only evaluated on two datasets that target the same task (positive to negative sentences or vice versa).",
            "4": "The approach would be more powerful if different types of styles have been considered.",
            "5": "The provided examples focus on changes for the language (e.g.",
            "6": "but) or antonym changes (e.g.",
            "7": "love vs. hate) but not clear how does it perform in more complex scenarios.",
            "8": "* The paper has focused on transferring only two styles to each other.",
            "9": "Not clear what happens with scaling the number of styles."
        }
    },
    "fONyQKyvsY": {
        "llm_review": {
            "0": "**Novel Dataset**: The paper introduces a new dataset of 14,053 articles from two state-backed disinformation websites, Reliable Recent News (RRN) and WarOnFakes (WoF).",
            "1": "This dataset is multilingual, covering Arabic, Chinese, English, French, German, and Spanish, which is a significant contribution to the field of Natural Language Processing (NLP) and disinformation studies.",
            "2": "**Comprehensive Methodology**: The authors provide a detailed methodology for data collection, including the use of the WordPress REST API and the extraction of translations.",
            "3": "This transparency in methodology allows for reproducibility and potential application to other similar studies.",
            "4": "**Linguistic and Temporal Analysis**: The paper performs a thorough linguistic and temporal analysis of the articles, including the use of LIWC2015 for linguistic properties and BERTopic for topic clustering.",
            "5": "This multi-faceted approach provides a deep understanding of the content and its dissemination patterns.",
            "6": "**Identification of Backdated Articles**: The study identifies articles with false publication dates, highlighting the deceptive practices of these disinformation websites.",
            "7": "This is a crucial finding that adds to the understanding of how disinformation is propagated.",
            "8": "**Public Availability of Dataset and Tools**: The authors have made the dataset and their extraction toolkit publicly available, which is commendable.",
            "9": "This openness promotes further research and collaboration in the field.",
            "10": "**Ethical Considerations**: The paper addresses ethical concerns related to the collection and analysis of disinformation content, ensuring that the research is conducted responsibly.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Limited Scope of Disinformation Sources**: While the dataset is novel, it is limited to only two disinformation websites.",
            "13": "The authors acknowledge this limitation, but it still restricts the generalizability of the findings to other state-backed disinformation sources.",
            "14": "**Lack of Validation for Topic Analysis**: The topic analysis model has not been formally validated against human or expert annotations.",
            "15": "This lack of validation raises questions about the accuracy and reliability of the identified topics.",
            "16": "**Comparison with Limited Baselines**: The LIWC analysis compares the disinformation content with New York Times articles, which may not be the most appropriate baseline for all types of online media.",
            "17": "A broader range of baselines, including other reputable news sources and different styles of journalism, would provide a more comprehensive comparison.",
            "18": "**Absence of Stance Detection**: The paper does not attempt to identify the stance of the articles towards the topics discussed.",
            "19": "Stance detection could provide additional insights into the nature of the disinformation and its intended impact.",
            "20": "**Potential for Misuse**: While the authors have taken steps to ensure ethical use of the dataset, there is always a risk that the dataset could be used to train generative models capable of creating new disinformation.",
            "21": "The authors should emphasize this risk more strongly and provide guidelines to mitigate it.",
            "22": "**Limited Analysis of Non-English Content**: The analysis primarily focuses on English content, with less emphasis on the other languages in the dataset.",
            "23": "A more balanced analysis across all languages would enhance the study's comprehensiveness.",
            "24": "**Conclusion:**\n\nOverall, the paper makes a significant contribution to the study of state-backed disinformation by providing a novel dataset and a comprehensive analysis of its content.",
            "25": "The strengths of the paper lie in its detailed methodology, thorough analysis, and public availability of resources.",
            "26": "However, the study's scope is limited to two websites, and there are areas where further validation and broader comparisons would strengthen the findings.",
            "27": "The ethical considerations are well-addressed, but the potential for misuse of the dataset should be more strongly emphasized.",
            "28": "Despite these weaknesses, the paper provides a valuable resource for future research in disinformation detection and analysis."
        },
        "YD3fDMVeKn": {
            "0": "This is an interesting paper.",
            "1": "I think the dataset can be very valuable not only for the NLP community, but also for analysis from other disciplines, such as discourse analysis, sociology, politics or journalism.",
            "2": "I found the analysis interesting, although more from a sociological, cultural perspective than from an NLP point of view.",
            "3": "There is no background analysis.",
            "4": "There is much work done in propaganda detection, both in the publication of linguistic resources, such as datasets, and in the development of models to detect it.",
            "5": "In addition, propaganda has been studied for many years from other disciplines.",
            "6": "All this background needs to be accounted for in a paper that presents a new resource for automatic propaganda detection.",
            "7": "Examples of the dataset would be very helpful in a paper that introduces a new dataset, as the reader learns about your work, but is not able to see it."
        },
        "03YFWWAWuf": {
            "0": "Scientifically sound article, relevant cues on disinformation, interesting discussion on the methods and their limitations, as well as on ethical issues.",
            "1": "Decisions made on the tools are clearly described and quite valuable.",
            "2": "Part of content analysis is specific to the websites of interest and cannot be generalized easily.",
            "3": "Beyond topic analysis which involves a great deal of fine-tuning and may not be easy to reproduce on other datasets, the article lacks more refined textual analysis, i.e.",
            "4": "proper sentiment detection other than word lists (section 3.5) and better word grouping or filtering methods than n-gram analysis (section 3.7)."
        },
        "9BGdyTKRqf": {
            "0": "This paper presents an interesting corpus and analysis of articles crawled from propaganda websites.",
            "1": "Upon publication, the presented corpus would offer a wide range of research possibilities.",
            "2": "Some manual annotations (for instance, on the factuality of the articles) would have been interesting; however, the presented work fits the short paper track very well."
        }
    },
    "XJRNw74kXK": {
        "llm_review": {
            "0": "**Novel Dataset Creation**: The authors introduce POSQA, a dataset specifically designed to probe the understanding of physical object sizes in large language models (LLMs).",
            "1": "This is a novel contribution as it addresses a relatively unexplored area in the evaluation of LLMs.",
            "2": "**Comprehensive Analysis**: The paper provides a thorough analysis of the performance of various LLMs, including Flan-T5, GPT-3, and GPT-3.5 Turbo, on the POSQA dataset.",
            "3": "The authors explore different prompting techniques and the impact of external knowledge augmentation, which adds depth to the study.",
            "4": "**Insightful Findings**: The study reveals that even the largest LLMs struggle with size comparison tasks in a zero-shot setting.",
            "5": "The authors' findings that LLMs rely more on contextual information rather than their internal weights for real-world comprehension are significant and provide valuable insights into the limitations of current LLMs.",
            "6": "**Adversarial Prompting**: The use of adversarial prompts with knowledge perturbation to test the robustness of LLMs is a strong methodological approach.",
            "7": "It highlights the vulnerability of LLMs to misleading information, which is crucial for understanding their limitations.",
            "8": "**Human Comparison**: The comparison of LLM performance with human annotators, both online and offline, adds a valuable perspective to the study.",
            "9": "The use of Krippendorff’s Alpha to measure consistency is a robust approach to evaluating agreement.",
            "10": "**Weaknesses:**\n\n1.",
            "11": "**Limited Scope of Dataset**: The POSQA dataset, while novel, is limited to 92 entities and focuses solely on size comparisons.",
            "12": "This narrow scope may not fully capture the complexities of real-world understanding and limits the generalizability of the findings.",
            "13": "**Lack of Diversity in Models**: The experiments are limited to a few models (Flan-T5, GPT-3, and GPT-3.5 Turbo).",
            "14": "Including a broader range of models, especially those with different architectures or training paradigms, could provide a more comprehensive understanding of the capabilities and limitations of LLMs.",
            "15": "**Insufficient Exploration of Prompt Sensitivity**: While the paper discusses the sensitivity of LLMs to different prompt formats, the exploration is somewhat limited.",
            "16": "A more detailed analysis of how different prompt structures affect performance across various models would strengthen the study.",
            "17": "**Absence of Longitudinal Analysis**: The paper does not explore how LLMs' understanding of size comparisons evolves with continued training or exposure to more diverse datasets.",
            "18": "A longitudinal analysis could provide insights into the learning dynamics of LLMs.",
            "19": "**Ethical Considerations**: While the authors mention ethical considerations, the discussion is brief.",
            "20": "A more detailed exploration of the ethical implications of LLMs' reliance on contextual information, especially in scenarios where misleading information could have significant consequences, would be beneficial.",
            "21": "**Conclusion:**\n\nOverall, the paper makes a valuable contribution to the field by introducing a novel dataset and providing a comprehensive analysis of LLMs' understanding of physical object sizes.",
            "22": "The findings highlight important limitations of current LLMs and suggest areas for future research.",
            "23": "However, the study could be strengthened by expanding the scope of the dataset, including a more diverse range of models, and providing a more detailed analysis of prompt sensitivity and ethical considerations."
        },
        "rq93pabyw8": {
            "0": "The paper presents a novel dataset aimed at evaluating the parametric knowledge of Large Language Models on a specific type of questions that has not been extensively explored in previous research.",
            "1": "The probing setup utilized in the study is well-structured and methodologically sound.",
            "2": "Furthermore, the conclusions drawn from the experimental results are well-supported and credible.",
            "3": "The paper mentions the study of the alignment of LLM knowledge with human comprehension of the world as one of its primary contributions.",
            "4": "However, this aspect is only briefly addressed in the discussion section, where a small-scale experiment is conducted.",
            "5": "Unfortunately, the analysis of this experiment is limited, as it solely compares ChatGPT's performance with that of human annotators.",
            "6": "Furthermore, the paper contains several grammatical errors that may impede comprehension, particularly for non-native English speakers.",
            "7": "To enhance the paper's clarity, I suggest addressing the listed grammatical errors in the Presentation Improvements field and conducting a more thorough analysis of the experiment comparing LLMs' knowledge to human understanding.",
            "8": "This could provide deeper insights into the alignment or disparities between the two."
        },
        "rGWB7fkvvT": {
            "0": "The dataset is interesting, new, and well-designed.",
            "1": "On the last point, its unique topic fills an apparent gap in the literature, allowing easily interpretable experimental design for the paper's main research questions on in-context learning.",
            "2": "The takeaways are mostly clear, not well-known, and interesting.",
            "3": "Writing is mostly clear.",
            "4": "**Main Point**\nSensitivity analysis on prompts and parameters is not provided.",
            "5": "For instance, temperature can be an important factor in generating factual responses (as in the studied case).",
            "6": "Similarly, LLMs can be sensitive to prompts in a way that is decidedly not human-like.",
            "7": "Experimenting with a variety of different prompt formats / parameters and testing for consistency is an important, missing analysis.",
            "8": "Doing so on the entire dataset, with all LLMs, and all prompts would be challenging - but it is reasonable to check these things on a smaller subset.",
            "9": "**Similar Minor Points**\n - Similarly to the above, but a more minor point, some sensitivity analysis on the answer mapping would be nice.",
            "10": "For instance, your error analysis in the appendix indicates trouble with the \"AIDS virus\" - this may be caused by AI safety features.",
            "11": "- There appears to be a general lack of analysis on statistical significance, which can be important when interpreting many results.",
            "12": "This may cause the authors/readers to draw false positive inferences.",
            "13": "I do consider this a minor point, since most takeaways are not inferred by \"close calls\" - most discussed differences in accuracy, for instance, are substantial.",
            "14": "- Some results may be \"over stated\", for instance, the gap between ChatGPT performance and average human performance is < 20%.",
            "15": "Arguably, this is the type of variability one might expect from humans of varying experiences/education levels.",
            "16": "*Post-rebuttal Revisions*\nThe main points are addressed by authors during the rebuttal, and authors have agreed to include these in the paper (Appendix with pointers in the main text is fine)."
        },
        "GzvOg1UIiz": {
            "0": "Interesting, simple, and focused (size comparison) approach to probe real-world understanding in LLMs.",
            "1": "The POSQA dataset is the main contribution of the paper.",
            "2": "The paper lays out the methodology and evaluation metrics in detail to systematically test the LLMs.",
            "3": "The results are relevant and interesting.",
            "4": "They show that LLMs are still far from humans in real-world understanding.",
            "5": "Moreover, model’s preference between context and weights is an important finding and can potentially lead to interesting future investigations.",
            "6": "The results might not be very general for actual “real-world understanding” of LLMs.",
            "7": "Size comparison is still just one part of the bigger picture about real-world understanding.",
            "8": "The link to the dataset is not active.",
            "9": "So, the claims made about the dataset can’t be verified.",
            "10": "I would request the authors to look into it.",
            "11": "No details about the human annotation process used for comparison with ChatGPT."
        }
    },
    "Bou2YHsRvG": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel method, Word Sense Pretraining for Neural Machine Translation (WSP-NMT), which leverages word sense-specific information from Knowledge Bases (KBs) for pretraining multilingual NMT models.",
            "1": "This approach addresses the significant challenge of lexical ambiguity in NMT, which is a well-known issue in the field.",
            "2": "**Comprehensive Evaluation**: The authors conduct extensive experiments to evaluate the effectiveness of WSP-NMT.",
            "3": "They show significant improvements in translation quality across multiple metrics (spBLEU, chrF++, and COMET22) and various language pairs.",
            "4": "The robustness of the approach is also tested in different data and resource-scarce scenarios, providing a thorough understanding of its performance.",
            "5": "**Detailed Analysis**: The paper provides a detailed analysis of the impact of different components of the proposed method, such as the quality of the WSD system and the inclusion of morphological inflections.",
            "6": "This helps in understanding the contributions of each component to the overall performance improvements.",
            "7": "**Fine-Grained Evaluation**: The authors also evaluate the disambiguation capabilities of their models on the DiBiMT benchmark, providing fine-grained accuracy improvements, especially in verb disambiguation.",
            "8": "This highlights the effectiveness of WSP-NMT in handling highly polysemous words.",
            "9": "**Practical Applications**: The paper discusses potential applications of WSP-NMT in domain-specific translation tasks, such as news, healthcare, and e-commerce, where minimizing disambiguation errors is crucial.",
            "10": "This practical perspective adds value to the research.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Resource Dependency**: The effectiveness of WSP-NMT heavily relies on the availability and quality of external resources such as WSD systems and Knowledge Bases.",
            "13": "For under-represented languages, where such resources are scarce or of low quality, the proposed method may not yield significant improvements.",
            "14": "This limitation is acknowledged by the authors but remains a critical challenge.",
            "15": "**Computational Overhead**: Incorporating WSD and generating morphological inflections add computational overhead to the pretraining process.",
            "16": "While the authors provide a comparison of running times for different WSD systems, the overall increase in computational cost for large-scale pretraining is not thoroughly discussed.",
            "17": "**Bias in Knowledge Bases**: The paper mentions the potential for gender bias in sense translations chosen from BabelNet.",
            "18": "Although random sampling is used to mitigate this risk, a more principled approach to handle such biases is not provided.",
            "19": "This could be an area for future research.",
            "20": "**Limited Scope of Evaluation**: While the paper provides a comprehensive evaluation for Romance languages and some Indo-Iranian languages, the scope of evaluation could be expanded to include more diverse language families.",
            "21": "This would help in understanding the generalizability of the proposed method across different linguistic typologies.",
            "22": "**Complexity of Implementation**: The proposed method involves multiple stages, including WSD, extraction of translations from BabelNet, and generation of morphological inflections.",
            "23": "This complexity might pose challenges for implementation and integration into existing NMT pipelines, especially for practitioners with limited resources.",
            "24": "#### Conclusion\n\nOverall, the paper presents a significant advancement in addressing lexical ambiguity in NMT through the innovative use of word sense-specific information from Knowledge Bases.",
            "25": "The comprehensive evaluation and detailed analysis provide strong evidence of the effectiveness of WSP-NMT.",
            "26": "However, the dependency on external resources, computational overhead, and potential biases in Knowledge Bases are important considerations that need to be addressed in future work.",
            "27": "Expanding the scope of evaluation and simplifying the implementation process could further enhance the impact and applicability of this research."
        },
        "drqrL0wzP7": {
            "0": "The quality of writing is high and reader-friendly.",
            "1": "The paper is well-organized.",
            "2": "The idea presented is logical and easy to understand.",
            "3": "Comprehensive experiments are conducted to verify the effectiveness of the proposed method in different scenarios, such as different language pairs across high, medium, and low resources.",
            "4": "For some experimental results, there is a lack of reasonable and sufficient explanation.",
            "5": "For instance, in figure 3, the authors' method underperforms the baseline in the en-fr and fr-en settings.",
            "6": "The reason and analysis for this are missing.",
            "7": "A majority of the experiments focus on the presentation of results.",
            "8": "The analyses of the method itself and the experimental outcomes are not comprehensive enough.",
            "9": "Given that the authors' method underperforms the baseline in some instances, one might question to what extent the performance improvement brought by this pretraining method can be attributed to the authors' claim of \"moving code-switched pretraining from the word level to the sense level, by leveraging word sense-specific information from Knowledge Bases\"."
        },
        "rY9db7Mnsq": {
            "0": "See the main contributions.",
            "1": "See the weaknesses in Paper Topic And Main Contributions."
        },
        "dva6eNI0YY": {
            "0": "The motivation of this paper is clear.",
            "1": "- The authors target the core weakness of the existing code-switched pre-training strategy, which struggles to address lexical ambiguity by generating noise with sense-agnostic lexicons.",
            "2": "The authors show their achievements in resolving lexical ambiguity effectively.",
            "3": "- It's especially noteworthy for the performance gains on verb inflections, which are difficult to disambiguate due to the plethora of polysemous words.",
            "4": "The experimental configuration in terms of high, medium, and low resource settings slightly departs from other studies.",
            "5": "- The authors set 100\\~200K, 500K\\~1M, 1.5M\\~3M as low, medium, and high resource settings based on parallel corpus size.",
            "6": "However, in the PC32 dataset paper [2] used by the previous work [1], which the authors follow the experimental setting, much larger datasets were used as rich-source.",
            "7": "Considering Figure 3 and 5 (if x-axis is scaled to a huge size of dataset), it might be unable to beat AA [1] in high-resource setting.",
            "8": "The applicability of WSP-NMT is questionable.",
            "9": "- The authors claim that WSP-NMT outperforms on low-resource language pairs.",
            "10": "Since many data-scarce language pairs are under-represented in WSD models and BabelNet, there might be several situations where it is difficult to utilize WSP-NMT.",
            "11": "[2] Lin et al., 2020, Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information"
        }
    },
    "I13VHLJjLO": {
        "llm_review": {
            "0": "**Innovative Approach**: The introduction of Reward-Augmented Decoding (RAD) is a novel approach to controlled text generation.",
            "1": "By leveraging a unidirectional reward model, the authors have managed to create a method that is both efficient and effective in steering language models towards generating text with desired attributes.",
            "2": "**Efficiency**: One of the standout features of RAD is its efficiency.",
            "3": "The use of a unidirectional reward model allows for caching of activations from prior generation steps, significantly reducing computational overhead.",
            "4": "This is particularly important when dealing with large language models, where computational costs can be prohibitive.",
            "5": "**Performance**: The experimental results demonstrate that RAD performs exceptionally well in generating non-toxic and sentiment-controlled text.",
            "6": "It outperforms other weighted decoding methods and matches the performance of state-of-the-art methods that involve re-training the language model.",
            "7": "This is a significant achievement, as it shows that RAD can achieve high-quality results without the need for expensive additional training.",
            "8": "**Scalability**: The paper shows that RAD is effective even when applied to very large language models, such as the LLaMA family with up to 65B parameters.",
            "9": "This scalability is crucial for real-world applications where large models are often used.",
            "10": "**Comprehensive Evaluation**: The authors have conducted thorough experiments to validate the effectiveness of RAD.",
            "11": "They have evaluated it on standard detoxification and sentiment-controlled generation tasks, providing a detailed comparison with various baseline methods.",
            "12": "The use of multiple metrics (toxicity, fluency, diversity) ensures a well-rounded evaluation of the method's performance.",
            "13": "**Minimal Computational Overhead**: The paper highlights that RAD introduces minimal computational overhead when applied to large language models.",
            "14": "This is a significant advantage, as it means that RAD can be used in practice without incurring substantial additional costs.",
            "15": "**Weaknesses:**\n\n1.",
            "16": "**Limited Scope of Experiments**: While the experiments on detoxification and sentiment-controlled generation are comprehensive, the paper could benefit from exploring additional controlled text generation tasks.",
            "17": "This would provide a more complete picture of RAD's capabilities and limitations.",
            "18": "**Memory Overhead**: Although the paper addresses the computational efficiency of RAD, it does not provide a detailed analysis of the memory overhead associated with caching activations.",
            "19": "This could be a potential limitation, especially when dealing with very large models and long sequences.",
            "20": "**Dependency on Reward Model Quality**: The effectiveness of RAD is heavily dependent on the quality of the reward model.",
            "21": "If the reward model is not well-trained or does not accurately capture the desired attributes, the performance of RAD could be compromised.",
            "22": "The paper could discuss this dependency in more detail and provide guidelines for training effective reward models.",
            "23": "**Tokenization Mismatch**: The paper mentions that the performance gap between RAD on GPT-2 Large and RAD on LLaMA may be attributed to differences in tokenization.",
            "24": "This is an important consideration, and the paper could benefit from a more in-depth discussion on how to address tokenization mismatches when applying RAD to different language models.",
            "25": "**Ethical Considerations**: While the paper touches on the ethical implications of controlled text generation, it could provide a more detailed discussion on the potential risks and ethical considerations associated with using RAD.",
            "26": "This would be particularly relevant for applications involving sensitive or controversial content.",
            "27": "**Future Work**: The paper briefly mentions future work, such as applying RAD to more sophisticated tasks like instruction following.",
            "28": "However, it could provide a more detailed roadmap for future research directions, including potential improvements to the RAD method and its application to other domains.",
            "29": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of controlled text generation with the introduction of Reward-Augmented Decoding.",
            "30": "The method's efficiency, scalability, and strong performance make it a valuable contribution.",
            "31": "However, there are areas for improvement, particularly in expanding the scope of experiments, addressing memory overhead, and discussing ethical considerations in more detail.",
            "32": "Despite these weaknesses, the paper provides a solid foundation for future research and practical applications of controlled text generation."
        },
        "i3u19r36zk": {
            "0": "- The proposed method is superior both in performance and decoding\n  speed in comparison with other weighted decoding methods and\n  re-training methods.",
            "1": "- The paper is relatively easy to follow.",
            "2": "- It is not clear why the proposed method achieves better performance\n  (e.g., average max toxicity) in comparison with other weighted\n  decoding methods and achieves results comparable to re-training\n  methods.",
            "3": "Discussion is required for this point."
        },
        "l19ZKksWUM": {
            "0": "* The proposed method is intuitive and sound.",
            "1": "Moreover, it is easy to implement and does not cost excessive computation resources.",
            "2": "* The experiments show that the method is effective.",
            "3": "* The description of the method is not clear.",
            "4": "Specifically, the term $\\hat{r}$ in Section 2.2 is not clearly explained.",
            "5": "The source of this label is questionable.",
            "6": "* The contribution of the paper is thin.",
            "7": "Essentially, this is a combination of the well-known re-ranking technique and the now popular reward models.",
            "8": "Therefore, it does not provide interesting technical insights."
        },
        "A11XlBZPCH": {
            "0": "The proposed method is simple yet effective and thus should influence subsequent research in controlled text generation.",
            "1": "The proposed method achieves a good trade-off between efficiency and effectiveness compared to previous weighted decoding and fine-tuning methods.",
            "2": "The results of the experiment are reliable because of the extensive comparison with previous models and the recalculation of the scores of previous models based on nice attention to the differences in API.",
            "3": "I found no major reasons for rejection, but it would be more insightful to explain why the proposed method performs better than other weighted decoding methods."
        }
    },
    "ZSHcpMXWxX": {
        "llm_review": {
            "0": "**Comprehensive Benchmark Creation:**\n   - The authors have created a well-rounded benchmark, DUMB, which includes nine diverse tasks for evaluating Dutch language models.",
            "1": "This includes four tasks that were previously unavailable in Dutch, thus filling a significant gap in the resources available for Dutch NLP.",
            "2": "**Introduction of Relative Error Reduction (RER):**\n   - The introduction of the RER metric is a novel approach to comparing model performance across tasks.",
            "3": "This method provides a more nuanced understanding of model improvements relative to a strong baseline, which is particularly useful when dealing with tasks of varying difficulty and metrics.",
            "4": "**Thorough Evaluation:**\n   - The paper evaluates 14 pre-trained language models, including monolingual, multilingual, and English models of varying sizes.",
            "5": "This extensive evaluation provides a comprehensive view of the current state of Dutch language modeling and highlights areas for improvement.",
            "6": "**Public Leaderboard and Open Source:**\n   - The establishment of a public leaderboard and the availability of the benchmark and reference model source code on GitHub promote transparency and encourage further research and development in Dutch NLP.",
            "7": "**Detailed Analysis:**\n   - The paper provides a detailed analysis of the performance of different models, considering factors such as model type, size, and pre-training language.",
            "8": "This analysis is valuable for understanding the strengths and weaknesses of current models and for guiding future model development.",
            "9": "**Encouragement for Future Research:**\n   - By identifying the limitations of current Dutch models and suggesting directions for potential developments, the paper encourages further research and the creation of larger and more effective Dutch language models.",
            "10": "**Weaknesses:**\n\n1.",
            "11": "**Limited Task Types:**\n   - While the benchmark includes a diverse set of tasks, it does not cover every possible NLP task type.",
            "12": "For instance, parsing tasks are excluded, and some tasks are simplified to classification tasks.",
            "13": "This might limit the benchmark's comprehensiveness in evaluating all aspects of language understanding.",
            "14": "**RER Metric Comparability:**\n   - The RER metric, while useful within the context of this benchmark, may not be directly comparable to aggregate scores from other benchmarks.",
            "15": "This could pose challenges when trying to compare the performance of models across different benchmarks.",
            "16": "**Generative Models Exclusion:**\n   - The benchmark and evaluation focus on Transformer-encoder models and do not include generative language models.",
            "17": "This exclusion might limit the applicability of the benchmark to a broader range of model architectures, especially as generative models become more prevalent in NLP.",
            "18": "**Model Availability and Comparison:**\n   - The paper includes more English models than Dutch models, which might skew the comparison.",
            "19": "Additionally, the absence of large-sized Dutch models limits the ability to fully evaluate the potential of Dutch language models compared to their multilingual and English counterparts.",
            "20": "**Resource Intensive:**\n   - The extensive hyper-parameter search and multiple runs for each model and task make the evaluation process resource-intensive.",
            "21": "This might pose challenges for researchers with limited computational resources.",
            "22": "**Licensing and Data Availability:**\n   - Some datasets used in the benchmark have licensing restrictions that prevent direct redistribution.",
            "23": "While the authors provide methods to obtain and preprocess these datasets, this adds an extra step for researchers and might limit accessibility.",
            "24": "**Conclusion:**\n\nOverall, the paper makes significant contributions to the evaluation of Dutch language models by introducing a comprehensive benchmark and a novel evaluation metric.",
            "25": "The detailed analysis and public resources provided are valuable for the NLP community.",
            "26": "However, the benchmark's limitations in task coverage, model comparability, and resource requirements should be addressed in future work to enhance its applicability and accessibility."
        },
        "SRTuh5IrR8": {
            "0": "The paper presents a thorough comparison of monolingual and multilingual Dutch pre-trained and non-Dutch pre-trained models across 9 different tasks.",
            "1": "The paper presents a new metric for performance comparison across different tasks.",
            "2": "An analysis based on current results is presented to support the nature of pre-training that could improve performance on this task for future work.",
            "3": "L535-536: It is unclear why RER correlation is a more intuitive way to compare performance across tasks and what it actually measures (missing an intuitive explanation rather than just comparing the numbers).",
            "4": "Also to clarify, it is clear what RER captures but not what the correlation does.",
            "5": "A regression analysis is presented to identify and reason why non-dutch models outperform dutch-pre-trained models, however while the analysis presents some insights on the nature of pretraining and model size, it is unclear why and how the choice of pretraining language would play a role."
        },
        "cxBZ2ms5fP": {
            "0": "The work presented in this paper is relevant to the further development of Dutch and multi-lingual LMs.",
            "1": "The benchmark is well constructed, tested quite extensibly with existing LMs and the results reveal directions for further research.",
            "2": "The paper is well written and has a logical structure.",
            "3": "I currently do not see any reasons to reject this paper."
        },
        "OVOUZPRAZW": {
            "0": "The benchmark datasets are an interesting contribution to the analysis of the Dutch language.",
            "1": "The paper includes extensive experimentation and meaningful analysis of existing models according to the tasks.",
            "2": "The inclusion of the Abusive Language Detection is of particular importance, as it is not included in GLUE or similar benchmarks.",
            "3": "The work might be significant only to researchers working on Dutch, but even then other researchers might find interesting ideas in this work to apply to other languages."
        }
    },
    "I5hTganf3z": {
        "llm_review": {
            "0": "**Novelty and Relevance**: The paper introduces VECHR, a novel dataset specifically designed for the classification of vulnerability types in the European Court of Human Rights (ECtHR) cases.",
            "1": "This is a significant contribution as it addresses a previously unexplored area in Natural Language Processing (NLP) and legal research.",
            "2": "**Expert-Annotated Dataset**: The dataset is annotated by legal experts, ensuring high-quality and reliable annotations.",
            "3": "This is crucial for the legal domain where the interpretation of texts requires specialized knowledge.",
            "4": "**Comprehensive Benchmarking**: The authors provide a thorough benchmarking of state-of-the-art models, including BERT, CaselawBERT, LegalBERT, Longformer, and hierarchical models.",
            "5": "This comprehensive evaluation helps in understanding the performance and limitations of current models in this specific task.",
            "6": "**Explainability and Robustness**: The paper not only focuses on classification performance but also on the explainability and robustness of the models.",
            "7": "The introduction of VECHR explain and VECHR challenge datasets for token-level explanation and out-of-domain robustness testing, respectively, adds significant value to the research.",
            "8": "**Detailed Analysis**: The paper provides a detailed analysis of the dataset, including label distribution, inter-annotator agreement, and the challenges posed by the task.",
            "9": "This thorough analysis helps in understanding the complexity and nuances of the dataset.",
            "10": "**Concept-aware Hierarchical Model**: The introduction of a concept-aware hierarchical model to improve robustness against distributional shifts is a novel approach.",
            "11": "This model shows promise in handling the challenges posed by the dataset.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Dataset Size**: The dataset, although expert-annotated, is relatively small with only 1,070 cases.",
            "14": "This limited size may affect the generalizability of the models trained on it.",
            "15": "The authors acknowledge this limitation but do not provide a clear path forward for scaling the dataset.",
            "16": "**Single Annotation per Case**: Each case is annotated by a single expert, which may introduce bias and limit the reliability of the annotations.",
            "17": "The paper mentions the importance of multiple annotations but does not implement this due to high workload and expertise requirements.",
            "18": "**Imbalance in Label Distribution**: The dataset exhibits a strong label imbalance, with some vulnerability types being significantly underrepresented.",
            "19": "This imbalance can affect the performance of the models and may require specialized techniques to handle it effectively.",
            "20": "**Explainability Evaluation**: The evaluation of explainability using Cohen’s Kappa score shows limited agreement between models and experts.",
            "21": "While this highlights the challenging nature of the task, it also indicates that current explainability methods may not be sufficient for this domain.",
            "22": "**Environmental Impact**: The paper briefly mentions the environmental impact of training models but does not provide detailed information on the computational resources used.",
            "23": "Given the increasing concern about the carbon footprint of AI research, a more detailed analysis would be beneficial.",
            "24": "**Ethical Considerations**: While the paper addresses ethical considerations, it could benefit from a more in-depth discussion on the potential misuse of the models and the implications of labeling individuals as vulnerable.",
            "25": "This is particularly important in the legal domain where the stakes are high.",
            "26": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the field of legal NLP by introducing a novel dataset and addressing the challenges of vulnerability classification in ECtHR cases.",
            "27": "The strengths of the paper lie in its novelty, expert-annotated dataset, comprehensive benchmarking, and focus on explainability and robustness.",
            "28": "However, the limited dataset size, single annotation per case, label imbalance, and challenges in explainability evaluation are notable weaknesses.",
            "29": "Addressing these limitations in future work could further enhance the impact and applicability of this research."
        },
        "iqRUNYla4e": {
            "0": "R1: It constructs a dataset for an important task.",
            "1": "R2: The dataset is constructed very well, with details provided on annotation agreement, annotators’ backgrounds, etc.",
            "2": "R3: The paper is very well written.",
            "3": "I learned some things about how to write a good appendix for a data resource paper.",
            "4": "Even though many pages of the appendix are used, it is written very cleverly.",
            "5": "Readers who are not reviewers or want to reproduce don’t need to read the appendix.",
            "6": "This is particularly hard to do for a short paper.",
            "7": "Kudos for that.",
            "8": "R4: Good well-thought effort is put into writing the limitations and ethics statement section.",
            "9": "It would be sad to reject such a good paper for just this reason but I will state it so they can improve it.",
            "10": "Hyperparameter information for fine-tuning like batch size, and learning rate is missing.",
            "11": "Also, what version of BERT is used is missing.",
            "12": "It is better to specify a base or large, cased or uncased, and other details for each model."
        },
        "VQX5bql8EG": {
            "0": "**Novel Dataset Creation**: The introduction of the VECHR dataset is the main contribution.",
            "1": "It fills a research gap by providing an expert-annotated multi-label dataset for vulnerability type classification, a previously unexplored area in NLP.",
            "2": "**Explainability Focus**: By including explanation rationale in the dataset, the paper aligns with the growing interest in explainable AI.",
            "3": "This aspect allows for more interpretable models, crucial for applications in legal and social contexts.",
            "4": "**Interdisciplinary Relevance**: The paper's focus on human rights and legal applications showcases the breadth of NLP applicability, bridging the gap between technology and social sciences.",
            "5": "**Limited Generalizability**: The focus on a very specific legal context (ECtHR) may limit the generalizability of the findings and the applicability of the dataset to other legal systems or vulnerability assessments.",
            "6": "A broader analysis or comparison with other contexts might have enhanced the paper's value.",
            "7": "**Distribution of content:** For a work on a dataset, specific definitions of data types, the data collection process, and detailed distribution of the dataset are crucial.",
            "8": "However, these contents are placed in the appendix, and there is a lack of necessary analysis in the main body."
        },
        "aTp91IkKP8": {
            "0": "This paper provides the dataset VECHR, which can bridge the NLP tools with experts in efficiently classifying and analyzing vulnerability, where vulnerability is important in ECtHR.",
            "1": "This dataset is small, especially the annotation scale of explanation rationale.",
            "2": "Due to the small size of the evaluation data, the evaluation results may be unstable.",
            "3": "This paper lacks the performance of LLM (such as LLaMA 13B, chatGPT) on this dataset."
        },
        "N3KSgD4g5A": {
            "0": "First of all, the paper is very well motivated, presented and written.",
            "1": "It was worth a read.",
            "2": "It is true that datasets are scarce in the legal domain.",
            "3": "Given the challenges and intricacies of the domain, NLP models are still far away from producing trustworthy explainable results which could be directly consumed without human intervention.",
            "4": "In this regard, the proposed dataset is a valuable contribution.",
            "5": "It is additionally novel given the task of vulnerability type analysis from European legal cases.",
            "6": "The paper proposes multiple test sets to evaluate various facets of compared models.",
            "7": "In this regard, the thought process of the authors and their efforts towards getting multiple test sets annotated is noteworthy.",
            "8": "It adds value to the paper.",
            "9": "Data collection and annotation steps are thoroughly explained with appropriate details.",
            "10": "Although I found certain discrepancies.",
            "11": "Please refer to my questions below.",
            "12": "Despite being a short paper, the authors have made sure to include necessary details in the appendix as well.",
            "13": "While more recent large language models could have been tried out, given that this is primarily a dataset and benchmarking paper, I am satisfied with the experiments.",
            "14": "All compared models have fairly low scores, which gives ample scope to future researchers to come up with better NLP models over time.",
            "15": "Finally, limitations of the work are clearly mentioned.",
            "16": "Although I am inclined towards accepting the paper, I have a few concerns:\n1.",
            "17": "Some of the dataset statistics reported in Section 4 and appendix I do not seem to correlate.",
            "18": "The dataset size is small with close to 50% of the documents not labelled with any vulnerability label.",
            "19": "Additionally, only 2 labels dominate the overall label distribution.",
            "20": "This raises questions on the dataset quality as well the scope for future models to perform well on this dataset.",
            "21": "At least one generative model could have been tried out leveraging the descriptions of vulnerability types to improve the classification performance.",
            "22": "Description of the proposed \"Concept-aware Hierarchical\" model is not clear."
        }
    },
    "D4Cb4gAWro": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method, TextReact, which integrates text retrieval with predictive chemistry models.",
            "1": "This approach is innovative as it leverages unstructured textual data from scientific literature to enhance the performance of chemoinformatics models.",
            "2": "**Empirical Validation**: The authors provide comprehensive empirical validation of their method on two important chemistry tasks: reaction condition recommendation and one-step retrosynthesis.",
            "3": "The results demonstrate significant improvements over state-of-the-art models that rely solely on molecular data.",
            "4": "**Detailed Methodology**: The paper offers a detailed explanation of the TextReact framework, including the SMILES-to-text retriever and the text-augmented predictor.",
            "5": "The use of contrastive learning for training the retriever and the incorporation of an auxiliary masked language model (MLM) objective in the predictor training are well-explained and justified.",
            "6": "**Generalization to New Tasks**: The authors show that TextReact can generalize to new chemistry tasks and unseen reaction classes, which is a significant advantage for practical applications in the field of chemistry.",
            "7": "**Robustness and Ablation Studies**: The paper includes robustness checks and ablation studies that highlight the importance of different components of the model, such as the MLM loss and the pretraining of the encoder.",
            "8": "These studies provide a deeper understanding of the model's performance and its reliance on various factors.",
            "9": "**Public Availability of Code and Data**: The authors have made their code and data publicly available, which promotes transparency and reproducibility in research.",
            "10": "This also allows other researchers to build upon their work.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Limited Task Scope**: While the paper demonstrates the effectiveness of TextReact on two chemistry tasks, it would be beneficial to see its application to a broader range of tasks within the field of chemistry.",
            "13": "This would provide a more comprehensive evaluation of the method's versatility and robustness.",
            "14": "**Dependence on Text Corpus Quality**: The performance of TextReact heavily relies on the quality and comprehensiveness of the text corpus used for retrieval.",
            "15": "If the corpus lacks relevant information or contains noisy data, the model's performance may degrade.",
            "16": "The paper does not thoroughly address how to handle such scenarios.",
            "17": "**Computational Complexity**: The method involves training both a retriever and a predictor, which can be computationally intensive.",
            "18": "The paper mentions the potential for joint optimization of the retriever and predictor but does not explore this avenue.",
            "19": "A discussion on the computational requirements and potential optimizations would be valuable.",
            "20": "**Evaluation on Time-Split Data**: While the paper includes a time-split evaluation to test generalization to newer data, the performance drop in this setting indicates challenges in handling out-of-distribution data.",
            "21": "Further analysis and potential solutions to improve performance in time-split scenarios would strengthen the paper.",
            "22": "**Scalability to Larger Datasets**: The paper does not discuss the scalability of TextReact to larger datasets or more complex chemical reactions.",
            "23": "As the size of the text corpus and the number of reactions increase, the efficiency and effectiveness of the retrieval and prediction components need to be evaluated.",
            "24": "**Interpretability of Predictions**: The paper focuses on the accuracy of predictions but does not address the interpretability of the model's outputs.",
            "25": "Understanding why certain texts were retrieved and how they influenced the predictions would be valuable for users in the chemistry domain.",
            "26": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of predictive chemistry by integrating text retrieval with chemoinformatics models.",
            "27": "The innovative approach, detailed methodology, and strong empirical results are commendable.",
            "28": "However, addressing the limitations related to task scope, corpus quality, computational complexity, and scalability would further enhance the impact and applicability of the proposed method."
        },
        "wSedisjIVh": {
            "0": "The paper is easy to follow and is nicely read in most parts.",
            "1": "The proposed retriever may benefit other practitioners and researchers in the field of chemistry if it is open-sourced.",
            "2": "The effectiveness of the proposed model has not been sufficiently demonstrated because state-of-the-art models were not compared in the experiments.",
            "3": "In the one-step retrosynthesis task, while several previous models exhibit superior performance than the performance of the proposed model, they were not considered in the conducted experiments (Table 4).",
            "4": "Please review previous studies [1,2] and this link [3] to ensure that essential baselines have not been overlooked.",
            "5": "If the authors feel that it is unfair to compare the previous best models I have mentioned with the proposed model (i.e., TextReact), please reply during the author response period.",
            "6": "I will change the initial score if the responses resolve my concerns.",
            "7": "[1] Zhu et al., O -GNN: incorporating ring priors into molecular modeling, ICLR 2023\n\n[2] Seidl et al., Improving few-and zero-shot reaction template prediction using modern hopfield networks, Journal of chemical information and modeling, 2022\n\n[3] https://paperswithcode.com/sota/single-step-retrosynthesis-on-uspto-50k"
        },
        "8wFPGhHLj7": {
            "0": "TextReact allows for accurate prediction of reaction properties with minimal overhead.",
            "1": "This work will likely inspire novel research into retrieval augmented reaction prediction.",
            "2": "The paper does not study any additional methods but bi-encoders.",
            "3": "The contribution would significantly be improved by integrating a cross-encoder or sparse retriever like BM25.",
            "4": "It is unclear how the model performs in more difficult molecular reactions where there can be one, many or no catalysts or solvents\n3.",
            "5": "Retrieval metrics are missing such as recall@100"
        },
        "aUpPgoL8XW": {
            "0": "The experiments provide strong support for their claim, achieving more than twice as high accuracy compared to the best baseline ChemBERTa.",
            "1": "The paper further considers two different types of train/test splits (random and time-based).",
            "2": "The advantage of the proposed method is confirmed in both scenarios.",
            "3": "Claims are further supported through careful ablations, such as testing a setting where the gold texts are removed and a setting where only the historical corpus is used in the time split setting.",
            "4": "The method still shows a solid advantage over a Transformer baseline (which was ~on par with ChemBERTa in other experiments).",
            "5": "The paper could be further improved by analyzing the influence of SMILES input vs. retrieved text input.",
            "6": "Given the comparison with the literature that only uses SMILES input, it seems that retrieved text provides an extremely rich source of information.",
            "7": "It leads to the question how a model would perform only based on retrieved text, if feasible.",
            "8": "The paper would benefit from a discussion of how useful such models are in practice (see Question)."
        }
    },
    "9K1urVN7ti": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces DueT, a novel transfer learning method that leverages dual-adapter tuning for vision and language models.",
            "1": "This approach is innovative as it incorporates adapters with a gating mechanism into both image and text encoders, which are initialized using pre-trained uni-modal models and then frozen.",
            "2": "This method effectively addresses the challenge of catastrophic forgetting while enabling efficient learning with a reduced number of trainable parameters.",
            "3": "**Parameter Efficiency**: One of the key strengths of DueT is its parameter efficiency.",
            "4": "By training only the adapters and keeping the main encoders frozen, DueT significantly reduces the number of trainable parameters compared to traditional fine-tuning methods.",
            "5": "This is particularly beneficial in scenarios with limited computational resources.",
            "6": "**Performance**: The experimental results demonstrate that DueT outperforms several baseline methods, including simple fine-tuning, LiT (Locked-image Tuning), and LoRA-based adapter methods, in terms of accuracy and parameter efficiency for zero-shot image and text retrieval tasks in both English and Japanese domains.",
            "7": "This highlights the effectiveness of the proposed method in transferring and connecting knowledge from pre-trained uni-modal encoders.",
            "8": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of DueT across multiple datasets and tasks, including zero-shot image classification and image-text retrieval.",
            "9": "The inclusion of both English and Japanese datasets further strengthens the validity of the results and demonstrates the method's applicability to different languages and domains.",
            "10": "**Detailed Analysis**: The paper includes detailed ablation studies and analysis of the gating mechanism, the number of training parameters, and the impact of different pre-trained models.",
            "11": "This comprehensive analysis provides valuable insights into the factors contributing to the success of DueT and helps to understand the trade-offs involved.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Scope of Evaluation**: While the paper evaluates DueT on zero-shot image classification and image-text retrieval tasks, it does not explore other potential applications of vision and language models, such as visual question answering, image captioning, or object detection.",
            "14": "Including these tasks could provide a more comprehensive assessment of the method's versatility and effectiveness.",
            "15": "**Comparison with Contemporaneous Work**: The paper mentions LilT, a contemporaneous study with a similar approach, but does not provide a direct experimental comparison.",
            "16": "A head-to-head comparison with LilT would have strengthened the paper's claims about the superiority of DueT.",
            "17": "**Complexity of Implementation**: The introduction of gated adapter units (GAUs) adds complexity to the model architecture.",
            "18": "While the paper provides a detailed description of the GAUs, the implementation details might be challenging for practitioners to reproduce without additional guidance or code availability.",
            "19": "**Effectiveness in Other Languages**: The paper demonstrates the effectiveness of DueT in English and Japanese, but it remains unclear how well the method would perform in other languages, especially those with different linguistic structures or limited pre-trained resources.",
            "20": "Further experiments on a diverse set of languages would be beneficial.",
            "21": "**Scalability**: Although DueT is shown to be parameter-efficient, the paper does not discuss the scalability of the method in terms of training time and computational resources required for very large datasets or models.",
            "22": "Providing insights into the scalability aspects would help practitioners understand the practical implications of adopting DueT.",
            "23": "**Conclusion:**\n\nOverall, the paper presents a significant contribution to the field of vision and language models by introducing DueT, a parameter-efficient transfer learning method that effectively leverages pre-trained uni-modal encoders.",
            "24": "The comprehensive evaluation and detailed analysis provide strong evidence of the method's effectiveness.",
            "25": "However, addressing the mentioned weaknesses, such as expanding the scope of evaluation and providing a direct comparison with contemporaneous work, would further strengthen the paper's impact and applicability."
        },
        "F83HmFp575": {
            "0": "- The proposed method is simple and effective.",
            "1": "It is technically valid and can enhance the image-text contrast learning framework.",
            "2": "- The authors describe their network design in detail, including the architecture of the proposed gated adapter unit.",
            "3": "- The authors considered an array of design choices for the baselines, and provided clear comparisons to demonstrate the effectiveness of DueT.",
            "4": "- Large-scale pre-training is conducted to validate the idea, including pre-training on public (YFCC) and private (JWeb) dataset, respectively.",
            "5": "- The paper is well-written and easy to understand.",
            "6": "I don't find serious concerns.",
            "7": "The only minor concern is that adding GAUs would also increase training parameters, from 1% to 58%, depending on the hyper-parameter.",
            "8": "But considering that all other layers are frozen, the increased num parameters are acceptable."
        },
        "ZhYTsthpTl": {
            "0": "- The paper does a good job of applying ideas from parameter-efficient fine-tuning tuning (PEFT) methods to the problem of image-text contrastive learning.",
            "1": "The results demonstrate the effectiveness of the proposed solution, evidenced by the strong results on zero-shot retrieval and image classification.",
            "2": "- Experiments are conducted not only on the standard English benchmarks, but also on Japanese datasets.",
            "3": "It is rare to come across a multimodal method that shows experiments on languages beyond English, so it's great to see a non-English language represented in the experiments.",
            "4": "- The methodology section is extremely well structured: 3.1 explains all the background well, 3.2 explains the GAU method well, and 3.3 motivates all the design choices very well, effectively explaining how different design choices correspond to different learning settings (L347-L359).",
            "5": "Everything was well-written and extremely easy to follow.",
            "6": "- The various ablation studies in Section 4.4 are also very well-motivated, and experiments are designed accordingly -- specifically, how the size of the bottleneck layer affects the model's learning ability, the utility of GAUs in different layers, the analysis of gate values at different layers, the effect of gate initialization, and effect of amount of pre-training data.",
            "7": "All of these are very interesting questions that have been addressed in the ablation study.",
            "8": "There are no real reasons to reject this work -- some additional details would be useful (mentioned in the Questions for Authors section), but these are not grounds for rejecting the work in my opinion."
        },
        "nyXfKxVH0u": {
            "0": "1: The author suggested a method to use adapters with pre-trained image and text encoders.",
            "1": "This reduced the number of parameters that needed to be trained.",
            "2": "The adapters had gates that allowed effective transfer and connection of knowledge from the pre-trained encoders for each modality.",
            "3": "2: The author conducted experiments with various text and image encoders, adapter units, parameter initialization methods, and efficiency measures, following the proposed approach.",
            "4": "The results demonstrate the effectiveness of the proposed method.",
            "5": "3: The analysis of solid ablation covered various aspects, such as parameter efficiency, the effectiveness of GAUs on different layers, the performance of gates, and so on 1: Using pre-trained image and text encoders for image-text contrastive learning is a common practice, lacking originality.",
            "6": "There are many similar works in the literature.",
            "7": "2: Testing on a limited number of languages or training sets is not sufficient to support the claims"
        }
    },
    "FKNtgr0qQy": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel approach to decision-making via sequence modeling, leveraging the success of language models.",
            "1": "This is a fresh perspective that could potentially bridge the gap between language processing and embodied AI.",
            "2": "**Comprehensive Experiments**: The authors conduct thorough experiments using the BabyAI environment, which is a well-established testbed for language-conditioned navigation tasks.",
            "3": "The use of both GoToLocal and MiniBossLevel environments adds robustness to their findings.",
            "4": "**Probing Analysis**: The use of probing to investigate the emergence of abstract state representations is a strong methodological choice.",
            "5": "It provides clear evidence of the internal workings of the models and their ability to infer intermediate states.",
            "6": "**Role of Language Instructions**: The paper highlights the importance of language instructions in the emergence of state representations.",
            "7": "This is a significant finding that underscores the interplay between language and action in embodied AI.",
            "8": "**Competitive Performance**: The results show that the Missing-State model performs competitively against the Complete-State model, even without access to intermediate states.",
            "9": "This demonstrates the potential of sequence modeling in decision-making tasks.",
            "10": "**Detailed Analysis**: The paper provides a detailed analysis of the impact of different input components (states, actions, language instructions) on the emergence of state representations.",
            "11": "This thoroughness adds depth to the study and helps in understanding the underlying mechanisms.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Generalization**: While the experiments are comprehensive within the BabyAI environment, the generalization to more complex and real-world environments is not addressed.",
            "14": "Future work should explore the applicability of these findings to more diverse and challenging settings.",
            "15": "**Fixed Layout Assumption**: The fixed initial layout assumption in some experiments might limit the generalizability of the findings.",
            "16": "Real-world tasks often involve dynamic and unpredictable environments, which are not fully captured in the current setup.",
            "17": "**Complexity of MiniBossLevel**: The performance drop in the MiniBossLevel environment suggests that the current models might struggle with more complex tasks.",
            "18": "This indicates a need for further improvements in the model architecture or training procedures to handle such complexity.",
            "19": "**Absence of Baseline Comparisons**: The paper lacks comparisons with other state-of-the-art methods in embodied AI or decision-making tasks.",
            "20": "Including such comparisons would strengthen the claims about the effectiveness of the proposed approach.",
            "21": "**Probing Limitations**: While probing is a useful tool, it has its limitations.",
            "22": "The paper could benefit from discussing these limitations and considering alternative methods to validate the emergence of state representations.",
            "23": "**Scalability Concerns**: The scalability of the proposed approach to larger and more diverse datasets is not discussed.",
            "24": "This is an important aspect to consider for practical applications of the method.",
            "25": "**Conclusion:**\n\nOverall, the paper presents a compelling study on the emergence of abstract state representations in embodied sequence modeling.",
            "26": "The innovative approach, comprehensive experiments, and detailed analysis are significant strengths.",
            "27": "However, the limitations in generalization, complexity handling, and baseline comparisons need to be addressed in future work.",
            "28": "The findings have important implications for the design of decision-making models in complex environments, and the paper provides a solid foundation for further research in this area."
        },
        "LdFpfK6uRS": {
            "0": "- A novel \"blindfolded\" navigation task that directly aims to test for an agents ability to learn useful intermediate representations of the world (in the minigrid domain).",
            "1": "- A throughout introspection of probing performances in different layers of the Transformer (Figure 4).",
            "2": "- A nice ablation that shows that language instructions impact to probe performances (Table 2).",
            "3": "- The contribution of this work would be stronger when the evaluation had been done for more than a single environment in the minigrid environment.",
            "4": "That sequence models are able to learn strong world representations has been already convincingly shown in other work, most famously in the World Models paper of Hu and Schmidhuber (2018) for the Doom and CarRacing environments.",
            "5": "- I find the results (in Table 1) contradict the conclusion that \"the models can still learn to infer the intermediate states\" (L554).",
            "6": "Actually, the results show that the model seems rather sensitive to the state information that is directly passed as an input or not.",
            "7": "The Missing-State model achieves higher (non-optimal) scores on \"Board\" information than on the \"Neighbor\" information because this information is given initially and carried over to further timesteps.",
            "8": "Similarly, the Complete-State model achieves lower scores at global information than on \"Neighbor\" information because it is highly influenced by the given state at a certain timestep.",
            "9": "The more interesting baseline would have been the CNN representations.",
            "10": "- The paper could be more precise in what it actually targets to measure.",
            "11": "Of course the model learns \"some\" useful internal state representation.",
            "12": "Are these internal representations the ground truth states?",
            "13": "The \"internal representation of a state\" is only measured via the proxy of probes.",
            "14": "Are these internal representations properly grounded?",
            "15": "The reported results are not specifically related to the given instruction (Can the agent identify the targeted object?"
        },
        "uuAshh4tsn": {
            "0": "The paper is well-written and straightforward.",
            "1": "The authors found meaningful differences in terms of grounded representations depending on whether they used instructions or missing/complete state model.",
            "2": "The evaluations are relatively limited and restricted to a single environment.",
            "3": "While the authors justified the probing technique used for the evaluation, it would be interesting to have additional ways to explore the grounding (perhaps some evaluation of transfer learning or visualization of the attention layer)."
        },
        "vHnLaSYlhC": {
            "0": "The paper is timely and sheds light on how a relatively new, and potentially widely used, new RL paradigm works.",
            "1": "The contribution is easy to understand, the paper very readable, and experiments thorough (e.g., the ablation on language instructions), and the results convincing.",
            "2": "Overall, I find that the contribution of this paper is clear, straightforward, and technically sound.",
            "3": "Overall, I found this to be a strong paper with a straightforward contribution.",
            "4": "However, it is missing some references to past work and its underlying motivation can be made more convincing.",
            "5": "The underlying motivation can be clarified.",
            "6": "The section \"A debate about sequence modeling\" can be strengthened.",
            "7": "In particular, it's not obvious that the view in Bender and Koller (2020) applies to embodied sequence modeling for decision making.",
            "8": "Bender and Koller argue that, in the context of __language modeling__, form prediction without external grounding does not lead to meaning.",
            "9": "In the language modeling case, however, there is a crucial separation between what the models are trained on (text corpus; form prediction) and how language is __used__ by humans (in the real world; grounded meaning).",
            "10": "In the case of embodied sequence modeling, there is __no clear separation__ between the (pre-)training data and the use; I've understood that the sequence modeling pre-training and actual deployment of the model occur in the same environment.",
            "11": "Therefore, I would argue that, intuitively, pre-training RL agents using sequence modeling _on the same environment_ does allow them to capture meaning.",
            "12": "In addition, the Bender and Koller (2020) argument has since been refuted; meaning doesn't need grounded referents (Piantadosi & Hill, 2023), and language model representations are able to predict external, visual representations (Ilharco et al., 2021).",
            "13": "I think that by saying, e.g., \"we take Bender and Koller's view for language/pixel modeling, but test this view empirically for embodied sequence modelling\", the motivation can be cleaned up.",
            "14": "Missing some key references \n\n- [Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?",
            "15": "](https://aclanthology.org/2021.emnlp-main.166) (Qiu et al., EMNLP 2021)\n\n- [Probing Contextual Language Models for Common Ground with Visual Representations](https://aclanthology.org/2021.naacl-main.422) (Ilharco et al., NAACL 2021)\n\n- [A Benchmark for Systematic Generalization in Grounded Language Understanding](https://arxiv.org/abs/2003.05161) (Ruis et al., NeurIPS 2020)\n\n- [Representation Learning for Grounded Spatial Reasoning](https://aclanthology.org/Q18-1004) (Janner et al., TACL 2018)"
        }
    },
    "cMMxJxzYkZ": {
        "llm_review": {
            "0": "**Comprehensive Empirical Study**: The paper provides a thorough empirical investigation into the performance of large language models (LLMs) like GPT-3 and GPT-3.5 in generating empathetic responses.",
            "1": "This is a significant contribution as it benchmarks these models against state-of-the-art (SOTA) models in empathetic dialogue generation.",
            "2": "**Innovative Improvement Methods**: The authors propose three novel methods to enhance the performance of LLMs in empathetic response generation:\n   - **Semantically Similar In-Context Learning**: This method improves the selection of in-context examples based on semantic similarity, which enhances the model's understanding and response generation.",
            "3": "- **Two-Stage Interactive Generation**: This approach involves a two-step process where the model first infers the user's emotions and situation before generating a response, leading to more contextually appropriate and empathetic replies.",
            "4": "- **Combination with Knowledge Base**: By integrating external commonsense knowledge from ATOMIC20, the model can generate more informed and empathetic responses.",
            "5": "**Extensive Evaluation**: The paper employs both automatic and human evaluations to assess the performance of the proposed methods.",
            "6": "Metrics such as Distinct-n, BERTScore, BLEU, and emotion prediction accuracy are used for automatic evaluation, while human ratings and A/B tests provide qualitative insights.",
            "7": "**Human Evaluator Simulation**: The exploration of using GPT-4 to simulate human evaluators is a novel and practical approach to reduce the cost and time associated with human evaluations.",
            "8": "The results indicate a promising correlation between GPT-4 and human evaluators, especially in the aspect of empathy.",
            "9": "**Detailed Analysis and Case Studies**: The paper includes detailed analysis and case studies that illustrate the effectiveness of the proposed methods.",
            "10": "These examples help in understanding how the improvements translate into better empathetic responses.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Limited Dataset**: The study primarily relies on the EMPATHETIC DIALOGUES dataset.",
            "13": "While this is a well-known benchmark, the paper acknowledges the limitation of not using additional datasets, which could provide a more comprehensive evaluation of the methods across different contexts and domains.",
            "14": "**Cultural and Contextual Variability**: The paper does not address the variability in expressing empathy across different cultures and contexts.",
            "15": "Empathy is a complex and culturally nuanced concept, and the models might not generalize well across diverse user bases without considering these factors.",
            "16": "**Over-Reliance on Automatic Metrics**: Although the paper includes human evaluations, there is a significant reliance on automatic metrics, which may not fully capture the nuances of empathetic responses.",
            "17": "Metrics like BLEU and BERTScore, while useful, might not align perfectly with human judgments of empathy and coherence.",
            "18": "**Scalability of Improvement Methods**: The proposed methods, especially the two-stage interactive generation and the integration with a knowledge base, might introduce additional computational overhead.",
            "19": "The paper does not discuss the scalability and efficiency of these methods in real-world applications where response time is critical.",
            "20": "**Generalization to Other Dialogue Tasks**: The focus of the paper is on empathetic response generation.",
            "21": "It would be beneficial to discuss how the proposed methods could be adapted or extended to other dialogue tasks, such as persuasive or informative dialogues, to understand their broader applicability.",
            "22": "#### Conclusion:\n\nOverall, the paper makes significant contributions to the field of empathetic dialogue generation by leveraging the capabilities of large language models and proposing innovative methods to enhance their performance.",
            "23": "The comprehensive empirical study, combined with detailed analysis and case studies, provides valuable insights.",
            "24": "However, addressing the limitations related to dataset diversity, cultural variability, and scalability would further strengthen the work and its applicability in real-world scenarios."
        },
        "qiFEdpfLws": {
            "0": "(1) The experiments are thorough and demonstrate the effectiveness of the authors’ proposed ideas.",
            "1": "(2) The clarity of the paper is good.",
            "2": "The contribution is incremental.",
            "3": "The proposed ideas to augment ChatGPT have been extensively studied in prior works for other tasks.",
            "4": "For example, using semantically similar exemplars for in-context learning [2,3], CoT prompting [4,5], and using external commonsense knowledge [1].",
            "5": "The authors should highlight the motivation of applying such ideas in the context of empathetic dialogue generation, the unique challenges of empathetic dialogue generation, and why applying such ideas can help address the challenges.",
            "6": "[1] Sabour, Sahand, Chujie Zheng, and Minlie Huang.",
            "7": "\"Cem: Commonsense-aware empathetic response generation.\"",
            "8": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "9": "Vol.",
            "10": "36.",
            "11": "No.",
            "12": "10.",
            "13": "2022.",
            "14": "[2] Rubin, Ohad, Jonathan Herzig, and Jonathan Berant.",
            "15": "\"Learning To Retrieve Prompts for In-Context Learning.\"",
            "16": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.",
            "17": "2022.",
            "18": "[3] Liu, Jiachang, et al.",
            "19": "\"What Makes Good In-Context Examples for GPT-3?.\"",
            "20": "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures.",
            "21": "2022.",
            "22": "[4] Kojima, Takeshi, et al.",
            "23": "\"Large language models are zero-shot reasoners.\"",
            "24": "Advances in neural information processing systems 35 (2022): 22199-22213.",
            "25": "[5] Wei, Jason, et al.",
            "26": "\"Chain-of-thought prompting elicits reasoning in large language models.\"",
            "27": "Advances in Neural Information Processing Systems 35 (2022): 24824-24837."
        },
        "olCDJOvggz": {
            "0": "Clear discussion of problem statement and proposal of three fundamental techniques for improved empathetic response generation.",
            "1": "Improved results over several baseline methods in both automatic and human eval metrics.",
            "2": "The examples shown in paper indicate that the proposed method is powerful and could be explored further for the problem of empathetic response generation.",
            "3": "A weakness is that the evaluation could have been more through.",
            "4": "Although expected, it is unclear which components bring the improvement in performance.",
            "5": "Are the in-context example responses very similar to the target test references?",
            "6": "What happens when you try to use all three of the proposed techniques together (ChatGPT + SS ICL + Two-stage + Knowledge)?",
            "7": "Some important details about the proposed components could have been added to the paper.",
            "8": "For example, how does the generated knowledge snippets from COMET in section 3.3.3 look like?"
        },
        "O2uBOaZpt7": {
            "0": "- Show improvement over existing sota on this benchmark\n- Introduce three methods of improving chatgpt for this task which seem to be mostly successful in improving performance\n- Comprehensive automatic and human evaluations.",
            "1": "Experiments and metrics seem reasonable\n- Generated output in Table 5 looks very impressive - Some of the results tables claim that statistical significance testing has taken place, but it’s unclear which things are being compared in the statistical tests.",
            "2": "This seems important to clarify since some of the differences are a bit smaller.",
            "3": "- This paper is empirically significant in establishing a new SOTA on this benchmark, but the technical novelty of the approach is more limited.",
            "4": "It’s not proposing a new modelling approach, and the ideas for improving over chatgpt (e.g.",
            "5": "two stage reasoning with emotion prediction, using a knowledge base, etc) are similar to prior approaches that have been successful in improving smaller models on this dataset."
        },
        "7mlXKA9gb7": {
            "0": "Overall, the paper is well-organized, well-written, and the evaluation is quite comprehensive.",
            "1": "Empathetic chat is a task that has garnered growing interest in recent years, and, as such, this work is valuable in serving as an evaluation benchmark on empathic quality of LLM-generated text.",
            "2": "In addition, the three proposed improvements to prompt-based empathetic dialogue generation are well-validated, and both the human and automatic evaluation are well-designed and straightforward.",
            "3": "The paper provides clear qualitative examples of the effects the author's proposed improvements have on empathetic dialogue.",
            "4": "The main limitation of this work, as the authors mention, is the focus on evaluation with only the EmpatheticDialogues dataset.",
            "5": "Conducting similar evaluations across more diverse datasets would be needed for more generalizable results.",
            "6": "The human evaluation has few annotators, which can bias the evaluation towards a narrow definition of empathy.",
            "7": "Within the human evaluation, the provided instructions on ranking empathy are not clear, and it appears that sub-dimensions of empathy are not included in the evaluation, which could improve interpretability of the results.",
            "8": "Discussion of the analysis on using LLMs to simulate human evaluators is cursory, and would warrant more analysis beyond the scope of the contributions of this work.",
            "9": "While this evaluation is timely, correlation alone cannot demonstrate that LLMs effectively simulate empathy reasoning, and the results of this analysis do not clearly bolster the contributions of this work."
        },
        "e2P95gh0pf": {
            "0": "* The paper offers a detailed examination of LLMs' performance in the domain of empathetic dialogue, providing valuable insights into their capabilities.",
            "1": "* The methods and templates introduced by the authors can serve as valuable tools for researchers and practitioners in the NLP community.",
            "2": "* The motivation behind the study is well-articulated, emphasizing the real-world relevance of the task.",
            "3": "Additionally, the related work section effectively lays down foundational knowledge *  Some of the methodologies in the paper, like the \"tricky\" choice of exemplars for ICL that matches most closely with the test set, raise concerns.",
            "4": "This potentially makes the improvements and all subsequent results unreliable.",
            "5": "* The paper lacks crucial details about the human evaluation process.",
            "6": "Information regarding the hiring process, demographic details of the evaluators, measures taken to ensure unbiased evaluations, and procedures to validate the accuracy of annotations is missing.",
            "7": "* The paper does not specify if the proposed prompt template was compared against other prompt types or variations."
        }
    },
    "Wom397PB55": {
        "llm_review": {
            "0": "**Novelty and Contribution**: TheoremQA introduces a unique and challenging dataset that focuses on theorem-driven question answering.",
            "1": "This is a significant contribution as it addresses a gap in existing datasets which primarily focus on fundamental math problems without requiring domain-specific knowledge.",
            "2": "**Diverse Coverage**: The dataset covers a wide range of theorems from various fields including Math, Physics, EE&CS, and Finance.",
            "3": "This broad coverage ensures that the dataset can be used to evaluate the capabilities of AI models across different scientific domains.",
            "4": "**High-Quality Curation**: The dataset is curated by domain experts, ensuring the quality and relevance of the questions.",
            "5": "The careful construction process, including the adjustment of questions to ensure the answers follow a desired format, adds to the robustness of the dataset.",
            "6": "**Comprehensive Evaluation**: The paper evaluates a wide spectrum of 16 large language and code models using different prompting strategies.",
            "7": "This comprehensive evaluation provides valuable insights into the current capabilities and limitations of these models in solving theorem-driven questions.",
            "8": "**Detailed Analysis**: The paper includes various analyses such as the impact of theorem augmentation, the performance on multimodal questions, and error analysis.",
            "9": "These analyses provide a deeper understanding of the models' performance and the challenges they face.",
            "10": "**Human-Level Performance Benchmarking**: The inclusion of human-level performance benchmarking provides a useful reference point for evaluating the performance of AI models on the dataset.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Limited Answer Types**: The dataset avoids questions with answers in symbolic form, matrix form, or figure form due to evaluation complexity.",
            "13": "This limitation might restrict the dataset's ability to fully evaluate the models' capabilities in handling more complex and diverse answer types.",
            "14": "**Answer Extraction Challenges**: The paper acknowledges that parsing the output from large language models can be challenging, and the current answer extraction method is not perfect.",
            "15": "This limitation might affect the accuracy of the evaluation results.",
            "16": "**Multimodal Question Performance**: The performance on multimodal questions is notably poor, and the current visual encoding models do not seem well-suited for representing diagrammatic images.",
            "17": "This highlights a significant area for improvement in handling multimodal inputs.",
            "18": "**Theorem Augmentation Effectiveness**: The simple strategy of concatenating theorem descriptions did not yield significant improvements.",
            "19": "This suggests that more sophisticated methods are needed to effectively integrate theorem knowledge into the models' reasoning process.",
            "20": "**Bias in Question Selection**: The dataset construction process specifically avoids hard-to-evaluate cases, which might introduce a bias in the types of questions included.",
            "21": "This could limit the dataset's ability to comprehensively evaluate the models' overall capabilities.",
            "22": "**Open-Source Model Performance**: The performance of open-source models is significantly lower compared to closed-source models like GPT-4.",
            "23": "While this highlights the gap in capabilities, it also suggests that the dataset might be too challenging for current open-source models, limiting its immediate applicability for a broader range of models.",
            "24": "#### Conclusion\n\nTheoremQA is a valuable and innovative dataset that addresses a critical gap in the evaluation of AI models' capabilities to solve theorem-driven questions.",
            "25": "Its diverse coverage, high-quality curation, and comprehensive evaluation provide significant insights into the strengths and limitations of current models.",
            "26": "However, there are areas for improvement, particularly in handling more complex answer types, improving multimodal question performance, and developing more effective theorem augmentation strategies.",
            "27": "Despite these limitations, TheoremQA represents a significant step forward in benchmarking AI models on challenging scientific problems."
        },
        "Hx9Qngoyte": {
            "0": "1.The proposed dataset, TheoremQA, is the first of its kind to focus on theorem-driven question-answering.",
            "1": "It covers a wide range of theorems from various fields such as Math, Physics, Electrical Engineering & Computer Science, and Finance.",
            "2": "This dataset can serve as a valuable resource for researchers in the field.",
            "3": "The authors have conducted a thorough evaluation of a wide spectrum of 16 LLMs on TheoremQA.",
            "4": "Their comprehensive analysis provides valuable insights into the performance of these models and their ability to integrate theorems and understand multimodal inputs.",
            "5": "The methodology adopted by the authors is sound and well-justified.",
            "6": "They have used innovative prompting strategies, including Chain-of-Thoughts (CoT) and Program-of-Thoughts (PoT), to guide the LLMs.",
            "7": "The paper does not provide detailed examples of the prompts used in the experiments.",
            "8": "Including such examples, particularly in the appendix, would have been beneficial for readers to better evaluate the appropriateness and effectiveness of the prompting strategies employed by the authors.",
            "9": "Without these examples, it is challenging to fully assess the validity of the authors' claims."
        },
        "akwS4mnXN0": {
            "0": "Motivation:\n\n- Addresses an important open problem - evaluating scientific reasoning capabilities of AI systems, which requires applying knowledge.",
            "1": "- Proposes a new benchmark dataset to advance progress in this direction.",
            "2": "Experiments:\n\n- Thorough comparative evaluation of 16 state-of-the-art models with different prompting strategies.",
            "3": "- Rigorous analysis of results reveals capabilities and limitations of current models.",
            "4": "Writing:\n\n- Clearly presented.",
            "5": "Does a good job explaining the dataset, experiments, results and analyses.",
            "6": "- Provides useful insights that can guide future research to overcome limitations.",
            "7": "- Shares data to enable further analysis.",
            "8": "This is a resource paper, and it excels in all aspects.",
            "9": "I believe it deserves acceptance."
        },
        "HwbpzpDy1A": {
            "0": "(1) The paper contributes a new dataset for the QA research community, which consists of university-level questions backed with corresponding Theorems\n(2) It performs rigorous experiments with current SoTA LLMs with this dataset to check the performance measures of those LLMs\n(3) Also, it gives insights into why LLMs fail in some particular cases to perform QA and shows a detailed error analysis and its limitations.",
            "1": "Although this paper proposed quality dataset questions along with the theorems, the distribution of the questions is somewhat imbalanced.",
            "2": "Out of 800 questions, more than half of the questions are from Mathematics only.",
            "3": "So, distribution is not even."
        }
    },
    "tSfZo6nSN1": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel framework, RECAP, which integrates both spatial and temporal information for generating precise and accurate radiology reports.",
            "1": "This dual focus on spatial and temporal data is a significant advancement over previous models that often neglected one or both aspects.",
            "2": "**Dynamic Disease Progression Reasoning**: The introduction of a dynamic disease progression reasoning mechanism is a notable contribution.",
            "3": "This mechanism helps in accurately selecting the attributes of each observation and progression, which is crucial for generating clinically accurate reports.",
            "4": "**Comprehensive Evaluation**: The authors conducted extensive experiments on two publicly available datasets, MIMIC-ABN and MIMIC-CXR.",
            "5": "The results demonstrate the effectiveness of RECAP, showing improvements in both natural language generation (NLG) metrics and clinical efficacy (CE) metrics.",
            "6": "**Detailed Methodology**: The paper provides a thorough explanation of the methodology, including the construction of the disease progression graph and the two-stage report generation process.",
            "7": "This detailed description helps in understanding the model's workings and its advantages over existing methods.",
            "8": "**Case Studies and Error Analysis**: The inclusion of case studies and error analysis provides valuable insights into the model's performance and areas for improvement.",
            "9": "This analysis helps in understanding the practical implications of the model and its potential limitations.",
            "10": "**Open Source Code**: The authors have made their code available on GitHub, which promotes transparency and allows other researchers to replicate and build upon their work.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Complexity and Computational Cost**: The proposed model is quite complex, involving multiple stages and components such as visual encoders, transformers, and graph convolutional networks.",
            "13": "This complexity might lead to high computational costs, making it challenging to deploy in real-time clinical settings.",
            "14": "**Dependence on Predefined Observations and Progressions**: The model requires predefined observations and progressions for training, which may not be available for all types of radiographs.",
            "15": "This limitation could restrict the model's applicability to other medical imaging domains.",
            "16": "**Error Propagation**: The two-stage framework might suffer from error propagation, where errors in the first stage (observation and progression prediction) could negatively impact the second stage (report generation).",
            "17": "This issue is acknowledged by the authors but not thoroughly addressed in the paper.",
            "18": "**Limited Historical Records**: The model's reliance on historical records for generating follow-up reports is a potential limitation.",
            "19": "In many cases, historical records may not be available, which could lead to misleading or incomplete reports.",
            "20": "The authors suggest rule-based removal operations as a mitigation strategy, but this solution might not be sufficient.",
            "21": "**Generalization to Other Datasets**: While the model performs well on the MIMIC-ABN and MIMIC-CXR datasets, its generalization to other datasets or medical imaging modalities is not explored.",
            "22": "Further experiments on diverse datasets would strengthen the claims of the model's robustness and versatility.",
            "23": "**Evaluation Metrics**: Although the paper uses a variety of evaluation metrics, including BLEU, METEOR, ROUGE, and CheXbert, the reliance on these metrics might not fully capture the clinical relevance and utility of the generated reports.",
            "24": "Incorporating more clinically-oriented evaluation methods or expert reviews could provide a more comprehensive assessment.",
            "25": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of radiology report generation by introducing a model that effectively integrates spatial and temporal information.",
            "26": "The RECAP framework shows promising results in generating precise and accurate reports, as demonstrated by extensive experiments and detailed analysis.",
            "27": "However, the complexity of the model, potential issues with error propagation, and dependence on predefined observations and historical records are notable limitations.",
            "28": "Addressing these weaknesses in future work could further enhance the model's applicability and performance in real-world clinical settings."
        },
        "1ac474Yj6F": {
            "0": "Medical report generation is a critical clinical task which can relieve radiologists from the heavy workload.",
            "1": "Using historical reports and mining dissease progresses are useful for report generation.",
            "2": "The proposed approach fails to outperform existing works.",
            "3": "For example, in Table 1, the B-4 of proposed approach is lower than the basic baseline ViT-transformer on MIMIC-ABN.",
            "4": "Why the ViT-transformer is not evaluated on MIMIC-CXR data set.",
            "5": "What if the patients are the first time visitors without historical reports.",
            "6": "The authors need to evaluate the proposed approach on new patients and old patients respectively.",
            "7": "The experiment setting is not fair.",
            "8": "For the proposed approach, the historical reports of patients are used to generate reports for current input.",
            "9": "While these data are unseen to baseline works.",
            "10": "These historical reports should be added into the training data set of baselines.",
            "11": "One existing work which also includes historical reports in modeling should be referenced and discussed.",
            "12": "DeltaNet: Conditional medical report generation for COVID-19 diagnosis, Coling 2022.",
            "13": "Since the proposed approach targets to mine the progress of diseases to generate better results.",
            "14": "Such intermediate results (the disease progress) should be evaluated in experiments.",
            "15": "The IU data set should be included in experiments."
        },
        "KKRUhSyin1": {
            "0": "- The Spatial/Temporal component of radiology reports has long been overlooked.",
            "1": "This work is a great step toward this direction.",
            "2": "- Extensive comparison to prior work\n- Use of RadGraph to compute a semantic graph and progression\n- Substantial improvements on CE Metrics.",
            "3": "- Results are reported in a wide range of metrics\n\n - The model is quiet complex and difficult to understand\n- It seems some information from the ground-truth reports are used as input to the model (at test-time).",
            "4": "I might be wrong about this one (see questions for the authors)"
        },
        "XDIqz0LEfQ": {
            "0": "Good motivation for using longitudinal data with disease progression information to generate radiology reports.",
            "1": "This can potentially be used to generate other types of reports or other clinical NLP/NLG tasks.",
            "2": "The comparison experiments and ablation studies show promising results of the proposed method, especially the benefit of considering information from the last visit of a patient.",
            "3": "The examples are also insightful.",
            "4": "The paper is overall well-written.",
            "5": "The experiments are restricted to the MIMIC dataset only.",
            "6": "It is unclear how this method may perform on a different dataset or different types of notes.",
            "7": "The model only considers the most recent last visit of the patient.",
            "8": "What if more historical visits are considered?"
        }
    },
    "wcgfB88Slx": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method for optimizing explanation-infused prompts in a black-box fashion, which is a significant contribution to the field of natural language processing and in-context learning.",
            "1": "The use of a two-stage framework to evaluate and select explanations is well thought out and addresses the variability in explanation quality effectively.",
            "2": "**Comprehensive Evaluation**: The authors evaluate their method across four diverse textual reasoning tasks, including question answering, mathematical reasoning, and natural language inference.",
            "3": "This broad evaluation demonstrates the robustness and generalizability of their approach.",
            "4": "**Proxy Metrics**: The introduction of proxy metrics (log likelihood and accuracy on new examples) to evaluate explanations in isolation is a clever way to reduce the computational cost associated with evaluating a large number of candidate explanations.",
            "5": "This is a practical solution that balances performance and efficiency.",
            "6": "**Empirical Results**: The empirical results are compelling, showing that the proposed method can improve the performance of prompts over crowdworker annotations and naive search strategies.",
            "7": "The reported improvements in accuracy across different datasets highlight the effectiveness of the approach.",
            "8": "**Detailed Analysis**: The paper provides a detailed analysis of the effectiveness of the proxy metrics and the overall framework.",
            "9": "The authors also discuss the limitations and potential failure cases of their approach, which adds to the transparency and reliability of their findings.",
            "10": "**Generalizability**: The authors demonstrate that the optimized explanations can generalize to out-of-domain datasets, which is an important aspect of practical applicability.",
            "11": "This shows that the method is not just overfitting to the specific datasets used in the experiments.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Complexity and Overhead**: While the two-stage framework and the use of proxy metrics are innovative, they introduce additional complexity and computational overhead.",
            "14": "The paper mentions the cost of running LLMs and the need for a computation budget, but it would be beneficial to have a more detailed analysis of the trade-offs between the computational cost and the performance gains.",
            "15": "**Limited Scope of Tasks**: Although the paper evaluates the method on four different tasks, these tasks are still within the realm of textual reasoning.",
            "16": "It would be interesting to see how the approach performs on other types of tasks, such as dialogue systems or more complex multi-modal tasks.",
            "17": "**Dependence on LLM Capabilities**: The approach heavily relies on the capabilities of large language models (LLMs).",
            "18": "The paper acknowledges that less capable LMs might see limited benefits from this approach.",
            "19": "This dependence on powerful LLMs could limit the applicability of the method in resource-constrained environments.",
            "20": "**Manual Engineering of Seed Explanations**: The method starts with a set of seed explanations, which are manually annotated.",
            "21": "While the paper aims to optimize these explanations, the initial requirement for high-quality seed explanations could be a bottleneck.",
            "22": "The process of obtaining these seed explanations is not discussed in detail.",
            "23": "**Evaluation on text-davinci-003**: The evaluation on text-davinci-003 is conducted on a smaller scale due to computational costs.",
            "24": "While the results are promising, a more extensive evaluation on this model would strengthen the claims about the generalizability and effectiveness of the approach across different LLMs.",
            "25": "**Limited Discussion on Failure Cases**: The paper briefly mentions the failure cases of the proxy metrics but does not provide an in-depth analysis of why these failures occur and how they can be mitigated.",
            "26": "A more thorough discussion on this aspect would be valuable for understanding the limitations of the approach.",
            "27": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the optimization of explanation-infused prompts for in-context learning.",
            "28": "The proposed method is innovative and demonstrates substantial improvements in performance across various tasks.",
            "29": "However, the complexity and computational overhead, along with the dependence on powerful LLMs, are notable limitations.",
            "30": "Addressing these weaknesses in future work could further enhance the applicability and impact of this approach."
        },
        "CAoakTfjkG": {
            "0": "Authors propose a new framework to find the best in-context explanations for CoT, and use two proxy metrics to approximate downstream performance and enable efficient search.",
            "1": "The metrics generally correlate positively with final accuracy.",
            "2": "The experiments covered different kinds of reasoning tasks, and the authors provided sufficient details of the statistics.",
            "3": "Moreover, the overall presentation of this paper is good.",
            "4": "The idea of selecting demonstrations from unlabeled data can be potentially applied to many scenarios.",
            "5": "Although the authors use an additional constraint, searching over possible combinations of candidate explanations to find the one with the best performance on dev set might still be very expensive, e.g., paper [1] uses a very similar strategy to (use brute force to) find best in-context demonstrations.",
            "6": "The effectiveness of selected explanations is largely dependent on the specific dataset, for instance, the ones for GSM8K dataset can hardly be applied to other benchmarks like Multiarith, SingleEQ/OP, AddSub, GSM-Hard, although they are very similar in topics and have identical input/output formats.",
            "7": "[1] Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives https://arxiv.org/abs/2305.08088"
        },
        "mJOMkuQZzP": {
            "0": "- The experiment design is solid and there are strong results that demonstrate the effectiveness of proposed metrics and framework.",
            "1": "- It is interesting to see that both proposed metrics $\\mathcal{S}_\\text{OSAcc}$ and  $\\mathcal{S}_\\text{OSLL}$ select better explanations than random chance.",
            "2": "Figure 3 is informative for understanding why these metrics help.",
            "3": "- The authors provide comprehensive analyses of the framework.",
            "4": "The analyses demonstrate that the proposed framework is compatible with a common prompting technique (self-consistency), and works well under a reduced computation budget.",
            "5": "- The proposed method makes use of a silver set of pseudo-labeled examples, and it seems fair to further consider a best-of-$k$ baseline, which samples $k$ candidate explanation sets and then chooses the best set based on its accuracy on the pseudo-labeled examples.",
            "6": "This baseline is known to be effective in https://arxiv.org/abs/2112.09332.",
            "7": "- To make searching over candidate explanations tractable, the framework uses a greedy proxy metrics that assume combining the best individual explanations lead to the best set of explanations.",
            "8": "This assumption limits the performance of the method (i.e., the optimal set of explanations could be unattainable).",
            "9": "- There are a few moving parts in the proposed framework (e.g., a pseudo-labeled silver set, ensemble of proxy metrics and candidate explanation sets).",
            "10": "This complexity could make it difficult to apply this framework in a real-world setting."
        },
        "HW6veT1NI1": {
            "0": "- interesting and timely problem statement.",
            "1": "- reasonable solution (I like the emphasis and discussion on computational budget)\n- well written and interesting analysis.",
            "2": "- by products like proxy metrics, pseudo labelling of unlabeled data can be useful for other applications.",
            "3": "The following are the questions I have and these are not necessarily 'reasons to reject'.",
            "4": "- I was looking for a comparison with the zero-shot chain of thought baseline which authors refer as ZOT (Kojima et al., 2022).",
            "5": "The example selection method has a cost.",
            "6": "Also, few shot experiments involve extra token usage cost than zero shot.",
            "7": "- Some of the numbers while comparing proposed method vs baselines seem to be pretty close.",
            "8": "Wondering, if authors did any statistical significance test?",
            "9": "- A parallel field to explanation selection is prompt/instruction engineering, where we often change the zeroshot instruction.",
            "10": "Another alternative is prompt-tuning via gradient descent.",
            "11": "Wondering if authors have any thoughts regarding the tradeoff.",
            "12": "- Few shot examples has various types of example biases such as majority bias, recency bias etc.",
            "13": "(http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf, https://aclanthology.org/2023.eacl-main.130/, https://aclanthology.org/2022.acl-long.556.pdf).",
            "14": "Wondering if authors have any thought on how the robustness look like with the application of their method?",
            "15": "I am looking forward to hear answers to these questions from the authors."
        }
    },
    "edwSiVzFpU": {
        "llm_review": {
            "0": "**Comprehensive Overview**: The paper provides a thorough and detailed survey of the end-to-end task-oriented dialogue (EToD) systems, covering a wide range of approaches and methodologies.",
            "1": "This makes it a valuable resource for researchers and practitioners in the field.",
            "2": "**First Survey in the Field**: The authors claim that this is the first comprehensive survey on EToD, which fills a significant gap in the literature.",
            "3": "This pioneering effort is commendable and sets a foundation for future research.",
            "4": "**New Taxonomy**: The introduction of a new taxonomy for EToD, dividing it into Modularly EToD and Fully EToD, helps in organizing the existing research and provides a clear framework for understanding the different approaches.",
            "5": "**Future Directions**: The discussion on potential future directions and challenges is insightful.",
            "6": "It highlights the areas that need further exploration and provides a roadmap for future research.",
            "7": "**Abundant Resources**: The creation of a public website with resources such as related papers, baseline projects, and leaderboards is a significant contribution.",
            "8": "It facilitates easy access to the latest developments in the field and promotes collaboration among researchers.",
            "9": "**Detailed Analysis**: The paper provides a detailed analysis of various models and approaches, including their performance on standard datasets.",
            "10": "This helps in understanding the strengths and weaknesses of different methods.",
            "11": "**Clear Structure**: The paper is well-structured, with clear sections and subsections that make it easy to follow.",
            "12": "The use of figures and tables to illustrate concepts and compare models is effective.",
            "13": "**Weaknesses:**\n\n1.",
            "14": "**Lack of Fine-Grained Analysis**: While the paper provides a high-level comparison of different approaches, it lacks a fine-grained analysis of the advantages and disadvantages of various models.",
            "15": "More detailed comparative analyses would be beneficial.",
            "16": "**Limited Discussion on Real-World Applications**: The paper focuses primarily on academic research and benchmarks.",
            "17": "A discussion on the practical applications and deployment of EToD systems in real-world scenarios would add more value.",
            "18": "**Scalability and Efficiency**: The paper does not address the scalability and efficiency of the different EToD approaches in detail.",
            "19": "These are critical factors for the deployment of dialogue systems in real-world applications.",
            "20": "**Evaluation Metrics**: While the paper discusses commonly used evaluation metrics, it does not critically analyze their effectiveness.",
            "21": "A discussion on the limitations of current metrics and potential improvements would be useful.",
            "22": "**Human-AI Interaction**: The paper briefly mentions the importance of human feedback in training LLMs but does not delve deeply into the challenges and methodologies for effective human-AI interaction in EToD systems.",
            "23": "**Ethical Considerations**: The paper touches on safety and risk mitigation but does not provide a comprehensive discussion on the ethical considerations and potential biases in EToD systems.",
            "24": "This is an important aspect that needs more attention.",
            "25": "**Future Directions Could Be More Specific**: While the future directions section is insightful, it could benefit from more specific and actionable research questions or hypotheses.",
            "26": "This would provide clearer guidance for researchers.",
            "27": "**Conclusion:**\n\nOverall, the paper \"End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions\" is a significant contribution to the field of task-oriented dialogue systems.",
            "28": "It provides a comprehensive overview, introduces a new taxonomy, and discusses future directions and challenges.",
            "29": "However, it could benefit from more fine-grained analysis, a discussion on real-world applications, and a deeper exploration of ethical considerations and human-AI interaction.",
            "30": "Despite these weaknesses, the paper is a valuable resource for researchers and practitioners and sets a solid foundation for future research in EToD systems."
        },
        "c29IgPlAnU": {
            "0": "The paper provides a thorough overview of the progress made in the field of task-oriented dialogue and examines emerging frontier areas while summarizing the associated challenges.",
            "1": "The writing is accessible and straightforward, making the content easily understandable to readers.",
            "2": "Lack of Discussion on Integration with Large Pre-trained Models: In the era of large pre-trained models, the paper fails to address the integration of task-oriented dialogue with these models, which could have significant implications for EToD research.",
            "3": "Omissions of Recent Progress: Some noteworthy recent developments in EToD, such as QToD (Q-TOD: A Query-driven Task-oriented Dialogue System) and Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog, are not included in the paper."
        },
        "LTDSZcggyy": {
            "0": "Please refer to the “Paper Topic And Main Contributions” part.",
            "1": "Please refer to the “Paper Topic And Main Contributions” part."
        },
        "OHPCFD1mc7": {
            "0": "This work is important as it is the first survey of the field.",
            "1": "The paper has clear taxonomy, good coverage, leaderboards and good discussion on future directions.",
            "2": "All the relevant data is made publicly available.",
            "3": "Overall, this work can provide visibility, transparency and traction to the field.",
            "4": "The paper lacks detailed discussion on the evaluation protocols for end-to-end TOD.",
            "5": "This includes automatic metrics, human evaluation and recent LLM based metrics like BERTScore and its variant."
        }
    },
    "9F6h0oIYsP": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel framework, CLKD-IMRD, which leverages contrastive learning and knowledge distillation for rumor detection.",
            "1": "This approach is innovative and addresses the challenges posed by incomplete modalities in social media posts.",
            "2": "**Comprehensive Methodology**: The methodology is well-detailed, explaining the use of multimodal feature extraction, multimodal feature fusion, and the integration of supervised contrastive learning and knowledge distillation.",
            "3": "This comprehensive approach ensures a thorough understanding of the proposed model.",
            "4": "**Extensive Experiments**: The authors conducted extensive experiments on four benchmark datasets (two English and two Chinese), demonstrating the effectiveness of their model.",
            "5": "The results show that CLKD-IMRD outperforms state-of-the-art methods, which is a significant contribution to the field.",
            "6": "**Knowledge Distillation for Incomplete Modalities**: The use of knowledge distillation to handle incomplete modalities is a notable strength.",
            "7": "This approach allows the model to perform well even when some modalities (e.g., images or text) are missing, which is a common issue in real-world social media data.",
            "8": "**Visualization and Analysis**: The paper includes T-SNE visualizations and attention visualizations, providing insights into how the model works and the importance of different modalities.",
            "9": "This helps in understanding the model's decision-making process and the interaction between textual and visual information.",
            "10": "**Ablation Studies**: The authors conducted ablation studies to analyze the impact of different components of their model.",
            "11": "This thorough analysis helps in understanding the contribution of each component to the overall performance.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Generalizability**: The experiments were conducted on specific datasets (Chinese Weibo and English Pheme/Twitter), which may not fully represent other rumor detection scenarios or platforms.",
            "14": "The generalizability of the model to different datasets and languages needs further exploration.",
            "15": "**Absence of Real-time Evaluation**: The evaluation primarily focused on offline performance measures.",
            "16": "The paper does not consider real-time or dynamic evaluation scenarios, which are crucial for practical applications of rumor detection models.",
            "17": "**Complexity and Scalability**: The proposed model involves multiple components, including contrastive learning, knowledge distillation, and co-attention mechanisms.",
            "18": "While effective, this complexity might pose challenges in terms of scalability and deployment in real-world applications.",
            "19": "**Impact of Comments**: The analysis of the impact of the number of comments on performance shows that increasing the number of comments does not significantly contribute to debunking rumors.",
            "20": "This raises questions about the optimal use of comments in the model and whether simpler approaches might be equally effective.",
            "21": "**ChatGPT Baseline**: The performance of ChatGPT as a baseline model is notably poor, especially on the Weibo-17 and Twitter datasets.",
            "22": "While this highlights the limitations of ChatGPT in this specific task, it also suggests that the comparison might not be entirely fair, given the different nature of ChatGPT's training and capabilities.",
            "23": "**Ethical Considerations**: The paper briefly mentions potential socio-economic biases in the datasets but does not delve deeply into the ethical implications of using such models.",
            "24": "A more thorough discussion on the ethical considerations and potential biases in rumor detection models would strengthen the paper.",
            "25": "**Conclusion:**\n\nOverall, the paper presents a significant contribution to the field of rumor detection by introducing a novel framework that effectively handles incomplete modalities.",
            "26": "The comprehensive methodology, extensive experiments, and detailed analysis are commendable.",
            "27": "However, the paper could benefit from addressing the limitations related to generalizability, real-time evaluation, and ethical considerations.",
            "28": "Despite these weaknesses, the proposed model shows great promise and provides a solid foundation for future research in multimodal rumor detection."
        },
        "45K55GZTVE": {
            "0": "Proposes a multi-modal model as a teacher model to guide the single-modal model.",
            "1": "The experimental results are very comprehensive.",
            "2": "It does not explain why supervised contrastive learning is used.",
            "3": "The most recent multi-modal rumor detection model compared is from 2020.",
            "4": "The effectiveness of the student model has not been compared with other models, making it difficult to prove the effectiveness of distillation.",
            "5": "Main experiment focused on multi-modal scenarios; more experiments should be conducted starting from the perspective of missing modalities, beacuse the main motivation is incomplete modalities."
        },
        "ycghNWLBzC": {
            "0": "- Good motivation and well-conducted experiments.",
            "1": "They compare their results to a range of baseline models.",
            "2": "- They tackle the problem of missing modality.",
            "3": "This is not only valuable for rumor detection but for multimodal classification of social media posts in general.",
            "4": "- The paper present different types of analysis to shed light on the benefits of their approach.",
            "5": "For instance, they perform an ablation study, a study using different versions of the student model, and a study on the effect of using different number of comments.",
            "6": "- Interesting findings, for instance, they show that using more comments does not necessarily improve performance.",
            "7": "This is interesting given the idea of \"more data is always better\".",
            "8": "This is also opposite to findings in multimodal social media classification: Xu, Chunpu, and Jing Li.",
            "9": "\"Borrowing Human Senses: Comment-Aware Self-Training for Social Media Multimodal Classification.\"",
            "10": "arXiv preprint arXiv:2303.15016 (2023).",
            "11": "- Experiments are only done on two datasets for binary classification.",
            "12": "Since you are presenting a new method it is worth applying the models to a range of benchmark datasets.",
            "13": "- Some choices are not justified/not clear.",
            "14": "See questions for the authors."
        },
        "odJR0vJ18R": {
            "0": "1) The work presents a rumor detection framework that relies on the supervised contrastive learning and teacher network.",
            "1": "The framework can capture semantic interactions among source texts, images, and user comments.",
            "2": "2) This paper presents a knowledge distillation driven rumor detection model that can probably handle incomplete modalities (i.e., lack of images or texts).",
            "3": "1) Multimodal rumor detection is not new, and this work focuses on incomplete modality case, where it may be lack of image or text information in given posts.",
            "4": "Thus it may lead to incremental contribution.",
            "5": "2) The proposed method was tested on only 2 small datasets, and it's not clear if it can be generalizable to different runor detection tasks."
        }
    },
    "U6SEUS76IE": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces Federated Interactive Distillation (FedID), a novel approach to federated learning that addresses the limitations of traditional federated learning (FL) and federated distillation (FD).",
            "1": "The interactive distillation mechanism, which uses a small amount of labeled data retained by the server to rectify local models, is innovative and shows promise in mitigating confirmation bias.",
            "2": "**Comprehensive Benchmarking**: The authors develop a benchmarking framework based on the GLUE benchmark, which includes multiple tasks with diverse data distributions.",
            "3": "This comprehensive evaluation across various NLP tasks provides a robust validation of the proposed method.",
            "4": "**Performance Improvement**: The experimental results demonstrate that FedID achieves superior performance compared to existing FL and FD methods in both homogeneous and heterogeneous federated scenarios.",
            "5": "The method shows robustness in handling non-IID data and performs well even with a small amount of labeled data.",
            "6": "**Communication Efficiency**: FedID reduces communication costs by slicing the unlabeled public dataset into smaller batches, which helps in avoiding potential network congestion and makes the approach more scalable.",
            "7": "**Detailed Analysis**: The paper provides a thorough analysis of the impact of various factors such as the size of the public dataset, the size of the labeled dataset, and the number of clients on the performance of the central model.",
            "8": "This detailed analysis helps in understanding the strengths and limitations of the proposed method.",
            "9": "**Weaknesses:**\n\n1.",
            "10": "**Assumption of Labeled Data**: The approach assumes that a small amount of labeled data is retained by the server.",
            "11": "While the authors argue that this is a common scenario, it may not always be practical in real-world applications where labeled data is scarce or unavailable.",
            "12": "This assumption limits the generalizability of the method.",
            "13": "**Increased Communication Frequency**: Although the communication costs are reduced by slicing the public dataset, the increased frequency of communication between the server and clients may still pose challenges in environments with limited bandwidth or high latency.",
            "14": "The paper does not provide a detailed analysis of the impact of communication frequency on the overall system performance.",
            "15": "**Limited Comparison with Larger Models**: The experiments primarily focus on BERT-base and similar-sized models.",
            "16": "While this is a reasonable choice for compatibility with FL methods, it would be beneficial to see how FedID performs with larger models like GPT-3 or T5, especially given the trend towards larger pre-trained language models in NLP.",
            "17": "**Security and Privacy Considerations**: The paper briefly mentions the potential for future work on aggregating differentially private local predictions.",
            "18": "However, it does not provide a detailed discussion on the current privacy guarantees of FedID or its resilience against potential attacks.",
            "19": "A more in-depth analysis of the security and privacy aspects would strengthen the paper.",
            "20": "**Complexity of Implementation**: The interactive distillation mechanism introduces additional complexity in the implementation of the federated learning system.",
            "21": "The paper does not provide sufficient details on the practical challenges and potential overheads associated with implementing FedID in real-world systems.",
            "22": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of federated learning for NLP by introducing FedID, a novel interactive distillation approach.",
            "23": "The comprehensive benchmarking and detailed analysis of various factors affecting performance are commendable.",
            "24": "However, the assumptions regarding labeled data, increased communication frequency, and limited discussion on security and privacy aspects are areas that need further exploration.",
            "25": "Despite these weaknesses, FedID shows great potential in improving the performance and scalability of federated learning systems for large-scale pre-trained language models."
        },
        "MExaeE6LHA": {
            "0": "Useful privacy-preserving training method with good performance\n2.",
            "1": "Reasonable and effective algorithm design\n3.",
            "2": "Comprehensive experiment to show the effectiveness of the method\n 1.",
            "3": "Lack case-level comparison to analyze/justify in detail why the proposed method alleviates confirmation bias\n2.",
            "4": "Lack of comparison with alternative designs (e.g.",
            "5": "for server-to-client interaction, can we also use the proxy dataset instead of the gradient?)",
            "6": "Lack real-world use case analysis for the proposed training scenario (in the limitation section the author explains a constructed scenario which is reasonable, but still it would be informative to find concrete use case)"
        },
        "4r60k5CkoC": {
            "0": "The FedID approach is generic and can potentially be applied to train non-NLP models as well in a Federated Learning setting.",
            "1": "In this paper, the authors specifically apply it to PLMs and show their efficiency and performance benefits for NLP tasks - making it relevant for EMNLP.",
            "2": "The experimental results are comprehensive providing interesting insights around public dataset distribution between clients and communication aspects.",
            "3": "The heterogenous setting is of course more interesting from a 'confirmation bias' perspective, where the authors show FedID's robustness when only a small amount of training data is available that are insufficient to train the local models.",
            "4": "FedID is more of an incremental contribution over existing Federated Distillation (FD) approaches, with a different distribution mechanism.",
            "5": "So the strength of paper is primarily on the applied part.",
            "6": "For Federated Learning contributions, it is always good to assess how resilient it is to malicious clients/server.",
            "7": "So I would encourage the authors to add this discussion."
        },
        "m8V8O9eD9m": {
            "0": "It is a good idea to solve the homogeneous and heterogeneous in federated distillation through some labeled data of the server\n2.",
            "1": "The author has done sufficient experiments to verify the effectiveness of the method 1.",
            "2": "The homogeneous and heterogeneous of PLM and server are the homogeneous and heterogeneous faced by other models.",
            "3": "It seems that there is not much difference.",
            "4": "As the author said, when the server model is PLM, the data can be reasonably divided into public data on the server and private data on the client, but it can also be used in other FDs in visual scenes, recommendation scenes, and audio scenes.",
            "5": "Therefore, the first exploration of the server model is that the homogeneous and heterogeneous problems in FD of PLM do not seem to be a contribution, so the contribution seems to be a bit weak.",
            "6": "Although many published federated learning papers lack theories, if there are some more important analyzes such as errors, it will make the papers more solid\n3.",
            "7": "Lack of any reproducible content, including code, experiment/hyperparameter settings,"
        }
    },
    "uPz5a2NvrG": {
        "llm_review": {
            "0": "**Innovative Approach:** The paper introduces a novel normal-abnormal semantic decoupling network that leverages abnormal pattern memory.",
            "1": "This approach is innovative and addresses the challenges posed by the high degree of similarity in radiological images and the presence of data bias and complex noise in medical data.",
            "2": "**Semantic Decoupling:** The method of decoupling normal and abnormal semantics is a significant strength.",
            "3": "By focusing on abnormal semantics for optimizing the visual extraction network, the approach effectively mitigates the impact of noisy normal semantics and reports.",
            "4": "**Abnormal Pattern Memory:** The development of an encoder for abnormal pattern memory is a notable contribution.",
            "5": "This improves the network’s ability to detect anomalies by capturing and embedding abnormal patterns in the visual encoder.",
            "6": "**Performance:** The proposed method demonstrates excellent performance on the benchmark MIMIC-CXR dataset, surpassing current state-of-the-art methods.",
            "7": "The results are well-documented and show significant improvements in various metrics.",
            "8": "**Comprehensive Evaluation:** The paper provides a thorough evaluation of the proposed method, including comparisons with previous studies, ablation studies, and qualitative analysis.",
            "9": "This comprehensive evaluation strengthens the validity of the proposed approach.",
            "10": "**Detailed Methodology:** The paper provides a detailed description of the methodology, including the visual extractor, semantic extractor, and decoder.",
            "11": "This clarity in presentation helps in understanding the intricacies of the proposed approach.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Complexity:** The proposed method is quite complex, involving multiple components such as the visual extractor, semantic extractor, and decoder.",
            "14": "This complexity might pose challenges in implementation and replication of the results.",
            "15": "**Dependence on RadGraph:** The method relies heavily on RadGraph for extracting semantics.",
            "16": "While RadGraph is a powerful tool, this dependence might limit the generalizability of the approach to other datasets or domains where RadGraph is not applicable.",
            "17": "**Handling of Rare Diseases:** The paper acknowledges the limitation in handling rare diseases due to the multi-label classification process.",
            "18": "This is a significant drawback as rare diseases are often critical in medical diagnosis and report generation.",
            "19": "**Noise in Reports:** While the approach mitigates the impact of noisy normal semantics, it does not completely eliminate the effect of noise in the report generation phase.",
            "20": "The paper suggests that optimizing the report using different weights may only partially alleviate the effects of noise.",
            "21": "**Limited Dataset:** The evaluation is primarily conducted on the MIMIC-CXR dataset.",
            "22": "While this is a large and widely used dataset, additional evaluations on other datasets would strengthen the generalizability of the proposed method.",
            "23": "**Ethical Considerations:** The paper briefly mentions the ethical considerations related to the use of the MIMIC-CXR dataset.",
            "24": "However, a more detailed discussion on the ethical implications of automated medical report generation, including potential biases and the impact on clinical practice, would be beneficial.",
            "25": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of medical report generation with its novel normal-abnormal semantic decoupling network and abnormal pattern memory.",
            "26": "The strengths of the paper lie in its innovative approach, comprehensive evaluation, and detailed methodology.",
            "27": "However, the complexity of the method, dependence on RadGraph, handling of rare diseases, and limited dataset evaluation are notable weaknesses.",
            "28": "Addressing these weaknesses in future work could further enhance the impact and applicability of the proposed approach."
        },
        "34uvjucb98": {
            "0": "It is interesting to separately model normal and abnormal reports.",
            "1": "The proposed approach could outperform baselines on the benchmark MIMIC-CXR dataset.",
            "2": "The ablation study shows that each of the component could help improve the performance.",
            "3": "The abnormal mode memory part is not quite clear, which is difficult to interpret.",
            "4": "The proposed method is somewhat incremental.",
            "5": "This paper only includes results of one dataset in the main text."
        },
        "RHLG5RNoTb": {
            "0": "The idea of using the codebook in VQVAE to store various anomaly information seems to be a right way to go.",
            "1": "The memory loss in the objective functions is novel and makes sense.",
            "2": "The experimental results seem to be very promising.",
            "3": "And the baselines are comprehensive.",
            "4": "I didn't find any obvous reason to reject the paper."
        },
        "0NlaBNIzU4": {
            "0": "The methodology achieves higher than state-of-the-art for MIMIC CXR report generation.",
            "1": "The methodology is novel, and involves an understanding the structure of radiology report findings beyond just caption generation problem for radiographic images.",
            "2": "In radiology reports, the radiologists tend to write more about the abnormalities than focus on the normal perceptions in the report, and this architecture utilizes that aspect in their modeling quite well.",
            "3": "The analysis of the model is rigorous and gives us a clear understanding of the types of errors to expect.",
            "4": "Though one of the core concepts of the paper is normal-abnormal semantics in reports, the classification from the reports is largely dependent on keyword-based method, which, the authors accept is largely noisy.",
            "5": "The definition of what is an abnormal semantics in a report varies depending on application, and there is no specification of any such definition followed in this paper.",
            "6": "This makes the normal/abnormal semantic section a little weak compared to the other sections.",
            "7": "-- After rebuttal: Even with the rebuttal, I think this portion could have been handled much better."
        }
    },
    "i0vMIpaEn4": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel divergence-based adaptive policy (DaP) for simultaneous machine translation (SiMT).",
            "1": "This approach decouples the adaptive policy from the translation model, which is a significant departure from traditional methods that tightly couple these components.",
            "2": "This decoupling offers increased flexibility and potential for improved performance.",
            "3": "**Efficiency**: The proposed DaP model extends a frozen wait-k model with lightweight parameters, making it both memory and computation efficient.",
            "4": "This is particularly important for real-time applications where computational resources may be limited.",
            "5": "**Comprehensive Evaluation**: The authors conduct extensive experiments across multiple benchmarks (Zh→En, De→En, En→Vi) and demonstrate that their approach outperforms strong baselines, including state-of-the-art methods like ITST.",
            "6": "The use of BLEU vs. AL and NLL vs. AL curves provides a thorough evaluation of the trade-off between translation accuracy and latency.",
            "7": "**Detailed Analysis**: The paper includes a detailed analysis of various aspects of the proposed method, such as the impact of different divergence measures, the number of decoder layers, and the maximum number of continuous read actions.",
            "8": "This thorough analysis helps in understanding the strengths and limitations of the approach.",
            "9": "**Practical Implications**: By drawing inspiration from human simultaneous translation and using statistical divergence to make read/write decisions, the paper provides a practical and intuitive method for improving SiMT systems.",
            "10": "This approach can potentially be applied to other real-time translation tasks, such as speech translation.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Complexity of Implementation**: While the proposed method is efficient, the implementation of the divergence-based adaptive policy may be complex.",
            "13": "The need to calculate divergence measures and integrate them with the translation model could pose challenges for practitioners.",
            "14": "**Limited Intrinsic Evaluation**: The paper primarily focuses on the impact of the adaptive policy on translation performance (BLEU and NLL scores).",
            "15": "However, there is a lack of intrinsic evaluation of the policy model itself.",
            "16": "Further investigation into the policy model's performance could provide more insights into its effectiveness.",
            "17": "**Sensitivity to Hyperparameters**: The performance of the DaP-SiMT model is influenced by several hyperparameters, such as the threshold for divergence and the maximum number of continuous read actions.",
            "18": "While the authors provide some analysis, a more detailed exploration of the sensitivity to these hyperparameters would be beneficial.",
            "19": "**Potential for Further Improvement**: The upper bound analysis shows that there is still a significant gap between the performance of the learned policy model and the ground truth divergence values.",
            "20": "This indicates potential for further improvement in modeling the divergence supervision signals.",
            "21": "**Generalization to Other Languages**: The experiments are conducted on three language pairs, but it is unclear how well the proposed method generalizes to other languages, especially those with different linguistic characteristics.",
            "22": "Additional experiments on a wider range of languages would strengthen the paper's claims.",
            "23": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of simultaneous machine translation by introducing a divergence-based adaptive policy.",
            "24": "The approach is innovative, efficient, and demonstrates strong performance across multiple benchmarks.",
            "25": "However, the complexity of implementation, sensitivity to hyperparameters, and potential for further improvement in the policy model are areas that could be addressed in future work.",
            "26": "Despite these limitations, the paper provides valuable insights and a solid foundation for further research in adaptive policies for SiMT."
        },
        "aqMh3QxdpG": {
            "0": "-\tSiMT has received more and more attention due to its low latency characteristics.",
            "1": "The proposed method achieves promising results, surpassing the current state-of-the-art ITST.",
            "2": "-\tThe proposed divergence-based policy is interesting and has certain interpretability.",
            "3": "-\tThe proposed method adopts a frozen multi-path wait-k translation model and only needs to train the policy module.",
            "4": "-\tThe article is well written and easy to follow.",
            "5": "-\tThe article has no obvious weaknesses to reject.",
            "6": "-\tA potential issue is that divergence-based policies may be affected by language pairs.",
            "7": "For example, the word order difference in De-En may make the model READ more words, and the author added Max-Continuous-READ to alleviate this issue."
        },
        "LZ6DUU7eAP": {
            "0": "The approach to independently train the adaptive policy based on the idea of divergence is original and the results support the benefits of this simpler approach.",
            "1": "The presentation of the paper is excellent and easy to understand the motivation behind the authors' decisions.",
            "2": "The translation quality improvement over the SoTA ITST model is not significant, but systematic across tasks and latency values."
        },
        "1e09TMAfJ4": {
            "0": "The main contribution of the paper is the proposal of a novel method to construct read/write supervision signals from a parallel training corpus based on statistical divergence.",
            "1": "The paper also presents a lightweight policy model that enables adaptive read/write decision-making for a well-trained multi-path wait-k translation model.",
            "2": "The proposed approach is both memory and computation efficient and offers an improved trade-off between translation accuracy and latency, outperforming strong baselines.",
            "3": "The paper makes contributions to the field of NLP engineering experiment and provides a new approach for SiMT that can be applied to various languages.",
            "4": "There are two papers which are very related to this paper:\n1.",
            "5": "Zheng et al.",
            "6": "Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation.",
            "7": "EMNLP, November 2019.",
            "8": "Zheng et al.",
            "9": "Simultaneous translation policies: from fixed to adaptive.",
            "10": "ACL, 2020, arXiv\n\nThe authors should compare their work with these two lines of work."
        }
    },
    "okV4KG4kMg": {
        "llm_review": {
            "0": "#### Strengths\n\n1.",
            "1": "**Novel Dataset Creation**: The authors introduce ExFunTube, a unique dataset of 10,136 user-generated, short-form funny videos from YouTube.",
            "2": "This dataset is significant because it covers a wide range of domains and types of humor, necessitating a multimodal understanding of the content.",
            "3": "The dataset is meticulously curated and annotated with timestamps and text explanations for funny moments, which is a valuable resource for future research in video humor understanding.",
            "4": "**Multimodal Approach**: The paper emphasizes the importance of both verbal and visual elements in understanding humor.",
            "5": "The authors develop a video filtering pipeline using GPT-3.5 to ensure that the videos selected for the dataset contain multimodal humor.",
            "6": "This approach is innovative and highlights the complexity of humor, which often relies on the interplay between different modalities.",
            "7": "**Zero-shot Video-to-Text Prompting**: The authors propose a zero-shot video-to-text prompting method to maximize the humor understanding capabilities of large language models (LLMs).",
            "8": "This method leverages state-of-the-art models to convert video content into text, which is then used to prompt LLMs for humor explanations.",
            "9": "This approach is practical and demonstrates significant improvements in LLMs' ability to explain humor.",
            "10": "**Comprehensive Evaluation**: The paper employs three different evaluation methods—model-based automatic scores, rationale quality experiments, and human evaluations—to assess the performance of their prompting approach.",
            "11": "This comprehensive evaluation provides a robust validation of their method and highlights its effectiveness in improving humor explanation performance.",
            "12": "**Detailed Analysis**: The authors provide a thorough analysis of their results, including an ablation study to understand the importance of each modality (visual, speech, and sound) in humor explanation.",
            "13": "They also analyze the performance of LLMs across different humor categories, providing insights into the strengths and limitations of their approach.",
            "14": "#### Weaknesses\n\n1.",
            "15": "**Subjectivity of Humor**: One of the inherent challenges in humor research is its subjective nature.",
            "16": "While the authors acknowledge this, the paper could benefit from a more in-depth discussion on how subjectivity was addressed during the annotation process and how it might impact the results.",
            "17": "For instance, cultural and geographic biases may influence humor explanations, and this aspect could be explored further.",
            "18": "**Dependence on Existing Models**: The proposed method relies heavily on the performance of existing state-of-the-art models, such as GPT-3.5, Whisper, and BLIP-2.",
            "19": "While this approach is practical, it also means that the method's effectiveness is constrained by the limitations of these models.",
            "20": "Future work could explore ways to mitigate this dependency, such as developing specialized models for humor understanding.",
            "21": "**Temporal Information of Sound**: The paper mentions that the temporal information of sound was not considered in their approach.",
            "22": "Given that timing can play a crucial role in humor, incorporating temporal aspects of sound could potentially enhance the model's understanding of humorous content.",
            "23": "This limitation should be addressed in future research.",
            "24": "**Manual Postprocessing**: The manual review process for filtering out inappropriate or harmful content is labor-intensive and may not be scalable.",
            "25": "Developing automated methods to ensure the safety and appropriateness of the dataset could improve the efficiency and scalability of the dataset creation process.",
            "26": "**Performance Gap with Human Levels**: Despite the improvements achieved with their prompting approach, the performance of LLMs still falls short of human levels.",
            "27": "This indicates that understanding and explaining humor remains a challenging task for AI models.",
            "28": "The paper could discuss potential directions for future research to bridge this performance gap, such as incorporating user feedback for personalized humor understanding.",
            "29": "#### Conclusion\n\nOverall, the paper makes significant contributions to the field of humor understanding in AI by introducing a novel dataset, proposing an innovative zero-shot video-to-text prompting method, and providing a comprehensive evaluation of their approach.",
            "30": "While there are some limitations, the strengths of the paper outweigh the weaknesses, and it provides a solid foundation for future research in this area.",
            "31": "The authors' work is a valuable addition to the field and opens up new avenues for exploring the complex and subjective nature of humor in AI."
        },
        "fOyPWR3L2v": {
            "0": "This paper has many strengths that make it worthy of acceptance, including the following:\n\n1.",
            "1": "The authors have devised an innovative and inspiring method for dataset creation, seamlessly capturing both textual and visual nuances.",
            "2": "By leveraging current state-of-the-art (SOTA) models and techniques, the paper showcases its relevance and modern approach.",
            "3": "The team has conducted thorough experiments and evaluations.",
            "4": "The research's modular framework stands out, designed to easily integrate or replace components with upcoming SOTA models, ensuring long-term adaptability.",
            "5": "The paper exhibits promising strengths; however, there are certain aspects that could be improved:\n\n1.",
            "6": "Audio information is not fully utilized.",
            "7": "A more extensive exploitation of this modality could enhance the system's comprehension of the data and potentially improve the overall results.",
            "8": "The choice of selected models and methods in the study could use more detailed justification.",
            "9": "Understanding why these specific models were chosen over others and their relative advantages would be beneficial.",
            "10": "Additionally, the potential pitfalls or errors introduced by these models are not sufficiently addressed."
        },
        "AVwfpIHeHc": {
            "0": "The article released a large-scale multimodal humor video dataset that provides novel multi-humor interval annotations, which can provide new data support for community understanding of videos.",
            "1": "Although there are numerous similar frameworks, the article provides a simple and feasible method for converting videos to text.",
            "2": "It can capture videos according to their plot/shot.",
            "3": "The article provides a feasible method for understanding humorous videos, which can achieve the SOTA understanding performance.",
            "4": "The article provides humor interval annotations, but the distribution of the number of humor points is not very balanced, and I believe that videos without humor intervals should be added (negative samples can assist the model in understanding the true meaning of humor).",
            "5": "The annotation of the released dataset was not fully evaluated for quality (or not displayed in the article), so the quality of the dataset cannot be guaranteed.",
            "6": "There is uncomfortable content in the released dataset.",
            "7": "For example, the second example in the attachment not only has formatting issues, but its content also makes me feel very uncomfortable while watching."
        },
        "BNQcQ512yc": {
            "0": "Construct a multi-modal humor video dataset ExFunTube from YouTube with high quality control and support from large language model(LLM), the dataset cover a wide range of domains with various types of humor, and contains annotate start and end timestamps of funny moments and provide text explanations for each moment.",
            "1": "This seems the only contribution.",
            "2": "The author believes that a flaw of the dataset in MAF is the reliance on visual information (at line 79).",
            "3": "However, my opinion is exactly the opposite.",
            "4": "Since it is a multimodal data set, it is reasonable to rely on any modality, and each modality has its importance.",
            "5": "Due to the conversion of visual and acoustic modalities into text descriptions, the final explanation of humor is entirely dependent on text modality, which leads to the loss of some important modal-specific information in the intermediate steps.",
            "6": "More appropriate feature encoding and modal fusion methods could be improved.",
            "7": "In section 4.1, I didn’t see any prompt demonstration.",
            "8": "And I didn’t understand why you emphasize the performance improvement (Line 334) in the section 4.1, which is the part of approach, rather than experimentation.",
            "9": "The proposed approach relies on tedious preprocessing steps for three modalities.",
            "10": "For example, BLIP-2 needs much computing resource and time.",
            "11": "And simultaneously with GPT3.5, the application of this approach is very unrealistic.",
            "12": "Some comparisons with various prompt baselines (same LLM) are missing.",
            "13": "Missing the experiments and analysis about the importance of visual, speech and sound in your task.",
            "14": "With LLMs, it seems natural for your proposed approach performing better than MAF with only traditional Transformer.",
            "15": "This paper is hard to follow and the experiments are difficult to re-produce."
        }
    },
    "FGBWDf7Z19": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method for unsupervised speech segmentation by fine-tuning the XLS-R model on noisy word boundaries.",
            "1": "This approach leverages recent advancements in self-supervised learning (SSL) and semi-supervised learning, which is a significant contribution to the field.",
            "2": "**Comprehensive Evaluation**: The authors evaluate their method on five different corpora featuring various languages, demonstrating the robustness and generalizability of their approach.",
            "3": "The inclusion of zero-shot performance evaluation further strengthens the validity of their method.",
            "4": "**Significant Performance Improvement**: The proposed method consistently improves the performance of existing state-of-the-art systems, achieving an average 130% increase in token-F1 scores.",
            "5": "This substantial improvement highlights the effectiveness of their fine-tuning strategy.",
            "6": "**Detailed Analysis**: The paper provides a thorough analysis of the impact of different pre-training datasets and the importance of various optimization techniques.",
            "7": "The ablation study helps in understanding the contribution of each component to the overall performance.",
            "8": "**Open Source Code**: The availability of the code on GitLab promotes transparency and reproducibility, allowing other researchers to validate and build upon the work.",
            "9": "**Weaknesses:**\n\n1.",
            "10": "**Limited Dataset Variety**: While the paper evaluates the method on five different languages, all datasets are from the ZeroSpeech Challenge, which may not fully represent the diversity of real-world speech data.",
            "11": "Testing on more varied and noisy datasets could provide a better understanding of the method's robustness.",
            "12": "**Dependency on Initial Segmentation Quality**: The method's performance heavily relies on the quality of the initial word boundaries provided by existing segmentation systems.",
            "13": "If the initial boundaries are of poor quality, the fine-tuning process may not yield significant improvements.",
            "14": "**Complexity of Hyperparameter Tuning**: The method involves several hyperparameters that need to be carefully tuned, which can be time-consuming and computationally expensive.",
            "15": "The paper could benefit from a more detailed discussion on the sensitivity of the method to these hyperparameters.",
            "16": "**Lack of Real-World Application Scenarios**: The paper primarily focuses on the technical aspects and performance metrics.",
            "17": "Including real-world application scenarios and user studies could provide more practical insights into the method's usability and effectiveness.",
            "18": "**Potential Overfitting**: Although the authors argue that overfitting is not a concern due to the unsupervised nature of the method, the iterative fine-tuning process might still lead to overfitting on the specific characteristics of the development datasets.",
            "19": "A more rigorous evaluation on completely unseen and diverse datasets could address this concern.",
            "20": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of unsupervised speech segmentation by leveraging the XLS-R model and fine-tuning it on noisy word boundaries.",
            "21": "The method shows substantial improvements over existing state-of-the-art systems and demonstrates strong generalizability across different languages.",
            "22": "However, the dependency on initial segmentation quality, the complexity of hyperparameter tuning, and the need for more diverse dataset evaluation are areas that could be further explored.",
            "23": "Despite these weaknesses, the paper makes a valuable contribution to the field and provides a solid foundation for future research."
        },
        "QPRPCShLCt": {
            "0": "1. the fact that simple finetuning on pseudo labels via the general cross-entropy loss can bring such significant gain is very interestingly, which reveals interesting properties of the pretrained self-supervised speech models.",
            "1": "2. this work examined using pseudo labels generated by a range of very different state-of-the-art unsupervised systems, namely DPDP, VG-HuBERT, and DP-Parse, and observed universal improvement.",
            "2": "This indicate the robustness of the proposed approach.",
            "3": "Training one model on all languages leads to better performance than training language specific models.",
            "4": "Although this has been observed in large scale speech models (e.g.",
            "5": "OpenAI's Whisper), it's the first time I observe this phenomenon in small scale studies (the total amount of data is below 80 hours.)",
            "6": "4. a few tricks has being proposed to (potentially) make their approach works better, namely augmentation, smoothing, loss selection, peak detection.",
            "7": "These tricks are not novel, but the usage is new and they make sense intuitively.",
            "8": "These tricks could be valuable for researchers working on speech segmentation\n Although significant improvements are shown, more explanations are desired: \n\n1. with regard to the impressive performance zero-shot DP-Parse in Table 1, why is zero-shot working so well?",
            "9": "What kind of words are being predicted?",
            "10": "Or is there a pattern?",
            "11": "; \n2. what leads to the discrepancy between using pseudo-labels from DPDP, VG-HuBERT, and DP-Parse"
        },
        "2d9hcEjXjY": {
            "0": "Exploring unsupervised ways to perform word segmentation in speech is an interesting direction 1.",
            "1": "I wonder if this is unsupervised, as it still receives supervision signals from a trained system.",
            "2": "Training that system requires supervised labels, and it is uncertain where we can get those labels if the experiments are unsupervised.",
            "3": "This paper does not compare to any force-alignment systems.",
            "4": "The task, in my understanding, can be achieved reasonably easily using force alignment with a GMM-based system.",
            "5": "Why is the system trained on Gold reference achieved 100.0 token-F1?",
            "6": "So is that system already perfect?",
            "7": "What on earth is the evaluation metric?",
            "8": "How is it computed?"
        },
        "60X75uO91q": {
            "0": "The idea of the proposed method is simple yet yields good results.",
            "1": "By fine-tuning the added random feed-forward layer, the paper demonstrates that the pre-trained speech SSL system can be effective for the downstream task of speech segmentation.",
            "2": "Additionally, the paper utilizes speech from different languages, which provides a valuable exploration into the understanding of SSL speech models on multilingual speech segmentation tasks.",
            "3": "The proposed method does not convincingly demonstrate its generalization and novelty, mostly showing the effectiveness of fine-tuning a certain speech SSL model on speech segmentation tasks.",
            "4": "In addition to the main approach, there are many extra tricks used during training and evaluation, such as post-processing, loss sample selection, and data augmentation steps.",
            "5": "It would be clearer to add some analysis or ablation study to convince the readers and show the distinct contributions of these different steps."
        }
    },
    "7MmYaN93lb": {
        "llm_review": {
            "0": "#### Strengths:\n\n1.",
            "1": "**Novel Research Question**: The paper addresses an important and relatively unexplored question in the field of multilingual neural machine translation (MNMT) - whether robustness to noise can transfer across different languages.",
            "2": "This is a significant contribution as it can have practical implications for improving the reliability of MNMT systems.",
            "3": "**Comprehensive Experimental Setup**: The authors have designed a thorough experimental protocol to investigate robustness transfer.",
            "4": "They consider various types of noise (character-level, word-level, and multi-level) and different translation directions (one-to-many and many-to-one), which provides a comprehensive understanding of the phenomenon.",
            "5": "**Empirical Findings**: The paper presents clear empirical evidence that robustness can indeed transfer across languages.",
            "6": "The findings that character-level noise robustness is more likely to transfer across related languages, while word-level noise robustness is more likely to transfer across distant languages, are particularly insightful.",
            "7": "**Visualization and Analysis**: The use of PCA-based visualization to analyze the encoder outputs is a strong point.",
            "8": "It provides a clear and intuitive understanding of how the model's representations change with noise and training, supporting the conclusions drawn from the experiments.",
            "9": "**Additional Experiments**: The authors conducted additional experiments to investigate the effect of shared encoders on robustness transfer, which adds depth to the study and strengthens the validity of their conclusions.",
            "10": "**Reproducibility**: The paper provides detailed descriptions of the datasets, attack methods, and model hyperparameters, which facilitates reproducibility of the experiments.",
            "11": "#### Weaknesses:\n\n1.",
            "12": "**Limited Scope of Noise Types**: While the paper considers character-level and word-level noise, it does not explore other types of noise that might occur in real-world scenarios, such as syntactic or semantic noise.",
            "13": "Including a broader range of noise types could provide a more comprehensive understanding of robustness transfer.",
            "14": "**Synthetic Noise**: The study relies on synthetic noise, which may not fully capture the complexity of naturally occurring noise in real-world data.",
            "15": "Future work could benefit from incorporating naturally occurring noise to validate the findings.",
            "16": "**Underlying Mechanisms**: The paper primarily focuses on empirical findings and does not delve deeply into the underlying mechanisms that drive robustness transfer.",
            "17": "A more detailed theoretical analysis or exploration of the model's internal workings could provide additional insights.",
            "18": "**Generalization to Other Models**: The study uses a specific MNMT model (multilingual_transformer_iwslt_de_en).",
            "19": "It is unclear whether the findings generalize to other architectures or models.",
            "20": "Including experiments with different models could strengthen the generalizability of the conclusions.",
            "21": "**Impact of Data Size and Quality**: The paper does not discuss the impact of data size and quality on robustness transfer.",
            "22": "It would be interesting to see how these factors influence the transferability of robustness, especially for low-resource languages.",
            "23": "**Ethical Considerations**: While the paper mentions adherence to ethical guidelines, it does not discuss potential ethical implications of robustness transfer in MNMT, such as biases that might be transferred along with robustness.",
            "24": "#### Conclusion:\n\nOverall, the paper makes a significant contribution to the field of MNMT by exploring the transferability of robustness across languages.",
            "25": "The comprehensive experimental setup, clear empirical findings, and insightful analysis are commendable.",
            "26": "However, the study could be further strengthened by addressing the limitations related to the scope of noise types, underlying mechanisms, generalization to other models, and the impact of data size and quality.",
            "27": "Future work in these areas could provide a more holistic understanding of robustness transfer in MNMT."
        },
        "fOpWWQsTbr": {
            "0": "* The investigation of robustness transferability has not been explored before.",
            "1": "* The results clearly indicate that robustness is transferable.",
            "2": "This finding can be used by follow-up work that aims to improve MNMT.",
            "3": "* The connection to related work is insufficient.",
            "4": "Apart from the observation that the authors investigate robustness in a multilingual setting, and previous work considered a bilingual setting, it does not become clear how this work fits in with the broader field.",
            "5": "Black-box methods and white-box methods are mentioned, but not explained.",
            "6": "No attempt is made to put the experimental results in context with earlier findings of the field.",
            "7": "The paper of the quality would improve with some more discussion: which findings are in agreement with earlier work?",
            "8": "Which disagree, and what are the possible reasons?",
            "9": "From the paper, it's also not clear how the types of noise that you consider are (dis)similar to earlier work.",
            "10": "* The findings lack depth\n\nMost of the findings are unsurprising, which is of course not necessarily an issue.",
            "11": "Unfortunately, for the surprising findings, there is no further investigation.",
            "12": "For instance, comparing Table 2 and Table 3, the authors find that the degree of robustness transfer does not seem to rely on source language similarity, which is a highly surprising finding.",
            "13": "I think the paper would benefit a lot from digging into this deeper.",
            "14": "* Important information is missing.",
            "15": "Presentation could use improvement.",
            "16": "It is not clear what data is used exactly.",
            "17": "Authors mention TED Talks and News Commentary, but it is not clear which experiments use which dataset.",
            "18": "Maybe they are combined?",
            "19": "It is also not clear which test set is used.",
            "20": "TED Talks, News Commentary, or the combination thereof?",
            "21": "It is unclear how to interpret \"bold\" in the Table; this is not mentioned anywhere and it's not as simple as \"best score\".",
            "22": "The differences in Figure 4 and Figure 5 are very subtle, and based on these plots I would not conclude that representations learned using noisy data is more compact compared to representations trained on clean data.",
            "23": "It would be easy to quantify this claim by calculating representational differences, which could be used as additional evidence.",
            "24": "The BLEU signature is missing."
        },
        "MVgXDzRDUa": {
            "0": "- The authors conduct a series of analyses on the core research question, from shallow to deep, providing sufficient (some even redundant) evidence for their claim.",
            "1": "- The findings may motivate more future work on the robustness of the multilingual encoders (known to be vulnerable as QE models).",
            "2": "- Many figures in the paper lead to similar conclusions (e.g., Figure 2 vs.",
            "3": "Figure 3, Figure 4 vs.",
            "4": "Figure 5), which take up lots of space in the paper and make the paper look verbose.",
            "5": "It would be better to move some of them to the appendix.",
            "6": "- Figure 4,5 look a little confusing and need more detailed descriptions.",
            "7": "- For most translation directions, the robustness transferred from EN-FR is not significant enough (still lags behind the results on clean corpus by ≈10 BLEU scores).",
            "8": "Although the authors use growth rates to make the benefits look more obvious in Tables 1,2,3, the transfer effects are still limited.",
            "9": "- I would like to see more results based on multilingual pre-trained models, e.g., mBART (Liu et al., 2020), mRASP2 (Pan et al., 2021).",
            "10": "Liu et al.",
            "11": "Multilingual Denoising Pre-training for Neural Machine Translation.",
            "12": "TACL 2020.",
            "13": "Pan et al.",
            "14": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation.",
            "15": "ACL 2021."
        },
        "bQzh03o4Kf": {
            "0": "The motivation of this paper is clear.",
            "1": "The proposed experiments illustrate some insightful findings.",
            "2": "Although adversarial attacks enhance the robustness of MNMT models, the sacrifice on clean test set seems not trivial (e.g., 1~2 BLEU score on the en-fr translation task in Table1).",
            "3": "It may be helpful to also investigate the trade-off between robustness and clean-performances of MNMT models.",
            "4": "Some cases would be helpful when explaining the findings in section 4.3 and 4.4, i.e., why character-level attack is more useful for enhancing MNMT model robustness in related languages?"
        }
    },
    "XMpzcC9L5z": {
        "llm_review": {
            "0": "A Case Study on BIG-bench\"\n\n#### Strengths\n\n1.",
            "1": "**Comprehensive Analysis**: The paper provides a thorough investigation into the predictability of large language model (LLM) capabilities using a substantial dataset from BIG-bench.",
            "2": "The authors explore various dimensions such as model families, parameter sizes, tasks, and in-context examples, offering a holistic view of the problem.",
            "3": "**Methodological Rigor**: The study employs a range of machine learning methods, including matrix completion, tree-based methods, and neural networks, to predict LLM performance.",
            "4": "This diversity in approaches ensures that the findings are robust and not biased towards a particular method.",
            "5": "**High Predictive Accuracy**: The results demonstrate that LLM performance can be predicted with high accuracy (R2 > 95%) using an MLP-based predictor.",
            "6": "This is a significant finding, suggesting that past experiment records can be effectively used to forecast future performance.",
            "7": "**Practical Implications**: The paper addresses practical concerns for LLM users, developers, and the research community.",
            "8": "By predicting LLM performance, users can make informed decisions about which models to try, developers can prioritize evaluations, and researchers can identify hard-to-predict capabilities.",
            "9": "**Innovative \"Small-bench\" Concept**: The introduction of the \"small-bench\" concept is a novel contribution.",
            "10": "The authors show that a smaller, well-chosen subset of tasks can effectively predict performance on the full BIG-bench, which has significant implications for efficient model evaluation.",
            "11": "**Empirical Validation**: The study is well-supported by empirical evidence.",
            "12": "The authors conduct extensive experiments, including different train-test splits and challenging generalization scenarios, to validate their findings.",
            "13": "**Task Diversity and Value**: The paper highlights the importance of task diversity and task value in constructing an effective \"small-bench.\"",
            "14": "This insight is valuable for designing future benchmarks and evaluation protocols.",
            "15": "**Reproducibility**: The authors provide detailed descriptions of their methods, data splits, and hyperparameters, which enhances the reproducibility of their work.",
            "16": "The inclusion of code availability further supports this.",
            "17": "#### Weaknesses\n\n1.",
            "18": "**Limited Scope of Models**: While the paper includes six model families, the diversity of models is still somewhat limited.",
            "19": "Including more model families, especially those with different architectures and training paradigms, could provide a more comprehensive understanding of LLM capabilities.",
            "20": "**Extrapolation Challenges**: The study primarily focuses on interpolation settings.",
            "21": "The results in extrapolation settings, such as predicting performance for larger models, are less promising.",
            "22": "This limitation is acknowledged but could be explored further.",
            "23": "**Dependence on BIG-bench**: The findings are based on the BIG-bench dataset, which, while extensive, may not fully represent real-world tasks.",
            "24": "The generalizability of the results to other benchmarks or real-world applications remains an open question.",
            "25": "**Evaluation Metrics**: The paper primarily uses RMSE and R2 as evaluation metrics.",
            "26": "While these are standard for regression tasks, additional metrics such as rank correlation are introduced but not deeply analyzed.",
            "27": "A more detailed discussion on the implications of different metrics could strengthen the evaluation.",
            "28": "**Search Algorithm Stability**: The performance of search algorithms for constructing \"small-bench\" subsets is noted to be unstable.",
            "29": "This suggests that the optimization process may need refinement.",
            "30": "Exploring more sophisticated search techniques or hybrid approaches could address this issue.",
            "31": "**Task Representation Learning**: The task representations learned by the MLP are used for clustering, but the paper does not delve deeply into the nature of these representations.",
            "32": "Understanding what these representations capture could provide deeper insights into task similarities and model performance.",
            "33": "**Impact of Pre-training Data**: The study does not consider the impact of pre-training data and other meta-data on LLM performance.",
            "34": "Including such factors could enhance the predictive models and provide a more nuanced understanding of LLM capabilities.",
            "35": "#### Conclusion\n\nOverall, the paper makes significant contributions to understanding and predicting LLM capabilities.",
            "36": "It offers practical insights and introduces innovative concepts like \"small-bench\" for efficient model evaluation.",
            "37": "While there are areas for improvement, such as expanding the scope of models and exploring extrapolation settings, the strengths of the paper far outweigh its weaknesses.",
            "38": "The findings have important implications for the future development and evaluation of LLMs, and the study sets a strong foundation for further research in this area."
        },
        "wXsywxorgC": {
            "0": "The paper introduces a novel method for predicting the performance of Large Language Models (LLMs), which has significant practical implications.",
            "1": "This method could aid developers and researchers in conserving resources when training models, given the vast array of possible combinations of models, hyperparameters, evaluation tasks, and in-context examples.",
            "2": "The results of the paper are notable.",
            "3": "The high R2  scores indicate that the BIG-bench dataset contains highly predictable patterns, thus validating the authors' approach to performance prediction.",
            "4": "In addition, the authors present the idea of “small-bench,” which could reduce computational barriers for models with limited training computation at their disposal.",
            "5": "They provide an understanding of how to construct “small-bench” effectively, suggesting potential applications of these methods to optimize other benchmarks in different contexts Only RMSE and R2 scores are reported.",
            "6": "While this limitation is addressed in the paper, they do not capture the complete picture of a model’s accuracy.",
            "7": "There are a relatively small number of model families in BIG-bench.",
            "8": "This could lead to results, even in the challenging Train-Test split, being due to intrinsic correlation between these model families that might not translate to other models.",
            "9": "There is little discussion of the variety of models and tasks in BIG-bench.",
            "10": "It’s unclear whether the model families present in BIG-bench are significantly different from each other (at least as much as another model with a different pre-training pipeline might be)."
        },
        "Q3AvcVvi5X": {
            "0": "- The problem the authors are trying to solve is real and very relevant to the needs of LLM evaluation: (1) how to predict an LLM's performance without actually running the experiment, and (2) how to quantitatively select the best subset of comprehensive benchmark for quick evaluation.",
            "1": "- The proposed approach is effective even for challenging scenarios.",
            "2": "- The presentation of the main idea is easy to follow, with comprehensive experimental results to back up.",
            "3": "- Choosing explanatory variables in your design matrix does not have a sufficient explanation.",
            "4": "For example, two models with the same architecture could perform widely differently if training one converges while the other fails.",
            "5": "- The authors do not discuss how the emergent abilities of LLMs could influence the regression models' performance.",
            "6": "For example, how do the LLMs' emergent abilities contribute to the RMSE of your regression models?",
            "7": "- The authors need to discuss what conditions should be met to apply their proposed approach.",
            "8": "For example, the authors do not analyze T0/T5 models (L967-969) because of the unavailability of sufficient data to train the model.",
            "9": "However, the authors do not discuss the minimal data requirement to develop a valid analysis with the proposed approach.",
            "10": "- The authors claim that there are limitations of BIG-Bench Hard (BBH) and BIG-Bench Lite (BBL) based on the proposed metrics.",
            "11": "However, to establish the discovered BIG-Bench subset's advantage over BBH and BBL, the authors should use an independent source of information.",
            "12": "For example, a previously ignored LLM due to low ranking on BBH and HHL is found to rank high on the discovered benchmark, and this model is able to do something that high-ranking models on BBH and BBL could not do well."
        },
        "KyZTVwOc5a": {
            "0": "The experiments in this article demonstrate that some capabilities of LLMs are predictable.",
            "1": "This method might serve as a reference metric for future LLM development.",
            "2": "Furthermore, the article emphasizes the significant impact of data on LLMs and suggests a vast space for further research.",
            "3": "This article relies solely on superficial information to assess the predictability of LLM capabilities, which weakens its persuasiveness.",
            "4": "The experiments in this article are quite limited, as all tests were conducted on just one dataset.",
            "5": "This might introduce potential biases and uncertainties regarding its generalization capabilities."
        }
    },
    "rJhk7Fpnvh": {
        "llm_review": {
            "0": "**Comprehensive Analysis**: The paper provides a thorough investigation into the sources of hallucination in large language models (LLMs) on natural language inference (NLI) tasks.",
            "1": "By examining multiple LLM families (LLaMA, GPT-3.5, and PaLM), the authors ensure that their findings are robust and not model-specific.",
            "2": "**Identification of Key Biases**: The identification of two primary biases—memorization at the sentence level and statistical patterns at the corpus level—offers valuable insights into why LLMs might produce hallucinations.",
            "3": "This is crucial for understanding the limitations of current models and guiding future improvements.",
            "4": "**Controlled Experiments**: The use of controlled experiments, such as the random premise task and the generic argument task, effectively isolates the factors contributing to hallucination.",
            "5": "This methodological rigor strengthens the validity of the findings.",
            "6": "**Practical Implications**: The discussion on the implications of these biases for real-world applications, such as question answering and summarization, highlights the practical relevance of the research.",
            "7": "This makes the paper not only theoretically significant but also practically useful.",
            "8": "**Detailed Related Work**: The paper situates its contributions within a well-documented context of related work, acknowledging previous findings and clearly delineating how it extends or differs from them.",
            "9": "**Open Source Code**: By providing the code and LLM outputs, the authors promote transparency and reproducibility, which are essential for scientific progress.",
            "10": "#### Weaknesses\n\n1.",
            "11": "**Limited Scope of Biases**: While the paper identifies two significant biases, it acknowledges that these are not exhaustive.",
            "12": "There may be other sources of hallucination that are not explored, which could limit the comprehensiveness of the conclusions.",
            "13": "**Dependence on Specific Datasets**: The experiments are primarily conducted on the Levy/Holt and RTE-1 datasets.",
            "14": "While these are well-known benchmarks, the findings might not generalize to other types of NLI datasets or tasks that involve different linguistic phenomena.",
            "15": "**Zero-shot vs. Few-shot**: The paper notes that LLMs perform poorly in zero-shot settings and better in few-shot settings.",
            "16": "However, the few-shot setting used involves minimal examples, which might not fully capture the potential of more extensive few-shot learning.",
            "17": "This could be explored further to understand the limits of few-shot learning in mitigating hallucinations.",
            "18": "**Instruction Ineffectiveness**: The attempt to instruct LLMs to ignore attestedness shows only marginal improvements.",
            "19": "This suggests that simple prompt engineering might not be sufficient to address deep-seated biases, but the paper does not explore more sophisticated techniques that might be more effective.",
            "20": "**Evaluation Metrics**: The use of AUC norm and F-1 scores provides a good measure of performance, but additional metrics like precision-recall curves or confusion matrices could offer more granular insights into model behavior across different conditions.",
            "21": "**GPT-4 Analysis**: The analysis of GPT-4 is limited and not as detailed as for the other models.",
            "22": "Given GPT-4's prominence, a more in-depth examination could provide valuable insights, especially since it claims state-of-the-art performance on various tasks.",
            "23": "#### Conclusion\n\nOverall, the paper makes significant contributions to understanding the sources of hallucination in LLMs on NLI tasks.",
            "24": "It identifies key biases and provides a robust experimental framework to study them.",
            "25": "However, the scope of biases explored is limited, and the effectiveness of simple prompt engineering techniques is marginal.",
            "26": "Future work could expand on these findings by exploring additional biases, using a broader range of datasets, and investigating more sophisticated methods to mitigate hallucinations."
        },
        "ocpSwMg3Tn": {
            "0": "The theory proposed in this paper is of important value in the task of characterization and mitigation of hallucinatory behavior in LLMs.",
            "1": "It was quite unclear how the experiments performed in the work to corroborate the authors’ theory did that.",
            "2": "In the random premise task – how could the authors ensure that the random predicate indeed resulted in NO-ENTAIL?",
            "3": "I understand that such random sampling has a very small probability of resulting in something that is not NO-ENTAIL, but given that many predicates have synonyms, and other predicates whom they entail, and hence by proxy the current hypothesis might also entail, it feels like it is crucial to ensure that indeed NO-ENTAIL was the case for all the instances (as this is not the train set, but rather the evaluation set).",
            "4": "Additionally, it was not clear how the generic argument task and the random argument task proved what the authors claimed.",
            "5": "All in all, the whole dataset transformation and the ensuing experimental setup felt very cumbersome, and not very clear."
        },
        "SUxmLppr6M": {
            "0": "- The paper shows an important and easy-to-miss limitation of current LMs, highlighting the need to disentangle performance on specific datasets form performance on the general task of interest.",
            "1": "- Very elegant experimental design, both showing evidence for the two new heuristics, and isolating the mechanism by which the model memorizes.",
            "2": "I don't see any reason to reject the paper.",
            "3": "One technical limitation is the use of proxies for occurrence in the training data.",
            "4": "Hopefully indexing the pretraining data of large LMs would enable measuring to what extent these proxies actually reflect occurrence in the training data."
        },
        "aHtYsPHtWX": {
            "0": "Overall I think the the paper presents a nice and creative methodology for showing that biases may lead LLMs to hallucinate.",
            "1": "At the beginning I had a hard time understanding the main contribution of the paper; however, once understood, it believe the conclusions made by this work are significant to the NLP field.",
            "2": "While the contribution may be considered as significant, I believe that it might be a bit limited (only NLI, only 3 LLMs) to be considered for the main EMNLP stage.",
            "3": "Also I find the paper a bit hard to follow.",
            "4": "There are not enough examples, and it took me a lot of time to understand the methodology while I believe it could be described in a better way (for example, relative frequency bias should be better explained).",
            "5": "Other than that I have no major concerns."
        }
    },
    "3AxESAk0Re": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method, STAIR, which maps images and texts to a sparse token space.",
            "1": "This approach is innovative as it combines the interpretability of sparse representations with the performance benefits of dense embeddings.",
            "2": "**Performance Improvement**: The STAIR model significantly outperforms the state-of-the-art CLIP model in image-text retrieval tasks, with notable improvements in Recall@1 on COCO-5k and Flickr30K datasets.",
            "3": "This demonstrates the effectiveness of the proposed method.",
            "4": "**Interpretability**: One of the key strengths of STAIR is its interpretability.",
            "5": "By mapping to a sparse token space where each token is a (sub-)word from a vocabulary, the model's outputs are more understandable to humans.",
            "6": "This is a significant advantage over dense embeddings, which are often considered black boxes.",
            "7": "**Multi-Stage Training Strategy**: The multi-stage training strategy proposed in the paper is well thought out and effectively grounds the sparse embeddings in meaningful tokens.",
            "8": "This strategy ensures that the embeddings are both interpretable and performant.",
            "9": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of the STAIR model, including zero-shot image-text retrieval, zero-shot visual classification, and linear probing tasks.",
            "10": "The results are consistently strong across multiple benchmarks.",
            "11": "**Quantitative and Qualitative Analysis**: The paper includes both quantitative and qualitative analyses of the model's interpretability.",
            "12": "This dual approach provides a comprehensive understanding of how the model performs and why it is more interpretable than previous methods.",
            "13": "**Weaknesses:**\n\n1.",
            "14": "**Complexity of Multi-Stage Training**: While the multi-stage training strategy is effective, it adds complexity to the training process.",
            "15": "This could be a barrier to adoption for practitioners who may prefer simpler training pipelines.",
            "16": "**Limited Discussion on Computational Overhead**: The paper does not provide a detailed analysis of the computational overhead introduced by the multi-stage training process.",
            "17": "Understanding the trade-offs between performance gains and computational costs would be valuable.",
            "18": "**Potential Bias in Training Data**: The paper mentions the use of a large internal dataset and web-crawled data, which could introduce biases.",
            "19": "A more detailed discussion on how these biases are mitigated or their impact on the model's performance would strengthen the paper.",
            "20": "**Generalization to Other Tasks**: While the paper demonstrates strong performance on image-text retrieval and classification tasks, it would be beneficial to see how the STAIR model performs on other vision-language tasks, such as visual question answering or image captioning.",
            "21": "**Sparse Embedding Limitations**: The paper acknowledges that relying solely on sparse embeddings may lead to challenges in capturing the full semantic richness of the data.",
            "22": "Exploring hybrid approaches that combine sparse and dense embeddings could address this limitation.",
            "23": "**Scalability of Token Space**: The use of a large vocabulary for the token space is a strength in terms of interpretability, but it may pose scalability challenges.",
            "24": "The paper could benefit from a discussion on how to manage the size of the token space as the model scales to larger datasets.",
            "25": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of vision-language models by introducing a method that combines the interpretability of sparse representations with the performance benefits of dense embeddings.",
            "26": "The STAIR model's strong performance across multiple benchmarks and its improved interpretability make it a valuable contribution.",
            "27": "However, the complexity of the multi-stage training process and potential biases in the training data are areas that could be further addressed.",
            "28": "Future work could explore hybrid approaches and scalability solutions to enhance the model's applicability and robustness."
        },
        "9xEdZdNOJL": {
            "0": "* The problem addressed by this paper, i.e., learning interpretable image and text representations, is important, which can facilitate the transparency of large VLMs.",
            "1": "* The proposed methods, i.e., the sparse embedding space and the multi-stage training strategy, are reasonable and effective.",
            "2": "* The experiments are well-designed and the results are convincing, which validate the advantage of STAIR in terms of both interpretability and effectiveness across various downstream vision-language tasks.",
            "3": "* The limitations of this work are properly recognized in the paper, which is accompanied by discussions on potential solutions and future directions.",
            "4": "* The paper is well-written and easy to follow.",
            "5": "* No quantitative evaluation supports the claim that STAIR is more efficient than CLIP in text-image retrieval.",
            "6": "* It is unclear whether STAIR can still enhance the interpretability and downstream performance, when combined with more advanced VLMs, e.g., BLIP[1].",
            "7": "[1] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"
        },
        "vb5Qn02eHP": {
            "0": "The paper explores the possibility of employing sparse and grounded token space in image-text retrieval to make embeddings interpretable, with performance improvement on CLIP.",
            "1": "1.Although I agree that the sparse embedding mentioned is more interpretable, this interpretability seems unnecessary in image-text retrieval.",
            "2": "Dense embeddings that machines can understand are enough.",
            "3": "On the other hand, the dense embeddings maybe achieve better approximations to natural semantic (manifold) spaces.",
            "4": "2.The paper claims the proposed STAIR can tackle “…are built on a fixed vocabulary, which cannot handle out-of-vocabulary concepts” in line 87-90.",
            "5": "However, the sparse token space adopts the vocabulary as the basis of embedding space.",
            "6": "How does STAIR deal with words out of the vocabulary?",
            "7": "It seems that there is no essential difference from the dense situation.",
            "8": "3.It is not clear whether the performance improvement comes from the sparse and grounded embedding or the designed training recipe.",
            "9": "How will performance change when porting the training recipe to models with dense embedding?",
            "10": "There seems to be a lack of ablation experiments in this regard.",
            "11": "4.In addition, STAIR needs to be applied to other CLIP or ALIGN based models to verify its effectiveness."
        },
        "VBix0ymHeJ": {
            "0": "A novel representation method for vision and language balancing performance, interpretability and efficiency.",
            "1": "Sufficient experiments verify the effectiveness of this method.",
            "2": "One major part of the training data of this model is 1B internal image-text pairs, which may not be released and cause difficulty to reproduce the experimental results.",
            "3": "As mentioned in Section 6.1, multi-stage training plays a crucial role in ensuring STAIR embedding grounded to meaningful tokens.",
            "4": "But there is no clear explanation about this or detailed discussion about the contribution of each training stage.",
            "5": "As claimed in lines 228-230. all parameters including BERT and MLM head are trained at the stage 1, the only difference between stage 1 and STAIR_{SINGLE-STAGE} is the text masking strategy.",
            "6": "But this masking strategy doesn't seem to resolve the redefining problem ( As declared in lines 218-220, this masking strategy is designed to avoid model learning a shortcut by relying on less common tokens to bridge the modality gap)."
        }
    },
    "DmrIEHJxN5": {
        "llm_review": {
            "0": "**Innovative Framework**: The paper introduces a novel framework, Cognitive Tree (CogTree), inspired by the dual process theory in cognitive science.",
            "1": "This framework effectively combines an intuitive system and a reflective system to decompose complex reasoning tasks into manageable sub-problems, which is a significant contribution to the field of natural language processing (NLP).",
            "2": "**Efficiency**: One of the standout strengths of this work is its ability to achieve performance comparable to GPT-3.5 (175B parameters) using significantly smaller models (1.5B and 7B parameters).",
            "3": "This efficiency in terms of computational resources and inference costs is highly valuable for practical applications.",
            "4": "**Comprehensive Evaluation**: The authors conduct extensive experiments on two challenging reasoning tasks, Entailment Bank (EB) and GSM8K, demonstrating the effectiveness of their approach.",
            "5": "The results show that CogTree outperforms or matches the performance of much larger models, which is impressive.",
            "6": "**Contrastive Learning**: The use of contrastive learning to enhance the reflective system's ability to distinguish between correct and incorrect decisions is a clever approach.",
            "7": "This method improves the model's validation capabilities, leading to better overall performance.",
            "8": "**Detailed Analysis**: The paper provides a thorough analysis of the results, including error analysis and ablation studies.",
            "9": "This level of detail helps in understanding the strengths and limitations of the proposed approach.",
            "10": "**Clear Presentation**: The paper is well-organized and clearly presents the methodology, experimental setup, and results.",
            "11": "The use of figures and tables to illustrate the framework and results enhances the readability and comprehension of the paper.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Scope of Datasets**: While the paper demonstrates the effectiveness of CogTree on two datasets (EB and GSM8K), it would have been beneficial to evaluate the framework on a broader range of reasoning tasks and datasets.",
            "14": "This would provide a more comprehensive understanding of the generalizability of the approach.",
            "15": "**Dependence on In-Context Examples**: The intuitive system relies heavily on in-context examples for problem decomposition.",
            "16": "This dependence may limit the model's ability to generalize to unseen problems or domains where such examples are not readily available.",
            "17": "**Reflective System Complexity**: The reflective system's reliance on contrastive learning and the need for generating and validating multiple candidate decompositions can introduce additional complexity and computational overhead.",
            "18": "While the paper addresses efficiency, the added complexity of the reflective system might offset some of the gains in smaller models.",
            "19": "**Scalability Concerns**: The paper mentions the limitation of computational resources, which prevented testing the method on larger models.",
            "20": "This raises questions about the scalability of the approach when applied to even larger models or more complex tasks.",
            "21": "**Potential Overfitting**: The use of specific datasets for training and validation raises the concern of potential overfitting.",
            "22": "It would be useful to see how the model performs on completely unseen datasets or in a zero-shot setting to better understand its robustness.",
            "23": "**Lack of Real-World Applications**: While the paper focuses on theoretical and experimental validation, it lacks discussion on real-world applications and how the proposed framework can be integrated into practical systems.",
            "24": "Including such discussions would enhance the paper's impact and relevance.",
            "25": "**Conclusion:**\n\nOverall, \"From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models\" presents a significant advancement in the field of NLP by introducing an innovative and efficient framework for complex reasoning tasks.",
            "26": "The strengths of the paper lie in its novel approach, efficiency, comprehensive evaluation, and clear presentation.",
            "27": "However, there are areas for improvement, including the scope of datasets, dependence on in-context examples, complexity of the reflective system, scalability concerns, potential overfitting, and lack of real-world applications.",
            "28": "Addressing these weaknesses in future work would further strengthen the contributions of this research."
        },
        "MJPUc8mPLh": {
            "0": "- The paper addresses two relevant issues: the low performance of LLMs in reasoning problems and their computational costs.",
            "1": "- It draws inspiration from cognitive science, making it easier to understand.",
            "2": "Additionally, it applies contrastive learning, where reasoning steps and negative results play a major role which makes the system more understandable.",
            "3": "- The experiments seem to be well-designed and robust.",
            "4": "- The results enhance the accuracy of the systems and, more importantly, significantly reduce the number of parameters required for the calculation, although it requires more intermediate steps.",
            "5": "- A key point of the method, the decomposition of the queries, is not explained in detail, making very difficult to understand how the sub-questions are generated.",
            "6": "In section 3.3 is mentioned \"by leveraging in-context examples\", but it's not clear to me what this means."
        },
        "424YY1bGnZ": {
            "0": "The question whether LMs are able to gain any sort of semantic knowledge and logical reasoning ability is a relevant one, so that paper is fully appropriate for the Audience of this Conference.",
            "1": "- It seems that LMs are not fully adequate to deal with higher-order tasks, such as complex logical reasoning.",
            "2": "Their usage should thus be supported by some sort of either theoretical or practical motivation."
        },
        "La4cpiOiUn": {
            "0": "A thorough experiment about the CogTree method.",
            "1": "Prove the effectiveness of finetuning and adding a reflective system.",
            "2": "Although the story of the intuitive and reflective system sounds fancy, essentially, it's a traditional system with a generator along with a scoring mechanism (discriminator).",
            "3": "The difference lies in the decomposition part, which was discussed widely in previous work like Least-to-Most prompting, Self-Ask,  Decomposition Distillation, DecompT5, etc.",
            "4": "Expected to see the performance gain compared with other SOTA finetune-based methods.",
            "5": "Expected to see a general method to adapt to datasets that do not have intermediate subquestions and answers, which is common in most situations."
        }
    },
    "YokfK5VOoz": {
        "llm_review": {
            "0": "**Timely and Relevant Topic**: The paper addresses a critical and timely issue in the field of natural language processing (NLP) and artificial intelligence (AI) — the potential for large language models (LLMs) to violate copyright laws through verbatim memorization and reproduction of copyrighted texts.",
            "1": "This is a pressing concern as LLMs become more integrated into various applications.",
            "2": "**Comprehensive Experiments**: The authors conducted extensive experiments across multiple LLMs, including both open-source and closed-source models.",
            "3": "This comprehensive approach provides a broad view of the extent to which different models memorize and reproduce copyrighted material.",
            "4": "**Diverse Data Sources**: By using a variety of data sources, including popular books and LeetCode coding problems, the paper demonstrates the potential for copyright violations across different types of content.",
            "5": "This diversity strengthens the argument and highlights the widespread nature of the issue.",
            "6": "**Clear Metrics and Methodology**: The use of Longest Common Subsequence (LCS) and Levenshtein Distance as metrics for evaluating verbatim memorization is well-justified and clearly explained.",
            "7": "The methodology for probing the models is also detailed, making the experiments reproducible.",
            "8": "**Insightful Findings**: The paper provides valuable insights into the relationship between model size and the extent of memorization, as well as the impact of content popularity on memorization rates.",
            "9": "These findings are crucial for understanding the risks associated with larger and more widely-used models.",
            "10": "**Ethical Considerations**: The authors address the ethical implications of their findings, emphasizing the need for responsible usage of copyrighted material and adherence to legal regulations.",
            "11": "This ethical perspective is important for guiding future research and development in the field.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Scope of Data**: While the paper uses a diverse set of data sources, the selection is still relatively narrow.",
            "14": "The focus on best-selling books and LeetCode problems may not fully represent the broader landscape of copyrighted materials.",
            "15": "Including a wider range of content types could provide a more comprehensive view.",
            "16": "**Lack of Legal Analysis**: The paper explicitly states that it does not draw legal conclusions, which is understandable given the complexity of copyright law.",
            "17": "However, a more detailed discussion of the legal framework and potential implications of the findings would enhance the paper's impact and provide clearer guidance for stakeholders.",
            "18": "**Potential for Optimized Prompts**: The authors acknowledge that their results are likely conservative and that more carefully optimized prompts could unlock even more verbatim memorization.",
            "19": "This suggests that the true extent of the issue may be underestimated.",
            "20": "Future work should explore more sophisticated prompting techniques to fully assess the risks.",
            "21": "**Ambiguity in Results Interpretation**: The paper notes instances where models confabulate or produce ambiguous outputs when asked to reproduce specific texts.",
            "22": "While this is an interesting finding, it complicates the interpretation of the results.",
            "23": "A more detailed analysis of these cases would help clarify the extent to which models truly memorize versus generate plausible but incorrect text.",
            "24": "**Focus on Larger Models**: The paper primarily highlights the risks associated with larger models, which is important.",
            "25": "However, it would be beneficial to also discuss potential mitigation strategies for smaller models and how these might scale as models grow in size and complexity.",
            "26": "**Limited Discussion on Mitigation**: While the paper identifies the problem, it offers limited discussion on potential solutions or safeguards to prevent copyright violations.",
            "27": "Future work should explore and evaluate methods for controlling memorization and ensuring compliance with copyright laws.",
            "28": "#### Conclusion\n\nOverall, \"Copyright Violations and Large Language Models\" is a well-executed and important study that sheds light on a critical issue in the field of NLP and AI.",
            "29": "The paper's strengths lie in its comprehensive experiments, clear methodology, and insightful findings.",
            "30": "However, it could benefit from a broader scope of data, more detailed legal analysis, and a deeper exploration of mitigation strategies.",
            "31": "Despite these weaknesses, the paper makes a significant contribution to the ongoing discussion about the ethical and legal implications of LLMs and sets the stage for future research in this area."
        },
        "9TaJAFEz5x": {
            "0": "- The problem that the authors tackle is an important and practical one -- it is essential to shed light on the legal implications of using LLMs in the wild, and this line of research could hopefully contribute to informing policy in the future.",
            "1": "- The authors analyze many recent state of the art LLMs, and therefore these results could immediately be of interest to researchers and LLM trainers who could test and mitigate copyright infringements in their usage of these models.",
            "2": "- I am unsure if the setting and findings are non-trivially different from previous work on memorization.",
            "3": "For instance, the method used is the exact same as Carlini et al., 2023 (the authors do declare this) -- in that both papers use 50 tokens to prompt the open source models and measure sequence overlaps with the generated token sequences.",
            "4": "Furthermore, the results that models are indeed memorizing copyrighted text is also something that can be predicted by Carlini et al.’s paper.",
            "5": "- There also seems to be some discrepancy about what counts as a copyright violation.",
            "6": "The abstract suggests a limit of 50-1000 words but the results are interpreted only using the 50-word limit.",
            "7": "- Due to the complexities surrounding the topic of copyright violation, this study essentially reduces to one involving long-text memorization (albeit within a focused domain: books and coding problems), which has already been shown for LLMs by Carlini et al., 2023 (though for different LLMs)."
        },
        "bJZaPgivLs": {
            "0": "The paper raises a very crucial question about legality of LLMs and provides some quantitative as well as qualitative evidence for copyright infringement by LLMs.",
            "1": "Update post rebuttal:\n\nI agree with their discerning rebuttal that the experiments around copyright violation specifically deserve to be considered, and in that their methods seem apt and sound.",
            "2": "Hence, I am updating both my soundness and excitement scores.",
            "3": "I would still like to see a more thorough discussion or some other way to assure readers of a legally sound narrative, for example, by listing lawyers who may have been consulted for the paper in the camera ready.",
            "4": "It is hard for Computer Science or Linguistics educated reviewers (like myself) to judge the legal arguments in the paper.",
            "5": "Despite the authors not explicitly making big legal claims, the major contribution of the paper is apparently best left to be judged in a court of law.",
            "6": "The other quantitative insights, e.g., that larger LLMs quote verbatim more, have already been established in literature, as cited by the authors themselves.",
            "7": "Therefore, this paper adds only a few qualitative examples and a few quantitative statistics on fraction of times the copyrighted source was quoted verbatim.",
            "8": "A larger study concerning more research questions and analysis will be better appreciated, e.g., what kind of material is more likely to be quoted verbatim?",
            "9": "Are there ways to modify (at least the smaller models) either at pretraining or at inference to avoid copyright infringement?"
        },
        "VDBHuR4U41": {
            "0": "people really want to know will LLM violate copyright law.",
            "1": "the definition of copyright violation is not clear."
        }
    },
    "fhEkqMyvb0": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel multi-head hierarchical attention model that effectively encodes the structure of long documents.",
            "1": "This approach is innovative and addresses the limitations of previous models that overfit to the writing style of specific news outlets.",
            "2": "**Robustness and Generalizability**: By considering both sentence-level semantics and document-level rhetorical structure, the proposed model demonstrates improved robustness and generalizability in detecting political bias.",
            "3": "This is a significant advancement over traditional document classification methods.",
            "4": "**Comprehensive Evaluation**: The authors conduct extensive experiments to compare their model against a BERT baseline.",
            "5": "They evaluate the model's performance on two disjoint test sets, sensitivity to training data, and sensitivity to test data.",
            "6": "This thorough evaluation provides strong evidence of the model's effectiveness.",
            "7": "**Human and Machine Evaluation**: The paper includes both machine and human evaluations to assess the informativeness of the main sentences identified by the model.",
            "8": "This dual approach strengthens the validity of the results and demonstrates the practical utility of the model.",
            "9": "**Structural Analysis**: The authors perform a detailed structural analysis of news articles, identifying distinct clusters that capture different narrative structures.",
            "10": "This analysis provides valuable insights into the rhetorical practices in journalism and validates the model's ability to capture these structures.",
            "11": "**Code Availability**: The authors have made their code available on GitHub, which promotes transparency and allows other researchers to replicate and build upon their work.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Language Scope**: The model is only applied to English news articles, which limits its applicability to non-English languages and non-Western contexts.",
            "14": "The authors acknowledge this limitation and suggest future work to expand the model to other languages and cultural backgrounds.",
            "15": "**Dataset Constraints**: The paper relies on existing datasets that may not capture the full range of political biases.",
            "16": "The authors note that the available datasets reflect a limited range of discretized political bias, and future work could focus on introducing better datasets and resources for modeling the many dimensions and nuances of bias.",
            "17": "**Complexity of Model**: The multi-head hierarchical attention model is complex and may require significant computational resources for training and inference.",
            "18": "This complexity could limit its practical deployment in real-world applications, especially for smaller organizations with limited resources.",
            "19": "**Interpretability**: While the model aims to be more explainable by propagating document-level labels to sentence-level relevance, the interpretability of the multi-head attention mechanism could still be challenging for non-expert users.",
            "20": "Further work on improving the interpretability of the model's decisions would be beneficial.",
            "21": "**Generalization to Other Domains**: The model is specifically designed for political bias detection in news articles.",
            "22": "Its applicability to other domains, such as social media or other types of documents, is not explored in the paper.",
            "23": "Future research could investigate the model's performance in different contexts.",
            "24": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of political bias detection in news articles.",
            "25": "The novel multi-head hierarchical attention model demonstrates improved robustness and generalizability by capturing both sentence-level semantics and document-level rhetorical structure.",
            "26": "The comprehensive evaluation and structural analysis provide strong evidence of the model's effectiveness.",
            "27": "However, the model's applicability is currently limited to English news articles, and its complexity may pose challenges for practical deployment.",
            "28": "Future work could focus on expanding the model to other languages, improving interpretability, and exploring its generalization to other domains."
        },
        "hxSypqUzVV": {
            "0": "[A1] The proposed model for bias detection seems novel and easy to replicate which allows it to be used by researchers in future studies.",
            "1": "[A2] The experiments done in the paper seems sound and the qualitative experiments are interesting.",
            "2": "[R1] Bias detection is a well-studied task at this point and the authors sidelined a lot of works and models previously proposed for bias detection (refer to missing citations).",
            "3": "As a result, the authors compared their model's performance only with BERT, which is not a state-of-the-art model when it comes to political bias detection in news articles.",
            "4": "Because BERT is not a good encoder when it comes to encoding long contexts such as news articles.",
            "5": "Different approaches have been proposed to perform political bias detection in news articles that are directly applicable to the setting of this paper.",
            "6": "Without comparison with those models, it is difficult to understand the effectiveness of the proposed approach in this paper.",
            "7": "[R2] The proposed model even underperforms BERT in the low-training set and balanced test data setting (refer to Table 2).",
            "8": "However, I did not find a good justification for this low performance in the paper.",
            "9": "Does it mean the proposed model requires more training data and has some bias toward a particular class label?",
            "10": "Per-class classification scores would be really helpful to evaluate that."
        },
        "jUMyWPDTIz": {
            "0": "(1) Their approach avoids the problem of recognizing writing styles of sources instead of principled analysis of bias, which seems an interesting idea\n\n(2) The machine learning models and statistical significance analysis are sophisticated\n\n(3) there is an effort to identify distinct article structures using k-means clustering \n(1) among the multiple papers I have refereed on stance prediction in this cycle, this is the one that I somehow found most difficult to understand.",
            "1": "See my comments on writing below.",
            "2": "(2) The analysis of structure type of Section 8 was not really convincing to me.",
            "3": "The k-means analysis showed that 92% of the articles reflected the inverted pyramid.",
            "4": "with the other two clusters so small as to seem insignificant.",
            "5": "(3) There are some weird results where the baseline model out-performed their model for smaller test sizes, although I applaud the authors for the honesty of including the non-flattering data in the paper."
        },
        "3gfmTmGXXo": {
            "0": "This paper is well written and easy to follow (except for section 4; see my questions below).",
            "1": "The author also presents thorough related work.",
            "2": "The author conducts extensive experiments, and employ several statistical tools to validate their results and findings.",
            "3": "Most of their experimental results seem to be statistically sound (see my concern of section 7.3 conclusion in \"Reasons To Reject\").",
            "4": "The experimental results are insightful.",
            "5": "For example, interesting structural analysis are presented in section 8, which uncovers structural properties which follows the general practice and theoretical background in journalism.",
            "6": "The unsupervised learning approach used here is also interesting, and opens up new possible directions for detecting sentence-level bias.",
            "7": "Comparison is not comprehensive.",
            "8": "One of your cited work, POLITICS (liu et al, 2022), developed a new LM trained on news articles with a special training objective.",
            "9": "To draw a fairer comparison, models more powerful than BERT need to be included.",
            "10": "Section 7.3: I don't think rejecting the null hypothesis could validate the claim \"This shows that our model is more invariant to the data it was trained on than the BERT classifier\".",
            "11": "Instead, rejecting the null hypotheses in almost all but 1 case shows that *your model is more invariant to the random seeds*.",
            "12": "In order to show that your model is invariant to the training data, your null hypothesis should be something like \"mu_1000 = mu_5000 = mu_10000 = mu_full\", i.e., multiple mean difference comparison.",
            "13": "To this end, you may consult ANOVA.",
            "14": "Despite the interestingness of section 8, I am concerned about the validity.",
            "15": "In other words, if you run your k-means algorithm again, you may end up with a different cluster?",
            "16": "It's advised to run k-means multiple times and try to determine the cluster id of an article based on its \"aggregated score\".",
            "17": "Some writings in section 4 are not clear.",
            "18": "For example, you should bold u_i in equation 4, and you are advised to explicitly say that \"the propagation is for structual analysis\" in line 332-335, otherwise people might get confused why you want to propagate the label to sentence-level."
        }
    },
    "j9E9xLlTmB": {
        "llm_review": {
            "0": "**Novel Evaluation Paradigm**: The paper introduces a new evaluation paradigm focusing on the cognitive plausibility of subword tokenization.",
            "1": "This is a significant departure from traditional evaluation methods that primarily focus on downstream task performance or engineering criteria such as compression rate.",
            "2": "By analyzing the correlation of tokenizer output with human performance on a lexical decision task, the authors provide a fresh perspective on the effectiveness of tokenization algorithms.",
            "3": "**Comprehensive Analysis**: The study compares three widely-used tokenization algorithms (BPE, WordPiece, and UnigramLM) across several languages and vocabulary sizes.",
            "4": "This comprehensive analysis helps in understanding the strengths and weaknesses of each algorithm in different linguistic contexts.",
            "5": "**Introduction of Chunkability Metric**: The introduction of the chunkability metric is a notable contribution.",
            "6": "This metric, which measures how well a tokenizer handles a sequence by considering the ratio of tokens to characters, provides a quantifiable way to evaluate the cognitive plausibility of tokenization.",
            "7": "**Cross-lingual and Multilingual Insights**: The paper provides valuable insights into the performance of monolingual versus multilingual tokenizers.",
            "8": "The findings that monolingual tokenizers align better with human responses than cross-lingual ones are particularly relevant for the development of more effective multilingual models.",
            "9": "**Morphological Coverage Analysis**: The analysis of morphological coverage and its relation to vocabulary size is another strength.",
            "10": "The findings suggest that current popular vocabulary sizes may be insufficient for morphologically-rich languages, which is an important consideration for future tokenizer development.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Limited Language Scope**: The cognitive analyses are limited to two Romance and two Germanic languages.",
            "13": "While the findings are insightful, the study would benefit from including a more diverse set of languages, particularly those with different morphological complexities, to generalize the results.",
            "14": "**Variability in Cognitive Data Collection**: The response times were collected from separate experiments with slight variations in data collection procedures.",
            "15": "This variability might affect the comparability of the results across languages.",
            "16": "A more standardized data collection approach would strengthen the validity of the findings.",
            "17": "**Focus on Relative Differences**: The paper focuses on relative differences between conditions rather than absolute values.",
            "18": "While this approach helps in identifying trends, it might overlook the significance of absolute performance metrics, which could provide a more comprehensive understanding of the tokenizers' effectiveness.",
            "19": "**Limited Explanation of Cognitive Correlation**: The paper finds that the UnigramLM algorithm produces less cognitively plausible tokenization behavior, but it does not delve deeply into the reasons behind this finding.",
            "20": "A more detailed analysis of why certain algorithms perform better in terms of cognitive plausibility would enhance the paper's contributions.",
            "21": "**Potential Overemphasis on Chunkability**: While the chunkability metric is a useful tool, the paper might overemphasize its importance.",
            "22": "Other factors, such as the semantic coherence of tokens and their alignment with human cognitive processes, could also play a significant role in evaluating tokenization algorithms.",
            "23": "**Ethical Considerations**: The paper mentions that claims about cognitive plausibility need to be made with caution due to the open nature of research on human language processing.",
            "24": "However, it does not provide a detailed discussion on the ethical implications of using cognitive data for evaluating NLP models.",
            "25": "A more thorough exploration of these ethical considerations would be beneficial.",
            "26": "**Conclusion:**\n\nOverall, the paper makes significant contributions to the field of NLP by introducing a novel evaluation paradigm for subword tokenization based on cognitive plausibility.",
            "27": "The comprehensive analysis, introduction of the chunkability metric, and insights into cross-lingual and multilingual tokenizers are notable strengths.",
            "28": "However, the study's limited language scope, variability in cognitive data collection, and potential overemphasis on chunkability are areas that could be improved.",
            "29": "Addressing these weaknesses would further enhance the paper's impact and applicability in the development of more effective and cognitively plausible tokenization algorithms."
        },
        "P7rnDFOx9z": {
            "0": "I see this as an impactful short paper: it proposes a fundamental rethinking of how we analyze and evaluate subword tokenizers.",
            "1": "I think the push to evaluate tokenizers in terms of cognitive plausibility is the most important contribution of the paper, above and beyond the specific method proposed, though I also see that method as being promising.",
            "2": "I'd like to see a more thorough analysis of the results along the following dimensions: \n* In correlating with human metrics, the authors don't control (to my knowledge) for potential confounds such as frequency, which is known to affect lexical decision time.",
            "3": "A regression analysis including such metrics would be more compelling; I would rate this paper significantly higher were one given and I think this is the most important point for revisions if the paper is accepted.",
            "4": "* In Figures 1 and 2, I'd like to see the y-axes rescaled either to -1 to 1 or -0.5 to 0.5 for a more consistent reading of the results; more generally, I'd like to see the authors acknowledge more explicitly that none of the correlations reported are particularly high (which is really fine to acknowledge in this setting, since their contribution is the evaluation metric).",
            "5": "I'd also like to see significance testing where the authors report differences in correlation between models.",
            "6": "* The authors mention the shortcomings of Pearson's correlation in their limitations section, but I'd like to see another statistic presented because of this -- something like Goodman-Kruskall might be promising to report.",
            "7": "I think all of these can be easily addressed in the additional page, and aside from the first point, they do not detract from my overall excitement about the proposal.",
            "8": "I'd also like to see some discussion of the segmentations in Table 1: the proposed tokens for e.g.",
            "9": "outfoxed to not correspond to the constituent morphemes of this word, and I'd like to see some discussion of this mismatch.",
            "10": "If the subword tokenizer is often finding tokens that don't directly correspond to morphemes, then I think the cognitive plausibility claims are probably weaker.",
            "11": "Again, though, this does not detract from the main aim of the paper, which is to provide a novel evaluation procedure."
        },
        "6zOb7bJeI9": {
            "0": "The research rationale is well defined and rises an important question: how cognitive plausible are tokenizer algorithm results?",
            "1": "Results suggest a new evaluation paradigm that deserves an open discussion with the conference community  No reason to reject it"
        },
        "baX5Apdf7U": {
            "0": "This paper is clear and well-written.",
            "1": "The cognitive perspective introduced in the task is interesting and provides new lights on the subword tokenisation mechanism.",
            "2": "This technique is of course efficient, but nothing indicates that it is used by the human parser.",
            "3": "These results could argue in favor of the idea that input segmentation is based on low-level techniques, delaying (or even avoiding in some cases) a lexical access to the mental lexicon.",
            "4": "The methodology is not original and the confirm known results."
        }
    },
    "s7Vh8OIIm6": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel hybrid inverted index (HI2) that combines embedding clusters and salient terms to accelerate dense retrieval.",
            "1": "This approach is innovative and addresses the limitations of traditional inverted file structures.",
            "2": "**Comprehensive Evaluation**: The authors conduct extensive experiments on popular retrieval benchmarks (MS MARCO and Natural Questions), demonstrating the effectiveness and efficiency of HI2.",
            "3": "The results show that HI2 consistently outperforms strong ANN baselines and achieves retrieval quality close to brute force search.",
            "4": "**Flexibility and Practicality**: HI2 can be implemented using both unsupervised algorithms (HI2_unsup) and neural networks with end-to-end learning (HI2_sup).",
            "5": "This flexibility makes the approach practical for various use cases and resource constraints.",
            "6": "**Detailed Analysis**: The paper provides a thorough analysis of the effectiveness-efficiency trade-off, the complementary nature of embedding clusters and salient terms, and the robustness of HI2 across different embedding models.",
            "7": "This detailed analysis adds depth to the evaluation and supports the claims made by the authors.",
            "8": "**Open Source Code**: The authors have made their code and model checkpoints publicly available, which promotes reproducibility and further research in this area.",
            "9": "**Weaknesses:**\n\n1.",
            "10": "**Complexity and Hyperparameters**: The hybrid inverted index introduces more hyperparameters compared to conventional IVF, which may require additional effort to tune for optimal performance.",
            "11": "This complexity could be a barrier for practitioners who are not familiar with the intricacies of the method.",
            "12": "**Independent Searching Mechanism**: The current implementation of HI2 searches clusters and terms independently.",
            "13": "A more integrated mechanism that dynamically controls the searching behavior based on the query could potentially improve performance further.",
            "14": "This limitation is acknowledged by the authors but remains an area for future work.",
            "15": "**Scalability Concerns**: While the paper demonstrates the efficiency of HI2, the scalability of the approach, especially in extremely large-scale retrieval scenarios, is not thoroughly explored.",
            "16": "The impact of the increased index size due to the hybrid nature of HI2 on storage and memory requirements could be discussed in more detail.",
            "17": "**Limited Comparison with Sparse Retrievers**: Although the paper includes some comparisons with sparse retrievers, the focus is primarily on dense retrieval methods.",
            "18": "A more detailed comparison with state-of-the-art sparse retrieval techniques could provide a more comprehensive understanding of the relative strengths and weaknesses of HI2.",
            "19": "**Potential for Further Optimization**: The paper mentions that the current PQ codec is still lossy and that there is potential for further optimization.",
            "20": "Exploring and integrating more advanced codecs could enhance the performance of HI2 even further.",
            "21": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of dense retrieval by introducing the hybrid inverted index.",
            "22": "The comprehensive evaluation and detailed analysis provide strong evidence of the effectiveness and efficiency of HI2.",
            "23": "However, the complexity and additional hyperparameters introduced by the method, as well as the potential for further optimization, are areas that could be addressed in future work.",
            "24": "The open-source release of the code and models is a commendable step towards promoting reproducibility and further research in this area."
        },
        "BYtwUIEEVU": {
            "0": "* The research topic is important.",
            "1": "* The paper is well written.",
            "2": "* The proposed techniques are solid.",
            "3": "* Some latest related works from the field of high dimensional similarity search are ignored such as [1].",
            "4": "They have been proved to have better performance than graph based methods such as HNSW compared here.",
            "5": "* The comparison with some inverted list based sparse retrieval methods are missing.",
            "6": "There are some works about compression over inverted lists and perfrom similarity search on them such as [2] [3].",
            "7": "They could also be extended as baseline methods.",
            "8": "Since they are optimized, they should perform better than the selected sparse method here.",
            "9": "* There is no result about space overhead."
        },
        "yH8CEQtwVL": {
            "0": "The idea to combine embedding clusters and salient terms is interesting.",
            "1": "The paper is well organized.",
            "2": "The description of the method design and implementation is clear and easy to follow.",
            "3": "The experimental results prove the effectiveness and efficiency of the proposed method on two popular benchmark datasets.",
            "4": "In the experiments, only the query latency is given.",
            "5": "The setup latency should also be given.",
            "6": "The usage of “lossless retrieval quality” is easy to be confusing."
        },
        "7szchI4gAO": {
            "0": "- The hybrid inverted index that it is fast and precise.",
            "1": "- It provides a way to combine lexical and semantic scores smoothly\n- It introduces a way to distill the data to become smaller, and faster, while maintaining precise Methodology weaknesses: It is not clear if they are seeing the same information as the hybrid index.",
            "2": "If they are seeing the same, it is fair and it is a matter of speed the comparison, if they are seeing differences it is a matter of the score but speed is not necessarily comparable, at least as how the manuscript presents results as compared with IVFPQ and HNSW."
        }
    },
    "GOBxWdRpfz": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel retrieval-augmented visual language model (Re-ViLM) that enhances the Flamingo model by incorporating external knowledge from a database.",
            "1": "This approach addresses the limitations of storing all knowledge within model parameters and improves efficiency in incorporating new data.",
            "2": "**Parameter Efficiency**: By storing certain knowledge explicitly in an external database, Re-ViLM reduces the number of model parameters required.",
            "3": "This is a significant advantage over traditional models that require enormous parameters to model abundant visual concepts and rich textual descriptions.",
            "4": "**Performance Improvement**: The experimental results demonstrate that Re-ViLM significantly boosts performance for image-to-text generation tasks, especially in zero-shot and few-shot settings.",
            "5": "The model outperforms the baseline Flamingo model across various benchmarks, including MSCOCO, Flickr30k, and NoCaps.",
            "6": "**Effective Filtering Strategy**: The paper proposes a simple yet effective filtering strategy to avoid the \"copy-and-paste\" behavior during training.",
            "7": "This strategy ensures that the model does not simply replicate retrieved captions, leading to better generalization and performance.",
            "8": "**Comprehensive Evaluation**: The authors conduct extensive experiments under zero-shot, few-shot, and fine-tuning settings.",
            "9": "The results are compared with several state-of-the-art models, providing a thorough evaluation of Re-ViLM's capabilities.",
            "10": "**Interleaved Dataset Construction**: The construction of an interleaved image-text dataset for pretraining is a notable contribution.",
            "11": "This dataset facilitates in-context few-shot learning, which is crucial for improving the model's performance in few-shot settings.",
            "12": "**Scalability**: The paper demonstrates that Re-ViLM can be scaled with different sizes of CLIP-ViT and RETRO models, showing consistent improvements with increasing model size and retrieval database.",
            "13": "**Weaknesses:**\n\n1.",
            "14": "**Complexity of Implementation**: The integration of a retrieval-augmented LM and the construction of an interleaved dataset add complexity to the implementation.",
            "15": "This might pose challenges for researchers and practitioners who wish to replicate or extend the work.",
            "16": "**Dependence on Retrieval Database**: The performance of Re-ViLM heavily relies on the quality and size of the retrieval database.",
            "17": "The need for a large and high-quality database might limit the applicability of the model in scenarios where such resources are not available.",
            "18": "**Limited Exploration of Other Frameworks**: The paper focuses on the Flamingo framework and does not explore the application of the retrieval-augmented design to other image-to-text frameworks.",
            "19": "This limits the generalizability of the proposed approach.",
            "20": "**Scalability Concerns**: While the paper demonstrates improvements with increasing model size, it does not address the potential challenges and limitations of scaling Re-ViLM to very large models (e.g., 80B parameters).",
            "21": "The scalability of the retrieval-augmented approach at such scales remains an open question.",
            "22": "**Evaluation on Limited Benchmarks**: The evaluation is conducted on a few well-known benchmarks (MSCOCO, Flickr30k, NoCaps).",
            "23": "While these are standard datasets, additional evaluation on more diverse and challenging datasets could provide a more comprehensive assessment of the model's capabilities.",
            "24": "**Potential for Overfitting**: The reliance on retrieved captions during training might lead to overfitting, especially if the retrieval database contains many near-duplicate captions.",
            "25": "Although the filtering strategy mitigates this issue, further investigation into its long-term effects on model performance is warranted.",
            "26": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of image-to-text generation by introducing a retrieval-augmented visual language model.",
            "27": "The proposed Re-ViLM model demonstrates substantial improvements in zero-shot and few-shot settings, addressing key limitations of existing models.",
            "28": "While there are some weaknesses and areas for further exploration, the strengths of the approach make it a valuable contribution to the field.",
            "29": "Future work could focus on extending the retrieval-augmented design to other frameworks, addressing scalability concerns, and evaluating the model on a broader range of benchmarks."
        },
        "QR2VSH4Kue": {
            "0": "- It is interesting to have such a trial of captioning with retrieval.",
            "1": "- The benefit of incorporating retrieval into captioning is not well demonstrated."
        },
        "A5alIcceo5": {
            "0": "The advantages of this article are as follows:\n1.",
            "1": "This paper uses existing public datasets to build a pre-training and evaluation dataset consisting of interleaved image-text pairs for multi-modal pre-training, facilitating contextual learning.",
            "2": "The Re-ViLM model outperforms baselines on various downstream tasks, and results from ablation studies also support the effectiveness of the proposed improvements.",
            "3": "The weakness of this paper is listed as follows:\n1.",
            "4": "The novelty of this paper was limited.",
            "5": "The Re-ViLM model basically followed the structure of the state-of-the-art visual LM and a few minor improvements were proposed.",
            "6": "The most serious problem with this paper is that there is no section discussing limitations."
        },
        "9HAaW7LcSP": {
            "0": "A new method is proposed, by building the model based on pre-trained models and utilizing retrieval-augmented knowledge, the proposed method leads to better results across different settings including zero-shot, few-shot and fully fine-tuning.",
            "1": "Some ablation studies are also conducted.",
            "2": "One important baseline is needed, which is directly evaluating with the retrieved captions (with the proposed filtering).",
            "3": "Meanwhile, more examples of retrieved examples with corresponding generated results are suggested to be shown.",
            "4": "Results with different number of retrieved images are suggested, which can better illustrate the effectiveness of the proposed architecture and retrieval-augmentation respectively.",
            "5": "Inference time and memory comparisons are not shown, they are suggested because extra time and memory are needed to perform the retrieval."
        }
    },
    "ZgJSDBU3px": {
        "llm_review": {
            "0": "**Domain-Specific Innovation**: The paper introduces CaseEncoder, a pre-trained model specifically designed for the legal domain.",
            "1": "This is a significant advancement as it addresses the limitations of general-domain PLMs in capturing the nuances of legal texts.",
            "2": "**Fine-Grained Legal Knowledge Integration**: The authors propose a novel approach to enhance the quality of training data by utilizing fine-grained law article information.",
            "3": "This method ensures that the model is trained on more relevant and legally significant data, which is a notable improvement over previous methods.",
            "4": "**Legal-Specific Pre-Training Tasks**: The design of legal-specific pre-training tasks, such as masked language modeling (MLM) and fine-grained contrastive learning, aligns well with the judging criteria of legal cases.",
            "5": "This approach helps the model to better understand and represent legal concepts.",
            "6": "**Innovative Loss Function**: The introduction of the Biased Circle Loss is a significant contribution.",
            "7": "This loss function enhances the model's ability to recognize case relevance in fine grains, which is crucial for legal case retrieval.",
            "8": "**Comprehensive Evaluation**: The experimental results on multiple benchmarks (LeCaRD, CAIL2021-LCR, and CAIL2022-LCR) demonstrate that CaseEncoder significantly outperforms existing models.",
            "9": "The use of multiple datasets strengthens the validity of the results.",
            "10": "**Visualization and Practical Implications**: The visualization of case embeddings and the discussion on potential downstream tasks, such as charge prediction and article prediction, provide practical insights into the applicability of CaseEncoder in real-world legal scenarios.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Assumption on Legal Systems**: The paper assumes that \"cases committing the same crime have articles in common,\" which may not hold true across all legal systems.",
            "13": "This assumption limits the generalizability of the proposed method to different legal systems and jurisdictions.",
            "14": "**Transitive Rule Limitation**: The transitive rule used in the fine-grained contrastive learning task may not be consistent under all circumstances.",
            "15": "This could potentially affect the accuracy of the model in certain cases, as acknowledged by the authors.",
            "16": "**Complexity and Scalability**: The fine-grained sampling method and the Biased Circle Loss introduce additional complexity to the model training process.",
            "17": "The scalability of these methods to larger datasets and more diverse legal systems is not thoroughly explored.",
            "18": "**Limited Scope of Evaluation**: While the paper evaluates the model on three datasets, all are based on the Chinese legal system.",
            "19": "The effectiveness of CaseEncoder on datasets from other legal systems and languages remains to be validated.",
            "20": "**Potential Overfitting**: The use of highly specific legal knowledge and tasks might lead to overfitting to the particularities of the training data.",
            "21": "This could limit the model's performance on unseen cases that do not fit the training patterns.",
            "22": "**Lack of Comparative Analysis with LLMs**: Although the paper includes OpenAI's API as a baseline, a more detailed comparative analysis with other large language models (LLMs) in the legal domain would provide a clearer picture of CaseEncoder's relative strengths and weaknesses.",
            "23": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of legal case retrieval by introducing CaseEncoder, a model that leverages fine-grained legal knowledge and legal-specific pre-training tasks.",
            "24": "The innovative approaches and comprehensive evaluation demonstrate the model's effectiveness.",
            "25": "However, the assumptions and limitations regarding different legal systems, the complexity of the methods, and the need for broader evaluation highlight areas for future research and improvement."
        },
        "aw86R2hBSv": {
            "0": "- The proposed model consistently outperforms the RoBERTa baseline as well as other reported methods.",
            "1": "- The ablations are helpful in that it shows how much impact each ingredient has on the final performance (e.g.",
            "2": "data sampling, training objective, etc.).",
            "3": "- This paper may not be suitable for the EMNLP venue for two reasons: 1) The proposed approach relies heavily on domain-specific techniques and the method is not general enough to provide much value for the wider NLP research community.",
            "4": "2) The reviewers may not have the necessary legal acumen to properly judge the claims of this paper.",
            "5": "For instance, the authors justifies the generality of its main methodology by claiming that \"the idea of annotating law articles as fine-grained legal knowledge can be applied ... even to the common law systems.",
            "6": "Therefore, the effectiveness of the proposed methodology in this paper is not limited to a specific legal system.",
            "7": "\", something EMNLP reviewers may not be able to verify.",
            "8": "This paper is probably better suited for a legal NLP conference or workshop.",
            "9": "- The paper focuses on a \"retrieval\" task but only considers embedding models among its baselines.",
            "10": "It would be great to add comparison to standard IR models.",
            "11": "For instance, the BM25 baseline, SoTA dense retrievers (e.g.",
            "12": "DRAGON https://huggingface.co/facebook/dragon-plus-query-encoder), SoTA sparse retrievers (e.g.",
            "13": "SPLADE++ https://github.com/naver/splade), or multi-vector retrievers (e.g.",
            "14": "ColBERT-v2 https://github.com/stanford-futuredata/ColBERT).",
            "15": "Complex domain-specific approaches are better justified if they're shown to significantly outperform off-the-shelf retrieval models.",
            "16": "- Some important details are missing.",
            "17": "Please correct me in the rebuttal if I missed anything, but I failed to find any mention of the \"pre-training data\" used in this paper.",
            "18": "It only mentions the evaluation datasets, but nothing is discussed on the data used to pre-train the model.",
            "19": "What's the corpus for MLM?",
            "20": "What's the case corpus used in the contrastive training?"
        },
        "UiYtTo6aKY": {
            "0": "- PLMs may not be a good fit for legal documents as it fails to capture their specificities, because their aim is to have a broad coverage.",
            "1": "This is the starting point of the paper and the identified problem it aims to solve.",
            "2": "This is very relevant to the current domain-specific NLP literature and concerns.",
            "3": "The authors seek to re-balance this by introducing a layer of legal knowledge, and aiming for a more domain-targeted pre-training.",
            "4": "Literature has indeed shown that models that are closer to a domain, eg legal, despite their smaller size, may still perform better on a range of tasks.",
            "5": "- Very precise task definition in section 3\n- Section 4.1: the fine-grain case sampling aims at addressing the usual bottleneck of explicit supervision, using \"branch level similarity\".",
            "6": "This is very relevant to the legal domain in particular that suffers from lack of resources.",
            "7": "- The idea of introducing legal pre-training is interesting, while most legal NLP work have been focusing on fine-tuning PLMs.",
            "8": "- Evaluation is made on several benchmarks of legal NLP and CaseEncoder always performs better.",
            "9": "Overall, this is a very interesting paper, well written and clear, that explores a new way of introducing knowledge and of fitting the specific needs and challenges of legal NLP.",
            "10": "- Section 4.1: because the sampling is based on law article, with the aim of retrieving similar cases, the authors make the assumption that \"cases committing the same crime have articles in common\" line 277.",
            "11": "One could argue that this may not always be the case, depending on the jurisdiction, the type of law if civil or common, and the area of law (e.g.",
            "12": "international law).",
            "13": "Another drawback is that it would not allow to differentiate among cases that include the same \"crime\".",
            "14": "This rather limited and bounded definition of case similarity is a limitation."
        },
        "EzSmnWSMPQ": {
            "0": "The idea of splitting legal articles into branches is interesting, and I think it is potentially valuable for future studies on this task.",
            "1": "The proposed branch-level similarity is a simple yet effective method to measure the relevance between cases and articles, which shows helpful to the contrastive learning process.",
            "2": "Extensive experiments demonstrate the effectiveness of the proposed method.",
            "3": "Although this paper mentioned two important kinds of concepts, i.e., key circumstances and key elements, they are indirectly modeled via two pretraining tasks.",
            "4": "In addition, this paper does not provide any case study to show that the model indeed comprehends these two kinds of concepts better.",
            "5": "Some parts of the model section are a little confusing."
        }
    },
    "adIeh9ZsfC": {
        "llm_review": {
            "0": "**Comprehensive Analysis**: The paper provides a thorough empirical study of frame selection methods for text-to-video retrieval (TVR).",
            "1": "It categorizes existing methods into text-free and text-guided approaches and introduces two new methods, redundancy-aware and low-quality-aware frame selection.",
            "2": "This comprehensive analysis helps in understanding the strengths and weaknesses of different frame selection strategies.",
            "3": "**Novel Contributions**: The introduction of redundancy-aware and low-quality-aware frame selection methods is a significant contribution.",
            "4": "These methods address specific issues such as temporal redundancy and low-quality frames, which are often overlooked in existing approaches.",
            "5": "**Detailed Experiments**: The paper conducts extensive experiments on multiple TVR benchmarks, including MSR-VTT, DiDeMo, and ActivityNet Captions.",
            "6": "The results are presented in a detailed manner, showing the impact of different frame selection methods on both retrieval performance and efficiency.",
            "7": "**Performance and Efficiency Trade-off**: The study effectively balances the trade-off between retrieval performance and efficiency.",
            "8": "By combining different frame selection methods, the paper demonstrates that it is possible to achieve competitive performance while reducing computational costs.",
            "9": "**Clear Presentation**: The paper is well-structured and clearly presents the methodology, experiments, and results.",
            "10": "The use of tables and figures to illustrate the performance of different frame selection methods is effective and aids in understanding the findings.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Manual Frame Number Selection**: One limitation mentioned in the paper is the manual selection of the number of key frames.",
            "13": "This approach may not be optimal for all videos, as the number of key frames can vary depending on the content.",
            "14": "An automatic method to determine the appropriate number of key frames would be more desirable.",
            "15": "**Limited Generalization**: While the paper demonstrates the effectiveness of the proposed methods on specific datasets, it would be beneficial to see how these methods perform on a wider range of datasets and in different application scenarios.",
            "16": "This would help in understanding the generalizability of the proposed frame selection methods.",
            "17": "**Complexity of Implementation**: The introduction of new frame selection methods, especially those involving additional parameters like the scorer network in low-quality-aware frame selection, may increase the complexity of implementation.",
            "18": "This could be a barrier for practitioners looking to adopt these methods in real-world applications.",
            "19": "**Comparison with Traditional Methods**: Although the paper compares the proposed methods with existing frame selection techniques, it would be useful to include a more detailed comparison with traditional key frame extraction methods.",
            "20": "This would provide a clearer picture of the advantages and limitations of the proposed methods in relation to well-established techniques.",
            "21": "**Scalability Concerns**: The paper does not address the scalability of the proposed methods in scenarios with very large video datasets.",
            "22": "It would be interesting to see how these methods perform in terms of both efficiency and effectiveness when applied to large-scale video retrieval tasks.",
            "23": "#### Conclusion\n\nOverall, the paper makes significant contributions to the field of text-to-video retrieval by providing a comprehensive empirical study of frame selection methods.",
            "24": "The introduction of redundancy-aware and low-quality-aware frame selection methods addresses important issues and demonstrates promising results.",
            "25": "However, there are areas for improvement, such as the need for automatic frame number selection, better generalization, and scalability considerations.",
            "26": "Despite these weaknesses, the paper provides valuable insights and lays a strong foundation for future research in this area."
        },
        "00cuwBppbl": {
            "0": "The Redundancy-Aware frame selection utilizes the k-medoids++ clustering algorithm to partition the representations of each frame into K groups.",
            "1": "Choose the representation of the center frame of each group on behalf of the whole group.",
            "2": "This eliminates redundant frames.",
            "3": "The Low Quality-Aware frame selection appends a score network after the vision encoder to assess the quality score of each frame.",
            "4": "Leverage the paired text as weak supervision to optimize the score network.",
            "5": "This eliminates the low-quality frames.",
            "6": "The six frame selection methods are verified in more detailed experiments, and the two proposed frame selection methods improve the efficiency of the TVR task without sacrificing the retrieval performance.",
            "7": "1.The evaluation indicators are not very reasonable.",
            "8": "R@sum is rarely seen in articles in TVR.",
            "9": "2.The experiment part uses too much space on the influence of different frame selections on the retrieval performance, which has little improvement.",
            "10": "However, it spends little space on the effect of different frame selections on the retrieval efficiency, whose improvement is very obvious.",
            "11": "3.In experiments, when combining different frame selections, the paper does not show how the hyper parameter K affects the results, which is similar to the analyses in Table 4.",
            "12": "Drawing the conclusion that proper frame selections improve the retrieval efficiency without sacrificing the retrieval performance of TVR,  should be more rigorous."
        },
        "thyr5WdQ8G": {
            "0": "- The authors proposed two simple yet effective text-free methods to improve the frame selection of text-to-video retrieval\n- Experimental results show that the combination of two proposed text-free methods can outperform other text-free baselines.",
            "1": "In the scenario of text-guided methods, the proposed redundant-aware method can also outperform the baselines.",
            "2": "I have some concerns about the following points:\n- in the training of the LQ-A module, is the text paired with a single frame or the whole video clip?",
            "3": "How to guarantee that the similarity can reflect the quality of a frame if the text is paired with a video clip?",
            "4": "A partially matched frame could also play an important role in a video clip.",
            "5": "How about considering the explicit lens movement, transition, and blurred frames, which are easy to detect, making the system interpretable?",
            "6": "-  How the LQ-A and Redun-A are combined?",
            "7": "The calculation of the score of Redun-A is not clear.",
            "8": "- The overall performance of text-free methods is better than the text-guided methods which involve additional query information and higher computational cost.",
            "9": "If it's properly implemented, could you please provide more analysis about it?"
        },
        "N9WedrHFP1": {
            "0": "Thorough review of the state of the art of post-deep learning key frame extraction methods.",
            "1": "Clear improvement over the state of the art.",
            "2": "Intuititively satisfying results.",
            "3": "Well written paper.",
            "4": "1, This is not a weakness peculiar to these authors.",
            "5": "In general the video summarization literature seems to show ignorance of video summarization techniques developed in the multimedia community more than 20 years ago.",
            "6": "Those were techniques that did not rely on the elaborate machine learning techniques on display at this moment but worked quite well.",
            "7": "Here are three examples of such papers:\n\nhttps://www.semanticscholar.org/paper/Motion-activity-based-extraction-of-key-frames-from-Divakaran-Peker/515f899f61c33782ec78045fa849e27e607d9985\n\nhttps://www.ee.columbia.edu/~sfchang/papers/talk-iciap-0903.pdf\n\nhttps://ieeexplore.ieee.org/document/809161\n\nAn inspection of the bibliographies of these papers will reveal that there have been a number of keyframe extraction techniques developed in the past.",
            "8": "Would be nice of the authors to address why their technique establishes novelty over those methods.",
            "9": "Note that I don't want to single these authors out since the entire vision community seems to be reinventing past techniques while also genuinely exploiting the power of deep learning techniques that did not exist in the times of the three papers cited above."
        }
    },
    "vjTnfxbkaL": {
        "llm_review": {
            "0": "**Novel Framework**: The paper introduces a novel Hierarchical Enhancement Framework (HEF) for Aspect-Based Argument Mining (ABAM).",
            "1": "The framework is well-structured and consists of four modules: basic module, argument unit enhancement module, aspect term enhancement module, and decision module.",
            "2": "This modular approach allows for a clear understanding of each component's role and contribution to the overall performance.",
            "3": "**Innovative Components**: The introduction of three novel components—Semantic and Syntactic Fusion (SSF), Batch-level Heterogeneous Graph Attention Network (BHGAT), and Span Mask Interactive Attention (SMIA)—is a significant contribution.",
            "4": "Each component addresses specific challenges in ABAM, such as optimizing underlying representations, detecting argument unit stances, and constraining aspect term recognition boundaries.",
            "5": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of the proposed framework on multiple datasets, including ABAM, AURC-8, SemEval-2016 Task 6A, and ABAM argument segment.",
            "6": "The use of various evaluation metrics, such as segment-level and token-level evaluations, ensures a comprehensive assessment of the framework's performance.",
            "7": "**Ablation Studies**: The inclusion of ablation studies helps in understanding the impact of each component within the HEF framework.",
            "8": "This analysis demonstrates the importance of each module and component, providing insights into their individual contributions to the overall performance.",
            "9": "**State-of-the-Art Performance**: The proposed HEF framework outperforms existing methods in both segment-level and token-level evaluations.",
            "10": "The significant improvements in metrics such as MicF1, MacF1, Token-Flat, and Token-Nested highlight the effectiveness of the framework and its components.",
            "11": "**Detailed Methodology**: The paper provides a detailed explanation of the methodology, including the task definition, encoder and decoder structures, and the specific operations of each component.",
            "12": "This clarity allows for reproducibility and a deeper understanding of the proposed approach.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Complexity**: The proposed framework, while innovative, is quite complex.",
            "15": "The integration of multiple components and modules may make it challenging for practitioners to implement and understand the entire system.",
            "16": "Simplifying certain aspects or providing more intuitive explanations could help in making the framework more accessible.",
            "17": "**Generalization**: The paper mentions that the BHGAT component is currently only suitable for classification tasks with topic information.",
            "18": "This limitation restricts the generalizability of the framework to other tasks.",
            "19": "Future work should focus on extending the applicability of BHGAT to more general tasks.",
            "20": "**Correlation Mining**: The paper acknowledges that the current framework does not extensively explore the correlation between argument units and aspect terms.",
            "21": "This is a crucial aspect of ABAM, and future work should aim to delve deeper into this correlation to fully utilize the guiding information between these entities.",
            "22": "**Dataset Diversity**: While the paper evaluates the framework on multiple datasets, the diversity of these datasets could be further improved.",
            "23": "Including more varied datasets from different domains could provide a more comprehensive evaluation of the framework's robustness and generalizability.",
            "24": "**Performance on AURC-8**: The performance improvement on the AURC-8 dataset is not as significant as on the ABAM dataset.",
            "25": "This discrepancy suggests that the framework may need further tuning or additional components to handle datasets with different characteristics effectively.",
            "26": "**Limited Discussion on Limitations**: The paper briefly mentions the limitations of the proposed framework but does not provide an in-depth discussion.",
            "27": "A more detailed analysis of the limitations and potential areas for improvement would provide a balanced view and guide future research directions.",
            "28": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of Aspect-Based Argument Mining with its Hierarchical Enhancement Framework.",
            "29": "The novel components and comprehensive evaluation demonstrate the framework's effectiveness and potential.",
            "30": "However, the complexity, limited generalization, and need for further exploration of correlations between argument units and aspect terms are areas that require attention.",
            "31": "Addressing these weaknesses in future work will enhance the framework's applicability and impact in the field."
        },
        "qt6d63GnLW": {
            "0": "1、The paper propused novel module the BHGAT component.",
            "1": "1、Semantic and Syntactic Fusion component proposed in the paper are not innovative.",
            "2": "The author's explanation of why this component is used lacks clarity.",
            "3": "2、The paper does not consider many state-of-the-art ABAM methods for comparison, it has been noted that the compared baseline models do not adequately address the ABAM task but rather other tasks.",
            "4": "Therefore, the comparison between the proposed method and the baseline model fails to convincingly demonstrate its advancement.",
            "5": "3、Equations and model structure diagrams, being essential components of your research, should align and corroborate with each other.",
            "6": "However, in this paper, there are evident discrepancies, such as eq9, eq10, eq11.",
            "7": "4、In the second part of the narrative, some of the elaborations are not clear enough, for example， in the Basic Module, the meaning of the symbols W and U and Q is not given.",
            "8": "5、Lack of clarity in expression: The writing of the article could be improved, it lacks clarity and precision, and certain paragraphs are too colloquial."
        },
        "1dijR01Az6": {
            "0": "The studied problem is interesting and important.",
            "1": "In general, this paper is well-written and organized.",
            "2": "The experimental results show that the proposed method significantly outperforms state-of-the-art baselines\n 1.The Semantic and Syntactic Fusion (SSF) component aiming to bridge the gap between distant words is not novel.",
            "3": "The topic of long-distance dependencies between words has been studied in previous works [1][2].",
            "4": "It’s not clear how the proposed method advances from them.",
            "5": "2.The overall framework figure is not clear.",
            "6": "This figure does not illustrate The key component modules (Figure 2).",
            "7": "This paper does not clearly explain how the three key challenges were addressed, and each module does not correspond to a challenge.",
            "8": "[1]\tYuanhe Tian, Guimin Chen, Yan Song, and Xiang Wan.",
            "9": "2021.",
            "10": "Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks.",
            "11": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4458–4471.",
            "12": "[2]\tLi, Y., Li, Z., Zhang, M., Wang, R., Li, S., & Si, L. (2019).",
            "13": "Self-attentive Biaffine Dependency Parsing.",
            "14": "In IJCAI (pp.",
            "15": "5067-5073)."
        },
        "lVwAE2ht4U": {
            "0": "The idea of hierarchical enhancement framework for aspect-based argument mining is innovative and intuitive.",
            "1": "This paper clearly and logically describes its contribution to hierarchical enhancement framework for aspect-based argument mining.",
            "2": "From the extensive and comprehensive experimental results of experiments in this paper, the proposed framework demonstrates significant performance improvements over existing approaches..\n3.",
            "3": "The framework studied in this paper is interesting and well motivated and the paper is well written.",
            "4": "Experiments are tested on real datasets.",
            "5": "It is better to choose more baselines from different articles for comparison.",
            "6": "The paper lacks related work.",
            "7": "Experiments on the effectiveness of SMIA component could be added to make the experiments more comprehensive.",
            "8": "Compared with LSTM-CRF, although the performance of SF-CRF is slightly better, will the computational complexity increase dramatically?"
        }
    },
    "oVAod8GRI9": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces NEURO SIM, a neuro-symbolic model for image manipulation using weak supervision.",
            "1": "This approach is innovative as it leverages VQA annotations instead of requiring direct supervision with manipulated images, which is a significant step forward in reducing the cost and effort associated with dataset creation.",
            "2": "**Comprehensive Dataset**: The authors have created a new dataset, CIM-NLI, which includes complex multi-hop instructions for image manipulation.",
            "3": "This dataset is a valuable contribution to the field and can serve as a benchmark for future research.",
            "4": "**Detailed Methodology**: The paper provides a thorough explanation of the NEURO SIM architecture, including the visual representation network, semantic parsing module, concept quantization network, and manipulation network.",
            "5": "The detailed description of the training process and loss functions is commendable.",
            "6": "**Extensive Experiments**: The authors have conducted extensive experiments to evaluate the performance of NEURO SIM.",
            "7": "They compare their model with state-of-the-art baselines and demonstrate its effectiveness in various settings, including zero-shot generalization and multi-hop reasoning.",
            "8": "**Interpretability**: One of the key strengths of NEURO SIM is its interpretability.",
            "9": "The model generates intermediate programs that can be inspected to understand the steps taken for image manipulation.",
            "10": "This is a significant advantage over purely neural models, which often act as black boxes.",
            "11": "**Human Evaluation**: The inclusion of human evaluation to assess the quality of the generated images adds credibility to the results.",
            "12": "The study shows that NEURO SIM performs well in terms of semantic correctness, which is crucial for practical applications.",
            "13": "#### Weaknesses:\n\n1.",
            "14": "**Rendering Quality**: While NEURO SIM performs well in terms of semantic correctness, the quality of the rendered images is not on par with some of the supervised baselines.",
            "15": "The authors acknowledge this and suggest that improving the image rendering module could be a future direction.",
            "16": "**Complexity of DSL**: The domain-specific language (DSL) used for parsing instructions and generating programs is quite complex.",
            "17": "This complexity might limit the model's applicability to other domains without significant modifications.",
            "18": "The authors mention the need for redefining the DSL for new domains, which could be a limitation.",
            "19": "**Dependence on VQA Annotations**: Although the use of VQA annotations reduces the need for direct supervision, it still requires a substantial amount of annotated data.",
            "20": "The effectiveness of NEURO SIM in scenarios with limited VQA data is not thoroughly explored.",
            "21": "**Limited Real-World Applications**: The experiments are primarily conducted on synthetic datasets like CLEVR and Minecraft.",
            "22": "While these datasets are useful for benchmarking, the applicability of NEURO SIM to real-world images and more complex scenes remains to be demonstrated.",
            "23": "**Error Analysis**: The paper provides some examples of errors made by NEURO SIM, but a more detailed error analysis could help in understanding the limitations of the model better.",
            "24": "For instance, categorizing errors based on the type of manipulation or the complexity of instructions could provide deeper insights.",
            "25": "**Comparison with LLMs**: The paper briefly mentions the use of GPT-4 for semantic parsing but does not provide a detailed comparison.",
            "26": "Given the recent advancements in large language models, a more thorough evaluation of how NEURO SIM compares with these models in terms of parsing accuracy and overall performance would be beneficial.",
            "27": "#### Conclusion:\n\nOverall, the paper presents a significant advancement in the field of image manipulation using natural language instructions.",
            "28": "The introduction of NEURO SIM, a weakly-supervised neuro-symbolic approach, and the creation of the CIM-NLI dataset are valuable contributions.",
            "29": "While there are some areas for improvement, particularly in image rendering quality and applicability to real-world scenarios, the strengths of the approach, such as interpretability and reduced supervision requirements, make it a promising direction for future research."
        },
        "DVom2O5TkK": {
            "0": "The strengths of this paper and the main benefits to the NLP community are as follows:\n- Novel Model: The paper introduces NEUROSIM, the first neuro-symbolic, weakly supervised, and interpretable model for text-guided image manipulation that does not require output images for training.",
            "1": "This model combines the strengths of neural and symbolic representations, offering modularity, interpretability, and improved generalizability.",
            "2": "- New Dataset: The authors extend the CLEVR dataset to create a new dataset called Complex Image Manipulation via Natural Language Instructions (CIM-NLI) and CIM-NLI-LARGE for testing zero-shot generalization.",
            "3": "These datasets provide a benchmark for evaluating models on the task of image manipulation using natural language instructions.",
            "4": "- Competitive Performance: Despite being weakly supervised, NEUROSIM is highly competitive with or beats state-of-the-art baselines that make use of supervised data for manipulation.",
            "5": "This demonstrates the effectiveness of the proposed approach in handling complex reasoning tasks.",
            "6": "- Interpretability: The model is interpretable, as it parses instructions into a symbolic program based on a Domain Specific Language (DSL) comprising object attributes and manipulation operations.",
            "7": "This allows for better understanding and debugging of the model's behavior.",
            "8": "Limited Scope: The paper focuses on image manipulation tasks involving multiple objects and complex multi-hop natural language instructions.",
            "9": "While this is an interesting and challenging problem, the scope of the paper might be limited for researchers working on other aspects of NLP or image manipulation\n2.",
            "10": "Generalizability: Although the paper introduces the CIM-NLI-LARGE dataset to test zero-shot generalization, it is unclear how well NEUROSIM would perform on real-world images or more diverse datasets.",
            "11": "Further evaluation on a wider range of datasets would help establish the model's generalizability.",
            "12": "Comparison with Other Models: The paper compares NEUROSIM with several other models in terms of problem setting, task complexity, and approach.",
            "13": "However, it would be beneficial to see more detailed comparisons, including the performance of NEUROSIM on tasks that other models excel at or struggle with."
        },
        "kSoFPqFR96": {
            "0": "Pros:\n\n1.",
            "1": "Neuro-symbolic methods have the advantage of being completely explainable and make very little errors.",
            "2": "Given the rise of image generation methods, having a neuro-symbolic editor on top of it is an interesting setup, and thus the task is well motivated.",
            "3": "The paper also makes dataset contribution which can be useful for future work.",
            "4": "The proposed neuro-symbolic method is competitive with strong baseline like Instruct-Pix2Pix (IP2P) even though it is trained on significantly less data.",
            "5": "Cons:\n\n1.",
            "6": "My main concern is that there is limited application for CLEVER like scenes which are constrained to add/remove/change.",
            "7": "Further, ``change'' attributes are also restricted.",
            "8": "Compare this to dataset used in IP2P where there are no constraints and is free form.",
            "9": "As such, a model trained on in this dataset is likely to have limited real world use case.",
            "10": "The authors introduce multi-hop reasoning, but these are very synthetic (App B.3).",
            "11": "In real world use case, the instructions are very likely to be only zero-hop since the user knows what they want to edit.",
            "12": "That is the dataset is artificially made tougher.",
            "13": "I feel the authors missed a simpler (but potentially stronger) baseline which uses a combination of neuro-symbolic reasoning and IP2P.",
            "14": "For instance, a neuro-symbolic reasoner (language only, no vision component) could create single-step (zero-hop) instructions which could then be executed by IP2P in a sequential manner.",
            "15": "This would also remove the issues of worse FID scores.",
            "16": "(Minor) The authors remark that other datasets namely CSS, CoDraw and i-CLEVER exist.",
            "17": "Some comparison on those dataset might be helpful for future work (I don't expect the conclusions to change though).",
            "18": "How does the model handle lighting conditions?",
            "19": "In the qualitative example (fig 4, top row), it appears Neuro-SIM is failing to replicate the conditions and thus the generated image is inconsistent.",
            "20": "In Table 3., how does Neuro-SIM with 54k examples perform?",
            "21": "Is the performance saturated?"
        },
        "BMhqsNrerM": {
            "0": "- Presents a unique approach to image editing by integrating Neuro-Symbolic approaches, it seems like a pretty novel method to me.",
            "1": "- I found the idea of using RL to do image editing intriguing, especially since its very difficult to get image editing training data for complex instructions\n- Very detailed evaluation that contains lots of experiments, analysis and details about the system, including a human evaluation and studies on many different settings.",
            "2": "- Positive results over the baselines at least for low data regimens\n- Releases a dataset that can be used in future work - There is little intrinsic motivation to manipulate these kind of synthetic images, meanwhile it seems clear there are many hurdles to applying this system to natural images.",
            "3": "Additionally the image editing operations themselves are relatively basic, most of the complexity/composition in the task comes from identifying what object the instructions is referring.",
            "4": "- The system if very complex but still has mixed results compared to the end-to-end baselines, although the authors have pointed out their system only requires VQA data to train.",
            "5": "- I am struggling a bit to understand why the REINFORCE algorithm works.",
            "6": "The reward seems to be very shallow, so I am failling to see how it could train the model to select the correct manipulation operator, or to prevent the model from simply selecting a random object in the scene.",
            "7": "I think more discussion here is needed.",
            "8": "The author rebuttal had a very helpful explanation of this, I would strongly encourage the authors to include it somewhere in the paper.",
            "9": "Despite these concerns overall I think this is an interesting and novel system and should be accepted."
        }
    },
    "HhoG04UD3E": {
        "llm_review": {
            "0": "**Comprehensive Dataset:**\n   - The PMIndiaSum dataset is extensive, covering 14 languages from four language families, and includes 196 language pairs.",
            "1": "This makes it the largest dataset to date for Indian languages, providing a significant resource for researchers working on multilingual and cross-lingual summarization.",
            "2": "**Detailed Construction Workflow:**\n   - The authors provide a thorough explanation of the data acquisition, processing, and quality assurance steps.",
            "3": "This transparency is crucial for reproducibility and for other researchers to understand the dataset's strengths and limitations.",
            "4": "**High-Quality Data:**\n   - The dataset is sourced from the Prime Minister of India's website, ensuring a high level of quality and consistency.",
            "5": "The authors also perform extensive cleaning and filtering to maintain the dataset's integrity.",
            "6": "**Benchmarking:**\n   - The paper includes benchmarks for monolingual, cross-lingual, and multilingual summarization using various methods such as fine-tuning, prompting, and translate-and-summarize.",
            "7": "This provides a solid baseline for future research.",
            "8": "**Public Availability:**\n   - The dataset is publicly available under the CC BY 4.0 license, allowing for free modification and redistribution.",
            "9": "This openness is commendable and will likely encourage widespread use and further development of the dataset.",
            "10": "**Inclusion of Lesser-Studied Languages:**\n   - The dataset includes languages like Manipuri (Meitei), which are often neglected in NLP research.",
            "11": "This inclusion helps to broaden the scope of research and ensures that less commonly studied languages are also represented.",
            "12": "**Quality Considerations:**\n   - The authors conduct empirical studies to validate their choice of using headlines as summaries and measure the degree of parallelism using LaBSE scores.",
            "13": "These steps help to ensure the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3045–3055, Hong Kong, China.",
            "14": "Association for Computational Linguistics.",
            "15": "**Weaknesses:**\n\n1.",
            "16": "**Domain Bias:**\n   - The dataset is sourced from a governmental website, which introduces a domain bias towards political news.",
            "17": "This limits the generalizability of models trained on this dataset to other domains.",
            "18": "**Language Support Limitations:**\n   - While the dataset covers 14 languages, it does not include all major Indian languages.",
            "19": "For instance, languages like Kannada and Urdu are not supported by mBART, which limits the applicability of certain models.",
            "20": "**Quality Assurance:**\n   - Although the authors have taken steps to ensure data quality, there is still a possibility of errors due to extraneous HTML elements that may not have been filtered out.",
            "21": "This could affect the performance of models trained on the dataset.",
            "22": "**Evaluation Metrics:**\n   - The evaluation relies heavily on automatic metrics like ROUGE and BLEU, which may not fully capture the quality of the generated summaries.",
            "23": "Human evaluation is limited and could be expanded to provide a more comprehensive assessment.",
            "24": "**Limited Human Evaluation:**\n   - The human evaluation conducted is limited to a small sample size and only three languages.",
            "25": "Expanding this evaluation to more languages and a larger sample size would provide a more robust validation of the dataset's quality.",
            "26": "**Potential Ethical Concerns:**\n   - The dataset includes news articles that may contain sensitive information about individuals.",
            "27": "While the data is publicly available, the authors should consider the ethical implications of using such data, especially in the context of potential misuse.",
            "28": "**Conclusion:**\n\nThe PMIndiaSum dataset is a significant contribution to the field of multilingual and cross-lingual summarization, particularly for Indian languages.",
            "29": "Its comprehensive coverage and detailed construction workflow make it a valuable resource for researchers.",
            "30": "However, the dataset's domain bias, language support limitations, and potential quality issues should be considered when using it.",
            "31": "Additionally, expanding human evaluation and addressing ethical concerns would further strengthen the dataset's utility and reliability."
        },
        "zJxEIkuM9T": {
            "0": "- This paper is well-written, well-documented, and easy to follow.",
            "1": "- This paper contributes to the summarization by presenting a new dataset that focuses on Indian languages (including low-resource ones).",
            "2": "- This author conducts extensive experiments and provides several benchmark results under different settings.",
            "3": "- The dataset is domain-specific and biased toward political news.",
            "4": "Therefore it might be questionable whether it could be used as a proper dataset if one wants to evaluate the general capability of a PLM.",
            "5": "- Although the author conducts many experiments, some analyses are not detailed enough (I guess it is due to the page limit).",
            "6": "I would suggest the author go into more detail about the analysis in the camera-ready version."
        },
        "r7Fjp1Uzrv": {
            "0": "- Paper is well written and clear and easy to follow.",
            "1": "The methodology is repeatable for researchers wanting to extend this work.",
            "2": "- The research follows standard experimentation and evaluation so it is comparable with work in the field\n- The corpus itself is of sufficient size and scope to have an impact on this language pairs and the research community\n- Supporting monolingual, cross-lingual, and multi-lingual summarization for under-resourced languages is a good contribution by the authors - Some of the language pairs are covered by other corpora but I do not get a feel for how this would compare in the tasks with different data\n- As noted by the authors the use of external translation models makes isolating the impact of this methodology and dataset difficult as that model may be and is likely ingesting additional data for some language pairs"
        },
        "mtKV7YzHV2": {
            "0": "- Large multilingual dataset for under-resourced language summarization\n- Thorough set of descriptive statistics for the dataset Unclear whether this is the appropriate venue for this paper; something like LREC might be a better fit."
        }
    },
    "BdpoEj33DZ": {
        "llm_review": {
            "0": "**Novelty and Contribution**: The paper introduces MAILEX, the first dataset specifically designed for event extraction from conversational email threads.",
            "1": "This is a significant contribution as it addresses a gap in the existing literature and provides a foundation for future research in this domain.",
            "2": "**Comprehensive Taxonomy**: The authors propose a new taxonomy covering 10 event types and 76 arguments, which is well-defined and grounded in the speech act theory.",
            "3": "This taxonomy is crucial for understanding and categorizing events in emails, making the dataset more structured and useful.",
            "4": "**Dataset Quality and Size**: The dataset includes 1.5K email threads and approximately 4K emails, annotated with around 8K event instances.",
            "5": "The substantial agreement between annotators (κ values of 0.791 and 0.810) indicates high-quality annotations.",
            "6": "**Experimental Evaluation**: The paper conducts a thorough evaluation of three different approaches: fine-tuned sequence labeling, fine-tuned generative extraction, and few-shot in-context learning.",
            "7": "This comprehensive comparison provides valuable insights into the challenges and performance of different methods in the context of email event extraction.",
            "8": "**Identification of Challenges**: The authors identify several unique challenges in email event extraction, such as non-continuous, shared trigger spans, non-named entity arguments, and the need to model email conversational history.",
            "9": "These insights are crucial for guiding future research in this area.",
            "10": "**Open Source**: The authors have made the source code and dataset publicly available, which is commendable as it promotes transparency and enables other researchers to build upon their work.",
            "11": "## Weaknesses\n\n1.",
            "12": "**Limited Context**: The dataset is based on the Enron email corpus, which may limit the generalizability of the findings to other domains or types of email communications.",
            "13": "The authors acknowledge this limitation but do not provide a clear plan for addressing it in future work.",
            "14": "**Modeling Email History**: While the authors emphasize the importance of modeling email history, their current approach of simply concatenating previous emails is not very sophisticated.",
            "15": "More advanced techniques, such as selective attention mechanisms or dynamic memory models, could be explored to improve performance.",
            "16": "**Error Analysis**: The error analysis for the in-context learning approach (GPT-3.5) is insightful, but similar detailed analyses for the fine-tuned models (BERT and BART) are lacking.",
            "17": "Providing more examples and a deeper analysis of the errors made by these models could offer additional insights into their limitations and areas for improvement.",
            "18": "**Handling Long-Span Arguments**: The paper highlights the challenge of extracting long-span, non-named entity arguments but does not propose specific solutions or improvements to address this issue.",
            "19": "Future work could focus on developing techniques specifically tailored to handle such arguments more effectively.",
            "20": "**Evaluation Metrics**: The evaluation metrics used (Precision, Recall, and F1 scores) are standard, but the paper could benefit from additional metrics or qualitative evaluations to provide a more comprehensive assessment of model performance, especially in handling complex email structures.",
            "21": "**Few-Shot Learning Performance**: The performance of GPT-3.5 in few-shot learning is significantly worse than the fine-tuned models.",
            "22": "While this highlights the challenge of the task, the paper could explore ways to improve few-shot learning performance, such as better prompt engineering or leveraging domain-specific knowledge.",
            "23": "## Conclusion\n\nOverall, the paper makes a significant contribution to the field of event extraction by introducing a novel dataset and taxonomy for email event extraction.",
            "24": "The comprehensive evaluation and identification of challenges provide valuable insights for future research.",
            "25": "However, there are areas for improvement, particularly in handling long-span arguments, modeling email history more effectively, and enhancing few-shot learning performance.",
            "26": "Despite these weaknesses, the paper lays a strong foundation for further advancements in email event extraction."
        },
        "ZF7OeHgpOK": {
            "0": "This paper makes several contributions towards the information extraction field.",
            "1": "First, they define a taxonomy for event extraction in corporate email threads.",
            "2": "Second, they release the MailEx dataset which would be a valuable resource for evaluating IE models.",
            "3": "Third, they contribute novel event extraction models and highlight the deficiencies of few-shot LLMs.",
            "4": "I did not find any major technical weaknesses in the paper.",
            "5": "However, the authors could improve the language of certain sections such as the end-to-end extraction model discussion to improve comprehension.",
            "6": "I would also recommend that the authors show statistical test results to support their model comparison claims."
        },
        "xoOXULs9wI": {
            "0": "A new dataset for Email domain itself is interesting and could be important for real applications.",
            "1": "The proposed baseline methods do not perform very well on this dataset, which leaves a large space for future works to further improve.",
            "2": "Though the dataset is very interesting, the granularity of proposed event type is not fine enough.",
            "3": "All the event type is about a specific task for business, like \"schedule a meeting\" and \"request data\".",
            "4": "This is more like emails in a company.",
            "5": "However, if we are talking about event in emails, emails should also cover all the event types of other texts.",
            "6": "People can talk about any events in email, but the key difference is: the context of email is more daily, which may make the task harder.",
            "7": "In my opinion, the authors should consider extend the type set to include all the event types that other datasets use.",
            "8": "The result of GPT-3.5 could be problematic, since the event template provided in the prompt is not easy to understand."
        },
        "UugkVM48jZ": {
            "0": "The dataset is novel and has addressed some of the critical characteristics of email data.",
            "1": "The proposed dataset can be a valuable study resource in this unexplored research area.",
            "2": "The authors have adequately described all the sections of the manuscript, including the taxonomy of the dataset, annotation details, and a detailed analysis of the dataset statistics and unique characteristics.",
            "3": "The authors have described three types of baselines for the proposed dataset and compared the results.",
            "4": "The authors have provided the experimental setting of the models that improve the reproducibility of the baseline models.",
            "5": "The authors also performed the qualitative analysis of the results based on the unique challenges of event and argument extraction of the proposed dataset.",
            "6": "No reason to reject the work."
        },
        "rL74iKrFVH": {
            "0": "The paper has several strengths:\n- Overall, it is well-written with a clear and well-motivated introduction.",
            "1": "- The high-quality annotated dataset will facilitate event extraction research in the email domain and prompt the study of email assistance.",
            "2": "There is a lack of clarity regarding the criteria of designing the schema.",
            "3": "For example, why differentiate the event type \"Request Meeting Data\" from \"Request Data\" as the previous event only cover 0.71% of all events?",
            "4": "The average number of words of arguments is 7.41 and some of them are non-continuous spans.",
            "5": "This will cause difficulties and ambiguities in identifying the arguments."
        }
    },
    "Z1wGHeHBrk": {
        "llm_review": {
            "0": "**Novelty and Relevance:** The paper addresses a significant gap in the evaluation of vision-language models (VLMs) by focusing on their ability to generalize \"visible\" physical knowledge, such as object color, size, and spatial relations.",
            "1": "This is a novel approach that extends beyond the typical visual reasoning tasks, providing a more comprehensive understanding of VLMs' capabilities.",
            "2": "**Automated Pipeline:** The authors have developed an automated pipeline to derive a large-scale dataset (VIPHY) from raw images.",
            "3": "This approach bypasses the need for crowdsourced annotations, which can be time-consuming and expensive, and allows for the creation of a more extensive and diverse dataset.",
            "4": "**Comprehensive Evaluation:** The paper provides a thorough evaluation of several state-of-the-art language models (LMs) and vision-language models (VLMs) on the VIPHY dataset.",
            "5": "The inclusion of a caption-pretrained baseline (CapBERT) is particularly insightful, as it highlights the limitations of current VLMs in retaining visual knowledge.",
            "6": "**Detailed Analysis:** The authors offer a detailed analysis of model performance across different dimensions, including the effect of label cardinality on color prediction and the robustness of models to contextual subtypes in size relations.",
            "7": "This level of analysis provides valuable insights into the strengths and weaknesses of the models.",
            "8": "**Human Performance Benchmark:** By comparing model performance to human performance, the paper provides a clear benchmark for evaluating the effectiveness of the models.",
            "9": "This comparison underscores the significant gap between current models and human capabilities in generalizing physical knowledge.",
            "10": "**Weaknesses:**\n\n1.",
            "11": "**Limited Scope of Spatial Relations:** The paper focuses on a limited set of spatial relations (above, below, similar level) and does not account for more complex spatial relationships that might be present in real-world scenarios.",
            "12": "This limitation could restrict the applicability of the findings to more nuanced spatial reasoning tasks.",
            "13": "**Assumptions in Spatial Knowledge Extraction:** The method for extracting spatial knowledge assumes that the camera's image plane is orthogonal to the ground, which may not always be the case.",
            "14": "This assumption could lead to inaccuracies in the derived spatial relations, particularly in images with unusual perspectives.",
            "15": "**Dependency on Pretrained Models:** The pipeline relies heavily on pretrained models (e.g., UniCL for subtype selection, OFA for color inference), which could introduce biases and errors from these models into the VIPHY dataset.",
            "16": "The paper does not provide a detailed evaluation of the accuracy of these pretrained models in the context of the tasks.",
            "17": "**Evaluation on Limited Datasets:** While the paper leverages Visual Genome and ADE20K datasets, it does not explore the generalizability of the findings to other datasets or real-world applications.",
            "18": "A broader evaluation across multiple datasets would strengthen the claims about the models' capabilities.",
            "19": "**Ethical and Bias Considerations:** The paper acknowledges potential biases in the datasets used but does not provide a detailed analysis of how these biases might affect the results.",
            "20": "A more thorough examination of the ethical implications and potential biases in the VIPHY dataset would be beneficial.",
            "21": "**Conclusion:**\n\nOverall, the paper makes a significant contribution to the field of vision-language models by introducing a novel dataset and evaluation framework for probing \"visible\" physical commonsense knowledge.",
            "22": "The automated pipeline and comprehensive evaluation provide valuable insights into the strengths and limitations of current models.",
            "23": "However, the paper could benefit from addressing the limitations in spatial relations, assumptions in knowledge extraction, dependency on pretrained models, and a broader evaluation across multiple datasets.",
            "24": "Additionally, a more detailed analysis of ethical considerations and potential biases would enhance the robustness of the findings."
        },
        "OtEuVrwFTd": {
            "0": "The problem about physical knowledge acquisition and reasoning of VLM is generally interesting.",
            "1": "The work contributed a new dataset in this line of work.",
            "2": "Latest VLMs such as InstructBLIP, LLaVA not included for experiment which makes the claim that existing VLMs struggle to effectively consolidate physical knowledge less appealing."
        },
        "gLjmaSAkQi": {
            "0": "- This work proposes a new task to estimate the attributes such as size, color and space from images, by leveraging VLM and other vision tools.",
            "1": "- The experiments are very extensive and solid.",
            "2": "- This paper is well-written and easy-to-follow.",
            "3": "- The authors provide both source code and data along with the submission, which I appreciate a lot.",
            "4": "I hope it can be made publicly avaible, once published.",
            "5": "- Some implementation details inside the proposed framework are not very informative from the submission.",
            "6": "For example, what object detectors do the authors use for cluster relation?",
            "7": "What depth estimation method does the authors use?",
            "8": "-  As I am not an expert in NLP but in Computer Vision, especially with some low-level vision background, the claim of 'Physical Commonsense Knowledge' does not seem very comfortable for me.",
            "9": "In fact, from the image preception and image formulation prespective, the physicial attributes may actually contain more attributes than color, space and size.",
            "10": "So, it would be highly appreciated if the authors can enhance this part, and provide some references to justify that, these three key factors can also align with the definition of physcial knowledge in vision community.",
            "11": "- Some other minor issues on presentation, please refer to the 'presentation improvement' section."
        },
        "Jt3PfSkiHJ": {
            "0": " - Introduces an innovative pipeline to automatically construct visual physics commonsense dataset that involves use of diverse tools: parser, depth estimation, vision language models, bounding box areas.",
            "1": "- Presents empirical findings that the vision language models show far worse performance in VIPHY than humans and calls for the need to address visible physics common understanding of such models to the NLP community.",
            "2": "- Demonstrates an in-depth evaluation of the performance of current vision language models, taking into account various factors such as types of questions, color categories, and label cardinality.",
            "3": "- No human validation of automated process which might be prone to error due to the following reasons:\n  - Colors are derived from prediction of VL model, specifically OFA that might not yield a correct answer.",
            "4": "-  Size relies on depth estimation models and bounding box areas to get the label.",
            "5": "How do you account for the  case when smaller objects are zoomed in for size questions (See the example in Question A)?",
            "6": "- Spatial: how do you validate that the objects are similar level with one another vs \"smaller/larger\".",
            "7": "What if objects are in front of / behind one another, and have different levels of bounding box coordinates?",
            "8": "- Unclear motivation of dataset collection for visible physics commonsense v.s.",
            "9": "prior work:\n  - Dataset is constructed based on VisualGenome, which already include color and spatial information (above, under, behind, in front of, etc) in their scene graphs attributes and relation labels, and size can be trivially inferred from object labels most of the times.",
            "10": "Why can we not just evaluate on a subset of VisualGenome scene graph tasks that deal with colors and spatial information?",
            "11": "- Winoground [1] was carefully hand-curated by expert annotators instead of relying on model-based predictions, and include far rich annotations testing visual-language compositionality that actually require looking at images to do well.",
            "12": "- ViComte [2] already covers similar and diverse visual commonsense knowledge, such as color, shape, material, size, and visual co-occurrence in their dataset.",
            "13": "How does the authors' work provide more benefits over this dataset?",
            "14": "- Limited coverage of \"visible commonsense\" understanding.",
            "15": "Color seems to be purely vision task, and size and spatial most of the times can be inferred from text information.",
            "16": "There are far more interesting cases that test such knowledge, such as counterfactual reasoning asking (what would happen if this action is applied to the object in this image).",
            "17": "- The findings of CapBERT is not novel as it is already explored in Zhang et.",
            "18": "al [2].",
            "19": "and the paper simply runs the same set up to validate their dataset.",
            "20": "- This in fact further highlights that this dataset is essentially prone to **severe text biases, in which visual understanding is not required to perform well for size and spatial questions**.",
            "21": "It is difficult to claim that the dataset tests *visible* physics commonsense knowledge if such information is not needed.",
            "22": "- Language model experiments are not as comprehensive and do not not cover performance of stronger LLMs such as ChatGPT.",
            "23": "It would be more interesting for the language community to understand in their physics commonsense understanding skills.",
            "24": "- Experiment results illustrate that the task does not have much too improve on from using current SoTA models.",
            "25": "Color questions are extracted from OFA models, and size + spatial questions do not require visual understanding to perform well.",
            "26": "Couldn't then the task be solved by 1) run OFA (which is not evaluated on this dataset) for color questions, 2) run LLMs such as GPT-4 for size + spatial reasoning tasks.",
            "27": "- More qualitative examples of when images are important to do well in the task.",
            "28": "[1]: Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality [Thrush et.",
            "29": "al.]",
            "30": "[2]: Visual Commonsense in Pretrained Unimodal and Multimodal Models [Zhang et.",
            "31": "al.]"
        },
        "YLJLgfUy0o": {
            "0": "The proposed dataset is relatively large in size and  might be a helpful resource to the community.",
            "1": "The writing is clear.",
            "2": "The evaluation lacks inclusion of recent models, which may limit the comprehensiveness of the assessment.",
            "3": "Among the four VLMs evaluated, only one is from 2022, while the rest are from earlier years.",
            "4": "Including more recent, potentially stronger models like OFA, Unified-IO etc., would make the analysis and conclusions more robust and up to date.",
            "5": "Despite the author's emphasis on the dataset's size as a novelty, previous benchmarks have derived similar conclusions and analyses using smaller, manually curated datasets as partially noted in the Related Works section.",
            "6": "The paper does not clarify how utilizing a larger, potentially noisier dataset adds unique value or insights to the community."
        }
    },
    "5sGLPiG1vE": {
        "llm_review": {
            "0": "The Concept Association Bias of Vision-Language Models\"\n\n#### Strengths\n\n1.",
            "1": "**Novelty and Relevance**: The paper introduces the concept of Concept Association Bias (CAB) in vision-language models, particularly focusing on CLIP.",
            "2": "This is a novel and relevant contribution to the field, as it highlights a specific type of bias that can significantly impact the performance of these models in tasks requiring fine-grained vision-language correspondence, such as Visual Question Answering (VQA).",
            "3": "**Comprehensive Experiments**: The authors conduct a series of well-designed experiments to demonstrate the existence and impact of CAB.",
            "4": "They use both synthetic datasets (Natural-Color Dataset and UNnatural-Color Dataset) and real-world datasets (VQA-v2 and Rel3D) to validate their findings.",
            "5": "This comprehensive approach strengthens the validity of their conclusions.",
            "6": "**Insightful Analysis**: The paper provides a detailed analysis of why CAB occurs, linking it to the way vision-language models process input as a \"bag of concepts.\"",
            "7": "This insight is valuable for understanding the limitations of current models and for guiding future research to mitigate such biases.",
            "8": "**Comparison Across Models**: The authors extend their analysis to other vision-language models, including BLIP, BLIP-2, and OFA, and compare their susceptibility to CAB.",
            "9": "This comparative analysis is useful for the community to understand how different training objectives (contrastive vs. autoregressive) influence the presence of CAB.",
            "10": "**Practical Implications**: The paper discusses practical implications of CAB, particularly in the context of VQA.",
            "11": "The authors show that reducing CAB can lead to improved performance on VQA tasks, which is a significant finding for practitioners working on vision-language applications.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Scope of Attributes**: While the paper focuses on color and part-whole relationships, it does not explore other types of attributes such as material, shape, or texture.",
            "14": "Including a broader range of attributes could provide a more comprehensive understanding of CAB.",
            "15": "**Fine-Tuning Limitations**: The paper suggests that fine-tuning with deeper modality interaction can mitigate CAB, but it also acknowledges that this approach may not solve the more general binding problem.",
            "16": "This limitation is important, but the paper could benefit from a more detailed exploration of alternative methods to address the binding problem.",
            "17": "**Prompt Variations**: The experiments on prompt variations show that CAB is relatively stable across different prompts.",
            "18": "However, the paper could include a more systematic analysis of how different types of prompts (e.g., negations, complex sentences) affect CAB, which could provide deeper insights into the robustness of the models.",
            "19": "**Evaluation Metrics**: The paper uses accuracy as the primary metric for evaluating CAB.",
            "20": "While this is a straightforward and understandable choice, incorporating additional metrics such as precision, recall, or F1-score could provide a more nuanced understanding of the models' performance.",
            "21": "**Generalization to Other Tasks**: The paper primarily focuses on VQA and color recognition tasks.",
            "22": "It would be beneficial to explore the impact of CAB on other vision-language tasks, such as image captioning, visual entailment, or visual reasoning, to understand the broader implications of this bias.",
            "23": "**CAB Score Interpretation**: The CAB score is introduced as a measure of the bias, but the paper could provide more detailed guidelines on how to interpret this score in different contexts.",
            "24": "For example, what constitutes a \"high\" or \"low\" CAB score, and how should practitioners use this information to improve their models?",
            "25": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the understanding of biases in vision-language models by introducing and analyzing the Concept Association Bias (CAB).",
            "26": "The comprehensive experiments, insightful analysis, and practical implications make it a valuable resource for researchers and practitioners in the field.",
            "27": "However, there are areas for improvement, such as exploring a wider range of attributes, providing more detailed analyses of prompt variations, and evaluating the impact of CAB on a broader set of tasks.",
            "28": "Addressing these weaknesses in future work could further enhance the understanding and mitigation of biases in vision-language models."
        },
        "n7x8k4D8z4": {
            "0": "Enough experiments and many inspiring findings.",
            "1": "The Concept Association Bias did not be scrutinized carefully.",
            "2": "Good writing.",
            "3": "The structure of the paper is clear and reading it is smooth and natural.",
            "4": "Some of the observations can be more solid.",
            "5": "For example, as described in line 493 - 495, the relation between the CAB Score and the accuracy can be quantified statistically whether the correlation is significant enough."
        },
        "qL0BQWLDsq": {
            "0": "The compositionality and attributed binding of VL models is an important problem.",
            "1": "This paper systematically studies this problem and gives some insights.",
            "2": "A comprehensive evaluation of different popular VL models ranging from models based on CE loss to models based on regressive loss, is performed.",
            "3": "The experimental parts are solid and CLIP with different encoders are evaluated.",
            "4": "There is no new effective method proposed to address the Concept Association Bias problem even though the work did some study of the effect of finetuning.",
            "5": "The novelty is limited and many similar works have been done previously, e.g.",
            "6": "[1][2][3][4].",
            "7": "[1] Learning to Compose Soft Prompts for Compositional Zero-Shot Learning\n\n[2] Training-Free Compositional Image and Text Matching\n\n[3] Does CLIP Bind Concepts?",
            "8": "Probing Compositionality in Large Image Models\n\n[4] Augmenting CLIP with Improved Visio-Linguistic Reasoning"
        },
        "1TxdgZCeDe": {
            "0": "I think the paper asks a focused interesting question, exhaustively investigates that, proposes a mitigation mechanism, and clearly points out the limitations of the proposed solution.",
            "1": "I like the expository style of writing.",
            "2": "I don't see many reasons."
        }
    },
    "wirDXDQwYZ": {
        "llm_review": {
            "0": "**Novelty and Contribution:**\n   - The paper introduces a novel framework, PRESQUE, which leverages pragmatic reasoning to interpret quantifier semantics in foundation models.",
            "1": "This is a significant contribution as it addresses a relatively unexplored area in natural language processing (NLP).",
            "2": "- The creation of the QuRe dataset is a valuable addition to the field.",
            "3": "It provides a resource for evaluating the quantifier understanding capabilities of language models, which was previously lacking.",
            "4": "**Methodology:**\n   - The combination of natural language inference (NLI) and the Rational Speech Acts (RSA) framework is innovative.",
            "5": "This approach effectively models the semantics of quantifiers by considering both literal and pragmatic interpretations.",
            "6": "- The detailed explanation of the PRESQUE framework, including the roles of the literal speaker and pragmatic listener, is well-articulated and provides a clear understanding of the methodology.",
            "7": "**Experimental Results:**\n   - The experimental results demonstrate that PRESQUE outperforms a literal reasoning baseline by 20% in predicting quantifier percentage scopes.",
            "8": "This is a substantial improvement and highlights the effectiveness of the proposed framework.",
            "9": "- The comparison of different NLI models (ALBERT, XLNet, BART, RoBERTa) and the analysis of their performance in the context of quantifier understanding is thorough and insightful.",
            "10": "**Human Evaluation:**\n   - The inclusion of human evaluation to assess the similarity between model interpretations and human interpretations of quantifiers adds credibility to the findings.",
            "11": "The use of Mechanical Turk for crowd-sourcing annotations and evaluations is well-executed.",
            "12": "**Comprehensive Analysis:**\n   - The paper provides a comprehensive analysis of the results, including qualitative examples and a discussion of the strengths and limitations of the models.",
            "13": "This helps in understanding the practical implications of the findings.",
            "14": "**Weaknesses:**\n\n1.",
            "15": "**Scope of Quantifiers:**\n   - While the paper focuses on a subset of generalized quantifiers, it does not cover the entire spectrum of quantifier semantics.",
            "16": "This limitation is acknowledged, but it would have been beneficial to explore a broader range of quantifiers to provide a more comprehensive evaluation.",
            "17": "**Domain Specificity:**\n   - The QuRe dataset is derived from Wikipedia sentences, which may limit the generalizability of the findings to other domains or languages.",
            "18": "The paper could have included experiments on additional datasets from diverse domains to strengthen the generalizability claims.",
            "19": "**Subjectivity in Annotations:**\n   - The process of assigning percentage scopes to quantifiers can be subjective and influenced by various factors such as the annotator's background and interpretation.",
            "20": "While the paper acknowledges this, it would have been useful to discuss potential strategies to mitigate this subjectivity.",
            "21": "**Comparison with Other Approaches:**\n   - The paper primarily compares PRESQUE with a literal reasoning baseline.",
            "22": "Including comparisons with other state-of-the-art approaches for quantifier understanding, if available, would have provided a more robust evaluation of the proposed framework.",
            "23": "**Complexity of Implementation:**\n   - The implementation of the PRESQUE framework, which involves combining NLI models with the RSA framework, may be complex and computationally intensive.",
            "24": "A discussion on the computational requirements and potential optimizations would have been beneficial.",
            "25": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the understanding of quantifier semantics in foundation models through the innovative PRESQUE framework.",
            "26": "The creation of the QuRe dataset and the comprehensive experimental analysis are commendable.",
            "27": "While there are some limitations regarding the scope of quantifiers, domain specificity, and subjectivity in annotations, the strengths of the paper outweigh these weaknesses.",
            "28": "The findings have important implications for improving the language understanding capabilities of NLP systems, particularly in tasks involving quantifiers."
        },
        "l2WtFnw8eX": {
            "0": "- The motivation and problem statement of the paper is clear, and the theoretical background and experimental design to support the authors’ claims are clearly explained, making the text itself highly complete.",
            "1": "- The attempt to limit the range of Generalized Quantifiers using the perspective of pragmatics seems novel, and the QuRe dataset also has a versatility that can be useful in the NLI field where quantifier-related reasoning is required in the future.",
            "2": "- I have no reason to reject."
        },
        "EivO1ByeJJ": {
            "0": "The paper makes a contribution to the NLP interpretation of generalized quantifiers (GQs), which are frequent in natural language communication but have been mostly neglected in NLP.",
            "1": "GQs are notoriously vague in their meaning but for some applications  (e.g.",
            "2": "in human-robot interaction) it is necessary to interpret the quantification in percentages.",
            "3": "The paper offers an interesting pragmatically inspired algorithm to determine percentage interpretation on the basis of existing LLMs that are fine-tuned for NLI.",
            "4": "It also introduces the new dataset QuRe of sentences that feature GQs.",
            "5": "The QuRe sentences are automatically (GPT-3.5-turbo) annotated with percentage range (derived from the original sentences that included percentage information plus potential modifications such as \"about\", \"at least\")  and 3 topic labels.",
            "6": "They are also manually annotated with a 3-level “specificity” score which indicates whether the sentence without the quantifier gives partial or full information about the percentages or whether it is indeterminable from the context.",
            "7": "For the 5 quantifiers in the Herbelot-Vecchio dataset percentage interpretations are crowd-sourced from 25 human judges and visually compared to (RoBERTa-large NLI-based ) PRESQUE results.",
            "8": "A random sample of the results on the Herbelot-Vecchio dataset is evaluated by human judges.",
            "9": "The results on the QuRe dataset are evaluated by employing five evaluations measure  that emphasize different properties (HIT@1, MRR, Cross-Entropy, F1@n, and consecutiveness}.",
            "10": "Although not good, the results are better than (random seeds) baselines and the generall quantifier strengths correspond to previous hierarchies based on human judgements.",
            "11": "Origin of QuRe und its distribution of GQs: The QuRe dataset is based on Wikipedia sentences that include the target percentages which means that the LLMs have most likely seen the target interpretation in their pre-training data.",
            "12": "That decreases the usefulness of the data set.",
            "13": "In addition, the distribution of the quantifier expressions in QuRe is very skewed (26,3 % \"some\" aka about 196 instances but less than 1% aka 7 or less instances of \"likely\", \"seldom\", \"occasionally\", \"none\" - the authors only provide relative frequencies).",
            "14": "The distribution of percentage scopes is not reported, but doesn't seem to be balanced either.",
            "15": "Linguistic soundness: The set of \"quantifier words\" is very heterogenous, some are not normally seen as quantifiers: The reference to Srivastava et al.",
            "16": "(2018) accounts for the inclusion of frequency adverbs such as \"usually\", \"rarely\", but the inclusion of bare adjectives such as \"tiny\"or \"large\" without a noun such as \"part\" or \"amount\" requires further explanation (I don't think that it is a problem that they are listed in the crowd-sourcing task without explanation, though).",
            "17": "Thoroughness/Informativeness: The quantitative evaluations on QuRe report only averaged values over all quantifiers.",
            "18": "Soundness: The human evaluation on the performance of L0 vs. L1 on the 50 test sentences that were randomly chosen from the Herbelot-Vecchio dataset (10 sentences per each of the 5 quantifiers) evaluated by 40 annotators is not tested for statistical significant differences.",
            "19": "[In favour of the authors' interpretation: If I interpret the numbers in figure 7 correctly the results for \"some\", \"all\" and the overall values are statistically significant (based on a chi-square test) and do indeed prefer the output of the PRESQUE L1].",
            "20": "Clarity: The paper works with two datasets with two different sets of \"quantifiers\".",
            "21": "The paper should always indicate which dataset is referred to,  e.g.",
            "22": "in all table/figure captions (as it is the case in figure 1 and table 3)."
        },
        "MtlQh9h9oQ": {
            "0": "The paper smartly designed a new corpus with quantifiers mapped to a percentage scope which be easily evaluated for quantifier semantics inference.",
            "1": "The authors presented the annotation and data statistics in detail.",
            "2": "A suite of comprehensive evaluations is conducted including the comparison of the distributions between human and model interpretation, automatic metrics, consecutiveness, etc.",
            "3": "Overall this paper presented a thorough introduction to data collection, analysis, and experimental results.",
            "4": "I have several clarification questions regarding the framework design and annotation procedure.",
            "5": "Please check below."
        }
    },
    "MXMA6vQtSZ": {
        "llm_review": {
            "0": "**Novelty and Relevance:** The paper addresses a relatively unexplored area in NLP, specifically the political bias in automatic summarization models.",
            "1": "Given the increasing reliance on automated systems for information dissemination, this study is both timely and relevant.",
            "2": "**Methodological Innovation:** The use of an entity replacement method to control for the content of the source document is a clever and effective way to isolate the impact of specific entities on summarization models.",
            "3": "This approach allows for a more controlled and precise analysis of bias.",
            "4": "**Comprehensive Analysis:** The study evaluates both extractive and abstractive summarization models, providing a broad view of how different types of models might encode biases.",
            "5": "The inclusion of multiple models (PRESUMM, PEGASUS, BART, and PROPHET NET) strengthens the generalizability of the findings.",
            "6": "**Detailed Results:** The paper provides a thorough analysis of the differences in summaries upon entity replacement, including overall similarity, entity representation, and temporal differences in status indicators.",
            "7": "This multi-faceted approach helps in understanding the nuances of how biases manifest in summarization models.",
            "8": "**Publicly Available Data and Code:** By making the data and code available, the authors promote transparency and reproducibility, which are crucial for advancing research in this area.",
            "9": "**Ethical Considerations:** The paper includes a thoughtful discussion on the ethical implications of their findings, highlighting both the potential harms and benefits of understanding biases in summarization models.",
            "10": "**Weaknesses:**\n\n1.",
            "11": "**Limited Scope of Entities:** The study focuses primarily on two US politicians, Donald Trump and Joe Biden.",
            "12": "While the inclusion of Obama and Bush as additional entities helps, the scope remains limited to high-profile US political figures.",
            "13": "The findings might not generalize to other politicians or entities in different contexts or countries.",
            "14": "**Language and Cultural Bias:** The study is conducted exclusively on English news articles.",
            "15": "Political bias can vary significantly across different languages and cultural contexts, and the findings may not be applicable to non-English or non-US contexts.",
            "16": "**Lack of Co-reference Resolution:** The paper does not address co-reference resolution, which could impact the accuracy of entity replacement, especially when dealing with pronouns or titles.",
            "17": "This omission might lead to incomplete or inaccurate replacements, affecting the validity of the results.",
            "18": "**Hyperparameter Settings and Decoding Algorithms:** The study uses default settings for the summarization models.",
            "19": "Different hyperparameter settings or decoding algorithms could lead to different results.",
            "20": "A more comprehensive analysis could include variations in these settings to understand their impact on bias.",
            "21": "**Absence of GPT-3+ Models:** While the paper acknowledges the exclusion of GPT-3+ models due to the timing of the study, the absence of these state-of-the-art models limits the comprehensiveness of the analysis.",
            "22": "Future studies should include these models to provide a more complete picture of biases in summarization.",
            "23": "**Normative Questions Left Unanswered:** The paper raises important normative questions about what constitutes an ideal automatic summary but does not attempt to answer them.",
            "24": "While this is a deliberate choice, providing some initial thoughts or frameworks for addressing these questions could have added value to the discussion.",
            "25": "**Conclusion:**\n\nOverall, the paper makes a significant contribution to the understanding of political bias in automatic summarization models.",
            "26": "The innovative methodology, comprehensive analysis, and ethical considerations are commendable.",
            "27": "However, the study's scope is somewhat limited, and future research should address these limitations by including a broader range of entities, languages, and model settings.",
            "28": "Additionally, incorporating state-of-the-art models like GPT-3+ and addressing co-reference resolution would further strengthen the findings.",
            "29": "Despite these weaknesses, the paper provides a solid foundation for future work in this important area."
        },
        "lZB70sZrMy": {
            "0": "- methodology of keeping everything the same except the entity name is simple and strong\n- code and data will be released\n- has important consequences for example related to how quickly (or not quickly) associations change in a language model (aka president vs vice president )\n- good awareness of limitiations - by only comparing Trump, Biden, Bush, Obama with such strong presences it is limited into the generalizability, but it shows at least the potential of the method.",
            "1": "- a lot of important information for being able to assess the results yourself are now put in the appendices.",
            "2": "Particularly appendix A and B"
        },
        "GD8niu6AsU": {
            "0": "The study provides empirical evidence of substantial differences across four summarization models when analyzing the summaries of news articles involving different politicians.",
            "1": "The research lays the groundwork for further exploration of biases in summarization models, encouraging future research and discussion in this essential research area.",
            "2": "The authors acknowledge the importance of considering normative questions related to what constitutes an ideal summary.",
            "3": "They emphasize the need for responsible use of summarization models and provide a framework for assessing biases.",
            "4": "While the paper states that the term \"bias\" is used to refer to significant differences, it might be beneficial to clarify and define the specific type of bias being explored, to avoid misunderstandings or misinterpretations.",
            "5": "To strengthen the study's findings, the paper could include a comparative analysis with human-generated summaries to ascertain the extent of bias in automatic summarization models compared to human summarizers."
        },
        "470YXeQvAq": {
            "0": "The use of an entity replacement method to analyze the portrayal of politicians in automatically generated summaries is interesting and has intellectual merit.",
            "1": "The observation and conclusion it leads to, i.e.",
            "2": "\"summarization model biases towards learn different representations for different entities and provide consistently different summaries depending on the  entities involved.",
            "3": "\", presents an important and interesting research problem for the community to address.",
            "4": "In the paper's current form, there doesn't seem to be an definitive conclusion which will help future researchers understand \"what exactly we need to work on to improve summarization models/task definitions\".",
            "5": "IMO there are many open questions left hanging that could be answered with further experiments and analysis, which will greatly strengthen the overall presentation + claims made in the paper.",
            "6": "Here are some improvements that I would suggest:\n1.",
            "7": "With all summarization models, include what summarziation datasets they are trained on, as this directly governs model's behavior on what types of summary it will produce.",
            "8": "It would be much easier to read + understand your results that way.",
            "9": "Include analysis for more \"control\" variables, e.g.",
            "10": "does the source of a document matter for what kind of summaries models will produce?",
            "11": "e.g.",
            "12": "Do models tend to generate Trump less frequently than Biden in left-leaning vs. right-leaning sources?",
            "13": "Including analysis like this will strengthen your claim that the bias comes from \"summarization model\", not something else.",
            "14": "Here are some potential extensions + further questions for analysis that the authors can think about --\n1.",
            "15": "What might be the cause of the Trump vs. Biden bias from the model or summarization task perspective.",
            "16": "Again, from my intuition, the choice of training data for summarziation models will have a large impact on the results,  e.g.",
            "17": "most of the models mentioned in the paper are trained on data from more left-leaning news sources.",
            "18": "Generalize the analysis to include more entities -- This will potentially reveal what types of entities do model exhibit biases towards, and help us understand the systematic causes behind the phenomena."
        }
    },
    "RN5KLywTll": {
        "llm_review": {
            "0": "Investigating their struggle with spatial reasoning\"\n\n#### Strengths\n\n1.",
            "1": "**Novel Benchmark Creation**: The authors introduce three new benchmarks (What’sUp, COCO-spatial, and GQA-spatial) specifically designed to isolate and evaluate spatial reasoning in vision-language (VL) models.",
            "2": "This is a significant contribution as it addresses the limitations of existing datasets that conflate spatial reasoning with other types of reasoning.",
            "3": "**Comprehensive Evaluation**: The paper evaluates a wide range of 18 VL models, covering various architectures, training objectives, and training data.",
            "4": "This extensive evaluation provides a thorough understanding of the current state of VL models in terms of spatial reasoning.",
            "5": "**Detailed Analysis**: The authors provide a detailed analysis of why VL models struggle with spatial reasoning.",
            "6": "They investigate the training data (LAION-2B) and identify key issues such as the rarity, ambiguity, and extraneous nature of spatial prepositions in the training captions.",
            "7": "**Insightful Experiments**: The paper includes several insightful experiments, such as visual analogies and finetuning with hard negative captions, to understand the limitations and potential improvements for VL models.",
            "8": "These experiments are well-designed and provide valuable insights into the models' capabilities and shortcomings.",
            "9": "**Open Source Contribution**: The authors release their data and code, which is a commendable effort to facilitate further research in this area.",
            "10": "This openness will likely encourage other researchers to build upon their work and contribute to the field.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Limited Scale of Benchmarks**: While the new benchmarks are a significant contribution, their scale is relatively limited compared to other large-scale datasets.",
            "13": "This limitation might affect the generalizability of the findings and the robustness of the conclusions drawn from the experiments.",
            "14": "**Modest Improvement with Finetuning**: The methods proposed to improve model performance, such as finetuning on specific datasets and incorporating caption priors, do not lead to significant improvements across all benchmarks.",
            "15": "This suggests that the proposed solutions might not be sufficient to address the underlying issues in VL models' spatial reasoning capabilities.",
            "16": "**Focus on Spatial Reasoning Only**: The paper focuses exclusively on spatial reasoning, which, while important, is just one aspect of the broader challenge of vision-language understanding.",
            "17": "A more comprehensive study that includes other types of reasoning could provide a more holistic view of the models' capabilities and limitations.",
            "18": "**Ambiguity in Preposition Usage**: The paper highlights the ambiguity in the usage of spatial prepositions in the training data but does not propose a concrete solution to address this issue.",
            "19": "While the study of the LAION dataset is insightful, it would be beneficial to see more concrete steps towards mitigating this ambiguity in future work.",
            "20": "**Human Performance Estimation**: The estimation of human performance on the benchmarks is based on a small sample size and annotations by experts.",
            "21": "A larger and more diverse set of human annotations could provide a more accurate baseline for comparison.",
            "22": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the field of vision-language understanding by highlighting the challenges VL models face with spatial reasoning and proposing new benchmarks to evaluate these capabilities.",
            "23": "The comprehensive evaluation and detailed analysis provide valuable insights, although the proposed solutions for improvement show only modest gains.",
            "24": "Future work could focus on scaling the benchmarks, addressing the ambiguity in preposition usage, and exploring other types of reasoning to provide a more comprehensive understanding of VL models' capabilities."
        },
        "KNlX0UoLDu": {
            "0": "- The proposed dataset, though relatively small, is a very useful resource to test models' ability to deal with spatial relationships.",
            "1": "I particularly value the controlled nature of the dataset, which prevents models from exploiting distribution/training biases.",
            "2": "I would just recommend the authors name it differently (see my point below).",
            "3": "- The set of models used for the experiments is very comprehensive and includes many different types of architectures.",
            "4": "- The results are extremely interesting, and highlight one key limitation of current L&V models -- spatial understanding -- that the community should take very seriously.",
            "5": "- It is not explained how human judgments on a sample of 100 data points were collected (if any): who are the annotators, how many per sample, how were they recruited, what was their native language, how much they were paid, what were the instructions, and what it means to compute a \"conservative accuracy\" on the task.",
            "6": "These are important details that should be given.",
            "7": "- Not a reason to reject, per se, but I don't think it is fair to name the new dataset RealCLEVR considering that it doesn't directly build on the actual CLEVR dataset (which contains images of geometrical objects with few basic features: color, texture, size, etc.).",
            "8": "In this sense, the name is misleading, and I would recommend the authors to consider using a different one.",
            "9": "- The paper lacks an insightful discussion on top of the various results: zero-shot, fine-tuning, and ablations.",
            "10": "What do the results implicate and what do they tell us about model abilities to deal with spatial prepositions and similar expressions?"
        },
        "S3pYvMUkxd": {
            "0": "- Interesting problem\n- Clear writing - Lack of novelty\n- Shallow literature review"
        },
        "yQXc44aw4g": {
            "0": "The contributions of this paper are substantial:\n\n* The number of tested VL models is astounding: 18!",
            "1": "* The authors have carefully introduced new data (dog on the table) to also capture rare spatial relations.",
            "2": "* Interesting analysis of LAION-2B which is used as a pretraining corpus for VLMs.",
            "3": "It is surprising that the strategies which come out of these analyses, do not work too well.",
            "4": "This is really good to know.",
            "5": "* I have to say that even after reading this paper, I now do not feel more informed about why VLMs fail at spatial reasoning than before.",
            "6": "It still seems that we have no idea of what goes on in these models and the combination of the training task and rarity of some prepositions in the training data are to blame, but the exploration of data-informed improvements did not succeed.",
            "7": "Why?",
            "8": "This makes the paper's impact onto the community less than I expected from title, abstract and introduction.",
            "9": "* The paper does not make very clear what previous work on spatial relations does and how it differs from it."
        }
    },
    "sCtJmxhvJe": {
        "llm_review": {
            "0": "**Novel Dataset Creation**: The paper introduces a novel dataset of GPT-generated English text annotated for gender bias.",
            "1": "This dataset is unique in that it provides fine-grained, real-valued scores for the degree of gender bias, which is a significant contribution to the field of NLP and bias detection.",
            "2": "**Methodological Rigor**: The use of Best-Worst Scaling (BWS) for annotation is a strong methodological choice.",
            "3": "BWS is known for its reliability in capturing subjective judgments, and the paper demonstrates its effectiveness in the context of gender bias annotation.",
            "4": "**Comprehensive Analysis**: The paper provides a thorough analysis of the generated data, including the impact of different seed types and prompting methods on the degree of gender bias.",
            "5": "This level of detail helps in understanding the nuances of bias generation in GPT models.",
            "6": "**Benchmarking and Evaluation**: The authors benchmark the dataset against existing models for gender bias and offensive language detection.",
            "7": "They also evaluate the performance of GPT-3.5-Turbo and GPT-4 in predicting gender bias scores, providing a comprehensive evaluation of current state-of-the-art models.",
            "8": "**Qualitative Insights**: The qualitative analysis of GPT-4's reasoning for its bias scores is insightful.",
            "9": "It highlights the limitations of current models in understanding and explaining gender bias, which is crucial for future improvements.",
            "10": "**Ethical Considerations**: The paper includes a detailed discussion on ethical considerations, including the potential impact on annotators and the limitations of the dataset.",
            "11": "This transparency is commendable and necessary for responsible AI research.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Annotator Diversity**: The annotators were all from India and had at least an undergraduate degree.",
            "14": "While this ensures a certain level of understanding, it limits the diversity of perspectives.",
            "15": "Gender bias is a highly subjective concept, and a more diverse annotator pool could provide a richer understanding of bias.",
            "16": "**Seed Selection and Prompting**: While the paper discusses the careful selection of seeds and prompting strategies, the process is still somewhat manual and subjective.",
            "17": "This could introduce biases based on the authors' perspectives.",
            "18": "Automating or diversifying this process could improve the robustness of the dataset.",
            "19": "**Focus on GPT-3.5-Turbo and GPT-4**: The paper primarily focuses on GPT-3.5-Turbo and GPT-4 for data generation and evaluation.",
            "20": "While these are state-of-the-art models, it would be beneficial to include a comparison with other models to understand the generalizability of the findings.",
            "21": "**Limited Scope of Themes**: The thematic analysis shows that the dataset may have a limited diversity of themes.",
            "22": "This could be a result of the seed selection process or the inherent biases in the GPT models.",
            "23": "Expanding the scope to include more diverse themes could provide a more comprehensive understanding of gender bias.",
            "24": "**Reasoning Capabilities of GPT-4**: The qualitative analysis reveals that GPT-4's reasoning for its bias scores is often flawed, especially for less explicit biases.",
            "25": "This indicates a need for further research into improving the contextual reasoning capabilities of LLMs.",
            "26": "**Scalability of BWS**: While BWS is effective, it is also resource-intensive, requiring multiple annotations for each statement.",
            "27": "This could limit the scalability of the approach for larger datasets or more complex tasks.",
            "28": "#### Conclusion\n\nOverall, \"Fifty Shades of Bias\" is a significant contribution to the field of NLP and bias detection.",
            "29": "The novel dataset, rigorous methodology, and comprehensive analysis provide valuable insights into the generation and perception of gender bias in GPT models.",
            "30": "However, there are areas for improvement, particularly in terms of annotator diversity, thematic scope, and the reasoning capabilities of LLMs.",
            "31": "Addressing these weaknesses in future work could further enhance the impact and applicability of this research."
        },
        "sM2FEOhbSv": {
            "0": "The authors highlight the importance of a nuanced approach to gender bias, noting that implicit gender bias can be subtle but is important to detect.",
            "1": "As such, the authors present a clear case for the benefit of having graded gender bias annotations.",
            "2": "They also clearly present the benefits of the BWS approach in terms of efficiency for annotating subjective judgements.",
            "3": "The authors analyse their generated data set to identify key themes using multiple methods, which allows them to draw some top-level conclusions about trends in the kind of biased content being produced.",
            "4": "Given the authors’ assertion that gender bias is highly subjective I would have liked to see more discussion on the method used to convert labels into scores.",
            "5": "Some implicit assumptions (that annotator gender has no impact on the importance of a judgement, that controversial statements with a mix of high and low ranks should receive a “middling” score) need to be made explicit and discussed.",
            "6": "On a related note, given the topic of the paper, I think it is a shame the authors did not engage with the literature on handling annotation disagreement for subjective labels - the only paper referenced that discusses how annotations are subjective is Blodgett et al.",
            "7": "2020, although the authors repeat this point in a number of places.",
            "8": "I would have liked to see the authors engage more with the limitations of using GPT models to generate biased text.",
            "9": "They acknowledge that the model is “limited by the dataset it has seen during training”, but the paper would benefit from more consideration of this point.",
            "10": "For example, it is likely that OpenAI “cleaned up” the training data seen by the model during training, or conducted alignment before release, meaning the range of explicitly gender biased sentences the model was exposed to was greatly reduced, limiting what it can produce.",
            "11": "It could have been interesting to do a qualitative analysis comparing the generated samples with sentences from a gender bias data set that was sampled from an online corpus (and in general, more qualitative analysis of the data set beyond identifying key words would have been beneficial)."
        },
        "ewsHOAACVA": {
            "0": "The authors provide a dataset for explicit rankings of the degrees of gender bias, expanding upon existing datasets for binary classification.",
            "1": "I believe that this is an important space to break into, as binary classification is very limiting and can constrain different viewpoints.",
            "2": "The dataset creation is analyzed thoroughly, from prompting methods to annotator agreement to sample bin themes.",
            "3": "The authors benchmarked the dataset across different types of models, including toxicity, gender bias, and, and offensive language.",
            "4": "One concern I have is in using a LLM to generate a new dataset.",
            "5": "This can drastically limit the ideas generated in a dataset and provide a constrained viewpoint in such a sensitive domain.",
            "6": "In addition to the sample generation, the limited diversity in annotators should be considered.",
            "7": "An ideal dataset would be diverse across various cultural opinions."
        },
        "qTU2Se7eRt": {
            "0": "LLM's gender bias responses were generated using a very carefully curated list of seeds, which the authors derived from existing related datasets (StereoSet and COPA).",
            "1": "The prompting methods are diverse and carefully chosen to ensure good coverage of different text types (conversation, completion, conversion).",
            "2": "The annotation process was carefully designed and monitored, using 20 annotators that were carefully instructed to do their job.",
            "3": "The quality and consistency of annotations is evaluated correctly.",
            "4": "The authors seem to know the related literature and resources very well, which they describe well and from which they choose the parts that best fit their research.",
            "5": "Theme identification is carried out in a rigorous manner, calculating PMI scores of unique words and sentences and an informed Dirichlet model and Convokit to identify meaningful n-grams.",
            "6": "The qualitative analysis complements well the plethora of qualitative results offered and offers further insights.",
            "7": "The authors make the annotated datasets freely available.",
            "8": "I can't think of any."
        }
    },
    "z1RYLqEpuP": {
        "llm_review": {
            "0": "**Novelty and Relevance**: The paper addresses a critical issue in cross-lingual question answering (QA) systems—attribution.",
            "1": "This is particularly important as QA systems become more prevalent and are used in diverse linguistic contexts.",
            "2": "The focus on attribution ensures that the answers provided by these systems are trustworthy and verifiable, which is crucial for their adoption and reliability.",
            "3": "**Comprehensive Dataset**: The introduction of the XOR-AttriQA dataset is a significant contribution.",
            "4": "The dataset includes approximately 10,000 annotated examples across five languages (Bengali, Finnish, Japanese, Russian, and Telugu), which is a valuable resource for the research community.",
            "5": "This dataset will likely foster further research in the area of cross-lingual QA and attribution.",
            "6": "**Detailed Analysis**: The paper provides a thorough analysis of the attribution levels of a state-of-the-art cross-lingual QA system (CORA).",
            "7": "The finding that a substantial portion of answers is not attributable to any retrieved passages, even when they exactly match the gold reference, is both surprising and insightful.",
            "8": "This highlights a critical gap in current QA systems and sets the stage for further improvements.",
            "9": "**Methodological Rigor**: The authors employ a wide range of attribution detection techniques, including Natural Language Inference (NLI) models and fine-tuning large language models like PaLM 2.",
            "10": "The use of both in-language and in-English attribution scenarios adds depth to the evaluation and provides a comprehensive understanding of the attribution problem.",
            "11": "**Practical Implications**: The paper not only identifies the problem but also proposes practical solutions.",
            "12": "The use of NLI models and PaLM 2 for attribution detection shows promising results, with significant improvements in attribution levels.",
            "13": "This has direct implications for the deployment of more reliable QA systems in real-world applications.",
            "14": "**Clear Contributions**: The paper clearly outlines its contributions, including the introduction of the XOR-AttriQA dataset, the evaluation of CORA's attribution levels, and the demonstration of effective attribution detection models.",
            "15": "This clarity helps in understanding the impact and scope of the work.",
            "16": "#### Weaknesses:\n\n1.",
            "17": "**Limited Language Scope**: While the paper covers five languages, it would be beneficial to include more languages, especially those with different linguistic structures and lower resources.",
            "18": "This would provide a more comprehensive evaluation of the attribution problem in cross-lingual QA systems.",
            "19": "**Translation Quality**: The paper relies on automatic translation for some of the evaluation scenarios.",
            "20": "While the authors note that translation errors did not significantly impact the results, a more detailed analysis of the translation quality and its potential effects on attribution would strengthen the findings.",
            "21": "**Evaluation Metrics**: The paper primarily uses accuracy and ROC-AUC as evaluation metrics for attribution detection models.",
            "22": "While these are standard metrics, additional metrics such as precision, recall, and F1-score could provide a more nuanced understanding of the model performance, especially in cases where the class distribution is imbalanced.",
            "23": "**Scalability of Solutions**: The proposed solutions, particularly the fine-tuning of large language models like PaLM 2, may not be feasible for all researchers or practitioners due to computational resource constraints.",
            "24": "Discussing the scalability and potential alternatives for resource-constrained environments would enhance the practical applicability of the work.",
            "25": "**User-Centric Evaluation**: The paper focuses on technical evaluation metrics but does not include user-centric evaluations.",
            "26": "Understanding how end-users perceive the attributed answers and their trust in the system would provide valuable insights into the real-world impact of the proposed solutions.",
            "27": "**Future Directions**: While the paper identifies critical research directions, it could benefit from a more detailed discussion on how to address the identified gaps, particularly in improving cross-lingual retriever systems and designing robust LLM-based metrics for QA evaluation.",
            "28": "#### Conclusion:\n\nOverall, the paper makes significant contributions to the field of cross-lingual question answering by highlighting the importance of attribution and providing practical solutions to improve it.",
            "29": "The introduction of the XOR-AttriQA dataset and the detailed analysis of attribution levels in state-of-the-art QA systems are particularly valuable.",
            "30": "Addressing the identified weaknesses, such as expanding the language scope and providing more user-centric evaluations, would further strengthen the impact of this work."
        },
        "hkBy9tf5QB": {
            "0": "Attribution is the most promising approach in addressing factual errors.",
            "1": "This paper finds that even the best XORQA system predictions lack attribution.",
            "2": "This paper proposed to use a large language model and natural language inference models to detect and rerank passages to improve the attribution-level of a state-of-the-art XORQA system.",
            "3": "I don't see a big reason to reject this paper."
        },
        "WqcbVJpVA6": {
            "0": "A novel dataset in a novel setup of cross-lingual question answering.",
            "1": "The evaluation of attribution has been done for only one model, although being popular but not the best in current time."
        },
        "jUo18HJPaB": {
            "0": "The paper is well written and easy to read.",
            "1": "It introduces attribution in cross-lingual QA.",
            "2": "In addition to presenting existing attribution problems in the current system, the paper also collects new annotations, proposes two methods for attribution classification and could further improve QA performance.",
            "3": "- It’s great to see the paper tries to improve explainability in QA systems by attributing answers to retrieved documents.",
            "4": "However, framing attribution detection as a binary classification problem does not provide further clarity compared to the ranking provided by the retriever.",
            "5": "- Domain is limited to wikipedia articles.",
            "6": "There’s a lack of discussion about how this work could be extended to other domains.",
            "7": "- Comparison between mT5–QA and mT5-NLI.",
            "8": "The amount of data used to finetune these two variants are significantly different.",
            "9": "I’m wondering if adding more QA data to the former, would that bring the gap closer?",
            "10": "Or training on a combination of QA and NLI data would further boost the performance?",
            "11": "- There’s no mention of the compute cost and the training details.",
            "12": "- The authors appear to have not invested much thought into the limitation section, as it does not discuss the paper's own limitations.l"
        }
    },
    "CgAfbI4kGS": {
        "llm_review": {
            "0": "**Novel Benchmark Introduction**: The paper introduces a novel benchmark, COMPLE QA, which is designed to comprehensively assess the influence of Knowledge Graph Completion (KGC) methods on Knowledge Graph Question Answering (KGQA).",
            "1": "This is a significant contribution as it addresses a gap in the current research landscape where the impact of KGC on downstream tasks has not been thoroughly studied.",
            "2": "**Comprehensive Dataset**: The benchmark includes a substantial dataset with a knowledge graph containing 3 million triplets across 5 distinct domains, over 5,000 question-answering pairs, and a completion dataset aligned with these questions.",
            "3": "This extensive dataset allows for a robust evaluation of KGC methods.",
            "4": "**Evaluation of Multiple Methods**: The paper evaluates four well-known KGC methods (TransE, DistMult, ComplEx, and RotatE) in combination with two state-of-the-art KGQA systems (DecAF and Pangu).",
            "5": "This comprehensive evaluation provides valuable insights into the effectiveness of different KGC methods in improving KGQA performance.",
            "6": "**Insightful Findings**: The study reveals that effective KGC can significantly mitigate the impact of knowledge graph incompleteness on question-answering performance.",
            "7": "Additionally, the finding that the best-performing KGC method does not necessarily lead to the best QA results underscores the need to consider downstream applications when developing KGC methods.",
            "8": "**Detailed Methodology**: The paper provides a detailed description of the benchmark construction, including the knowledge graph, question-answering dataset, and completion dataset.",
            "9": "This transparency in methodology allows for reproducibility and further research based on the benchmark.",
            "10": "**Weaknesses:**\n\n1.",
            "11": "**Limited Scope of Downstream Tasks**: While the focus on KGQA is justified, the paper could have explored the impact of KGC on other downstream tasks such as recommendation systems or semantic search.",
            "12": "This would provide a more holistic understanding of the utility of KGC methods.",
            "13": "**Exclusion of Large Language Models**: The study does not employ large language models for few-shot learning in question answering, which are currently popular in the field of NLP.",
            "14": "Including such models could provide additional insights and validate whether the conclusions hold true in these trending methods.",
            "15": "**Impact of Incorrect Triplets**: The paper acknowledges that incorrect triplets introduced by KGC methods can negatively impact QA performance.",
            "16": "However, it does not propose specific strategies to mitigate this issue.",
            "17": "Future work could focus on developing methods to filter out incorrect triplets more effectively.",
            "18": "**Correlation Analysis**: The paper presents a correlation analysis between KGC performance metrics and QA performance.",
            "19": "While this is useful, the analysis could be expanded to include more sophisticated statistical methods to better understand the relationship between KGC and QA performance.",
            "20": "**Ethical Considerations**: The paper briefly touches on the ethical implications of KGC methods but does not delve deeply into potential biases and inaccuracies that could be introduced.",
            "21": "A more thorough discussion on how to identify, measure, and rectify biases within KGC algorithms would strengthen the ethical considerations of the study.",
            "22": "**Conclusion:**\n\nOverall, the paper makes a significant contribution to the field by introducing a novel benchmark for assessing the impact of KGC methods on KGQA.",
            "23": "The comprehensive dataset, detailed methodology, and insightful findings provide a solid foundation for future research.",
            "24": "However, the study could be further strengthened by exploring the impact of KGC on a broader range of downstream tasks, including large language models, and addressing the issue of incorrect triplets more effectively.",
            "25": "Additionally, a deeper analysis of ethical considerations would enhance the overall impact of the research."
        },
        "KOmEgITkwH": {
            "0": "The experimental results seem solid.",
            "1": "The writing is clear and easy to follow\nThe question is interesting and worth investigation 1.",
            "2": "The overall setting is weird to me.",
            "3": "The incompletion ratio is set to 20, 50, 80%.",
            "4": "however, how would one opt to such a KG system with such an extremely high incompletion ratio?",
            "5": "This is not realistic.",
            "6": "The incompletion in KG can sometimes lead to improvement in QA.",
            "7": "This is an interesting finding, but how would you support the QA system with this finding?",
            "8": "I did not see a deeper analysis of leveraging this and how to overcome the drawbacks."
        },
        "ADvxxb4nNW": {
            "0": "* The paper is well written and motivated, and it is easy to follow.",
            "1": "* The experimental design is sound and detailed.",
            "2": "* Interesting and novel task that has been overlooked in the literature.",
            "3": "Has potential on further defining the practical usefulness of KGC by influencing the way of evaluating of KGC in the context of practical applications.",
            "4": "The paper lacks analysis that would give insights on why better KGC performance does not align with better KGQA performance.",
            "5": "This includes quantitative analysis, e.g,.",
            "6": "the entity types / relations for which this phenomenon is more frequent, and qualitative analysis, e.g., questions that are anyway harder to answer even if the correct triples have been predicted by KGC.",
            "7": "Since this is a short paper, this could be included in the Appendix."
        },
        "bZhVg943r6": {
            "0": "A strength of this paper is its focus on grounding a long studied academic Graph task (KGC) in a long standing downstream NLP task (KGQA).",
            "1": "The intuition to ground a task in its benefits to other tasks gives weight to the importance of their proposal on application in this domain.",
            "2": "Additionally, the authors start from a widely cited and popular benchmark (GrailQA) that includes questions from IID, compositional, and zero-shot domains; this adds empirical weight to their findings because this is widely thought to be a high quality benchmark.",
            "3": "Lastly, the authors select a representative set of popular KGC methods.",
            "4": "In terms of writing, this paper is well written and easy to follow.",
            "5": "I think there are two reasons to consider rejecting this paper:\n1) The notion of incompleteness for their KGC task is somewhat artificial; the authors obtain relations of answer entities in dev/test set, and  \"incomplete\" them by obscuring one of the entities in the relation, and then \"randomly choose a proportion P of these triplets as the final validation and test sets\".",
            "6": "This challenge here is that not all triples of answer entities are equally meaningful for QA; i.e.",
            "7": "it is possible that some and probably likely that some, if not most, triples, are largely meaningless.",
            "8": "As a result, obscuring them would be expected to have little/no impact on QA.",
            "9": "This risks significantly diluting the correlation between KGC quality and downstream QA.",
            "10": "Specifically, a KGC model might look great because it fills in lots of relations but then look bad for QA because some large percent of those relations were meaningless.",
            "11": "This can potentially mislead the reader into thinking that a particular KGC model is strong on KGC but bad for QA, when in fact the measurement of KGC model strength on this benchmark may just be an artifact of the benchmark.",
            "12": "I think this can be improved by applying stratified sampling over predicates to ensure that we are controlling not just for the entities we obscure but also for relative importance of the information the KGC model completes.",
            "13": "Further motivating the need for a deeper look into this approach is Appendix D, which seems to indicate a weak correlation between KGC and KGQA.",
            "14": "It would be interesting to see the set of questions for which there is little to no correlation, as this is where the most opportunity is to improve this benchmark.",
            "15": "Ultimately I view this as something that dilutes, but does not invalidate, the conclusions.",
            "16": "There is a correlation, and that correlation needs to be explored further, but unfortunately the authors don't dig into the link between KGC and KGQA much to explain this correlation.",
            "17": "2) Modeling techniques studied; the authors study two KGQA techniques, Pangu and DecAF.",
            "18": "While both exhibit strong performance, these are not the most natural nor most representative baseline choices.",
            "19": "Neither of these techniques are popular or widely cited; DecAF is somewhat popular and adopts an IR approach to KGQA, in which documents of triples are retrieved as evidence to inform answer generation, while Pangu is not and employs a neurosymbolic planner/critic pipeline.",
            "20": "This paper would be made more sound by the inclusion of more popular and representative techniques, especially those of the semantic parsing variety, of which there are many popular approaches (citations below).",
            "21": "The lack of semantic parsing baseline is concerning for a few reasons, including that 1) these are the most widely studied approach to KGQA and 2) semantic parsing provides a more direct means to assess KGC.",
            "22": "W/r/t 2) specifically, these approaches would enable the authors to measure the benefit of KGC on generated parse success rate.",
            "23": "For example, using a parsing approach trained on the complete KG and evaluated on the incomplete one would provide a realistic measurement of the impact of missing data, as parses that seek to return the obscured data would no longer be executable.",
            "24": "The authors should give more justification behind why they chose DecAF and Pangu, beyond just \"here are 2 SOTA techniques\".",
            "25": "While they discuss the lack of an LLM-only baseline in the limitation, they don't include semantic parsing.",
            "26": "@article{Ye2021RNGKBQAGA,\n  title={RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering},\n  author={Xi Ye and Semih Yavuz and Kazuma Hashimoto and Yingbo Zhou and Caiming Xiong},\n  journal={ArXiv},\n  year={2021},\n  volume={abs/2109.08678},\n  url={https://api.semanticscholar.org/CorpusID:237562927}\n}\n\n@inproceedings{Abdelaziz2021ASP,\n  title={A Semantic Parsing and Reasoning-Based Approach to Knowledge Base Question Answering},\n  author={I. Abdelaziz and Srinivas Ravishankar and Pavan Kapanipathi and Salim Roukos and Alexander G. Gray},\n  booktitle={AAAI Conference on Artificial Intelligence},\n  year={2021},\n  url={https://api.semanticscholar.org/CorpusID:235363625}\n}"
        },
        "2abDelRfFD": {
            "0": "- Interesting research area\n- Combination of results from different domains (QA, embedddings)\n - No major scientific contribution\n- No surprising insights\n- Results achieved on one dataset"
        }
    },
    "lCy3RwscMn": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method to decompose a complex task into simpler binary subtasks, which are then used to generate interpretable features.",
            "1": "This approach leverages the strengths of large language models (LLMs) and smaller transformer models to create a more interpretable and effective machine learning model.",
            "2": "**Interpretable Features**: The concept of Natural Language Learned Features (NLLF) is a significant contribution.",
            "3": "By using binary questions to generate these features, the authors provide a way to make the decision-making process of the model more transparent and interpretable.",
            "4": "**Versatility**: The method is demonstrated on two distinct tasks: detecting incoherence in students' answers and screening abstracts for systematic literature reviews.",
            "5": "This shows the versatility and generalizability of the approach.",
            "6": "**Performance**: The proposed method not only enhances the performance of classifiers but also allows for the use of interpretable models like decision trees.",
            "7": "The results show that the decision tree models using NLLF and expert features can outperform state-of-the-art LLMs in some cases.",
            "8": "**Explainability**: The paper emphasizes the importance of explainability in AI models, especially in high-stakes domains.",
            "9": "The use of decision trees with NLLF provides a clear and interpretable decision path, which is crucial for understanding and trusting AI decisions.",
            "10": "**Detailed Methodology**: The paper provides a comprehensive explanation of the methodology, including the generation of binary subtasks, zero-shot labeling, and the training of the NLLF generator.",
            "11": "This makes it easier for other researchers to replicate and build upon this work.",
            "12": "**Ethical Considerations**: The authors acknowledge the importance of ethical considerations in AI and aim to create models that are more interpretable and causal, aligning with the principles of responsible AI.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Complexity of Implementation**: While the methodology is detailed, the implementation involves multiple steps, including the generation of binary questions, zero-shot labeling, and training of a transformer model.",
            "15": "This complexity might be a barrier for practitioners who are not well-versed in these techniques.",
            "16": "**Dependence on LLMs**: The approach relies heavily on the capabilities of large language models for generating binary questions and weak labels.",
            "17": "This dependence might limit the applicability of the method in scenarios where access to powerful LLMs is restricted due to computational or financial constraints.",
            "18": "**Performance Variability**: The performance of the NLLF generator varies significantly between tasks.",
            "19": "For instance, the NLLF generator performs much better on the IAD task compared to the SAC task.",
            "20": "This variability suggests that the method might not be equally effective across all types of tasks.",
            "21": "**Feature Selection**: The feature selection process, while necessary, adds another layer of complexity.",
            "22": "The use of a genetic algorithm for feature selection might not be straightforward for all users and could be computationally expensive.",
            "23": "**Limited Evaluation**: The evaluation is limited to binary classification tasks.",
            "24": "It remains to be seen how well the approach scales to multi-class classification or more complex tasks like multi-hop reasoning.",
            "25": "**Manual Grouping of Questions**: The process of manually grouping similar binary questions to reduce redundancy could introduce subjectivity and might not be scalable for larger datasets.",
            "26": "**Potential for Bias**: The method of generating binary questions and weak labels using LLMs could introduce biases present in the LLMs themselves.",
            "27": "This could affect the fairness and reliability of the resulting models.",
            "28": "#### Conclusion\n\nOverall, the paper presents a promising approach to making AI models more interpretable by leveraging the strengths of LLMs and smaller transformer models.",
            "29": "The introduction of NLLF and the use of decision trees for interpretability are significant contributions.",
            "30": "However, the complexity of the implementation, dependence on LLMs, and variability in performance across tasks are areas that need further exploration and improvement.",
            "31": "Future work should focus on addressing these limitations and demonstrating the applicability of the method to a broader range of tasks and scenarios."
        },
        "qUsat9IDoL": {
            "0": "The idea is very novel.",
            "1": "The idea is very interesting in terms of decomposing tasks into sub-tasks.",
            "2": "The proposed method is somewhat complicated to apply to real-world scenarios.",
            "3": "While it presents innovative concepts and potential benefits, its implementation may pose challenges for practical use.",
            "4": "The intricate nature of the approach demands a high level of expertise and resources, which could limit its adoption in real-life applications."
        },
        "VRMtgQ5gt9": {
            "0": "It is an interesting work towards interpretable prediction.",
            "1": "The authors propose to utilize LLM to extract attributes and use the data generated by LLM to train BERT-like model to obtain Natural Language Learned Feature (NLLF).",
            "2": "NLLF and other man-crafted features are used in the decision tree which has more explainability than the black-box models.",
            "3": "The experimental results are promising which outperform the sota models (ChatGPT with/without CoT and BERT-like model).",
            "4": "- The authors only conduct experiments on 2 binary datasets: Scientific Abstract Classification (SAC) and Incoherent Answer Detection (IAD).",
            "5": "These two tasks are simple and, in my view, not high-stake enough.",
            "6": "It is still doubt that how the framework works on other difficult tasks.",
            "7": "- The paper introduces explainable AI in the introduction; however, the topic of the paper is interpretable methods.",
            "8": "The authors do not explain the definition of interpretability and do not explain the difference between these two concepts.",
            "9": "The authors also do not provide the work in this line.",
            "10": "- In the experiment, the only comparison model with explainability is ChatGPT with CoT.",
            "11": "It is better to see at least one more explainable model.",
            "12": "Also, self-ask* performs better in high-stake questions.",
            "13": "It would be better if there are some comparisons with self-ask.",
            "14": "*: Press, Ofir, et al.",
            "15": "\"Measuring and narrowing the compositionality gap in language models.\"",
            "16": "arXiv preprint arXiv:2210.03350 (2022)."
        },
        "WFVZhSYA70": {
            "0": "+ The idea of breaking the main task into intermediatory sub tasks is quite interesting and can be helpful for the further research.",
            "1": "+ The visualizations and diagrams are quite expressive.",
            "2": "- The experiments conducted didn't involve well-known baselines.",
            "3": "- The proposed algorithm also takes time and needs computational resources for huge datasets.",
            "4": "This point should be mentioned in the limitations section."
        }
    },
    "dFlGP1l65l": {
        "llm_review": {
            "0": "**Comprehensive Dataset and Scope:**\n   - The paper utilizes a large and diverse dataset of over 4 million news articles from 815 unique outlets, covering a period of nearly two years.",
            "1": "This extensive dataset allows for a robust analysis of media storms across a wide range of sources, including national, local, mainstream, and alternative news outlets.",
            "2": "**Innovative Methodology:**\n   - The authors develop a state-of-the-art news article similarity model using a bi-encoder MPNet approach, which is both efficient and effective for large-scale analysis.",
            "3": "The model's performance is validated against the SemEval news article similarity task, achieving near state-of-the-art results.",
            "4": "**Detailed Analysis of Media Storms:**\n   - The paper provides a thorough characterization of media storms, including their duration, topical distribution, and temporal dynamics.",
            "5": "The analysis confirms previous theoretical findings and offers new insights into the nature of media storms, such as their explosive rise and gradual decline in coverage.",
            "6": "**Mechanisms of Storm Development:**\n   - The study investigates two primary mechanisms behind media storms: gatekeeping and intermedia agenda setting.",
            "7": "The authors provide empirical evidence supporting these mechanisms, showing how media storms influence broader issue coverage and how different types of outlets interact in the news ecosystem.",
            "8": "**Publicly Available Resources:**\n   - The authors make their model, data, and replication code publicly available, which is a significant contribution to the research community.",
            "9": "This transparency allows other researchers to build on their work and conduct further studies on media storms and related phenomena.",
            "10": "**Weaknesses:**\n\n1.",
            "11": "**Reliance on Existing Datasets:**\n   - The study relies on three existing datasets (NELA-GT-2020, NELA-GT-2021, and NELA-Local), which may not fully capture the entire news ecosystem.",
            "12": "The completeness and representativeness of these datasets are not entirely clear, potentially limiting the generalizability of the findings.",
            "13": "**Scalability and Efficiency:**\n   - While the bi-encoder approach is efficient, the article clustering pipeline still requires significant computational resources.",
            "14": "The need to filter named entities and restrict comparisons to articles published within eight days may exclude some relevant articles and affect the accuracy of the clustering process.",
            "15": "**Causal Inference Limitations:**\n   - The study provides correlational evidence for the mechanisms of media storm development but does not establish causality.",
            "16": "Identifying the causal factors behind media storms remains a challenging task, and the paper does not fully address this limitation.",
            "17": "**Exclusion of Common Entities:**\n   - The exclusion of very common named entities (e.g., \"Biden,\" \"Trump\") from the similarity comparisons might lead to the omission of significant media storms related to these entities.",
            "18": "This decision, while necessary for computational reasons, could impact the comprehensiveness of the analysis.",
            "19": "**Topic Modeling Approach:**\n   - The use of Latent Dirichlet Allocation (LDA) for topic modeling, while effective, may not capture the full complexity of the topics covered in the news articles.",
            "20": "More advanced topic modeling techniques or a combination of methods could provide a deeper understanding of the topical distribution of media storms.",
            "21": "**Conclusion:**\n\nOverall, \"When it Rains, it Pours: Modeling Media Storms and the News Ecosystem\" is a well-executed and insightful study that makes significant contributions to the understanding of media storms.",
            "22": "The authors' innovative methodology, comprehensive dataset, and detailed analysis provide valuable insights into the dynamics of media coverage.",
            "23": "However, the study's reliance on existing datasets, scalability challenges, and limitations in causal inference highlight areas for future research.",
            "24": "The publicly available resources offered by the authors will undoubtedly facilitate further exploration and advancement in this important area of media studies."
        },
        "sd6I19M3HB": {
            "0": "- This paper is an explicit study on media storms.",
            "1": "I agree with the authors that the analysis of media storms is interesting and that the community can gain lots of insight from them.",
            "2": "- While the authors didn’t use a large amount of data to train their article similarity model, they ran it on a large number of articles to create a large dataset.",
            "3": "This dataset can be useful for future work to study media storms.",
            "4": "- The analysis done by the authors confirms what readers expect should happen in media storms.",
            "5": "However, the fact that this happens over the authors large analysis confirms that the dataset the authors are proposing is solid.",
            "6": "- The paper is well written and easy to follow.",
            "7": "- The main thing I would have liked to see in this paper is some results using the media storms data for a downstream task.",
            "8": "Some of these could also be tasks the authors propose, like identifying trends in future media storms based on historical media storms.",
            "9": "- Instead, this paper mostly just focuses on analyzing the storms (and some of the results are not surprising given the definition of media storms, like the fact that topic coverage increases).",
            "10": "It would be cool if the authors also use the storms data for other tasks, or at least discuss how it can be used.",
            "11": "Here are some examples I can think of:\n        - Can we predict when a media storm is about to happen?",
            "12": "When it’s going to end?",
            "13": "How long is it going to be?",
            "14": "- Can we predict which users are likely to be involved in a media storm?",
            "15": "Sources?",
            "16": "How does their perspectives change over time?",
            "17": "- Can we predict which outlets are likely to lead the coverage?",
            "18": "During a storm, can we determine this based on past storm data?",
            "19": "- Do some media storms cover more factual information and others spread more fake news?",
            "20": "When does each tend to happen?",
            "21": "- This can be useful if we know a media storm is happening, then maybe we should analyze the data around the media storm more carefully.",
            "22": "- Apart from the above, the main contribution of this paper is a dataset and an analysis of it, which in my opinion doesn't make the contribution of the paper extremely strong, thus my ambivalent Excitement score."
        },
        "Lume5E3gtr": {
            "0": "A. In-depth qualitative and quantitative analysis of media storms, what they are, their implications and how they come into being.",
            "1": "While popularity clusters and temporal evolution have been studied in depth with respect to social media and news coverage, outbursts are still significant aspects to study for understanding significance of events.",
            "2": "This study, dataset and modeling approach have the potential to serve as good, improved baselines for research of this kind.",
            "3": "By selecting the first 288 tokens and the last 96 tokens, how are the authors ensuring the true semantic meaning of the articles is being covered rather than context and linguistic fluff use to build up a news article and conclude it respectively?",
            "4": "A lot of articles consist of a premise and conclusion which can possibly consist of other related events or anecdotal pieces etc, that might not be relevant to the exact topic at hand.",
            "5": "Whereas, some articles are to the point.",
            "6": "When trying to pair such articles with a VERY high threshold of 0.9, there is a strong possibility of missed correlations and missed article linkup, which can possibly alter results (even if slightly).",
            "7": "It'll be good to have some more depth about the data in terms of average length of articles when data is truncated etc., and how these scenarios are being handled in the study."
        },
        "mRJciXj37D": {
            "0": "There is a lot of work done on understanding media storms which agrees with existing knowledge of media storms\n2.",
            "1": "There is a detailed recipe on how to extract the storms from the news articles 1.",
            "2": "There is no novel method\n2.",
            "3": "There is no novel conclusion or observation\n3.",
            "4": "Many conclusions are just one of the possible explanations - I suggest authors to focus on finding experiments to show that conclusion they think is more likely is the right conclusion"
        }
    },
    "I8VTNsq5eB": {
        "llm_review": {
            "0": "**Novel Framework:** The CESAR framework is a significant contribution to the field of dialog systems.",
            "1": "It modularizes dialog tasks and allows for the automatic generation of complex compositional instructions without manual effort.",
            "2": "This is a substantial advancement over previous methods that required manual creation of such tasks.",
            "3": "**Scalability:** The framework demonstrates scalability in generating rich instructions for dialog tasks.",
            "4": "The ability to programmatically induce complex tasks is a notable strength, as it reduces the need for extensive human labor and allows for the creation of a large number of compositional tasks.",
            "5": "**Comprehensive Benchmark:** The introduction of InstructDial++ is a valuable addition to the field.",
            "6": "It includes 63 datasets with 86 basic tasks and 68 composite tasks, providing a robust benchmark for evaluating dialog systems.",
            "7": "This comprehensive dataset can help in advancing the state-of-the-art in dialog systems.",
            "8": "**Experimental Rigor:** The paper includes rigorous experiments that demonstrate the effectiveness of the CESAR framework.",
            "9": "The results show that models trained on InstructDial++ outperform those trained on previous benchmarks, particularly in handling compositional tasks.",
            "10": "**Generalization:** The framework's ability to improve performance on both seen and unseen task compositions is impressive.",
            "11": "This indicates that CESAR can help models generalize better to new and complex instructions.",
            "12": "**Detailed Analysis:** The paper provides a detailed analysis of the performance of different models on atomic and compositional tasks.",
            "13": "This thorough evaluation helps in understanding the strengths and weaknesses of the proposed framework.",
            "14": "**Weaknesses:**\n\n1.",
            "15": "**Complexity of Implementation:** While the CESAR framework is powerful, its implementation might be complex for practitioners who are not well-versed in the intricacies of dialog systems.",
            "16": "The paper could benefit from providing more detailed guidelines or examples on how to implement the framework.",
            "17": "**Limited Scope of Datasets:** Although InstructDial++ includes a large number of datasets, the paper acknowledges that it is not comprehensive.",
            "18": "There are other datasets that could be included to further enhance the benchmark.",
            "19": "Future work could focus on expanding the dataset to include more diverse dialog scenarios.",
            "20": "**Evaluation Metrics:** The paper primarily uses accuracy and Rouge-L metrics for evaluation.",
            "21": "While these are standard metrics, they may not fully capture the nuances of dialog quality.",
            "22": "Incorporating more diverse evaluation metrics, such as human evaluations or other qualitative measures, could provide a more holistic assessment of the framework's performance.",
            "23": "**Negative Conditions:** The paper mentions that it has not explored negative conditions in compositional tasks.",
            "24": "This is an important aspect of dialog systems, as handling negative conditions can be challenging.",
            "25": "Future work could focus on incorporating and evaluating negative conditions to make the framework more robust.",
            "26": "**Automatic Metrics Limitations:** The paper acknowledges that automatic metrics are not necessarily robust.",
            "27": "This is a valid concern, as automatic metrics may not always align with human judgments.",
            "28": "Incorporating more human evaluations could help in addressing this limitation.",
            "29": "**Action Fields:** The paper notes that the action fields are mostly lexical and some are semantic.",
            "30": "A more comprehensive set of actions could improve the framework's ability to handle a wider range of dialog tasks.",
            "31": "Future work could focus on expanding the action fields to include more diverse and complex actions.",
            "32": "**Conclusion:**\n\nOverall, the CESAR framework is a significant advancement in the field of dialog systems.",
            "33": "It addresses the challenge of generating complex compositional instructions and provides a scalable solution that can improve the performance of dialog models.",
            "34": "The introduction of InstructDial++ as a comprehensive benchmark is a valuable contribution.",
            "35": "However, there are areas for improvement, such as expanding the dataset, incorporating more diverse evaluation metrics, and exploring negative conditions.",
            "36": "Despite these limitations, the CESAR framework represents a substantial step forward in the development of advanced dialog systems."
        },
        "9GtGslfs8T": {
            "0": "1) The paper introduces a novel framework, CESAR, which addresses the challenge of generating complex instructions for dialog tasks.",
            "1": "2) CESAR unifies a large number of dialog tasks in the same format and allows programmatic induction of complex instructions without any manual effort.",
            "2": "3) The paper enhances the InstructDial benchmark with new datasets and tasks and demonstrates the scalability of CESAR in providing rich instructions.",
            "3": "1) The paper lacks a detailed explanation of the CESAR framework, making it difficult for readers to fully understand the methodology.",
            "4": "2) The evaluation section could benefit from more detailed analysis and comparisons with existing methods or baselines.",
            "5": "3) The paper does not provide implementation details or code availability, which could hinder reproducibility."
        },
        "TWz8GkCHPD": {
            "0": "The paper proposed a new form to represent compositional tasks.",
            "1": "The paper created a new benchmark InstructDial++ to evaluate model's capability on compositional tasks.",
            "2": "Firstly, I suspect that the assumption of this paper is wrong.",
            "3": "Are open source models really less capable of handling compositional tasks?",
            "4": "I did not find any evaluation on current state-of-the-art Llama models in the paper.",
            "5": "Secondly, I saw in this paper (Table 7) that ChatGPT does not perform well on author's generated compositional task compared to the finetuned model (Table 5).",
            "6": "It is likely that the author created an \"artful\" dataset."
        },
        "Y7mNHijUIB": {
            "0": "The paper introduces a unique framework that merges various 1-D atomic tasks, I think the motivation is clear and sound.",
            "1": "The established InstructDial benchmark is expanded with an addition of 68 novel downstream tasks.",
            "2": "While the paper introduces 68 new downstream tasks based on seven 2-D Cesar tasks, it mainly offers overall performance.",
            "3": "This obscures the individual quality and contribution of each task.",
            "4": "There's a possibility that only a few high-quality tasks drive the improvements, while the majority add little value.",
            "5": "The methodological clarity regarding CESAR's task combination into coherent instruction prompts remains ambiguous.",
            "6": "It's essential to discern the reliability of these instructions and ensure task diversity, preventing overlapping or overly similar tasks.",
            "7": "The paper doesn't sufficiently differentiate itself from FLAN-T5, which already encompasses several dialogue tasks and also detailed task design.",
            "8": "It would great if the paper could discuss the differences.",
            "9": "The generalization experiments of CESAR-FLAN-xxl, initialized from FLAN-xxl, are somewhat undermined.",
            "10": "Given that FLAN-xxl already integrates around 400 datasets and over 1.5k tasks, and InstructDial also introduces several tasks for each dataset.",
            "11": "The paper's claim of \"unseen generalization\" isn't wholly convincing unless both datasets and tasks from FLAN-xxl and InstructDial are explicitly excluded from evaluation.",
            "12": "Besides, it may also leak data information for seen domains if different tasks come from the same datasets."
        },
        "BTcyqGtLuL": {
            "0": "The proposed unified framework presents a novel idea.",
            "1": "The motivation and details of the approach have been described in great detail.",
            "2": "Abundant experiments have validated the authors' method.",
            "3": "I haven't identified any significant shortcomings of the paper."
        },
        "HcIaomlZql": {
            "0": "The paper is generally well written and well structured.",
            "1": "Although the introduction and description of CESAR were somewhat challenging to follow due to many new terms being introduced, the accompanying tables and figures helped in following along.",
            "2": "The authors have identified a critical gap between performance of closed and open-source LLMs in task-oriented dialogues and clearly delineated their motivations with preliminary experiments to support their hypothesis that compositional instructions can help improve performance on complex dialog tasks.",
            "3": "The CESAR framework is well defined and designed to be scalable across different tasks within task-oriented dialogue.",
            "4": "They have explained how n-D tasks are defined, how discriminative tasks can be incorporated and how they ensure order-invariance while linearizing their inputs\n\nThe InstructDial++ dataset is a meaningful contribution (the authors have committed to sharing the dataset on Github) and results from Table 3 - 4 indicate that fine-tuning FLAN model on InstructDial++ shows performance gains over a FLAN model trained on InstructDial\n\nThe quantitative results indicate that CESAR-FLAN performs better than existing approaches on a good number of atomic tasks and seems to out-perform other approaches in compositional tasks.",
            "5": "One of the primary motivations of the paper is to bridge the gap between closed LLMs and open LLMs, hence there should have been more comparison and evaluation between CESAR and GPT-3.5-turbo in the main paper.",
            "6": "The authors have tabulated some results in the appendix section and they have carefully analyzed why the numbers might be deceiving, however it would be better to show several qualitative examples across CESAR and GPT-3.5-turbo and analyze trends in performance, it not clear whether the gap has been bridged or not\n\nIn L213 it is stated that scalable generation of compositional tasks is one aspect that CESAR aims to solve, however it is not clear in the paper how the compositional tasks are generated in a  scalable fashion using CESAR"
        }
    },
    "cOxL1tlSQw": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel approach to stance detection by focusing on dynamic interactions between messages and their replies, rather than the traditional static stance detection.",
            "1": "This is a significant contribution as it addresses the limitations of static stance detection, such as poor generalization capabilities and strong topic dependence.",
            "2": "**Comprehensive Annotation Scheme**: The authors present a well-defined annotation scheme for dynamic stance detection, which includes a rich set of labels to capture the nuances of online interactions.",
            "3": "This scheme is more granular and manageable, making it easier for annotators to apply consistently.",
            "4": "**Multilingual Corpus**: The creation of the Dynamic Stance Corpus (DySC) in two middle-resourced languages, Catalan and Dutch, is a valuable contribution.",
            "5": "It provides a resource for further research in stance detection in languages other than English, which is often underrepresented in NLP research.",
            "6": "**Empirical Analysis**: The paper provides a thorough empirical analysis of the differences between static and dynamic stance annotations.",
            "7": "This analysis highlights the limitations of static stance detection and demonstrates the advantages of the dynamic approach.",
            "8": "**Baseline Models and Experiments**: The authors fine-tuned several monolingual and multilingual models on the DySC corpus and conducted extensive experiments to investigate the portability of the trained models across topics and languages.",
            "9": "The results show that dynamic stance detection models exhibit better cross-topic generalization compared to static stance models.",
            "10": "**Cross-lingual Transfer**: The paper explores the cross-lingual transfer capabilities of the models, showing that combining training data from different languages can improve performance, especially for under-resourced languages.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Limited Scope of Topics**: While the paper covers multiple topics, the selection is still limited.",
            "13": "Expanding the range of topics could provide a more comprehensive evaluation of the dynamic stance detection approach.",
            "14": "**Annotation Challenges**: The paper acknowledges that dynamic stance annotation is more challenging than static stance annotation, as evidenced by the lower inter-annotator agreement scores.",
            "15": "This could impact the reliability of the annotations and the subsequent model performance.",
            "16": "**Model Performance**: Although the models show some learning in cross-topic scenarios, the performance is still relatively low, especially for the dynamic stance detection task.",
            "17": "This indicates that there is significant room for improvement in the models and the approach.",
            "18": "**Generalizability**: The study focuses on Catalan and Dutch, which are middle-resourced languages.",
            "19": "While this is a strength, it also raises questions about the generalizability of the findings to other languages, especially low-resourced or high-resourced languages.",
            "20": "**Complexity of Annotation Scheme**: The dynamic stance annotation scheme, while comprehensive, may be too complex for practical applications.",
            "21": "Simplifying the scheme without losing essential information could make it more accessible for broader use.",
            "22": "**Limited Use of Fine-grained Classes**: The paper mentions that not all fine-grained classes identified for annotation were used in the experiments due to the relatively few instances.",
            "23": "This could limit the understanding of the full potential of the dynamic stance detection approach.",
            "24": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the field of stance detection by introducing a dynamic approach that captures the interactions between messages and their replies.",
            "25": "The creation of the DySC corpus in Catalan and Dutch is a valuable resource, and the empirical analysis provides important insights into the differences between static and dynamic stance detection.",
            "26": "However, there are challenges related to the complexity of the annotation scheme, the limited scope of topics, and the model performance that need to be addressed in future work.",
            "27": "Expanding the range of topics, simplifying the annotation scheme, and improving model performance through advanced techniques could enhance the applicability and impact of this research."
        },
        "Jj4OWEUPbR": {
            "0": "The paper presents a new way of modeling the tasks of stance detection dynamically instead of statically.",
            "1": "I see this as a promising new way of framing the task.",
            "2": "Together with the new task, the paper presents a new resource in Catalan and Dutch annotated for static and dynamic stance detection.",
            "3": "This is a rich new dataset that allows for analyzing static and dynamic stance annotations.",
            "4": "The paper presents initial experiments with language models that give valuable first insights into the generalizability of the two annotation frameworks.",
            "5": "I did not see real reasons to reject the paper.",
            "6": "There are, however, a couple of points that should be clarified: \n\nAnnotation analysis: I am really not so sure about the comparison of static versus dynamic stance annotations and their consequences for the data (Section 4.2).",
            "7": "The annotations resulting from both annotation frameworks are considered side by side and it is discussed what aspects of a debate they can capture.",
            "8": "My problem with this comparison is that the two annotation frameworks essentially capture a different task; one (static) assesses a text with respect to a claim or topic (predefined, explicit) while the other (dynamic) assesses a text with respect to another text.",
            "9": "In the latter, the claim in the first text remains entirely implicit.",
            "10": "While the overall topic of a discussion may be related to the topic of covid vaccines, the first text may contain a claim that is entirely unrelated.",
            "11": "This is not really an insight that arises from the data.",
            "12": "Rather, it is inherent to the design of the two different frameworks.",
            "13": "I think it is valid to show the consequences of these differences in the data, but I think it is important to be clear about the fact that the frameworks have different goals and thus capture different information.",
            "14": "Update after rebuttal: Thank you for the clarification.",
            "15": "I am confident that the authors can make the necessary changes in the final version of the paper.",
            "16": "Automatic stance detection: Please clarify whether the train and test splits for dynamic and static stance detection are the same.",
            "17": "I understand that there is a difference in the number of instances (and labels) due to the different annotation frameworks, but right now, it is not clear whether the texts are the same.",
            "18": "It would be great to have a bit more information about topic overlap between train and text in the ‘all topics experiments’.",
            "19": "Update after rebuttal: Thank you for the clarification!",
            "20": "Topic independence is a huge plus of the dynamic way of modeling stance.",
            "21": "I think it would be good to make this (and its consequence for automatic approaches) more prominent in the framing of the paper.",
            "22": "Minor remark: I don’t think that Dutch can really be called a low-resource language with respect to NLP resources.",
            "23": "‘Low resource’ may be true with respect to stance resources, but not in terms of general NLP resources (see for instance Joshi et al.",
            "24": "2020).",
            "25": "Perhaps check this.",
            "26": "Reference\nJoshi, P., Santy, S., Budhiraja, A., Bali, K. and Choudhury, M., 2020, July.",
            "27": "The State and Fate of Linguistic Diversity and Inclusion in the NLP World.",
            "28": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp.",
            "29": "6282-6293)."
        },
        "6IZtpD28UH": {
            "0": "- Useful resource: The paper presents an annotated dataset in two languages and on several topics, while ensuring cross-lingual comparability.",
            "1": "- Baseline results: The paper presents basic experiments, including on cross-topic and cross-lingual transfer, that will be useful as a reference for future work.",
            "2": "Unfortunately, the presentation of the research questions and the results is not very clear, and especially the relation to “static” stance detection remains unclear: Does this paper propose an improvement over previous approaches, or does it propose an entirely new task?",
            "3": "- Unclear presentation: The experiments with automatic stance detection (Section 5) focus on a comparison between “Static Training” and “Dynamic Training”.",
            "4": "However, the two concepts are not formally defined.",
            "5": "Is “Dynamic Training” multi-task learning on dynamic and static stance detection?",
            "6": "If so, the use of different subsets used in the experiments (L496–501) make it difficult to see whether this objective has a positive effect.",
            "7": "Or is “Dynamic Training” just learning to classify the relation to the parent text, independent of the static stance?",
            "8": "In that case, it is not clear in what respect this approach “address[es] the limitations of traditional Static Stance detection” (L569), since it would appear to be a new task.",
            "9": "- Claim not supported by example: A main claim of the paper (L420f.)",
            "10": "is that static stance and dynamic stance are not equal: Two statements can have the same stance w.r.t a topic but still disagree with each other.",
            "11": "While Table 4 provides quantitative support for this claim, Example 1 (L437–446) does not seem to be a valid illustration.",
            "12": "In fact, Reply (b) seems to be critical of the COVID-19 vaccination campaign, and not in favor.",
            "13": "I would encourage the authors to check the example and make sure it does not contain an annotation error."
        },
        "nSuGYP1bXp": {
            "0": "There is merit in the fact that the authors annotate both static and dynamic stance in order to show the relations between the two levels of analysis.",
            "1": "I also like the crosslinguistic approach in Catalan and Dutch.",
            "2": "The paper has a couple of gaps that I address below in the more detailed comments.",
            "3": "Overall I think this is rather an LREC paper, given its focus."
        }
    },
    "jQozdfjJSZ": {
        "llm_review": {
            "0": "**Innovative Dataset Creation:**\n   - The authors have created a large-scale, multi-modal dataset, MingOfficial, which includes both structured career records and textual historical data for 13,031 officials from the Ming Dynasty.",
            "1": "This dataset is a significant contribution to the field of Chinese historical studies and digital humanities, providing a rich resource for researchers.",
            "2": "**Integration of Multi-Modal Data:**\n   - The integration of structured data (career records) and unstructured data (historical texts) using a Graph Neural Network (GNN) is a novel approach.",
            "3": "This allows for a more comprehensive analysis of historical figures by combining different types of information.",
            "4": "**Graph Neural Network Application:**\n   - The use of GNNs to learn representations of officials and their social structures is a strong methodological choice.",
            "5": "The GNN-based model effectively captures the nuanced relations between officials, which is demonstrated by the significant improvement in downstream tasks.",
            "6": "**Performance Improvement:**\n   - The experimental results show a substantial improvement in identifying nuanced identities, such as civil officials holding military power, with an F1 score increase from 24.6% to 98.2%.",
            "7": "This highlights the effectiveness of the proposed approach.",
            "8": "**Public Availability:**\n   - Making the MingOfficial dataset publicly available is a commendable step.",
            "9": "It encourages further research and collaboration in the field, potentially leading to new insights and advancements.",
            "10": "**Comprehensive Analysis:**\n   - The paper provides a thorough analysis of the dataset and the proposed method.",
            "11": "The exploratory analysis of the dataset and the detailed error analysis add depth to the study, demonstrating the practical implications and limitations of the approach.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Comparative Analysis:**\n   - The paper lacks a detailed comparative analysis with other potential methods for identifying target groups.",
            "14": "While the authors mention the scarcity of related work, a more extensive comparison with existing approaches in similar domains could strengthen the validation of their method.",
            "15": "**Interpretability of GNN Models:**\n   - The interpretability of the GNN models is limited.",
            "16": "The paper does not provide insights into how the model makes predictions based on the data.",
            "17": "Understanding which career records or official relations significantly contribute to the model’s predictions would be valuable for researchers.",
            "18": "**Data Quality and Scope:**\n   - The quality and scope of the data are potential limitations.",
            "19": "The dataset might contain inaccuracies and omissions, which could affect the results.",
            "20": "Additionally, the data is confined to the Ming Dynasty, limiting the generalizability of the findings to other historical periods or cultures.",
            "21": "**Scalability:**\n   - The scalability of the method remains untested.",
            "22": "While the approach is effective for the present dataset, its performance with significantly larger datasets or different time frames is uncertain.",
            "23": "**Assumptions and Limitations:**\n   - The study relies on certain assumptions, such as the completeness of the database and textual records.",
            "24": "If these assumptions fail, the method might overlook key aspects influencing an official’s acquisition of military power.",
            "25": "Future work should address these limitations by testing the method on more comprehensive and diverse datasets.",
            "26": "**Complexity of Annotation Process:**\n   - The annotation process for determining civil officials’ military power is resource-intensive and requires deep domain knowledge.",
            "27": "This complexity might limit the scalability and applicability of the approach to other datasets or historical contexts.",
            "28": "**Conclusion:**\n\nThe paper presents a significant contribution to the field of Chinese historical studies and digital humanities by introducing the MingOfficial dataset and a novel GNN-based representation learning framework.",
            "29": "The integration of multi-modal data and the substantial improvement in identifying nuanced identities demonstrate the effectiveness of the proposed approach.",
            "30": "However, the study has limitations related to comparative analysis, interpretability, data quality, scalability, and the complexity of the annotation process.",
            "31": "Addressing these limitations in future work could further enhance the impact and applicability of the research."
        },
        "2d8E8fVYAo": {
            "0": "The paper addresses a fascinating and essential topic in the field of Chinese studies.",
            "1": "This topic is of significant interest to scholars and researchers in history and linguistics.",
            "2": "The paper provides sufficient information to reproduce the experimental results.",
            "3": "The paper's reliance on only one dataset could be perceived as a potential limitation, as it may hinder the ability to establish the universality or general applicability of the proposed method.",
            "4": "The proposed method appears to be primarily built upon existing research, and it lacks sufficient innovation."
        },
        "4in4T7vcWt": {
            "0": "+ representation learning of historical people is an interesting research question\n+ the MingOfficial dataset could be a useful resource - Is the base language model used to encode historical text actually trained on ancient mandarin?",
            "1": "How did the authors evaluate this adopted language model?",
            "2": "- Why is military power selected as a special attribute for representation learning?",
            "3": "It would be nice to have an ablation study removing this learning objective or adding other attributes as part of the learning signal.",
            "4": "- There is no Figure 11 in the paper, but line 434 indicates otherwise.",
            "5": "In addition, there is a ?",
            "6": "?",
            "7": "on line 436.",
            "8": "- I suggest having an ethics statement to discuss relevant concerns if any.",
            "9": "Also, the limitations section is incomplete on line 629."
        },
        "jHqpofWNOy": {
            "0": "This paper will publish a new dataset containing Ming dynasty officials’ career records, annotated personnel types, and related historical texts, which can benefit the following research work in this field.",
            "1": "This work introduced how to process and annotate the proposed dataset, which offered a guideline to make annotations for other similar tasks.",
            "2": "Also, the exploratory analysis can help understand this dataset better.",
            "3": "The authors provided a graph construction method to present Ming dynasty’s complex political landscape, which can be also referred to in other data-driven Chinese historical research studies.",
            "4": "The authors conducted extensive experiments to evaluate the proposed GNN-based framework.",
            "5": "This manuscript needs to be carefully revised in several aspects, including completing the discussion of limitations and addressing various spelling errors (see Typos Grammar Style below).",
            "6": "From the experiment results, it can be found that incorporating coP-P and simP-P always negatively impacted the performance.",
            "7": "So, I think there is a need to find a more effective way to fuse these two views of P-P interactions.",
            "8": "The experiments were conducted on only one dataset.",
            "9": "It would be better if the evaluation of the proposed framework could be performed on more datasets.",
            "10": "The scale of the introduced dataset is relatively small, with a more limited number of annotated data available.",
            "11": "However, I understand the difficulty to build such an interesting dataset."
        }
    },
    "fL8AKDvELp": {
        "llm_review": {
            "0": "**Innovative Approach**: The introduction of HyperRouter is a novel approach that balances between fixed and trainable routers in Sparse Mixture of Experts (SMoE) training.",
            "1": "This method addresses the representation collapse issue effectively while maintaining efficiency.",
            "2": "**Comprehensive Experiments**: The paper provides extensive experimental results across various tasks and datasets, demonstrating the superior performance and efficiency gains of HyperRouter compared to existing methods.",
            "3": "The experiments are well-detailed and cover both pre-training and fine-tuning scenarios.",
            "4": "**Scalability**: The paper shows that HyperRouter scales well with larger models, maintaining its performance advantages.",
            "5": "This is crucial for practical applications where model size and complexity are continually increasing.",
            "6": "**Detailed Analysis**: The paper includes a thorough analysis of the representation capabilities of HyperRouter compared to other methods.",
            "7": "The mathematical derivation of the Jacobians provides a deep understanding of why HyperRouter performs better.",
            "8": "**Public Implementation**: The authors have made their implementation publicly available, which is a significant contribution to the research community.",
            "9": "This allows for reproducibility and further exploration by other researchers.",
            "10": "**Efficiency in Inference**: The paper highlights that HyperRouter achieves the same performance threshold with fewer experts during inference, significantly enhancing the efficiency of deploying large language models (LLMs) in real-world applications.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Limited Dataset Scope**: While the experiments are comprehensive, they are conducted on medium-scale datasets and relatively small TransformerXL models due to computational constraints.",
            "13": "This limits the generalizability of the results to larger datasets and more complex models.",
            "14": "**Resource Constraints**: The paper mentions that the large TransformerXL model was only trained for 100K iterations due to resource limitations.",
            "15": "This might not fully showcase the potential of HyperRouter in large-scale settings where more extensive training could yield different results.",
            "16": "**Complexity of HyperNetwork**: The introduction of a fixed hypernetwork and trainable embeddings adds complexity to the model.",
            "17": "While the paper demonstrates the benefits, it would be helpful to have a more detailed discussion on the trade-offs in terms of computational overhead and implementation complexity.",
            "18": "**Router Analysis**: The paper provides an analysis of the router's entropy, but it would benefit from a more in-depth exploration of how different initialization and training strategies for the hypernetwork and embeddings affect the overall performance.",
            "19": "**Future Work**: While the paper opens several promising venues for future research, it could provide more concrete directions or preliminary results on potential improvements, such as incorporating regularization techniques or sharing hypernetworks among layers.",
            "20": "**Ethical Considerations**: The paper briefly mentions the ethical implications of training large-scale LLMs, such as computational costs and potential biases in the data.",
            "21": "However, a more detailed discussion on how HyperRouter could mitigate these issues or any specific steps taken in this work would strengthen the ethical considerations.",
            "22": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the training and inference of Sparse Mixture of Experts models through the introduction of HyperRouter.",
            "23": "The innovative approach, comprehensive experiments, and detailed analysis make it a valuable contribution to the field.",
            "24": "However, addressing the limitations related to dataset scope, resource constraints, and complexity would further strengthen the work.",
            "25": "The public implementation and discussion on future research directions are commendable and provide a solid foundation for continued exploration in this area."
        },
        "MRZU3GNVAk": {
            "0": "The results based on a small scale Transformer-XL on both pretraining and finetuning tasks are very convincing.",
            "1": "It shows for pretraining tasks and during inference, HyperRouter achieves the same performance as competitors with half the number of experts, warrant its advantage for LLM deployment.",
            "2": "It also shows for finetuning tasks, there is consistent accuracy gains over competitors on all datasets.",
            "3": "The additional experiment results about the inference FLOPs, parameter comparison, and entropy analysis and routing visualization in the appendix add more strength to the analysis.",
            "4": "They provide extra angles to help understand why the proposed approach is advantageous.",
            "5": "All experiment results are based on a small-scale transformer-XL.",
            "6": "Based on the configuration in the paper, the model has about 1.8M parameters.",
            "7": "The SMoE-Droput paper reported results on BERT base (110M parameters), RoBERTa base (125M), and Transformer-XL (18M).",
            "8": "So the reader of this paper may not have a clear picture of what the performance gains and inference advantages are on large and medium scale transformers (LLMs).",
            "9": "Will the conclusions generalize to larger-scale LLMs or the gain will diminish as it scales?",
            "10": "We need more empirical evidence.",
            "11": "[Update] The authors addressed the two questions with proper empirical evidence and explanation in the rebuttal."
        },
        "PSjLbwupWX": {
            "0": "- HyperRouter introduces a novel approach that balances fixed and trainable routers in SMoE training, enhancing the routing policy and mitigating representation collapse issues, resulting in more efficient and effective large language model training.",
            "1": "- The authors conduct experiments on both pre-training and fine-tuning for various datasets and note that HyperRouter:\na) Outperforms both SMoE and SMoE-Dropout (in the pre-training regime)\nb) Significantly outperforms SMoE-Dropout when using only one expert (in the pre-training regime)\nc) Performs competitively with SMoE275 Dropout while only using half of the experts (in the pre-training regime)\nd) In the fine-tuning regime- it outperforms all the other strategies on SST-2 and IMDB datasets with only 8 experts\n\n - The experiments in the paper have a narrow scope since the authors use only one kind of transformer architecture.",
            "2": "The results are are not very conclusive for future research to build on.",
            "3": "- I'm not sure if utilizing just 4 layers (out of 18) from TransformerXL would accurately predict the model's behavior.",
            "4": "The authors might have opted for a smaller model for directly translating the results."
        },
        "xbaLgNzseo": {
            "0": "The authors propose an interesting approach that facilitates dynamic router parameter generation.",
            "1": "The evaluation demonstrates the potential and performance improvement of the proposed approach.",
            "2": "The paper needs a comprehensive analysis of sparse MoE, including the communication overhead (all to all).",
            "3": "Currently, it's not clear where the performance gain comes from, basically, different number of experts incurs different communication overhead.",
            "4": "The evaluation needs experiments on distributed deployment and a larger model.",
            "5": "For the arguments that the existing approach has two key limitations, the authors should present key experiment results for demonstration."
        }
    },
    "kgxtMJHe7w": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method called selective labeling, which significantly reduces the cost of data labeling for document extraction models.",
            "1": "This approach is particularly valuable given the high costs associated with traditional labeling methods.",
            "2": "**Simplified Annotation Task**: By transforming the labeling task into a binary \"yes/no\" question, the authors have made the annotation process faster and less cognitively demanding for human annotators.",
            "3": "This simplification is a practical and effective way to reduce labeling time and errors.",
            "4": "**Active Learning Integration**: The combination of selective labeling with a custom active learning strategy is a strong point.",
            "5": "The method focuses on the most uncertain predictions, which ensures that the labeling effort is concentrated where it is most needed.",
            "6": "**Empirical Validation**: The paper provides thorough experimental validation across three different domains (Supply Chain, Retail Finance, and Tax Forms).",
            "7": "The results demonstrate that selective labeling can achieve nearly the same accuracy as models trained on fully labeled datasets but at a fraction of the cost.",
            "8": "**Detailed Analysis**: The authors offer a comprehensive analysis of various aspects of their approach, including selection metrics, sampling methodologies, and the impact of the initial labeled dataset size.",
            "9": "This detailed examination helps in understanding the robustness and applicability of the proposed method.",
            "10": "**Practical Implications**: The proposed method has significant practical implications for businesses that rely on document extraction models.",
            "11": "By reducing the labeling cost by an order of magnitude, the approach makes it feasible to develop high-quality extraction models for a wide range of document types.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Dependence on Candidate Generators**: The approach relies heavily on the quality of candidate generators.",
            "14": "If the candidate generation step has low recall, the overall performance of the extraction model can be limited.",
            "15": "The paper acknowledges this but does not provide a solution for improving candidate generation.",
            "16": "**Limited Scope of Models**: The method is primarily validated on a specific type of extraction model that uses candidate generation and binary classification.",
            "17": "While the authors mention the potential for adapting the method to sequence labeling models, this adaptation is not explored in the paper.",
            "18": "**Score Calibration Complexity**: The need for score calibration to handle uncalibrated model predictions adds complexity to the approach.",
            "19": "While the paper provides a solution, it might be challenging to implement in practice, especially for non-expert users.",
            "20": "**Proprietary Datasets**: The datasets used for validation are proprietary and not publicly available.",
            "21": "This limits the ability of other researchers to reproduce the results and validate the approach on different datasets.",
            "22": "**Initial Dataset Size Sensitivity**: The performance of the selective labeling method is sensitive to the size of the initial labeled dataset.",
            "23": "While the method shows significant improvements even with small initial datasets, the performance gains are less pronounced when the initial dataset is very small.",
            "24": "**Future Work and Limitations**: The paper outlines several areas for future work, such as adapting the method to sequence labeling models and exploring more complex annotation tasks.",
            "25": "However, these are not addressed in the current study, leaving some questions about the generalizability and scalability of the approach.",
            "26": "#### Conclusion:\n\nOverall, the paper presents a compelling and practical approach to reducing data-labeling costs for document extraction models.",
            "27": "The selective labeling method, combined with active learning, offers a significant reduction in annotation costs while maintaining high accuracy.",
            "28": "The detailed empirical validation and analysis strengthen the credibility of the approach.",
            "29": "However, the dependence on candidate generators, the complexity of score calibration, and the use of proprietary datasets are notable limitations.",
            "30": "Future work addressing these issues and exploring the adaptation to other model types would further enhance the impact and applicability of the proposed method."
        },
        "jZZCX9I7at": {
            "0": "Evaluation of two step annotation consisting of selection of candidate from extracted results and annotating labels of the selected candidates.",
            "1": "In named entity recognition, which is categorized as sequential labeling problem as in document extraction of this paper, similar problems have been studied.However, they are not mentioned.",
            "2": "Some of them are listed in the missing reference section.",
            "3": "The reviewer feels that the combination of existing methods including the missing reference ones and active learning consist of this paper's method."
        },
        "1Syk1gHgXP": {
            "0": "- **Simple and sound method:** The proposed method is straight forwarded and easily appliable.",
            "1": "- **Extensive evaluation:** The authors deeply investigated the behaviour of the proposed method under multiple circumstances, which revealed insightful results.",
            "2": "- **Felt there are details missing:** Although, the focus is on the SL method itself and not on the extractive models, it would be beneficial if the paper becomes more self-contained by briefly addressing the candidate generator and scorer model used during the experiments, instead of providing only a brief glimpse.",
            "3": "Additionally, exploring the synergy of different models and the proposed SL method could add an interesting perspective to the paper.",
            "4": "- **Missing limitation:** After reading the paper, I felt that there was one limitation that was not addressed.",
            "5": "From what I understood, if a rare field is not present in the initial set of documents, then it would be impossible for the extraction model to generate candidates for that missing field, meaning that field would never receive an annotation.",
            "6": "- **Hard reproducibility and no codebase:** The paper's reproducibility is challenging due to the private nature of the chosen datasets (as correctly mentioned in the limitation section).",
            "7": "To overcome this, conducting additional experiments on public datasets would enhance the broader dissemination of the work.",
            "8": "Moreover, even though the datasets are private, releasing the codebase could facilitate a larger adoption of the method and encourage benchmarking on other datasets."
        },
        "lsC358HyiD": {
            "0": "This paper focuses on a practical problem concerning information extraction from visually rich documents while addressing cost issues with training and human annotations.",
            "1": "Although it does not provide a SOTA model, it provides a novel combination to address the stated research problem.",
            "2": "While the authors acknowledge its limitations on dataset release, it would benefit from additional details regarding dataset creation and comparison with other baseline datasets.",
            "3": "Furthermore, a comparison with other state-of-the-art models would strengthen the paper's credibility and impact."
        }
    },
    "g4FAvRcSuf": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel self-supervised method for training behavior cloning transformers in text games.",
            "1": "This approach auto-generates training data by exploring trajectories that lead to rewards, which is a significant advancement over traditional supervised methods that require human-generated playthroughs.",
            "2": "**Empirical Validation**: The authors provide thorough empirical analysis across three benchmark text games, demonstrating that their method achieves about 90% of the performance of supervised systems.",
            "3": "This is a strong validation of the effectiveness of their approach.",
            "4": "**Efficiency**: The method is computationally efficient, as it iteratively explores only the most promising paths, reducing the search space and training costs.",
            "5": "This is particularly important for large action spaces and complex environments.",
            "6": "**Generality and Utility**: The paper emphasizes the importance of generalizable training data.",
            "7": "By evaluating the performance of small models on unseen development games, the authors ensure that the training data is not only useful but also generalizable across different game variations.",
            "8": "**Detailed Methodology**: The paper provides a clear and detailed description of the methodology, including path crawling, path grouping, and incremental path crawling.",
            "9": "This makes it easier for other researchers to replicate and build upon this work.",
            "10": "**Comparison with Baselines**: The authors compare their method with several baselines, including a supervised behavior cloning transformer, a DRRN reinforcement learning model, and GPT-4.",
            "11": "This provides a comprehensive context for evaluating the performance of their approach.",
            "12": "**Open Source**: The authors have released their code as open source, which is a commendable practice that promotes transparency and reproducibility in research.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Limited Scope of Benchmarks**: While the paper demonstrates the effectiveness of the method on three benchmark text games, it would be beneficial to see its performance on a wider variety of games or more complex environments.",
            "15": "This would help in understanding the scalability and robustness of the approach.",
            "16": "**Sparse Rewards and Large Action Spaces**: The paper acknowledges the limitations of the method in environments with extremely sparse rewards or very large action spaces.",
            "17": "However, it does not provide a clear strategy for addressing these challenges.",
            "18": "Future work could focus on improving the method's performance in such scenarios.",
            "19": "**Overfitting Risk**: The method of merging multiple solution paths by choosing the highest-performing trajectories on the development set may lead to overfitting.",
            "20": "The paper mentions this risk but does not provide a detailed analysis or alternative strategies to mitigate it.",
            "21": "**Dependence on Pretrained Models**: The approach relies heavily on the capabilities of pretrained language models like T5.",
            "22": "While this is a common practice, it may limit the applicability of the method to scenarios where such models are not available or practical.",
            "23": "**Evaluation Metrics**: The paper primarily uses game score and the number of steps to evaluate performance.",
            "24": "While these are important metrics, additional evaluation criteria such as the diversity of generated solutions or the robustness of the model to different types of errors could provide a more comprehensive assessment.",
            "25": "**Error Analysis**: The paper provides some error analysis, particularly for the GPT-4 baseline in the sorting game.",
            "26": "However, a more detailed error analysis for the self-supervised model could help in understanding its limitations and areas for improvement.",
            "27": "**Human Supervision for Bias**: The paper suggests that self-supervised training data should be manually inspected for bias in tasks that could produce harmful outcomes.",
            "28": "This introduces a dependency on human supervision, which the method aims to minimize.",
            "29": "Developing automated techniques for bias detection could be an area for future research.",
            "30": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of self-supervised learning for text games.",
            "31": "The proposed method is innovative, efficient, and demonstrates strong empirical performance.",
            "32": "However, there are areas for improvement, particularly in handling sparse rewards, large action spaces, and mitigating overfitting risks.",
            "33": "Expanding the scope of benchmarks and providing more detailed error analysis could further strengthen the work.",
            "34": "Despite these limitations, the paper makes a valuable contribution to the field and opens up new avenues for research in self-supervised behavior cloning transformers."
        },
        "SkyUGCfgTw": {
            "0": "The method is simple and intuitive, almost like a beam search of trajectories and shows solid empirical results on all three games.",
            "1": "The writing is clear and easy to follow.",
            "2": "The proposed technique is also general enough to be applied to more complicated game settings with tweaks.",
            "3": "On concern with Table 1 is that the self-supervised method is only compared to an oracle i.e.",
            "4": "the supervised model, and two baselines to which it is not directly comparable since the way these are trained are so different i.e.",
            "5": "DRRN or GPT4.",
            "6": "It would be significantly improved by including a baseline that ablates the effect of the data creation using the iterative method i.e.",
            "7": "a random path crawler that generates an equal number of synthetic paths which is then used to train the agent.",
            "8": "This would show the effect of how helpful it is to perform the iterative selection step."
        },
        "AA1TuT43NQ": {
            "0": "* The text games are interesting benchmarks that mark the capability of language models solving real-world problems.",
            "1": "* The proposed method is intuitive and easy to understand.",
            "2": "This might potentially increase the impact of this work.",
            "3": "* The generalization of this method might be poor as it heavily relies on the forms of games.",
            "4": "Specifically, the authors set the grouping heuristic as the actions in each text game.",
            "5": "However, this practice is not generalizable as it is not likely to expect all text games to fit this heuristic.",
            "6": "Therefore, the proposed method might be limited to a small set of tasks.",
            "7": "* The description of the method is not clear or formal.",
            "8": "In Section 2, the authors illustrate their method mainly by figures and examples.",
            "9": "Although it might suffice to describe their implementation, it is unclear how readers should adapt this method to their test cases in a principled way.",
            "10": "* Lack of important baselines.",
            "11": "The GPT-4 baseline is only evaluated in a zero-shot fashion.",
            "12": "However, in recent years, the community has developed multiple ways to properly prompt the model.",
            "13": "The authors should evaluate these recent techniques to make sure the baseline is proper."
        },
        "XU6cyvoBVU": {
            "0": "The paper is well-written and the method clear.",
            "1": "Despite not performing as well as GPT-4 on some tasks, the T5-based model is substantially smaller and approaches the performance ceiling defined by supervised behavior cloning approaches.",
            "2": "To the best of my knowledge, the paper uses reasonable baselines and provides a clear description of tradeoffs between models.",
            "3": "The paper’s primary contribution seems unlikely to generalize beyond text games: in particular, searching over all possible actions (up to the initial reward) is only possible because language in text games is highly restricted (and we assume access to the set of valid actions).",
            "4": "Additionally, path grouping based on generalization seems to work only because the tasks evaluated in this paper are limited in their complexity, such that many instances of a task can be solved with the same sequence of macro-level actions.",
            "5": "Furthermore, intermediate rewards will not exist in most real-world tasks, limiting the usefulness of incremental path crawling.",
            "6": "**In response to the rebuttal**: I do not believe that these criticisms violate the *CL reviewing guidelines as the authors state.",
            "7": "The motivation behind text games is to \"simulate complex natural language problems in controllable settings\" (Osborne, et al.",
            "8": "2022, cited in the author response).",
            "9": "As a result, it is not at all unreasonable to suggest that methods for text games should (at least attempt to) generalize to the real-world domains which they are intended to simulate.",
            "10": "After reading the author response, I have chosen to keep my score unchanged."
        }
    },
    "OgK0kMz5Va": {
        "llm_review": {
            "0": "**Novelty and Relevance**: The paper addresses a significant gap in the application of Vision-Language Models (VLMs) like CLIP for zero-shot species recognition.",
            "1": "This is a relevant and timely problem, especially for fields like ecology and biodiversity research where labeled data is scarce and expensive to obtain.",
            "2": "**Clear Motivation**: The authors provide a clear and compelling motivation for their work.",
            "3": "They highlight the practical importance of zero-shot species recognition and the limitations of existing methods that use scientific names in prompts.",
            "4": "**Simple and Effective Solution**: The proposed method of translating scientific names to common English names is straightforward yet highly effective.",
            "5": "This simplicity makes the approach easy to implement and understand, which is a significant advantage.",
            "6": "**Comprehensive Evaluation**: The paper includes extensive experiments on multiple datasets (iNat, Aves, Flowers102, and CUB200), demonstrating the effectiveness of the proposed method across different species types.",
            "7": "The results are presented clearly, showing significant improvements in accuracy.",
            "8": "**Insightful Analysis**: The authors provide a detailed analysis of why scientific names perform poorly and how common names improve performance.",
            "9": "They also investigate the frequency of names in the training set of OpenCLIP, which adds depth to their findings.",
            "10": "**Practical Contributions**: The paper offers practical contributions by providing a method that can be easily adopted by researchers and practitioners working on species recognition.",
            "11": "The approach does not require additional labeled data or complex modifications to existing models.",
            "12": "#### Weaknesses:\n\n1.",
            "13": "**Limited Scope of Descriptions**: While the paper mentions using descriptions generated by large-language models (LLMs) like GPT-4, it does not explore this aspect in depth.",
            "14": "The marginal gains from descriptions suggest that there might be room for improvement or alternative approaches that could be investigated.",
            "15": "**Dependence on External Resources**: The method relies on external resources like online collections and museums to translate scientific names to common names.",
            "16": "This dependency might limit the applicability of the approach in cases where such resources are not readily available or comprehensive.",
            "17": "**Generalization to Other Domains**: The paper focuses specifically on species recognition.",
            "18": "It would be beneficial to discuss the potential generalization of the proposed method to other specialized domains where scientific or technical terms are used.",
            "19": "**Ethical Considerations**: The paper briefly mentions potential biases in VLMs and LLMs but does not delve into the ethical implications of using these models for species recognition.",
            "20": "A more thorough discussion on this topic would strengthen the paper.",
            "21": "**Lack of Qualitative Results**: While the paper provides quantitative results, it lacks qualitative examples that could help readers better understand the practical impact of the proposed method.",
            "22": "Including visual examples of correctly and incorrectly classified species could enhance the presentation.",
            "23": "**Future Work**: The paper could benefit from a more detailed discussion on future work.",
            "24": "For instance, exploring how to improve the generation of descriptions or integrating additional contextual information could be promising directions.",
            "25": "#### Conclusion:\n\nOverall, \"Prompting Scientific Names for Zero-Shot Species Recognition\" is a well-written and impactful paper that addresses a significant problem in the field of species recognition.",
            "26": "The proposed method is simple yet effective, and the comprehensive evaluation demonstrates its potential.",
            "27": "While there are some areas for improvement, particularly in the exploration of descriptions and ethical considerations, the paper makes a valuable contribution to the field and provides a solid foundation for future research."
        },
        "49SLUeyD5y": {
            "0": "This paper presents a method to facilitate the conversion of scientific names into universal names  for the purpose of enhancing the accuracy of zero sample species recognition using Vision Transformer (ViT) models.",
            "1": "The proposed method is relatively simple yet effective.",
            "2": "While the researchers implemented a method for translating text during the model inference stage, the proposed approach is relatively simplistic and does not appear to introduce any new or innovative techniques or algorithms.",
            "3": "Additionally, the research fails to conduct a horizontal comparison of the prompt template, which could provide valuable insights into the efficacy and accuracy of the underlying model."
        },
        "E2BKcSmbtG": {
            "0": "The proposed idea is very simple and it is validated on 4 benchmark datasets by consistently outperforming the baselines.",
            "1": "The idea of using VLMs for fine-grained datasets is a bold move and can open-up an interesting research venue.",
            "2": "In real-world scenario with hundreds of species from the same genus or family, I wonder how this method will generalize.",
            "3": "To illustrate, there are more than 2000 beetles and not many of them has common names.",
            "4": "One of the reasons that the model works well on CUB dataset is that the CUB dataset is not very fine-grained.",
            "5": "Most of bird species are well known ones and have common names.",
            "6": "There are most 4-5 species belonging to the same genus in that dataset.",
            "7": "No ZSL methods are tested as a baseline.",
            "8": "It would be very beneficial for the paper to show how the proposed method (VLM + translated class names) performs against some of traditional ZSL methods [1, 2, 3, 4]\n\n\n\n[1] Xian, Y., Lampert, C. H., Schiele, B., & Akata, Z.",
            "9": "(2018).",
            "10": "Zero- shot learning— A comprehensive evaluation of the good, the bad and the ugly.",
            "11": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(9), 2251– 2265.",
            "12": "[2] E. Schonfeld, S. Ebrahimi, S. Sinha, T. Darrel, and Z. Akata.",
            "13": "Generalized zero- and few-shot\nlearning via aligned variational autoencoders.",
            "14": "In CVPR, 2019.",
            "15": "[3] S Badirli, Z Akata, G Mohler, C Picard, M Dundar.",
            "16": "Fine-Grained Zero-Shot Learning with DNA as Side Information.",
            "17": "In NeurIPS, 2021.",
            "18": "[4] B. Romera-Paredes and P. H. Torr.",
            "19": "An embarrassingly simple approach to zero-shot learning.In ICML, 2015."
        },
        "30vJrcKg8N": {
            "0": "Interesting finding backed by great improvement over the state of the art.",
            "1": "The technique itself is \"embarrasingly\" simple as per the authors own description.",
            "2": "While that is not a knock against it, the authors do not dig deeper into what the implications of their result are.",
            "3": "It is not shocking that CLIP would be trained with data that does not have scientific names in it.",
            "4": "However, scientific names are assigned so there are unambiguous names for species.",
            "5": "Common names on the other hand are often ambiguous and downright confusing.",
            "6": "For example, prairie dogs are rodents while dogs are canines, Similarly guinea pigs are not porcine at all.",
            "7": "A lion is a feline but a sea lion is a mariine mammal.",
            "8": "The authors are satisfied with the improvements in the results that they have achieved but do not probe for cases where there may not be a bijective mapping between scientific and common names.",
            "9": "update: The authors have provided a convincing response to this question."
        }
    },
    "XkexLrJDss": {
        "llm_review": {
            "0": "**Innovative Application of LLMs**: The paper explores a novel application of large language models (LLMs) like GPT-4 in generating code-tracing questions for introductory programming courses.",
            "1": "This is a significant contribution to both the fields of computer science education and natural language processing (NLP).",
            "2": "**Comprehensive Evaluation**: The authors have established a set of human evaluation metrics to assess the quality of questions produced by the model compared to those created by human experts.",
            "3": "This thorough evaluation provides a clear comparison and highlights the strengths and weaknesses of LLM-generated questions.",
            "4": "**Dataset Contribution**: The creation and sharing of a unique dataset of human and LLM-generated tracing questions is a valuable resource for the education and NLP research communities.",
            "5": "This dataset can be used for further research and development in the field.",
            "6": "**Detailed Analysis**: The paper provides a detailed analysis of the capabilities and potential of LLMs in generating diverse code-tracing questions.",
            "7": "The use of various metrics like BERTScore, BLEU, and ROUGE to evaluate the diversity and quality of the questions is commendable.",
            "8": "**Human Evaluation**: The involvement of expert evaluators in assessing the quality of the questions adds credibility to the findings.",
            "9": "The confusion matrix and the analysis of expert perceptions provide valuable insights into the discernibility between human and AI-generated questions.",
            "10": "**Ethical Considerations**: The paper includes an ethical statement, highlighting the ethical considerations taken into account during the research.",
            "11": "This is an important aspect, especially when dealing with educational tools and AI.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Scope of Models**: The study primarily focuses on GPT-4, with some comparisons to GPT-3.5-turbo.",
            "14": "While this provides valuable insights, the generalizability of the findings to other LLMs like CodeT5+ is not addressed.",
            "15": "Future work could benefit from a broader range of models.",
            "16": "**Lack of Real-World Testing**: The study does not involve actual students in the evaluation of the LLM-generated questions.",
            "17": "While expert evaluations are valuable, real-world testing with students would provide a more comprehensive understanding of the educational efficacy of the generated questions.",
            "18": "**Few-Shot vs. Zero-Shot Analysis**: The paper discusses the differences between few-shot and zero-shot generation but does not delve deeply into the reasons behind the observed biases in few-shot generation.",
            "19": "A more detailed analysis of these biases and potential solutions would be beneficial.",
            "20": "**Personalization of Questions**: The study does not explore the personalization of tracing questions based on individual student submissions.",
            "21": "Personalization could significantly enhance the learning experience, and this aspect warrants further investigation.",
            "22": "**Dataset Diversity**: While the dataset is a valuable contribution, it is primarily sourced from a single online Java course and a few YouTube videos.",
            "23": "Expanding the dataset to include a wider variety of sources and programming languages would improve the robustness and applicability of the findings.",
            "24": "**Evaluation Criteria**: The evaluation criteria used by the expert evaluators are well-defined, but the paper could benefit from a more detailed discussion on the rationale behind the chosen criteria and how they align with educational objectives.",
            "25": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the field by exploring the potential of LLMs in generating code-tracing questions for introductory programming courses.",
            "26": "The strengths of the paper lie in its innovative application, comprehensive evaluation, and detailed analysis.",
            "27": "However, there are areas for improvement, such as expanding the scope of models, involving real-world testing, and exploring personalization.",
            "28": "Addressing these weaknesses in future work would further enhance the impact and applicability of the research."
        },
        "5ZzXWVvlUI": {
            "0": "This paper has several strengths that would make it a valuable contribution to the NLP community:\n\n- Novel application area: This is the first work exploring LLMs for code tracing question generation, an important educational application distinct from prior NLP work.",
            "1": "Expanding LLMs to this new domain is an interesting direction.",
            "2": "- Rigorous human evaluation: The paper introduces a thoughtful evaluation methodology including multiple human ratings and textual similarity metrics.",
            "3": "This provides a model for human evaluation of LLM-generated content.",
            "4": "- High-quality dataset: The human question dataset compiled is a unique asset that can enable further research.",
            "5": "Releasing this data would be very beneficial.",
            "6": "- Insightful analysis: The paper provides a nuanced analysis around the promise but also limitations of LLMs for this application.",
            "7": "The insights into quality, diversity, and discernibility of LLM questions are impactful.",
            "8": "- Interdisciplinary relevance: This work sits at the intersection of NLP, education, and human-AI collaboration.",
            "9": "Demonstrating the potential of LLMs for enhancing education would be appealing to a broad audience.",
            "10": "- Practical implications: With further development, the proposed techniques could lead to LLM integration in educational platforms and classrooms.",
            "11": "This could significantly aid programming education and learning.",
            "12": "Presenting this work would expand the NLP community's understanding of LLMs' capabilities on a non-traditional task.",
            "13": "It would also highlight an interesting new application domain with practical relevance.",
            "14": "The novel human evaluation framework and high-quality dataset are additional assets that could catalyze further work in this direction.",
            "15": "Overall, this is a timely exploration that would be well-suited for an NLP venue.",
            "16": "Some potential weaknesses and risks to consider:\n\n- Small dataset size: The dataset has only 176 question pairs from limited sources.",
            "17": "A larger, more diverse dataset could strengthen the evaluation and generalizability of findings.",
            "18": "- Limited scope: The focus is narrow - only on intro Java programming tracing questions.",
            "19": "Testing breadth by targeting other domains/question types would be informative."
        },
        "jCWdoU2Je1": {
            "0": "- The code-tracing scenario proposed by this paper is novel.",
            "1": "- A dataset has been constructed for this scenario.",
            "2": "- An exploration of LLM's application in this scenario.",
            "3": "- The metrics has been established to evaluate the quality of the generated code-tracing questions.",
            "4": "- The study only considered GPT-4 and has not incorporated other models.",
            "5": "- There is no personalized question recommendation based on different people's submissions, which shows a lack of thorough exploration of the scenario.",
            "6": "- The study lacks attempts to use this approach in practical testing scenarios.",
            "7": "- The metrics for evaluating code-tracing questions are not comprehensive enough.",
            "8": "- The setting of the metrics is somewhat subjective and lacks an automatic evaluation."
        },
        "2A43cCjeB2": {
            "0": "* The work well demonstrates how to prompt LLMs for a series of code-tracing questions when given a snippet of codes.",
            "1": "* When the generated data is released, it may bring benefits to the field of programming education and dialogue systems.",
            "2": "* The experiment results show the potential and limitations of the LLMs-generated questions for code-tracing."
        }
    },
    "6RuXWFEQzg": {
        "llm_review": {
            "0": "**Real-world Alignment**: The paper addresses a significant gap in the current OpenRE methods by aligning the problem setting with real-world data characteristics.",
            "1": "This includes handling unlabeled data that encompasses known, novel, and negative instances, and considering long-tail relation types for novel classes.",
            "2": "This makes the proposed method more practical and applicable to real-world scenarios.",
            "3": "**Innovative Methodology**: The introduction of KNoRD (Known and Novel Relation Discovery) is a notable contribution.",
            "4": "The method effectively classifies explicitly and implicitly expressed relations from known and novel classes within unlabeled data.",
            "5": "The use of constrained and unconstrained predictions to model explicit and implicit relationships is particularly innovative.",
            "6": "**Comprehensive Evaluation**: The experimental evaluations are thorough, covering several Open-world RE benchmarks.",
            "7": "The results demonstrate that KNoRD consistently outperforms existing methods, achieving significant gains.",
            "8": "This provides strong evidence of the effectiveness of the proposed approach.",
            "9": "**Detailed Problem Statement and Methodology**: The paper provides a clear and detailed problem statement and methodology.",
            "10": "The four discrete stages of KNoRD are well-explained, making it easier for readers to understand and potentially replicate the study.",
            "11": "**Open Source Code**: The authors have openly provided all code, experimental settings, and datasets used in the study.",
            "12": "This transparency is commendable and facilitates further research and validation by the community.",
            "13": "**Ablation Studies**: The inclusion of ablation studies helps in understanding the relative importance of each design choice behind KNoRD.",
            "14": "This adds depth to the evaluation and provides insights into the components that contribute most to the model's performance.",
            "15": "#### Weaknesses\n\n1.",
            "16": "**Dependence on Human-annotated Data**: The method requires human-annotated data, which is expensive and time-consuming to create.",
            "17": "This limitation is acknowledged by the authors, but it remains a significant barrier to the widespread adoption of the method.",
            "18": "**Fixed Number of Novel Classes**: The method cannot automatically determine the ground truth number of novel classes in unlabeled data.",
            "19": "The authors set the number of novel classes to twice the number of known classes, which may not always be optimal.",
            "20": "Developing an automated method for class abstraction is left for future work.",
            "21": "**Performance with Negative Instances**: The low F1 scores of the model and all leading OpenRE models within experiments with negative instances highlight an area for growth.",
            "22": "Discovering novel classes among instances with no relation is particularly challenging, and the current methods struggle with this task.",
            "23": "**Generalization to Document-level RE**: The method focuses on sentence-level relation classification.",
            "24": "Without further testing, it is unclear how well these methods would work for document-level relation classification.",
            "25": "This limits the generalizability of the findings.",
            "26": "**GPT 3.5 Performance**: The performance of GPT 3.5 in this setting is underwhelming, even with in-context learning and cosine similarity mapping.",
            "27": "This suggests that more advanced techniques are required to enable GPT 3.5 to accurately classify and discover relationships from textual data.",
            "28": "The paper could have explored more sophisticated prompting techniques to potentially improve GPT 3.5's performance.",
            "29": "**Complexity of Implementation**: The proposed method involves multiple stages and complex processes, such as prompt-based training, clustering with majority-vote bifurcation, and cross-entropy training with weak labels.",
            "30": "This complexity might pose challenges for practitioners looking to implement the method in real-world applications.",
            "31": "#### Conclusion\n\nOverall, the paper makes significant contributions to the field of OpenRE by proposing a more realistic problem setting and introducing an innovative method, KNoRD, that effectively addresses the challenges of this setting.",
            "32": "The comprehensive evaluation and open-source code further enhance the impact of the work.",
            "33": "However, the dependence on human-annotated data, the fixed number of novel classes, and the challenges with negative instances and document-level RE are notable limitations that need to be addressed in future research."
        },
        "f10ejH5fwz": {
            "0": "The issues and settings that this paper focuses on are important and meaningful.",
            "1": "This paper is well-written.",
            "2": "The experiments are sufficient and combined with large models such as ChatGPT for analysis.",
            "3": "My main concern with this paper is the Generalized Relation Discovery setting it claims to propose, which is actually not the first time the authors have proposed it, and has been proposed in earlier work such as ARD.",
            "4": "Unfortunately, I can not see enough comparison and elaboration of ARD in this paper."
        },
        "3vpUBjBvgJ": {
            "0": "The paper is well written.",
            "1": "The technical quality is good, with a method well designed and well presented.",
            "2": "The results are quite promising, with outperformance over the baselines, on three datasets (TA-CREAD, ReTACRED, and FewRel).",
            "3": "The assumption of no distribution shift is a bit strong.",
            "4": "It's likely that known relation classes found in labeled data do not appear in the unlabeled data.",
            "5": "Could you justify the construction of novel classes in 385 - 389?",
            "6": "E.g., why the ratio 15% is used?",
            "7": "Is there any previous standard setting on the evaluation of this problem?"
        },
        "mapmsb4qel": {
            "0": "The paper is well-written and easy to understand.",
            "1": "The assumptions and claims for building the KNoRD model are clearly stated.",
            "2": "It's laudable that the motivation behind creating the KNoRD model is to handle the realistic real-world data setting.",
            "3": "The KNoRD model has the potential to generate weak data labels, and it would be interesting to use it for synthetic data creation.",
            "4": "The KNoRD method works only on sentence inputs, can't easily be extended to document-level RE, and requires human-annotated labeled data.",
            "5": "There are various hyperparameters (top-3 tokens for relation representation, P% for weak labels in clustering, known cluster size, etc.)",
            "6": "used throughout the KNoRD modeling, which could make it hard to reproduce the results or even apply to a new unlabeled dataset."
        }
    },
    "TnpFFjHCcw": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method for conversational semantic parsing by leveraging dynamic context graphs.",
            "1": "This approach dynamically creates subgraphs for each user utterance, which is a significant improvement over static methods that may not capture the evolving context of a conversation effectively.",
            "2": "**Graph Neural Networks**: The use of graph neural networks (GNNs) to encode the subgraphs is a strong point.",
            "3": "GNNs are well-suited for capturing the structural information of graphs, and their application here allows the model to handle a large number of unseen nodes efficiently.",
            "4": "**Context-Dependent Type Linking**: The introduction of context-dependent type linking is a notable contribution.",
            "5": "This method helps in disambiguating types based on the surrounding context, which is crucial for accurate semantic parsing in conversations.",
            "6": "**Comprehensive Evaluation**: The experimental evaluation on the SPICE dataset is thorough.",
            "7": "The paper reports results on various question types and provides a detailed analysis of the model's performance on different discourse phenomena such as ellipsis and coreference.",
            "8": "**Performance Improvements**: The proposed model, DCG CL, shows significant performance improvements over the baseline model (BertSP GL) across multiple metrics, including F1 score and exact match accuracy.",
            "9": "This demonstrates the effectiveness of the dynamic context modeling approach.",
            "10": "**Scalability**: The paper addresses the scalability issue by not relying on encoding the entire knowledge graph (KG) in memory.",
            "11": "Instead, it dynamically extracts relevant subgraphs, making the approach more feasible for large-scale KGs.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Dependency on Pre-trained NER**: The model relies on an off-the-shelf named entity recognition (NER) system for entity linking.",
            "14": "This dependency could lead to error propagation if the NER system fails to recognize entities correctly.",
            "15": "Jointly learning the NER and semantic parsing tasks could potentially mitigate this issue.",
            "16": "**Limited Exploration of Relation Information**: The paper does not explicitly consider relations in the KG.",
            "17": "While the focus is on entities and types, incorporating relation information could further enhance the model's ability to generate accurate SPARQL queries.",
            "18": "**Complexity of Implementation**: The proposed approach involves multiple components, including BERT for encoding utterances, GNNs for encoding subgraphs, and a transformer decoder.",
            "19": "This complexity might make the implementation and training process more challenging compared to simpler models.",
            "20": "**Handling of Long Conversations**: While the model shows improvements in handling longer interactions, the performance still drops as the conversation length increases.",
            "21": "Further work is needed to better manage long-range dependencies and maintain context over extended dialogues.",
            "22": "**Error Analysis**: The error analysis provided in the paper is somewhat limited.",
            "23": "While it identifies some common sources of errors, a more detailed breakdown of specific failure cases and potential solutions would be beneficial.",
            "24": "**Generalization to Unseen Domains**: The paper does not extensively discuss the model's ability to generalize to unseen domains within Wikidata.",
            "25": "Evaluating the model's performance on a broader range of domains would provide a better understanding of its robustness.",
            "26": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of conversational semantic parsing by introducing dynamic context graphs and leveraging graph neural networks.",
            "27": "The proposed approach shows substantial improvements over existing methods and addresses several key challenges in the task.",
            "28": "However, there are areas for further improvement, particularly in handling long conversations, incorporating relation information, and reducing dependency on pre-trained NER systems.",
            "29": "Despite these weaknesses, the contributions of the paper are valuable and pave the way for future research in this area."
        },
        "2haOejygW1": {
            "0": "(1) This paper is easy to understand\n(2) The experiment results are strong In the era of foundation model, it is beneficial to check the performance of ChatGPT."
        },
        "eLHyS0FaQ0": {
            "0": "•\tThe strengths of the paper are in the good description of the model, detailed analysis of the results, reasonable experiment structure and baselines.",
            "1": "•\tResults are presented well with a detailed ablation study; The ablation analysis and case studies provide additional insight into the effectiveness of the approach.",
            "2": "•\tCode and data are going to be released\n•\tIt presents a novel approach with strong performance, and clear motivation.",
            "3": "•\tThis is a very well written paper, coherent and easy to understand.",
            "4": "•\tArchitecture of the model is well-defined in figure 2.",
            "5": "•\tOverall, this is an effective paper with strong results •Most of the improvement is coming from disambiguation based on popularity of an entity in training data.",
            "6": "More experiments are needed to see the generalizability of this approach on new dataset and unknown entities"
        },
        "UIQCR3zvwv": {
            "0": "- An interesting approach which builds upon previous work and introduces dynamic context graph for semantic parsing\n- Extended comparison and analysis with BertSP - Analysis and comparison with similar model like LasagneSP is missing.",
            "1": "-  The model lacks novel design"
        }
    },
    "KEH6Cqjdw2": {
        "llm_review": {
            "0": "**Novel Perspective and Task Definition:**\n   - The paper introduces a new perspective on hate speech detection by grounding it in legally enforceable definitions.",
            "1": "This is a significant step forward as it aligns the detection system with the goals of regulators and legal standards, which is crucial for real-world applications.",
            "2": "**Comprehensive Dataset:**\n   - The authors have curated a dataset annotated by legal experts based on violations of eleven possible legal definitions.",
            "3": "This dataset is enriched with expert-generated samples and an automatically mined challenge set, which enhances its robustness and applicability.",
            "4": "**Methodological Rigor:**\n   - The paper employs a variety of machine learning methods, including zero-shot and few-shot prompting, and experiments with several large language models (LLMs).",
            "5": "This comprehensive approach provides a solid foundation for evaluating the effectiveness of different models in this context.",
            "6": "**Empirical Baselines:**\n   - The authors establish empirical baselines using state-of-the-art LLMs, which is valuable for future research.",
            "7": "The inclusion of parameter-efficient tuning and self-training methods further strengthens the study.",
            "8": "**Legal and Ethical Considerations:**\n   - The paper addresses the importance of aligning hate speech detection with legal definitions, which is crucial for ensuring that the outputs are relevant and enforceable.",
            "9": "This focus on legal enforceability is a significant strength.",
            "10": "**Detailed Analysis and Discussion:**\n   - The paper provides a thorough analysis of the results, including the performance of different models and the quality of their reasoning.",
            "11": "This detailed discussion helps in understanding the strengths and limitations of the proposed approach.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Scope of Legal Definitions:**\n   - While the paper focuses on Canadian and some international legal definitions, it does not consider definitions from other significant jurisdictions like the European Union comprehensively.",
            "14": "This limits the generalizability of the findings.",
            "15": "**Annotation Challenges:**\n   - The paper acknowledges the inherent subjectivity in hate speech detection and the challenges in obtaining consistent annotations.",
            "16": "However, it does not provide a detailed analysis of the inter-annotator agreement or the potential biases introduced by the annotators.",
            "17": "**Model Performance and Hallucinations:**\n   - The paper reports that even the best-performing models, like GPT-4, exhibit hallucinations and lack robust reasoning capabilities.",
            "18": "This is a significant limitation, especially for high-stakes tasks like legal hate speech detection.",
            "19": "**Data Augmentation and Quality:**\n   - The paper uses automatic data augmentation to increase the dataset size, but the quality of these augmented samples is not thoroughly evaluated.",
            "20": "There is a risk that noisy or low-quality data could impact the model's performance.",
            "21": "**Real-World Applicability:**\n   - While the paper discusses potential applications, it does not provide a concrete implementation or evaluation of the system in real-world scenarios.",
            "22": "This limits the practical insights that can be drawn from the study.",
            "23": "**Ethical and Misuse Considerations:**\n   - The paper briefly touches on the ethical implications and potential misuse of the system but does not provide a comprehensive analysis.",
            "24": "Given the high stakes involved, a more detailed discussion on mitigating misuse and ensuring ethical deployment would be beneficial.",
            "25": "**Conclusion:**\n\nOverall, the paper makes a significant contribution to the field of hate speech detection by introducing a legally enforceable framework and providing a comprehensive dataset annotated by legal experts.",
            "26": "The methodological rigor and detailed analysis are commendable.",
            "27": "However, there are limitations related to the scope of legal definitions, annotation challenges, model performance, data quality, real-world applicability, and ethical considerations.",
            "28": "Addressing these weaknesses in future work would further strengthen the impact and applicability of the proposed approach."
        },
        "6ML4dihyPQ": {
            "0": "* The built dataset will be useful for researchers to evaluate classifiers\nfollowing hate speech definitions based on legally enforceable definitions.",
            "1": "* The paper also presents a useful benchmark for comparison of future systems,\nwhich includes various models, model setups (zero-shot, tuning) and evaluation\nsetups.",
            "2": "Additionally, the presented experiments provide a useful analysis on the\nperformance and behaviour of LLMs on legally enforceable hate speech detection.",
            "3": "* The writing of the paper is high quality and detailed.",
            "4": "* I did not find any significant reasons for rejecting the paper.",
            "5": "Please see\nbelow for some of my concerns/issues."
        },
        "2c29qtLCmm": {
            "0": "The paper introduces a unique perspective on hate speech detection, which focuses on grounding the task in legally enforceable definitions.",
            "1": "The release of a gold label dataset annotated by legal experts is a substantial contribution to the research community.",
            "2": "Such a dataset can serve as a reliable benchmark for evaluating and comparing hate speech detection systems.",
            "3": "The paper primarily focuses on aligning hate speech detection with legal definitions.",
            "4": "It may lack a broader analysis of other aspects related to hate speech and its impact."
        },
        "w71JGVEYU5": {
            "0": "- Extend hate speech with prompt templating\n- zero-shot classification for legally defined hate speech with many LLMs\n- Fine0tuned models for legally enforceable hate speech\n- Model result explanation - How do we extend the approaches to other (countries' legal documents)?",
            "1": "- Data collection and annotation are not clear\n- The Enforceable Annotation might have ethical issues.",
            "2": "What will be the reward for the 10 law experts?",
            "3": "why did they volunteer?",
            "4": "Does it count toward their study (credit), or will they co-author the paper?",
            "5": "This is a serious issue, which might lead to the low quality of the data.",
            "6": "- The concept of \"editing samples\" is not clear\n- Majority voting from 11 definitions is not clear?",
            "7": "- You could compare your result with SoTA approaches, for example with HateXplain models."
        }
    },
    "GQ1rtVVIy2": {
        "llm_review": {
            "0": "**Novelty and Relevance:** The paper addresses a novel and relevant problem in the field of mental health and natural language processing (NLP) by attempting to identify early maladaptive schemas (EMS) from mental health question texts.",
            "1": "This is particularly important given the increasing use of community QA forums for mental health support.",
            "2": "**Methodological Diversity:** The authors explore both Large Language Models (LLMs) and non-LLM approaches for EMS identification.",
            "3": "This comprehensive approach allows for a thorough comparison of different methodologies and their effectiveness in this context.",
            "4": "**Use of Schema Therapy Framework:** The paper leverages the well-established Schema Therapy framework, specifically Young’s Schema Questionnaire (YSQ), to ground the identification of EMS.",
            "5": "This provides a solid theoretical foundation for the study and ensures that the methods are aligned with clinical practices.",
            "6": "**Expert Annotation:** The dataset used for evaluation is annotated by qualified counselors with practical experience in Schema Therapy.",
            "7": "This ensures the quality and reliability of the annotations, which is crucial for the validity of the study.",
            "8": "**Detailed Evaluation:** The paper provides a detailed evaluation of the methods, including standard performance measures (precision, recall, F1) and additional metrics to assess the handling of null cases.",
            "9": "This thorough evaluation helps in understanding the strengths and limitations of each approach.",
            "10": "**Open Source Contribution:** The authors have made their code and data available for academic purposes, which promotes transparency and allows other researchers to build upon their work.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Small Dataset:** The dataset used for evaluation is relatively small, with only 30 questions annotated by experts.",
            "13": "While this is a good starting point, the small sample size limits the generalizability of the findings.",
            "14": "Larger datasets would be needed to validate the results more robustly.",
            "15": "**LLM Sensitivity to Prompts:** The paper highlights the sensitivity of LLMs to specific prompt texts, which can lead to inconsistent predictions.",
            "16": "This is a known issue with prompt-based approaches, but the paper could benefit from a more in-depth discussion on how to mitigate this problem.",
            "17": "**Explainability of LLM Predictions:** While LLMs show promising performance, their predictions lack explainability compared to non-LLM methods.",
            "18": "The paper acknowledges this but does not provide concrete solutions or future directions to improve the interpretability of LLM outputs.",
            "19": "**Handling of Null Cases:** Both LLM and non-LLM methods struggle with reliably addressing null cases (questions with no EMS labels).",
            "20": "The paper suggests that combining the two approaches might help, but this is not empirically tested or demonstrated.",
            "21": "**Limited Scope of Evaluation:** The evaluation is limited to a specific set of mental health topics (anxiety, depression, trauma, self-esteem, and anger-management).",
            "22": "It would be beneficial to test the methods on a broader range of mental health issues to assess their generalizability.",
            "23": "**Potential Ethical Concerns:** The paper briefly mentions ethical considerations but does not delve deeply into the potential risks and ethical implications of using automated methods for mental health support.",
            "24": "This is a critical area that requires careful consideration, especially given the sensitive nature of mental health data.",
            "25": "**Conclusion:**\n\nOverall, the paper makes a significant contribution to the field by addressing a novel problem and exploring multiple methodologies for EMS identification.",
            "26": "The use of Schema Therapy as a theoretical framework and the detailed evaluation are commendable.",
            "27": "However, the study is limited by the small dataset, sensitivity of LLMs to prompts, and the lack of explainability in LLM predictions.",
            "28": "Future work should focus on addressing these limitations, expanding the scope of evaluation, and exploring ethical implications in greater depth."
        },
        "WFCmKs0LUI": {
            "0": "The paper introduces an interesting problem of Early Maladaptive Schema Identification in mental health texts.",
            "1": "Evaluated various non-LLMs and LLM-based methods and offered a thorough analysis of the potential and limitations of different techniques for EMS label prediction.",
            "2": "The paper maintains a formal tone and technical language appropriately.",
            "3": "However, some sections could be further polished for clarity, particularly in conveying the details of complex methodologies.",
            "4": "The methodology section introduces three distinct approaches logically, but it lacks detailed explanations for certain techniques.",
            "5": "The methodology section of the paper introduces various approaches for predicting early maladaptive schemas (EMS) labels but falls short in several critical aspects.",
            "6": "The description of the Similarity-based Voting Predictor (SVP) is insufficiently detailed.",
            "7": "The exact process of computing candidate EMS labels based on sentence similarity using multiple sentence transformer models is unclear.",
            "8": "The Entailment-based Prediction model (EPM) is introduced briefly but lacks clarity in its application.",
            "9": "The section does not elaborate on how entailment is determined between YSQ statements and question sentences."
        },
        "7Sd1Nil4gp": {
            "0": ">Describing two methods for predicting EMS labels through a novel application of sentence similarity and textual entailment.",
            "1": ">Providing an evaluation of the methods using LLMs and non-LLMs \n>Constructing a small EMS dataset.",
            "2": ">The overall novelty of this paper is limited.",
            "3": "The proposed SVP method exists.",
            "4": "In addition, the EMS prediction task is similar to depressive symptoms detection tasks.",
            "5": "Therefore, there are many related methods leveraging semantic information, such as:\nZhang, Z., Chen, S., Wu, M., & Zhu, K. Q.",
            "6": "(2022a).",
            "7": "Psychiatric scale guided risky post screening for early detection of depression.",
            "8": "arXiv preprint arXiv:2205.09497.",
            "9": "Zhang, Z., Chen, S., Wu, M., & Zhu, K. (2022b).",
            "10": "Symptom identification for interpretable detection of multiple mental disorders on social media.",
            "11": "In Proceedings of the 2022 conference on empirical methods in natural language processing (pp.",
            "12": "9970–9985).",
            "13": "In addition, although the LLMs' method and evaluation are useful for the EMS prediction task, there are no interesting findings, and the results can not help justify the lack of interpretability.",
            "14": "The authors could add a chain of thought to learn more about the performance of LLMs.",
            "15": ">Some contents need to be included and need to be more clearly described: The specific structure and usage of the EPM method.",
            "16": "Which F1 (micro, macro, average) has been used?",
            "17": "The label distribution of the dataset (imbalanced or balanced)?",
            "18": ">Some experiments should be added, such as ablation studies about non-LLMs-based models.",
            "19": ">The Fleiss Kappa is low.",
            "20": "What is the reason, are two annotators enough, and is the guideline set reasonable?"
        },
        "7mwAHx8Wja": {
            "0": "* A new idea to apply a concept from schema theory in psychology to NLP texts- the work can be beneficial to other clinical NLP researchers and be impactful for studying triage.",
            "1": "* The paper is well-written and easy to understand.",
            "2": "* Many obvious experiments come to mind given its a first attempt at understanding the concept in language domain -- for example, the models could be finetuned for the methods, or a model trained on mental health datasets specifically could be used.",
            "3": "Studies could be done across multiple models to check for inherent knowledge of identifying EMS.",
            "4": "The choice of models is not well-motivated either.",
            "5": "* Consistency and Explainability: although its a section, the discussion on non-LLM is limited to one line and doesn't have any specific experiments designed for either consistency or explainability.",
            "6": "* The work is small and limited in its applications, I would like to see more experiments and settings for a thorough evaluation.",
            "7": "The work in its current form might be better suited for a related workshop."
        },
        "OoBRH9iqKM": {
            "0": "The paper highlights a significant application of natural language processing in the realm of mental health.",
            "1": "The authors have curated a valuable dataset for identifying EMS from patient texts or conversations.",
            "2": "The paper's dataset presentation lacks clarity, making it challenging to follow without specific examples.",
            "3": "The discussion of dataset de-identification, critical for dataset release, is absent.",
            "4": "The inter-annotator agreement is notably low.",
            "5": "There's a lack of discussion on the adjudication strategy and revisions that shaped the final test dataset.",
            "6": "Incorporating few-shot learning, especially for null labels, and implementing contrastive learning could enhance the results.",
            "7": "Consider making the dataset publicly available, as it could be a valuable resource for the community.",
            "8": "I kindly request the authors to provide a more detailed discussion of the dataset and consider its public availability for the benefit of the community."
        }
    },
    "islVqaCzfa": {
        "llm_review": {
            "0": "**Novelty and Relevance**: The paper addresses a significant gap in the field of automatic code editing by introducing InstructCoder, a dataset specifically designed for general-purpose code editing tasks.",
            "1": "This is a novel contribution as most existing datasets and models focus on code generation rather than editing.",
            "2": "**Comprehensive Dataset**: InstructCoder is a well-constructed dataset with over 114,000 instruction-input-output triplets covering a wide range of code editing scenarios.",
            "3": "The dataset is systematically expanded using an iterative process, ensuring diversity and relevance to real-world programming situations.",
            "4": "**Methodology**: The use of GitHub commits as seed tasks and the iterative process of generating new tasks using ChatGPT is innovative.",
            "5": "The scenario-conditional generation approach ensures that the dataset is diverse and contextually rich, which is crucial for training robust models.",
            "6": "**Empirical Validation**: The paper provides extensive empirical validation of the dataset and the models fine-tuned on it.",
            "7": "The results show that models fine-tuned on InstructCoder exhibit significant improvements in code editing performance, achieving accuracy levels on par with ChatGPT.",
            "8": "**Scalability**: The paper demonstrates that the performance of models improves log-linearly with the scale of the dataset, highlighting the importance of dataset size in training effective models.",
            "9": "**Human and GPT-4 Evaluation**: The use of both human evaluators and GPT-4 for assessing the correctness of code edits adds robustness to the evaluation process.",
            "10": "The high agreement between human and GPT-4 evaluations further validates the effectiveness of the proposed approach.",
            "11": "**Open Source**: The dataset and source code are made available to the public, which promotes transparency and allows other researchers to build upon this work.",
            "12": "## Weaknesses\n\n1.",
            "13": "**Limited Language Scope**: The dataset and experiments are primarily focused on Python.",
            "14": "While Python is a widely used language, the applicability of the approach to other programming languages is not explored.",
            "15": "This limits the generalizability of the findings.",
            "16": "**Complexity of Real-World Edits**: The paper acknowledges that the dataset does not encompass code changes involving cross-file contexts, which are common in real-world development.",
            "17": "This is a significant limitation as many practical code edits involve multiple files and complex dependencies.",
            "18": "**Evaluation Metrics**: The paper relies heavily on human and GPT-4 evaluations for assessing the correctness of code edits.",
            "19": "While these methods are robust, they are also subjective and may not capture all aspects of code quality, such as performance and maintainability.",
            "20": "**Noise in Data**: The paper mentions that the data collected from GitHub commits can be noisy and may contain incomplete contextual information.",
            "21": "Although efforts are made to filter and clean the data, the presence of noise could still affect the quality of the dataset and the performance of the models.",
            "22": "**Parameter-Efficient Finetuning**: The paper uses LoRA for parameter-efficient finetuning, which is computationally efficient but may not fully leverage the capabilities of large language models.",
            "23": "Exploring other finetuning methods could potentially yield better results.",
            "24": "**Limited Baseline Comparisons**: While the paper compares the performance of models fine-tuned on InstructCoder with ChatGPT and other open-source models, it does not include comparisons with other state-of-the-art code editing models or techniques.",
            "25": "This limits the ability to contextualize the improvements achieved.",
            "26": "**Ethical Considerations**: The paper briefly mentions ethical guidelines but does not delve into potential ethical implications of using large language models for code editing, such as the risk of generating insecure or malicious code.",
            "27": "## Conclusion\n\nOverall, the paper makes a significant contribution to the field of automatic code editing by introducing a novel dataset and demonstrating the effectiveness of instruction-finetuning for improving code editing capabilities of large language models.",
            "28": "The strengths of the paper lie in its comprehensive dataset, innovative methodology, and robust empirical validation.",
            "29": "However, there are some limitations related to the scope of the dataset, evaluation metrics, and potential noise in the data.",
            "30": "Addressing these weaknesses in future work could further enhance the impact and applicability of this research."
        },
        "xFiek5kj4s": {
            "0": "- Code editing is a very important capability that has been less explored in the GPT-era (though there is extensive work in code editing in the past, see \"Missing References\"), so it is nice to see work that is focused on this.",
            "1": "- The CodeInstruct dataset offers a nice training set that can be used for further research.",
            "2": "- Incorporating the notion of \"scenarios\" when building the dataset is quite clever and novel.",
            "3": "- Establishing that the gap between ChatGPT and open-source LLMs with respect to code editing can almost be closed with fine-tuning on dataset generated using prompt-based data generation techniques is interesting and may have implications for further research.",
            "4": "- By comparing against Alpaca and CodeAlpaca, the authors demonstrate that their data generation technique is superior to other prompt-based automatic data generation techniques for this task.",
            "5": "However, it is not clear whether such techniques are better than just directly using human-written examples from GitHub.",
            "6": "In Lines 163-175, the authors claim that GitHub commits are too noisy to use directly; however, this is not empirically validated.",
            "7": "GitHub serves as an extremely large data source, and given that one of the findings of this paper is that the scale of the data is a profound factor of code-editing ability, it is important to understand whether the scale of the data reduces the impact of noise.",
            "8": "Moreover, commit messages are not the only source of NL instructions from GitHub.",
            "9": "Another source is pull request comments (and the corresponding code edits).",
            "10": "In fact, there is already a large-scale benchmark for this: CodeReviewer (see missing references).",
            "11": "As a point of reference, it would be important to understand what the effect is of fine-tuning open-source LLMs on CodeReviewer, and how this compares to fine-tuning on CodeInstruct.",
            "12": "- The evaluation is limited and weak.",
            "13": "First, the test set entails only 134 examples which are manually curated by the authors of the paper, and it appears that they are all in Python.",
            "14": "Next, the main form of evaluation is prompting GPT-4 to judge the correctness of model predictions.",
            "15": "While this has been explored for other tasks, this has not been established as a valid evaluation strategy for code editing.",
            "16": "In Appendix E, the authors provide a justification for using GPT-4 for evaluation by comparing with human evaluation.",
            "17": "However, I do not find this convincing for a few reasons.",
            "18": "1) The human evaluation is done by authors of this paper and not by external evaluators, and no information is given about annotator agreement, 2) The human evaluation entailed three classes (correct, partial, wrong), while GPT-4 evaluates based on 2 classes (\"Yes\" or \"No\").",
            "19": "For comparison, they group \"partial\" with \"correct\" while it should actually be considered wrong.",
            "20": "3) Finally, the consistency ratio is 68.4%, which is lower than what I would expect for strong evaluation."
        },
        "G8MeyofOTh": {
            "0": "* Introduces a new dataset, following plausible steps to construct.",
            "1": "* Empirically shows the effectiveness of the benchmark by consistently improving the test accuracies (boh by GPT-4 and human) throughout LLMs.",
            "2": "* To justify using the automatic evaluation by GPT-4 as a main metric, authors need more supported analyses.",
            "3": "I suggest Cohen's kappa between GPT-4 and human decisions, and analyze the (dis)agreed categories.",
            "4": "* (minor) This benchmark seems a bit easy-- ChatGPT Zero-shot: 90.5% by GPT-4 and 79.3% for correct scores by human (though it cannot be an apples-to-apples comparison, competition-level code generation like CodeContests by LLMs shows below 10% of Pass@1 (= the accuracy of generating a single code for each problem)).",
            "5": "Can you analyze (and report) the category-wise performance, then build a hard subset for both 1) giving directions that what LLMs still bad at and 2) room for improvements that future work may acheive?"
        },
        "kHuLf4l3MF": {
            "0": "I appreciate that the authors created the dataset used for fine-tuning LLM for code editing tasks.",
            "1": "I believe this dataset will draw more attention and inspire further research in this area.",
            "2": "The dataset is built on the task seeds mined from github which give a more realistic starting point compared to other related dataset.",
            "3": "The analysis and plots show that the dataset covers diverse topics and intents.",
            "4": "I like the analysis done by the authors.",
            "5": "The LLAMA-33B fine-tuned on this CodeInstruct dataset has compareble performance to ChatGPT.",
            "6": "The way to collect and filter the task seeds from git commits is not clearly described.",
            "7": "I think this process is the most challenging part in collecting code-related data from Github.",
            "8": "- Authors mentioned that \"We used Codex to clarify the changes made between versions and improve the commit messages, resulting in more precise and informative instructions\", but they did not described how they filter the commit messages and they did not provide any examples in this process.",
            "9": "- I hope authors can provide details on how they classifiy the commit messages into 768 seed tasks.",
            "10": "What is the criteria here.",
            "11": "Is it simply by manually check?",
            "12": "My concern in evaluation is that there is only 134 data for evaluation which might be relatively small.",
            "13": "Second, I would suggest evaluating on other out-of-box code editing dataset to gain more confidence about the quality of the dataset.",
            "14": "(see missing related work for other datasets)\n   - I am wondering if authors manually clean each example in the test set to ensure they are reasonable and correct.",
            "15": "I think it is important for evaluate the models.",
            "16": "- When comparing with Model fine-tuned on CodeAlpaca dataset, I am wondering if authors will also report the performance on test set of CodeAlpaca.",
            "17": "Maybe the difference in performance is because of the data distribution shift.",
            "18": "The dataset only contains Python edits.",
            "19": "It seems not hard to include other programming languages"
        }
    },
    "OwxjgsX68V": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel augmentation scheme, CASSI, which leverages dependency parsing and semantic similarity to generate high-quality, contextually diverse augmentations.",
            "1": "This approach is innovative and addresses the limitations of existing methods that often result in annotation corruption and lack of context diversity.",
            "2": "**Comprehensive Evaluation**: The authors conduct extensive experiments across multiple languages and settings, including monolingual, cross-lingual, and noisy text conditions.",
            "3": "This comprehensive evaluation demonstrates the robustness and effectiveness of CASSI in various scenarios.",
            "4": "**Performance Improvement**: The results show that CASSI consistently outperforms existing methods such as DAGA and MELM, particularly in low-resource settings.",
            "5": "The significant improvements in Micro-F1 and Macro-F1 scores across different languages and dataset sizes highlight the effectiveness of the proposed method.",
            "6": "**Detailed Analysis**: The paper provides a thorough analysis of the results, including ablation studies and discussions on the quality of augmentations.",
            "7": "This detailed analysis helps in understanding the strengths and limitations of the proposed method.",
            "8": "**Code Availability**: The authors have made their code available on GitHub, which promotes transparency and reproducibility of the research.",
            "9": "**Weaknesses:**\n\n1.",
            "10": "**Dependency on Dependency Parsers**: The proposed method relies on the availability of high-quality dependency parsers for the target language.",
            "11": "This might limit the applicability of CASSI to extremely low-resource languages where such parsers are not available.",
            "12": "**Complexity and Computational Cost**: The process of generating and filtering candidate augmentations involves multiple steps, including dependency parsing, semantic similarity calculation, and language model scoring.",
            "13": "This complexity might result in higher computational costs, especially for larger datasets.",
            "14": "**Grammatical Incorrectness**: While the method aims to generate fluent and semantically correct sentences, there are instances of grammatical incorrectness and incoherence in the augmentations.",
            "15": "This issue needs to be addressed to ensure the quality of the generated data.",
            "16": "**Limited Improvement for High-Resource Languages**: The performance improvements for high-resource languages with larger datasets are marginal.",
            "17": "This suggests that the method's benefits are more pronounced in low-resource settings, and its impact diminishes as the dataset size increases.",
            "18": "**Evaluation Metrics**: The paper primarily focuses on Micro-F1 and Macro-F1 scores for evaluation.",
            "19": "While these metrics are standard for NER tasks, additional metrics such as precision, recall, and entity-level F1 scores could provide a more comprehensive evaluation of the method's performance.",
            "20": "**Conclusion:**\n\nOverall, the paper presents a significant contribution to the field of low-resource NER by introducing a novel augmentation scheme that effectively addresses the limitations of existing methods.",
            "21": "The comprehensive evaluation and detailed analysis demonstrate the robustness and effectiveness of CASSI.",
            "22": "However, the dependency on high-quality dependency parsers, complexity, and instances of grammatical incorrectness are areas that need further improvement.",
            "23": "Despite these limitations, CASSI shows great potential for enhancing NER performance in low-resource settings."
        },
        "tvy69nsa6K": {
            "0": "- A novel technique that aims to improve the performance of NER systems in low-resource settings.",
            "1": "- The technique is well-explained and backed by a solid experimental setup that includes numerous experiments in various settings, including monolingual and cross-lingual settings.",
            "2": "- Section 4 offers interesting insights, including a more in-depth discussion about the results on a specific language and an ablation study that goes through various hyper-parameters of the text augmentation pipeline.",
            "3": "- Since the authors have included the code as supplementary material, I assume that it will be open-sourced.",
            "4": "None"
        },
        "x3BdsnaBT9": {
            "0": "The motivation to augment data based on dependency parse tree is well-founded and interesting.",
            "1": "The proposed method outperforms previous data augmentation methods in monolingual NER.",
            "2": "It would be beneficial if the authors could demonstrate the effectiveness of the method on few-shot cross-domain NER, rather than only conducting experiments on cross-lingual settings, which are more common."
        },
        "JADAKj1d0a": {
            "0": "- The paper is well written and easy to follow.",
            "1": "Frequently, when doubts would arise, they were often addressed in the next sentence or paragraph.",
            "2": "Well done.",
            "3": "- CASSI explicitly considers both diversity and naturalness of the generations whereas many existing techniques yield ungrammatical, implausible, or incoherent texts.",
            "4": "Some would argue that low-quality synthetic texts are acceptable because they still improve downstream model performance and generalization.",
            "5": "However, in the real world, people often place significant weight on readability and having a technique that improves performance while being natural can help overcome objections from human decision makers and increase the likelihood of use.",
            "6": "- The evaluation setup acknowledges the limitations of the augmentation to low resource settings and sufficiently demonstrates its benefits relative to existing baselines.",
            "7": "- In general, the claims of “high-quality” and increased “contextual diversity” are left unquantified, at least directly.",
            "8": "The high-quality claim may potentially be substantiated via improvements to model performance but given the gist of the filtering criteria suggests that a quality == naturalness argument is being made.",
            "9": "Authors could use an approach like cleanlab (https://github.com/cleanlab/cleanlab) to support the claim that CASSI does not introduce any new label issues brought on by shifts in augmented semantics."
        }
    },
    "7O9bTjLgTQ": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method for visualizing and interpreting the semantic information flow within transformer-based language models.",
            "1": "This approach is innovative and provides a new perspective on understanding the internal workings of these models.",
            "2": "**Detailed Analysis**: The authors provide a thorough analysis of the attention heads and memory values in transformers.",
            "3": "They explore the semantic alignment between different components of the model, which is a significant contribution to the field of interpretability in machine learning.",
            "4": "**Visualization Tool**: The creation of a dynamic tool to visualize the information flow in Generative Pre-trained Transformers (GPTs) is a major strength.",
            "5": "This tool simplifies the complex data into easy-to-read plots, making it accessible for researchers and practitioners to understand the model's internal processing.",
            "6": "**Case Studies**: The paper includes several case studies that demonstrate the usability of the visualization tool.",
            "7": "These case studies provide practical examples of how the tool can be used to gain insights into the model's behavior.",
            "8": "**New Insights**: The authors uncover new insights about the role of layer norms as semantic filters and the presence of neurons that act as regularization vectors.",
            "9": "These findings contribute to the broader understanding of transformer models.",
            "10": "**Comprehensive Background**: The paper provides a comprehensive background on the transformer architecture and the logit lens method.",
            "11": "This helps readers understand the context and significance of the proposed approach.",
            "12": "**Open Source Code**: The authors have made their code and tool available on GitHub, promoting transparency and enabling other researchers to replicate and build upon their work.",
            "13": "## Weaknesses\n\n1.",
            "14": "**Limited Scope**: The paper focuses primarily on GPT-2 and does not extensively explore other transformer-based models.",
            "15": "While the authors claim that their approach is applicable to other models, additional experiments on different architectures would strengthen this claim.",
            "16": "**Complexity of Visualization**: While the visualization tool is a significant contribution, the resulting graphs can still be complex and difficult to interpret for users who are not familiar with the underlying concepts.",
            "17": "Further simplification or user guidance might be necessary.",
            "18": "**Evaluation Metrics**: The paper relies heavily on qualitative analysis and case studies to demonstrate the effectiveness of the visualization tool.",
            "19": "Including more quantitative evaluation metrics would provide a more robust assessment of the tool's performance.",
            "20": "**Generalization**: The findings and insights derived from the analysis of GPT-2 may not necessarily generalize to other transformer models or different types of tasks.",
            "21": "More diverse experiments would help validate the generalizability of the results.",
            "22": "**Interpretability of Projections**: The paper uses the logit lens method to project hidden states and neurons to the vocabulary space.",
            "23": "While this approach is useful, it may not capture all the nuances of the model's internal representations.",
            "24": "Exploring alternative projection methods could provide a more comprehensive understanding.",
            "25": "**Assumptions in Pruning**: The pruning approach used to simplify the flow graphs is based on the assumption that the most activated neurons are the most relevant.",
            "26": "This assumption may not always hold true, and important information could be lost in the pruning process.",
            "27": "**Causal Analysis**: The paper does not employ causal techniques to verify the findings.",
            "28": "Incorporating causal analysis would strengthen the claims about the roles of different components in the model's predictions.",
            "29": "## Conclusion\n\nOverall, the paper presents a novel and valuable approach to visualizing and interpreting the semantic information flow in transformer-based language models.",
            "30": "The detailed analysis, innovative visualization tool, and new insights contribute significantly to the field of interpretability in machine learning.",
            "31": "However, the paper could benefit from additional experiments on different models, more quantitative evaluation metrics, and a deeper exploration of alternative projection methods.",
            "32": "Despite these weaknesses, the work provides a strong foundation for future research in understanding the internal mechanisms of transformer models."
        },
        "lqzyFxlYwK": {
            "0": "This work might significantly impact the explainability research of NLP and DL communities.",
            "1": "The theoretical analysis part is thorough.",
            "2": "It appears that the authors have conducted sufficient research on GPT-related works.",
            "3": "The experimental part needs to be more sufficient.",
            "4": "The authors need to devise more persuasive comparative experiments to verify the superiority of the proposed method.",
            "5": "The author needs to emphasize academic innovation, clearly explaining their work's principles.",
            "6": "An excellent academic paper is always inspirational; it can motivate people to explore more.",
            "7": "It is suggested that the author redesign the structure of the paper.",
            "8": "The current structure of the article is more like a technical report than an academic paper."
        },
        "4gZra4ytyJ": {
            "0": "The main reason to accept would be the release of the tool to create the flow graphs, which may aid future research.",
            "1": "This is especially true for the most useful parts of the flow graph, which would be the less fine-grained ones, as will be discussed later.",
            "2": "Of the three main contributions, the one that strikes as new and interesting is the observed phenomena of semantic filters by layer norm layers.",
            "3": "It is interesting how function words contribution is decreased after applying them, highlighting the utility of these layers to increase the weight of content \"words\" (or at least hidden states close to those words.).",
            "4": "I would have enjoyed more quantitative results regarding this.",
            "5": "The method proposed seems too fine-grained.",
            "6": "It works when one already knows what to look for.",
            "7": "Each flow graph is focused on a single layer.",
            "8": "The projection of specific heads (either queries, keys or values) to the subdimensions of embeddings at the LM head (ie.",
            "9": "j:j+h/d as in line 236) doesn't seem to be that useful.",
            "10": "The fact it needs to be projected through W_O and that no quantitative results to properly measure  any patterns or useful information from these projections, hinders the overall approach.",
            "11": "Moreover, the projections for each head are not properly explained, one has to deduce just from line 236 that each hidden state is projected through W_O and then to the corresponding dimensions of the LM head, not just for A_j but also for keys, queries and values.",
            "12": "While \"full\" hidden states or weights that share the full dimension (d) are well projected to specific tokens as shown by work such as Dar et al.",
            "13": "(2022), the new contribution of this work which is looking at the head level is only demonstrated through a couple of examples and histogram figures, but I am not sure what one really learns other than that certain heads have higher norms and therefore their influence in the output of the attention block is bigger.",
            "14": "What is the contribution of the visual flow there?",
            "15": "When I look at Figure 1 and 5, I do not see any meaningful \"projections\" at the head level.",
            "16": "Its utility is only portrayed when one already knows of a specific head phenomena from other work, as shown in Figure 6.",
            "17": "And while some plots help grasp some degree of interpretability outside the head level (ie.",
            "18": "layer norm, attention block, feed-forward, residual, ie, hidden states that share the same dimension), this was already shown by previous work.",
            "19": "Still, a tool to properly visualise these flows, with top tokens for each hidden state is very nice, and considered a reason to accept.",
            "20": "The results themselves are unsurprising.",
            "21": "While this by itself is not a reason to reject, combined with the fact that other works have already discussed most of the points the paper sort of hinders the \"reasons to accept\".",
            "22": "This can be seen as a reason not to accept, rather than to reject.",
            "23": "Why only GPT2?",
            "24": "How can we know this tool will be useful in future research if it was only tested on a single model?",
            "25": "If the restriction is that only decoder systems benefit from this visualisation, why not try other decoder only models and show the flexibility of this approach?",
            "26": "It seems that most of what is discussed in the paper was already known for GPT2, so if the point is to present a new tool that helps in explainability of transformer models, as the title would suggest, showcasing other models would be important.",
            "27": "Overall I feel like the paper is trying too many things at once.",
            "28": "It presents as a tool for visualising the flow of information, and gives some nice examples on where it is useful, but at the same time focuses on contributions which were already known or a bit redundant/expected, with qualitative experiments/examples.",
            "29": "In truth, the tool itself should be the main contribution, and less words could be devoted to try to justify new contributions in a shallow way, which are not that surprising anyways, while more relevant examples on how the tool works are relegated to the Appendix.",
            "30": "To sum up, if the paper is trying to show how information flows in the model, other papers have already done so in a more clear and concise way, however, doing so through a visual tool projecting to specific tokens to help interpretability is novel."
        },
        "pqBlidzuKw": {
            "0": "* This work proposes a tool to visualize the simplified information flow.",
            "1": "Tools like this (if accessible) can facilitate research in understanding a model's decision.",
            "2": "* Although the analysis part is only based on a single dataset (CounterFact) and a single model variant (GPT), one of the case studies shows the applicability by checking the finding through their visualization tool aligns with the finding in a previous work.",
            "3": "* The core method used for analysis is *LogitLens*, which was originally proposed in a well-known blog post, but it was shown to be brittle in a  recent work (https://arxiv.org/pdf/2303.08112.pdf), as also cited by the authors.",
            "4": "The authors claim that the basic approach of LogitLens is applied because they are interested in the interim hypothesis instead of the final layer’s output, which may not be convincing enough because the Din et al.",
            "5": "paper also presented differences between the original LogitLens versus the improved version.",
            "6": "* EDIT: As mentioned in the authors' response, I think this is less of a concern because (1) The paper I mentioned was published too closed to the submission deadline, and (2) there are components where applying these methods would be difficult/inefficient.",
            "7": "* As mentioned in the strength section, with results shown from a single dataset and a single model variant can be a weakness, although not necessarily a reason to reject.",
            "8": "* The issues of the proposed visualization tool.",
            "9": "(These are more of presentation issues, but putting in this section because the visualization tool is the core contribution of this work)\n    * EDIT: This should've been addressed per the author's response.",
            "10": "* More careful consideration such as colorblind friendliness or contrast level should be taken towards the interface design of the visualization tool.",
            "11": "* As also mentioned by the authors in the Limitation section, the approach prunes out neurons that are less activated for better visualization, but it has to be under the hypothesis that the more activated neurons are more important for prediction, which may lead to misleading results if this hypothesis fails to hold."
        }
    },
    "1mGD6ZLTwv": {
        "llm_review": {
            "0": "**Novelty and Relevance**: The paper addresses a critical and timely issue in the field of NLP, focusing on the privacy risks associated with large language models, particularly in the context of summarization tasks.",
            "1": "This is a highly relevant topic given the increasing deployment of these models in sensitive domains such as healthcare and finance.",
            "2": "**Comprehensive Analysis**: The authors provide a thorough investigation of membership inference (MI) attacks on summarization models.",
            "3": "They explore both text similarity and data robustness as potential signals for MI attacks, offering a well-rounded analysis of the problem.",
            "4": "**Experimental Rigor**: The paper includes extensive experiments on three widely used datasets (SAMsum, CNN/DailyMail, and MIMIC-cxr) and two popular summarization models (BART and FLAN-T5).",
            "5": "The experiments are well-designed and the results are presented clearly, demonstrating the effectiveness of the proposed attacks.",
            "6": "**Practical Implications**: The study not only highlights the privacy risks but also discusses potential safeguards and the inherent trade-offs between privacy and utility.",
            "7": "This practical perspective is valuable for both researchers and practitioners in the field.",
            "8": "**Detailed Methodology**: The paper provides a detailed description of the experimental setup, including the data splitting, model training, and evaluation protocols.",
            "9": "This transparency allows for reproducibility and further exploration by other researchers.",
            "10": "**Robustness Analysis**: The introduction of document augmentation methods to assess the robustness of the models is a significant contribution.",
            "11": "The authors show that evaluating the model's response to perturbations can be an effective signal for MI attacks, even in a document-only attack scenario.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Limited Defense Strategies**: While the paper discusses some defense mechanisms like DP-SGD and L2 regularization, the exploration of these defenses is relatively shallow.",
            "14": "The paper could benefit from a more in-depth analysis of advanced defense techniques and their impact on both privacy and model performance.",
            "15": "**Generalizability of Results**: The study focuses on summarization tasks, and while the results are compelling, it remains unclear how well these findings generalize to other NLP tasks.",
            "16": "A broader investigation across different types of language models and tasks would strengthen the paper's conclusions.",
            "17": "**Feature Importance Analysis**: The paper includes a feature importance analysis for the baseline MI attack, but this analysis is somewhat limited.",
            "18": "A more detailed examination of why certain features are more indicative of membership could provide deeper insights into the model's behavior and potential vulnerabilities.",
            "19": "**Impact of Model Size and Complexity**: The paper does not thoroughly explore how the size and complexity of the language models affect their susceptibility to MI attacks.",
            "20": "Understanding whether larger or more complex models are more vulnerable would be valuable for guiding future model development.",
            "21": "**Ethical Considerations**: While the paper includes an ethics statement, it could benefit from a more detailed discussion of the ethical implications of MI attacks and the responsibilities of researchers and practitioners in mitigating these risks.",
            "22": "**Clarity in Presentation**: Some sections of the paper, particularly those describing the technical details of the attacks and defenses, could be clearer.",
            "23": "More diagrams or visual aids could help in explaining complex concepts and methodologies.",
            "24": "#### Conclusion\n\nOverall, \"Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks\" is a well-executed and important study that sheds light on the privacy vulnerabilities of large language models.",
            "25": "The paper's strengths lie in its comprehensive analysis, practical implications, and robust experimental design.",
            "26": "However, it could be improved by a deeper exploration of defense strategies, a broader generalization of results, and a more detailed feature importance analysis.",
            "27": "Despite these weaknesses, the paper makes a significant contribution to the field and provides a solid foundation for future research on privacy in NLP models."
        },
        "Dvb8ChBS58": {
            "0": "The authors present a simple but effective method for MI attacks that works specifically with summarization.",
            "1": "Unlike previous methods that focused on classification tasks, this approach doesn't need scores or probabilities and can work with just black-box API access.",
            "2": "The method can even be used without the summarized document, making it very useful for real-world situations.",
            "3": "The paper also includes extra tests and analysis, such as how well the method can be transferred to different setups.",
            "4": "The authors have thought about how to protect against these kinds of attacks and the balance between keeping data private and making it useful.",
            "5": "Overall, this is a solid paper that adds valuable insights to the field of natural language processing and information security.",
            "6": "It's a good read for anyone interested in these subjects.",
            "7": "At times the main inference was hard to follow and justify logical jumps, however, the abundance of figures/tables covered those minor discrepancies."
        },
        "xhK6IpC2Re": {
            "0": "- Tackling a problem of a MI attack for summarization\n- Proposal of input document only MI attack for summarization using input documents modified with word synonym replacement, sentence swapping and back translation.",
            "1": "-\tThe reason why this paper focuses on summarization is not presented.",
            "2": "Previous work focused on Machine Translation (MT)."
        },
        "azLHBSj30P": {
            "0": "- Novel task formulation \n\n- Extensive experimentation and ablation studies highlighting the impact of various factors \n\n- Variety of datasets employed in the study.",
            "1": "- Propose plausible defense mechanisms.",
            "2": "- Not the most up to date models employed."
        }
    },
    "8xyd9i1XLb": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a new method, MoPe θ, for identifying if a given text is in the training data of a pre-trained language model.",
            "1": "This method is based on adding noise to the model parameters and measuring the drop in log-likelihood, which approximates the trace of the Hessian matrix.",
            "2": "This is a novel approach compared to existing loss-based attacks and perturbation-based methods.",
            "3": "**Comprehensive Evaluation**: The authors evaluate MoPe θ across a range of language models from 70M to 12B parameters.",
            "4": "They show that MoPe θ is more effective than existing loss-based attacks and recently proposed perturbation-based methods, particularly at smaller model sizes.",
            "5": "**Detailed Analysis**: The paper provides a thorough analysis of the role of training point order and model size in attack success.",
            "6": "It also empirically demonstrates that MoPe θ accurately approximates the trace of the Hessian in practice.",
            "7": "**Challenging Conventional Wisdom**: The results challenge the conventional wisdom that the loss of a point alone is sufficient to determine extractability.",
            "8": "The authors show that there are training points that can be recovered using MoPe θ that have average loss, casting doubt on prior works that use the loss of a point as evidence of memorization or \"unlearning.\"",
            "9": "**Practical Implications**: The findings have significant implications for the study of memorization and privacy in language models.",
            "10": "The paper suggests that perturbation-based attacks that approximate the curvature in the loss landscape around a given model and point perform better than attacks that simply threshold the loss.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**White-box Setting**: The MoPe θ attack operates in a white-box setting, assuming access to the model parameters.",
            "13": "This is a limitation compared to existing attacks that typically operate in a black-box setting, where only access to the model outputs or the loss is assumed.",
            "14": "While the authors argue that this setting is still practical, it does limit the general applicability of the attack.",
            "15": "**Computational Cost**: The MoPe θ attack can be computationally expensive, especially for large model sizes.",
            "16": "Computing perturbed models takes time, and the authors note that they typically use a limited number of perturbations (n ≤ 20) for computational reasons.",
            "17": "This could limit the feasibility of the attack in practice.",
            "18": "**Limited Model Sizes**: The evaluation is limited to model sizes present in the Pythia suite (up to 12B parameters).",
            "19": "It is unclear how the results would scale to larger model sizes, which is an important consideration given the trend towards increasingly large language models.",
            "20": "**Hyperparameter Optimization**: The authors were only able to optimize over a limited set of noise values for MoPe θ due to computational constraints.",
            "21": "A more exhaustive hyper-parameter search could potentially improve the performance of the attack.",
            "22": "**Inverse Scaling with Model Size**: The paper finds that MoPe θ and DetectGPT success actually scales inversely with model size, which is surprising and counterintuitive.",
            "23": "The authors suggest that this may be due to increasing error in the Hutchinson trace estimator, but this warrants further investigation.",
            "24": "**Limited Scope of Application**: While the paper demonstrates the effectiveness of MoPe θ on language models, it is unclear how well this method would generalize to other types of machine learning models.",
            "25": "The authors briefly explore this with an MNIST network but find that MoPe θ performs worse than loss in this setting.",
            "26": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of privacy attacks on language models with the introduction of MoPe θ.",
            "27": "The novel approach, comprehensive evaluation, and detailed analysis are commendable.",
            "28": "However, the white-box setting, computational cost, and limited scope of application are notable limitations.",
            "29": "Future work could address these issues by exploring black-box settings, optimizing hyperparameters more exhaustively, and testing the method on larger models and different types of machine learning models."
        },
        "8hIbp2HsgJ": {
            "0": "The strengths of this paper include the development of a novel membership inference attack that has improved accuracy over existing loss-based attacks, the examination of the role of training point order and model size in attack success, and the demonstration that MoPe accurately approximates the trace of the Hessian in practice.",
            "1": "If this paper were to be presented at the conference or accepted into Findings, the main benefits to the NLP community would be a better understanding of the potential security risks associated with large language models and a new method for identifying whether a given text is in the training data of a pre-trained language model.",
            "2": "This could lead to improved security measures for language models and better protection of sensitive information.",
            "3": "One potential weakness of this paper is that it focuses solely on the development and evaluation of a membership inference attack, without discussing potential solutions or mitigation strategies to address the security risks identified.",
            "4": "Additionally, the paper only considers white-box access to the model's parameters, which may not be representative of real-world attack scenarios."
        },
        "zNCGHrULsT": {
            "0": "The authors demonstrate a strong and relatively efficient approach to MIA without requiring shadow model training.",
            "1": "The work is timely and relevant given the explosion of LLMs trained with data of dubious provenance.",
            "2": "Success of the attack is really only demonstrated on auto-regressive LLMs.",
            "3": "Experimental comparison with shadow-training approaches would be valuable, at least for smaller models, or explain why these are not explored.",
            "4": "An important limitation is white box access, but this is not a deal-breaker."
        },
        "6ckX1UpN5k": {
            "0": "+ Well written work except for the points below in the first weakness.",
            "1": "My notes include\n  \"nice\" in response to several well-made points throughout the paper which is not common for me.",
            "2": "+ Attack discovers is effective at a different set of instances than loss attacks (i.e.",
            "3": "answers the\n  points nicely made on line 96).",
            "4": "This and the AUC behaviour comparison is suggesting that there\n  may be a fundamentally different sort of signal being employed in the MoPE attack (as opposed to\n  being just a better LOSS).",
            "5": "This also means the methods can be combined to get non-trivial benefit\n  as shown in the Ensemble Attack.",
            "6": "This strength has some caveats under the second weakness below.",
            "7": "- The description of the principal method does not match the actual method and thus the reasoning\n  behind it is misplaced and/or potentially irrelevant.",
            "8": "From the start, the method is suggesting\n  that loss landscape around training points is sharper than around random (non-training) points:\n\n       \"based on the idea that when the model loss is localized around a training point, it is\n        likely to lie in sharper local minima than if the point was a test point—a previously\n        unexplored fact distinct from the magnitude of the loss\"\n\n       \"loss around x′ ∈ D_train should be sharper than around a random point\"\n\n  The actual method perturbs the model parameters though one would expect perturbations to points\n  given the above descriptions.",
            "9": "The initial point seemed intuitive while model perturbations are\n  not.",
            "10": "The connection between model perturbations and point perturbations are not described or\n  explained.",
            "11": "Suggestion: Explain why input perturbations are not done in favor of model perturbations.",
            "12": "(if\n  true) explain how model perturbation is the same as, or sufficiently similar to, input\n  perturbation.",
            "13": "Other motivations might include the difficulty of perturbing inputs of discrete\n  tokens (though this can be addressed by perturbing embeddings).",
            "14": "Finally, if input perturbations\n  are possible, compare the model perturbation approach to the input perturbation one,\n  experimentally.",
            "15": "In addition to this point, the connection to Hessian trace is specific to the (I claim) less\n  intuitive model perturbation.",
            "16": "The benefit of making such a connection are not discussed.",
            "17": "Suggestion: describe the theoretical or otherwise benefits of making the connection between model\n  parameter perturbation as per MoPE and Hessian traces.",
            "18": "- The actual method, perturbing model parameters, might have more connections to prior methods\n  other than LOSS.",
            "19": "Perturbing a model is moving it away from the set of parameters that was\n  attained with a particular training point and in same way, getting closer to a model that was\n  trained without that point.",
            "20": "There are membership methods based on comparisons of \"with point\" vs\n  \"without point\" models.",
            "21": "Cited works include [Carlini 2021].",
            "22": "Such methods, however, were not\n  compared to MoPE experimentally nor conceptually.",
            "23": "Suggestion: Include a deeper discussion on attacks based on reasoning with models known to not be\n  trained on target input.",
            "24": "Provide experimental comparisons of MoPE and such methods.",
            "25": "- Unexplained restrictions on target models.",
            "26": "In particular, the authors claim that single epoch\n  training is common as a justification of not experimenting with models trained with multiple\n  though the discussion of checkpoints (around line 410) suggests that not only is it also common\n  to train for more than one pass, these models were already readily available during\n  experimentation and had to be specifically avoided.",
            "27": "Suggestion: Compare MoPE to baselines (LOSS and otherwise) on models trained with multiple\n  passes.",
            "28": "Demonstrate that the benefit of MoPE over baselines is or is not associated with the\n  single-pass model training regimen.",
            "29": "Other suggestions:\n\n- Additionally I suggest the authors revisit their related work sections and be less quick to\n  dismiss comparisons against other works due to some difference in setting that could be easily\n  adopted to make a direct experimental comparison possible."
        }
    },
    "gd8TxhKoLv": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel framework, PROTEGE, which leverages a prompt-based encoder-decoder architecture combined with a hill-climbing algorithm to generate diverse and high-fidelity questions from web articles.",
            "1": "This dual-stage approach is innovative and addresses the limitations of existing methods.",
            "2": "**Diversity and Fidelity**: The authors emphasize the importance of both diversity and fidelity in question generation.",
            "3": "The proposed method shows significant improvements in these metrics, with a reported 16% increase in diversity and 8% increase in fidelity over strong baselines.",
            "4": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of PROTEGE on multiple datasets, including SEARCH QA, SQUAD, NQ, and MS MARCO.",
            "5": "The use of various metrics such as Distinct-n, Entropy-n, BERTScore, and fidelity metrics ensures a comprehensive assessment of the model's performance.",
            "6": "**Human Evaluation**: The inclusion of human evaluation adds credibility to the results.",
            "7": "The human audits show that PROTEGE improves readability, diversity, and answerability of the generated questions compared to the baseline.",
            "8": "**Detailed Analysis**: The paper includes detailed ablation studies to understand the impact of different prompt signals, the effect of ORACLE prompting, and the role of the greedy algorithm.",
            "9": "This thorough analysis helps in understanding the strengths and limitations of the proposed approach.",
            "10": "**Practical Applications**: The paper highlights practical applications of the proposed method in knowledge bases, chatbots, and educational tools.",
            "11": "This demonstrates the real-world relevance and potential impact of the research.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Complexity**: The proposed method involves multiple stages and components, including prompt-based controlled generation and a greedy hill-climbing algorithm.",
            "14": "This complexity might make it challenging to implement and reproduce the results without access to detailed implementation details and code.",
            "15": "**Dependence on Prompts**: The effectiveness of PROTEGE heavily relies on the quality and relevance of the prompts.",
            "16": "The paper mentions the use of ORACLE prompts during training, which might not always be feasible in real-world scenarios.",
            "17": "The performance drop when using HEURISTIC prompts indicates a potential limitation in practical applications.",
            "18": "**Limited Generalization**: While the paper shows improvements on specific datasets, the generalization of the method to other domains or languages is not thoroughly explored.",
            "19": "The authors mention future work on extending the model to non-English languages, but current results are limited to English.",
            "20": "**Resource Intensive**: The training of the proposed model requires significant computational resources, including large GPU clusters.",
            "21": "This might limit the accessibility of the method to researchers and practitioners with limited resources.",
            "22": "**Hallucination Issues**: The paper acknowledges that the model may hallucinate or generate incomplete product names when using certain prompt signals.",
            "23": "This issue needs to be addressed to ensure the reliability and accuracy of the generated questions.",
            "24": "**NLG Metrics**: The paper reports that PROTEGE does not always outperform baselines on NLG metrics for certain datasets (e.g., NQ and MS MARCO).",
            "25": "This indicates that while the model improves diversity and fidelity, it might not always generate questions that are semantically closer to the ground-truth.",
            "26": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of automatic question generation with its novel PROTEGE framework.",
            "27": "The strengths of the approach, including improved diversity and fidelity, comprehensive evaluation, and practical applications, outweigh the weaknesses.",
            "28": "However, addressing the limitations related to complexity, prompt dependence, generalization, resource requirements, hallucination issues, and NLG metrics will be crucial for further improving the method and ensuring its broader applicability."
        },
        "7Y1UJp7qRb": {
            "0": "- The experimental results are promising, and the paper conducts a thorough analysis of the results in terms of diversity and fidelity.",
            "1": "The inclusion of human evaluations in Figure 2 supplements the reliability of the findings.",
            "2": "- The idea of employing separate cross-attention architecture for conditional generation is a novel contribution to my knowledge.",
            "3": "- The proposed method shows inferior performance against baselines when evaluated using NLG metrics (METEOR, BLEU, ROUGLE).",
            "4": "It would be beneficial to include additional examples in the Appendix, explaning why tehse scores are relatively low.",
            "5": "- Some important details are missing in the experiment setting.",
            "6": "For instance, the paper does not specify the number of examples used in human evaluation or the number of participants involved in the evaluation process.",
            "7": "Furthermore, there are concerns about the reliability of the fidelity metrics based on the trained neural network.",
            "8": "The suitability of the trained BERT model for measuring the fidelity score of generated questions is questionable, considering results in Table 2."
        },
        "RTopz2q5D6": {
            "0": "This study introduced a new method (PROTEGE) to generate questions based on LLM architecture.",
            "1": "The study performed various experiments on three datasets and provided a lot of insight into the results.",
            "2": "The article can open a period of generating Q&A data more automatically and faster than previous methods.",
            "3": "The study has not shown practical applications to help people for this research.",
            "4": "We want more empirical diversity (add more languages if possible)\nAnd what is the way to make sure the questions that are generated stick to the passage and are answerable?"
        },
        "xNdrKtzeOH": {
            "0": "Easy-to-read and well-written paper.",
            "1": "Analyzed through various experiments and metrics.",
            "2": "Study about the relevance between diversity and fidelity.",
            "3": "A relatively simple framework structure.",
            "4": "The key difference from existing question generation models is the addition of an extra layer at the end of the encoder stage to extract embedding of prompt.",
            "5": "Furthermore, these extracted embeddings are utilized in the decoder stage through cross-attention.",
            "6": "However, applying attention across different input modalities is one of the commonly used approaches in NLP.",
            "7": "Lack of detail explanation.",
            "8": "In Section 2.2, a detail explanation of heuristic method is missing.",
            "9": "In Table 4, it is unclear the meaning of pre-greedy and post-greedy.",
            "10": "Insufficient explanation about limited performance.",
            "11": "In table 2, the NLG metrics for fidelity are worse in NQ and MS MARCO dataset.",
            "12": "It needs more analysis."
        }
    },
    "glxrubmH91": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces a novel method, RAPL, which addresses the challenges in few-shot document-level relation extraction (FSDLRE) by leveraging relation-aware prototype learning.",
            "1": "This is a significant contribution as it tackles the pervasive data scarcity problem in real-world scenarios.",
            "2": "**Comprehensive Methodology**: The authors provide a detailed explanation of their methodology, including the construction of relation prototypes at the instance level and the use of relation-weighted contrastive learning.",
            "3": "This thoroughness helps in understanding the intricacies of their approach.",
            "4": "**Task-Specific NOTA Prototypes**: The paper highlights the importance of task-specific NOTA (none-of-the-above) prototypes, which is a novel idea.",
            "5": "This addresses the issue of generic NOTA prototypes that may not capture the specific semantics of different tasks.",
            "6": "**Extensive Experiments**: The authors conduct extensive experiments on two benchmarks, FREDo and ReFREDo, demonstrating the effectiveness of their method.",
            "7": "The results show that RAPL outperforms state-of-the-art approaches by an average of 2.61% F1 across various settings.",
            "8": "**Ablation Studies and Analysis**: The paper includes comprehensive ablation studies and analyses, which help in understanding the contribution of each component of the proposed method.",
            "9": "This adds to the robustness and credibility of the findings.",
            "10": "**Case Study**: The inclusion of a case study provides a practical illustration of the method's effectiveness and highlights both its strengths and areas for improvement.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Complexity and Efficiency**: The incorporation of a relation encoder and the search process for support NOTA instances add to both memory and time expenses.",
            "13": "This complexity might hinder the practical applicability of the method in resource-constrained environments.",
            "14": "**Assumption of Pre-Specified Entities**: The method assumes that entity information is pre-specified, which may affect its robustness.",
            "15": "This assumption limits the method's applicability in scenarios where entity recognition is not straightforward.",
            "16": "**Performance on Cross-Domain Tasks**: While the method shows significant improvements on in-domain tasks, the performance gain on cross-domain tasks is relatively lower.",
            "17": "This indicates that the method may still struggle with domain adaptation, which is a critical aspect of real-world applications.",
            "18": "**Over-Prediction Tendency**: The case study reveals that the method tends to exhibit cases of over-prediction, resulting in relatively lower precision.",
            "19": "This suggests that there might be a need for further refinement to balance precision and recall.",
            "20": "**Scalability with Number of Support Instances**: Although the method demonstrates a certain level of scalability, the performance does not always positively correlate with the number of support relation instances.",
            "21": "This indicates that the method might not fully leverage additional support instances.",
            "22": "**Limited Exploration of LLMs**: The preliminary exploration of large language models (LLMs) for FSDLRE tasks is limited.",
            "23": "Given the recent advancements in LLMs, a more thorough investigation could provide additional insights and potentially improve the method.",
            "24": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of few-shot document-level relation extraction with its novel RAPL method.",
            "25": "The strengths of the paper lie in its comprehensive methodology, extensive experiments, and detailed analyses.",
            "26": "However, there are areas for improvement, particularly in terms of complexity, robustness, and performance on cross-domain tasks.",
            "27": "Addressing these weaknesses could further enhance the method's applicability and effectiveness in real-world scenarios."
        },
        "8Fqm2Q5BSy": {
            "0": "1.The proposed issues worth investigation for Few-shot Doc RE and the proposed method seems reasonable.",
            "1": "2.The proposed method achieves great performances on the public benchmarks.",
            "2": "3.It is glad to see a Preliminary Exploration of LLM for FSDLRE.",
            "3": "1.Some technical details are not clear enough.",
            "4": "Please refer to Question to authors for details.",
            "5": "2.An analysis, which is about how the issue of NOTA is tackled, should be added to support the claim."
        },
        "8MNX5PkSHI": {
            "0": "The paper is very well-written and easy to follow.",
            "1": "The RAPL model proposed in the paper handled an important task of the few-shot document-level relation extraction task.",
            "2": "Although the work can seem incremental on the FREDo benchmark, the claims and rationales proposed in the paper are intuitive and justified with experimental results.",
            "3": "The methodology is explained in detail, and combining contrastive learning with instance-level aggregation for prototype tuning is interesting.",
            "4": "The RAPL model is incremental work, and the architecture uses the weak BERT encoder, which might cause a performance bottleneck.",
            "5": "The method assumes that entity mentions should be specified for a query document, making it less robust.",
            "6": "One of the paper's main contributions is to build a task-specific NOTA prototype; however, precise experiments to evaluate NOTA relations are missing."
        },
        "N0L5RjFCOB": {
            "0": "The paper studies an important task and proposes an interesting approach to address the FSDLRE problem.",
            "1": "The experiments are overall clear and well structured.",
            "2": "The paper is clearly written and easy to access.",
            "3": "Overall I feel the paper is solid, presenting some interesting findings and demonstrate the effectiveness of its proposed approach via multiple experiments.",
            "4": "There are a few places where the authors can further improve the current writing:\n1.",
            "5": "Considering adding one more dataset (maybe modified one existing DocRE benchmark) to demonstrate this proposed method can generalize.",
            "6": "Adding a few more case studies in the experiment section to intuitively show the output and also to promote the FSDLRE problem setting.",
            "7": "Adding more recent DocRE literature in the reference."
        }
    },
    "BpibUh0aB3": {
        "llm_review": {
            "0": "\"**\n\n**Strengths:**\n\n1.",
            "1": "**Innovative Approach to Creativity Measurement:**\n   - The paper introduces a novel method to evaluate the creativity of large language models (LLMs) using the Divergent Association Task (DAT).",
            "2": "This approach is grounded in cognitive psychology and provides an objective measure of creativity by assessing the semantic distance between generated words.",
            "3": "**Comprehensive Evaluation:**\n   - The study evaluates multiple LLMs, including GPT-4, GPT-3.5-Turbo, Oasst-Llama-30B, Vicuna-13B, and ChatGLM-6B, providing a broad perspective on the capabilities of different models.",
            "4": "The comparison across various models and decoding strategies adds depth to the analysis.",
            "5": "**Detailed Analysis of Decoding Strategies:**\n   - The paper explores the impact of different decoding strategies, such as greedy search and top-p sampling, on the creativity of LLMs.",
            "6": "This analysis highlights the trade-offs between creativity and stability, providing valuable insights for future research and practical applications.",
            "7": "**Validation of DAT for LLMs:**\n   - The authors validate the applicability of the DAT for LLMs by comparing the results with human data and random baselines.",
            "8": "This validation strengthens the credibility of the findings and ensures that the DAT is a reliable measure of creativity for LLMs.",
            "9": "**Clear Presentation of Results:**\n   - The paper presents the results in a clear and organized manner, using figures and tables to illustrate key findings.",
            "10": "The relationship between DAT scores and surprisal is particularly well-explained, providing a nuanced understanding of the factors influencing creativity in LLMs.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Limited Scope of Creativity:**\n   - The study focuses primarily on \"little-C\" creativity, which pertains to everyday creative thinking, rather than \"big-C\" creativity, which involves groundbreaking and highly original ideas.",
            "13": "While this focus is justified, it limits the generalizability of the findings to more complex and high-stakes creative tasks.",
            "14": "**Potential Confounding Variables:**\n   - The paper acknowledges the potential influence of word frequency on measuring semantic distance but does not fully address how this confounding variable might affect the results.",
            "15": "Although an attempt is made to control for surprisal, a more thorough analysis of other potential confounders would strengthen the conclusions.",
            "16": "**Reliance on Pre-trained Models:**\n   - The study relies on pre-trained models and does not explore the impact of fine-tuning or additional training on the creativity of LLMs.",
            "17": "Investigating how different training regimes influence creative output could provide a more comprehensive understanding of the factors that enhance or inhibit creativity in LLMs.",
            "18": "**Subjectivity in Creativity Evaluation:**\n   - While the DAT provides an objective measure of creativity, the evaluation of creative quality remains inherently subjective.",
            "19": "The paper could benefit from incorporating additional methods, such as human expert evaluations or more diverse creativity tasks, to triangulate the findings and provide a more holistic assessment of LLM creativity.",
            "20": "**Limited Discussion on Practical Implications:**\n   - The paper provides valuable insights into the creative capabilities of LLMs but falls short in discussing the practical implications of these findings.",
            "21": "A more detailed exploration of how these insights can be applied in real-world scenarios, such as creative writing, art generation, or problem-solving, would enhance the relevance and impact of the study.",
            "22": "**Conclusion:**\n\nOverall, the paper makes a significant contribution to the understanding of creativity in large language models by introducing and validating the Divergent Association Task as a measure of creative thinking.",
            "23": "The comprehensive evaluation of different models and decoding strategies provides valuable insights, although the study's scope and potential confounding variables limit the generalizability of the findings.",
            "24": "Future research should explore additional training methods, incorporate diverse evaluation techniques, and discuss practical applications to build on the promising results presented in this study."
        },
        "hlqiLg6nUo": {
            "0": "- The paper provides a focused analysis of creativity in LLMs, and addresses various interesting factors related to creativity (surprisal, temperature).",
            "1": "- The authors propose a simple and computationally efficient way of assessing creativity in LLMs, leveraging findings from creativity assessment in human language processing.",
            "2": "The paper makes various assumptions that are not adequately addressed:\n- The findings for creativity found in humans where the DAT task predicts human creativity does not necessarily transfer to LLMs; taking this as a given skips the step of validating the finding of Olson et al.",
            "3": "(2021) for the domain of LLMs.",
            "4": "- Cosine similarity has traditionally been shown to correlate with semantic relatedness, but its one-dimensional nature makes it suboptimal to measure semantic similarity in isolation.",
            "5": "E.g., by requesting to only predict nouns we are already restricting token predictions to come from a particular \"region\" in the embedding space.",
            "6": "Since cosine similarity does not differentiate between semantic relatedness and other linguistic dimensions (syntact/morphological/etc.",
            "7": "), a model could \"cheat\" by generating words that are not only semantically unrelated, but also syntactically.",
            "8": "- DAT is computed only using the GLoVe cosine distances, but the impact of this choice is not addressed.",
            "9": "GLoVe vectors should not be taken as a gold truth for semantic similarity."
        },
        "Rg9aFkUE96": {
            "0": "The question is definitely timely and pressing, careful studies on the matter are needed and likely to get us new knowledge.",
            "1": "See above: I think the paper does not show what it purports to show due to a conceptually flawed experimental design.",
            "2": "-----------------------------------\nI think the analyses provided are now sufficiently informative.",
            "3": "The rhetoric should probably be toned down a bit about this being creativity rather than a narrow aspect of it, but I do see the pros of having a catchy title.",
            "4": "The presentations should also be improved."
        },
        "OnNLQbwkY9": {
            "0": "The paper discusses an interesting take on creativity of LLMs.",
            "1": "The authors specifically looks at single nouns but do not adequately justify why combinations of multiple words (specifically creativity in language is tightly related to the ability to put together two seemingly different words that could mean something interesting, taken together) were not taken into consideration or why words other than nouns were not considered.",
            "2": "These would be necessary points to be discussed in the limitations section."
        }
    },
    "WQamRhhbsf": {
        "llm_review": {
            "0": "**Novel Hypothesis and Insightful Analysis**: The paper presents a novel hypothesis that large language models (LLMs) rely heavily on co-occurrence statistics from their pre-training data, which leads to factual inaccuracies.",
            "1": "This is a significant contribution to understanding the limitations of LLMs and provides a new perspective on why these models might fail in certain scenarios.",
            "2": "**Comprehensive Experiments**: The authors conduct extensive experiments using various models (GPT-Neo, GPT-J, GPT-3.5, and ChatGPT) and datasets (LAMA-TREx).",
            "3": "The use of different model sizes and finetuning settings provides a thorough analysis of the impact of co-occurrence statistics on factual knowledge.",
            "4": "**Clear Presentation of Results**: The results are presented clearly with well-designed figures and tables.",
            "5": "The correlation between co-occurrence statistics and factual knowledge accuracy is effectively demonstrated, making it easy for readers to understand the key findings.",
            "6": "**Practical Implications**: The paper discusses practical implications, such as the potential for hallucinations and biased responses due to reliance on co-occurrence statistics.",
            "7": "This is important for the development of more reliable and accurate language models.",
            "8": "**Mitigation Strategies**: The authors propose a mitigation strategy by finetuning on a debiased dataset, which is a practical approach to address the identified issue.",
            "9": "Although the results show limited effectiveness, it opens the door for further research in this direction.",
            "10": "**Open Source Code**: The availability of the code on GitHub enhances the reproducibility of the experiments and allows other researchers to build upon this work.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Limited Scope of Models and Data**: While the paper tests several models, it primarily focuses on the GPT family and the Pile dataset.",
            "13": "Testing a broader range of models and datasets could strengthen the generalizability of the findings.",
            "14": "**Debiased Finetuning Effectiveness**: The proposed debiased finetuning strategy shows limited effectiveness in generalizing to test cases.",
            "15": "The paper could benefit from exploring additional or alternative debiasing techniques to provide more robust solutions.",
            "16": "**Real-World Application Analysis**: The paper focuses on factual knowledge probing, which may not fully represent real-world applications of LLMs.",
            "17": "Analyzing the impact of co-occurrence bias on downstream tasks like question answering or summarization would provide a more comprehensive understanding of the issue.",
            "18": "**Detailed Analysis of Failure Cases**: While the paper provides some examples of failure cases, a more detailed qualitative analysis could offer deeper insights into why certain facts are more prone to co-occurrence bias and how this affects different types of knowledge.",
            "19": "**Scalability of Results**: The paper acknowledges the limitation of computational resources, which restricts the scope of tested models.",
            "20": "Future work could benefit from scaling up the experiments to include more diverse and larger models to validate the findings further.",
            "21": "**Ethical Considerations**: Although the paper mentions the ethical implications of biased and factually incorrect responses, it could delve deeper into the potential societal impacts and how the proposed solutions might mitigate these risks.",
            "22": "#### Conclusion\n\nOverall, the paper makes a significant contribution to understanding the limitations of LLMs related to co-occurrence statistics.",
            "23": "The comprehensive experiments and clear presentation of results provide valuable insights into the issue.",
            "24": "However, the scope of models and data, the effectiveness of the proposed mitigation strategy, and the analysis of real-world applications could be expanded to strengthen the findings.",
            "25": "Further research in these areas will be crucial for developing more reliable and accurate language models."
        },
        "rfX6ne8ne4": {
            "0": "-- The paper addresses an important and timely topic, namely the ability of LLMs to act as knowledge bases.",
            "1": "-- The related work section is very elaborate and provides insight into the field at hand.",
            "2": "-- The arguments of the paper are well-presented, and the writing is generally clear.",
            "3": "-- I am somewhat confused as to the exact claim the paper is making.",
            "4": "While the results clearly show a correlation between the co-occurrence statistics of a triplet and the performance of the model on it, it is not clear to me whether this in fact proves that there is a bias where such simple surface statistics push the language model astray from making the right prediction.",
            "5": "What it does show is that questions where the degree of co-occurrence is smaller are more difficult for the model.",
            "6": "The introduction reads “in which frequently co-occurred words are preferred over the correct answer.” I could not see how the experiments directly make this point.",
            "7": "Simple correlation seems to me insufficient in this case, since making mistakes with little co-occurrence doesn’t mean that there is a different option with higher co-occurrence.",
            "8": "In order to show that the behavior is biased, I would expect the paper to shows that the surface statistics interfere in some sense with the prediction of the model, in a way that would make it predict such answer even when it is not true.",
            "9": "For example, I would have expected the paper to examine questions which we would expect (based on their prevalence in the training data) the model to answer correctly, and show that in these cases it tends to make more errors where there is a strong collocation and that the mistakes is towards the collocating words.",
            "10": "If the paper indeed makes this kind of more subtle claim and I have missed it, I would welcome a response from the authors on this matter.",
            "11": "Thank you.",
            "12": "Following rebuttal: the results you have posted are helpful and address this comment.",
            "13": "Please include them in the next version.",
            "14": "-- The results of the attempts to mitigate the bias are not very strong.",
            "15": "I should say that I do not see it in itself as grounds for rejection.",
            "16": "-- Some important presentational details are not sufficiently clear (see below)."
        },
        "EIKFZmuV6r": {
            "0": "In my opinion, the most interesting aspect of the paper are the frequency baselines introduced to analyse the structure of the training dataset and to explain the behaviour of the models.",
            "1": "They bring the co-occurrence/frequency bias influencing the behaviour of the models to fore, and the analysis is convincing.",
            "2": "The main point of the paper -- that LLMs suffer from co-occurrency bias -- is not particularly new.",
            "3": "Other papers investigating this issue are mentioned in the Related Work section.",
            "4": "The authors claim that their work is the fist \"to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs\" (ll.",
            "5": "136--139), but there is no discussion of why fine-tuning should help at all, and in the end it does not, which amounts to a weak negative result.",
            "6": "The section on mitigating occupies less than 1.5 pages and does not contain any methodological insights."
        },
        "6J9oy63MjV": {
            "0": "-\tThe paper contains detailed analysis of shortcut problem regarding token co-occurrence.",
            "1": "-\tThe paper investigates an important area of verifying the factual knowledge of LLMs.",
            "2": "-\tThe paper is well-written, and is free of significant presentation issues.",
            "3": "-\tAlong with the identification of the problem, the paper also proposes to mitigate the problem.",
            "4": "-\tOne more experiment should be done to verify the claim that \"answers with higher co-occurrence are more likely to be generated\": The authors should count in each question, whether the model's generated answer has a high count in the pretraining corpus.",
            "5": "Currently there is only a table (Table 1) showing similar results, i.e., the wrong answers have a relatively lower count in the pretraining corpus.",
            "6": "However, quantitative results over the whole dataset should be given.",
            "7": "-\tThere are existing work probing the shortcut learning problem of language models.",
            "8": "The authors should elaborate more on the work to claim that they are the first to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs.",
            "9": "For example: [2]."
        }
    },
    "6srsYdjLnV": {
        "llm_review": {
            "0": "Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus\"\n\n## Strengths\n\n1.",
            "1": "**Timely and Relevant Topic**: The paper addresses the critical issue of gender bias in machine translation (MT), which is a timely and relevant topic given the increasing awareness and demand for gender inclusivity in language technologies.",
            "2": "**Comprehensive Approach**: The authors take a comprehensive approach by not only creating a benchmark dataset (GeNTE) but also exploring various evaluation methods for gender-neutral translation.",
            "3": "This dual focus on data creation and evaluation methodology is commendable.",
            "4": "**Survey-Informed Design**: The creation of the GeNTE corpus is informed by a preliminary survey on the perception and use of neutral language.",
            "5": "This ensures that the dataset is grounded in real-world user preferences and perceptions, enhancing its relevance and applicability.",
            "6": "**Detailed Corpus Creation Process**: The paper provides a detailed description of the corpus creation process, including the selection and annotation of data, as well as the challenges faced in creating gender-neutral references.",
            "7": "This transparency is valuable for reproducibility and for other researchers looking to create similar resources.",
            "8": "**Evaluation of Existing Metrics**: The authors conduct a thorough evaluation of existing reference-based MT metrics and highlight their limitations in assessing gender-neutral translations.",
            "9": "This critical analysis is essential for understanding the current state of MT evaluation and for guiding future research.",
            "10": "**Proposal of a Reference-Free Method**: The introduction of a reference-free evaluation method using a classifier trained on synthetic data is innovative and addresses some of the limitations of reference-based metrics.",
            "11": "This method shows promise in providing a more robust evaluation of gender-neutral translations.",
            "12": "**Open Access to Resources**: The authors make the GeNTE dataset and the evaluation code freely available, which is a significant contribution to the research community.",
            "13": "This openness facilitates further research and development in the area of gender-neutral MT.",
            "14": "## Weaknesses\n\n1.",
            "15": "**Limited Language Pair**: The study focuses exclusively on the English-Italian language pair.",
            "16": "While Italian is a representative example of grammatical gender languages, the findings and resources may not be directly applicable to other languages with different gender systems.",
            "17": "Future work should consider extending the approach to other languages.",
            "18": "**Dependence on Closed-Source Models**: The use of closed-source models like Amazon Translate, DeepL, and GPT-3.5-turbo for generating and evaluating translations raises concerns about reproducibility and accessibility.",
            "19": "These models are regularly updated, which may lead to different results in future studies.",
            "20": "**Post-Editing of MT Outputs**: The manual post-editing of MT outputs to create gender-neutral translations introduces a level of human intervention that may not fully reflect the capabilities of current MT systems.",
            "21": "This approach, while necessary given the current limitations of MT models, may not provide a completely accurate picture of the state of gender-neutral MT.",
            "22": "**Evaluation on Synthetic Data**: The classifier for the reference-free evaluation method is trained on synthetic data generated by GPT-3.5-turbo.",
            "23": "While this approach is innovative, the reliance on synthetic data may not fully capture the complexities and nuances of natural language, potentially affecting the classifier's performance in real-world scenarios.",
            "24": "**Limited Exploration of Direct Non-Binary Language**: The paper focuses on indirect non-binary language strategies, which avoid gendered terms.",
            "25": "However, it does not explore direct non-binary language strategies, such as the use of neopronouns or neomorphology, which are also important for gender inclusivity.",
            "26": "Future work could consider incorporating these strategies.",
            "27": "**Lack of Robustness Testing**: The evaluation protocols are tested on scenarios where the MT models succeed in generating the expected (either neutral or gendered) output.",
            "28": "The robustness of these protocols in scenarios where the models fail to generate the expected output is not tested, which could be an important area for future research.",
            "29": "## Conclusion\n\nOverall, the paper makes a significant contribution to the field of gender-neutral machine translation by providing a new benchmark dataset, evaluating existing metrics, and proposing an innovative reference-free evaluation method.",
            "30": "While there are some limitations, such as the focus on a single language pair and the use of synthetic data, the strengths of the paper far outweigh these weaknesses.",
            "31": "The resources and insights provided by the authors will undoubtedly spur further research and development in creating more inclusive and unbiased MT systems."
        },
        "U2gOwybJgt": {
            "0": "Very useful test set.",
            "1": "Excellent introduction and motivation for the problem of gender neutral translation.",
            "2": "The background and survey are really nicely presented, as is the small-scale study surveying translator's preferences wrt neutral translations.",
            "3": "Overall,  a very strong case is made for working on the problem and for the GeNTE corpus.",
            "4": "The work is likely to be influential.",
            "5": "As the authors note in their limitations section,  the collection focuses on English->Italian translation,   and so the data and classifier models can't be used directly for other target languages.",
            "6": "Personally,  I'm ok with this,  as I think these subtle issues are best analyzed for specific target languages.",
            "7": "While this is a limitation, the paper also provides a starting framework for approaching other target languages.",
            "8": "I may have missed this in the discussion,   and I did not read the appendices,  but apart from a summary statement (line 366 - `the linguist supported the translators throughout the process and finally checked all the neutralizations')  there isn't much discussion about the quality of the data set that was produced.",
            "9": "It would be interesting to know whether there were challenges with inter-annotator agreement, i.e.",
            "10": "whether the linguist needed to intervene often,  or how often the annotators 'failed to neutralise' (Table 2, iii-F,  line 395)."
        },
        "1VGyJYsna9": {
            "0": "The paper focuses on a special case of gender bias across translations -- when the source gender is ambiguous or ungiven whereas the translation defaults to specific genders.",
            "1": "An open-source survey is conducted on 100 participants to assess how acceptable and preferable are the gender-neutral translations for the general audience.",
            "2": "The demographical details of the participants are illustrated in the Appendix.",
            "3": "The paper points out that 97.2% of segments collected from Europarl are biased toward gendered references in Italian.",
            "4": "Thus, GeNTE devotes the effort to manually correcting referent gendered and neutral sentences.",
            "5": "Some major concerns:\n- Gender neutrality assumption in the source language.",
            "6": "Though some nouns are lexically gender-neutral in English, document context could render these nouns gender-specific.",
            "7": "Relevant questions: 1) is context taken into consideration while filtering gender-neutral English sources?",
            "8": "2) did the survey include questions regarding source gender neutrality?",
            "9": "- Over-neutralization.",
            "10": "For specific nouns as in example C of Table 1, much less participants prefer the gender-neural alternative in Italian.",
            "11": "Is it necessary for these target translations to be gender-neutral?",
            "12": "Also in Table 2, why would we need to neutralize ii and iii for translations if the source English is already gendered?",
            "13": "- On multiple N references.",
            "14": "How frequently do the three translations overlap or are identical?",
            "15": "Have you considered multi-reference evaluations?",
            "16": "- Sentence vs. document level.",
            "17": "How are the sentence-level N-vs-G tags migrated, are they inherited directly from the document level?",
            "18": "- Problematic reference-free gender-neutral MT evaluation.",
            "19": "The binary N-G classification accuracy only evaluates the scenario when a gender-neutral noun is translated into Italian.",
            "20": "It does not provide any information on the overall translation quality of the documents/sentences thus not linearly dependent and cannot be compared proportionally to standard MT metrics, BLEU, TER, and METEOR.",
            "21": "Unfortunately, the much higher \"classifier\" number in Table 5 does not lead to the conclusion that the reference-free evaluation is \"promising\" (line 625).",
            "22": "Minor clarifications are necessary:\n- There is no concrete accuracy assessment on the two rounds of GPT-generated training data;\n- \"Grammatical gender languages\" is a confusing term in this paper.",
            "23": "Only concepts that have M-F-N cognates are examined in this paper, but not other nouns that have inherent gender in Italian, such as desks, beds, etc."
        },
        "DLMyO1oIbf": {
            "0": "The main strengths of this paper are: \n1.",
            "1": "It presents the first test set and benchmark for measuring how well MT systems do at appropriately generating gender-neutral language.",
            "2": "This will be a valuable resource for understanding this dimension of the translation task.",
            "3": "Before launching the evaluation task, they ran a study assessing the naturalness of language that had been edited to be gender neutral and how speakers reacted to the edited language.",
            "4": "This was an important step in justifying the subsequent work, showing that people typically appreciated the neutral language, and that it was not overly contorted.",
            "5": "They show that conventional MT metrics don't capture this dimension of the MT task well, and that other methods are needed.",
            "6": "They train a classifier that is effective at predicting the use of gender-neutral language.",
            "7": "The main weaknesses are:\n1.",
            "8": "The methods used to collect the test set data may not have been ideal.",
            "9": "E.g., they used a regex to find instances of gender-unambiguous language, including gendered pronouns.",
            "10": "However, in English, gendered pronouns are sometimes used in what should be gender-neutral circumstances (\"To each his own\").",
            "11": "They don't supply statistics about how difficult/natural/realistic it was to create the eval set.",
            "12": "I.e., we don't see how often the translators tasked with creating the eval set had difficulty in creating natural gender-neutral edited language, only that one example was so difficult to make gender neutral that only one of the translators attempted it.",
            "13": "Minor:\n1.",
            "14": "To create the benchmark set, they post-edited MT outputs to use more gender-neutral language, thus not a fully realistic scenario.",
            "15": "However, they discuss this in the limitations section, and it seems like a warranted compromise, given the extreme bias of MT systems to use gendered language.",
            "16": "The reference-free gender classifier doesn’t account for how good the translation is overall, and thus would not penalize translations that are incorrect other than their handling of gender."
        },
        "HqA9jA2kG3": {
            "0": "A thorough review of existing research in neutralization and gendered translations in MT.",
            "1": "A carefully constructed parallel corpus containing sentences that facilitate evaluation in both gender-ambiguous and gender-neutral scenarios.",
            "2": "A review of how existing reference-based metrics fare in detecting the quality of neutral translations, and their particular shortcomings.",
            "3": "For example, the fact that lower frequencies of neutral expressions affect the neural metrics more than the n-gram overlap-based ones, and the final finding that even n-gram metrics are not fully reliable at the sentence level.",
            "4": "A new, reference-free evaluation metric that accounts for the shortcomings of the reference-based ones.",
            "5": "The reasons for a new metric are well-motivated, but it is unclear how good the new metric is at gauging the quality of the resulting translation, aside from detecting whether a translation is gendered or not.",
            "6": "This new classification metric perhaps needs to be validated with the human study to make sure it's measuring the quality of translations as well.",
            "7": "Because of this reason, it is unclear what it means when the authors say \"the classifier outperforms\" (line 614).",
            "8": "This is something I might have misunderstood, in which case I welcome the authors to clarify further.",
            "9": "Although, I really appreciate the authors being honest about the thoughtful limitations section."
        }
    },
    "Jk6LA0NGOU": {
        "llm_review": {
            "0": "**Novel Approach**: The paper introduces LEAP, a novel system that incorporates explicit planning into the inference procedure of language models for logical reasoning.",
            "1": "This is a significant advancement as it allows the system to make more informed decisions by anticipating future outcomes.",
            "2": "**Performance**: The proposed system demonstrates superior performance compared to existing methods on multiple standard datasets.",
            "3": "Notably, the system performs competitively with GPT-3 despite being significantly smaller in size, and it outperforms chain-of-thought prompting on the challenging PrOntoQA dataset.",
            "4": "**Training Strategy**: The paper presents a robust training strategy that mitigates the issue of model exploitation, which is a common problem in planning-based systems.",
            "5": "This adversarial training approach ensures that the verification model remains reliable and accurate.",
            "6": "**Empirical Validation**: Extensive empirical studies are conducted to validate the effectiveness of explicit planning.",
            "7": "The results are comprehensive and show clear improvements over baseline methods.",
            "8": "**Interpretability**: The planning-based reasoning system is more interpretable, which is crucial for user-centric and safety-critical applications.",
            "9": "The system explicitly shows the reasoning steps, making it easier to understand and trust the decisions made by the model.",
            "10": "**Generalizability**: The system is designed to be general, allowing for the use of various pretrained language models as its components.",
            "11": "This flexibility makes the approach widely applicable across different tasks and datasets.",
            "12": "**Detailed Methodology**: The paper provides detailed explanations of the methodology, including pseudocode for the algorithms used.",
            "13": "This transparency is beneficial for reproducibility and for other researchers who may want to build upon this work.",
            "14": "**Weaknesses:**\n\n1.",
            "15": "**Computation Cost**: One of the main limitations of the proposed framework is its increased computational cost.",
            "16": "The explicit planning process requires significantly more computation compared to baseline methods, which may limit its practicality in resource-constrained environments.",
            "17": "**Dependency on Pretrained Models**: The system heavily relies on pretrained language models like T5 and GPT-3.5.",
            "18": "While this leverages the strengths of these models, it also means that the performance of LEAP is tied to the quality and capabilities of these pretrained models.",
            "19": "**Limited Evaluation on Diverse Datasets**: Although the system is evaluated on multiple datasets, the diversity of these datasets could be further expanded.",
            "20": "Evaluating the system on a wider range of logical reasoning tasks and domains would provide a more comprehensive understanding of its generalizability and robustness.",
            "21": "**Complexity of Implementation**: The detailed methodology, while thorough, also indicates a high level of complexity in implementation.",
            "22": "This could be a barrier for practitioners who may not have the expertise or resources to implement such a sophisticated system.",
            "23": "**Scalability**: The paper does not extensively discuss the scalability of the system to very large datasets or real-world applications.",
            "24": "It would be beneficial to understand how the system performs in more practical, large-scale scenarios.",
            "25": "**Evaluation Metrics**: The paper primarily uses accuracy and AUROC as evaluation metrics.",
            "26": "While these are standard, additional metrics such as precision, recall, and F1-score, especially in the context of logical reasoning, could provide a more nuanced evaluation of the system's performance.",
            "27": "**Future Work**: While the paper mentions potential future extensions, it could benefit from a more detailed discussion on how the current limitations could be addressed in future work.",
            "28": "This would provide a clearer roadmap for further improvements and research directions.",
            "29": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of logical reasoning with language models by introducing explicit planning.",
            "30": "The proposed system, LEAP, shows impressive performance improvements and offers a more interpretable reasoning process.",
            "31": "However, the increased computational cost and complexity of implementation are notable drawbacks.",
            "32": "Future work should focus on addressing these limitations and further evaluating the system's scalability and generalizability."
        },
        "VO4x9DK0Pr": {
            "0": "The idea of combining LM and planning is novel and meaningful.",
            "1": "The idea is supported by detailed description of the method and extensive experiments and analysis.",
            "2": "The method applies to both small models and large models, which is a practical advantage (although the cost may be higher than other methods).",
            "3": "This paper is not well-organized.",
            "4": "Much important information is presented in the appendix, like figures of models (fig.",
            "5": "6) and the results of analysis (fig.",
            "6": "8, 9).",
            "7": "There also seems to be two sections for related work (sec 3.4 and 5).",
            "8": "The baselines methods are limited.",
            "9": "For small models, the method is based on tuning, so comparing to prompted GPT-3.5 is not convincing.",
            "10": "Instead, the paper may include some tuning baselines for deductive reasoning like RuleTaker [1] and Neural Unification [2]."
        },
        "kggjBSDaju": {
            "0": "The method makes an interesting connection between logical reasoning and planning / contrastive learning\n2.",
            "1": "The empirical results are pretty strong - small models can match the performance of GPT-3 1.",
            "2": "The writing and presentation of the paper can be improved.",
            "3": "I didn't have any idea of the method until I really read through the method section.",
            "4": "The paper would benefit from first giving a high-level overview of the method at the beginning of the paper than just saying they do planning.",
            "5": "The method is quite complex, making people question whether this can really be deployed in the real-world.",
            "6": "However, I still think there is value in studying this.",
            "7": "Not too much of a concern for me."
        },
        "Nx2W4ShVJk": {
            "0": "The work presented is of high quality.",
            "1": "The paper is easy to read and the ideas are explained in a clear way.",
            "2": "All the necessary information to understand the method is provided.",
            "3": "The proposed method is particularly interesting and promising for tackling challenging planning and reasoning tasks, in particular the fact that the algorithm is model-agnostic and can be integrated with other LMs.",
            "4": "The authors provide very extensive experiments demonstrating the quality of their method.",
            "5": "The various ablation studies also clearly identify the proposed systems A and B as the reasons for the improved performance, highlighting precisely the contributions of each system.",
            "6": "The paper contains many references to the appendix as it contains important results that should be in the main paper, including a part of the conclusion.",
            "7": "The back and forth while reading can be detrimental to comprehension.",
            "8": "Fortunately, the information needed to understand the method and assess the claim is in the main content.",
            "9": "While the experiments are extensive, there is little comparison with existing methods.",
            "10": "It would be great to include a comparison with the state-of-the-art for each dataset beyond the single GPT-3.5.",
            "11": "Further comparison with other planning methods, such as those mentioned in the related work, might be interesting (as far as I understand, the only comparison is with the work of [Creswell et al., 2023])."
        }
    },
    "HewtRLig9V": {
        "llm_review": {
            "0": "Analysing the Potential of Large Language Models for Learning Conceptual Spaces\"**\n\n**Strengths:**\n\n1.",
            "1": "**Innovative Research Question**: The paper addresses a novel and intriguing research question about the potential of Large Language Models (LLMs) to learn conceptual spaces, which is a significant step forward in understanding the capabilities of these models beyond traditional NLP tasks.",
            "2": "**Comprehensive Evaluation**: The authors conduct a thorough evaluation across multiple domains, including taste and physical properties like mass, size, and height.",
            "3": "This broad scope provides a well-rounded view of the LLMs' capabilities and limitations.",
            "4": "**Comparison with Smaller Models**: The paper provides a detailed comparison between LLMs and smaller, fine-tuned models like DeBERTa and bi-encoders.",
            "5": "This comparison is valuable as it highlights that smaller models can sometimes outperform larger ones, which has important implications for the efficiency and practicality of deploying these models.",
            "6": "**Use of Real-World Datasets**: The use of real-world datasets, such as the taste ratings from Martin et al.",
            "7": "(2014) and the household mass dataset from Standley et al.",
            "8": "(2017), adds credibility to the findings and ensures that the experiments are grounded in practical, real-world scenarios.",
            "9": "**Detailed Methodology**: The paper provides a clear and detailed description of the methodologies used, including the prompts for LLMs and the fine-tuning process for smaller models.",
            "10": "This transparency allows for reproducibility and a better understanding of the experimental setup.",
            "11": "**Qualitative Analysis**: The inclusion of qualitative analysis, such as the specific examples of predictions made by the davinci model for sweetness, helps to illustrate the strengths and weaknesses of the models in a tangible way.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Exploration of Prompts**: While the paper acknowledges the importance of prompt selection, the exploration of different prompts seems limited.",
            "14": "Given the known sensitivity of LLMs to prompt phrasing, a more extensive investigation into various prompt formulations could provide deeper insights.",
            "15": "**Manual Post-Processing for ChatGPT and GPT-4**: The need for manual post-processing of the rankings provided by ChatGPT and GPT-4 due to issues like missing objects, paraphrasing, and duplicates is a significant limitation.",
            "16": "This manual intervention could introduce biases and affects the scalability of the approach.",
            "17": "**Filtered Training Data Analysis**: The analysis of the impact of filtered training data on the DeBERTa model's performance is insightful but somewhat limited.",
            "18": "A more detailed examination of how different types of properties (e.g., perceptual vs. functional) affect the model's learning could provide a more nuanced understanding.",
            "19": "**Generalization to Other Domains**: While the paper focuses on taste and physical properties, it remains to be seen how well the findings generalize to other perceptual domains.",
            "20": "Further experiments in additional domains would strengthen the conclusions.",
            "21": "**Dependence on Pre-trained Knowledge**: The paper highlights the dependence of smaller models on pre-trained knowledge from datasets like McRae and CSLB.",
            "22": "However, it does not fully explore the implications of this dependence, such as potential biases in these datasets and their impact on the models' performance.",
            "23": "**Limited Discussion on Practical Applications**: The paper could benefit from a more detailed discussion on the practical applications of learning conceptual spaces from LLMs.",
            "24": "For instance, how can these findings be leveraged in real-world AI systems, and what are the potential benefits and challenges?",
            "25": "**Conclusion:**\n\nOverall, the paper makes a significant contribution to the field by exploring the potential of LLMs for learning conceptual spaces.",
            "26": "It provides a comprehensive evaluation across multiple domains and offers valuable insights into the performance of both large and smaller models.",
            "27": "However, there are areas for improvement, particularly in the exploration of prompts, the handling of manual post-processing, and the generalization of findings to other domains.",
            "28": "Despite these limitations, the paper lays a strong foundation for future research in this area and opens up new avenues for leveraging LLMs in cognitive and perceptual tasks."
        },
        "98MsKVb8OF": {
            "0": "- The paper is a nice demonstration of the capability of LLMs to learn semantic properties of objects.",
            "1": "- The paper provides potentially useful data on which properties are easier or harder to learn (eg bitterness is harder than sweetness).",
            "2": "- Models are prompted in a careful way, using probabilities of completions rather than just a sample, and using a ranking and a pairwise comparison methodology.",
            "3": "- The results are straightforward to interpret.",
            "4": "- The findings are unlikely to move the needle for people who are unconvinced that LLMs can meaningfully learn semantics.",
            "5": "- The authors are not able to determine exactly why the LLMs succeed whereas smaller ones failed."
        },
        "wfqtzZU6tJ": {
            "0": "There is a lot of debate and discussion right now around if/how LLMs can encoded information about the nonlinguistic world.",
            "1": "Connecting LLMs to theory on conceptual spaces is interesting and a worthwhile direction.",
            "2": "The authors also introduce a new dataset on taste which seems interesting and others might like to study.",
            "3": "I have some concerns/confusion around both the experimental design and the theoretical connections the authors want to make.",
            "4": "Theory:\n* I am struggling a bit with how/why we should be interpreting the results of the experiments as evidence that LLMs encode conceptual spaces, and even more so with what we should actually require in order for something to count as having a conceptual space.",
            "5": "From my understanding of the Gardenfors work, this is a line of distributional semantics theory that is notable because the dimensions of the vectors correspond to sensory (symbolic) primatives.",
            "6": "It is often (as I have cited it and seen it cited) contrasted directly with distributional semantics models from NLP which are derived from text, because by-definition the dimensions of the space for text-based DSMs reflect word co-occurances and not perceptual primitives.",
            "7": "So, this is to say: in some ways, no matter what, LLMs just by-definition are not conceptual spaces.",
            "8": "What makes a conceptual space a conceptual space is that it is not derived from text.",
            "9": "That said, I think your project could still be enlightening if you could show that, e.g., the space that the LLMs learn is _isomorphic_ to the conceptual space, similar to the Abdou et al paper.",
            "10": "This seems like it would be a very interesting finding.",
            "11": "But from what I can gather from your experiments, we cannot conclude that, since the experiments themselves just measure word co-occurances/LM probabilities directly.",
            "12": "Thus, I think from the presented experiments, all we can conclude is that LLMs representations are correlated with perceptual features.",
            "13": "This isn't necessarily news, but still could be worth documenting.",
            "14": "However, I would like to see the paper rewritten with more nuance around the conceptual spaces theory, especially if published without changes to the experiments.",
            "15": "Experimental design:\n* The methods you use to get rankings are arguably not measures of the representations directly (as you claim in the intro).",
            "16": "Rather, all your experiments rely on e.g., LM continuations or perplexity scores.",
            "17": "These don't tell us much about the representation of a word itself (e.g., the representation of the word \"cheese\") but rather tell us about how the model uses the representation to perform the task of language modeling.",
            "18": "To make claims about the representations themselves, e.g., to try to show an isomorphism between the LLM space and the conceptual space, I think you need to work with the word representations themselves.",
            "19": "This is not trivial in a multilayered contextualized LM (the way it used to be easy to do with e.g., word2vec).",
            "20": "But I think you could find a way.",
            "21": "Then, you might consider using something like relational similarity analysis (RSA) in order to measure whether the LLM space and the conceptual space have similar geometries.",
            "22": "* Roberta performs well when fine-tuned on McRae.",
            "23": "Have you looked at whether there is overlap between the facts included in McRae and those included in your test sets?",
            "24": "I wouldn't be shocked if McRae contains some facts about size or taste..."
        },
        "PcHCB6iwnY": {
            "0": "* This paper presents an interesting exploration of yet unexplored conceptual spaces.",
            "1": "I like this human-inspired analysis direction.",
            "2": "* The authors focus on an interesting commonsense problem that suffers deeply from reporting bias, and show where it is possible to trust LLMs.",
            "3": "* The paper is mostly well-written.",
            "4": "* The choice of models is satisfactory.",
            "5": "* Table 1: Some error analysis due to the surprising results would be much better than just stating them and acknowledging they are indeed surprising.",
            "6": "* As I did not fully understand Table 2, I cannot judge it.",
            "7": "* The conclusion \"The key to achieving good results with such smaller models is to have access to suitable training data.\"",
            "8": "is rather trivial.",
            "9": "I wonder what is the real contribution of this paper besides being an interesting analysis."
        }
    },
    "rXn9WO4M2p": {
        "llm_review": {
            "0": "**Novelty and Contribution**: The paper introduces a novel method, PRESENCE, for reweighting pre-training data samples using self-influence (SI) scores.",
            "1": "This is a significant contribution as it addresses the gap in model-driven reweighting for pre-training data, which has not been explored extensively before.",
            "2": "**Comprehensive Analysis**: The authors provide an extensive analysis of the relationship between SI scores and sample quality.",
            "3": "They demonstrate that SI scores can effectively identify noisy and domain-mismatched samples, which is crucial for improving the quality of pre-training data.",
            "4": "**Two-Staged Reweighting Strategy**: The two-staged reweighting strategy proposed in the paper is innovative.",
            "5": "By emphasizing high SI score samples in the initial stage and de-emphasizing them in the later stage, the method promotes novelty and stability in model pre-training.",
            "6": "**Empirical Validation**: The paper includes thorough empirical validation across multiple model sizes, datasets, and tasks.",
            "7": "The results consistently show that PRESENCE outperforms baseline models trained on randomly sampled data or SI score-based filtered data.",
            "8": "**Scalability**: The adaptation of PRESENCE for large-scale pre-training using microbatch gradient reweighting is a practical approach.",
            "9": "It ensures that the method can be applied to large datasets and models without significant computational overhead.",
            "10": "**Detailed Experimental Setup**: The paper provides detailed descriptions of the pre-training and fine-tuning setups, making it easier for other researchers to replicate the experiments.",
            "11": "#### Weaknesses\n\n1.",
            "12": "**Computational Overhead**: While the paper claims that PRESENCE is computationally efficient compared to sequential filtering methods, it still introduces a significant overhead.",
            "13": "The training time is increased by 30%, which might be a concern for large-scale pre-training tasks.",
            "14": "**Hyperparameter Sensitivity**: The two-staged reweighting strategy relies on specific hyperparameters (τ1, τ2, and I).",
            "15": "The paper does not provide a detailed analysis of how sensitive the method is to these hyperparameters or how they were chosen.",
            "16": "This could make it challenging to apply the method to different datasets or models without extensive hyperparameter tuning.",
            "17": "**Limited Layer Usage for SI Calculation**: The SI scores are calculated using only the first layers of the encoder and decoder for computational optimization.",
            "18": "While this is practical, it might not capture the full complexity of the model's learning dynamics.",
            "19": "Using more layers could potentially provide more accurate reweighting signals.",
            "20": "**Lack of Automated Reweighting Strategy**: The paper mentions the potential for automated reweighting strategies using temperature scaling schedules but does not explore this in detail.",
            "21": "An automated approach could further enhance the method's applicability and reduce the need for manual tuning.",
            "22": "**Evaluation on More Diverse Tasks**: The paper evaluates PRESENCE on a set of standard NLP tasks.",
            "23": "However, it would be beneficial to see its performance on more diverse and challenging tasks, such as those involving multimodal data or more complex reasoning.",
            "24": "**Impact of Model Maturity on SI Scores**: The paper briefly mentions the relationship between model maturity and the reliability of SI scores.",
            "25": "A more in-depth analysis of how the model's training progress affects the SI scores and their effectiveness in reweighting would strengthen the paper's claims.",
            "26": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of language model pre-training by introducing a novel data reweighting method based on self-influence scores.",
            "27": "The comprehensive analysis, innovative two-staged reweighting strategy, and empirical validation across multiple settings are commendable.",
            "28": "However, the method's computational overhead, sensitivity to hyperparameters, and limited exploration of automated reweighting strategies are areas that could be improved.",
            "29": "Despite these weaknesses, PRESENCE represents an important step towards more efficient and effective pre-training of language models."
        },
        "tbtyFtVBhA": {
            "0": "- Using gradient norm/self-influence for weighting the samples during pre-training is a novel idea for NLP\n- The paper is generally well-written and easy to follow\n - Lack of comparison to other pre-training data filtering methods.",
            "1": "- Significant pre-training computation overhead.",
            "2": "- Detailed explanations for the above two reasons to reject: PRESENCE is essentially a data filtering method, so I expect to see the comparison with some baselines that filter the pre-training data.",
            "3": "**The current paper only shows that PRESENCE works, but it does not show it is better than other methods**.",
            "4": "As the authors admitted in the Limitation, PRESENCE induces 30% pre-training computation overhead.",
            "5": "While the paper says that \"`we believe the training overhead of PRESENCE is significantly lesser compared to the overhead of existing methods such as offline filtering`\", this is not justified by any experiments and numbers.",
            "6": "Moreover, offline filtering methods can be done once, and the filtered datasets can be used to train any models, while PRESENCE (the online version) needs to be applied every time when pre-training a model.",
            "7": "This makes PRESENCE less unlikely to be applied in reality.",
            "8": "- Some experiment results cannot convince me that PRESENCE is good enough.",
            "9": "The relationship between temperature $\\tau$ and the downstream performance shown in Figure 5 does not seem to justify the two-stage pre-training.",
            "10": "It seems that using a positive or negative $\\tau$ can yield improvement over no reweighting.",
            "11": "This makes me curious if it is really important to reweight using a positive or negative $\\tau$.",
            "12": "The results from Table 4 (cross-lingual zero-shot transfer) also do not show the advantage of using PRESENCE.",
            "13": "- It is unclear why this paper selects multilingual transferability as the downstream task for evaluation.",
            "14": "In the mT5 paper, they also show the results on SQuAD.",
            "15": "I wonder if PRESENCE only works for multilingual PLMs and cross-lingual transfer downstream tasks."
        },
        "Xy7l8N861Z": {
            "0": "Data reweighting is an important research problem for the pre-training language models, especially in today's emergence of large LM models.",
            "1": "The experimental study is comprehensive, and the results are promising.",
            "2": "The SI score is used to filter out noisy samples for pre-training dataset selection is practicable.",
            "3": "The source code has yet to be made available in the reviewed version, preventing me from verifying the effectiveness of this method through the code.",
            "4": "I am also uncertain whether the code will be made public after accepting the paper.",
            "5": "The method's current process is undeniably intricate, entailing the optimization of multiple pipelines.",
            "6": "These pipelines encompass sophisticated components, such as a two-phase learning strategy for online adaptation and a two-stage offline filtering procedure.",
            "7": "By embracing this sophisticated methodology, we doubt its  potential  availability in real-world applications.",
            "8": "(minor)  More than involving three tasks and five datasets is required to verify the effectiveness of this method.",
            "9": "I suggest expanding to more General Language Understanding Evaluation (GLUE) tasks."
        },
        "dNEz1YyPxD": {
            "0": "The paper explores the area of training on high quality samples, which helps with improved performance on downstream tasks.",
            "1": "Various sizes of the model were used for experimentation to demonstrate impact of the quality framework on the models.",
            "2": "The experimentation is limited to T5 based models, it will useful to carry out similar experimentation on other LMs"
        }
    },
    "54WhV6RTzi": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel two-step approach to radiology report generation that separates content extraction from style generation.",
            "1": "This is a significant improvement over traditional methods that directly generate reports from images, which often conflate content and style, leading to inaccuracies.",
            "2": "**Use of RadGraph**: Leveraging RadGraph for content extraction is a strong point.",
            "3": "RadGraph provides a structured representation of the clinical content, which helps in focusing the model on clinically relevant information.",
            "4": "This is reflected in the improved performance metrics.",
            "5": "**Few-Shot Prompting with LLMs**: The use of large language models (LLMs) like GPT-3.5 for style generation through few-shot prompting is a clever strategy.",
            "6": "It allows the model to adapt to the specific style of individual radiologists with just a few examples, making the generated reports indistinguishable from those written by humans.",
            "7": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of the proposed method, including both quantitative metrics (RadGraph F1, CheXbert similarity, BLEU, BERT Score) and human evaluation by clinical experts.",
            "8": "This multi-faceted evaluation strengthens the validity of the results.",
            "9": "**Human Style Evaluation**: The human evaluation component, where clinical experts were unable to distinguish between AI-generated and human-written reports, is particularly compelling.",
            "10": "It demonstrates the effectiveness of the style generation step and the potential for real-world application.",
            "11": "**Ethical Considerations**: The paper addresses ethical considerations, particularly the responsible use of medical data and the risks associated with LLMs generating false or misleading content.",
            "12": "This shows a conscientious approach to the deployment of AI in clinical settings.",
            "13": "**Weaknesses:**\n\n1.",
            "14": "**Reliance on RadGraph Accuracy**: The approach heavily relies on the accuracy of the RadGraph extraction model.",
            "15": "Any errors in the initial extraction step could propagate through the system, potentially affecting the final report's quality.",
            "16": "The paper could benefit from a more detailed discussion on how to mitigate these errors.",
            "17": "**Financial and Reproducibility Concerns**: The use of third-party services like Azure OpenAI for LLMs introduces financial costs and potential reproducibility issues.",
            "18": "The paper acknowledges this but does not provide a clear solution.",
            "19": "Future work could explore more cost-effective and reproducible alternatives.",
            "20": "**Limited Scope of Evaluation Metrics**: While the paper uses a variety of metrics, it could benefit from additional clinical relevance metrics beyond RadGraph F1 and CheXbert similarity.",
            "21": "Including more diverse clinical metrics could provide a more comprehensive evaluation of the model's performance.",
            "22": "**Generalization to Other Modalities**: The paper focuses on chest X-rays and does not address the generalizability of the approach to other imaging modalities like mammograms or MRIs.",
            "23": "Future work should explore the applicability of this method to a broader range of medical images.",
            "24": "**Potential for Overfitting to Style**: The few-shot prompting approach for style generation, while effective, might lead to overfitting to the specific examples provided.",
            "25": "The paper could discuss strategies to ensure the model generalizes well to new styles without requiring extensive retraining.",
            "26": "**Limited Discussion on Model Interpretability**: The paper does not delve deeply into the interpretability of the generated reports.",
            "27": "Ensuring that the generated reports are not only accurate but also interpretable by clinicians is crucial for real-world adoption.",
            "28": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of automated radiology report generation by disentangling content and style.",
            "29": "The innovative use of RadGraph and few-shot prompting with LLMs shows great promise.",
            "30": "However, addressing the identified weaknesses, particularly around RadGraph accuracy, financial costs, and generalizability, will be crucial for the broader adoption and impact of this approach."
        },
        "Rfxegx1Ned": {
            "0": "The proposed approach is sound and the utilization of RadGraph is well-motivated.",
            "1": "The idea is interesting.",
            "2": "The beneficial performance shown in the evaluations validates the effectiveness of the proposed approach.",
            "3": "Several claims should be proved.",
            "4": "- The work argues that existing methods, which generate reports directly from images, may lead to clinically inaccurate reports due to conflating content and style.",
            "5": "Could you give some examples or evidence to prove it?",
            "6": "- Why is it more accurate to write a report that matches a physician's style?",
            "7": "What is the advantages of usefulness of matching a physician's style?",
            "8": "More model details should be provided and clarified.",
            "9": "- How to serialize the RadGraph into a condensed summary of the clinical content of the report?",
            "10": "- How to ensure/define the order of entities in the generated Serialized RadGraph?",
            "11": "- For example, in Figure 2, how to obtain the \"acute cadiopulmonary process\"?",
            "12": "How to obtain the \"no\"?",
            "13": "- How to obtain the Serialized RadGraph from the Graph?",
            "14": "- What is the purpose of introducing a text encoder?",
            "15": "If I understand correctly that the input is just the image, what and where is the input text?",
            "16": "- What is the full input of GPT-3.5 under both few-shot and zero-shot settings?",
            "17": "Could you give an example to illustrate the full input and full output of GPT-3.5?",
            "18": "- How many examples are used to prompt GPT-3.5?",
            "19": "More experimental details should be provided and clarified.",
            "20": "- Could you give more details about the Image to Serialization task?",
            "21": "What are the input and output of this task?",
            "22": "What is the meaning of comparing the content of the generated serialization with the ground-truth report?",
            "23": "Why is only the F1 score calculated?",
            "24": "What are the results of other metrics, e.g., BELU?",
            "25": "Therefore, it is important to clarify what are the input and output of this task.",
            "26": "- In Table 2, what is the baseline model?",
            "27": "Pure Transformer or R2Gen?",
            "28": "How many Transformer layers and the number of parameters are used in the baseline?",
            "29": "- Besides, while the paper reports beneficial performance, a detailed comparison with more existing image-to-report methods is missing.",
            "30": "Including such a comparison would provide a clearer understanding of the proposed approach's advantages.",
            "31": "- When evaluating the AI-generated reports, are the physicians given the test set CXR images?",
            "32": "- The criteria used in human study is subjective.",
            "33": "Authors are encouraged to introduce more information about three clinicians, i.e.",
            "34": "working experience and expert title.",
            "35": "Senior and junior clinicians may find differences when considering whether the same report is AI-generated or not.",
            "36": "- I strongly recommend the author to further evaluate the quality of the generated reports, instead of asking the physician to judge whether the report is AI-generated or not."
        },
        "1E3BHKUaPU": {
            "0": "The authors attempted to address the issue of clinical report generation from a new perspective in their methodology, and they achieved results indicating that ChatGPT can reference style well enough to generate reports that are indistinguishable from those written by humans.",
            "1": "This study doesn't persuasively explain \"why style should be considered in the medical domain\" as proposed by this study, and it doesn't sufficiently explore the clinical importance of such a consideration.",
            "2": "Experimental results lack comparisons with other studies.",
            "3": "(It's necessary to verify statistically significant results by conducting repeated experiments on the same data set.)",
            "4": "While this study incorporates a few ideas and uses RadGraph and ChatGPT, it does not offer considerable novelty in its methods or contribute significantly to the field."
        },
        "Ra70Uv3Ivg": {
            "0": "\n- The authors propose a straightforward yet effective approach that involves converting raw reports into structured data.",
            "1": "This process aids in the removal of non-semantic content.",
            "2": "- They also introduce a style-aware report generation technique and carry out a human evaluation to assess its effectiveness.",
            "3": "- Easy to follow - Is there no loss of information from the Serialization to Report process?",
            "4": "What are your thoughts on the reason behind the slight difference in performance, with a RadGraphF1 of 0.221 ± 0.004 for Table1 and 0.228 ± 0.004 for Table2?",
            "5": "- In the Image Encoder section (Section 3.4), it is mentioned that if there are multiple images in one study, they are aggregated.",
            "6": "It would be beneficial to evaluate the test performance based on the number of images present in each study, showcasing the performance for cases where a study contains one image, two images, and so on.",
            "7": "- In the Content Generation Model section (Section 3.4), it is mentioned that the Text Encoder receives the clinical document as input, but the specific details are not provided.",
            "8": "- It would be valuable to experiment with both the Baseline and the Proposed Model without the Text Encoder and share the results.",
            "9": "This paper does not propose a new architectural design but shows the effectiveness of serialized data.",
            "10": "Given this focus, I think that showcasing results across various scenarios would be valuable to illustrate the impact more comprehensively.",
            "11": "- In the 4.2 Serialization to Report experiment, the zero-shot performance is observed to be 0.722 RadGraphF1.",
            "12": "Despite using ground-truth serialization, the performance is not nearly perfect.",
            "13": "Is it plausible to attribute this discrepancy to the fact that the performance of the content extractor (Report -> RadGraph preprocessing) might not be flawless?"
        }
    },
    "wWFWwyXElN": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper presents a novel approach to leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning tasks.",
            "1": "This is particularly valuable given the scarcity of training data in many languages.",
            "2": "**Comprehensive Evaluation**: The authors conduct extensive experiments using multiple LLMs (Dolly-v2, StableVicuna, ChatGPT, and GPT-4) and evaluate their performance on three different datasets (XCOPA, XWinograd, and XStoryCloze).",
            "3": "This thorough evaluation provides a clear picture of the effectiveness of different LLMs in data augmentation.",
            "4": "**Detailed Analysis**: The paper includes a detailed analysis of the generated data, including human evaluations of text naturalness and logical coherence.",
            "5": "This adds depth to the study and helps in understanding the strengths and limitations of the generated data.",
            "6": "**Practical Contributions**: The authors fine-tune smaller multilingual models (mBERT and XLMR) using the synthesized data and demonstrate significant improvements in cross-lingual performance.",
            "7": "This practical contribution is valuable for the NLP community, especially for those working with low-resource languages.",
            "8": "**Public Release of Data**: The authors release the generated data for public use, which promotes transparency and reproducibility.",
            "9": "This is a commendable practice that benefits the broader research community.",
            "10": "**Human Evaluation**: The inclusion of human evaluation to assess the naturalness and logical coherence of the generated examples across different languages is a strong point.",
            "11": "It provides qualitative insights that complement the quantitative results.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Limited Language Coverage**: While the paper covers a diverse set of languages, it still leaves out many low-resource languages.",
            "14": "The effectiveness of the proposed approach in truly low-resource settings remains to be fully explored.",
            "15": "**Dependency on LLMs**: The approach heavily relies on the availability and performance of LLMs like GPT-4, which are not open-source and can be costly to use.",
            "16": "This limits the accessibility of the proposed method for researchers with limited resources.",
            "17": "**Quality of Generated Data**: The paper acknowledges that the quality of the generated data varies across languages, with some languages like Tamil showing poor performance.",
            "18": "This highlights a limitation in the robustness of LLMs for certain languages, which needs further investigation.",
            "19": "**Scalability Concerns**: The paper demonstrates improvements with larger datasets, but the scalability of generating and fine-tuning on such large datasets might be a concern for practical applications, especially in resource-constrained environments.",
            "20": "**Instruction Sensitivity**: The effectiveness of data generation is sensitive to the instructions provided to the LLMs.",
            "21": "The paper mentions iterative refinement of instructions, but a more systematic approach to optimizing instructions could be beneficial.",
            "22": "**Evaluation Metrics**: The paper primarily uses accuracy as the evaluation metric.",
            "23": "While this is a standard metric, additional metrics such as F1-score, precision, and recall could provide a more comprehensive evaluation of model performance.",
            "24": "**Ethical Considerations**: The paper briefly mentions ethical considerations but does not delve deeply into the potential biases and ethical implications of using LLMs for data generation.",
            "25": "A more thorough discussion on this aspect would strengthen the paper.",
            "26": "**Conclusion:**\n\nOverall, the paper presents a promising approach to enhancing cross-lingual performance through LLM-powered data augmentation.",
            "27": "The comprehensive evaluation and practical contributions are commendable.",
            "28": "However, there are areas for improvement, particularly in addressing the limitations related to language coverage, dependency on proprietary LLMs, and ethical considerations.",
            "29": "Future work could focus on expanding the approach to more low-resource languages, optimizing instructions systematically, and exploring additional evaluation metrics."
        },
        "mGkOBEKrQ4": {
            "0": "\n1.",
            "1": "The paper focuses on an important research problem in NLP: multilingual commonsense reasoning.",
            "2": "In the last few years, large language modeling research has primarily focused on English.",
            "3": "It's important to see how the benefits from English large language models can be used in other lower resource languages, and this paper does a good job studying this research question on commonsense reasoning tasks.",
            "4": "This paper has a comprehensive experimental setup, and experiments with 3 commonsense reasoning datasets, three multilingual models, and experiments with data generated by 3 large language models.",
            "5": "The paper has experiments generating English examples (zero-shot cross lingual generalization), translated examples, as well as examples in the language itself.",
            "6": "The paper also has some good analysis with human evaluation experiments on the quality of the generated examples.",
            "7": "This paper is very well written, and easy to follow.",
            "8": "The authors extensively discuss and are upfront about the limitations of their work.",
            "9": "I am wondering what is the effect of these data augmentation methods when the base model is made larger / more powerful.",
            "10": "The current experiments focus on fairly small multilingual models (mBERT / XLM), which are <500M parameters.",
            "11": "It would be interesting to see how these results generalize to larger models like mT5 or BLOOM.",
            "12": "This is important because it's possible that these larger models need lesser examples to generalize, and data augmentation will not as useful then.",
            "13": "A related point to weakness #1, the paper is missing baseline experiments which use the large language models themselves to perform the task, through few-shot learning or fine-tuning on original data using the GPT3 fine-tuning API (please me know in case I missed these experiments while reading the paper!).",
            "14": "Since models like ChatGPT / GPT-4 are effectively used to create training data for downstream fine-tuning, I'm suspecting they can already perform the task pretty well?",
            "15": "This has been acknowledged by the authors in their limitations section, but there are several commerical restrictions on using ChatGPT/GPT4 outputs, [1] says \"one may not use output from the Services to develop models that compete with OpenAI\".",
            "16": "This may restrict the applicability of the proposed approach for downstream applications.",
            "17": "In future versions of the paper, it will be helpful to see how stronger instruction-tuned / aligned open LLMs like LLAMA 2 [2] or TULU [3] can help with the data augmentation.",
            "18": "However, note that both these models came out in the last two months (during / after EMNLP submission period), so this is not a valid weakness for the current version of the paper.",
            "19": "Finally, in the long term I am not very convinced by using one language model \n\n[1] - https://openai.com/policies/terms-of-use  \n[2] - https://huggingface.co/blog/llama2  \n[3] - https://arxiv.org/abs/2306.04751"
        },
        "0D3s3Do2fs": {
            "0": "The paper is well-motivated and studies an important problem of performing data generation in multilingual, low-resource settings for a challenging task of commonsense reasoning.",
            "1": "Although the general-purpose LLMs exhibit impressive zero-shot learning capabilities, smaller task-specific models can often be more accurate, cost-effective, and practical, given that enough training data is available.",
            "2": "Therefore, improving the accuracy of task-specific models by generating synthetic data for fine-tuning is still an important research topic.",
            "3": "One of the weaknesses of the presented approach is that it does not provide a mechanism to control the diversity of the synthesized examples.",
            "4": "Although the authors claim that their method generates diverse examples (lines 61-62), it has neither been supported empirically nor discussed in the paper.",
            "5": "Notably, the results presented in Section 5.3 suggest that the benefits of generating larger amounts of data are rather limited, possibly due to the low diversity of the synthesized examples.",
            "6": "In addition, the clarity of the paper needs to be improved.",
            "7": "Specifically, the authors generated 2-4k new data points to fine-tune the models for each data set.",
            "8": "It is unclear whether all instances were synthesized in one inference step or the models were prompted multiple times for each data set.",
            "9": "Moreover, given that the number of valid examples obtained with different models differs substantially (see the success rates in Table 3), it needs to be clarified why an equal number of synthesized data points is reported for each LLM used for generation.",
            "10": "Presumably, the models were prompted multiple times until the required number of examples was reached, but the paper needs to explain this.",
            "11": "Generating small batches of examples by prompting the model multiple times, each time using a different set of seed examples, might also improve the diversity and, in turn, the quality of the synthesized data.",
            "12": "Another point is that the choice of the number of synthesized samples per data set should be discussed in the paper.",
            "13": "The number of examples is neither fixed nor proportional to the size of the corresponding original data set.",
            "14": "For XWinograd, which has the largest training set out of the examined benchmarks, additional 2k examples are generated, which doubles its size.",
            "15": "In contrast, the size of XCOPA and XStoryCloze increases 10 and  6.7 times, respectively.",
            "16": "Finally, the authors need to discuss their method's effectiveness in comparison with other data augmentation approaches.",
            "17": "Note that after deduplication and discarding invalid instances, only up to 42% of examples are retained (in the case of open-access models).",
            "18": "The usage of closed generative LLMs might also not be cost-effective.",
            "19": "The reader would benefit from a more in-depth discussion of the advantages and disadvantages of the proposed method.",
            "20": "The limitations section needs to be updated accordingly."
        },
        "yecuuLWASQ": {
            "0": "- well written paper, with well defined methodology and strong evaluation\n- promising experimental results\n- resources and findings that are useful to the community None, it's a very solid paper."
        }
    },
    "qo17ZiVnH2": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel framework that leverages the reasoning capabilities of Large Language Models (LLMs) to proactively generate questions that fill in the missing details from image-to-text conversions.",
            "1": "This is a significant step forward in addressing the information gap in Visual Question Answering (VQA) tasks.",
            "2": "**Model Agnostic Framework**: The proposed method is model agnostic, meaning it can be applied to various LLMs and Vision-Language Models (VLMs).",
            "3": "This flexibility is a strong point as it allows the framework to be used across different systems and datasets.",
            "4": "**Performance Improvement**: The method shows consistent improvements over baseline methods on OK-VQA and A-OKVQA datasets.",
            "5": "The average gain of 2.15% on OK-VQA and 1.25% on A-OKVQA is a notable achievement, demonstrating the effectiveness of the approach.",
            "6": "**Detailed Analysis**: The paper provides a thorough analysis of the results, including ablation studies and case studies.",
            "7": "This helps in understanding the impact of different components of the framework and the reasons behind the performance improvements.",
            "8": "**Comprehensive Experiments**: The authors conduct extensive experiments across different LLMs and VLMs, ensuring that the proposed method is robust and generalizable.",
            "9": "The inclusion of various baselines and comparison with state-of-the-art methods adds credibility to the results.",
            "10": "**Addressing Ambiguity and Inaccuracy**: The framework not only fills in missing details but also rectifies inaccurate information and minimizes ambiguity in the converted textual information.",
            "11": "This is crucial for improving the reliability of VQA systems.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Complexity of Implementation**: The proposed framework involves multiple steps, including question generation, summarization, and refinement.",
            "14": "This complexity might make it challenging to implement and integrate into existing systems, especially for practitioners who are not experts in the field.",
            "15": "**Dependence on Pre-trained Models**: The method relies heavily on pre-trained LLMs and VLMs, which might not be accessible or feasible for all researchers due to computational and resource constraints.",
            "16": "This could limit the applicability of the framework in practice.",
            "17": "**Limited Scope of Evaluation**: While the paper demonstrates improvements on OK-VQA and A-OKVQA, it does not explore the applicability of the framework to other vision-language tasks that require external knowledge, such as visual commonsense reasoning.",
            "18": "This limits the generalizability of the findings.",
            "19": "**Potential for Noise Introduction**: The process of generating and refining questions and answers could introduce noise, especially if the generated questions are not highly relevant or if the VLMs provide incorrect answers.",
            "20": "Although the refinement module aims to address this, there is still a risk of noise affecting the final performance.",
            "21": "**Scalability Concerns**: The framework's performance might be affected by the scalability of the question generation and refinement processes.",
            "22": "As the number of generated questions increases, the computational cost and complexity also rise, which could be a bottleneck for large-scale applications.",
            "23": "**Lack of Real-world Application Examples**: The paper primarily focuses on benchmark datasets and does not provide examples of real-world applications where the proposed method could be beneficial.",
            "24": "Including such examples could strengthen the practical relevance of the work.",
            "25": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of Visual Question Answering by introducing a framework that leverages LLMs to proactively ask questions and fill in missing details.",
            "26": "The method shows consistent improvements over baselines and addresses key challenges such as information gaps, inaccuracies, and ambiguities.",
            "27": "However, the complexity of implementation, dependence on pre-trained models, and potential scalability issues are notable concerns.",
            "28": "Future work could focus on simplifying the framework, exploring its applicability to other tasks, and providing real-world application examples to enhance its practical relevance."
        },
        "iL0PiBaWya": {
            "0": "This paper convincingly illustrates the necessity of providing nuanced, query-specific information to complement the coarse, generalized image captions diretly output from caption model without any query information.",
            "1": "It recognizes a shortcoming in existing approaches that convert image to text for application in large language models to solve visual question-answering tasks.",
            "2": "The one-off conversion process often loses important details, underscoring the importance of query-related information to infuse sufficient visual details into the Language Learning Model .",
            "3": "The proposed framework is thoughtfully and intuitively designed, reflecting an in-depth understanding of the problem at hand.",
            "4": "The research is supported by thorough and comprehensive experiments.",
            "5": "It includes a comparative analysis with most related works, and an exhaustive ablation study, all of which affirm the effectiveness of the proposed method.",
            "6": "The study presents a commendable motivation and idea, yet the implementation of the method seems disproportionately complex given the relatively marginal improvement.",
            "7": "The method encompasses several modules, including a caption model, VQA model, filter, and large language models.",
            "8": "However, the advancement over 'Prophet' does not appear to warrant the complexity and increased inference time of the method.",
            "9": "The articulation of the method and experiments is not clear.",
            "10": "For instance, (1) at line 214, the method for calculating scores in Table 1 is unclear; (2) lines 264-265 lack sufficient detail, relegating crucial information to the appendix and leaving the motivation and methods for obtaining ground-truth VQA accuracy unclear; in Appendix A, P_{i,q} and ACC_{soft} aren't sufficiently explained; (3) at lines 275-276, it is unclear how z_q is calculated.",
            "11": "While Equation 8 appears to use all information, it remains unexplained if only 'q' is used to get z_q; (4) in Section 3.3, the example prompt is not consistent with Figure 2, where the figure does not display `caption`; (5) the 'Prophet' results in Tables 2 and 3 are inconsistent; (6) for Table 4, the application of answers from 'Blip2' remains unclear.",
            "12": "Are they used for few-shot examples?",
            "13": "If so, how does 'ourts-w/o' function?",
            "14": "(7) Table 4 should provide an explanation for the meaning of 'n' and 'E' in its caption; (8) the statement at lines 414-415 claiming an improvement from 40.7 to 47.63 seems unreasonable, as 40.7 directly uses 'Blip2', a VLM, while 47.63 uses a different method involving LLM.",
            "15": "The improvement could be attributed not just to more detailed information but also enhanced reasoning ability.",
            "16": "In the case study, Figure 3 (1) suggests that 'Prophet's' output of 'grilled cheese' with a probability of 0.66 should indeed be the correct answer"
        },
        "FQmhsFMBJx": {
            "0": "- Interesting design choice requiring very minimal training of interfacing LLM and VLMs, instead of training adapters for image input to llm like llava etc.",
            "1": "- The accuracies on the VQA datasets look promising.",
            "2": "- They show their approach holds with various LLMs.",
            "3": "- The results section could be organized better.",
            "4": "Some lines like “Line (f) converts line (h) to few-shot setting” - are lacking details.",
            "5": "What does it exactly mean?",
            "6": "Fewer context?",
            "7": "Fewer VQA training data points?",
            "8": "How are they sampled?",
            "9": "- Another baseline to include is the accuracy of the LLM to answer the question without any image information or just the caption.",
            "10": "Table 6 compares the accuracy to using BLIP-2 in answering the question.",
            "11": "Since their final method has the LLM answering the question, it would be nice to see how much the BLIP and filtered information is helping the LLM."
        },
        "yB2wLxzrDs": {
            "0": "The idea of this paper is relatively interesting, proposing a method that leverages the interaction between LLM (Language Large Models) and VLM (Vision-Language Models) to obtain finer-grained image information, which in turn aids visual reasoning tasks.",
            "1": "It offers the research community a new perspective on interactive use with large models.",
            "2": "Simultaneously, it addresses, to some extent, the current limitation of LLMs being unable to access visual information.",
            "3": "The experiments in the article are comprehensive, with rich visualization results.",
            "4": "These contents validate the points proposed in the paper.",
            "5": "The performance comparisons and ablation studies demonstrate the efficacy of the proposed method.",
            "6": "Furthermore, tasks like OKVQA and AOKVQA are inherently challenging.",
            "7": "The method presented in this paper achieves competitive results on these tasks, further underscoring its effectiveness.",
            "8": "The method put forward in the paper is not only effective but also intuitive, easy to replicate, and clear-cut.",
            "9": "Generally speaking, I believe the novelty is somewhat lacking.",
            "10": "For instance, earlier this year (published on arxiv on March 12th, more than three months before the EMNLP deadline), there was a paper titled \"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions\" that employed an approach similar to what you've proposed, aiming to capture more visual information to produce image captions with richer details.",
            "11": "Admittedly, in terms of the overall workflow, the structure of your paper seems more comprehensive, encompassing broader considerations.",
            "12": "However, on the whole, I don't feel surprised when I read your paper, as the core content bears resemblances to previous work.",
            "13": "I believe you should explore one or two additional vision-language tasks.",
            "14": "As you've acknowledged in the limitation section, your experiments solely focus on OKVQA and AOKVQA tasks.",
            "15": "Essentially, these two tasks are quite similar, with AOKVQA being a more advanced version of OKVQA.",
            "16": "Based on the results in Table 3, the performance of OKVQA Prophet (ensemble) only improves by a mere 0.2% with the addition of your proposed method, which hardly convinces me of its efficacy in this context.",
            "17": "A 0.2% increment could even be attributed to minor experimental errors, rendering the results somewhat dubious.",
            "18": "Hence, rather than tackling both AOKVQA and OKVQA, it would be more sensible to solely concentrate on AOKVQA.",
            "19": "On the one hand, the outcomes from Table 3 for AOKVQA appear reasonable and credible; on the other hand, it avoids the redundancy of undertaking two similar tasks.",
            "20": "If a wider array of tasks were addressed, incorporating both wouldn't be an issue, but with just these two, their similarity is more glaring.",
            "21": "Therefore, if you decide to pursue two tasks, I'd recommend focusing on Visual Commonsense Reasoning (VCR https://visualcommonsense.com/) and AOKVQA.",
            "22": "I noticed that you mentioned VCR in the limitation section as well.",
            "23": "Personally, I'm more intrigued by the combination of VCR and AOKVQA.",
            "24": "I believe readers could glean more insights from these two tasks, given the significant differences in task format (direct generation vs. multiple-choice) and textual content between VCR and AOKVQA.",
            "25": "Such diverse content seems far more engaging to me.",
            "26": "Given the need to extract more detailed image information for enhanced visual reasoning, why haven't you considered dense captioning?",
            "27": "Would using dense captioning yield favorable results?",
            "28": "(You can refer to the approach on https://github.com/JialianW/GRiT) Naturally, integrating dense captioning might introduce the problem of hallucination, making the outcomes potentially noisier.",
            "29": "Despite the possibility of such issues arising, I'd like to see some experiments incorporating dense captioning, whether in the main content or the appendix.",
            "30": "This is a frequently discussed question.",
            "31": "Your work stems from the perspective that LLM cannot visualize images, which is indeed correct at present.",
            "32": "However, with the evolution of MLLM and the anticipated unveiling of GPT-4's multimodal capabilities, the value of your work is bound to face significant challenges.",
            "33": "From my viewpoint, I'd expect authors working on LLM Aided visual reasoning to proactively address the challenges presented by GPT-4 and its implications on their research.",
            "34": "However, I haven't observed this aspect in your work."
        }
    },
    "ILQnct9H4H": {
        "llm_review": {
            "0": "**Novel Benchmark**: The introduction of TRIGO as a benchmark for formal mathematical proof reduction is a significant contribution.",
            "1": "It addresses a gap in current ATP benchmarks by focusing on trigonometric expression reduction, which involves complex numerical reasoning and manipulation of formulas.",
            "2": "**Comprehensive Dataset**: The dataset is well-constructed, combining manually annotated real-world problems with automatically generated samples.",
            "3": "This dual approach ensures that the dataset is both realistic and extensive, covering a wide range of problem difficulties and distributions.",
            "4": "**Detailed Annotation Process**: The paper provides a thorough description of the interactive annotation system used to manually label the reduction steps.",
            "5": "This system ensures the accuracy and consistency of the annotations, which is crucial for training and evaluating generative language models.",
            "6": "**Lean Formalization**: The conversion of annotated steps into Lean formal language is a notable strength.",
            "7": "This formalization allows for strict verification of the correctness of each proof step, leveraging the Lean theorem prover's capabilities.",
            "8": "**Extensive Experiments**: The paper conducts extensive experiments with various models, including GPT-2 and GPT-4, and explores different training and evaluation settings.",
            "9": "This comprehensive evaluation provides valuable insights into the strengths and limitations of current generative language models in formal mathematical reasoning.",
            "10": "**Expert Iteration**: The use of expert iteration to discover diverse and better proof paths is a novel approach that significantly improves model performance.",
            "11": "This iterative process highlights the importance of diverse training data in enhancing model capabilities.",
            "12": "**Analysis of Model Performance**: The paper provides a detailed analysis of model performance, including single-step generation accuracy and the impact of different decoding and search methods.",
            "13": "This analysis helps identify specific challenges and areas for improvement in current models.",
            "14": "#### Weaknesses\n\n1.",
            "15": "**Limited Generalization to Larger Numbers**: The paper reveals that current models, including GPT-4, struggle with generalizing numerical operations to larger unseen numbers.",
            "16": "This limitation is significant and suggests that further research is needed to improve models' numerical reasoning capabilities.",
            "17": "**Dependence on Pre-trained Models**: The significant improvement achieved by pre-training on PACT raises questions about the generalizability of the results.",
            "18": "It would be beneficial to explore whether similar improvements can be achieved with other pre-training datasets or methods.",
            "19": "**Complexity of Lean Formalization**: While the Lean formalization is a strength, it also adds complexity to the dataset construction process.",
            "20": "The manual effort required to convert annotated steps into Lean formal language may limit the scalability of this approach.",
            "21": "**Evaluation Metrics**: The paper primarily uses the pass rate as the evaluation metric, which may not fully capture the models' capabilities.",
            "22": "Additional metrics, such as the diversity of proof paths or the complexity of generated proofs, could provide a more comprehensive evaluation.",
            "23": "**Limited Exploration of Alternative Models**: The paper focuses primarily on GPT-2 and GPT-4 models.",
            "24": "Exploring a wider range of models, including those specifically designed for mathematical reasoning, could provide a more complete understanding of the current state of the field.",
            "25": "**Potential Bias in Data Collection**: The real-world problems in the dataset are collected from high school exercises and exams, which may introduce a bias towards certain types of problems.",
            "26": "Ensuring a diverse and representative dataset is crucial for evaluating model performance accurately.",
            "27": "**Challenges with \"Have\" Tactics**: The paper identifies the generation of \"have\" tactics as a significant challenge for current models.",
            "28": "While this is an important finding, the paper could provide more detailed analysis and potential solutions to address this issue.",
            "29": "#### Conclusion\n\nOverall, the paper presents a valuable contribution to the field of automated theorem proving by introducing the TRIGO benchmark.",
            "30": "The comprehensive dataset, detailed annotation process, and extensive experiments provide a solid foundation for future research.",
            "31": "However, addressing the identified weaknesses, such as improving generalization to larger numbers and exploring alternative models, will be crucial for advancing the capabilities of generative language models in formal mathematical reasoning."
        },
        "MXGGrOgjKr": {
            "0": "- The paper is very well-written and well motivated\n- TRIGO represents an extensive resource that can support future research on the evaluation and improvement of the mathematical and symbolic reasoning capabilities of Language Models and Machine Learning approaches in general.",
            "1": "- The paper proposes an original methodology for data generation that combines human experts with a formal environment.",
            "2": "The proposed methodology can serve as an inspiration for future work in the field.",
            "3": "- Extensive empirical evaluation on state-of-the-art models.",
            "4": "- I believe the paper would benefit from a deeper discussion on how the quality of the generated dataset is verified.",
            "5": "For example, human annotation is involved, but I could not find any discussion or report of inter-annotator agreement and quality check.",
            "6": "- The evaluation is performed using a single family of generative models (i.e.",
            "7": "GPT).",
            "8": "As GPT4 is not freely available to the community and much of the details are unknown, I believe the experiments would benefit from an additional comparison between open models (for instance comparing GPT2 with T5).",
            "9": "*Update*: This limitation is partially addressed in the authors's rebuttal with new results."
        },
        "HmdA5gKshB": {
            "0": "The paper contributes new data resources for mathematical proof reduction.",
            "1": "Trigonometry seems to be a relatively underexplored area in proof datasets.",
            "2": "The paper presents relatively thorough experiments to understand the gaps between the curated and generated datasets.",
            "3": "The paper also assess the impact of different proof search algorithms, expert iteration, and an expanded set of angles.",
            "4": "Although it’s a well-curated dataset and it’s always important to contribute additional benchmarks, the specifics of the impact are not clear to me from reading the paper alone.",
            "5": "Is the automatic proof generation the main methodological contribution?",
            "6": "Is it possible to train on Trigo and have performance improve on other proof benchmarks?",
            "7": "Some of the steps of the dataset curation process could be described with more precision.",
            "8": "* Line 203: “To expand our dataset, we further collect additional trigonometry reduction problems from different websites.” It seems like only a small percentage of problems were sourced from outside Tiku.",
            "9": "It would be great to specify the websites, the number from each website, and why additional sourcing was needed, since it seems that the total dataset size did not increase by much.",
            "10": "* Section 4.2 It would be great to show some examples, such as screenshots, of the annotation software and flow.",
            "11": "It’s also not clear if the annotators during this step are the authors themselves, or if there were other annotators.",
            "12": "* It would be great to attach an appendix to Section 4.3 to clarify on which additional steps that PhD students had to fix.",
            "13": "And again, it would be great to specify how they differ from the initial annotator pool."
        },
        "OSFNnKgb4Q": {
            "0": "- Studying mathematical reasoning capabilities of LLMs is an important research direction\n- Useful datasets are rare, the authors contribute to the data scarcity problem\n- I see the most value of this dataset in the manual annotation effort done by Ph.D. students (page 4, line 288+)\n- Experimental results show that the dataset can be used to effectively train even small LLMs; promising results - Some of the results are not surprising (with more human-level input, the model will improve in distribution)\n- The dataset is limited in size, making it only usable for fine-tuning\n- The comparison to GPT-4 is unfair, this needs to be more clearer in the paper (because despite the introduction/abstract, I doubt that GPT-4 has seen that much Lean)"
        }
    },
    "7LBhEJ1DII": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method for measuring character substitution costs using Vision Transformers (ViT) trained with augmented digital fonts.",
            "1": "This approach is innovative and leverages recent advancements in self-supervised learning and contrastive learning, which are cutting-edge techniques in the field of computer vision.",
            "2": "**Extensibility**: One of the significant strengths of this study is its extensibility.",
            "3": "The method can be applied to any character set, including low-resource settings and ancient scripts, as demonstrated with ancient Chinese characters.",
            "4": "This broad applicability is a substantial advantage over traditional string matching methods that rely on handcrafted features.",
            "5": "**Improved Record Linkage**: The paper provides empirical evidence that using the cosine distance between characters' representations as the substitution cost in an edit distance matching algorithm significantly improves record linkage accuracy.",
            "6": "This is a crucial contribution, as accurate record linkage is fundamental in quantitative social science research.",
            "7": "**Comprehensive Evaluation**: The authors evaluate their method on multiple datasets, including real-world data from Japanese supply chains and synthetically generated data for various languages using the CJK script.",
            "8": "This comprehensive evaluation across different OCR engines and languages strengthens the validity of their findings.",
            "9": "**Practical Implementation**: The authors provide a Python package, HomoglyphsCJK, which offers a simple, off-the-shelf implementation of their method.",
            "10": "This practical contribution makes their approach accessible to a broader audience, including researchers who may not have extensive technical expertise.",
            "11": "**Insightful Analysis**: The paper includes an error analysis that highlights the types of errors that homoglyphic matching can and cannot address.",
            "12": "This analysis provides valuable insights into the limitations and potential areas for improvement in their method.",
            "13": "**Weaknesses:**\n\n1.",
            "14": "**Limited Comparison with End-to-End Methods**: While the paper acknowledges that end-to-end deep neural methods could potentially outperform string matching, it does not provide a detailed comparison with such methods.",
            "15": "Including a more thorough comparison with state-of-the-art end-to-end approaches would strengthen the argument for the proposed method's effectiveness.",
            "16": "**Dependence on OCR Quality**: The method's performance is inherently tied to the quality of the OCR output.",
            "17": "In cases where OCR errors are not homoglyphic or where OCR destroys too much information, the proposed method may not perform well.",
            "18": "This limitation is acknowledged but not deeply explored in the paper.",
            "19": "**Parameter Tuning**: The paper mentions that insertion and deletion costs are set to one and that it is straightforward to scale these costs using parameters estimated on a validation set.",
            "20": "However, the impact of different parameter settings on the method's performance is not thoroughly investigated.",
            "21": "A more detailed exploration of parameter tuning could provide additional insights into optimizing the method.",
            "22": "**Ancient Character Evaluation**: While the extension to ancient Chinese characters is fascinating, the evaluation of this application is somewhat qualitative.",
            "23": "A more quantitative assessment of the method's performance on ancient scripts would provide a stronger validation of its extensibility.",
            "24": "**Scalability Concerns**: The paper mentions that the method is highly scalable, but it does not provide detailed information on the computational resources required for training and inference.",
            "25": "Including a discussion on the scalability and computational efficiency of the method would be beneficial for practitioners considering its implementation.",
            "26": "**Limited Discussion on Uncertainty Quantification**: The paper briefly mentions that string matching methods do not offer uncertainty quantification, which is a limitation.",
            "27": "However, it does not propose any solutions or workarounds for this issue.",
            "28": "Addressing how uncertainty could be quantified or mitigated in their method would be a valuable addition.",
            "29": "**Conclusion:**\n\nOverall, \"Quantifying Character Similarity with Vision Transformers\" presents a significant advancement in the field of record linkage by leveraging self-supervised learning and Vision Transformers.",
            "30": "The method's extensibility and practical implementation are notable strengths, making it a valuable tool for researchers in quantitative social science.",
            "31": "However, the paper could benefit from a more detailed comparison with end-to-end methods, a deeper exploration of parameter tuning, and a quantitative evaluation of its application to ancient scripts.",
            "32": "Despite these weaknesses, the study makes a substantial contribution to improving record linkage accuracy and broadening the contexts in which linked data can be effectively used."
        },
        "VHKRTEI5SG": {
            "0": "I think the topic is interesting.",
            "1": "The main technical component of the paper is an application of contrastive learning with multiple fonts to learn the similarity between two characters.",
            "2": "The way the method uses contrastive learning looks straightforward, so the technical contribution is limited.",
            "3": "Details of string matching over the proposed similarity metric are not provided.",
            "4": "This may significantly reduce the reproducibility of the method if it does not publish the code.",
            "5": "Some minor points:\n\nThe paper is redundant, repeating similar sentences.",
            "6": "This makes the paper hard to read.",
            "7": "The paper claims that the method to train the similarity metric is self-supervised, but for me, this is not self-supervised because it has labels to represent the same characters."
        },
        "2HFs6DntKM": {
            "0": "The method is simple but looks very useful and straightforward to reproduce.",
            "1": "A quick reference (p. 3 column 1 second paragraph) suggests they have a package for distribution.",
            "2": "The new method outperforms previous string matching algorithms on this task.",
            "3": "They make the believable claim that previous methods (e.g.",
            "4": "end-to-end neural networks) may have somewhat higher accuracy but are costly to (re-)tune for each use case.",
            "5": "The paper is mostly clear and well written, with some exceptions noted below.",
            "6": "The idea isn't rocket science (but I do think it's nice).",
            "7": "The paper is somewhat repetitive.",
            "8": "The content seems a bit short for a long paper.",
            "9": "Then again, I think it would be quite difficult to squeeze it into the length of a short paper.",
            "10": "I guess it just needs to be a short long paper.",
            "11": "The abstract and the limitations/conclusions section have appropriate content but need editing (see below)."
        },
        "E10w7klUfi": {
            "0": "The approach is novel, well-argued, and effective, and addresses several inherent difficulties of the problem\n- Training data are reasonably augmented to focus on the composition & structure of characters rather than style of writing / font\n- The authors overcome a paucity of evaluation data by synthesizing their own, but they do provide some evaluation on some real data that are available — this may be a limitation of the evaluation, however the methods for data synthesis proposed seem sound and it seems likely results on a synthetic evaluation will generalize to real data\n- Limitations are clearly discussed and accurate, and the authors acknowledge that there are more sophisticated end-to-end deep neural methods that may solve the record linking problem more directly and decisively, and incorporate language understanding into the process, but these methods may not always be practical.",
            "1": "This is a well-written paper which clearly and effectively argues for a novel and interesting approach, providing an adequate evaluation of the approach and a clear discussion of its limitations — this reviewer sees no clear or overwhelming reason to reject.",
            "2": "One limitation not discussed in the paper (edit: the authors have, in their rebuttal, agreed to revise the paper to explicitly address this) is that these experiments focus on a single script, and it’s not demonstrated how the approach would generalize to other alphabets, but the authors picked a good example of a high cardinality alphabet where the limitations of an unmodified edit distance in record linking would be most pronnounced."
        }
    },
    "bWXIut4pNM": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel framework, INGENIOUS, which leverages submodular optimization to select highly informative subsets of training data for pre-training language models.",
            "1": "This approach is innovative and addresses a significant challenge in the field of NLP, namely the high computational cost and environmental impact of training large language models.",
            "2": "**Efficiency Gains**: The proposed method demonstrates substantial efficiency gains.",
            "3": "By using only a fraction of the data, INGENIOUS achieves up to 99% of the performance of fully-trained models.",
            "4": "This is a significant contribution as it shows that it is possible to reduce training costs and time without a substantial loss in performance.",
            "5": "**Comprehensive Evaluation**: The paper provides a thorough empirical evaluation of the proposed method.",
            "6": "It includes experiments with multiple language models (BERT, BioBERT, GPT-2) and evaluates the performance on various downstream tasks.",
            "7": "The results are convincing and demonstrate the effectiveness of the approach.",
            "8": "**Open Source Contribution**: The authors have made their framework publicly available, which is commendable.",
            "9": "This allows other researchers to reproduce the results and build upon the work, fostering further research in this area.",
            "10": "**Environmental Impact**: The paper addresses the environmental impact of training large language models, which is an important and timely issue.",
            "11": "By reducing the amount of data used for training, INGENIOUS contributes to more sustainable AI practices.",
            "12": "**Detailed Analysis**: The paper includes detailed analysis and ablation studies, which help in understanding the impact of various design choices and parameters.",
            "13": "This adds to the robustness of the findings and provides valuable insights for future research.",
            "14": "**Weaknesses:**\n\n1.",
            "15": "**Scalability Concerns**: While the paper addresses scalability to some extent, the submodular maximization based on pairwise sample similarity can still be memory-intensive.",
            "16": "This might limit the applicability of the method to very large datasets or models without significant computational resources.",
            "17": "**Limited Model Variants**: The experiments are performed on relatively smaller models (BERT-Base, GPT-2 Small) compared to the largest models like GPT-3 or PaLM.",
            "18": "It would be beneficial to see how the method scales and performs with these larger models, although the authors acknowledge resource constraints.",
            "19": "**Dependency on Initial Representations**: The effectiveness of the subset selection depends on the quality of the initial representations obtained from the language model.",
            "20": "If the initial warm-start phase does not produce good representations, the subset selection might not be optimal.",
            "21": "This dependency could be a limitation in scenarios where the initial model is not well-trained.",
            "22": "**Potential Biases**: The paper mentions that the framework is susceptible to biases and toxic words within the pre-training corpora.",
            "23": "While this is a common issue with language models, it would be beneficial to explore methods to mitigate these biases within the INGENIOUS framework.",
            "24": "**Complexity of Implementation**: The proposed method involves several steps, including warm-starting, partitioning, and probabilistic sampling, which might make the implementation complex.",
            "25": "Providing more detailed guidelines or code examples could help practitioners adopt the method more easily.",
            "26": "**Evaluation on More Diverse Tasks**: While the paper evaluates the method on standard benchmarks like GLUE and LAMA, it would be interesting to see its performance on more diverse and challenging tasks, such as those involving multi-modal data or real-world applications with domain-specific requirements.",
            "27": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the efficient pre-training of language models by using informative data subsets.",
            "28": "The strengths of the paper lie in its innovative approach, comprehensive evaluation, and potential for reducing environmental impact.",
            "29": "However, there are some concerns regarding scalability, dependency on initial representations, and potential biases.",
            "30": "Addressing these weaknesses in future work could further enhance the applicability and robustness of the INGENIOUS framework."
        },
        "Yy52uMo34W": {
            "0": "This paper investigates the issue of resource consumption in training large language models (LLMs) by sampling high-quality information from the training dataset to ensure efficient learning of effective information.",
            "1": "This topic is crucial for the efficient training of LLMs.",
            "2": "The article provides a clear training strategy and validates its effectiveness on multiple models (but not large enough), which is inspiring.",
            "3": "While this paper's research is insightful, it has some limitations.",
            "4": "The current issues with training data quality for LLMs go beyond efficiency and information quantity, including concerns about values, toxicity, and biases.",
            "5": "Additionally, the paper's validation was limited to BERT and GPT2-small models and evaluated using GLUE metrics, which may not be sufficient to support the conclusions."
        },
        "e2q0mbEKGD": {
            "0": "The paper addresses an important problem of the pre-training cost of language models for efficient NLP.",
            "1": "The paper presents a novel approach to remove redundant texts in pre-training corpora in NLP domains.",
            "2": "C. Experimental results that language models trained with informative subsets outperform vanilla models in terms of convergence speed and final performance.",
            "3": "Although extracting data subsets work well in computer vision tasks, texts have different properties compared with images in nature.",
            "4": "The paper focuses on the size of datasets and lacks discussion about the different modality.",
            "5": "Most experiments are conducted on models with ~110M parameters.",
            "6": "It is unclear whether the proposed method consistently improves the efficiency with scaled models.",
            "7": "C. Lack of analysis and discussion leaves several questions about the reasons of performance improvements."
        },
        "GN7S7SQheN": {
            "0": "Firstly, the proposed method, INGENIOUS, demonstrates an impressive performance.",
            "1": "After fine-tuning, the model pre-trained using INGENIOUS-selected subsets achieves nearly identical performance compared to models pre-trained with the complete dataset, and even surpasses them with reduced training costs (as seen in Table 3).",
            "2": "Then, the experimental design is comprehensive, covering comparisons across various architectures, previous sampling methods, and different downstream tasks.",
            "3": "The evaluation is also multi-faceted, assessing both performance and cost considerations.",
            "4": "Additionally, the paper maintains a well-organized structure.",
            "5": "The experiment and analysis sections are good in depth and detail, though the methodology section could benefit from a more detailed elaboration.",
            "6": "First, I’m confused by Figures 3 and 4.",
            "7": "In these illustrations, INGENIOUS appears to consistently and significantly outperform the baseline BERT model which undergoes full pre-training, even when it is trained up to 1 million pre-training steps.",
            "8": "However, based on the context along with Tables 1&2 and the detailed Table 6 in Appendix F, it becomes evident that the most proficient model pre-trained with INGENIOUS achieves a 98.6% performance compared to the fully pre-trained BERT.",
            "9": "This contrast in results is confusing.",
            "10": "And if Figures 3&4 is sensible, what is the rationale behind the phenomenon that pre-training on selected subsets can surpass the performance of a fully pre-trained model?",
            "11": "Does this suggest that the original pre-training corpus contains harmful noise that interferes with learning?",
            "12": "Secondly,  for a thorough evaluation of INGENIOUS' effectiveness in selecting valuable subsets for pre-training, it is necessary to see a comparison between the pre-training performance of a model using INGENIOUS-selected data and one using the entire dataset, before any downstream fine-tuning.",
            "13": "The clean pre-training performance gains of INGENIOUS-selected data can demonstrate the evaluation process better."
        },
        "KCibErzysB": {
            "0": "Overall, the paper is well written.",
            "1": "The experiments are thorough, and I particularly appreciate the conversion of performance metrics to costs, which lets practitioners more easily evaluate trade-offs.",
            "2": "There are 3 potential changes that would improve this work:\n* First, something that didn't come across was the importance and intuition behind the choice of the similarity kernel.",
            "3": "What types of kernels work best?",
            "4": "Are there, e.g., cheap empirical metrics that can effectively estimate the clustering kernel in eq.",
            "5": "Could you estimate this similarity kernel through something very simple, such as SentenceBERT embeddings?",
            "6": "What features is it capturing that makes it particularly good?",
            "7": "* Including a comparison to one of the methods mentioned in the computer vision setting would have been more useful than comparing to, e.g.",
            "8": "loss-based sampling.",
            "9": "I understand that these are not always applicable and typically require a supervised set-up, but some of them can probably be adapted to language tasks relatively easily.",
            "10": "* Not sure I understand why increasing the subset size in some cases actually hurts performance?",
            "11": "e.g., Table 4.",
            "12": "I think the paper could benefit by elaborating on this, and why some subsets of the dataset are actually harmful to model performance.",
            "13": "--\n\nEdit: I acknowledge the authors' comments below.",
            "14": "Although I still have some questions/concerns about the subset ablations (and hope that the authors will eventually include a more granular analysis of why this happens, and if decoder only models exhibit some type of this behavior), I am moving to increase my soundness score as they have addressed my other comments."
        }
    },
    "R7Op9CHdPz": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel framework, Cognitive pathways VQA (CopVQA), which emphasizes causal reasoning in multimodal predictions.",
            "1": "This is a significant departure from traditional methods that primarily focus on unimodal aspects or data augmentation.",
            "2": "**Causal Reasoning**: The emphasis on causal reasoning between interpreting and answering stages is a fresh perspective in the VQA domain.",
            "3": "This approach mirrors human cognition and aims to improve the generalization of VQA models.",
            "4": "**Performance Improvement**: The experimental results demonstrate that CopVQA consistently outperforms existing baselines across various datasets, including real-life and medical data.",
            "5": "Notably, it achieves a new state-of-the-art on the PathVQA dataset and comparable accuracy to current SOTAs on VQA-CPv2, VQAv2, and VQA-RAD with significantly fewer parameters.",
            "6": "**Generalization**: The framework shows a marked improvement in out-of-distribution (OOD) generalization, which is a critical challenge in VQA.",
            "7": "The results on the VQA-CPv2 dataset, designed to test OOD generalization, are particularly impressive.",
            "8": "**Modular Design**: The use of a Mixture of Experts (MoE) framework to disentangle the VQA task into specialized experts is a well-thought-out design.",
            "9": "This modular approach aligns with principles of knowledge modularity and allows for more efficient and effective learning.",
            "10": "**Comprehensive Evaluation**: The paper provides a thorough evaluation of CopVQA across multiple datasets and baselines.",
            "11": "The inclusion of both quantitative and qualitative analyses strengthens the validity of the results.",
            "12": "**Ablation Studies**: The paper includes detailed ablation studies to understand the individual effects of different components in CopVQA.",
            "13": "This helps in validating the design choices and understanding the importance of each component.",
            "14": "**Weaknesses:**\n\n1.",
            "15": "**Complexity and Tuning**: The proposed method requires careful fine-tuning of hyperparameters such as the number of experts (N1, N2) and the architecture of the experts.",
            "16": "This can be time-consuming and may require significant computational resources.",
            "17": "**Sensitivity to Data**: The paper mentions that CopVQA is sensitive to data with limited occurrences, such as brand names or country names.",
            "18": "This limitation could affect the model's performance in real-world applications where such data is common.",
            "19": "**Inference Time**: While the paper claims that CopVQA does not increase the number of parameters beyond the baseline models, the inference time and computational overhead of activating and managing multiple experts are not thoroughly discussed.",
            "20": "This could be a potential drawback in practical applications.",
            "21": "**Generalization to Other Tasks**: The paper focuses on VQA, but it would be interesting to see how the proposed framework generalizes to other multimodal tasks.",
            "22": "The applicability of CopVQA to a broader range of tasks remains an open question.",
            "23": "**Limited Discussion on Failure Cases**: While the paper provides examples of successful debiasing, it lacks a detailed discussion on failure cases and the specific scenarios where CopVQA might struggle.",
            "24": "Understanding these limitations could provide valuable insights for further improvements.",
            "25": "**Comparison with More Baselines**: The paper compares CopVQA with a few selected baselines.",
            "26": "Including a broader range of state-of-the-art methods, especially those focusing on different aspects of VQA (e.g., attention mechanisms, transformer-based models), could provide a more comprehensive evaluation.",
            "27": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the field of Visual Question Answering by introducing a novel framework that emphasizes causal reasoning.",
            "28": "The results are promising, showing improved performance and generalization across multiple datasets.",
            "29": "However, the complexity of the method, sensitivity to certain types of data, and the need for careful tuning are potential drawbacks.",
            "30": "Future work could focus on addressing these limitations and exploring the applicability of CopVQA to other multimodal tasks."
        },
        "uDIpIJY37I": {
            "0": "- The intuition from cognitive neuroscience about knowledge modularity and cognitive pathways is interesting\n- This work first proposes a causal reasoning method, CopVQA  that formulates VQA as two layers of cognitive pathways (i.e.",
            "1": "interpreting and answering)\n- The proposed model CopVQA achieves SOTA for PathVQA dataset I am not an expert in causality, but I try my best to read this work, the implementation part is like an increment of [1].",
            "2": "And for the experiment part, I am willing to see more results of other baselines on PathVQA and VQA-RAD\n\n[1] Niu, Yulei, et al.",
            "3": "“Counterfactual VQA: A Cause-Effect Look at Language Bias.” 2021 CVPR"
        },
        "Iwb5e9uAcL": {
            "0": "- Interesting and timely problem statement\n- strong results on medical data\n- I like cognitive pathways angle brought in to VQA\n- well-written paper and interesting analysis These are not 'reasons to reject', but some questions I have:\n- I like the analysis in B.2, but was curious if authors have identified any possible data contaminations in the test set of the results they have reported in the paper?",
            "1": "- In NLP, Use of 'instructions' have strongly improved generalization (https://aclanthology.org/2022.acl-long.244.pdf, and https://openreview.net/forum?id=gEZrGCozdqR).",
            "2": "Wondering authors would like to discuss if they have considering using instructions in their setup?",
            "3": "Some of the issues authors see in sec 6.2 and 7 may be solved via instructions.",
            "4": "- How sensitive is the model to hyper-parameters?",
            "5": "I see a mention in the limitation section 'Require careful fine-tuning', but was looking for more details."
        },
        "hwqtKU2XZK": {
            "0": "The idea of CopVQA is interesting，which formulates VQA as two layers of cognitive pathway to  boost the causal reasoning.",
            "1": "CopVQA achieves the better performance on several VQA datasets.",
            "2": "There is doubt about the effectiveness of the complex reasoning module, as the improvement in downstream tasks by the model is limited.",
            "3": "Compared to sota pre-trained methods, there is a significant performance gap with this method.",
            "4": "Does it have scaling and generalization ability?",
            "5": "The method section is too redundant, and the experiments are not detailed enough, including experimental settings, model parameters, training time, inference time, etc."
        },
        "aZCXMmgwKy": {
            "0": "The quantitative results of the newly proposed framework indicate a new SOTA.",
            "1": "Newly designed cognitive pathways are innovative and the corresponding loss function described such reasoning paths well.",
            "2": "c. The qualitative analyses proves the effectiveness of such two-layer reasoning path way.",
            "3": "d. Enough case studies explains the advantages of such method.",
            "4": "e. Paper is well-organized.",
            "5": "Some details of such model remains unclear.",
            "6": "In 6.2 \"Discussion on the debiased samples\", it claims to reduce biases from the texts in question.",
            "7": "However, there is no statistics of such phenomenon.",
            "8": "c. The training details about training time and hardware requirements are missing."
        }
    },
    "D9oq45WsKq": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel method, Ensemble-Instruct, which leverages smaller, open-access language models (LMs) to generate high-quality instruction-tuning data.",
            "1": "This is a significant departure from previous methods that rely on large, proprietary models like GPT-3.",
            "2": "**Categorization and Simplification**: The authors propose categorizing tasks into those requiring inputs and those that do not, and simplifying the in-context learning (ICL) templates accordingly.",
            "3": "This approach makes prompt learning easier for the LMs and improves the quality of the generated data.",
            "4": "**Output Ensembling**: The use of ensembling over multiple LM outputs to select high-quality synthetic examples is a strong point.",
            "5": "This method helps in improving both the accuracy and diversity of the generated data, which is crucial for training robust models.",
            "6": "**Empirical Validation**: The paper provides extensive empirical validation, showing that the proposed method outperforms the Self-Instruct approach when using smaller models.",
            "7": "The results demonstrate significant improvements in performance for both vanilla and instruction-tuned LMs.",
            "8": "**Open-Source Contribution**: The authors have made their codebase and synthetic instruction-tuning dataset publicly available.",
            "9": "This transparency and contribution to the community are commendable and will likely facilitate further research and development in this area.",
            "10": "**Scalability**: The method is shown to be scalable to larger models, as evidenced by the experiments with models ranging from 6B to 40B parameters.",
            "11": "This scalability is a crucial aspect for practical applications.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Complexity of Implementation**: The proposed method involves multiple steps, including categorization, separate pipelines for different types of tasks, and ensembling.",
            "14": "This complexity might make it challenging for practitioners to implement and replicate the results without significant effort.",
            "15": "**Dependence on Multiple LMs**: The method relies on a heterogeneous mixture of LMs, which might not always be feasible for all researchers or organizations due to resource constraints.",
            "16": "This dependence could limit the accessibility and applicability of the approach.",
            "17": "**Manual Evaluation**: While the paper includes a manual evaluation of the generated data, it is conducted by one of the authors.",
            "18": "This could introduce bias, and a more rigorous evaluation involving multiple annotators would strengthen the validity of the results.",
            "19": "**Limited Analysis of Bias**: The paper does not provide a detailed analysis of potential biases in the generated data.",
            "20": "Given that the method uses multiple LMs, each with its own training data and biases, it is important to understand how these biases might affect the final synthetic data.",
            "21": "**Evaluation on User-Oriented Tasks**: The evaluation on the 252 user-oriented tasks is limited to automatic metrics.",
            "22": "Human evaluation of these tasks would provide a more comprehensive understanding of the practical utility and quality of the generated data.",
            "23": "**Resource Constraints**: The paper mentions that the experiments were conducted using an internal API serving models from HuggingFace, which might not be accessible to all researchers.",
            "24": "This could limit the reproducibility of the results.",
            "25": "**Conclusion:**\n\nOverall, the paper presents a significant advancement in the generation of instruction-tuning data using smaller, open-access LMs.",
            "26": "The innovative approach of categorization, simplification, and ensembling is well-validated through extensive experiments.",
            "27": "However, the complexity of the method, dependence on multiple LMs, and limited analysis of biases are areas that could be improved.",
            "28": "The open-source contribution is a strong point, and the scalability of the method makes it a valuable addition to the field.",
            "29": "Future work could focus on simplifying the implementation, conducting more rigorous evaluations, and addressing potential biases in the generated data."
        },
        "cAWjeiTWft": {
            "0": "- The simplicity of the methodology is a key advantage, as it facilitates easy application, which is commendable.",
            "1": "- Despite utilizing smaller models, the approach achieves notable performance, particularly demonstrating improved results even with a limited number of samples.",
            "2": "- The insightful analysis of the differences between ILM and LM, following experimentation with both models, adds valuable understanding to the study.",
            "3": "- The proposed methodology appears disconnected from the primary motivation of the paper, which revolves around the model's size and openness.",
            "4": "Although it was demonstrated using small models, there is potential for its applicability to all black-box models.",
            "5": "To better showcase the effectiveness of the methodology, conducting experiments with larger models would be beneficial.",
            "6": "- The analysis of the effectiveness of categorization and simplification lacks in-depth exploration, and it would be advantageous to delve further into this aspect.",
            "7": "While section 3.2 briefly mentions the balance between type A and B, a more comprehensive analysis would add valuable insights.",
            "8": "- The absence of a comparison with other datasets, such as Alpaca mentioned in the paper, is somewhat regrettable.",
            "9": "Including such comparisons would provide a more comprehensive evaluation of the proposed approach.",
            "10": "Additionally, a qualitative comparison with other methods would be valuable in further validating the proposed methodology."
        },
        "7vSjcBqptH": {
            "0": "Paper is well written and easy to follow.",
            "1": "Ensemble-Instruct technique shows clear gains over Self-Instruct across a wide variety of settings.",
            "2": "Ablation studies show that both aspects (simplified prompts, ensemble) are both useful in leading to superior performance.",
            "3": "Synthetic dataset of instruction-tuning examples can be used by others to build on.",
            "4": "The dataset only includes 45k examples, but I can see this being expanded to 100K+ examples if the codebase is opened up to others.",
            "5": "Anything that works with open-source, permissive licenses is great for the community, especially when doing so outperforms the black-box counterparts.",
            "6": "Ensembling is not a particularly novel way to denoise generated samples.",
            "7": "More sophisticated methods exists and it would have been nice to compare against them.",
            "8": "Could be improved with more experiments on how many candidates to ensemble, or how many examples to include during ICL.",
            "9": "Missing larger scale human evaluation for qualitative review."
        },
        "ESN4z28PjZ": {
            "0": "* The paper proposes a feasible method to create high-quality ICL samples for small (and hence less-capable) LLMs.",
            "1": "This topic has practical importance in many application areas.",
            "2": "* The evaluation is quite comprehensive, covering many settings and using several popular permissive LLMs.",
            "3": "* The main performance metric used is the Rouge-L score, which is far from being satisfactory.",
            "4": "If human evaluation is too costly, at least some AI evaluation could be done (e.g., using GPT-4).",
            "5": "* The key step is the high-quality sample selection from multiple LLMs.",
            "6": "Although these LLMs were built by different groups, there might still be some hidden correlation between their outputs (e.g., due to training on the same subset of data).",
            "7": "Hence, the current method is not really selecting \"high-quality\" samples, but more like selecting \"popular\" samples, which may further magnifies the latent biases in the training data.",
            "8": "* There is no detailed case study, e.g., comparing the samples generate by the proposed method and those by the Self-Instruction method.",
            "9": "Additional, such case studies can be carried out among different LLMs used in this study.",
            "10": "It is desirable to see insightful analysis or trends from these studies."
        }
    },
    "uemYdRTVvP": {
        "llm_review": {
            "0": "**Novelty and Relevance**: The paper addresses a significant and timely issue in the field of Natural Language Processing (NLP) - the detection of AI-generated text at the sentence level.",
            "1": "This is particularly relevant given the increasing use of large language models (LLMs) like GPT-3.5 and GPT-4, which can generate highly human-like text.",
            "2": "The introduction of sentence-level detection is a novel contribution that fills a gap in the current research landscape, which predominantly focuses on document-level detection.",
            "3": "**Comprehensive Dataset**: The authors have created a new dataset, SeqXGPT-Bench, specifically designed for sentence-level AI-generated text detection.",
            "4": "This dataset includes a mix of human-written and AI-generated sentences, which is more reflective of real-world scenarios where users might use LLMs to polish or modify parts of a document rather than generating entire documents.",
            "5": "**Methodological Innovation**: The proposed SeqXGPT method leverages log probability lists from white-box LLMs as features for detection.",
            "6": "The use of convolutional and self-attention networks to process these features is innovative and well-justified, drawing parallels to wave-like features in speech processing.",
            "7": "**Experimental Rigor**: The paper includes extensive experiments comparing SeqXGPT with several baseline methods, including log p(x), DetectGPT, Sniffer, and RoBERTa-based models.",
            "8": "The results demonstrate that SeqXGPT significantly outperforms these baselines in both sentence and document-level detection tasks.",
            "9": "**Generalization Capability**: SeqXGPT shows strong generalization capabilities on out-of-distribution (OOD) datasets, which is a critical aspect for practical applications.",
            "10": "This is a notable strength as many existing methods tend to overfit to the training data and perform poorly on OOD data.",
            "11": "**Ablation Studies**: The paper includes ablation studies that highlight the importance of different components of the SeqXGPT model, such as the convolutional and Transformer layers.",
            "12": "This helps in understanding the contribution of each component to the overall performance.",
            "13": "#### Weaknesses\n\n1.",
            "14": "**Complexity and Interpretability**: While the use of convolutional and self-attention networks is innovative, it also adds complexity to the model.",
            "15": "The interpretability of such models can be challenging, and the paper does not provide much insight into how the model makes its decisions.",
            "16": "More discussion on the interpretability of the model would be beneficial.",
            "17": "**Limited Exploration of Instructions**: The paper mentions that the instructions used for GPT-3.5-turbo were not extensively explored.",
            "18": "Given that instructions can significantly influence the generated text, a more thorough investigation into the impact of different instructions on the detection performance would have strengthened the study.",
            "19": "**Human-Like AI-Generated Text**: The paper acknowledges that the current model does not incorporate semantic features, which could help in detecting more human-like AI-generated text.",
            "20": "This is a limitation, especially as LLMs continue to improve and generate increasingly human-like text.",
            "21": "Future work should consider integrating semantic features to enhance detection capabilities.",
            "22": "**Dataset Diversity**: While the SeqXGPT-Bench dataset is comprehensive, it is constructed using a limited number of LLMs.",
            "23": "Including a wider variety of models, especially newer and more advanced ones, could provide a more robust evaluation of the detection methods.",
            "24": "**Real-World Application Scenarios**: The paper primarily focuses on the technical aspects of sentence-level detection.",
            "25": "However, it would benefit from a discussion on potential real-world applications and the challenges associated with deploying such a detection system in practice.",
            "26": "For instance, how would SeqXGPT handle mixed-content documents in different domains or languages?",
            "27": "**Evaluation Metrics**: The paper uses Precision, Recall, and Macro-F1 Score as evaluation metrics.",
            "28": "While these are standard metrics, additional metrics such as the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) could provide a more comprehensive evaluation of the model's performance.",
            "29": "#### Conclusion\n\nOverall, the paper makes a significant contribution to the field of AI-generated text detection by introducing the challenge of sentence-level detection and proposing a novel method, SeqXGPT, to address this challenge.",
            "30": "The creation of a new dataset and the extensive experimental evaluation are commendable.",
            "31": "However, there are areas for improvement, particularly in terms of model interpretability, exploration of instructions, and integration of semantic features.",
            "32": "Addressing these weaknesses in future work could further enhance the robustness and applicability of the proposed method."
        },
        "RgLkzPfsqW": {
            "0": "Novel Contribution: The paper addresses the important and challenging task of fine-grained AI-generated text (AIGT) detection at the sentence level, which is a significant advancement over existing document-level AIGT detection methods.",
            "1": "The authors propose a new approach called SeqXGPT, which demonstrates promising results in both sentence and document-level AIGT detection challenges.",
            "2": "This novel contribution fills a gap in the literature and provides valuable insights into fine-grained AIGT detection.",
            "3": "Performance and Generalization: The experimental results show that SeqXGPT outperforms existing methods, such as DetectGPT and Sniffer, in sentence-level AIGT detection.",
            "4": "SeqXGPT exhibits excellent performance not only in discriminating human-generated sentences but also in detecting AI-generated sentences.",
            "5": "Furthermore, SeqXGPT demonstrates strong generalization capabilities on out-of-distribution datasets, indicating its robustness and potential for real-world applications.",
            "6": "Dataset Construction: The authors synthesize a sentence-level AIGT detection dataset, which is crucial for studying fine-grained AIGT detection 1.",
            "7": "Lack of Novelty: The paper does not present a significant advancement or novel contribution to the field of fine-grained AI-generated text (AIGT) detection.",
            "8": "The proposed approach, SeqXGPT, is similar to existing methods such as DetectGPT and Sniffer.",
            "9": "The paper fails to demonstrate how SeqXGPT significantly outperforms or improves upon these existing methods.",
            "10": "Insufficient Experimental Evaluation: The experimental results provided in the paper are limited and do not provide a comprehensive evaluation of the proposed approach.",
            "11": "The paper lacks a thorough comparison with state-of-the-art methods and fails to provide statistical significance tests to support the claimed performance improvements.",
            "12": "Additionally, the evaluation is primarily focused on synthetic datasets, which may not accurately reflect real-world scenarios.",
            "13": "Incomplete Analysis and Discussion: The paper lacks a thorough analysis and discussion of the limitations and potential drawbacks of the proposed approach.",
            "14": "For example, the authors do not explore the impact of incorporating semantic features or investigate the influence of diversified instructions on AIGT detection.",
            "15": "The paper also does not address more complex scenarios where a document contains sentences."
        },
        "1T77OByRWS": {
            "0": "1) The method proposed in this paper can effectively solve the difficulties of sentence-level AIGT detection.",
            "1": "2) The experiments designed in this paper cover 3 different sentence-level AIGT detection settings, all of which achieve SOTA.",
            "2": "3) This article is written smoothly and helps readers understand.",
            "3": "In the setting of Particular-Model Binary AIGT Detection, the table of experimental results only includes the results of GPT-2 and GPT-Neo, lacking test results on other LLMs."
        },
        "pHSePBU55u": {
            "0": "This paper will provide a practical guide to construct an AI-generated text detection software.",
            "1": "In particular, this method can be generally applied to any collection of LMs (already or to-be available), which makes the method presented in this paper a nice addition to the practical armory of any NLP researcher or industrial programmer.",
            "2": "The dataset used to train and test the proposed method appears to be somewhat narrow in how it is produced.",
            "3": "As bootstrapping is the first-step for any new task, so OOD test results the authors provided are a must-and-nice addition, but the intensity of the OOD test should have been greater in my opinion (as, once the paper published, interested people will be doing their own OOD tests by implementing their own versions)."
        }
    },
    "KfJffhdWO1": {
        "llm_review": {
            "0": "**Innovative Framework**: The paper introduces METRIC EVAL, a novel framework grounded in measurement theory, which is a significant contribution to the field of Natural Language Generation (NLG).",
            "1": "This framework provides a structured approach to evaluate the reliability and validity of NLG evaluation metrics, which is crucial for advancing robust and effective NLG models.",
            "2": "**Comprehensive Analysis**: The framework covers both reliability and validity, addressing various sources of measurement error.",
            "3": "This comprehensive approach ensures that the evaluation metrics are scrutinized from multiple angles, providing a holistic understanding of their effectiveness.",
            "4": "**Empirical Validation**: The paper includes a detailed case study on summarization metrics using the SummEval dataset.",
            "5": "This practical application of the framework demonstrates its utility and provides concrete examples of how the framework can be used to identify issues in existing metrics.",
            "6": "**Statistical Rigor**: The use of statistical tools such as test-retest reliability, internal consistency reliability, multitrait-multimethod (MTMM) analysis, and confirmatory factor analysis adds rigor to the evaluation process.",
            "7": "These tools help quantify the uncertainty and identify the sources of measurement error, making the evaluation process more robust.",
            "8": "**Actionable Insights**: The findings from the case study provide actionable insights for metric developers and users.",
            "9": "For instance, the identification of lower stability in LLM-based metrics and the conflation between coherence and relevance in expert ratings can guide future improvements in metric design and evaluation processes.",
            "10": "**Clear Presentation**: The paper is well-structured and clearly presents the theoretical background, the proposed framework, and the empirical results.",
            "11": "The use of figures and tables enhances the readability and helps in understanding the complex statistical analyses.",
            "12": "**Weaknesses:**\n\n1.",
            "13": "**Dependence on Benchmark Datasets**: The framework's effectiveness is dependent on the quality and design of the benchmark datasets used for evaluation.",
            "14": "The paper acknowledges this limitation but does not provide a detailed discussion on how to ensure the reliability and validity of the benchmark datasets themselves.",
            "15": "**Limited Scope of Validity**: While the framework covers concurrent and construct validity, it does not address other types of validity, such as predictive validity.",
            "16": "Including a discussion on these additional aspects could provide a more comprehensive evaluation of the metrics.",
            "17": "**Generalizability**: The case study focuses on summarization metrics using the CNN/Daily Mail dataset.",
            "18": "It remains unclear how well the findings generalize to other NLG tasks and datasets.",
            "19": "Future work should explore the applicability of the framework across different NLG tasks and domains.",
            "20": "**Complexity of Statistical Tools**: The use of advanced statistical tools may pose a challenge for practitioners who are not well-versed in these methods.",
            "21": "Providing more intuitive explanations or simplified versions of these tools could make the framework more accessible to a broader audience.",
            "22": "**Human Evaluation Issues**: The paper highlights the issues with human evaluation data but does not provide concrete solutions to address these problems.",
            "23": "Further research is needed to develop standardized and reproducible human evaluation methods that can serve as a reliable foundation for evaluating automatic metrics.",
            "24": "**Future Directions**: While the paper provides a solid foundation, it could benefit from a more detailed discussion on future research directions.",
            "25": "For instance, exploring the integration of METRIC EVAL with other evaluation frameworks or developing new metrics based on the insights gained from this framework could be valuable.",
            "26": "**Conclusion:**\n\nOverall, the paper makes a significant contribution to the field of NLG by introducing a robust framework for evaluating evaluation metrics.",
            "27": "The comprehensive analysis, empirical validation, and statistical rigor are commendable.",
            "28": "However, addressing the limitations related to benchmark datasets, expanding the scope of validity, ensuring generalizability, simplifying the statistical tools, and providing solutions for human evaluation issues would further enhance the framework's utility and impact."
        },
        "Z8xMKqdE1f": {
            "0": "+ nice combination of existing methods (from measurement theory) to a very relevant current topic (evaluation of metrics for NLG)\n+ nice and intuitive showcase none"
        },
        "PzqntNVHpX": {
            "0": "- Interesting framework that addresses an important task: estimating the quality of commonly used metrics.",
            "1": "- Thorough description of measurement theory and how it is applied, enabling other researchers to use this framework for more cases.",
            "2": "- Case study extensive in regards to metrics (a good selection of reference-free and reference-based metrics).",
            "3": "- Uncertainty to what degree measurement theory (which has been developed to assess human evaluation) can also be applied to automatic metrics.",
            "4": "- Case study somewhat limited in scope in regards to the task (only summarization).",
            "5": "- Limited relevant work section."
        },
        "IJHv5vKK4z": {
            "0": "As NLG models improve over time, finding good metrics to accurately measure the quality of their output in a way that best approximates their real-world performance is an important task.",
            "1": "This paper presents an interesting idea from educational testing that breaks down the required qualities of a good metric and shows how they relate to different factors.",
            "2": "The paper is well written and the ideas are communicated clearly.",
            "3": "The authors also present a case study using text summarization as an example task and show how different metrics score along different dimensions, and what this means for their robustness and validity.",
            "4": "The authors plan to make the code for their framework publicly available as well.",
            "5": "There can be other sources of error beyond those discussed in the paper (and an investigation of where they occur in the evaluation process), although the authors concede this in the limitations section already.",
            "6": "The related work section could be more comprehensive and discuss the merits and limitations of prior works in more detail."
        }
    },
    "0SIyWZEOmJ": {
        "llm_review": {
            "0": "**Comprehensive Cross-Linguistic Analysis**: The study extends the investigation of surprisal effects on reading times beyond English to include seven languages: Danish, Dutch, English, German, Japanese, Mandarin, and Russian.",
            "1": "This broad scope provides valuable insights into whether the findings in English generalize to other languages.",
            "2": "**Use of Multiple Language Models**: The authors employ both a multilingual model (mGPT) and monolingual models trained on Wikipedia text.",
            "3": "This dual approach helps to assess the robustness of the findings across different types of language models.",
            "4": "**Detailed Methodology**: The paper provides a thorough description of the datasets, language models, and statistical methods used.",
            "5": "This transparency allows for reproducibility and critical evaluation of the methods.",
            "6": "**Consideration of Spillover Effects**: The study accounts for spillover effects, which are known to influence reading times.",
            "7": "This consideration enhances the accuracy of the findings.",
            "8": "**Comparison with Concurrent Work**: The authors situate their findings within the context of concurrent research, particularly the work by Wilcox et al.",
            "9": "(2023).",
            "10": "This comparison helps to highlight the unique contributions and limitations of their study.",
            "11": "**Open Data and Code**: The availability of the full regression code and results on GitHub promotes transparency and allows other researchers to build on this work.",
            "12": "#### Weaknesses\n\n1.",
            "13": "**Inconsistent Evidence for Superlinearity**: The evidence for superlinearity is inconsistent and highly sensitive to the language model used.",
            "14": "This instability makes it difficult to draw definitive conclusions about the nature of the surprisal effect across languages.",
            "15": "**Limited Statistical Methodology**: The use of a simple linear mixed-effects model to test for superlinearity may not capture the full complexity of the data.",
            "16": "More sophisticated methods, such as those involving cubic splines or non-linear models, could provide a more nuanced understanding of the surprisal effect.",
            "17": "**Genre Differences in Corpora**: The eyetracking corpora used are not matched across languages in terms of genre, which could introduce confounding variables.",
            "18": "For example, the Danish corpus consists of speeches, while the Dutch corpus consists of a novel.",
            "19": "These genre differences may affect reading times independently of surprisal.",
            "20": "**Language Model Training**: The language models are trained to the same target perplexity, which may not reflect the same level of quality across languages due to inherent differences in language entropy rates.",
            "21": "This could affect the accuracy of surprisal estimates.",
            "22": "**Handling of Reaction Time Distributions**: The use of linear regression assumes a Gaussian error distribution, which may not be appropriate for human reaction times.",
            "23": "More realistic error distributions, such as Log-Normal or Ex-Gaussian, could yield different results.",
            "24": "**Small Sample Sizes for Some Languages**: The smaller datasets for German, Japanese, Mandarin, and Russian may limit the statistical power to detect effects, particularly superlinear effects.",
            "25": "**Lack of Typological Patterns**: The study does not find a clear typological pattern in the presence of superlinear effects, which raises questions about the generalizability of the findings.",
            "26": "#### Conclusion\n\nThe paper makes a significant contribution to the field of psycholinguistics by extending the investigation of surprisal effects on reading times to multiple languages.",
            "27": "The use of multiple language models and consideration of spillover effects are notable strengths.",
            "28": "However, the inconsistent evidence for superlinearity, limitations in statistical methodology, and potential confounding factors related to genre differences and language model training suggest that further research is needed to draw more definitive conclusions.",
            "29": "Future studies should consider using more sophisticated statistical methods, matched corpora, and realistic error distributions to better understand the functional form of the surprisal effect across languages."
        },
        "7hdjUtyBDe": {
            "0": "(a) The authors address a timely topic that has been the focus of recent discussion in the literature\n\n(b) The authors go beyond previous monolingual analyses, which is a valuable contribution\n\n(c) The paper is well written and clear\n\n(d) Although the authors present their handling of spillover has a limitation (I see their point) their method allows for variable spillover between datasets.",
            "1": "This is better than many recent papers which set a fixed sized spillover window regardless of dataset.",
            "2": "(a) Previous studies have demonstrated an effect of unigram surprisal (i.e.",
            "3": "frequency) on reading times.",
            "4": "Frequency is absent from this paper and not used in the baseline model M0.",
            "5": "I’m worried that the overall picture of results could change if frequency was added in as an additional predictor.",
            "6": "(The authors do mention that word length with random by-participant intercepts was the maximal consistently random effects structure – does this mean that they tried random slopes, or that they tried random slopes plus also other main effects, such as frequency?)",
            "7": "(b) The likelihood ratio tests were computed on the log likelihoods of the training data, correct?",
            "8": "I think the paper could be made much stronger if the tests were conducted on a held-out portion of the dataset.",
            "9": "Especially as models grow more complex, testing on held-out data grows crucial to avoid charges of overfitting.",
            "10": "(My apologies if you are already doing this – it was not clear to me from the text!)"
        },
        "fnIToUVGUJ": {
            "0": "The paper contributes useful crosslinguistic data to an ongoing debate about the connection between surprisal and reading time.",
            "1": "The methods seem sound and the results are clearly presented.",
            "2": "The opening sections of the paper seem to slightly mischaracterize Surprisal Theory.",
            "3": "As presented in Levy 2008, Surprisal Theory posits that comprehension difficulty (e.g.",
            "4": "reading time) is proportional to surprisal -- meaning that the linking function is linear.",
            "5": "This would mean that theories that posit e.g.",
            "6": "a superlinear linking function do not fall under Surprisal Theory per se."
        },
        "V7LguMTFB9": {
            "0": "- The question is of interest for cognitive modeling\n- The principle cross-linguistic contribution is important and necessary for this subfield to progress\n- Data analysis is well-motivated and connects well with previous work in this domain - A key limitation concerns the different corpora that were available for the different languages.",
            "1": "These corpora differ substantially in size and in genre (e.g.",
            "2": "naturalistic text vs constructed sentences).",
            "3": "These differences in genre plausibly affect how adequate GPT2 -- given the idiosyncrasies of its own training set -- serves as a cognitive estimator.",
            "4": "The analysis across languages also differ in other ways that may impact the results, including the specific measures available (first-pass gaze duration vs first fixation duration) and the size of the pre-target context that was used to sum \"spill-over\" surprisal values.",
            "5": "Taken together, I found it difficult to understand where differences between \"linear\" and \"non-linear\" effects might reflect cross-linguistic differences, as opposed to idiosyncratic differences of the available corpora (like genre) or other analysis choices.",
            "6": "I think for this sort of work to be maximally impactful, it would be great to see a robustness analysis varying some of these parameters (e.g.",
            "7": "corpus size and genre via sub-sampling the available materials; spill-over size etc.)",
            "8": "A final point is that the main conclusions: that languages differ in whether the effect of surprisal is linear or super-linear, is grounded in comparing a set of \"significant\" model comparison findings against \"non-significant\" model comparisons.",
            "9": "As presented, I don't think this inference goes through (\"the difference between significant and non-significant is not itself statistically significant\" - German & Stern 2006).",
            "10": "I think this could be addressed by testing for a statistical interaction of language and model-type."
        }
    },
    "siiVduxdRz": {
        "llm_review": {
            "0": "**Innovative Approach:**\n   - The paper introduces the Language-specific Matrix Synthesis (LMS) method, which is a novel approach to reduce the number of parameters in multilingual models while maintaining or even improving performance.",
            "1": "This is a significant contribution to the field of multilingual NLP.",
            "2": "**Parameter Efficiency:**\n   - The LMS method is shown to be highly parameter-efficient, achieving comparable or better performance than existing methods like the Switch Transformer with significantly fewer parameters.",
            "3": "This is particularly important for scaling models to handle hundreds of languages.",
            "4": "**Fuse Distillation (FD):**\n   - The introduction of Fuse Distillation (FD) to condense multilingual knowledge from multiple language-specific modules into a single shared module is another innovative idea.",
            "5": "This method improves model inference and storage efficiency, which is crucial for practical deployment.",
            "6": "**Comprehensive Evaluation:**\n   - The paper evaluates the proposed methods on multiple tasks, including multilingual machine translation (MMT), multilingual named-entity recognition (MNER), and multilingual question answering (MQA).",
            "7": "This demonstrates the generalizability and robustness of the methods.",
            "8": "**Detailed Analysis:**\n   - The paper provides a thorough analysis of the parameter efficiency of LMS and the effectiveness of FD.",
            "9": "The ablation studies and comparisons with strong baselines like the Switch Transformer and CLSR add credibility to the findings.",
            "10": "**Code Availability:**\n   - The authors have made their code available, which promotes transparency and reproducibility of the research.",
            "11": "**Weaknesses:**\n\n1.",
            "12": "**Homogeneous Batches Requirement:**\n   - One limitation of the LMS method is the need for homogeneous batches, which may not always be feasible in real-world scenarios.",
            "13": "The paper mentions this limitation but does not provide a concrete solution or workaround.",
            "14": "**Complexity of Implementation:**\n   - While the paper introduces innovative methods, the implementation of LMS and FD might be complex and require significant changes to existing model architectures.",
            "15": "This could be a barrier to adoption for practitioners.",
            "16": "**Limited Discussion on Computational Overhead:**\n   - The paper briefly mentions that the additional computational cost of constructing low-rank matrices is negligible, but it would benefit from a more detailed discussion on the computational overhead and potential impact on training and inference times.",
            "17": "**Scalability of FD:**\n   - While FD shows promising results in condensing multilingual knowledge, its scalability to a very large number of languages (e.g., more than 100) is not thoroughly explored.",
            "18": "Further experiments and analysis on this aspect would strengthen the paper.",
            "19": "**Evaluation on More Diverse Datasets:**\n   - The evaluation is primarily focused on the IWSLT’14 and OPUS-100 datasets.",
            "20": "Including more diverse and larger datasets could provide a more comprehensive assessment of the methods' effectiveness across different languages and domains.",
            "21": "**Comparison with More Baselines:**\n   - The paper compares LMS and FD with the Switch Transformer and CLSR, but additional comparisons with other state-of-the-art multilingual models could provide a more complete picture of the methods' relative performance.",
            "22": "**Conclusion:**\n\nOverall, the paper presents significant advancements in the field of multilingual NLP by introducing parameter-efficient methods for handling multiple languages.",
            "23": "The LMS and FD methods are innovative and show strong performance across various tasks.",
            "24": "However, the paper could benefit from addressing the limitations related to batch requirements, implementation complexity, and scalability.",
            "25": "Despite these weaknesses, the contributions of the paper are substantial and have the potential to impact future research and applications in multilingual NLP."
        },
        "owFPRACujx": {
            "0": "-  Both techniques proposed in the paper seem interesting.",
            "1": "I like the idea of LMS, and language-pair LMS looks an interesting.",
            "2": "- Idea of the fused distillation is very interesting as well.",
            "3": "Even though I am not fully convinced by the LMS-FD-Shared results, the fact that it outperforms NMT-Naive model is very interesting, and I believe different directions could be explored to further improve the results - The training details of different variants are not fully documented and would harm reproducibility , eg.",
            "4": "1) do you strat from pretrained model and add LMS layers on top of it or do you train everythin from scratch?",
            "5": "2) Do LMS-FD-*  models start from LMS model or do you train LS and shared modules simultaneously?",
            "6": "- I do not have full understanding about how do authors compute inference parameters.",
            "7": "Eg.",
            "8": "we could distinguish the case of language-specifc from multilingual inference.",
            "9": "In the case when we know the language of interest (mandatory when we rely on LMS) we could load only the parameters specific to this language(s) and it would mean that we would have much less parameters for inference than for training: thus in my understanding LMS-FD-LS should have the same amount of inference parameters as LMS-FD-Share .",
            "10": "Similarly, CLSR should only activate language-specific modules, and have less parameters at inference that at training.",
            "11": "But this doesn't seem to be the case in the tables reported by authors.",
            "12": "Do they consider multilingual inference?",
            "13": "Some clarifications would help.",
            "14": "-  Moreover, if the authors wanted to reinforce their argument about efficiency, only counting the amount of the parameters doesn't give the full picture (parameter count only address memory efficiency): inference speed, the amount of flops (in training and inference) would provide additional and important view to get a full picture.",
            "15": "- I am not sure the choice of CLSR as a baseline.",
            "16": "There are many PEFT methods these days that could serve as a reasonable baseline for such approach (eg.",
            "17": "laguage-specific or language-pair specific adapter layers seems like a very natural baseline to me, and should probably be comparable in terms of parameter efficiency )"
        },
        "0P1oepcpIg": {
            "0": "* The idea of using LoRA to tune the FFNs to be language specific is interesting.",
            "1": "The proposed approach for distillation is also interesting.",
            "2": "* The authors call their method \"Language-Specific Matrix Synthesis\", but their method is essentially LoRA for multilingual settings.",
            "3": "I still think it's interesting, but this attempt to make it sound more novel than it really is is a negative point in my opinion.",
            "4": "* The paper claims their method \"significantly outperforms previous language-specific or mixture of experts methods\" (lines 77-78), yet they don't compare with any of those methods.",
            "5": "There is plenty of related work that is cited on section 7, but no comparisons were made.",
            "6": "While it is interesting that the authors test their method on different tasks, the lack of any meaningful baseline is a serious omission.",
            "7": "I do think the idea is interesting and that this can be an interesting paper, but as it stands I strongly believe it would benefit from further work."
        },
        "FxxaQIBi0A": {
            "0": "The idea is simple yet smart and intuitive, which is supported by the strong performance in the desired downstream task.",
            "1": "Beside the performance, it scales to a number of languages, and even propose a technique to pack all the knowledge into a single module, to further reduce the complexity.",
            "2": "It's well grounded and solid study.",
            "3": "I'm mostly convinced that the paper should get accepted, as there is nothing to complain."
        }
    },
    "qMSG8S7zh0": {
        "llm_review": {
            "0": "**Comprehensive Evaluation:**\n   - The paper provides a thorough investigation of various state-of-the-art probabilistic methods for improving uncertainty calibration in neural summarization models.",
            "1": "This comprehensive evaluation across multiple benchmarks (XSUM, CNN/DailyMail, and RedditTIFU-long) ensures that the findings are robust and generalizable.",
            "2": "**Novel Evaluation Protocol:**\n   - The introduction of a new evaluation protocol tailored to measure uncertainty calibration performance in summarization models is a significant contribution.",
            "3": "This protocol includes sequence-level Expected Calibration Error (ECE) and selective generation via abstention, which are well-suited for the domain-specific quality scores like ROUGE.",
            "4": "**Diverse Methods:**\n   - The study covers a wide range of probabilistic methods, including classic approaches like Monte Carlo Dropout (MCD) and Deep Ensemble (DE), as well as more recent methods like Batch Ensemble (BE) and Spectral-normalized Gaussian Process (SNGP).",
            "5": "This diversity allows for a comprehensive comparison and understanding of the strengths and weaknesses of each method.",
            "6": "**Practical Insights:**\n   - The paper provides practical insights into the effectiveness of different probabilistic methods in improving both the generation and uncertainty quality of summarization models.",
            "7": "The findings, such as the superior performance of SNGP+DE in terms of reducing ECE and improving ROUGE scores, are valuable for practitioners looking to enhance the reliability of their models.",
            "8": "**Failure Patterns:**\n   - The identification of notable failure patterns, such as the underperformance of MCD in challenging test environments like Reddit, is crucial.",
            "9": "This cautionary note helps in understanding the limitations of certain methods and guides the selection of appropriate techniques based on the data setting.",
            "10": "**Weaknesses:**\n\n1.",
            "11": "**Limited Adaptation of Methods:**\n   - While the paper adapts existing probabilistic methods to the Pre-trained Language Model (PLM) setup, the adaptations are described as \"very basic.\"",
            "12": "More sophisticated adaptations could potentially yield better results and provide deeper insights into the interplay between these methods and the unique properties of PLMs.",
            "13": "**Computational Complexity:**\n   - The study acknowledges the high computational cost associated with some of the methods, particularly Deep Ensembles.",
            "14": "While the paper suggests that DE combined with SNGP is the best approach when there are no time and memory constraints, this recommendation may not be practical for many real-world applications where resources are limited.",
            "15": "**Limited Exploration of New Methods:**\n   - The paper primarily focuses on well-established probabilistic methods.",
            "16": "While this is valuable, the exploration of newer or less conventional methods could provide additional insights and potentially uncover more effective techniques for uncertainty calibration in summarization.",
            "17": "**Evaluation on Limited Metrics:**\n   - The evaluation primarily relies on ROUGE scores and ECE.",
            "18": "While these are standard metrics, incorporating additional evaluation metrics, such as human evaluation for summary quality or other calibration metrics, could provide a more holistic assessment of the models' performance.",
            "19": "**Generalization to Other Tasks:**\n   - The study is focused on summarization tasks.",
            "20": "While the findings are relevant to this domain, it would be beneficial to explore the generalizability of these methods to other NLP tasks, such as machine translation or text generation, to understand their broader applicability.",
            "21": "**Conclusion:**\n\nOverall, the paper makes a significant contribution to the field of uncertainty calibration in neural summarization models.",
            "22": "The comprehensive evaluation, novel protocol, and practical insights are valuable for both researchers and practitioners.",
            "23": "However, there is room for improvement in terms of method adaptation, computational efficiency, exploration of new techniques, and evaluation on a broader set of metrics and tasks.",
            "24": "Future work addressing these aspects could further enhance the reliability and trustworthiness of probabilistic neural summarization models."
        },
        "CmTZFRIeMS": {
            "0": "- In-depth Analysis: The paper provides a comprehensive comparison of various calibration techniques, including Monte Carlo Dropout, Deep Ensembles, Batch Ensemble, and Spectral-normalized Neural Gaussian Process (SNGP), and adapts these probabilistic methods to LLM setup for summarization task.",
            "1": "This detailed analysis includes both generation quality assessment for calibrated model and quality of its uncertainty (length-normalized log-probabilities), which would be invaluable for future research in the field of NLP.",
            "2": "- Clear Presentation: The overall logic flow of the paper is fluent and easy to follow.",
            "3": "Both figures and tables are nicely drawn with subtle explanations.",
            "4": "Overall, I like this paper.",
            "5": "My main concern is that some details seems missing and need further explanations:\n- For the main method (section 3), formal definitions (preliminaries) are necessary before showing equations.",
            "6": "For example in Line 186-200, what are the meanings of x, y, t, T?",
            "7": "Is u(y|x) a vector or a number (I expect it to be a vector because it is a distribution among the vocabulary space)?",
            "8": "Otherwise, readers may find it hard to follow the theoretical part.",
            "9": "- For the evaluation (section 4.2), how is the sequence-level ECE calculated?",
            "10": "For example, in line 256-257, if the predicted sequence is “i made fun of a guy walking in giant slippers on father’s day” and the ground truth is “thought a guy’s shoes were the reason he walked funny, turns out he has cerebral palsy.”, does y^ equal y and what is p^?",
            "11": "Lack of concrete examples makes it difficult to understand how the accuracy of a prediction (sequence) and the confidence are calculated.",
            "12": "- For the abstention (section 4.3), what is the meaning of the quality in the y-axis of Figure 1?",
            "13": "If it is ROUGE, then make it 'ROUGE score vs Abstention', instead of introducing an undefined new term."
        },
        "l4usVikbCH": {
            "0": "- Well written paper, and interesting and informative to read\n- Investigation of state-of-the-art probabilistic methods in improving uncertainty calibration in neural summarization models.",
            "1": "-  Clear demonstration of the positive impact of probabilistic methods on summary quality and calibration performance.",
            "2": "- In-depth analysis of failure patterns in widely-adopted probabilistic methods, providing insights for choosing appropriate methods based on data settings.",
            "3": "- The significance and insights from experiment 4.1 are not fully clear, requiring more detailed explanations.",
            "4": "- Although the paper provides good experiments and analysis, it is not entirely novel given the advancements in probabilistic methods for classification tasks."
        },
        "4GVSu1VX3L": {
            "0": "The paper addresses an important topic regarding how to make the output of neural language models more reliable and trustworthy.",
            "1": "The problem of miscalibration is understudied in text summarization, meaning that this work could open new research directions toward this topic.",
            "2": "The experimental setup is poor.",
            "3": "The authors compared several probabilistic methods over a single language model (T5-base).",
            "4": "They should evaluate multiple language models of different sizes (e.g., BART-base, BART-large, FLAN-T5-XXL).",
            "5": "Furthermore, they should include a long document summarization dataset (and thus a long sequence model such as LED or PEGASUS-X) to make the evaluation analysis more robust.",
            "6": "There is not a novel method to address the problem of miscalibration.",
            "7": "The authors state to experiment with large language models (LLMs), but in practice, they use T5-base, which is far from being an LLM (like GPT-3).",
            "8": "The dataset statistics (i.e., number of source/target words and sentences) should be included to show and demonstrate the difference between the benchmarked datasets.",
            "9": "The paper contains many typos (see \"Typos Grammar Style And Presentation Improvements\" for a non-exhaustive list), meaning the document's drafting has not been curated enough.",
            "10": "The authors did not mention how costly each benchmarked method is regarding time and space."
        },
        "k35il7zGoS": {
            "0": "The paper performs systematic evaluation of the probabilistic methods in uncertainty calibration in neural summarization models.",
            "1": "The evaluation is performed on classic and state-of-the-art methods, on three datasets, and with evaluation metrics of ROUGE, ECE, correlation between quality and predictive confidence, and quality vs abstention rate.",
            "2": "It is not quite reassuring to see that the authors use the \"average\" performances from the three datasets (as in Tables 1 and 2) to prove the effectiveness of the probabilistic models.",
            "3": "The average rank as in Table 1 is more reasonable.",
            "4": "And yet, the average rank is not used in Table 2, which needs explanation.",
            "5": "The evaluation is systematic, covering quality (with ROUGE), calibration (sequence-level and token-level ECE), quality vs predictive confidence, and quality vs abstention rate.",
            "6": "However it is a bit farfetched to say it's novel."
        }
    },
    "AEkFAAprvF": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces a novel framework, ViStruct, which leverages the inherent structure of programming languages to represent visual structural information.",
            "1": "This is a creative and potentially powerful method for encoding complex visual relationships in a consistent and explicit manner.",
            "2": "**Curriculum Learning**: The use of curriculum-based learning to progressively teach the model from basic visual concepts to more complex event structures is well-motivated and aligns with human learning processes.",
            "3": "This approach helps in building a strong foundation before moving on to more complex tasks.",
            "4": "**Comprehensive Dataset**: The authors have compiled and released a collection of datasets tailored for visual structural knowledge extraction.",
            "5": "This ViStruct Suite is a valuable resource for the community and supports the training and evaluation of models on a wide range of visual tasks.",
            "6": "**Weakly-Supervised Learning**: The use of weakly-supervised methods to generate visual event structures from captions is a smart way to leverage abundant web data, making the training process more scalable and less reliant on expensive annotations.",
            "7": "**Evaluation and Results**: The paper provides thorough experimental results, demonstrating the effectiveness of ViStruct on various visual structure prediction tasks.",
            "8": "The consistent improvements over baselines across multiple tasks highlight the robustness and generalizability of the proposed approach.",
            "9": "**Public Code Release**: The availability of the code on GitHub enhances the reproducibility of the research and allows other researchers to build upon this work.",
            "10": "**Weaknesses:**\n\n1.",
            "11": "**Complexity of Implementation**: The approach, while innovative, introduces significant complexity in terms of implementation.",
            "12": "The use of programming language representations and the curriculum learning framework may pose challenges for practitioners looking to adopt this method.",
            "13": "**Dependence on Pre-trained Models**: ViStruct relies heavily on pre-trained vision-language models like OFA-base.",
            "14": "The performance and applicability of ViStruct might be limited by the capabilities and limitations of these base models.",
            "15": "**Evaluation on Limited Tasks**: While the paper evaluates ViStruct on several tasks, it would be beneficial to see its performance on a broader range of visual tasks, including those that involve more diverse and complex visual scenes.",
            "16": "**Scalability Concerns**: The curriculum learning approach, while effective, may face scalability issues when applied to very large datasets or more complex visual domains.",
            "17": "The replay buffer mechanism, in particular, might become a bottleneck.",
            "18": "**Generalization to Other Domains**: The paper focuses on visual structural knowledge extraction.",
            "19": "It would be interesting to see how well the proposed methods generalize to other domains, such as video understanding or 3D scene analysis.",
            "20": "**Limited Discussion on Limitations**: The paper could benefit from a more detailed discussion on the limitations of the proposed approach and potential areas for improvement.",
            "21": "This would provide a more balanced view and help guide future research.",
            "22": "**Conclusion:**\n\nOverall, ViStruct presents a significant advancement in the field of visual structural knowledge extraction.",
            "23": "The innovative use of programming language representations and curriculum learning offers a new perspective on how to tackle complex visual tasks.",
            "24": "While there are some challenges and limitations, the strengths of the approach and the promising results make it a valuable contribution to the field.",
            "25": "Future work could focus on addressing the scalability and generalization issues, as well as exploring the application of ViStruct to a wider range of tasks and domains."
        },
        "A4jdslRJo0": {
            "0": "- This paper defines a visual structural knowledge pyramid and collects ViStruct Suite including multi-level structural knowledge aligned with the pyramid.",
            "1": "This knowledge pyramid and the curated dataset would help to promote future studies.",
            "2": "- This paper devises an elaborated training framework for VLMs to improve their visual structural knowledge extraction ability.",
            "3": "- Even though the curriculum-guided code-vision representation is an interesting idea, this paper lacks an in-depth ablation study on the proposed method.",
            "4": "For example, the additional experiments on the diverse order of curriculum and the design of a replay buffer help to convince the effectiveness of the proposed method.",
            "5": "- In ViStruct Suite, annotations in the visual event detection stage might contain noises since they are obtained by the off-the-shelf semantic role labeling system.",
            "6": "A detailed analysis of the curated dataset would be needed."
        },
        "BlR9jfm4BU": {
            "0": "The idea of encoding an image using programming language is quite interesting.",
            "1": "This programming language based approach could potentially further achieve improved performance using code based LLM.",
            "2": "The paper converts the visual relation prediction task into a masked language prediction problem.",
            "3": "This is very interesting.",
            "4": "I would recommend either add a section describing the inference stage or move the section A from appendix to main text.",
            "5": "For several rounds of reading the main text, I couldn't figure out how to do the inference.",
            "6": "After reading the appendix, I realized it is doing mask prediction.",
            "7": "I hope the author could provide more implementation details for how to reproducing the results in either main text or appendix.",
            "8": "I found it might be a little bit hard to find those information.",
            "9": "I think the author might also want to mention that the program language (generated) is not intended for directly running from some python environment.",
            "10": "In the first round of reading, I misunderstood that the generated code is intended for directly running."
        },
        "sQkTaUMrL5": {
            "0": "I like the idea of structuralizing the vision information (concept, attribute, location, relation, etc) into codes.",
            "1": "Compared with text, codes are more organized and more efficient to express high-level logic.",
            "2": "The curriculum learning framework propagates from the basic concept level up to the event level according to the complexity, which is intuitively reasonable.",
            "3": "Although almost no new methods are proposed, combining previous methods to adapt to the new problem is still meaningful.",
            "4": "Main concern: Only one backbone (OFA-base) is evaluated and the model size is too small.",
            "5": "At least two or three different pre-trained VLM backbones are required to demonstrate the generalizability of the proposed method as it's a plug-in method on top of pre-trained VLMs.",
            "6": "Also, the model scale is too small (just base).",
            "7": "I wonder if the conclusion that curriculum learning helps can still hold with larger pre-trained VLM, i.e., more pre-trained natural text knowledge injected.",
            "8": "Too few previous works are compared in Tab1 and Tab2 and they are too outdated.",
            "9": "I didn't see any evaluation on grounding and attribute prediction.",
            "10": "As they are also one of the curriculum levels, it's necessary to evaluate them."
        },
        "iN85PRiqUA": {
            "0": "Novel idea: Representing visual knowledge as code is a new idea as per my knowledge, including masked code prediction, and curriculum learning.",
            "1": "Scalable w.r.t new/larger/better models: I believe as VLMs improve, this technique should also improve.",
            "2": "The ViStruct Suite might be useful for benchmarking/training of future visual structure extraction models.",
            "3": "Evaluation is performed on multiple tasks 1.",
            "4": "Relatively low performance increase:  Although Table 3 and Table 4 show best results are achieved by Vistruct, the numerical values do not seem to be consistent with the value which are made bold in all columns (some non bold numbers are the highest it seems?).",
            "5": "I would request the authors to correct this.",
            "6": "Also see my next point regarding baselines considered.",
            "7": "Insufficient comparison with baselines: There are numerous methods for scene graph generation/relation prediction which do not seem to be included in Table3 and Table 4.",
            "8": "They are for example [1,2,3] and it is possibly many other scene graph generation methods (also see this somewhat old repository for baselines https://github.com/microsoft/scene_graph_benchmark).",
            "9": "In light of these advances in SGG, the utility of the current method is questionable.",
            "10": "Comparison with all are not necessary, but some latest/recent baselines should be included.",
            "11": "Limited datasets used: This work uses visual genome for benchmarking the VRD and SGC tasks.",
            "12": "Prior methods routinely used datasets like OpenImages.",
            "13": "Benchmarking on an extra dataset can improve this work.",
            "14": "Generalizability to different pre-trained models: It seems necessary to study the effect of changing pre-trained models.",
            "15": "Currently, table 3 baselines and OFA based model of this paper is not really comparable I believe.",
            "16": "I do not have a direct solution to this, but having more pre-trained backbones can improve this work and give the reader more clarity of the performance of the model w.r.t pretrained backbone.",
            "17": "Curriculum learning: more details are required to claim performance improvements.",
            "18": "for eg.",
            "19": "is the current ordering of curriculum necessary?",
            "20": "are same computation resources/epochs/data size used for the ablation which compares curriculum based learning with non curriculum based learning?",
            "21": "Applications of visual structure: Although there are many papers that tackle the problem of visual structure/scene graph generation (and probably some are better than this method as per the previous point), the utility of generating a scene graph alone is questionable.",
            "22": "Instead, applications of using scene graphs might be more interesting to improve this work, for eg.",
            "23": "this work cites Yüksekgönül et al., 2022 which shows that VLMs have a bag of words nature.",
            "24": "Can the current method extract visual relation ships which can then be used to make progress on ARO task provided by Yüksekgönül et al.",
            "25": "Joint training for image-text matching and code completion as done in this work, is a possible direction.",
            "26": "This point is largely a suggestion and not exactly a reason to reject.",
            "27": "References\n1.",
            "28": "Graphical Contrastive Losses for Scene Graph Generation https://arxiv.org/abs/1903.02728\n1.",
            "29": "RelTR: Relation Transformer for Scene Graph Generation: https://arxiv.org/abs/2201.11460v3\n1.",
            "30": "Noisy label correction for robust scene graph generation https://openaccess.thecvf.com/content/CVPR2022/papers/Li_The_Devil_Is_in_the_Labels_Noisy_Label_Correction_for_CVPR_2022_paper.pdf"
        }
    },
    "JHd4FSJSC5": {
        "llm_review": {
            "0": "**Innovative Approach**: The paper introduces AncSetFit, a novel method that enhances the SetFit approach by incorporating semantic label information through anchor statements.",
            "1": "This is a creative way to leverage the limited data available in few-shot learning scenarios.",
            "2": "**Efficiency**: AncSetFit maintains the efficiency of SetFit while improving performance in extremely low-data scenarios.",
            "3": "This is particularly important for practical applications where computational resources and data are limited.",
            "4": "**Empirical Evidence**: The authors provide comprehensive empirical evidence demonstrating the effectiveness of AncSetFit across multiple datasets.",
            "5": "The method consistently outperforms SetFit and other baselines in scenarios with very few training examples.",
            "6": "**Robustness**: The ablation study shows that AncSetFit is robust to variations in the anchor statements, which suggests that the method can generalize well even when the semantic information is not perfectly aligned.",
            "7": "**Public Code Availability**: The authors have made their code publicly available, which promotes transparency and allows other researchers to replicate and build upon their work.",
            "8": "**Clear Visualizations**: The use of PCA visualizations to illustrate the effect of fine-tuning with anchor statements is helpful in understanding how the method works.",
            "9": "#### Weaknesses\n\n1.",
            "10": "**Limited Scope**: The method is specifically designed for text classification tasks where the classes can be described with short textual statements.",
            "11": "This limits its applicability to more complex tasks that cannot be easily summarized in this way.",
            "12": "**Performance on Larger Datasets**: While AncSetFit shows significant improvements in extremely few-shot settings, the performance gains diminish as the amount of training data increases.",
            "13": "This suggests that the method's advantage is primarily in very low-data scenarios.",
            "14": "**Complexity of Anchor Statements**: The method's effectiveness relies on the quality of the anchor statements.",
            "15": "For tasks with semantically complex or lengthy class descriptions, the method may not perform as well.",
            "16": "This is evident from the RAFT leaderboard results where AncSetFit did not outperform SetFit on average.",
            "17": "**Comparison with Larger Models**: Although AncSetFit is efficient, it still lags behind larger models like ADAPET_XXL in some tasks.",
            "18": "The trade-off between model size and performance is not fully explored, and it would be beneficial to understand the conditions under which AncSetFit is preferable.",
            "19": "**Hyperparameter Sensitivity**: The margin parameter (ϵ) in the loss function is a hyperparameter that needs to be tuned.",
            "20": "The paper does not provide a detailed analysis of how sensitive the method is to this parameter, which could be important for practitioners.",
            "21": "**Real-world Applicability**: The paper primarily focuses on benchmark datasets.",
            "22": "It would be useful to see more real-world applications and how AncSetFit performs in those scenarios, especially in domains where data is inherently sparse.",
            "23": "#### Conclusion\n\nOverall, the paper presents a significant advancement in the field of few-shot text classification by introducing AncSetFit.",
            "24": "The method is innovative, efficient, and shows strong performance in extremely low-data scenarios.",
            "25": "However, its applicability is somewhat limited to tasks with easily describable classes, and its performance gains diminish with larger datasets.",
            "26": "Future work could focus on extending the method to more complex tasks and exploring its performance in real-world applications."
        },
        "3ZUQ9HOQwQ": {
            "0": "This work utilizes semantic label Information and improves the performance of SetFit.",
            "1": "This work aims to improve SetFit but compared with experimental design in SetFit, existing results in this work fail to demonstrate AncSetFit effectiveness."
        },
        "PAFjDzU7Kx": {
            "0": "- This paper proposes a simple but effective few-shot text classification method.",
            "1": "- Compared with ADAPET, the performance of their proposed method is lower or on par, although their method has some merits over ADAPET."
        },
        "KJDhb8Y7jw": {
            "0": "AncSetFit improves the original work (SetFit) without being more computationally demanding.",
            "1": "The experimental comparisons are lacking.",
            "2": "The authors focuses on the comparison with SetFit but neglects most other methods.",
            "3": "They only report ADAPET as comparison (and only for some of the datasets) which performs better than AncSetFit in 3 out of the 4 instances.",
            "4": "Moreover, the paper of SetFit has more comparisons that in AncSetFit have not been considered at all."
        }
    }
}