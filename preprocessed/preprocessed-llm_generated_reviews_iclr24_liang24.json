{
    "tmsqb6WpLz": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the finetuning of language models: the balance between enhancing domain-specific knowledge and the unintended consequence of forgetting general abilities.",
            "1": "- The study introduces a novel method of using instruction-following LLMs like ChatGPT to generate controlled-variable text examples for probing the model, which provides a detailed analysis of the impacts on topic, style, and factual knowledge.",
            "2": "Potential reasons for acceptance\n   - The paper provides a thorough and systematic analysis of the effects of finetuning on language models, offering valuable insights into the dynamics of learning and forgetting.",
            "3": "- The use of ChatGPT to generate controlled-variable text examples is an innovative approach that minimizes human effort in data curation and ensures high-quality text samples.",
            "4": "- The findings have significant implications for future research on improving domain adaptation and addressing the challenges of continual language learning.",
            "5": "- The paper includes detailed experimental setups and results, making the study reproducible and transparent.",
            "6": "Potential reasons for rejection\n   - **Limited scope of data and models:**\n     - The study is conducted on a limited number of datasets (PubMed, C4) and models (GPT-2 XL, LLaMA 2 7B, LLaMA 2 13B), which may not generalize to other domains or larger models.",
            "7": "- The finetuning experiments are limited to a maximum of 1 million documents, which may not reflect the behavior of models trained on significantly larger datasets.",
            "8": "- **Approximate decomposition of text-generating factors:**\n     - The decomposition of text into topic, style, and factual knowledge is only approximate and may not capture the true generating process of text.",
            "9": "- The boundary between content and style is not always clear, and interdependence between these factors may affect the accuracy of the analysis.",
            "10": "- **Quality and reliability of generated rewrites:**\n     - The quality of the generated rewrites by ChatGPT may be affected by safety alignment, pre-training bias, and hallucination, which could introduce inaccuracies in the analysis.",
            "11": "- The success rate of 95% for adherence to instructions, while high, still leaves room for potential errors in the generated data.",
            "12": "- **Data dependency and generalizability:**\n     - The quantitative observations may reflect specific characteristics of the chosen corpus, and the findings may not generalize to other domains or datasets with different distributions.",
            "13": "Suggestions for improvement\n   - **Expand the scope of data and models:**\n     - Conduct experiments on a wider range of datasets from different domains to test the generalizability of the findings.",
            "14": "- Include larger and more diverse language models to understand how the dynamics of learning and forgetting scale with model size and complexity.",
            "15": "- **Refine the decomposition of text-generating factors:**\n     - Explore more sophisticated methods for decomposing text into generating factors to better capture the true process and reduce interdependence between content and style.",
            "16": "- Consider additional factors that may influence the model's behavior, such as context or discourse structure.",
            "17": "- **Enhance the quality and reliability of generated rewrites:**\n     - Implement additional checks and validation steps to ensure the accuracy and adherence of the generated rewrites to the intended instructions.",
            "18": "- Address potential biases and hallucinations in the generated data by using multiple LLMs or incorporating human review.",
            "19": "- **Increase the scale of finetuning experiments:**\n     - Extend the finetuning experiments to larger datasets and longer training durations to observe the long-term effects of learning and forgetting.",
            "20": "- Investigate the impact of different finetuning strategies, such as curriculum learning or progressive finetuning, on the balance between domain adaptation and general ability retention."
        },
        "6MAbHbYsrO": {
            "0": "* I find the topic of the investigation quite novel.",
            "1": "I believe that the approach taken is original and innovative, in particular building a corpus that allows disentangling style/topic/factual knowledge.",
            "2": "I also like the way LoRa was used to measure the capacity required for learning different facets.",
            "3": "* The authors are sharing the data and code.",
            "4": "* The reported experiments have provided some applicable insights, e.g.",
            "5": "wrt the data mixing.",
            "6": "* Using synthetic data, generated by ChatGPT, might introduce some hidden biases.",
            "7": "It is not given that the same findings could be found if we had natural data.",
            "8": "* It is not clear if the same approach can be generalized to any other characteristics?",
            "9": "Typos:\n* “by just changing the order of decomposition in 1” -> “...in Eq."
        },
        "KX6fDquf1N": {
            "0": "Investigating the changes language models undergo after finetuning continues to be a highly relevant and evolving area of study, despite prior coverage in academic literature.",
            "1": "The research offers key empirical insights into the differential impact of finetuning on language models, revealing a more pronounced effect on style and topic preferences compared to factual knowledge.",
            "2": "These findings enhance our understanding of language model training dynamics and are instrumental in formulating more effective training methodologies.",
            "3": "The researchers conducted extensive experiments on three language models of considerable size, particularly from an academic perspective.",
            "4": "* The assertion that each prediction by a language model can be broken down into components of writing style, topic, and factual knowledge requires further justification or explanation.",
            "5": "The paper should present a stronger argument or provide additional evidence to substantiate this claim.",
            "6": "* The primary message or conclusion of the paper is ambiguous.",
            "7": "The authors need to clarify the central thesis to ensure that readers can grasp the main contribution of the work.",
            "8": "what is the takeaway from this research?",
            "9": "* While the prose is generally lucid, the paper's structure, particularly the introduction, could use refinement to enhance its readability and impact.",
            "10": "For improved clarity and presentation, the following suggestions are offered:\n1.",
            "11": "The introduction would benefit from concrete examples illustrating the key domains of style, topic, and factual knowledge to help contextualize the subsequent findings.",
            "12": "Details regarding the fine-tuning process are scant.",
            "13": "Clarification of which specific models are assessed and the precise nature of the fine-tuning would provide a more robust understanding of the study's scope.",
            "14": "The transition from discussing fine-tuning effects to probing methods in the third paragraph of the introduction is somewhat abrupt.",
            "15": "A smoother segue that connects these topics would aid reader comprehension.",
            "16": "In the fourth paragraph, where style and topic biases are introduced, it would be helpful to include examples or elaborate on what these biases entail to furnish readers with a clearer picture of these concepts."
        },
        "IeQh1AMom9": {
            "0": "- The method how the analysis is performed is novel.",
            "1": "Creating training and evaluation corpora with controlled differences (topics, style, factual knowledge) by prompting instruction following LLMs is interesting and inspiring.",
            "2": "- The analysis is extensive and is performed under various configurations (like the choice of LM, size of the training corpora)\n- The outcomes of analysis are interesting and relevant to future research that study lifelong learning of LMs.",
            "3": "- Although other configurations are very extensive, the choice of training and evaluation corpora and exclusively original or variants of PubMed and C4.",
            "4": "- The three text-generating factors (styles, topics, facts) may not always be clearly separable of extensive enough in every corpora.",
            "5": "The authors discussed this limitation in their limitation section.",
            "6": "- Clarity issue: I feel the plots very hard to read because the captions are too generic and not self-contained.",
            "7": "I suggest to briefly summarize the findings or implications in the captions.",
            "8": "- Clarity issue: some legends in plots such as Figure 4 are not explained in text (e.g.",
            "9": "readers may be confused about \"C4 -factuals\" before they associate them with \"C4-counterfactual\" in Table 1)\n- Though the authors pointed out the hardness of learning factual knowledge without learning style and topic bias, the authors' attempts failed to improve such performance at the end of Sec.",
            "10": "I suggest to provide some future directions about how the analysis will be beneficial the challenge of learning factual knowledge above.",
            "11": "- The authors focused on evaluation of LM loss throughout the paper.",
            "12": "I think this is fine for style and topics, but factual knowledge, evaluating LM loss is not clean enough because only a few tokens in a sentence are related to facts.",
            "13": "The authors could create cloze-style  or question answering evaluation sets that focus exclusively on generation of factual knowledge."
        },
        "tAmKnN5JGR": {
            "0": "- Quality\n\t- The motivation is well-founded and the claims are sound.",
            "1": "- Experimental analysis is very detailed and explanatory.",
            "2": "- Clarity\n\t- Paper is clearly presented and easy to follow.",
            "3": "- Quality\n\t- As the topic of a document can be determined by the factual knowledge it contains then it might be redundant to keep the topic as a relevant factor in the text generation process and only style and factual knowledge might suffice which then could directly align with the syntax and semantics of the underlying text respectively.",
            "4": "- Significance\n\t- This paper presents a detailed technical analysis of the fine-tuning process of a language model on domain-specific downstream tasks/datasets.",
            "5": "However, the outcomes of the study conform with the expected outcomes of fine-tuning a model on domain-specific data and hence this paper misses to provide any significant gainful insight into the fine-tuning process due to the following reasons:\n\t\t- In the PubMed dataset, as academic style is present across all abstracts with different factual knowledge, it is expected that the model will readily adapt to the academic style first before capturing the diverse type of factual knowledge.",
            "6": "- Just like in topic modeling, the topic of a document is a broad sentiment and can be easily determined using a set of keywords.",
            "7": "So, it will be easy for the model to detect/understand the topic of a document before reading the whole document and capturing the factual knowledge inside it.",
            "8": "Therefore, it is expected for the model to easily understand the topic and style factors before capturing the factual knowledge inside it.",
            "9": "- I am keen to hear the response of the authors on this and hope that they can change my point of view."
        }
    },
    "pszewhybU9": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces INSTAG, an innovative open-set instruction tagging method for analyzing the diversity and complexity of supervised fine-tuning (SFT) data for large language models (LLMs).",
            "1": "- The study provides a novel approach to quantify instruction diversity and complexity, which are crucial for improving the alignment of LLMs with human instructions.",
            "2": "- The proposed method leverages the capabilities of ChatGPT to automatically generate fine-grained tags, offering a scalable solution for large datasets.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a significant gap in the current understanding of how to quantify the diversity and complexity of SFT data, which is essential for enhancing LLM performance.",
            "4": "- The introduction of INSTAG and its application to various open-source SFT datasets demonstrate the method's effectiveness and potential for broader applications.",
            "5": "- The experimental results show that models fine-tuned with data selected using INSTAG outperform those trained on considerably larger datasets, highlighting the practical benefits of the proposed approach.",
            "6": "- The paper provides a comprehensive analysis of the impact of data diversity and complexity on model performance, offering valuable insights for future research and development in the field.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology**:\n     - The paper could benefit from a more detailed explanation of the tagging process and the specific prompts used to guide ChatGPT in generating tags.",
            "8": "- The normalization procedure, while described, could be further clarified with more concrete examples and a step-by-step breakdown.",
            "9": "- **Limited evaluation of INSTAG's generalizability**:\n     - The study primarily focuses on a specific set of open-source SFT datasets.",
            "10": "It would be beneficial to see how INSTAG performs on a wider variety of datasets, including those from different domains or with different characteristics.",
            "11": "- The robustness of the tagging method across different LLMs and fine-tuning scenarios is not thoroughly explored.",
            "12": "- **Potential bias in tag generation**:\n     - The reliance on ChatGPT for tag generation may introduce biases inherent in the model, which could affect the quality and consistency of the tags.",
            "13": "- The paper does not sufficiently address how these biases are mitigated or the potential impact on the overall findings.",
            "14": "- **Scalability and efficiency concerns**:\n     - While the paper claims that INSTAG is scalable, the computational resources required for tagging large datasets using ChatGPT are not discussed in detail.",
            "15": "- The efficiency of the proposed method in terms of time and cost compared to other potential approaches is not evaluated.",
            "16": "Suggestions for improvement\n   - **Enhance methodological clarity**:\n     - Provide a more detailed description of the tagging process, including specific examples of prompts and the resulting tags.",
            "17": "- Include a step-by-step breakdown of the normalization procedure with illustrative examples to improve understanding.",
            "18": "- **Broaden the evaluation scope**:\n     - Test INSTAG on a more diverse set of datasets, including those from different domains or with varying characteristics, to demonstrate its generalizability.",
            "19": "- Explore the robustness of the tagging method across different LLMs and fine-tuning scenarios to validate its applicability.",
            "20": "- **Address potential biases**:\n     - Discuss the potential biases introduced by using ChatGPT for tag generation and how these biases are mitigated.",
            "21": "- Include an analysis of the impact of these biases on the overall findings and the quality of the tags.",
            "22": "- **Evaluate scalability and efficiency**:\n     - Provide a detailed analysis of the computational resources required for tagging large datasets using ChatGPT.",
            "23": "- Compare the efficiency of INSTAG in terms of time and cost with other potential approaches to highlight its advantages and limitations.",
            "24": "- **Improve presentation and readability**:\n     - Ensure that all figures and tables are clearly labeled and easy to interpret.",
            "25": "- Consider adding more visual aids, such as flowcharts or diagrams, to illustrate complex processes and enhance reader comprehension."
        },
        "ZxWsaXyKKX": {
            "0": "- The tagging method is interesting, and well-validated both as an analysis and as a selection method.",
            "1": "The use of ChatGPT as a tagger, and the rules used to simplify the generated tags, are novel and effective.",
            "2": "- The data selection method appears well-validated, and achieving strong results with only 6,000 examples is impressive.",
            "3": "The method outperforms a random selection baseline and other popular open models such as Vicuna and WizardLM.",
            "4": "- The analyses of performance against complexity, diversity, and dataset size are useful, providing useful guidelines for future researchers in data collection and selection.",
            "5": "- The method’s reliance on a strong tagging model (in this case, ChatGPT) is not explored, despite a very brief mention of training a distilled tagger model at the end of the work.",
            "6": "It would be interesting and useful to see how well this method works with openly available models, or over a variety of different quality models (e.g.",
            "7": "how does using GPT-4 as a tagger compare?",
            "8": "How about Vicuna?",
            "9": "etc.).",
            "10": "- The tag-based analysis is somewhat restrictive, as discussed in section 4.3.",
            "11": "It would be interesting to take the semantics of the tags themselves into account somehow, since (a) some tags may be closer to each other and so overlap in terms of diversity, and (b) certain tags may express queries that are naturally more complex than others (e.g.",
            "12": "‘solve’ vs ‘inquiry’ tags).",
            "13": "- MT-bench evaluation involves a relatively small number of questions (80), which may be easier to cover with 6,000 examples.",
            "14": "I wonder how well the TagLM model would perform with more questions (e.g.",
            "15": "the alpacaEval setting, which has 800 examples), or on more traditional benchmarks such as MMLU, Big Bench, etc.",
            "16": "In general, it would be interesting to see if the selection method is biased towards certain capabilities compared to others.",
            "17": "Overall, I think this work is strong, and my main qualm is that the evaluation of the data selection method is somewhat weak, only exploring one evaluation setting.",
            "18": "However, the method proposed is interesting and novel, and produces useful insights for data selection in instruction tuning."
        },
        "wxgOxtE7Lk": {
            "0": "- The proposed method is straightforward, intuitive, and simple to implement.",
            "1": "- The study demonstrates that metrics driven by the statistical analysis of automatically generated tags—specifically complexity and diversity—can effectively probe the characteristics of IFT datasets.",
            "2": "- This work provides further empirical evidence that the quality, rather than the quantity, of IFT datasets is crucial for aligning language models successfully.",
            "3": "- The primary mechanism of the proposed method is dependent on the automated, open-ended tagging capability of GPT-4; therefore, there is a risk that the analysis in this paper might be influenced by any inherent biases present within GPT-4.",
            "4": "- Further examination of data instances categorized by the proposed metrics would be advantageous.",
            "5": "Specifically, exploring the semantic or syntactic traits defining IFT datasets identified as diverse and complex by these metrics would be informative.",
            "6": "- The presented work is largely empirical, which may raise concerns within the community regarding the foundational grounding of its results."
        },
        "15E4wpMZSn": {
            "0": "This paper proposes an automatic method to evaluate the diversity and complexity of human instructions.",
            "1": "Based on the proposed InsTag, this paper further presents a method of selecting human instructions, which can potentially reduce the cost of the alignment phase of large language models.",
            "2": "Selecting more diverse and complex samples is an existing idea, and this article is more similar to the implementation and application of this idea.",
            "3": "The definition of complexity seems a little strange.",
            "4": "Is there any explanation from other papers?",
            "5": "If not, I hope the authors can give a more detailed explanation for it.",
            "6": "The author is recommended to verify the performance of the proposed method on more benchmarks."
        },
        "JWoXQgCtoG": {
            "0": "The proposed data measurement and data selection methods are simple, both concept-wise and practice-wise.",
            "1": "The experimental results are strong – TagLM is the only model that achieves such high performance on MT-bench with <10K data examples as far as I know.",
            "2": "The proposed approach can be utilized to measure existing datasets, as shown in Figure 2 which are interesting.",
            "3": "Table 4 indicates that more sft data does not necessarily give better performance and InsTag is able to select the effective ones.",
            "4": "Evaluation is a bit weak, MT-Bench scores are the only metric used across entire paper – other datasets such as AlpacaEval and human evaluation could further strengthen the claims in the paper.",
            "5": "While I appreciate that the authors distill a local tagger in Section 5, it is unknown how much SFT performance would be sacrificed by using the tags from this local tagger.",
            "6": "The strong results on Table 3 is from ChatGPT tagger if I understand correctly, and it may be too expensive to use ChatGPT in practice when we have a large data pool to measure."
        }
    },
    "LWqeF78c1q": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the integration of vision and language in multimodal video transformers, a crucial aspect for advancing AI systems' understanding of the real world.",
            "1": "- It leverages neuroscientific evidence to probe the alignment of these models with brain regions, providing a novel approach to evaluate and improve multimodal models.",
            "2": "- The study's findings contribute to both machine learning and neuroscience, offering insights into how current models process multimodal information and how they can be improved.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and interdisciplinary approach by combining AI and neuroscience to evaluate multimodal models.",
            "4": "- It provides empirical evidence on the alignment of multimodal video transformers with brain regions, contributing to the understanding of cross-modal connections and interactions.",
            "5": "- The study offers practical suggestions for improving multimodal models, such as fine-tuning for tasks requiring vision-language inferences.",
            "6": "- The methodology and findings have implications for both fields, potentially guiding future research and model development.",
            "7": "Potential reasons for rejection\n   - **Limited scope of models and datasets:**\n     - The study investigates only one type of multimodal video model, which may limit the generalizability of the findings.",
            "8": "- The use of a single brain dataset (TV shows) may not capture the full range of multimodal interactions and their relevance to different contexts.",
            "9": "- **Unimodal biases in the TVQA dataset:**\n     - The TVQA dataset used for fine-tuning may have unimodal biases, which could affect the study's conclusions about multimodal interactions.",
            "10": "- The reliance on this dataset may limit the applicability of the findings to other multimodal tasks and datasets.",
            "11": "- **Insufficient evidence of multimodal interactions:**\n     - The study finds no evidence of brain-relevant multimodal interactions in the pre-trained model, which may indicate limitations in the model's ability to capture complex interactions.",
            "12": "- The improvements observed after fine-tuning are limited to specific brain regions, suggesting that the approach may not fully address the challenges of multimodal integration.",
            "13": "Suggestions for improvement\n   - **Expand the scope of models and datasets:**\n     - Investigate a broader range of multimodal video models to assess the generalizability of the findings.",
            "14": "- Use additional brain datasets with different types of multimodal stimuli to capture a wider range of interactions and contexts.",
            "15": "- **Address unimodal biases in the TVQA dataset:**\n     - Consider using or developing benchmark datasets that require true multimodal integration to perform tasks, reducing the impact of unimodal biases.",
            "16": "- Validate the findings with multiple datasets to ensure robustness and applicability to various multimodal tasks.",
            "17": "- **Enhance the analysis of multimodal interactions:**\n     - Explore alternative methods for probing multimodal interactions in models, such as using more complex tasks or stimuli that inherently require integration of multiple modalities.",
            "18": "- Investigate the specific types of information and interactions that contribute to improved brain alignment, providing deeper insights into the mechanisms of multimodal processing.",
            "19": "- **Provide more detailed explanations and visualizations:**\n     - Include more detailed explanations and visualizations of the brain alignment results, particularly for the regions where significant differences are observed.",
            "20": "- Clarify the implications of the findings for both machine learning and neuroscience, highlighting potential avenues for future research and model development."
        },
        "nokzsylf4t": {
            "0": "The paper has some interesting novel findings such as - a)no evidence of brain-relevant multimodal interactions encoded in the pre-trained model.",
            "1": "b) Early and middle layers of the pre-trained and fine-tuned model are similarly brain aligned c)Task-dependent changes in top layers are not aligned with brain representations.",
            "2": "d) Fine-tuning for vision-language question-answering improves brain alignment in some regions.",
            "3": "The paper suggest that while multi-modal transformers show promise in integrating vision and language, there's room for enhancement in aligning them with brain processes.",
            "4": "But it is not clear what might be the potential approaches for better alignment\n2.",
            "5": "The experimentation settings lacks some clarity.",
            "6": "For example, did all the participants watching the TV show actually seeing it first time?",
            "7": "Or some of them might have seen this show before?",
            "8": "Have they heard about the name of the show/synopsis of the show before?",
            "9": "The flow of the paper is hard to follow, the writing could be more concise\n4.",
            "10": "The paper has some interesting novel findings on brain alignment but it is not clear how findings like these are actually impactful"
        },
        "NBefQS6WW5": {
            "0": "The authors use state-of-the-art transformer models\n\nThe authors ask interesting questions about the potential for shared representation of vision and language information\n\nThe highly uncontrolled nature of the stimuli (TV shows) makes the interpretation complex but is also interesting in bringing the questions to real-world relevance.",
            "1": "I could not find any evidence in the paper that the fMRI signals provide either visual or language information during the task.",
            "2": "The first figure after the initial definitions goes on to show correlations between the neural network representations and the fMRI signals but there is no indication of what those fMRI signals are actually representing.",
            "3": "The overall results are extremely weak.",
            "4": "In the best case scenarios the Pearson correlations are about 0.1 and in most cases, they hover between 0.01 and 0.05.",
            "5": "The fraction of explained variance is the square of the Pearson correlation coefficient.",
            "6": "With a correlation of 0.1, that means that the neural network, in the best-case scenario, can explain around 1% of the fMRI signals.",
            "7": "To make matters worse, the conclusions that the authors are interested in drawing are based on comparisons between different conditions.",
            "8": "Take for instance, the first two columns in the second panel in fig.",
            "9": "2 (pCingulate).",
            "10": "Vision+audio yields a correlation of about 0.03 (that is, about 0.0009 of variance explained), whereas only audio yields a correlation of 0.02 (that is, 0.0004 of variance), and vision yields a slightly negative (!)",
            "11": "correlation.",
            "12": "Conclusions are drawn based on a difference of 5x1^(-4) in variance!"
        },
        "0AGfeQZ2Dh": {
            "0": "Utilizing the brain as a benchmark for multi-modal integration provides an intriguing perspective, especially for the machine learning community.",
            "1": "Over the past decade in neuroscience, spurred by seminal works like that of Yamins et al.",
            "2": "(2014), there has been a surge in efforts to correlate deep neural networks with brain activity, yielding significant insights for neuroscience.",
            "3": "In this study, the authors ingeniously invert this approach, seeking to extract insights from neuroscience to better comprehend expansive machine learning models.",
            "4": "This innovative approach is not only captivating but also holds promise if appropriately substantiated.",
            "5": "While the premise of the study is intriguing, it could benefit from further refinement.",
            "6": "A primary concern is the treatment of multi-modal integration in the human brain as a static rather than a dynamic process[2].",
            "7": "Human sensory processing occurs in sequences — visual information traverses from the retina through the thalamus to the visual cortex before reaching the prefrontal cortex.",
            "8": "Audio processing follows a distinct timeline, and semantic systems kick in later.",
            "9": "Additionally, fMRI signals exhibit a delay spanning several seconds.",
            "10": "In contrast, vision-language models lack these temporal delay characteristics.",
            "11": "The study's definition and measurement of alignment seem to bypass these temporal nuances, making it a potential oversight.",
            "12": "Furthermore, the research could have delved deeper into cross-comparisons of different fMRI datasets and vision-language models.",
            "13": "It's imperative to discern whether findings remain consistent across various model choices.",
            "14": "If one model aligns more closely with the brain than another, what implications arise from this?",
            "15": "Does a higher alignment score for Model A over Model B necessarily denote its superiority?",
            "16": "And if so, by what margin?",
            "17": "An intriguing proposition would be to investigate the effects of fine-tuning a model based on its brain alignment score.",
            "18": "What outcomes would this entail, and what do these outcomes signify?",
            "19": "Lastly, the authors' claim about audio-visual information being processed in the Angular Gyrus — a prominent language region — isn't groundbreaking.",
            "20": "This observation is already documented in the neuroscience literature [1,2,3,4].",
            "21": "[1] Thakral, Preston P., Kevin P. Madore, and Daniel L. Schacter.",
            "22": "\"A role for the left angular gyrus in episodic simulation and memory.\"",
            "23": "Journal of Neuroscience 37.34 (2017): 8142-8149.",
            "24": "[2] Chambers, Christopher D., et al.",
            "25": "\"Fast and slow parietal pathways mediate spatial attention.\"",
            "26": "Nature neuroscience 7.3 (2004): 217-218.",
            "27": "[3] Fang, Mengting, et al.",
            "28": "\"Angular gyrus responses show joint statistical dependence with brain regions selective for different categories.\"",
            "29": "Journal of Neuroscience 43.15 (2023): 2756-2766.",
            "30": "[4] Bonnici, Heidi M., et al.",
            "31": "\"Multimodal feature integration in the angular gyrus during episodic and semantic retrieval.\"",
            "32": "Journal of Neuroscience 36.20 (2016): 5462-5471."
        },
        "wjUPrb8BI4": {
            "0": "The topic and study are interesting.",
            "1": "It is interesting to study to what extent the current large models align with human brain.",
            "2": "This paper has also shown some interesting findings.",
            "3": "For example, vision modality significantly improves brain alignment over language regions, etc.",
            "4": "I would need to say sorry, this paper is out of my domain.",
            "5": "I am not able to provide a valid assessment, and I am unable to fully understand this paper.",
            "6": "There are so many terms that I don't understand without background.",
            "7": "For example, \"repetition time\", \"voxel\", \"vision-language integration\", etc.",
            "8": "In addition, I think the writing is unclear and unconcise to me, which might be one of the reasons that make me not able to understand this paper.",
            "9": "There are many very long sentences.",
            "10": "For example, \"This improvement cannot be due to the further processing of language-specific information in the joint encoder since the depth of language input processing is the same in both conditions and is unlikely to be due to vision-only information since these regions are known to support language processing.\"",
            "11": "These make it very hard to read and understand the whole paper in additional to unknown terms.",
            "12": "If that is not my problem, then I think the paper writing needs to be improved.",
            "13": "I do have one suggestion: Section 3.1 \"Model Representations\" should be named as \"Transformer Representations\" or \"Artificial Neural Network Representations\".",
            "14": "As this paper is across domains, it is better to clearly name each term for that specific domain.",
            "15": "\"Model\" can represent many things.",
            "16": "But here I think you are referring to the transformer."
        }
    },
    "hUs8YHAUEr": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical challenge in the field of large language models (LLMs) for code generation, which is the difficulty in generating correct answers in a single attempt.",
            "1": "- The proposed Multi-Perspective Self-Consistency (MPSC) framework introduces a novel decoding strategy that incorporates both inter-consistency across multiple perspectives and intra-consistency within a single perspective.",
            "2": "- The approach is innovative in its use of a multipartite graph to embed consistency information and determine the optimal output, which is a significant advancement over previous methods that only consider intra-consistency.",
            "3": "Potential reasons for acceptance\n   - The MPSC framework demonstrates substantial performance improvements on several popular benchmarks, including HumanEval, HumanEval Plus, MBPP, and CodeContests, surpassing even GPT-4 in some cases.",
            "4": "- The paper provides a comprehensive evaluation and detailed experimental results, showcasing the effectiveness of the proposed method.",
            "5": "- The approach is model-agnostic and task-agnostic, suggesting its potential applicability to a wide range of textual generation tasks beyond code generation.",
            "6": "- The paper is well-structured and clearly explains the methodology, making it accessible and reproducible for other researchers.",
            "7": "Potential reasons for rejection\n   - **Complexity and Practicality**\n     - The proposed method involves generating multiple outputs from various perspectives, which may be computationally expensive and impractical for real-time applications.",
            "8": "- The reliance on a code interpreter for measuring inter-consistency may limit the applicability of the method to other domains where such deterministic measures are not available.",
            "9": "- **Evaluation Scope**\n     - The evaluation is primarily focused on code generation tasks.",
            "10": "The paper does not provide empirical evidence of the framework's effectiveness in other textual generation scenarios, which could limit its perceived generalizability.",
            "11": "- **Clarity and Detail**\n     - Some aspects of the graph construction and optimization process could be explained in more detail to enhance understanding, particularly for readers who may not be familiar with graph-based methods.",
            "12": "- The paper could benefit from a more thorough discussion of the limitations and potential drawbacks of the proposed approach.",
            "13": "Suggestions for improvement\n   - **Expand Evaluation**\n     - Conduct experiments on a broader range of textual generation tasks, such as math problem solving and question answering, to demonstrate the generalizability of the MPSC framework.",
            "14": "- Include a comparison with additional state-of-the-art methods to provide a more comprehensive evaluation of the proposed approach.",
            "15": "- **Optimize Efficiency**\n     - Explore ways to reduce the computational overhead of generating multiple outputs from various perspectives, possibly through more efficient sampling techniques or parallel processing.",
            "16": "- Investigate the feasibility of applying the MPSC framework in real-time applications and provide insights into potential optimizations.",
            "17": "- **Enhance Clarity**\n     - Provide more detailed explanations and visualizations of the graph construction and optimization process to improve clarity and accessibility for readers.",
            "18": "- Include a discussion on the potential limitations and challenges of applying the MPSC framework to other domains, along with suggestions for addressing these challenges.",
            "19": "- **Future Work**\n     - Outline specific future work directions, such as developing methods for measuring inter-consistency in natural language outputs, to encourage further research and development in this area."
        },
        "Pev718ZXIq": {
            "0": "- The authors did a thorough exploration on what factors could potentially affect code generation performances in LLMs, and it's interesting to see that all three perspectives, solution, specification, and test cases all play important roles in improving the final performance.",
            "1": "- The experiments are done on a fairly comprehensive set of code generation models, including GPT, Code Llama and WizardCoder.",
            "2": "- For intra-consistency, the authors also did a comprehensive study over what kind of measure functions can help the most.",
            "3": "- Novelty: [1] already proposed to construct a graph and explore more fine-grained consistency via MAX-SAT solving (although for natural language based reasoning tasks).",
            "4": "The similarities and differences should be better discussed in the current paper.",
            "5": "[1] Jung et al.",
            "6": "Maieutic prompting: Logically consistent reasoning with recursive explanations.",
            "7": "EMNLP 2022.",
            "8": "- Generalizability: although framed as \"multi-perspectives\", this current paper only explores a single use-case of code generation, and with very specific perspectives: solution, specification, and test cases.",
            "9": "It would be interesting to show if this method can be generalized to other tasks (e.g., math tasks, commonsense reasoning, or symbolic reasoning).",
            "10": "- Added complexity and ad-hoc design choices: The current framing adds a lot of complexity to the existing baselines, and many of the design choices are not well justified.",
            "11": "In practice it would be difficult to deploy such a method to ensure optimal performance.",
            "12": "E.g., \n1) designing the perspectives: for each task, how much manual effort is needed to design those perspectives?",
            "13": "(and writing the prompts for each perspective?)",
            "14": "How sensitive is the final performance w.r.t.",
            "15": "the prompts written for each perspective?",
            "16": "2) constructing the graph: each edge needs to be specifically designed (section 3.2), why those choices and how much difference does it make if designed differently?",
            "17": "For intra-consistency, similarly the authors designed many different measures, and based on the experiment results the best measure varies depending on the task.",
            "18": "How would one pick which measure to use in practice?",
            "19": "3) solving the optimization: depending on the number of nodes and edges, the solving part could be very expensive; even with the iterative algorithm, it might take many rounds to reach convergence.",
            "20": "This point needs to be better discussed.",
            "21": "4) choice of parameters: what is the value of \\alpha in experiments?",
            "22": "From Figure 4, on human-eval the performance varies a lot depending on \\alpha.",
            "23": "The final reported performance seems to be the \\alpha that achieves the best performance.",
            "24": "But without a training/dev set, how could one pick \\alpha in an unsupervised way?",
            "25": "- Fair evaluation: Table 2 shows the proposed method yields some gains over several baselines.",
            "26": "But digging deeper, a fair comparison should be made between methods that use *the same number of generated samples*.",
            "27": "MPSC uses many more samples (200 solutions, 100 specifications, 500 test cases) while most of the baselines only use solutions (200 samples only).",
            "28": "In addition, MPSC-label is not a fair comparison given it uses human labels.",
            "29": "1) if given the same number of samples, i.e., baselines with 800 samples vs MPSC, how does the performance compare?",
            "30": "2) what is the variance of the proposed method, given that more samples from different perspectives are drawn?",
            "31": "3) how does the proposed method compare with [2]?",
            "32": "[2] also uses test cases to improve language models' code generation.",
            "33": "4) Table 4 shows the test cases give the most gains, so maybe a simple baseline could be added: use the generated test cases to filter out incorrect generated solutions, and then apply self-consistency on the filtered set.",
            "34": "[2] Chen et al.",
            "35": "Teaching Large Language Models to Self-Debug.",
            "36": "2023."
        },
        "yIlPVyh9AM": {
            "0": "This work investigates both intra-consistency and inter-consistency of LLMs\n2.",
            "1": "The proposed MPSC achieves significant improvement on code generation\n3.",
            "2": "The proposed MPSC can act like a plug-in to enhance other LLMs 1.",
            "3": "The MPSC is complicate and unstable which limits the reproducibility\n2.",
            "4": "The cost of MPSC is very high which limits its application\n3.",
            "5": "The narrative of the paper is not clear"
        },
        "daZ9VTBkql": {
            "0": "The improvement over the baselines is significant.",
            "1": "However, it's unclear whether this improvement is due to the allowance of code execution.",
            "2": "The paper is easy to read.",
            "3": "The method proposed in this paper can be applied to both closed-source large models (like ChatGPT) and open-source large models.",
            "4": "A key issue with the methodology and experiments of this paper is that the proposed method actually requires the execution of code, rather than merely generating it.",
            "5": "For instance, in Listing 3 of Appendix 3, one needs to execute the solution to complete the validation with test cases.",
            "6": "The authors need to emphasize this distinction from other methods.",
            "7": "Clearly, when code execution is allowed, the task becomes easier.",
            "8": "This issue makes the main experimental results of the paper, as shown in Table 2, unfair, as other methods do not require code execution.",
            "9": "Can the method proposed in this paper be applied under conditions where code execution is not allowed?",
            "10": "If so, what are the results?",
            "11": "The authors have not demonstrated this.",
            "12": "Is the learning approach designed in this paper overly complicated?",
            "13": "Is it possible to avoid using w(vi,vj) and f(vi) and directly employ an MLP or other ensemble methods to obtain the answer?",
            "14": "For instance, self-consistency actually uses max-vote directly.",
            "15": "Overly complex optimization algorithms make the methodological contributions of this paper ambiguous.",
            "16": "The specific \"perspectives\" used in this paper do not entirely align with intuition.",
            "17": "For example, in Figure 2, the paper views the solution, specification, and test case as three perspectives, whereas conceptually, they are three answer components."
        },
        "h2IKpFA0Y6": {
            "0": "MPSC is a natural extension of self-consistency for code generation, where the consistency among the solution, test cases and specifications can be precisely verified by code execution.",
            "1": "The experiments show remarkable performance improvement compared to strong baselines that utilize multiple samples and/or test cases.",
            "2": "The main weaknesses of this work are: (1) the implementation details are unclear; and (2) some ablation studies are missing.",
            "3": "Specifically, I have the following questions:\n\n1.",
            "4": "How is MBR-Exec implemented?",
            "5": "I do not understand why MBR-Exec can perform worse than self-consistency.",
            "6": "To my understanding, self-consistency selects the final program based on the exact match; i.e., selecting the most frequently appeared code in all samples.",
            "7": "On the other hand, MBR-Exec selects programs based on the frequency of execution results.",
            "8": "Does MBR-Exec utilize the given test cases as in the original paper?",
            "9": "For MPSC-Label, how are the golden test cases utilized?",
            "10": "Do you directly filter out those programs that do not pass the test cases?",
            "11": "In general I do not understand why MPSC-Weighted Cardinality can sometimes outperform MPSC-Label.",
            "12": "It is interesting to see that GPT-3.5 with MPSC can outperform GPT-4.",
            "13": "However, sampling 200 solutions is still very expensive.",
            "14": "Do you have results with fewer number of samples, e.g., 10 or 100?",
            "15": "What is pass@200, which should be the upper bound of the performance?",
            "16": "It is helpful to add discussion on the quality of generated test cases and specifications.",
            "17": "For example, what are the true positive and false negative rates?",
            "18": "Also, I think MPSC is well-suited for code generation, but how to extend it to other domains remains unclear."
        },
        "5ADLi1fzbC": {
            "0": "- The multi-perspective method is well-motivated and well-suited for tasks like code generation.",
            "1": "- The authors conduct comprehensive evaluation and show significant performance improvement over various baselines.",
            "2": "- The paper is well-written.",
            "3": "- The main limitaiton of the work is the useability on a broader range of tasks.",
            "4": "Despite MPSC is claimed to be task-agnostic, only code generation was presented in the paper, which greatly limits the impact of this work.",
            "5": "On one hand, the authors only study code competition task, and it is unknown whether the framework can work in code generation in the wild.",
            "6": "On the other hand, the authors should consider including at least one more NL task to demonstrate the extensibility of the framework.",
            "7": "- It is unclear whether and how well the framework can generalize towards more perspectives.",
            "8": "In code generation, there are only three perspectives, which is quite limited.",
            "9": "It would be great to think about and demonstrate that MPSC can work with arbitary number of perspectives.",
            "10": "- The perspectives are manually curated now, which can be a limitation for tasks with vague perspective definitions.",
            "11": "It would be great to discuss whether manual curation of perspectives is required and if not how that would impact the end performance."
        }
    },
    "RadQVWAucN": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical challenge in text-based recommendation systems, which is the limitation of incomplete or insufficient information within item descriptions.",
            "1": "- It introduces a novel approach, LLM-R EC, which leverages large language models (LLMs) to augment input text through diverse prompting strategies to enhance personalized recommendations.",
            "2": "- The study explores four distinct prompting techniques: basic prompting, recommendation-driven prompting, engagement-guided prompting, and a combination of recommendation-driven and engagement-guided prompting.",
            "3": "Potential reasons for acceptance\n   - The proposed LLM-R EC framework demonstrates significant improvements in recommendation performance across different datasets, showcasing its effectiveness.",
            "4": "- The approach is versatile and can be applied to various types of items, including well-classified items like movies and more novel, user-generated content like recipes.",
            "5": "- The study provides comprehensive empirical evidence, including detailed experiments and comparisons with baseline models, to support the effectiveness of the proposed method.",
            "6": "- The paper highlights the potential of LLMs to generate high-quality input text that better captures item characteristics and aligns with user preferences, contributing to the field of personalized recommendation.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper could benefit from a more detailed explanation of the prompting strategies and how they are implemented.",
            "8": "- The process of selecting and leveraging important neighbor items for engagement-guided prompting is not thoroughly explained, which may affect reproducibility.",
            "9": "- **Limited evaluation scope:**\n     - The study evaluates the proposed method on only two datasets (Movielens-1M and Recipe), which may not be sufficient to generalize the findings to other domains or types of items.",
            "10": "- The paper does not provide a detailed analysis of the impact of different LLMs (e.g., GPT-3 vs. LLaMA-2) on the recommendation performance, which could be valuable for understanding the robustness of the approach.",
            "11": "- **Insufficient discussion on limitations:**\n     - The paper does not adequately discuss the potential limitations or challenges of the proposed method, such as the computational cost of using LLMs for text augmentation.",
            "12": "- There is a lack of discussion on the potential biases introduced by LLMs and how they might affect the recommendation outcomes.",
            "13": "Suggestions for improvement\n   - **Enhance clarity in methodology:**\n     - Provide a more detailed explanation of the prompting strategies, including specific examples and step-by-step descriptions of how they are implemented.",
            "14": "- Clarify the process of selecting important neighbor items for engagement-guided prompting and provide more details on the neighbor sampling method.",
            "15": "- **Expand evaluation scope:**\n     - Evaluate the proposed method on additional datasets from different domains to demonstrate its generalizability and robustness.",
            "16": "- Include a detailed analysis of the impact of different LLMs on the recommendation performance to provide insights into the effectiveness of various models.",
            "17": "- **Discuss limitations and challenges:**\n     - Include a section discussing the potential limitations and challenges of the proposed method, such as computational cost and potential biases introduced by LLMs.",
            "18": "- Provide suggestions for mitigating these limitations and discuss future research directions to address them.",
            "19": "- **Improve presentation and readability:**\n     - Ensure that all figures and tables are clearly labeled and referenced in the text to enhance readability.",
            "20": "- Consider providing a summary of key findings and contributions at the beginning of the results section to help readers quickly grasp the main outcomes of the study."
        },
        "hj8ybdu8yG": {
            "0": "* Although the idea of using LLM-augmented item description as textual feature for enhancing existing recommender models is in gist similar to the KAR approach, the proposed four prompt templates and the way of using the augmented textual feature is mildly novel in the context of LLM for recommendation.",
            "1": "* The design of prompt templates as well as the combination of different prompts are sound and justified.",
            "2": "* The empirical study supports the effectiveness claim of the proposed approach.",
            "3": "* The limitation discussion of the proposed approach seems to be missing from the paper.",
            "4": "Inherited from LLMs, the proposed approach might have issues like computation efficiency, lacking awareness of latest event and the cost of keeping the model up-to-date, hallucination in general.",
            "5": "Discussing these limitations would provide more practical insights about the proposed approach.",
            "6": "* The proposed approach applied on movie recommendation and recipe recommendation, explore its capability in e-commerce (Like in the KAR paper) or other scenario would further strengthen the generality claim of the paper."
        },
        "jVnOF98Ty5": {
            "0": "A nice part of the paper is the comprehensive evaluations of the 4 prompting techniques.",
            "1": "There are some valuable insights obtained from the evaluations.",
            "2": "However, I do not see too much novelty from the paper beyond the evaluations.",
            "3": "The paper specifically targets personalized recommendation evident from the title.",
            "4": "I suppose the engagement-guided and recommendation+engagement mixed prompting can achieve some degree of personalization with the addition of important neighbors in the prompting.",
            "5": "However, to achieve true personalization, identifying important neighbors is itself a critical problem.",
            "6": "For example, what if there are many neighbors, how do you choose most important neighbors?",
            "7": "Do you take raw engagement counts into account?",
            "8": "Do you value more recent engagements?",
            "9": "The paper does not have not many details on the topics at all.",
            "10": "Without these technical details, the paper becomes just empirical evaluating of several prompting rules."
        },
        "bW73GOmzc1": {
            "0": "Demonstrates the potential of large language models for data augmentation in Recommender Systems (RecSys).",
            "1": "Introduces four prompting methods to extract information from Large Language Models (LLMs).",
            "2": "Inadequate experimental validation of the proposed methods.",
            "3": "Limited applicability to datasets lacking textual information.",
            "4": "Fails to make a clear contribution to the field of RecSys."
        },
        "Ye7rAISHTn": {
            "0": "LLM-Rec investigates diverse prompting strategies to enhance personalized text-based recommendations.",
            "1": "The prompting strategies of recommendation-driven and engagement-guided exhibit the capability to tap into the language model's comprehension of both general and personalized item characteristics.",
            "2": "Empirical experiments show discernible improvements in recommendation performance by incorporating augmented input text generated by LLMs.",
            "3": "Provides insights into the potential of leveraging LLMs for personalized recommendation.",
            "4": "Further experiments explore the potential of concatenating descriptions from multiple prompting strategies, enabling better performance.",
            "5": "The overall presentation is clear.",
            "6": "Some of the mistakes make it confused.",
            "7": "(1) In Table 1, there is a wrong bold in the second column.",
            "8": "(2) “Table 3”in the 7th page is typed as “Figure 3” by mistakes.",
            "9": "(3) The citation at the end of the 5th page is not in the right format.",
            "10": "The evaluation of the prompting strategies could benefit from increased interpretability.",
            "11": "For instance, there is no experiment validating whether engagement-guided prompting effectively utilizes neighbor information to guide the LLM.",
            "12": "It may seem counterintuitive to assert that important neighbor information is considered merely by including item names without a more detailed description and categorization.",
            "13": "The paper lacks a detailed discussion on the potential limitations and challenges of using LLMs for personalized recommendation, such as computational complexity, model interpretability, and potential biases in the generated prompts.",
            "14": "In the main experiment, Table 1 indicates that most of the time, combining both recommendation-driven and engagement-guided strategies leads to a decrease in performance, which appears to contradict the findings in Figure 6 and raises doubts about the generalization of these strategies.",
            "15": "The contribution of this work can be further enhanced if it provides more explainability instead of focus more on improving performance with additional features."
        }
    },
    "XnyZfCerSX": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Ghostbuster, a novel system for detecting AI-generated text, which does not require access to token probabilities from the target model.",
            "1": "- The method leverages weaker language models and a structured search over their features to train a classifier, which is a unique approach compared to existing methods.",
            "2": "- The release of three new datasets for benchmarking AI-generated text detection across multiple domains adds significant value to the research community.",
            "3": "Potential reasons for acceptance\n   - The proposed method achieves impressive performance, with an average of 99.0 F1 across all three datasets, significantly outperforming existing approaches like GPTZero and DetectGPT.",
            "4": "- The approach is robust, generalizing well across different domains and models, which is crucial for practical applications.",
            "5": "- The introduction of new datasets for benchmarking AI-generated text detection is a valuable contribution that can facilitate further research in this area.",
            "6": "- The paper addresses an important and timely issue, given the increasing use of AI-generated text in various applications and the associated concerns about authenticity and trustworthiness.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The description of the structured search and feature selection process could be more detailed and clearer, making it easier for readers to understand and replicate the approach.",
            "8": "- The explanation of how the weaker language models are chosen and their specific roles in the feature generation process is somewhat vague.",
            "9": "- **Limited evaluation on diverse datasets:**\n     - The datasets used for evaluation are predominantly in British and American English, which may limit the generalizability of the results to other languages and dialects.",
            "10": "- The evaluation does not include a wide range of text types, such as technical writing, social media posts, or legal documents, which could provide a more comprehensive assessment of the model's performance.",
            "11": "- **Ethical considerations and potential biases:**\n     - The paper acknowledges the potential for misclassification of non-native English speakers' text as AI-generated but does not provide a detailed analysis or mitigation strategies for this issue.",
            "12": "- The ethical implications of using such a detection system in real-world scenarios, such as in educational settings, are not thoroughly discussed.",
            "13": "Suggestions for improvement\n   - **Enhance clarity and detail in methodology:**\n     - Provide a more detailed explanation of the structured search and feature selection process, including specific examples and pseudocode to aid understanding.",
            "14": "- Clarify the criteria for selecting the weaker language models and their specific contributions to the feature generation process.",
            "15": "- **Expand evaluation to diverse datasets:**\n     - Include additional datasets that cover a wider range of languages, dialects, and text types to better assess the generalizability of the model.",
            "16": "- Conduct experiments on shorter texts and mixed human-AI generated documents to evaluate the model's performance in more varied and realistic scenarios.",
            "17": "- **Address ethical considerations and biases:**\n     - Perform a detailed analysis of the model's performance on non-native English speakers' text and propose specific mitigation strategies to reduce potential biases.",
            "18": "- Discuss the ethical implications of deploying such a detection system in real-world applications, including potential risks and guidelines for responsible use.",
            "19": "- **Improve robustness and generalization:**\n     - Investigate methods to enhance the model's robustness to adversarial attacks, such as paraphrasing or light editing of AI-generated text.",
            "20": "- Explore additional features and techniques to improve the model's generalization across different AI text generation models and prompt variations."
        }
    },
    "d3UGSRLbPo": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of hallucinations in large language models (LLMs), which is a significant barrier to their reliability and widespread adoption.",
            "1": "- The proposed approach of actively detecting and mitigating hallucinations during the generation process is novel and has the potential to improve the trustworthiness of LLMs.",
            "2": "- The work demonstrates the effectiveness of the approach through extensive experiments with different models and tasks, showcasing its broad applicability.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a crucial problem in the field of natural language processing, contributing to the improvement of LLMs' reliability.",
            "4": "- The proposed method is innovative, breaking down the complex task of hallucination detection and mitigation into simpler, manageable steps.",
            "5": "- The experimental results are compelling, showing a significant reduction in hallucinations across various tasks and models.",
            "6": "- The approach is well-documented, with detailed descriptions of the methodology and experiments, making it reproducible and transparent.",
            "7": "Potential reasons for rejection\n   - **Limited scope of evaluation:**\n     - The experiments are primarily conducted on a specific set of tasks and models, which may not fully represent the diversity of real-world applications.",
            "8": "- The evaluation does not include a comprehensive analysis of the approach's performance on a wider range of LLMs and more diverse datasets.",
            "9": "- **Dependence on external knowledge sources:**\n     - The approach relies heavily on web search for validation, which may not always be feasible or reliable in all scenarios.",
            "10": "- The effectiveness of the method could be compromised in environments where access to external knowledge sources is restricted or unavailable.",
            "11": "- **Potential computational overhead:**\n     - The active detection and mitigation process involves multiple steps, including web searches and validation queries, which could introduce significant computational overhead.",
            "12": "- The paper does not provide a detailed analysis of the computational costs and efficiency of the proposed approach, which is crucial for practical deployment.",
            "13": "Suggestions for improvement\n   - **Expand the evaluation scope:**\n     - Conduct experiments on a broader range of tasks and datasets to demonstrate the generalizability and robustness of the approach.",
            "14": "- Include more diverse LLMs in the evaluation to ensure the method's applicability across different model architectures and training paradigms.",
            "15": "- **Address dependence on external knowledge:**\n     - Explore alternative methods for validation that do not rely solely on web search, such as leveraging internal model knowledge or pre-compiled knowledge bases.",
            "16": "- Investigate the impact of varying the quality and availability of external knowledge sources on the performance of the approach.",
            "17": "- **Analyze computational efficiency:**\n     - Provide a detailed analysis of the computational costs associated with the active detection and mitigation process.",
            "18": "- Explore optimizations to reduce the computational overhead, such as parallel processing or more efficient validation techniques.",
            "19": "- **Enhance mitigation strategies:**\n     - Investigate more sophisticated mitigation techniques that can handle complex hallucinations and ensure the coherence and relevance of the generated text.",
            "20": "- Consider incorporating user feedback or interactive elements to refine the mitigation process and improve the overall user experience."
        },
        "gWakadDjSk": {
            "0": "- The multi-step approach presented is generally sound.",
            "1": "The approach can use black-box models hidden behind an API, and several possible solutions for each individual step are presented and evaluated to some extent.",
            "2": "- The experiments are primarily based on GPT-3.5, but there are also experiments with Vicuna-1.5 to validate the results.",
            "3": "In addition, the use of an open model supports easier reproduction and improves the overall accessibility of the presented methodology.",
            "4": "- Hallucinations are a relevant problem with current LLMs and are a limitation to their general applicability.",
            "5": "- The proposed multi-step approach is likely to increase generation latency significantly.",
            "6": "While this is noted superficially, and an improvement for one of the many steps is roughly sketched out, an in-depth discussion is missing - in particular, there are no experiments or theoretical discussions about the overall latency.",
            "7": "I am not of the opinion that high latency is a problem for all use cases, but it would be important to have a proper discussion about this limitation and where it is a problem.",
            "8": "- The overall experimental design is not described in sufficient detail.",
            "9": "In particular, it is not clear how the data used for Section 3.1 relate to those used for Sections 3.2 and 3.3.",
            "10": "If they were the same data, I would be concerned about the reliability of the results in the later sections, since the hyperparameters of each step, such as the aggregation method used to obtain concept uncertainty, are chosen to maximize the metrics in a data set.",
            "11": "- It is not clear to what extent retrieval alone explains the reduction in hallucinations.",
            "12": "Given that the proposed method uses (multiple) web search queries, a natural baseline would be to consider the article generation task based on retrieved facts about the article topic, which would have some favorable properties (e.g., lower latency, less technical complexity) compared to the proposed multi-step approach.",
            "13": "A proper ablation/evaluation against this baseline could help to delineate this effect.",
            "14": "- Some of the design decisions seem to be taken quite ad-hoc; for instance, the choice of a method for key concept identification seems to be based on qualitatively looking at a few examples (Table 4, Section B.1)"
        },
        "Mcbg2Brtfa": {
            "0": "- The proposed method is clearly described.",
            "1": "- The \"propagation of hallucination\" analysis very nicely show the necessity of actively reducing hallucination from the generation.",
            "2": "Although  sentence-by-sentence actively doing retrieval and rewrite has been explored in prior work, there's little quantitive analysis studying how previous hallucination can affect future generations.",
            "3": "- Experimental results indicate that the proposed method is very effective at reducing span-level hallucinations for long-form generation.",
            "4": "- The improvements on multi-hop QA is large, and the gains can be well explained by the \"active\" hallucination detection and revision mechanism.",
            "5": "- It would be nice to highlight the novelty of proposed framework from existing work.",
            "6": "A very related work is [1], where the authors do active retrieval and rewrite actively when decoding each sentence, and they also use LLM output logits to find low-confidence spans for query generation.",
            "7": "There are also several previous works that reduces LLM hallucinations at the response-level, using a similar framework as this work by prompting LLMs for span extraction, query generation, retrieval, and revise.",
            "8": "For example, [2] and [3] uses such a framework to revise LLM responses and reduces hallucination; [4] prompted LLMs for extracting and checking claims as an automatic evaluation framework.",
            "9": "This paper should discuss these related work, discuss the main differences, and maybe consider them as baselines.",
            "10": "- The paper lacks ablations to justify some of its key components.",
            "11": "For example, though there is a strong motivation for applying the method \"actively\" when generating every sentence, the end-to-end evaluation does not show how it helps reduce hallucination compared to applying it at the end of the generation.",
            "12": "Similarly, I couldn't find ablation for only fact-checking low-confidence phrases v.s.",
            "13": "fact checking all key phrases.",
            "14": "- The presentation quality can be improved.",
            "15": "Section 2 enumerates many modeling choices for each component, but it is difficult to tell what is the final method being used, and why it works better than the others.",
            "16": "A suggestion is to describe the best approach in section 2, and leave other choices to ablation studies.",
            "17": "Section 3 and 4 cover many experiments, making it confusing to tell which is the most experiment and what are the main messages.",
            "18": "[1] Jiang, Zhengbao, et al.",
            "19": "\"Active retrieval augmented generation.\"",
            "20": "arXiv preprint arXiv:2305.06983 (2023).",
            "21": "[2] Gao, Luyu, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao et al.",
            "22": "\"Rarr: Researching and revising what language models say, using language models.\"",
            "23": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.",
            "24": "16477-16508.",
            "25": "2023.",
            "26": "[3] Chen, Anthony, et al.",
            "27": "\"PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions.\"",
            "28": "arXiv preprint arXiv:2305.14908 (2023).",
            "29": "[4] Min, Sewon, et al.",
            "30": "\"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.\"",
            "31": "arXiv preprint arXiv:2305.14251 (2023)."
        },
        "30KmVVyPDS": {
            "0": "The paper works on an important problem of mitigating hallucinations \nThe paper presents an early detection approach and demonstrates effectiveness with two LLMs \nThe paper is extremely well-written, with clear goals, a well-described approach, and a detailed Appendix\n\nWhile it is always possible to nitpick on experimental design issues, we need to be mindful of the fact that this work is presented within the scope of a single ICLR submission.",
            "1": "With that in mind, the paper does an excellent job.",
            "2": "It is unclear how effective this method will be for generations beyond the first five sentences.",
            "3": "Post-rebuttal response: After going over the discussions, reviews, and rebuttals.",
            "4": "I feel that my initial assessment of the paper had gaps.",
            "5": "I lean towards the majority view that the paper needs improvement.",
            "6": "I have updated my scores accordingly."
        },
        "wAzNud1tNl": {
            "0": "The paper addresses the topic of hallucinations which is a relevant and timely topic.",
            "1": "The paper does not mentioned the highly relevant work of Kadavath et al., [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221), which also uses the model uncertainty to detect hallucinations.",
            "2": "Since uncertainty is used as a major signal in the proposed pipeline, the novelty of the proposed approach is not clear.",
            "3": "The paper does not study important choices in details.",
            "4": "For instance, the web search procedure is not very clear.",
            "5": "The paper says “In some cases, multiple web searches were required to check the correctness of different facets of a sentence”.",
            "6": "Are these searched human-supervised?",
            "7": "What are the stopping criteria?",
            "8": "I would suggest adding the web-search procedure in an algorithm block so that the readers can understand it better.",
            "9": "Similarly, the paper does not discuss exactly what kind of \"important\" concepts are identified by the model.",
            "10": "Could you provide some examples?",
            "11": "Are the models supposed to extract all relevant concepts?",
            "12": "Is the concept extraction supposed to work well across different application domains (e.g., questions answering)?",
            "13": "What if we are working with non-instruction tuned models?",
            "14": "It is not clear how good the instruction models were at following different instructions in Table 3.",
            "15": "Did the authors perform a systematic analysis here?"
        }
    },
    "vg31nvdrzF": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Self-Context Adaptation (SeCAt), a novel self-supervised approach that enables small visual language models to perform open-ended few-shot learning.",
            "1": "- The method leverages clustering and pseudo-caption generation to create self-contexts, which are used to fine-tune the model without requiring labeled data.",
            "2": "- The approach demonstrates that small models (around 1B parameters) can outperform larger models in few-shot learning tasks, challenging the notion that model scale is crucial for such tasks.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a significant problem in the field of visual language models by enabling small models to perform few-shot learning, which has practical implications for those with limited computational resources.",
            "4": "- The proposed method is innovative and well-motivated, providing a new way to leverage self-supervised learning for few-shot tasks.",
            "5": "- The experimental results are comprehensive and demonstrate the effectiveness of the approach across various datasets and task granularities.",
            "6": "- The paper is well-written and clearly explains the methodology, making it accessible to a broad audience.",
            "7": "Potential reasons for rejection\n   - **Limited evaluation on more complex tasks**\n     - The paper primarily focuses on few-shot learning tasks and does not extend the evaluation to other open-ended vision-language tasks such as image captioning or visual question answering.",
            "8": "- The generalizability of the approach to more complex and diverse tasks remains uncertain.",
            "9": "- **Dependence on specific clustering and pseudo-caption generation techniques**\n     - The success of the method relies heavily on the clustering and pseudo-caption generation process, which may not be universally applicable or optimal for all types of datasets.",
            "10": "- The paper does not explore alternative clustering methods or the impact of different pseudo-caption generation strategies in detail.",
            "11": "- **Scalability and efficiency concerns**\n     - While the method is designed to be efficient, the clustering and pseudo-caption generation steps may still be computationally intensive for very large datasets.",
            "12": "- The paper does not provide a detailed analysis of the computational cost and scalability of the approach.",
            "13": "Suggestions for improvement\n   - **Extend evaluation to more complex tasks**\n     - Include experiments on other open-ended vision-language tasks such as image captioning and visual question answering to demonstrate the broader applicability of the approach.",
            "14": "- Provide a more detailed analysis of the performance on these tasks to highlight the strengths and limitations of the method.",
            "15": "- **Explore alternative clustering and pseudo-caption generation techniques**\n     - Investigate the impact of different clustering algorithms and pseudo-caption generation strategies on the performance of the model.",
            "16": "- Provide a comparative analysis to identify the most effective techniques for various types of datasets.",
            "17": "- **Analyze computational cost and scalability**\n     - Include a detailed analysis of the computational cost and scalability of the approach, particularly for very large datasets.",
            "18": "- Discuss potential optimizations and strategies to improve the efficiency of the clustering and pseudo-caption generation steps.",
            "19": "- **Provide more qualitative examples**\n     - Include additional qualitative examples and visualizations to illustrate the effectiveness of the self-context adaptation process.",
            "20": "- Highlight specific cases where the method performs particularly well or struggles, providing insights into the strengths and limitations of the approach."
        },
        "17SSIbwA9s": {
            "0": "It is an interesting and important topic to explore the potential few-shot learning ability on small-scale vision language models.",
            "1": "The overall method is simple and clear.",
            "2": "The main problem of this paper is that their design and experiments can not support the claim that the proposed model has the few-shot or in-context learning ability.",
            "3": "Few-shot learning for vision language models requires the model to first infer the underlying task according to the context and give the accordingly response.",
            "4": "The paper only includes the fast concept binding task which exactly matches the proposed training paradigm as an evaluation for the few-shot learning ability.",
            "5": "The proposed training method only teaches the model to pick one \"category\" name from the given context based on the visual similarity between the query image and context images instead of learning general multi-modal abilities.",
            "6": "Therefore, it is foreseeable to have good performance on those fast concept binding tasks compared with other real vision-language models.",
            "7": "The authors should test the model on different tasks (eg.",
            "8": "VQA) in the few-shot setting to validate its claim.",
            "9": "Otherwise, the position of this paper should be reconsidered and might need to be compared with meta-learning methods."
        },
        "W4BwlcQnkH": {
            "0": "They authors define an effective recipe for few-shot visual language models alignment that boosts performance of small models.",
            "1": "The introduction of the \"self-context\" is an extremely interesing concept as it can be controlled arbitrarily to match granularity/complexity of the task at hand.",
            "2": "Kudos for the plan to release the code.",
            "3": "The concept of k-ways should be explained better when it is introduced.",
            "4": "I found myself having to go back multiple times while reading the work.",
            "5": "The quantitative comparisons are missing the strongest baseline (OpenFlamingo, the upper bound).",
            "6": "For completeness and transparency it should be included."
        },
        "ahAOwhDKnl": {
            "0": "- This paper investigates a question that is both scientifically interesting and practically important: is large scale necessary for emerging few-shot learning capabilities in VLMs?",
            "1": "- The paper is well-written and easy to follow for the most part\n- It is an interesting finding that smaller language models can match or exceed the performance of larger ones on some tasks - Some related work is missing.",
            "2": "The authors should cite work that attempts to automatically construct few-shot learning tasks without supervision / class labels from the ‘older’/’traditional’ few-shot literature too (e.g.",
            "3": "in the context of few-shot classification on vision tasks) and newer.",
            "4": "Some examples are [A, B] (see references below, and several additional references therein).",
            "5": "- The few-shot learning problems used for evaluation are quite simple.",
            "6": "It would be great to apply this approach on other multi-modal few-shot learning tasks from related work (e.g.",
            "7": "Flamingo)\n\n- I’m a bit concerned that the few-shot binding task in particular is one that the proposed method is particularly suited for, and thus the reported results may be more optimistic than they may have been for different few-shot learning tasks.",
            "8": "The reason I think this is that the particular way in which self-contexts are formed for finetuning can be thought of as ‘binding’ visual concepts to new (random / unrelated) nouns.",
            "9": "So the proposed approach can be viewed as finetuning the LM specifically on ‘binding few-shot tasks’ and thus as being ‘tailor-made’ for such tasks.",
            "10": "- The motivation of the proposed pseudo-labeling and self-context creation of SeCAt should be strengthened.",
            "11": "Why is it better than finetuning on actual paired image-text as is done in previous works?",
            "12": "If I understand the argument correctly, the point is to simulate *any* new concept, and teach the model to in-context learn by figuring out to match ‘symbols’ to new items, rather than recalling known semantic relationships between words and visual concepts.",
            "13": "And the argument is that this is more important for smaller models?",
            "14": "It would however greatly strengthen the motivation of the current approach to actually demonstrate empirically the failure of using ‘standard’ interleaved text-image finetuning versus their proposed method (in the context of exactly the same small language models).",
            "15": "It would be very interesting to investigate how the relative performance of those two (usual interleaved text-image finetuning versus SeCAt) changes as the size of the model increases.",
            "16": "Please let me know if this is already one of the baselines and I’m missing it.",
            "17": "- Related to the above point, for the ablation in Table 3b, aside from ‘nonsense’, ‘numbers’ and ‘nouns’, an additional row could be for assigning the actual ‘matching’ noun.",
            "18": "The implicit hypothesis if I understand correctly is that that would be worse, but it would be good to examine this empirically.",
            "19": "- For ablation 3e that studies the impact of the model size on few-shot learning capabilities, there is an important aspect that isn’t studied, namely, how much does adding SeCAt *improve* the few-shot learning capabilities of models of different sizes (that is, plotting the relative improvement of few-shot learning in the ‘raw’ model versus after SeCAt finetuning).",
            "20": "And ideally, tying this in with what I wrote above, this would also be compared to the relative improvement that would have been obtained by more ‘standard’ interleaved text-image finetuning.",
            "21": "References\n=========\n- [A] Unsupervised Learning Via Meta-Learning.",
            "22": "Hsu et al.",
            "23": "ICLR 2019.",
            "24": "- [B] Unsupervised Meta-learning via Few-shot Pseudo-supervised Contrastive Learning.",
            "25": "Jang et al.",
            "26": "ICLR 2023."
        },
        "TjEGM8CLNT": {
            "0": "(1) The paper is easy to follow and well-written.",
            "1": "Especially, figures are well-designed and ideas are well presented.",
            "2": "(2) Empirical results show the strong performance of the proposed method.",
            "3": "They provide sufficiently good ablation studies to deeper understand the logic behind the performance gain.",
            "4": "While the paper is interesting to read, I have several major concerns about novelty and effectiveness of the method.",
            "5": "(1) Using the unrelated name to improve the in-context learning ability is not new in language model [1].",
            "6": "While the domain is slightly different, author need to discuss the difference with the study and clarify the contribution.",
            "7": "Especially, according to [1], they show that symbol tuning does not improve performance on small scale model (with relevant labels) does not improve performance, which is somewhat contradicting with the claim in this paper.",
            "8": "[1] Symbol tuning improves in-context learning in language models\n\n\n(2) While the paper claim that it is open-ended, the evaluated task is limited to the standard few-shot classification task.",
            "9": "In other words, the trained model is strongly biased toward generate shorter sentences compared to the existing methods like Frozen and FROMAGe, which are not designed to just solve classification.",
            "10": "In that sense, I think author should compare the method with the standard few-shot baseline, rather than just comparing with the pre-trained vision language models."
        }
    },
    "iSAgvYhZzg": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant challenge in the field of autonomous user interface (UI) agents by proposing a novel multimodal solution, Auto-UI, which directly interacts with the interface without relying on environment parsing or application-specific APIs.",
            "1": "- The introduction of the chain-of-action technique, leveraging previous action histories and future action plans, is a novel approach that enhances the agent's decision-making capabilities.",
            "2": "- The evaluation on a new device-control benchmark, AITW, with 30K unique instructions, demonstrates the practical applicability and effectiveness of the proposed method.",
            "3": "Potential reasons for acceptance\n   - The proposed Auto-UI achieves state-of-the-art performance with an action type prediction accuracy of 90% and an overall action success rate of 74%, which is a significant improvement over existing methods.",
            "4": "- The chain-of-action technique is a novel contribution that effectively addresses the challenges of inference inefficiency and error propagation in autonomous UI agents.",
            "5": "- The paper provides a comprehensive evaluation on a large-scale benchmark dataset, demonstrating the robustness and generalizability of the proposed method across various tasks and environments.",
            "6": "- The approach does not require additional annotations or screen parsing, making it more practical and easier to deploy in real-world applications.",
            "7": "Potential reasons for rejection\n   - **Lack of detailed comparison with existing methods:**\n     - The paper could provide more detailed comparisons with existing state-of-the-art methods, particularly in terms of computational efficiency and error rates.",
            "8": "- A more thorough analysis of the limitations and potential drawbacks of the proposed method compared to existing approaches would strengthen the paper.",
            "9": "- **Limited discussion on scalability and real-world deployment:**\n     - The paper does not provide sufficient discussion on the scalability of the proposed method to different device types and screen resolutions.",
            "10": "- There is a lack of detailed analysis on the potential challenges and solutions for deploying the proposed method in real-world applications.",
            "11": "- **Insufficient ablation studies:**\n     - While the paper includes some ablation studies, more detailed experiments isolating the impact of each component of the chain-of-action technique would provide a clearer understanding of their individual contributions.",
            "12": "- Additional ablation studies on different model architectures and hyperparameters would further validate the robustness of the proposed method.",
            "13": "Suggestions for improvement\n   - **Enhance comparison with existing methods:**\n     - Provide more detailed comparisons with existing state-of-the-art methods, including computational efficiency, error rates, and other relevant metrics.",
            "14": "- Include a discussion on the limitations and potential drawbacks of the proposed method compared to existing approaches.",
            "15": "- **Discuss scalability and real-world deployment:**\n     - Include a detailed discussion on the scalability of the proposed method to different device types, screen resolutions, and real-world environments.",
            "16": "- Address potential challenges and solutions for deploying the proposed method in real-world applications, including considerations for computational resources and user experience.",
            "17": "- **Expand ablation studies:**\n     - Conduct more detailed ablation studies isolating the impact of each component of the chain-of-action technique to provide a clearer understanding of their individual contributions.",
            "18": "- Perform additional ablation studies on different model architectures, hyperparameters, and training settings to further validate the robustness and generalizability of the proposed method.",
            "19": "- **Improve clarity and presentation:**\n     - Enhance the clarity and presentation of the methodology section by providing more detailed explanations and visualizations of the proposed model architecture and techniques.",
            "20": "- Include more detailed examples and case studies to illustrate the practical applications and benefits of the proposed method in real-world scenarios."
        },
        "pS9WH5HwaI": {
            "0": "The proposed Auto-UI approach demonstrates a level of originality in addressing the challenges of autonomous user interface agents.",
            "1": "By directly interacting with the interface instead of relying on environment parsing or application-specific APIs, it offers a novel solution that bypasses common inefficiencies and risks associated with existing approaches.",
            "2": "The introduction of the chain-of-action technique also adds a unique element to the decision-making process of the agent.",
            "3": "The approach is evaluated through experiments with the AITW benchmark.",
            "4": "The inclusion of 30,000 unique instructions covering various multi-step tasks provides a comprehensive assessment of the Auto-UI system.",
            "5": "Achieving a state-of-the-art performance demonstrates the effectiveness and reliability of the proposed solution.",
            "6": "Overall, the paper is clear and easy to follow.",
            "7": "The text provides a clear description of the challenges faced by existing approaches, introduces the Auto-UI solution, and explains the chain-of-action technique.",
            "8": "The inclusion of experimental results contribute to a clear understanding of the proposed methodology and its performance.",
            "9": "By addressing the challenges of inference inefficiency and error propagation, Auto-UI offers a more efficient and reliable approach to task automation.",
            "10": "The multimodal solution and the elimination of environment parsing and reliance on application-specific APIs provide a significant advancement in the development of autonomous UI agents.",
            "11": "Furthermore, the state-of-the-art performance achieved on the AITW benchmark showcases the practical applicability and potential impact of the proposed approach.",
            "12": "While the authors highlight the chain-of-action technique as a contribution, it appears to primarily concatenate the output actions, which can be confusing.",
            "13": "It would be helpful to provide a more detailed explanation or clarification of how the chain-of-action technique enhances the decision-making process and contributes to the overall effectiveness of the Auto-UI approach.",
            "14": "The experiment section lacks an explanation for the rationale behind selecting specific baselines.",
            "15": "It would be valuable to include a justification for choosing the particular baselines used in the evaluation.",
            "16": "Additionally, providing information on the performance of a GPT4 model, if available, would offer a useful benchmark to compare the performance of the proposed Auto-UI approach."
        },
        "ydnTigJlJ8": {
            "0": "- The proposed framework is claimed to be much lighter weight than methods that try to take the whole web information into textualized format for agents to comprehend.",
            "1": "- The formatted action is sound and should be generalizable to other web-search domains.",
            "2": "- The paper is pretty easy to follow, with illustrations onto the points.",
            "3": "- The generalization ablation studies are helpful to gauge the capacity of the proposed framework.",
            "4": "- The paper does not describe much about the actual training details, in that sense, to me, the proposed method is still a kind of BC, where the target decoding is optimized towards mimicking the golden action sequences.",
            "5": "(Unless some RL or other mechanism is used here, which is not described.)",
            "6": "In my opinion, the novelties here mainly lie in the multimodal representations (both modality taken into account) and the format of the action performed.",
            "7": "- I’m a bit skeptical about the ICL baseline, first of all more details (e.g., how actions are represented, how OCRed results are used) of that baseline need to be described, at least in the appendix.",
            "8": "Secondly, it also needs to be evaluated at the action plan level, my guess is that this method should be quite accurate on those but might fail more on the lower-level executions.",
            "9": "Thirdly, it is indeed unfair simply because the model is not taking the images into account, which could be the key towards the success of the proposed method in this work.",
            "10": "So, at least a multimodal version of it needs to be taken into consideration, or, a better spatial representation of the html syntax is required.",
            "11": "(HTML can be many times too coarse to represent a spatial layout.)",
            "12": "- Similar to above, the third baseline, fine-tuning LLMs, need to have a version with multimodal inputs.",
            "13": "- An error analysis is required both on the quantitative and qualitative sides, what are the major errors that these models exhibit?"
        },
        "kltAhQRXQy": {
            "0": "It is novel that the paper pays attention to the limitations in the real-world applications of autonomous agents and seeks to provide an agent that does not need extra intermediate environment parsing or interval application-dependent APIs.",
            "1": "The paper proposes a chain-of-action technique which helps the agent to decide step-by-step.",
            "2": "The Figure 1 in this paper is somewhat not clear enough, making it difficult to understand the two paradigms in (a) and (b).",
            "3": "The author does not provide a specific explanation of the Sandbox Paradigm and the First Principles Thinking Paradigm, which is confused.",
            "4": "We find some grammar mistakes in the paper, for example, on page 2, paragraph 2, line 5, do you want to express inefficiency instead of efficiency?",
            "5": "The authors don't explain exactly what touch_point, lift_point, etc.",
            "6": "mean in the first place, causing some confusion.",
            "7": "The authors do not provide a specific example between Auto UI and other baselines in Section 5, which is not clear to understand the effectiveness of the provided Auto UI."
        },
        "8rGkFYabUM": {
            "0": "This work proposes a chain of action operation, leveraging the action history and future actions for current action prediction.",
            "1": "Based on Llama 2, it incorporates a pretrained image encoder into the pretrained LLM for action decision, and shows promising results on AITW dataset.",
            "2": "A potential weakness is where is the gain from?",
            "3": "It looks PaLM and ChatGPT are pretty low on this dataset, while they only take text input, and BC models and Auto-UI models take image screen as input, and get very high results, it is unclear where is the gain from?",
            "4": "image encoder?",
            "5": "or a chain of action input?"
        }
    },
    "DQCZiKb3Uy": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant challenge in reinforcement learning (RL) by proposing a novel approach to leverage vision-language models (VLMs) for generating promptable representations that can be used to initialize RL policies.",
            "1": "- The approach aims to bridge the gap between the extensive world knowledge encoded in VLMs and the need for RL agents to quickly learn new behaviors and tasks.",
            "2": "- The novelty lies in using VLMs to generate task-specific embeddings through prompts, which are then used to train RL policies, potentially improving performance on visually-complex tasks.",
            "3": "Potential reasons for acceptance\n   - The proposed method demonstrates a clear improvement over existing approaches, as evidenced by the experimental results on Minecraft tasks.",
            "4": "- The paper provides a thorough evaluation of the approach, including comparisons with baseline methods and ablation studies to highlight the importance of different components.",
            "5": "- The approach is well-motivated and addresses a relevant problem in the field of RL, making it a valuable contribution to the community.",
            "6": "- The method is flexible and can be applied to various RL tasks, potentially benefiting a wide range of applications.",
            "7": "Potential reasons for rejection\n   - **Limited scope of evaluation:**\n     - The experiments are conducted only on Minecraft tasks, which may not fully represent the diversity of RL tasks in real-world applications.",
            "8": "- The generalizability of the approach to other domains and tasks is not thoroughly explored.",
            "9": "- **Complexity of prompt design:**\n     - The process of designing effective prompts is currently manual and may require domain-specific knowledge, which could limit the approach's applicability.",
            "10": "- The paper does not provide a detailed discussion on how to automate or simplify the prompt design process.",
            "11": "- **Dependence on pre-trained VLMs:**\n     - The approach relies heavily on the quality and capabilities of pre-trained VLMs, which may not always be available or suitable for all tasks.",
            "12": "- The paper does not address potential limitations or challenges in using different VLMs or adapting them to specific tasks.",
            "13": "- **Computational resources:**\n     - The method involves using large pre-trained models, which may require significant computational resources for both training and inference.",
            "14": "- The paper does not discuss the computational cost or efficiency of the approach in detail.",
            "15": "Suggestions for improvement\n   - **Expand the evaluation:**\n     - Conduct experiments on a wider range of RL tasks and domains to demonstrate the generalizability and robustness of the approach.",
            "16": "- Include comparisons with additional baseline methods and state-of-the-art approaches in different domains.",
            "17": "- **Automate prompt design:**\n     - Develop and evaluate methods for automating the prompt design process, potentially using techniques like reinforcement learning or optimization.",
            "18": "- Provide guidelines or best practices for designing effective prompts for different types of tasks.",
            "19": "- **Address VLM limitations:**\n     - Discuss potential limitations and challenges in using different VLMs, including their availability, suitability, and adaptability to specific tasks.",
            "20": "- Explore ways to fine-tune or adapt VLMs to improve their performance on specific RL tasks.",
            "21": "- **Optimize computational efficiency:**\n     - Investigate methods to reduce the computational cost of the approach, such as model compression, distillation, or efficient inference techniques.",
            "22": "- Provide a detailed analysis of the computational resources required for training and inference, and discuss potential trade-offs between performance and efficiency."
        },
        "YCh26JGSxU": {
            "0": "[1] The method proposed in this paper, PR2L, is very straightforward, and the story sounds reasonable.",
            "1": "The organization of the paper is very clear, and it is polished well.",
            "2": "[2] The experimental analysis is quite comprehensive, and ablation experiments have demonstrated the effectiveness of each part of the proposed method.",
            "3": "Conducting experiments in a challenging environment like Minecraft is persuasive.",
            "4": "[1] Concerns about computational cost.",
            "5": "Obtaining promptable representation requires running a complex VLM on every image returned from the environment, along with the prompt and answer.",
            "6": "This cost is prohibitively high and not practical.",
            "7": "[2] Concerns about the performance of the learned policy.",
            "8": "The visually rich representation obtained at such a high computational cost should greatly improve the performance of the policy.",
            "9": "However, the authors did not provide videos of rollouts on relevant tasks, making it difficult to judge the effectiveness of the promptable representation in a real Minecraft environment (the three tasks used in the paper are not very complex).",
            "10": "[3] Insufficient literature review.",
            "11": "Since the authors conducted experiments in Minecraft, they should have provided a more comprehensive discussion of articles that control and plan within Minecraft.",
            "12": "However, the authors left out the following important literature.",
            "13": "Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction.",
            "14": "Video pretraining (vpt): Learning to act by watching unlabeled online videos.",
            "15": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.",
            "16": "CLIP4MC: An RL-Friendly Vision-Language Model for Minecraft.",
            "17": "GROOT: Learning to Follow Instructions by Watching Gameplay Videos.",
            "18": "Learning from Visual Observation via Offline Pretrained State-to-Go Transformer."
        },
        "xI0hli5jGN": {
            "0": "+The study is relevant and could be of interest to many audiences with a background in large models, reinforcement learning, and representation learning.",
            "1": "+The method is well-motivated and technically sound.",
            "2": "Pretrained VLM indeed provides open vocabulary and even knowledge-aided representations for multimodal input, which can be quite beneficial to open-world environments.",
            "3": "Plus, it is plausible to tweak the representation further via prompting.",
            "4": "This is a very neat idea.",
            "5": "+The results look impressive.",
            "6": "Although the method is only evaluated on limited (3) tasks in a single environment (Minecraft), the advantages over the baselines and ablative approaches are significant.",
            "7": "I do think the authors did a good job of comparing it against several interesting baselines, including no generation, no prompt, etc.",
            "8": "Having said those above, I do have some major concerns about the evaluation part of this paper.",
            "9": "I also would like to point out some minor issues as well.",
            "10": "-Albeit the promises shown by the results on 3 tasks on Minecraft, I don't think the approach is thoroughly evaluated, especially given their claim on \"leverage contextual prior information\" and \"visually-complex RL tasks\" (see abstract).",
            "11": "I have the following suggestions:\n\n1) There are some other approaches that are designed to tackle similar issues, especially in Minecraft, ex.",
            "12": "[1,2,3].",
            "13": "Although I agree some settings could be different (RL vs. IL, etc), they all deliver some backbone design or objective functions that could facilitate better representations.",
            "14": "Comparisons against these missing baselines would help the reader with a better understanding of the significance of the proposed method.",
            "15": "2) Minecraft is indeed a challenging domain in terms of open-world and complex visual observations.",
            "16": "However, the tasks being evaluated here (spider, cow, sheep) do not seem to be challenging enough to justify the effectiveness of the proposed method, especially on the claimed \"leverage contextual prior information\".",
            "17": "These mobs are indeed very common and the tasks themselves do not seem to involve complex visual stimuli.",
            "18": "My suggestion is to try some long-term and open-ended tasks like surviving, collecting items, etc.",
            "19": "[7] offers a few of them and worth taking a look at.",
            "20": "Minor: some references on open-world representation learning and Minecraft agents should be cited: [1-6].",
            "21": "[1] open-world control: https://arxiv.org/abs/2301.10034\n\n[2] VPT: https://arxiv.org/abs/2206.11795\n\n[3] STG transformer: https://arxiv.org/abs/2306.12860\n\n[4] DEPS: https://arxiv.org/abs/2302.01560\n\n[5] Plan4MC: https://arxiv.org/abs/2303.16563\n\n[6] GITM: https://arxiv.org/abs/2305.17144\n\n[7] MCU: https://arxiv.org/abs/2310.08367"
        },
        "nZPUgdAv07": {
            "0": "PR2L presents an interesting and creative way of utilizing pre-trained VLMs as representations for visual policy learning.",
            "1": "It is unlike prior pre-trained representations for control work in which the features are generic (i.e., directly encoding the image observation); it is also different from recent Vision-Language-Action work (Brohan et al., 2023) in that it does not require fine-tuning a pre-trained VLM and enable high-frequency policies that are disentangled from the VLM backbone.",
            "2": "As VLMs are increasingly adopted in decision-making pipelines, PR2L is a timely work that presents a lightweight and simple alternative to the existing literature.",
            "3": "The paper itself is generally well-written and free of grammatical errors.",
            "4": "This paper's weaknesses mainly lie in its experimental methodologies.",
            "5": "The only form of prompt that PR2L uses essentially amounts to object detection in the scene.",
            "6": "This introduces a confounding factor of whether PR2L outperforms baselines because it is able to recognize objects better in the scene.",
            "7": "The paper claims that the prompts are different from instruction following; however, the prompts are still manually constructed and task-specific.",
            "8": "It's unclear the advantage of doing so as instructions, by construction, should exist as it is a direct form of task specification.",
            "9": "The improvements of PR2L over its various ablations appear only moderate.",
            "10": "Furthermore, the best prompt format for the tasks are not consistent; for Spider, it is helpful to include contextual information of what a spider looks like in MineCraft, whereas for the other two tasks, it is more helpful to disregard such information.",
            "11": "Therefore, applying PR2L to a new task may require substantial prompt engineering for the best performance.",
            "12": "PR2L is only evaluated on 3 tasks; these tasks are also the simplest in the MineDojo benchmark.",
            "13": "The paper would be strengthened if more tasks and domains are evaluated.",
            "14": "Currently, it is not convincing that PR2L can be generally applied to other visuomotor control domains.",
            "15": "Relatedly, PR2L does not outperform MineCLIP on most tasks; given that MineCLIP exists and is open-sourced, PR2L's stated advantages can be better demonstrated via a new domain in which foundation pre-trained representations do not already exist.",
            "16": "BLIP-2's vision encoder may not be the strongest baseline for pre-trained vision encoders.",
            "17": "Several prior works such as VC-1, R3M, MVP, VIP are trained for decision-making and robotics tasks and may constitute stronger baselines in that category."
        },
        "BZDCpA1kdy": {
            "0": "- The main contribution of the paper “Prompting VLM via auxiliary information and task context” allows extracting more meaningful representation from a VLM is quite interesting and easily applicable to a range of tasks.",
            "1": "Instead of fine-tuning VLM for specific domains, it’s easier to plug-and-play existing VLM and extract meaningful representations via prompting.",
            "2": "- Overall, the paper is well written and is systematic in its evaluation.",
            "3": "I also appreciate the authors willingness to address concerns preemptively (lack of visual tokens as input to the policy MLP, not fine-tuning VLM similar to RT-1).",
            "4": "- Given that the approach is using a VLM, it’d be nice to test the model for “unseen” tasks, containing objects and instructions not seen during training.",
            "5": "For instance, does the policy generalise form “Combat Spider” to “Combat Zombie”?",
            "6": "- I also recommend a stronger evaluation on Minecraft benchmark consistent with the evaluations done in MineDOJO.",
            "7": "Currently, the paper shows result on only three tasks.",
            "8": "For a more exhaustive evaluation, MineDOJO recommends evaluation on a collection of starter tasks (32 programmatic and 32 creative tasks).",
            "9": "- While I understand that the authors didn’t have the resources to train a RT-1 style baseline, would it still be possible to train an action decoder on top of the VLM to produce actions.",
            "10": "I think having a strong RT-1 style baseline is very important to properly evaluate the question (2) mentioned in the paper — “How does PR2L compare to approaches that directly “ask” the VLM to generate the best possible actions for a task specified in the prompt?”\n\nOverall, I liked the main contribution of the paper.",
            "11": "But I believe, in its current form, the evaluation in the paper is a bit weak and could be made more exhaustive (unseen tasks, more exhaustive MineCraft evaluation).",
            "12": "**Update**: After reading the rebuttal, most of my questions are adequately answered.",
            "13": "I still believe that directly training an action decoder (while keeping VLM frozen is a good baseline) and should be included in the paper.",
            "14": "I also feel that the paper will be stronger if they include more exhaustive minecraft experiments (to check for generalisation on unseen tasks / objects) and more environments which are visually more complex like Habitat / AI2 Thor.",
            "15": "Based on authors response, I am increasing my score."
        }
    },
    "HgVEz6wwbM": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a novel approach to understanding prompt engineering for large language models (LLMs) through the lens of control theory.",
            "1": "- It formalizes LLM systems as discrete stochastic dynamical systems and explores the concept of the reachable set of output token sequences.",
            "2": "- The paper provides both theoretical and empirical analyses, offering new insights into the controllability of LLMs, which is a poorly understood area.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a significant gap in the understanding of prompt engineering for LLMs, which is crucial for their effective deployment.",
            "4": "- The theoretical framework based on control theory is innovative and provides a new perspective on LLM behavior.",
            "5": "- The empirical results are robust, demonstrating the practical applicability of the theoretical findings across multiple LLMs, including Falcon-7b, Llama-7b, and Falcon-40b.",
            "6": "- The paper's findings have potential implications for improving the design and optimization of LLM systems, making them more controllable and reliable.",
            "7": "Potential reasons for rejection\n   - **Complexity and Accessibility**\n     - The mathematical formalism and control theory concepts may be too complex for a broad audience, limiting the paper's accessibility.",
            "8": "- The paper assumes a high level of familiarity with both LLMs and control theory, which may not be the case for all readers.",
            "9": "- **Empirical Validation**\n     - The empirical analysis is limited to specific datasets (e.g., Wikitext) and LLMs, which may not generalize to other datasets or models.",
            "10": "- The experiments focus on single-token outputs, which may not fully capture the dynamics of more complex, multi-token sequences.",
            "11": "- **Practical Implications**\n     - While the theoretical insights are valuable, the paper could benefit from more concrete examples of how these insights can be applied in real-world LLM applications.",
            "12": "- The discussion on the practical cost and feasibility of implementing the proposed control strategies is limited.",
            "13": "Suggestions for improvement\n   - **Simplify and Clarify**\n     - Simplify the presentation of the mathematical formalism and control theory concepts to make the paper more accessible to a broader audience.",
            "14": "- Provide more intuitive explanations and visual aids to help readers grasp the key ideas.",
            "15": "- **Expand Empirical Analysis**\n     - Extend the empirical analysis to include a wider range of datasets and LLMs to validate the generalizability of the findings.",
            "16": "- Investigate the controllability of multi-token sequences and more complex tasks to provide a more comprehensive understanding of LLM dynamics.",
            "17": "- **Practical Applications**\n     - Include more detailed examples and case studies demonstrating how the theoretical insights can be applied to improve LLM performance in real-world scenarios.",
            "18": "- Discuss the computational cost and feasibility of implementing the proposed control strategies in practical applications.",
            "19": "- **Future Work and Open Questions**\n     - Expand the discussion on open questions and future research directions, particularly in areas such as chain-of-thought control, distributional control, and composable LLM systems.",
            "20": "- Explore the potential for LLMs to learn control strategies autonomously and the implications for developing more advanced and adaptable LLM systems."
        },
        "SaHhisLK4E": {
            "0": "- I think the papers main selling point is to offer the perspective to analyze prompt engineering through a control theory lens.",
            "1": "Unfortunately, it not novel and has been previously suggested in [1].",
            "2": "- Nevertheless, the idea to use control theory---and notions of controllability/reachability in particular---to analyze LLMs is quite elegant.",
            "3": "- The paper asks a number of interesting questions in section 7.",
            "4": "[1] Soatto, Stefano, et al.",
            "5": "\"Taming AI Bots: Controllability of Neural States in Large Language Models.\"",
            "6": "arXiv preprint arXiv:2305.18449 (2023).",
            "7": "- It is not clear that the analysis in section 4 is of any relevance as it stands for two reasons:\n 1)  I am not sure it makes sense to give a \"topological/norm/metric\" controllability condition as in (4).",
            "8": "Is there a natural topology here to justify this?",
            "9": "As far as I understand tokenization imposes a more or less arbitrary choice of topology so it is not clear to me that characterizing controllability (an algebraic concept) by a topological one makes any sense.",
            "10": "2) Moreover, I cannot find any suggestion in the paper that the bound is of any practical use---here the obvious questions is: is it sharp and if not how loose?",
            "11": "- Control theory is fundamentally a study of dynamical systems.",
            "12": "The statement that \" For simplicity, the time set T and the transition map ϕ are omitted from this\ndefinition\" made in section 3 then suggests that you are actually no longer really in the realm of control---which was your main selling point to begin with.",
            "13": "- The experiments are somewhat sparringly commented and the reader is left wondering how these were conducted.",
            "14": "What is the precise definition of a \"solved instance\" and how were these instances generated?",
            "15": "My critique pertains to section 6 mainly but the appendix also suffers from uncommented plots.",
            "16": "- While satisfactory, the level of writing could be better.",
            "17": "There are a number of grammatical errors, but beyond that and more importantly, the paper does not feel very well structured.",
            "18": "I think this is due in part to a lack of clear delineation of what the paper's contribution is.",
            "19": "- It would be better if the analysis was stated in standard thm/proof style.",
            "20": "Currently, commentary is interwoven with the proof of claim (4) making the derivation unnecessarily obtuse.",
            "21": "Ideally, the proof should be prefaced by exactly that which is to be shown and then broken down into an overview of the relevant components, followed by the proof itself.",
            "22": "Let me finally say that I find this particularly strange since the paper has a number of definitions (potentially too many...) stated in the standard mathematical style."
        },
        "5I8nziqpFc": {
            "0": "The idea of introducing $k-\\epsilon$ controllability is interesting and it is a nice connection between LLM and control theory.",
            "1": "The theoretical bound presented in Section 4, though hard to check in reality, is an initial step towards understanding the steerability of LLM theoretically.",
            "2": "In addition, I do appreciate the authors provide an interesting discussion on the open problems of LLM in control.",
            "3": "Section 4 only considers a self-attention head, which is quite simple and limited (compared to the current model used in LLM).",
            "4": "What are the difficulties in generalizing such results to a more complex model?",
            "5": "The presentation of Section 4 can be further improved.",
            "6": "The relationship between state controllability (Definition 7) and $k-\\epsilon$ controllability (Definition 6) should be discussed, i.e., implications of your theory result in Section 4.",
            "7": "In addition, I am confused about some notations: are $u_i$, $x_i$ the embeddings of the tokens?",
            "8": "Previously the u and x are presented as tokens, it does not make sense to make $\\|u_i\\|\\le 1$ and $\\|x_i\\| \\le 1$ if they are tokens.",
            "9": "In addition, is this assumption valid in real LLMs?",
            "10": "Although introducing the controllability of LLM from a control perspective is interesting, the experimental results of checking the controllability of the LLMs are not very exciting given the existing results from previous work [Zou 2023].",
            "11": "The experiment setup is almost identical to GCG work and the obtained results are also within expectation.",
            "12": "Instead, proposing a new method to study the controllability of black-box LLMs will be more interesting."
        },
        "xGQRJOmZaI": {
            "0": "- The paper presents a compelling perspective, which could be fruitful.",
            "1": "- Despite being a little bit disorganized, I appreciate the basic idea behind the theoretical analysis, showing that there is some fundamental bottleneck that makes LLMs not arbitrarily steerable (under some assumptions).",
            "2": "- I have concerns on the significance of the empirical results.",
            "3": "In particular, I suspect LLMs might be very easily steerable, and that any limitation in the ability to push them to a particular output is just due to limitations in the optimization method that is used to find the prompt.",
            "4": "As a noticeable example, one might say that, for the definition of steering that has been employed in the paper, a sufficiently capable LLM can always be steered by prepending to it an prefix that reads like \"after reading n words/token, output this particular word/token\".",
            "5": "If the optimization procedure does not find such a solution, it seems more a limitation of that than a fact related to the inability of LLMs to be steered.",
            "6": "- The paper is at times not well-organized.",
            "7": "The discussion section, that is usually a summary of the takeaways from the paper, looks more like a discussion of related work, and also has some incomplete points, the theoretical results would be better understood inside of a theorem latex environment, and so on."
        },
        "CKo9hA40d5": {
            "0": "The primary novelty of this paper lies in its formulation of the prompt engineering problem as an optimal control problem.",
            "1": "I truly appreciate this idea.",
            "2": "It is widely acknowledged that the choice of prompts heavily influences the LLM performance.",
            "3": "The interplay between the LLM's weights and the input prompt jointly determines the \"states\" of the LLM.",
            "4": "This insight is intuitive but non-trivial, very different from conventional supervised learning approaches.",
            "5": "It is commendable that the authors have translated this observation into a mathematical framework and provided an initial analysis.",
            "6": "Weakness or Questions\n\n1.",
            "7": "I have a concern regarding the controllability metric.",
            "8": "When an LLM is controllable, there exists a prompt capable of compelling the LLM to produce a desired output, even if that output is factually incorrect.",
            "9": "Thus, this formulation does not seem to establish a real connection with the specific capabilities of LLM, such as reasoning and knowledge memorization.",
            "10": "Moreover, The attribute of controllability appears to lean more towards a negative property, signifying the LLM's susceptibility to prompt manipulation and potential vulnerabilities.",
            "11": "The theory only requires the weight matrices to meet a particular bound on their largest eigenvalues.",
            "12": "The analysis framework does not depend on any training data or training algorithms.",
            "13": "Even randomly generated weights (as long as they satisfy the specific bound) can render the LLM entirely controllable.",
            "14": "This raises concerns about whether this theoretical framework can effectively explain the underlying mechanism of a well-trained LLM.",
            "15": "The results do not seem to offer practical guidance for prompt design.",
            "16": "The analysis does not appear to provide insights into how to stimulate the capabilities of an LLM effectively.",
            "17": "The proposed methods both require access to the desired output (the ground truth) during the search process."
        },
        "wtIbFkDig2": {
            "0": "This paper is one of the first of its kind in applying a control theoretical concepts to the study of LLMs.",
            "1": "Control theory offers very powerful tools that have the potential of providing a more formal understanding of LLMs behavior.",
            "2": "For this reason, the approach introduced in this paper has great potential to advance our understanding of language technologies.",
            "3": "This reviewer positively values the originality of this paper.",
            "4": "Moreover, the topic addressed of controlling language generation with the appropriate prompt is very relevant, and having rigorous tools opens the door to a formal treatment of LLMs.",
            "5": "In this paper, they provide an algorithm to find “magic words” based on a mathematical result inspired by control theory concepts.",
            "6": "More importantly, they list a series of open questions that could be addressed from a control-theoretical point of view.",
            "7": "This paper has some issues with the formalization of the theoretical concepts, as well as with the presentation of the results.",
            "8": "Since this paper introduces for the first time control theoretical concepts in the light of LLMs, it is paramount that the definitions are accurate and properly capture the ideas underpinning dynamical systems.",
            "9": "This is not the case for this paper.",
            "10": "The control theoretical concepts in this paper are not properly communicated, and Definition 4 has several flaws.",
            "11": "The definition given corresponds to an input-output system, as opposed to a dynamical system: in order for it to be a dynamical system, the state space should be V as opposed to V^*.",
            "12": "Moreover, the definition given for k-\\epsilon controllability is very far the definition of controllability in dynamical systems.",
            "13": "Please refer to Feedback Control Systems (Amstrom and Murray, 2009) for details.",
            "14": "This reviewer is concerned that, if published in the current form, this paper can introduce more confusion than clarifications in the realm of using control theoretical tools for LLMs analysis.",
            "15": "Aside from this, the main result should be framed as a theorem and provided in the body of the paper, together with the algorithms, not in the appendices, as these are the main results of the paper.",
            "16": "Furthermore, the paper currently lacks necessary formal definitions (such as the definition of the V^* set), and the control theoretical section lacks clarity in the exposition.",
            "17": "Substantial modifications are needed to improve the rigor of the presentation in Sections 3 and 4."
        }
    },
    "3fEKavFsnv": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of detecting machine-generated texts (MGTs) from human-written texts (HWTs), which is increasingly important due to the rise of large language models (LLMs) like ChatGPT.",
            "1": "- The proposed method, MMD-MP, introduces a novel multi-population aware optimization for maximum mean discrepancy (MMD) to improve the stability and reliability of detecting distributional discrepancies between MGTs and HWTs.",
            "2": "- The paper provides extensive experimental validation across various LLMs, demonstrating the superior performance of MMD-MP in both paragraph-based and sentence-based detection tasks.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a timely and relevant problem in the field of natural language processing, given the proliferation of LLMs and the associated risks of MGTs.",
            "4": "- The proposed MMD-MP method is novel and well-motivated, offering a significant improvement over existing MMD-based approaches by reducing variance and enhancing stability in discrepancy estimation.",
            "5": "- The experimental results are comprehensive and robust, showing clear advantages of MMD-MP over state-of-the-art baselines in multiple scenarios, including detection of unknown LLM texts.",
            "6": "- The theoretical analysis provided in the paper supports the empirical findings, adding depth and rigor to the proposed approach.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability of the paper**\n     - The paper is dense with technical details, which may be challenging for readers to follow, especially those not deeply familiar with MMD and kernel-based methods.",
            "8": "- Some sections, particularly the theoretical analysis, could benefit from clearer explanations and more intuitive descriptions to enhance understanding.",
            "9": "- **Generalization to other types of MGTs**\n     - While the paper demonstrates strong performance on a variety of LLMs, it is not clear how well the method generalizes to other types of MGTs that may have different characteristics or come from different domains.",
            "10": "- The paper could include more diverse datasets and LLMs to further validate the generalizability of the proposed method.",
            "11": "- **Computational complexity and scalability**\n     - The paper does not provide a detailed analysis of the computational complexity and scalability of the MMD-MP method, which could be a concern for practical deployment in large-scale applications.",
            "12": "- It would be beneficial to include a discussion on the efficiency of the method and potential optimizations for handling large datasets.",
            "13": "Suggestions for improvement\n   - **Enhance clarity and readability**\n     - Simplify the presentation of technical details where possible and provide more intuitive explanations, especially in the theoretical analysis sections.",
            "14": "- Include more visual aids, such as diagrams and flowcharts, to help illustrate key concepts and the overall methodology.",
            "15": "- **Expand experimental validation**\n     - Test the proposed method on a wider range of datasets and LLMs to further demonstrate its robustness and generalizability.",
            "16": "- Include experiments on different types of MGTs, such as those generated by domain-specific models or models trained on different languages.",
            "17": "- **Address computational complexity**\n     - Provide a detailed analysis of the computational complexity of the MMD-MP method and discuss potential optimizations for improving efficiency.",
            "18": "- Include experiments that evaluate the scalability of the method on larger datasets and in real-world scenarios.",
            "19": "- **Improve theoretical analysis**\n     - Clarify the theoretical analysis with more intuitive explanations and examples to make it accessible to a broader audience.",
            "20": "- Consider including additional theoretical results that further support the empirical findings and provide deeper insights into the behavior of the MMD-MP method."
        },
        "1xfleiUOdP": {
            "0": "Innovative Approach to a Contemporary Challenge: The paper addresses the pressing and contemporary issue of distinguishing machine-generated texts (MGTs) from human-written texts, especially given the sophisticated capabilities of modern LLMs.",
            "1": "The introduction of the multi-population aware optimization method, MMD-MP, is a novel approach that seeks to enhance the accuracy and stability of MGT detection, providing a fresh perspective on the problem.",
            "2": "The authors haven't limited their research to just theoretical propositions; they have put their methods to the test.",
            "3": "They conducted extensive experiments across various LLMs, providing empirical evidence of the superiority of their approach.",
            "4": "Such thorough testing not only validates their methods but also offers a benchmark for future research in this domain.",
            "5": "The paper offers clear theoretical insights, particularly the exploration of the optimization mechanism of MMD and its associated challenges.",
            "6": "Propositions and corollaries, such as the asymptotics of MPP and test power, further underscore the research's depth and provide a strong theoretical foundation for their method.",
            "7": "The use of statistical properties and theoretical analysis makes the paper robust and comprehensive.",
            "8": "The paper relies heavily on mathematical formulations, such as the objective function \n$$ J(P, Q; k_u) = MPP(P, Q; k_u)/σ(P, Q; k_u) $$\nand the estimator equation \n$ MPP_u(S_p, S_q; k_u) = \\frac{1}{n(n − 1)} \\sum H^{ij} $, \nwhere  $ H^{ij}=k_u(x_i, x_j) − k_u(x_i, y_j) − k_u(y_i, x_j) $.",
            "9": "While these equations are crucial for understanding the paper's methodology, they can be intimidating and might make the content less accessible to readers who aren't well-versed in mathematical notation or kernel-based methodologies.",
            "10": "The paper introduces MMD-MP to handle the variance issue in MMD when dealing with multiple text populations.",
            "11": "However, the mathematical representation \n$ σ^2_{s} = 4 (E[H^2_{i3}] − E[H^2_{i2}] ) $, \nwhere \n$ H^2_{i3}, H^2_{i2} $\ndenote different $ H^{ij} $, suggests that the variance calculation is still inherently tied to the distance metrics.",
            "12": "This could lead to concerns about the real-world applicability and stability of the proposed solution, especially in scenarios with vast and diverse text samples.",
            "13": "The algorithms presented, such as the training deep kernel with MMD-MP (Algorithm 1) and testing with MMD-MP for 2ST (Algorithm 2), are specifically tailored to address the MGT detection problem.",
            "14": "Given the intricate mathematical formulations, it may be challenging to adapt or generalize these algorithms to other domains or problems without significant modifications.",
            "15": "For researchers or practitioners looking for broader applications, this specificity might be a limitation."
        },
        "fna9T6YXvs": {
            "0": "The paper's motivation for using MMD to discriminate between highly similar distributions, particularly in short sentences, is compelling.",
            "1": "Short sentences often pose a greater challenge due to their simpler structure and wording, and MMD offers an effective solution.",
            "2": "The proposed method is supported by both theoretical analysis and experimental results.",
            "3": "The experimental design is comprehensive.",
            "4": "The experiments involving the detection of text generated by unknown or unseen LLMs are interesting.",
            "5": "Additionally, evaluating both short and long text is a well-rounded approach.",
            "6": "The investigation into how the number of training data instances impacts performance strengthens the effectiveness of the proposed MMD method.",
            "7": "One recent SOTA baseline for detecting machine-generated texts is missing [1].",
            "8": "The paper has also shown impressive performance in short texts and needs to be discussed and compared in the paper.",
            "9": "[1]Stylometric Detection of AI-Generated Text in Twitter Timelines, Kumarage et.",
            "10": "al."
        },
        "cndAm7SqsR": {
            "0": "- This paper introduces MMD-MP, a multi-population aware optimization method, which achieves better machine-generated text detection rate than baseline.",
            "1": "- This paper has a very clear mathematical formulation of the MMD-MP method, as well as provide reasoning on why they choose to make this optimization on top of vanilla MMD.",
            "2": "- Test power is not very straightforward to interpret.",
            "3": "I suggest adding an easier-to-understand explanation in additional to the current definition in section 2.",
            "4": "In my understanding, the higher the test power number is, the more “confident” the model is to the idea that the two distribution is different, right?",
            "5": "- Easier-to-read figures:\n    - This might be a nit-pick but figure 1 and 2 are very packed.",
            "6": "I understand that the authors want to plot MMD-D and MMD-MP out for better comparison, but maybe consider removing the triangles & squares & circles?",
            "7": "- What does the last two column mean in table 1?",
            "8": "Is it when $q=3$, so the three “clusters” are human-written text, ChatGPT text, and GPT3-S text?",
            "9": "- The method is overall, slightly complicated.",
            "10": "The experiments when $q=3$ are conducted on “different enough” language models.",
            "11": "I wonder if better LMs are included in the comparison (GPT-4, chatGPT), how will the method perform."
        },
        "SwJw48fdSS": {
            "0": "The paper attacks a crucial problem and it is clearly written.",
            "1": "The paper is technically very sophisticated, and provides extensive theoretical justification for the proposed methods.",
            "2": "Results on the tested setting are very clearly positive.",
            "3": "The paper considers variations in text due to using different LLMs, but it basically ignores variation due to different genres, styles, and human authors.",
            "4": "I think this is an artifact of sticking to a single small corpus (in the main paper), where it is easy to generate new LLM outputs by using the provided prompts, but all other attributes have to remain fixed.",
            "5": "These other attributes are probably more important in real-world applications.",
            "6": "I am not sure whether experiments on such a small training corpus are meaningful.",
            "7": "Due to the cross-validation setup, it is very possible that all models that are trained on the corpus are over-fitting to this setting.",
            "8": "Baselines like ChatGPT-D that may be trained on much larger external corpora are therefore at a disadvantage.",
            "9": "Another potential disadvantage is that the single-sentence detection test is not very realistic; it would have been better to perform this test at the paragraph level.",
            "10": "A final problem with the experiment section is that many details are missing - see questions below.",
            "11": "The proposed method is quite complicated.",
            "12": "Apart from complexity of implementation, a potentially large disadvantage is that the loss is quadratic in training corpus size.",
            "13": "The authors don’t address this issue at all.",
            "14": "Although it has a lot of supporting math, the main contribution is actually a small modification (dropping a term in the loss), that is quite intuitive.",
            "15": "There is nothing wrong with this, but the paper would have been stronger had it focused more on providing this intuition up front, and less on the formal details."
        }
    },
    "KOTsHW6mBI": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue of tracing adversary users in the context of large language models, which is a step beyond merely identifying machine-generated text.",
            "1": "- The proposed method, Multi-bit Watermark via Position Allocation (MPAC), introduces a novel approach to embedding traceable multi-bit information during language model generation.",
            "2": "- The method leverages zero-bit watermarking and extends it to embed and extract long messages without finetuning, maintaining text quality, and ensuring robustness against strong attacks.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a significant problem in the field of large language models, addressing the need for tracing adversary users to counteract malicious misuses.",
            "4": "- The proposed MPAC method is innovative and builds on existing zero-bit watermarking techniques, offering a practical solution for embedding multi-bit information.",
            "5": "- The method demonstrates robustness against realistic attacks such as interleaving human texts and paraphrasing, which is a notable improvement over existing methods.",
            "6": "- The experimental results show that the method can effectively embed and extract multi-bit messages with high accuracy while maintaining text quality.",
            "7": "- The paper provides a thorough analysis of the method through the lens of coding theory, offering insights into the factors affecting performance and potential improvements.",
            "8": "Potential reasons for rejection\n   - **Complexity and Implementation:**\n     - The method may be perceived as complex to implement, especially for practitioners who are not familiar with coding theory and watermarking techniques.",
            "9": "- The paper may lack sufficient practical guidance or examples for implementing the proposed method in real-world scenarios.",
            "10": "- **Evaluation and Comparison:**\n     - The evaluation may not cover a wide enough range of language models and datasets to demonstrate the generalizability of the method.",
            "11": "- The comparison with other existing methods may not be comprehensive enough, potentially overlooking some relevant techniques or metrics.",
            "12": "- **Scalability and Performance:**\n     - The method's performance in terms of encoding and decoding latency may not be thoroughly evaluated, especially for longer messages or larger datasets.",
            "13": "- The scalability of the method to handle very large language models or extremely long texts may be a concern, and the paper may not address these aspects sufficiently.",
            "14": "- **Detection Performance:**\n     - The detection performance of the method may degrade with increasing bit-width, which could limit its effectiveness in certain scenarios.",
            "15": "- The paper may not provide a clear solution or mitigation strategy for the reduced separability of machine and human text when embedding longer messages.",
            "16": "Suggestions for improvement\n   - **Implementation Guidance:**\n     - Provide more detailed practical guidance and examples for implementing the MPAC method, including code snippets or pseudocode.",
            "17": "- Include a step-by-step tutorial or case study demonstrating the application of the method in a real-world scenario.",
            "18": "- **Evaluation and Comparison:**\n     - Expand the evaluation to include a wider range of language models and datasets to demonstrate the generalizability of the method.",
            "19": "- Conduct a more comprehensive comparison with existing methods, including additional metrics and relevant techniques that may have been overlooked.",
            "20": "- **Scalability and Performance:**\n     - Evaluate the encoding and decoding latency more thoroughly, especially for longer messages and larger datasets, to assess the method's performance and scalability.",
            "21": "- Investigate and address potential scalability issues when applying the method to very large language models or extremely long texts.",
            "22": "- **Detection Performance:**\n     - Explore and propose potential solutions or mitigation strategies for the reduced separability of machine and human text when embedding longer messages.",
            "23": "- Consider alternative statistics or techniques to improve the detection performance and reduce the false positive rate, especially for longer messages."
        },
        "HOEnxgDW6i": {
            "0": "The introduction of a multi-bit watermark presents an innovative concept within this research domain.",
            "1": "Experimental results affirm the robustness of the proposed watermark algorithm.",
            "2": "The multi-bit watermark methodology, at a cursory glance, seems akin to an expansion of the red-green list from [1] into a more nuanced multi-color list.",
            "3": "Given that the detection methodologies remain consistent, the paper should elucidate how the multi-bit approach enhances the robustness intrinsic to [1].",
            "4": "As this constitutes the crux of the paper's contribution, it is imperative for the authors to furnish theoretical insights and a robust analysis concerning the multi-bit watermark's robustness.",
            "5": "There is a noticeable absence of a comparative robustness analysis between the proposed method and that established in [1] within the experimental evaluations.",
            "6": "Considering the resilience of the [1] watermark against text alterations, it is essential for the authors to demonstrate superior robustness to substantiate their contributions effectively.",
            "7": "While the authors purport that the multi-bit watermark sustains the caliber of the text, the empirical evidence, particularly in Table 1, suggests parity in quality between texts generated by the multi-bit and zero-bit watermarks.",
            "8": "It is incumbent upon the authors to extend their comparison to texts generated without watermarks to convincingly affirm that the multi-bit watermark preserves text quality.",
            "9": "Figure 5(a) depicts a correlation wherein an augmentation in bit-width correlates with diminished robustness.",
            "10": "This trend ostensibly advocates for the utilization of the zero-bit watermark, prompting the question of whether, in terms of robustness, [1] might present a more formidable solution than the proposed multi-bit watermark.",
            "11": "[1] A Watermark for Large Language Models.",
            "12": "John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein.",
            "13": "ICML (2023)"
        },
        "4LLc4kidSQ": {
            "0": "### Originality\n\nThe proposed method is relatively simple and is a nice extension of the zero-bit watermarking scheme.",
            "1": "This means that any intuition as well as tools and analysis that are based on Kirchenbauer et al.",
            "2": "[2023a] should also carry over and benefit the understanding of this scheme.",
            "3": "### Quality\n\nThis paper contains thorough experiments considering multiple sets of parameters, models, metrics, and attacks.",
            "4": "There is a good amount of numerical results and textual analyses in both the main text and the appendix, making the results convincing.",
            "5": "### Clarity\n\nThe paper is well-written and easy to follow.",
            "6": "Section 3 and Figure 1 provide a simple description of the scheme.",
            "7": "The messages are effectively conveyed; all the figures and tables are easy to interpret.",
            "8": "### Significance\n\nWatermarking for detecting machine-generated content is an intellectually interesting work, but I’m not sure about its practical benefits for two reasons.",
            "9": "First, with open-source LLMs, it is virtually impossible to enforce the “post-processing” watermarks.",
            "10": "Second, under the assumption that there’s an adversary trying to remove the watermark (e.g., via back-translation or paraphrase), it is very difficult to achieve a satisfactory TPR with a low enough FPR.",
            "11": "This problem is heavily asymmetric in the sense that the cost of an FP is gigantic (wrongly detecting a human-written text as a machine’s) compared to an FN so an acceptable FPR has to be very low in practice (~1e-5).",
            "12": "### Comparison to Steganography\n\nThe paper briefly mentions steganography and claims that it is different from watermarking.",
            "13": "However, I’m not convinced that they are different.",
            "14": "The undetectability property of steganography is exactly desired by LLM watermarking, i.e., a watermark should not modify the distribution of the “cover text” (or LLM-generated text) in a noticeable way.",
            "15": "Generally, steganography and watermarking differ in their purpose, but in the instance of LLM, they are identical concepts.",
            "16": "So it seems important that this paper (and other watermarking papers) is **compared (theoretically and/or empirically) to steganography on natural language** such as [1] and [2].",
            "17": "### Methodology\n\n- List decoding can be better parameterized by a confidence threshold rather than a fixed length $|\\mathbb{L}|$ like a 95%-confidence interval.",
            "18": "More precisely, given a confidence threshold (says 95%), we want to make a statement like: “With probability 95%, the true message is among these five messages” (think of [conformal prediction](https://en.wikipedia.org/wiki/Conformal_prediction), for example).",
            "19": "There should be a principled way to either model this directly or compute this by aggregating the confidence score at each position ($c_i$)—This can be a bit tricky since $c_i$’s are not independent.",
            "20": "- Also including the list decoding as part of the bit accuracy metric (e.g., in Figure 4 and 5) seems irrelevant and unjustified to me.",
            "21": "It’s not clear how the practitioners would be able to utilize the 16 other less-confident messages in the LLM detection/attribution setting.",
            "22": "- One very nice property of the zero-bit watermarking scheme proposed by Kirchenbauer et al.",
            "23": "[2023a] is the theoretical guarantee on the number of tokens in the green list (Theorem 4.2).",
            "24": "It would be nice to see a similar theoretical analysis on this paper.",
            "25": "### Experiments\n\n**Metric.",
            "26": "** AUC is a relatively misleading metric in practice for watermarking.",
            "27": "As mentioned earlier, each FP is very costly so an appropriate metric in this scenario is usually something like TPR at a very low level of FPR (e.g., 1e-2, 1e-4, 1e-6, etc.).",
            "28": "**Comparison to zero-bit watermarks.",
            "29": "** I would like to see a comparison of the efficiency of this multi-bit watermark to the previously proposed zero-bit ones (at least, Kirchenbauer et al.",
            "30": "[2023a]) for just detecting machine-generated texts.",
            "31": "This is important because prior to attributing the text to which model/service, we have to first decide whether the text is generated by a machine or a human.",
            "32": "In theory, this scheme should trade off the ability to encode longer messages with the watermark detection efficiency.",
            "33": "### Other Minor Issues\n\n- Figure 2 caption: “Clean bit error - corrupted bit error” was a little bit unclear as the metrics have not been defined at this point.",
            "34": "- [Typo] Page 9: “**Across Model Scales, Datasets, Hash Schemes.",
            "35": "** The results for larger models (13B, 70B) and other datasets are in Appendix A.4.” should be Appendix A.6?",
            "36": "References\n- [1] https://arxiv.org/abs/1909.01496\n- [2] https://arxiv.org/abs/2210.14889"
        },
        "VmMJAs5HNt": {
            "0": "Strength:\n•\tThe introduction gives a concise overview of the significance of machine-generated text identification, the approaches used in the past, and the reasons why more sophisticated techniques are required.",
            "1": "•\tThe paper provides various graphs and visual representations, aiding in understanding the performance under different conditions.",
            "2": "•\tOne of the strengths of the paper is its demonstration of the model's performance under copy-paste attacks, which tests the robustness of their watermarking method.",
            "3": "Weakness:\n•\tThe performance of the model seems to be reliant on token length.",
            "4": "When messages are longer, there seems to be a dip in performance, which may not be suitable for all applications.",
            "5": "For example: As mentioned under Figure 5, embedding longer messages at fixed bits per token seems to decrease the performance, especially when reaching up to 64-bit.",
            "6": "•\tMetrics like AUC and bit-accuracy are essential, and the paper might benefit from a more qualitative or user-centric analysis.",
            "7": "Reliance solely on quantitative metrics might not capture the full user experience or real-world applicability.",
            "8": "For example: The paper primarily discusses results in terms of AUC, bit-accuracy, and other such metrics.",
            "9": "A discussion on real-world applications potential user feedback could provide a more complete picture of the method's usefulness.",
            "10": "•\tWhile the paper evaluates against human-induced copy-paste attacks, it doesn't delve deeply into other potential real-world challenges.",
            "11": "For instance, how would the system perform against more sophisticated adversarial attacks or in scenarios with heavy noise?",
            "12": "•\tThe method's reliability on decoding depends heavily on the pseudo-random generator function.",
            "13": "If someone can reverse-engineer or predict the pseudo-random function, the watermark can potentially be tampered with or removed."
        },
        "edvoPdr4qg": {
            "0": "(1) The paper proposes an innovative and practical method to address the problem of misuse of large language models, extending beyond mere identification to traceability.",
            "1": "(2) The proposed method, Multi-bit Watermark via Position Allocation (MPAC), allows embedding and extraction of long messages without model access or fine-tuning, which is a significant improvement over existing methods.",
            "2": "(3) The authors also offer detailed empirical findings from their experiments, showing that their method can effectively embed 8-bit messages in short text lengths with over 90% bit accuracy.",
            "3": "(1) The paper did not provide comparisons to other multi-bit watermarking methods, such as the method presented by Yoo et al.",
            "4": "in \"Robust multi-bit natural language watermarking through invariant features\" Providing comparisons to related work would have strengthened the paper.",
            "5": "(2) The detection of the watermark is an important part.",
            "6": "More details on the watermark detection methodology would have been beneficial.",
            "7": "(3) Embedding capacity limited by the text distribution and model.",
            "8": "Low entropy distributions like code have an inherently lower capacity.",
            "9": "(4) The author could consider testing on other models besides LLAMA."
        },
        "1qAQTh8O65": {
            "0": "Encoding multi-bit information is an important problem, and a useful extension to 0-bit watermarking.",
            "1": "This lets us specific identify users abusing LLMs, going beyond the problem of simply detecting LLM generated content.",
            "2": "The approach presented seems reasonable, and an improvement over simple baselines of: 1) a unique watermarking key for each message we want to encode,  or 2) r=2 (the Kirchenbauer set up) and then simply picking the red and green lists based on the position.",
            "3": "The experiments are thorough, and the method seems to work for encoding multi-bit messages without degrading the quality anymore than the 0-bit scheme (the 0-bit scheme already degrades the LLM quality a bit).",
            "4": "I found the presentation a bit difficult to follow.",
            "5": "I think the paper would be much easier to follow if there were fewer forward references, and the paper was a bit more self contained.",
            "6": "It might also be worth dedicating space to central contributions in the paper, and having a list of claimed contributions in the paper.",
            "7": "I would like to see the decoding scheme in the main paper, and not the appendix since that is central to understanding a lot of the other details that are in the main paper.",
            "8": "There are concerns that there's not enough bandwidth to even do 0-bit watermarking, particularly for LLMs that have gone through RLHF/SFT as that process significantly reduces the entropy available for watermarking.",
            "9": "It is unclear to me if the proposed approach is useful in practice for encoding multi-bit messages.",
            "10": "There are a few details missing.",
            "11": "It is claimed that list decoding improves performance, but it is unclear to me how the decoded list is used in this accuracy computation.",
            "12": "If one of the messages in the list matches the encoded message, is that counted as a success?",
            "13": "I think the innovation over the 0-bit UMD scheme is somewhat incremental for a machine learning conference, at least from an ML standpoint."
        }
    },
    "BkvdAYhyqm": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of interpretability in large language models (LLMs), which is essential for their application in high-stakes fields such as medicine and neuroscience.",
            "1": "- The proposed method, Summarize and Score (SASC), offers a novel approach to generating natural language explanations for black-box text modules, which could significantly enhance the transparency and trustworthiness of LLMs.",
            "2": "- The application of SASC to various contexts, including synthetic modules, BERT model internals, and fMRI voxel responses, demonstrates its versatility and potential impact across different domains.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-defined and innovative method (SASC) for generating natural language explanations for black-box text modules, addressing a significant gap in the field of LLM interpretability.",
            "4": "- The experimental evaluation is thorough, covering multiple contexts and demonstrating the effectiveness of SASC in recovering ground truth explanations and providing insights into model internals and brain responses.",
            "5": "- The inclusion of an explanation score adds a valuable metric for assessing the reliability of the generated explanations, which is crucial for practical applications.",
            "6": "- The paper provides detailed descriptions of the methodology, experimental setup, and results, ensuring reproducibility and transparency.",
            "7": "Potential reasons for rejection\n   - **Limited scope of explanations:**\n     - SASC focuses on explaining the top responses of a module, which may not capture the full behavior of the module.",
            "8": "- The reliance on ngrams may limit the ability to explain patterns requiring long context or low-level text patterns.",
            "9": "- **Dependence on pre-trained LLMs:**\n     - The effectiveness of SASC is contingent on the capabilities of the pre-trained LLM used for summarization and generation, which may introduce biases or limitations.",
            "10": "- If the pre-trained LLM fails to perform its tasks sufficiently well, the generated explanations may require human vetting, reducing the method's automation advantage.",
            "11": "- **Evaluation limitations:**\n     - The evaluation of explanations for BERT transformer factors and fMRI voxel modules relies on manual inspection and comparison to human-given explanations, which may introduce subjectivity.",
            "12": "- The paper does not provide a comprehensive analysis of the limitations and potential biases introduced by the helper LLM in generating synthetic texts for evaluation.",
            "13": "Suggestions for improvement\n   - **Expand the scope of explanations:**\n     - Consider extending SASC to explain the entirety of a module's responses, not just the top responses, to provide a more comprehensive understanding of the module's behavior.",
            "14": "- Explore incorporating additional information beyond ngrams, such as positional information or longer context, to capture more complex patterns.",
            "15": "- **Enhance evaluation methods:**\n     - Develop more objective and automated evaluation metrics for assessing the quality and relevance of generated explanations, reducing reliance on manual inspection.",
            "16": "- Conduct a more detailed analysis of the potential biases and limitations introduced by the helper LLM, and explore ways to mitigate these issues.",
            "17": "- **Improve robustness and generalizability:**\n     - Investigate the performance of SASC with different pre-trained LLMs and across a wider range of text modules to ensure robustness and generalizability.",
            "18": "- Provide more detailed guidelines and best practices for selecting and configuring the pre-trained LLMs used in SASC to optimize performance and reliability."
        },
        "gfFWnxt087": {
            "0": "I think on balance the authors did a good exploration of their method, and the method is generally well described.",
            "1": "By evaluating on embedding models, you have a ground truth target to evaluate against.",
            "2": "Assuming the embedding model is accurate, which is not an unreasonable assumption.",
            "3": "By interpreting BERT transformer factors against previous human evaluations -- this is a direct comparison against a human baseline.",
            "4": "By looking at fMRI data, this evaluates SASC on a noisy scenario where the various brain regions are can only be observed under a noisy setting.",
            "5": "The assumptions used in the paper are weak and reasonable, and do not require differentiability assumptions, a residual/skip connection assumption (for example in the logit lens paper).",
            "6": "Compared to \"Language models can explain neurons in language models\" paper from OpenAI, their approach of using a corpus rather than per-token scoring is more sound, and it allows for more context dependent explanations.",
            "7": "Given that interpretability is a significant issue in the use of large language models, this paper addresses a timely and important problem.",
            "8": "I have a couple of concerns on the evaluation done in this paper.",
            "9": "I think their corpus + score approach is more sound than the OpenAI's approach of token-wise scoring.",
            "10": "But there is one problem I would like to see resolved:\n\n   a.",
            "11": "There is no ablation study on the corpus size and the effect on explanation scores.",
            "12": "Ideally the authors could explore the outcome using for example 10k, 20k, 40k, 80k, .... all n-grams.",
            "13": "The scoring step in my view is suspect, and introduces an unnecessary confound in terms of their procedures.",
            "14": "It seems very wrong to evaluate $E(f(\\text{Text}^{+})-f(\\text{Text}^{-}))$.",
            "15": "In this case, you are asking the language model to generate the phrases unrelated to an explanation, so the final score is not just related to how good the positives are, but also how \"good\" the negatives are.",
            "16": "Perhaps a more correct step should be $E(f(\\text{Text}^{+})-f(\\text{Text}^{\\text{all}}))$.",
            "17": "This is less problematic if this was just used as an internal component, but I think it is problematic when the authors use it as a point of comparison in Table 4, Figure 3, Table 6, fMRI experiments, Table A4, and Table A8.",
            "18": "The \"Default\" setting in \"synthetic modules\" experiment is not very sound in my view.",
            "19": "Ideally the exploration of the 54 modules should not be relying on a corpus which is known to contain the relevant ngrams, but the exploration of the modules should use the same (random) selection of ngrams.",
            "20": "It is a bit questionable that the authors use manual inspection to evaluate their method.",
            "21": "Scanning both the main text and the supplemental, the authors do not use Amazon Mechanical Turk or Prolific, and do not include any details on this human study.",
            "22": "I can only assume that this experiment was done by the authors themselves, which is prone to bias.",
            "23": "A more accurate approach would be to take a different text embedding model (for example bge-large, other models are fine too), compute the n-way cosine similarities among the 54 models to the 54 explainations, and compute the argmax agreement (is the n-th explanation generated by the SASC cosine similarity maximized by the n-th model), or even the cosine distance when n-way classification is not appropriate.",
            "24": "The clarity in some specific methods are poor.",
            "25": "There is no details in either the main text or the supplemental.",
            "26": "Concretely, I was unable to find details for the following:\n\n    a. ngram summarization baseline used in Table 1.",
            "27": "I looked at the references provided by the author (Kadar et al.",
            "28": "and Na et al.",
            "29": "), neither are actually performing ngram summarization.",
            "30": "The first citation uses a omission based approach, while the second uses a parsing + text replication approach.",
            "31": "b .",
            "32": "Can the authors describe how they get the spearman rank correlation sem?",
            "33": "I would not otherwise ask if the effect is strong, but the correlation is very low.",
            "34": "Do the authors find the sem via a bootstrap?",
            "35": "Could the authors report a t-statistic instead?",
            "36": "It seems like this proposed method would only work on semantic selectivity, and it is unclear if this method can work on low-level text patterns.",
            "37": "This is okay, but the authors should more clearly discuss this limitation in their paper.",
            "38": "Overall I like the paper.",
            "39": "And the proposed method is well described, but the author's evaluation broadly does not strike me as sound.",
            "40": "I would happily re-evaluate if the authors can answer my questions."
        },
        "STLKWfafgu": {
            "0": "This paper tackles the timely and important topic of interpreting black box natural language processing models like large language models.",
            "1": "The proposed SASC framework enables inspecting model internals and explaining model predictions in an interpretable way.",
            "2": "The SASC framework is novel and theoretically grounded.",
            "3": "The approach of using a separate helper model to generate explanations is creative.",
            "4": "The experiments reveal fascinating patterns in how different models generate explanations.",
            "5": "The results indicate SASC has potential for analyzing text models in fields like social science, medicine, and neuroscience.",
            "6": "Please refer to the questions section"
        },
        "uP5JKi3wVD": {
            "0": "The paper proposes a new method called Summarize and Score (SASC) that generates natural language explanations for black-box text modules.",
            "1": "The authors evaluate SASC on synthetic modules, explaining modules found within a pre-trained BERT model and generating explanations for the response of individual fMRI voxels to language stimuli.",
            "2": "The results show that SASC can generate high-quality explanations that are both accurate and human-readable.",
            "3": "The proposed method has significant implications for improving the interpretability of machine learning models.",
            "4": "The paper does not compare SASC to other state-of-the-art methods for generating natural language explanations for black box models.",
            "5": "It would be useful to compare SASC to other methods, such as LIME and SHAP, to determine how it performs in comparison.",
            "6": "In SASC design, the first step is to input the n-grams from the reference corpus into the text module, which could contain many instances.",
            "7": "How to ensure the efficiency of the proposed method remains undiscussed.",
            "8": "It would be better for authors to provide a detailed analysis of the computational complexity of SASC.",
            "9": "It would be useful to provide information on the computational requirements of the proposed method and potential ways to optimize it.",
            "10": "The design of synthetic scoring is heuristic and without any theoretical analysis to support this design.",
            "11": "And most importantly, there is no ablation study to verify the effectiveness of this model design."
        }
    },
    "4bUeP3qrNu": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical question in the field of artificial intelligence and cognitive psychology: whether language models (LMs) replicate human reasoning biases or can overcome them.",
            "1": "- It provides a systematic comparison of syllogistic reasoning between humans and LMs, specifically focusing on the PaLM 2 family of transformer LMs.",
            "2": "- The study's findings contribute to understanding the extent to which LMs can perform logical reasoning tasks and how their performance scales with model size.",
            "3": "Potential reasons for acceptance\n   - The paper presents a thorough and well-structured analysis of syllogistic reasoning in LMs, comparing their performance with human reasoning.",
            "4": "- It uses a robust methodology, including zero-shot chain-of-thought prompting and a detailed evaluation of LM responses.",
            "5": "- The findings are significant, showing that larger LMs can outperform humans in syllogistic reasoning tasks while still exhibiting some human-like biases.",
            "6": "- The paper contributes to the broader discussion on the capabilities and limitations of LMs in performing logical reasoning tasks, which is relevant for both AI research and cognitive science.",
            "7": "Potential reasons for rejection\n   - **Limited scope of syllogistic reasoning tasks:**\n     - The study focuses solely on syllogistic reasoning, which may not fully represent the broader spectrum of logical reasoning tasks.",
            "8": "- The conclusions drawn from this specific task may not generalize to other types of reasoning or problem-solving tasks.",
            "9": "- **Insufficient exploration of alternative prompting strategies:**\n     - The paper primarily uses zero-shot chain-of-thought prompting, with limited discussion on the impact of other prompting strategies.",
            "10": "- A more comprehensive evaluation of different prompting methods could provide a deeper understanding of how to best elicit reasoning from LMs.",
            "11": "- **Over-reliance on a single family of LMs:**\n     - The study evaluates only the PaLM 2 family of LMs, which may limit the generalizability of the findings to other LMs.",
            "12": "- Including a comparison with other state-of-the-art LMs could strengthen the conclusions and provide a more comprehensive view of LM reasoning capabilities.",
            "13": "- **Potential bias in the generated syllogism dataset:**\n     - The dataset of syllogisms used for LM evaluation is generated to avoid overlap with training data, but the selection process may introduce biases.",
            "14": "- A more detailed analysis of the dataset's representativeness and potential biases would enhance the study's validity.",
            "15": "Suggestions for improvement\n   - **Expand the scope of reasoning tasks:**\n     - Include a variety of logical reasoning tasks beyond syllogisms to assess the generalizability of the findings.",
            "16": "- Evaluate LMs on tasks such as conditional reasoning, probabilistic reasoning, and causal inference.",
            "17": "- **Explore alternative prompting strategies:**\n     - Conduct a systematic evaluation of different prompting methods, including few-shot prompting and various chain-of-thought techniques.",
            "18": "- Analyze the impact of these strategies on LM reasoning performance and biases.",
            "19": "- **Include a broader range of LMs:**\n     - Compare the performance of the PaLM 2 family with other state-of-the-art LMs, such as GPT-3, BERT, and T5.",
            "20": "- Assess whether the observed trends hold across different architectures and training paradigms.",
            "21": "- **Analyze the generated syllogism dataset:**\n     - Provide a detailed analysis of the dataset's representativeness and potential biases.",
            "22": "- Consider using a standardized dataset of syllogisms from cognitive psychology literature to ensure consistency and comparability.",
            "23": "- **Investigate the impact of training data composition:**\n     - Examine how the composition of the training corpus (e.g., inclusion of source code) influences LM reasoning performance.",
            "24": "- Conduct controlled experiments to isolate the effects of different types of training data on logical reasoning capabilities."
        },
        "Lpv1x3Fzgy": {
            "0": "The subject matter is very interesting and very well-grounded in theories in cognitive science.",
            "1": "The whole systematic investigation is very well-designed.",
            "2": "My only concern is that it is quite unclear how the human data was post-processed as each syllogism is associated with 139 responses.",
            "3": "If there is no further post-processing as the one on LM-produced data, then this is an insurmountable issue for me.",
            "4": "I am saying this because the LM's outputs are actually majority voting results based on 30 runs.",
            "5": "If humans did not do the same, then, for me, machines and humans were doing two very different tasks, making the conclusions extremely biased and, therefore, unreliable.",
            "6": "Thinking of a situation where one asks an LM to run 30 times on a test case and 20 of them are correct, the accuracy is 100% after majority voting.",
            "7": "Nevertheless,  if one asks humans to do the same but without majority voting, then the accuracy is only 66%, but their (i.e., humans and the LM) behaviours are very similar to each other.",
            "8": "For me, a better solution is dropping the majority voting and computing distributional similarities when comparing LMs with humans (I mean directly rather than the one in section 4.2)."
        },
        "vuOjrTOyoE": {
            "0": "The paper makes a clear contribution in understanding how humanlike LMs are by exploiting the already existent study of human performance on syllogisms.",
            "1": "This study allows us to make a fine-grained examination of 64 syllogism types (2 premises * 4 orderings * 8 moods), each of which has human performance recorded from 139 participants.",
            "2": "This approach makes the claims made in the submission believable.",
            "3": "- The findings are useful but not surprising.",
            "4": "- Some natural settings are omitted, including few-shot prompting which is only mentioned.",
            "5": "While the paper can't do everything, it seems a bit incomplete to not check the possibility that few-shot (which is more aligned with the human data) makes a big difference.",
            "6": "- Only the PaLM 2 model family is considered, though that's understandable from a limited budget point of view.",
            "7": "But we may not be able to discount the possibility that the LM behavior changes significantly with, e.g., GPT-4."
        },
        "HN2A3mqiRF": {
            "0": "This work covers various human biases in syllogistic reasoning, presenting an in-depth comparison through extensive experiments.",
            "1": "It examines the distribution of responses, variable ordering, and generation of syllogistic fallacies in both language models and humans.",
            "2": "By employing the Mental Models Theory, this work provides a fresh perspective on understanding the logical reasoning abilities of language models.",
            "3": "The deductive reasoning capability of LLMs has been widely explored.",
            "4": "The comparison of reasoning abilities between humans and language models in syllogism has already been explored in Dasgupta et al (2022).",
            "5": "And Saparov & He (2022) also utilizes similar controlled techniques for analyzing the capabilities of language models.",
            "6": "Besides, this work directly utilizes human results on syllogistic reasoning from previous work and only explore PaLM 2 models on a small set of data, which limits the contribution of this paper.",
            "7": "The motivation to use mReasoner and the explanation of its details are not well presented.",
            "8": "The description of the datasets and models for experiments are insufficient, such as the size of model parameters.",
            "9": "Conducting experiments on different language models would enhance the conclusion of the study.",
            "10": "There are some errors:\n> - In Section 2.3, Saparov & He (2022) and Saparov et al.",
            "11": "(2023) both analyze natural language, contrary to what the authors claim about formal logic forms.",
            "12": "> - There are spelling typos, such as \"efffect\" in the third paragraph of Section 4.2."
        },
        "iZTJetucbC": {
            "0": "- Uses cognitive science experiments and methods to understand large language models and make relatively direct comparison with human behavior.",
            "1": "- Detailed analysis of model responses, e.g.",
            "2": "looking at variable-ordering effects, beyond just summary statistics like accuracy.",
            "3": "- Compares LM responses to a cognitive model fit to human data, to identify which factors drive model behavior at scale.",
            "4": "- Appendix compares different prompting strategies as well.",
            "5": "- Results are only presented on one family of models, which are behind a closed API.",
            "6": "This has two problems: (1) While it is nice that this family has four scale variants, the results are still presented as about \"LMs\" in general, whereas we have learned about one in particular.",
            "7": "Another model family or two would help understand whether the results are in fact general.",
            "8": "(2) Behind a closed API, the results are not necessarily reproducible.",
            "9": "I'd like to see at least one open model as well.",
            "10": "- The dataset used for evaluation is relatively small.",
            "11": "- Difficult choices are made for parsing LM generation to decide when it has generated a conclusion to the syllogism.",
            "12": "I would have liked to hear more about this choice point and whether they looked at other formats (e.g.",
            "13": "multiple choice).",
            "14": "- The tendency of the model to _never_ output \"nothing follows\" means that all of the interesting analyses in the paper were on a subset of the syllogisms (27/64) where nothing follows is never correct.",
            "15": "This also limits the generality of the results."
        }
    },
    "VaZa8zj0Yw": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Lyfe Agents, a novel approach to creating low-cost, real-time generative agents for social interactions in virtual environments.",
            "1": "- The proposed techniques, including the option-action framework, asynchronous self-monitoring, and Summarize-and-Forget memory mechanism, are innovative and inspired by cognitive processes in the human brain.",
            "2": "- The work addresses a significant challenge in the field of autonomous agents: reducing computational costs while maintaining high levels of intelligence and autonomy.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-defined problem and offers a clear, innovative solution that is both theoretically sound and practically relevant.",
            "4": "- The proposed techniques are thoroughly evaluated in a custom 3D virtual environment, demonstrating their effectiveness in various social interaction scenarios.",
            "5": "- The results show a substantial reduction in computational costs compared to existing alternatives, making the approach highly practical for real-world applications.",
            "6": "- The paper is well-structured, with a clear explanation of the methods, experiments, and results, making it easy to follow and understand.",
            "7": "Potential reasons for rejection\n   - **Lack of comparison with state-of-the-art methods:**\n     - The paper does not provide a detailed comparison with other state-of-the-art generative agent frameworks, which could help to contextualize the improvements offered by Lyfe Agents.",
            "8": "- Including quantitative benchmarks against existing methods would strengthen the claims of cost reduction and performance improvements.",
            "9": "- **Limited scope of evaluation scenarios:**\n     - The evaluation is limited to a few specific scenarios within the custom LyfeGame environment, which may not fully represent the diversity of real-world applications.",
            "10": "- Expanding the evaluation to include more varied and complex scenarios would provide a more comprehensive assessment of the agents' capabilities.",
            "11": "- **Potential scalability issues:**\n     - While the paper demonstrates cost reductions for a small number of agents, it does not address potential scalability issues when deploying a larger number of agents in more complex environments.",
            "12": "- Discussing the scalability of the proposed techniques and providing empirical evidence of their performance in larger-scale settings would be beneficial.",
            "13": "Suggestions for improvement\n   - **Include a detailed comparison with state-of-the-art methods:**\n     - Provide quantitative benchmarks and comparisons with other leading generative agent frameworks to highlight the specific advantages of Lyfe Agents.",
            "14": "- Discuss the relative strengths and weaknesses of the proposed approach in comparison to existing methods.",
            "15": "- **Expand the evaluation scenarios:**\n     - Test the agents in a wider variety of scenarios, including more complex and dynamic environments, to better demonstrate their versatility and robustness.",
            "16": "- Include scenarios that involve more diverse social interactions and challenges to showcase the agents' adaptability.",
            "17": "- **Address scalability concerns:**\n     - Conduct experiments with a larger number of agents and in more complex environments to evaluate the scalability of the proposed techniques.",
            "18": "- Provide a detailed analysis of the computational costs and performance metrics as the number of agents and the complexity of the environment increase.",
            "19": "- **Enhance the discussion on limitations and future work:**\n     - Provide a more thorough discussion of the current limitations of Lyfe Agents and potential areas for future research and improvement.",
            "20": "- Suggest specific directions for extending the framework to incorporate more advanced features, such as pixel-space vision and simulated robotic bodies."
        },
        "xjW141wtk0": {
            "0": "- Tackles the problem of cost efficient generative agents and effective memory systems.",
            "1": "The topic is very relavant to the conference and as a field.",
            "2": "The paper is presented in a simpe manner, and the presented material is easy to understand.",
            "3": "- The economic value seems like a valuable topic to explore, since agents should not need a heavy LLM to perform low level actions, which is very expensive\n- provided clear advantage over generative agents: cost, which is also significant reduction - The paper uses mainly people in the field already practice, e.g., reflecting on top of reflections in generative agents is similar to the self-monitoring in lyfe agents, and time as a inverse importance metric in generative agents is similar to summary-then-forget\n- paper is missing alot of key implementation details in order to reproduce, see questions section for a list of them\n- the environment seem like a navigatable chatroom (only walk/talk actions), seems a bit simplified to incentivize more complex social behaviors"
        },
        "FUZ77x6kwl": {
            "0": "**Novelty**: The specific architecture of the generative agents developed in this work appears novel, although I am not an expert in this particular field.",
            "1": "**Quality**: The development of a comprehensive framework for social interactions by multi-generative agents likely required a significant engineering effort.",
            "2": "From an engineering perspective, the quality of the proposed work is high.",
            "3": "Each technical component of the proposed agent is thoroughly evaluated in the ablation study.",
            "4": "**Clarity**: The paper is mostly well-written and easy to understand.",
            "5": "Although many technical implementation details are omitted from the main sections, this is acceptable for a conference paper submission with a page limit.",
            "6": "**Significance**: The demonstrated results appear significant (although I'm not certain if they truly outperform other work, as discussed in the next section.)",
            "7": "While the proposed work seems to be of high quality from an engineering perspective, its significance as a conference paper is somewhat limited in its current form, due to what I perceive as insufficient experimental evaluation.",
            "8": "Here are my concerns:\n- Although I understand that all the components of the proposed agent contribute to task success rates, it's unclear how crucial they are for the agent to be \"cost-effective\", which was the original motivation of this work as stated in the introduction.",
            "9": "- While the proposed agent is performant, it's unclear if this performance is consistent across various base LLMs.",
            "10": "The experiment appears to use GPT-3.5.",
            "11": "What would happen if we use different LLMs such as Alpaca and GPT4ALL, or LLMs with various model capacities?",
            "12": "Without this information, it remains unclear whether using GPT-3.5 is a crucial requirement or not.",
            "13": "Also, from the perspectives of reproducibility and transparency, using open-sourced LLMs would be more desirable.",
            "14": "- The cost analysis was not very convincing.",
            "15": "Currently, \"cost per agent per human hour\" is reported.",
            "16": "However, for tasks with specific goals such as murder mystery, isn't it more essential to show the cost required until the task is successfully completed?",
            "17": "Otherwise, agents with very low throughput (due to, say, insufficient compute resources) will be evaluated as low cost, which I don't believe this paper intends to argue.",
            "18": "Furthermore, reporting costs in USD seems unhelpful.",
            "19": "This would result in running open-sourced models in a region with cheap electricity as an optimal choice.",
            "20": "What happens if the service provider changes API usage prices in the future?",
            "21": "Why not just report the number of interactions or number of tokens consumed until task completions?"
        },
        "mIER9ZndTm": {
            "0": "The pipeline in Figure 2 is clear and interesting.",
            "1": "The paper is well-written and presents its ideas in a clear, concise manner.",
            "2": "This paper is difficult to regard as a research paper, or even as a technical report.",
            "3": "It more closely resembles a manual for implementing an LLM-based agent framework in two scenarios designed by the authors.",
            "4": "The logic of this paper is not rigorous.",
            "5": "The authors claim that existing LLM-based agents are redundant in leveraging fewer queries to empower the agents.",
            "6": "However, the problem they address is not convincing.",
            "7": "The authors treat the redundancy issue as common knowledge within the community, which lacks objectivity.",
            "8": "The overall logic of this paper is unclear.",
            "9": "What motivates the proposal of the three modules for Lyfe Agents, and how do these modules differ from those in other LLM-based agents?",
            "10": "The improvements offered by these modules are difficult to discern.",
            "11": "The authors do not fairly compare Lyfe Agents with existing LLM-based agents.",
            "12": "The paper's most significant claim pertains to the 'low-cost' agents depicted in Figure 6.",
            "13": "However, the authors should attempt to keep all variables consistent when comparing the queries of LLMs.",
            "14": "Comparing generative agents with Lyfe Agents in different scenarios is unjust!",
            "15": "Also, the lyfe agents should compare with other general social simulation agents such as [1,2].",
            "16": "There are no objective evaluation metrics for Lyfe Agents other than cost per hour (which, as stated in point 3, is an unfair comparison).",
            "17": "The police success rate shown in Figure 3b is absurd.",
            "18": "It is based on a self-designed script with predetermined and clear answers, which is not convincing and should not serve as an evaluation metric in a scientific paper.",
            "19": "The proposed framework in figure 2 should be a general LLM-based agents framwork.",
            "20": "The lyfe agent should be evaluated in other well-know scenarios such as games and generative agents [3,4].",
            "21": "Generative agents [3] propose reflection to summarize the high-level memory.",
            "22": "What is the difference between the proposed memory mechanism and the one in generative agents [3]?",
            "23": "[1] Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen.",
            "24": "Agentsims: An open-source sandbox for large language model evaluation.",
            "25": "arXiv preprint arXiv:2308.04026, 2023.",
            "26": "[2] Lei Wang, Jingsen Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, and Ji-Rong Wen.",
            "27": "Recagent: A novel simulation paradigm for recommender systems.",
            "28": "arXiv preprint arXiv:2306.02552, 2023.",
            "29": "[3] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein.",
            "30": "Generative agents: Interactive simulacra of human behavior.",
            "31": "In In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST ’23), UIST ’23, New York, NY, USA, 2023.",
            "32": "Association for Computing Machinery.",
            "33": "[4] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.",
            "34": "Voyager: An open-ended embodied agent with large language models.",
            "35": "arXiv preprint arXiv:2305.16291, 2023."
        },
        "d3q8pxHDxw": {
            "0": "- I like how the authors have grounded their three new modules in brain-inspired research.",
            "1": "All three of the modules appear to be valid and useful in terms of improving performance of the agents, and I appreciate that they are also designed in order to be cost effective.",
            "2": "- I think the scenarios that the authors have developed in order to evaluate their agents are smart and well-designed, and useful for picking out interesting emergent behaviours amongst LLM agents.",
            "3": "In particular the murder mystery scenario, and the corresponding ablation that demonstrates the importance of all of the proposed modules.",
            "4": "I have two main problems with the paper that, in my opinion, need to be addressed.",
            "5": "1) Whilst the intentions and role of all of the modules are well explained, the details of their implementations are not particularly well explained within the main body of the text.",
            "6": "For example, take the option-action selection module: what are the implementation details for the LLM call that is used to output an options along with a subgoal?",
            "7": "What are the available options, is this defined by the LLM or a discrete list provided beforehand?",
            "8": "What are the details of the exit conditions that lead to the option-action selection module being called?",
            "9": "Whilst there are some more details included in the appendix, I believe some more important implementation points should be included in the main body of the text.",
            "10": "2) I am not currently convinced by the cost analysis that is provided by the authors.",
            "11": "This is particularly important as one of the key-selling points of the framework, according to the authors, is the cost effectiveness of the framework.",
            "12": "I am not disputing the project cost of the Stanford GenAgent work.",
            "13": "However, I just have a few queries about the cost provided for the Lyfe Agent.",
            "14": "Firstly, is the difference in cost primarily coming from the fact that GPT3.5 is used for LyfeAgent vs. GPT-4 for Stanford GenAgent?",
            "15": "In my opinion, this isn't necessarily a fair comparison - instead would it not be fairer to evaluate more in terms of tokens inputted / outputted when solving the scenarios?",
            "16": "For example, how many tokens need to be inputted / outputted by LyfeAgent vs. Stanford GenAgent in order to arrive at a Murder mystery solution?",
            "17": "In my opinion this is a fairer comparison."
        },
        "QuCG0BSAP0": {
            "0": "•\tAgent “Brain”: The authors combine and enhance different modeling techniques that are motivated by the human brain.",
            "1": "This approach shows promising results in the simulation of emergent social behavior.",
            "2": "I want to highlight the combination of the self-observation and the dynamic memory system.",
            "3": "In combination with the retrieval process based on vector similarity, I find this a fair contribution to the conference.",
            "4": "•\tMultiple Experiments + Ablation: The work contains three experiments covering different domains of collaboration and conversation.",
            "5": "The results show that the agents successfully manage all scenarios.",
            "6": "In addition, the ablation setup provides insights into the importance of each submodule in the larger context.",
            "7": "Thus, I find the results sound in terms of technical claims and experimentation setup.",
            "8": "•\tImplementation of the virtual world: I see no benefit in highlighting the 3D environment work this work contribution.",
            "9": "The interaction is purely text-based, and the agents do not perceive their visuals as image inputs.",
            "10": "Thus, the work could be reduced to a text-based role-play scenario.",
            "11": "•\tAppendix Structure: I found relevant information about the agent architecture missing in the content but given inside the appendix.",
            "12": "Thus, moving relevant information into the body of the paper would emphasize the main contribution.",
            "13": "•\tReliance on GPT3.5: Like previous agent-based work (Park et al.",
            "14": "), the observations and consequences are limited to a singular foundation model.",
            "15": "As this work employs complex memory and decision mechanisms, the behavior of competitive models like Llama-2 70B or Falcon 180B, when prompted with this approach, may strengthen impact."
        }
    },
    "w49jlMWDSA": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant challenge in fine-grained image classification by proposing a novel method, GIST, to generate image-specific text descriptions from image-only datasets.",
            "1": "- The approach leverages large language models (LLMs) and vision-language models to create detailed and relevant text descriptions, which are then used to fine-tune vision-language models for improved classification performance.",
            "2": "- The method demonstrates notable improvements over existing state-of-the-art techniques, highlighting its potential impact on the field.",
            "3": "Potential reasons for acceptance\n   - The proposed method, GIST, shows a clear improvement in classification accuracy across multiple fine-grained datasets, both in full-shot and few-shot scenarios.",
            "4": "- The approach is innovative in its use of LLMs to generate diverse and detailed text descriptions, which are then matched to images using a vision-language model.",
            "5": "- The paper provides a comprehensive evaluation of the method, including comparisons with several strong baselines and detailed ablation studies.",
            "6": "- The introduction of a new fine-grained image classification dataset, Fitzpatrick40, adds value to the research community.",
            "7": "Potential reasons for rejection\n   - **Limited novelty in methodology**\n     - The method primarily combines existing techniques (LLMs and vision-language models) rather than introducing fundamentally new concepts.",
            "8": "- The novelty may be perceived as incremental, focusing on the application of known models in a new context.",
            "9": "- **Dependence on large language models**\n     - The approach relies heavily on the capabilities of LLMs like GPT-3 and GPT-4, which may not always generate accurate or relevant descriptions, as acknowledged by the authors.",
            "10": "- The need for manual checking of generated descriptions for certain datasets could limit the scalability and practicality of the method.",
            "11": "- **Evaluation on limited datasets**\n     - The evaluation is conducted on four datasets, which, while diverse, may not fully represent the wide range of fine-grained classification tasks.",
            "12": "- Additional experiments on more varied and challenging datasets could strengthen the validation of the method's effectiveness.",
            "13": "- **Potential bias in generated descriptions**\n     - The quality and relevance of the generated text descriptions are crucial for the method's success, and any biases or inaccuracies in the LLM outputs could negatively impact performance.",
            "14": "- The paper does not thoroughly address how to mitigate potential biases in the generated text.",
            "15": "Suggestions for improvement\n   - **Enhance novelty and originality**\n     - Consider incorporating additional novel elements or techniques that go beyond the current combination of LLMs and vision-language models.",
            "16": "- Explore new ways to generate or refine text descriptions that do not solely rely on existing LLMs.",
            "17": "- **Address dependence on LLMs**\n     - Investigate methods to reduce the reliance on LLMs or improve the accuracy and relevance of the generated descriptions without manual intervention.",
            "18": "- Develop automated validation or correction mechanisms for the generated text to ensure higher quality and consistency.",
            "19": "- **Expand evaluation**\n     - Conduct experiments on a broader range of fine-grained classification datasets to demonstrate the generalizability and robustness of the method.",
            "20": "- Include more challenging and diverse datasets to provide a more comprehensive assessment of the method's performance.",
            "21": "- **Mitigate potential biases**\n     - Implement strategies to identify and mitigate biases in the generated text descriptions, ensuring fair and unbiased classification results.",
            "22": "- Provide a detailed analysis of potential biases and their impact on the method's performance, along with proposed solutions to address them."
        },
        "NndbacTh7J": {
            "0": "+ The proposed method is simple, straightforward, and sound.",
            "1": "+ The paper is clearly written and easy to understand.",
            "2": "+ The proposed method achieves better results than comparable methods.",
            "3": "- The proposed method requires domain (dataset) specific designs such as prompts for LLM.",
            "4": "However, in a few-shot setting, it is hard to determine the designs because of the scarcity of validation and test sets.",
            "5": "This makes the proposed method less general to real-world few-shot tasks.",
            "6": "- The paper lacks of comparison with other fine-tuning techniques such as prompt tuning approaches.",
            "7": "Please analyze and compare with other families of VL finetuning.",
            "8": "- If I understood correctly, the proposed method requires full images for the target domain.",
            "9": "In a real-world setting, it may be hard to obtain such images, especially for medical images.",
            "10": "- Why is a summary is required?",
            "11": "We can see the performance improvement by doing this, but I found no proper explanations for why.",
            "12": "- Manual cleaning mentioned in Section 5 seems not fancy.",
            "13": "In a specific field, manual cleaning requires an expensive effort of experts such as the medical field.",
            "14": "In addition, instead of manually cleaning, we can make labels for more images under the same effort, which is likely to produce much more improvements than manual cleaning."
        },
        "CgNbobrj43": {
            "0": "(1) The motivation of prompting LLM for fine-grained classification is well presented.",
            "1": "(2) The explanations and illustrations of the three-step workflow is well-formulated and mostly clear.",
            "2": "(1) The motivation of prompting LLM for visual classification is not that novel to the community and the authors listed (Maniparambil et al., 2023) as an example.",
            "3": "(2) The contribution is limited since the core idea is the same as typical data labeling workflow that uses LLM.",
            "4": "More specifically, during prompting LLM, the user-provided prompt is not coming for free, it is also a kind of human knowledge or preference prior.",
            "5": "There is no discussion on this difficulty of prompt preparation as compared to typical easy prompt such as “a photo of class name”.",
            "6": "Thus, the workflow of generating image-text pairs has no difference from data labeling workflow that uses LLM.",
            "7": "(3) In the experiments, Table 1, the proposed workflow which uses GPT-3, CLIP and intricate prompts, does not get significant improvement over FLYP which uses CLIP and a typical easy prompt, though the comparison setting is unfair.",
            "8": "(4) The most conflicting method (Maniparambil et al., 2023) is not compared in the experiments."
        },
        "v1Oe5Vuj0z": {
            "0": "This paper presents a new method named GIST for fine-grained classification.",
            "1": "The proposed method adopts a large language model to generate domain-specific class descriptions and matches the texts and images with a pre-trained vision-language model.",
            "2": "This paper trains a CLIP classifier with the generated and matched descriptions.",
            "3": "The proposed GIST achieves good results on several benchmarks with full/few-shot settings.",
            "4": "I'm concerned about the technical contribution of this paper.",
            "5": "Using a large language model to generate/augment text for CLIP training has been explored in several works [1,2].",
            "6": "This paper lacks the ablations about matching texts and images.",
            "7": "It's unclear whether the matching based on a pre-trained CLIP will impact the downstream tasks.",
            "8": "In addition, I'm concerned about how many captions are matched to one image and whether more captions will help.",
            "9": "This paper lacks studies on the impact of fine-tuning CLIP and whether more data (images) will further improve.",
            "10": "[1] Fan et.al.",
            "11": "Improving CLIP Training with Language Rewrites.",
            "12": "NeurIPS 2023.",
            "13": "[2] Maniparambil et.al.",
            "14": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts.",
            "15": "ICCVW 2023."
        }
    },
    "L4nOxziGf9": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenge of underspecification in vision-language models (LVLMs) by proposing a novel framework, Rephrase, Augment, and Reason (REPARE).",
            "1": "- The approach leverages the strengths of large language models (LLMs) and vision encoders to improve zero-shot performance in visual question answering (VQA) tasks.",
            "2": "- The method is gradient-free and does not require additional training data, making it a practical solution for enhancing existing LVLMs.",
            "3": "Potential reasons for acceptance\n   - The proposed REPARE framework demonstrates significant improvements in zero-shot VQA performance across multiple datasets and LVLM architectures.",
            "4": "- The paper provides a comprehensive analysis of the framework's effectiveness, including ablation studies and comparisons with paraphrasing baselines.",
            "5": "- The approach is innovative in its use of visually-grounded information and rationales to rephrase and augment questions, addressing the issue of underspecification effectively.",
            "6": "- The empirical results are robust, showing consistent performance gains and establishing upper bounds through oracle settings.",
            "7": "Potential reasons for rejection\n   - **Cost and Efficiency Concerns**\n     - The REPARE framework involves generating multiple question candidates and selecting the best one, which increases computational cost and inference time.",
            "8": "- The linear scaling of cost with the number of candidates may be prohibitive for real-time applications or large-scale deployments.",
            "9": "- **Dependence on Existing LVLMs**\n     - The framework relies heavily on the capabilities of existing LVLMs for generating captions, rationales, and rephrased questions.",
            "10": "- If the underlying LVLMs have limitations or biases, these could propagate through the REPARE framework, potentially affecting the quality of the rephrased questions.",
            "11": "- **Limited Scope of Evaluation**\n     - The evaluation is primarily focused on VQA tasks, and it is unclear how well the framework would generalize to other vision-language tasks or datasets.",
            "12": "- The datasets used for evaluation, while standard, may not fully capture the diversity of real-world scenarios where underspecification occurs.",
            "13": "Suggestions for improvement\n   - **Optimize Computational Efficiency**\n     - Explore methods to reduce the computational cost of generating and selecting question candidates, such as more efficient scoring functions or parallel processing techniques.",
            "14": "- Investigate the impact of reducing the number of candidates (n) on performance to find a balance between cost and accuracy.",
            "15": "- **Broaden Evaluation Scope**\n     - Extend the evaluation to include a wider range of vision-language tasks and datasets to demonstrate the generalizability of the REPARE framework.",
            "16": "- Include more diverse and challenging datasets that better represent real-world scenarios with varying levels of underspecification.",
            "17": "- **Address Potential Biases**\n     - Conduct a thorough analysis of potential biases in the underlying LVLMs and how they might affect the REPARE framework.",
            "18": "- Implement mechanisms to detect and mitigate biases in the generated captions, rationales, and rephrased questions to ensure fairness and robustness.",
            "19": "- **Enhance Qualitative Analysis**\n     - Provide more detailed qualitative examples and case studies to illustrate the specific improvements made by REPARE in addressing underspecification.",
            "20": "- Include user studies or human evaluations to assess the perceived quality and relevance of the rephrased questions from a human perspective."
        },
        "wKRcZg36pl": {
            "0": "- The motivation is straight-forward, clear, and reasonable.",
            "1": "- The improvements seem solid and the analyses support the improvements that come from the proposed pipeline.",
            "2": "- The code and the data are provided and the authors promised public release.",
            "3": "This is very important because the proposed pipeline is complicated and not easily reproducible.",
            "4": "- The pipeline seems overcomplicated and involves many steps that are indispensable to the overall performance.",
            "5": "Unlike plain CoT, which usually conducts inference once, the proposed pipeline conducts inference multiple times using the LVLM and involves off-the-shelf tools twice.",
            "6": "The complexity may affect the reproduction of the method and the incurred (computation and time) cost may hinder the adoption of the method in application.",
            "7": "- The main results (Table 1) need more explanation.",
            "8": "(1) For example, the standard deviation considering the oracle implementation is high.",
            "9": "I did not expect that using the optimal candidate would lead to higher variance.",
            "10": "Are there results regarding the choice of the number of the question candidates?",
            "11": "(2) I would love to see more QA datasets (from diverse sources) tested on.",
            "12": "(3) I wonder if the asymmetric strength hypothesis holds, is it possible that stage 2 and stage 3 can be changed to using the original question and the extracted details in texts without the image?",
            "13": "(4) All analyses are based on BLIP-2, an encoder-decoder model.",
            "14": "I don't think I find a discussion on the effect of model architecture (encoder-decoder or decoder-only).",
            "15": "- The writing and the organization can be improved.",
            "16": "Personally, I would like Section 3 Methodology to be more straightforward.",
            "17": "From what I understand, Stage I (ii) adopts different post-processing from (i) and (iii).",
            "18": "The paragraph before Section 3.2 states \"we prompt the LVLM to answer each question\" and the first paragraph in Section 3.2 states \"To select which question to answer\", which are contradictory.",
            "19": "I had to check the appendix and the footnotes multiple times to guess what's going on."
        },
        "hcbaENDxKB": {
            "0": "The performance improvement from the method seem to be solid.",
            "1": "The paper is mostly clear with extensive experiments.",
            "2": "Some of the text requires further clarification.",
            "3": "The underlying hypothesis should be stated more clearly, which in my understanding is that distribution of more specified questions are more aligned with the training data of LVLMs and the answers with higher confidence are more likely to be correct."
        },
        "ndBQp2mfUP": {
            "0": "- The paper is easy to read and generally well-written\n- Interesting idea of improving VL models in VQA tasks by just modifying one modality (e.g text).",
            "1": "- Improvements over baselines (BLIP2, MiniGPT4) looks reasonable.",
            "2": "- Evaluation suite should be improved.",
            "3": "For example including: TextVQA, VizWiz.",
            "4": "Additionally, authors should consider evaluating tasks such as HatefulMemes which might be more challenging to the proposed approach.",
            "5": "Also, consider recent evaluation tasks such as MME.",
            "6": "- Including recent instruct multimodal models (e.g.",
            "7": "LLaVa, Qwen-VL) would be an interesting experiment to see if the gain with Repare is still relevant in these models.",
            "8": "- Minor:\n    - >One approach involves additional VL pretraining ...",
            "9": "In this case, LENS does not involve additional multimodal pretraining.",
            "10": "- Why not include BLIP2-XXL as one of your main baselines and improve it with Repare?"
        }
    },
    "9F0xInGNBF": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a novel framework, VideoPrompter, which enhances zero-shot video understanding by combining pre-trained discriminative vision-language models (VLMs) with generative video-to-text and text-to-text models.",
            "1": "- The proposed method addresses the limitations of existing approaches by simultaneously refining class representations and enriching visual features, which is a significant advancement in the field of zero-shot video understanding.",
            "2": "- The framework demonstrates consistent improvements across multiple benchmarks and various VLMs, showcasing its effectiveness and potential impact on the field.",
            "3": "Potential reasons for acceptance\n   - The proposed framework is innovative and addresses a critical gap in zero-shot video understanding by enhancing both visual and text-based classifier representations.",
            "4": "- The paper provides comprehensive experimental results across three different zero-shot settings, demonstrating the effectiveness of the approach.",
            "5": "- The framework is adaptable to various existing VLMs, making it a versatile and valuable contribution to the field.",
            "6": "- The authors provide detailed explanations and justifications for their design choices, enhancing the credibility and reproducibility of the work.",
            "7": "Potential reasons for rejection\n   - **Complexity and Implementation Challenges**\n     - The proposed framework involves multiple components, including video-to-text and text-to-text generative models, which may increase the complexity of implementation.",
            "8": "- The requirement for pre-trained models and the integration of various components might pose challenges for practitioners looking to adopt the framework.",
            "9": "- **Evaluation and Benchmarking**\n     - While the paper demonstrates improvements across multiple benchmarks, it would be beneficial to include more diverse datasets and real-world scenarios to further validate the generalizability of the approach.",
            "10": "- The paper could provide more detailed comparisons with state-of-the-art methods to highlight the specific advantages and limitations of the proposed framework.",
            "11": "- **Interpretability and Explainability**\n     - The interpretability of the model decisions, while discussed, could be further elaborated with more examples and visualizations to enhance understanding.",
            "12": "- The paper could benefit from a more in-depth analysis of the impact of different components and design choices on the overall performance.",
            "13": "Suggestions for improvement\n   - **Simplify Implementation**\n     - Provide detailed implementation guidelines and code snippets to help practitioners integrate the proposed framework into their existing systems.",
            "14": "- Consider offering a simplified version of the framework that balances performance improvements with ease of implementation.",
            "15": "- **Expand Evaluation**\n     - Include additional datasets and real-world scenarios to further validate the generalizability and robustness of the proposed approach.",
            "16": "- Provide more detailed comparisons with state-of-the-art methods, highlighting specific advantages and limitations.",
            "17": "- **Enhance Interpretability**\n     - Include more examples and visualizations to illustrate the interpretability of the model decisions, helping readers understand how the framework enhances video understanding.",
            "18": "- Conduct a more in-depth analysis of the impact of different components and design choices on the overall performance, providing insights into the contributions of each part of the framework.",
            "19": "- **Reproducibility and Accessibility**\n     - Ensure that all code, models, and datasets used in the experiments are publicly available and well-documented to facilitate reproducibility.",
            "20": "- Consider providing a user-friendly interface or toolkit that allows researchers and practitioners to easily experiment with and extend the proposed framework."
        },
        "EYpZBP60Ni": {
            "0": "The experiments and ablation studies are conducted comprehensively, validated on different pre-existing architectures, and taken into account various types of video data.",
            "1": "Employing the video-to-text model (not limited to VGPT and caption models) is novel and worthy to explore in the video field.",
            "2": "Fusing the text and video representations is depicted to be beneficial in bridging the gap between video and textual labels in the embedding space.",
            "3": "The manuscript provides a detailed explanation and examples of prompting the GPT to refine the simple textual label, which in turn enhances reproducibility.",
            "4": "In the 'video-to-text guided visual feature enhancement' (section 2.2), the adopted VGPT relies on CLIP-ViT-L and vicuna, where the computational cost of performing multiple inferences (including text embedding and filtering) far exceeds that of the basic video understanding model.",
            "5": "This limits the practical value of the proposed approach.",
            "6": "Except for CLIP, an image-language pre-trained model targeted specifically for the image field, the proposed approach shows relatively limited performance gain in other video-based models (ViFi-CLIP, AIM, ActionCLIP), considering the additional computational requirements.",
            "7": "The configurations of adopted pre-trained models (AIM, ActionCLIP, …) remain unclear, which datasets are these models pre-trained on (e.g.",
            "8": "K400, K700, …)?",
            "9": "For AIM, do the authors directly remove the classification layers?"
        },
        "7IhGy5eD3U": {
            "0": "This paper studies an important problem of adapting pre-trained vision-language models to downstream tasks in zero-shot settings.",
            "1": "The introduced method is lucid and holds promise for extension across a wide range of VLMs.",
            "2": "The experimental results look good.",
            "3": "VideoPrompter is able to increase the zero-shot performance of VLMs across multiple tasks.",
            "4": "The paper is well-presented.",
            "5": "The efficiency of VideoPrompter hasn't been thoroughly examined.",
            "6": "Given that VideoPrompter appears to require generating 10 times the number of samples and the use of an additional text-to-video model, it could substantially raise the inference costs, both for evaluating existing VLMs and in practical applications.",
            "7": "The selection of Video-ChatGPT as the video-to-text model seems arbitrary.",
            "8": "Alternative models, such as Video-LLaMA [A], should be considered and discussed.",
            "9": "An ablation study on the video-specific language descriptors is missing.",
            "10": "[A] Zhang, H., Li, X., & Bing, L. Video-llama: An instruction-tuned audio-visual language model for video understanding.",
            "11": "arXiv preprint arXiv:2306.02858."
        },
        "Gt5qdFFKFi": {
            "0": "The studied problem is interesting.",
            "1": "Video understanding with large foundation models is of wide interest in the community.",
            "2": "The authors put together state-of-the-art large foundation models and improve the zero-shot inference performance on video understanding tasks.",
            "3": "The idea of generating more descriptions for class names and using high-level context is not new in prompting large foundation models (e.g.",
            "4": "the prior works cited in this paper).",
            "5": "This is model ensembling for enhancing zero-shot performance.",
            "6": "Can the authors justify the main novelty of this paper?",
            "7": "VGPT is used to generate the text description of the query video, and which is then converted to an image-like text embedding.",
            "8": "Why not just prompting VGPT for the downstream applications (e.g.",
            "9": "action classification)?",
            "10": "Comparison to this baseline is an important justification to the proposed method.",
            "11": "Several components are added to the solution, while the ablations are not sound enough.",
            "12": "For example, how important are the three description types (parent context, language attributes, and language descriptions)?",
            "13": "The claim for the comparison to CUPL (Pratt et al., 2022) is not very clear (section 3.1.4).",
            "14": "The authors claim that VideoPrompter only requires 3 text descriptions instead of 50 descriptions adopted in CUPL.",
            "15": "However, VideoPrompter adopts a VGPT model while CUPL does not.",
            "16": "Is using VGPT a better choice in terms of the cost?",
            "17": "The paper criticizes prior work that “these methods require access to the true distribution of the target task, which can be prohibitive in test-time adaptation and data-scarce environments”.",
            "18": "However, the proposed method optimizes the selection of hyperparameters (e.g.",
            "19": "temperature) directly on the target dataset (see Figure 3).",
            "20": "The high-level action context is restricted to a tree-type relation.",
            "21": "However, some child classes may belong to multiple parent concepts.",
            "22": "For example, “surfing” can belong to both “playing sports” and “water activities”."
        },
        "CsiiZNAJmc": {
            "0": "- The paper presents a set of experiments on various problem settings: action recognition, video-to-text and text-to-video retrieval, time-sensitive tasks and on different datasets.",
            "1": "- The ablations are solid and thorough.",
            "2": "- Experiments show strong improvement w.r.t baselines.",
            "3": "- Since at least 3 foundation models have been used (CLIP, GPT, VGPT), how do we know if those models are trained with examples overlapped with the downstream datasets (e.g., HMDB-51, UCF101, SSv2, K400, MSR-VTT, Charades).",
            "4": "- The novelty seems moderate if not low.",
            "5": "As the paper mentions the main contributions are 1) introducing video-to-text to enhance visual embeddings and 2) applications to videos.",
            "6": "- The written presentation could be further improved:\n     1) section 2.1 could be renamed to \"Overview\" and try to capture the big picture of the framework.",
            "7": "The author(s) can refer back to Fig.",
            "8": "1 for the big picture (in the current flow of presentation, there is no big picture and it flows in with overwhelming many details and notations).",
            "9": "Then sections 2.2 and 2.3 can be further followed up from 2.1 to provide detailed of components.",
            "10": "2) table 6 is presented in page 7, yet never been referred from the text?"
        }
    },
    "QXRScRrwNr": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the in-context learning ability of retrieval-augmented encoder-decoder language models, which is a relatively underexplored area compared to decoder-only models.",
            "1": "- The proposed model, RAVEN, introduces novel techniques such as Fusion-in-Context Learning and a combination of retrieval-augmented masked and prefix language modeling to enhance few-shot performance.",
            "2": "- The work demonstrates significant improvements over the state-of-the-art ATLAS model, highlighting the potential of retrieval-augmented encoder-decoder models in in-context learning.",
            "3": "Potential reasons for acceptance\n   - The paper provides a comprehensive analysis of the limitations of the ATLAS model and proposes effective solutions to address these issues.",
            "4": "- The introduction of RAVEN and its novel techniques shows substantial performance improvements in both zero-shot and few-shot settings.",
            "5": "- The experimental results are extensive and demonstrate the effectiveness of the proposed methods across multiple datasets and tasks.",
            "6": "- The paper encourages further research in the area of retrieval-augmented encoder-decoder language models, which could lead to significant advancements in the field.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability:**\n     - Some sections of the paper, particularly the methodology, are dense and may be difficult for readers to follow without a strong background in the field.",
            "8": "- The paper could benefit from clearer explanations and more detailed descriptions of the proposed techniques and their implementation.",
            "9": "- **Comparative analysis:**\n     - While the paper compares RAVEN to several baselines, it lacks a detailed analysis of why RAVEN outperforms these models, particularly in terms of the specific contributions of the proposed techniques.",
            "10": "- The paper could provide more insights into the limitations of the baselines and how RAVEN addresses these issues.",
            "11": "- **Generalizability:**\n     - The paper focuses primarily on open-domain question answering tasks.",
            "12": "It would be beneficial to see how the proposed methods perform on a wider range of tasks to demonstrate their generalizability.",
            "13": "- The paper could include more diverse datasets and tasks to strengthen the claim that RAVEN is effective across different scenarios.",
            "14": "Suggestions for improvement\n   - **Improve clarity and readability:**\n     - Simplify the language and structure of the methodology section to make it more accessible to a broader audience.",
            "15": "- Provide more detailed explanations and visual aids (e.g., diagrams, flowcharts) to illustrate the proposed techniques and their implementation.",
            "16": "- **Enhance comparative analysis:**\n     - Include a more detailed analysis of the performance differences between RAVEN and the baselines, highlighting the specific contributions of the proposed techniques.",
            "17": "- Discuss the limitations of the baselines in more detail and explain how RAVEN addresses these issues.",
            "18": "- **Expand evaluation:**\n     - Evaluate the proposed methods on a wider range of tasks and datasets to demonstrate their generalizability and robustness.",
            "19": "- Include additional benchmarks and comparisons to other state-of-the-art models to provide a more comprehensive evaluation of RAVEN's performance.",
            "20": "- **Provide implementation details:**\n     - Include more detailed information about the implementation of the proposed techniques, such as hyperparameters, training procedures, and computational resources used.",
            "21": "- Ensure that the code and model checkpoints are made publicly available to facilitate reproducibility and further research."
        },
        "smCgUYVjKv": {
            "0": "Propose an interesting question “in-context learning in retrieval augmented language model”.",
            "1": "Decoder-only language models are good at in-context learning while hard to use retrieved information.",
            "2": "Encoder-decoder language models are good at retrieval augmented generation due to their separated encoder while unstable for in-context learning.",
            "3": "This paper aims to introduce in-context learning ability into encoder-decoder language models.",
            "4": "This paper is well-written and easy to follow.",
            "5": "The proposed method is well-motivated and very easy to implement.",
            "6": "Detailed experimental results on diverse datasets and the additional analysis can answer the main concern of motivation.",
            "7": "Although I admit that exploring the in-context learning (ICL) in retrieval-augmented language model is an interesting research question, I think the specific research points of this paper are still limited.",
            "8": "This paper only study how to make the language model perform better on question-answering task under the retrieval-augmented paradigm.",
            "9": "This is inconsistent with our accepted definition and expectation of ICL.",
            "10": "ICL enables the language model to learn and perform various tasks using a few input examples.",
            "11": "Just focusing on knowledge-intensive tasks such as question-answering cannot be called ICL.",
            "12": "It is more like few-shot learning for question-answering.",
            "13": "The technical contribution of this paper is limited.",
            "14": "- In order to bridge the gap between pre-training and inference in ATLAS, this paper introduces prefix language modeling.",
            "15": "It just puts the special token at the end of the texts.",
            "16": "Prefix language modeling is a well-known method in the pre-training of decoder-only language models such as GPT.",
            "17": "- In order to input more examples to the retrieval-augmented language model, this paper just feeds different examples to FID encoder with each passage.",
            "18": "The above two points are the core technical contribution of this paper which is limited.",
            "19": "This paper emphasizes the advantages of the encoder-decoder architecture model in retrieval-augmented compared to the decoder-only model, but its core technical contribution is to change the masked language modeling of the encoder model to the prefix language modeling of the decoder-only model.",
            "20": "This point is very contradictory.",
            "21": "Some decoder-only pre-trained language models with considerable size such as LLama-7B should be used as baselines.",
            "22": "Many papers show LLama-7B can achieve better performance in Table 3.",
            "23": "I hope the author can carefully analyze the advantages of the encoder-decoder architecture model compared to the decoder-only model in terms of retrieval-augment."
        },
        "gLxRpjl35S": {
            "0": "(1) The explored topic, combining in-context learning (ICL) and retrieval augmented generation (RAG) is very interesting.",
            "1": "(2) The paper is well-written, and the proposed idea is simple and easy to follow.",
            "2": "(3) The paper identifies two drawbacks of ATLAS on ICL, including (1) the mismatch between pretraining and testing and (2) the restricted context length, which makes sense to me.",
            "3": "(4) Compared with ATLAS, the performance is significantly improved.",
            "4": "(1) Though the proposed method has a significantly better few-shot performance than ATLAS, the work seems to be incremental compared with ATLAS in the aspect of the modeling.",
            "5": "The first method is to continuously pretrain ATLAS by masking the later part of the sequence.",
            "6": "The second method is to blend different limited in-context examples into different passages in the Fusion-in-Decoder architecture.",
            "7": "The whole architecture is exactly the same as ATLAS, which makes me feel the proposed method is more like the tricks for ATLAS for better ICL performance, including a fine-tuning method and a prompt engineering method.",
            "8": "(2) Though the proposed method outperforms ATLAS, it still cannot match decoder-only LLM as shown in Tables 3 and 5.",
            "9": "(I do raise questions for this comparison in the question section.)"
        },
        "CufqsO7GdG": {
            "0": "Innovative Approach: The introduction of Fusion-in-Context Learning (FiCL) to address the limitations of constrained context length in base models like T5 or ATLAS is a significant innovation.",
            "1": "This strategy improves scalability and performance in in-context learning scenarios.",
            "2": "Comparative Performance: RAVEN models demonstrate superior performance compared to other models like GPT-3, PaLM, Codex, and RETRO in tasks such as Natural Questions and TriviaQA, particularly in zero-shot and few-shot settings.",
            "3": "The paper demonstrates the effectiveness of the proposed method primarily in the context of encoder-decoder models.",
            "4": "However, its effectiveness in popular left-to-right language models, which are widely used, is not explicitly addressed.",
            "5": "This omission can limit the understanding of how the proposed method might perform or be adapted to these prevalent LMs such as LLaMA and GPT."
        }
    },
    "J7AwIJvR3d": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical question in the field of computational linguistics and cognitive neuroscience: the differences between language models (LMs) and human brains in processing language.",
            "1": "- It introduces a novel approach by using Magnetoencephalography (MEG) data to systematically explore these differences.",
            "2": "- The study identifies three key phenomena (emotional understanding, figurative language processing, and physical commonsense) that LMs struggle to capture, and demonstrates that fine-tuning LMs on related datasets can improve their alignment with human brain responses.",
            "3": "Potential reasons for acceptance\n   - The paper tackles an important and underexplored area, providing valuable insights into the divergences between LMs and human brains.",
            "4": "- The use of MEG data offers high temporal resolution, allowing for a detailed analysis of the temporal progression of language processing.",
            "5": "- The methodology of using an LLM-based hypothesis proposer to automatically generate and validate hypotheses is innovative and could be applied to other research areas.",
            "6": "- The findings have practical implications for improving LMs by fine-tuning them on specific types of knowledge, potentially enhancing their performance on various NLP tasks.",
            "7": "Potential reasons for rejection\n   - **Limited dataset and generalizability**\n     - The study relies on a relatively small dataset (MEG responses from a limited number of participants reading specific chapters of a book), which may not be representative of broader language processing scenarios.",
            "8": "- The fine-tuning datasets used (e.g., Social IQa, Fig-QA, PiQA) may not fully capture the diversity and complexity of the phenomena being studied, potentially limiting the generalizability of the findings.",
            "9": "- **Methodological concerns**\n     - The process of annotating the text for the three phenomena (emotion, figurative language, physical commonsense) is not detailed enough, raising questions about the consistency and reliability of the annotations.",
            "10": "- The use of a single base model (GPT-2 XL) for all experiments may limit the applicability of the findings to other, more recent LMs.",
            "11": "- **Statistical significance and validation**\n     - Some of the p-values reported for the hypotheses may not be statistically significant, and the study acknowledges the small sample size as a limitation.",
            "12": "- The validation of the findings using only one additional chapter of the book may not be sufficient to confirm the robustness of the results.",
            "13": "Suggestions for improvement\n   - **Expand the dataset**\n     - Include a larger and more diverse set of participants and texts to ensure the findings are more generalizable.",
            "14": "- Consider using additional neuroimaging techniques (e.g., fMRI, EEG) to complement the MEG data and provide a more comprehensive understanding of the brain's language processing.",
            "15": "- **Enhance methodological rigor**\n     - Provide a more detailed description of the annotation process for the three phenomena, including inter-annotator agreement metrics to ensure consistency and reliability.",
            "16": "- Experiment with multiple base models, including more recent LMs, to test the generalizability of the findings across different architectures.",
            "17": "- **Improve statistical analysis**\n     - Conduct more extensive statistical analyses to ensure the reported p-values are robust and the findings are statistically significant.",
            "18": "- Validate the findings using a broader range of texts and contexts to confirm the robustness and applicability of the results.",
            "19": "- **Incorporate multimodal data**\n     - Explore the potential of incorporating additional modalities (e.g., visual, motor) to capture a broader spectrum of human knowledge and improve the alignment between LMs and human brain responses."
        },
        "UsPEfRLk2J": {
            "0": "The overall approach is interesting.",
            "1": "The authors attempt to learn in a data-driven way stimuli that are poorly predicted by the model and then try to leverage these insights to improve the model.",
            "2": "This approach seems relatively novel and potentially promising.",
            "3": "The writing is generally clear and the approach was well motivated.",
            "4": "I don’t think the statistical procedure used to establish significance is sufficient and it is not described in enough detail to evaluate it.",
            "5": "In Appendix F, the authors say they permute the “data” using “random permutation”.",
            "6": "What is the data matrix?",
            "7": "Are they computing timepoints?",
            "8": "Computing timepoints is not valid as this destroys the temporal structure of the data.",
            "9": "In addition, the null hypothesis being tested is that there is no relationship between the response and either of the two predictors, which is not the correct null.",
            "10": "The correct null is that there is no difference in prediction between the two predictors (but both predictors explain real variance nonetheless).",
            "11": "The authors need a valid and clearly described statistical procedure to establish their main finding.",
            "12": "In the main figure of the paper, the authors should report a meaningful measure of effect size.",
            "13": "The fraction of significant channels does not give one a sense of the magnitude of the improvement both because significance does not reflect effect size and MEG channels are highly correlated, so it is not particularly impressive when all channels pass significance.",
            "14": "For example, the authors could plot the correlation between the measured and predicted response (as in the appendix scatter plots) for their base LM and a control model such as GloVE.",
            "15": "They could then first test whether their LM showed improved predictions compared with GloVE, which would be a good sanity check, and then could compare the improvement from their fine-tuned model with this increment.",
            "16": "I would also find it interesting to evaluate the performance improvement from fine-tuning with the differences between various LM models.",
            "17": "Could you get a similar improvement by using a slightly larger vs. smaller model or just training the LM model on a bigger language dataset?",
            "18": "The analyses used to motivate the fine-tuning metric is not compelling.",
            "19": "None of the descriptions are significant (which the authors acknowledge) and the procedures used to establish significance are not described.",
            "20": "Even if they were significant it is not clear how reliable the effects are.",
            "21": "For example, if you ran the procedure twice on different subjects would you get the same thing?",
            "22": "Moreover, the labels the authors give feel arbitrary and do not obviously (to me) capture the consensus from the proposed results.",
            "23": "I am not confident for example that if you showed this list to two sets of cognitive scientists that they would reliably yield the same categories (i.e., emotional, physical, figurative).",
            "24": "Moreover, the authors show that performance improves for 2 of the 3 tasks they select with weak improvements for one task.",
            "25": "Is there any reason to believe that this is better than what you would do if you had simply skipped the whole initial procedure and just tried to come up with any three cognitively relevant and distinct tasks?",
            "26": "I am not convinced that the data-driven approach, while interesting, adds value here.",
            "27": "There is no investigation of what it is about the emotion task that yields the best improvements.",
            "28": "For example, the emotion task has more training examples than the other tasks.",
            "29": "Perhaps, the emotion task is simply more difficult in some sense than the other tasks and training the network on harder tasks yields bigger improvements?",
            "30": "There is also no evaluation of whether the fine-tuned models perform well on their task on a different dataset from the one they are fine-tuned on.",
            "31": "If they do not generalize well to another dataset for the same task, it would not be surprising if they do not generalize well to neural predictions.",
            "32": "I am assuming for Figure 5 that the authors are reporting MSE improvement for the corresponding model, i.e.",
            "33": "when comparing emotion words with non-emotional words, they are using the emotion-trained model.",
            "34": "Please clarify this.",
            "35": "Assuming this is the case, it would be helpful to know if the effect is specific to the training task, i.e.",
            "36": "do you also see a boost for the emotional words when you train on the figurative task, and do you see a boost for figurative words when you train on the emotional task.",
            "37": "Please also clarify whether there was correction for multiple comparisons in this figure.",
            "38": "In addition, in panel C of Figure 5, the authors seem to report MSE improvements for the physical task that are at least as large if not larger than those in the other two panels, which seems to conflict with the findings from Figure 4.",
            "39": "Please clarify/resolve this apparent discrepancy.",
            "40": "It is not clear why they did not select the top 100 sentences that were best and worst predicted, averaging across words.",
            "41": "This was the unit of analysis for the proposer-verifier system and so it would be good to know that the sentences were indeed predicted well and badly, respectively.",
            "42": "I think it would be helpful to have a slightly longer (i.e., more than one sentence) description of the propser-validator model.",
            "43": "From the description, it is not clear how the hypotheses were validated during trained.",
            "44": "It would be nice if the authors could list all of the emotional and non-emotional words somewhere (same for the other two categories).",
            "45": "I understand if that is not feasible, but I think that might provide an easier way to validate whether the labels are good.",
            "46": "The Krippendorff’s alpha values seem modest, which made me wonder whether they are high quality."
        },
        "oirNSVwniu": {
            "0": "I like the freshness of the ideas in this paper, however (see below) some of them are too fresh.",
            "1": "But I still would like to note originality of the experimental paradigm and the questions posed in this work.",
            "2": "The idea of comparing representations and looking for differences and then figuring out what were the common properties leading to the most different representations is insightful and very interesting.",
            "3": "MEG signal is very noisy and it captures not only the activity associated with the task (reading the text), but also almost all of the cortical activity that happened to happen at the time.",
            "4": "For the rest of the analysis of this paper it is very important that MEG responses we are predicting are actually responses to the text stimuli, but, knowing MEG, I would say this is very unlikely the case.",
            "5": "So while you are predicting something from LM representations, it is very unlikely that what you are predicting are responses to text stimuli.",
            "6": "Please let me know if MEG source separation or some other new technique has made this possible and I am wrong on this point.",
            "7": "But here is a simple experiment you can use to see where the problem is: out of 9,651 words you have recorded responses to, how many of those words you can identify from the MEG signal alone?",
            "8": "By just training a decoder from MEG to words.",
            "9": "Most probably not many, if any.",
            "10": "So how do we know that the MEG signal you are trying to predict contains _any_ signal produced by text processing by the brain?",
            "11": "Figure 1: \"LLM-based hypothesis proposer is employed to formulate natural language hypotheses explaining the divergence\" -- this sentence is a bit too vague for a scientific paper, the role and function of the Proposer remains unclear after reading such explanation.",
            "12": "Page 2, Contribution 2: How do we know that the \"explanations\" provided by such a model are even true?",
            "13": "Obviously it will generate _an_ explanation, and it would sound plausible as most of the LLM-generated texts do, but how is this approach a method of scientific discovery?",
            "14": "How do we replicate / falsify / validate these \"hypotheses\" that the Proposer is generating?",
            "15": "They might be complete rubbish, how do we know they are not?",
            "16": "Page 2, Contribution 3: Too far-reaching conclusions.",
            "17": "The experiments provided in this work do not do enough to prove these claims to be scientific discoveries.",
            "18": "Figure 2: How much of this correlation can be attributed to the fact that the regression models were trained on very similar (MEG) data?",
            "19": "In order to know if those correlations of up to 0.45 are meaningful one needs to conduct an ablation test, where instead of actual LM representations you feed into your regression models just some random noise.",
            "20": "Or to compute the correlations with MEG activity that you know is not the right activity for the presented stimuli.",
            "21": "My suspicion is that due to the structure of the data that was modelled the correlations would still be quite high.",
            "22": "Page 3, Section 2.3: \"As shown in Figure 2, we observe a temporal pro- gression of accurately predicted areas after word onset\" -- What you see is not necessarily a response to the text that the subject is reading, it might be just a response to a visual stimulus appearing in front of their eyes, or some other stimulus-related brain process that does not necessarily contain any signal generated by the neurons that capture the word representation in the subject's brain.",
            "23": "Page 3, Section 3.1: This idea is novel, but relies on too many assumptions and simplifications to be taken seriously as a tool for scientific discovery.",
            "24": "The responses given by such an LLM will always contain _some_ analysis, no matter which pairs of D0 and D1 corpora you give it, you will _always_ be able to pick Top 10 reasons for differences.",
            "25": "But it does not mean that those differences are actual, valid, or have any relation to the MEG signal.",
            "26": "And there was not explanation provided why we should think that they are or do.",
            "27": "What if you measure the same set of subjects on the same task on a different day, and/or use a different numpy random seed for your analysis pipeline - would the same top 10 (and top 3) differences come to the top?",
            "28": "What if you use another chapter of the book?",
            "29": "Would the top 3 differences still be consistent?"
        },
        "iog3wWwqpc": {
            "0": "The exploration of task-based language model representations and their alignment with the brain is an emerging research direction.",
            "1": "Pretrained model representations contain various sources of information sources, whereas task-based representations focus specifically on task-related details.",
            "2": "Considering that three NLP tasks, the authors' examination of their task-based encoding model's performance on MEG recordings is intriguing.",
            "3": "The idea of task-based modeling and their alignment with Brain is not new.",
            "4": "Recently, several research works utilized task-based representations and shown better brain alignment [1], [2], [3], [4].",
            "5": "[1] Oota et al.",
            "6": "2022, Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?, NAACL-2022\n\n[2] Sun et al.",
            "7": "2023, Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?",
            "8": "IJCAI-2023\n\n[3] Aw et al.",
            "9": "2023, Training language models to summarize narratives improves brain alignment, ICLR-2023\n\n[4] Sun et al.",
            "10": "2023, Tuning In to Neural Encoding: Linking Human Brain and Artificial Supervised Representations of Language, ECAI-2023\n\n2.",
            "11": "The novelty in the paper is limited as it primarily delves into a comparison between three task-based model representations and their brain alignment.",
            "12": "Notably, the authors have not discussed or compared with 3 previous works [1], [2] and [3].",
            "13": "There are lot of questions left in the methodology:\n* Why do authors consider the last layer representations of GPT-2?",
            "14": "All the previous linguistic brain encoding studies reveal that intermediate layer representations are well aligned with brain [5], [6], [7].",
            "15": "* Why do authors consider longer context length i.e.",
            "16": "100?",
            "17": "Previous linguistic brain encoding studies reveal that context-length of 10-50 have better brain alignment [5], [8].",
            "18": "[5] Toneva et al.",
            "19": "2019, Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain), NeurIPS-2019\n\n[6] Jain et al.",
            "20": "2018, Incorporating context into language encoding models for fmri, NeurIPS-2018\n\n[7] Oota et al.",
            "21": "2023, Joint processing of linguistic properties in brains and language models, NeurIPS-2023\n\n[8] Oota et al.",
            "22": "2023, MEG Encoding using Word Context Semantics in Listening Stories, Interspeech-2023\n\n4.",
            "23": "The paper lacks clarity regarding its implications.",
            "24": "Given the emphasis on comparing fine-tuned representations with vanilla model representations, the influence of the dataset is crucial.",
            "25": "If the authors had fine-tuned the model on other emotional datasets, different findings might have emerged.",
            "26": "Additionally, the authors present these task-based findings specifically related to narrative story reading.",
            "27": "It's essential to ascertain whether these findings hold true for the listening modality, for instance, by considering datasets like MEG-MASC story listening.",
            "28": "The focus of the authors is primarily on contextual word representations from the last layer of Language Model Models (LLMs).",
            "29": "It would be valuable to explore the representation similarity between pre-trained and fine-tuned layer representations.",
            "30": "Moreover, investigating which layers are predominantly affected during the fine-tuning process would add depth to the analysis.",
            "31": "The clarity can be improved:\n* providing more explicit details concerning the methodology and experimental procedures.",
            "32": "* Figure 4 is hard to follow.",
            "33": "More details are need.",
            "34": "Several citations are missing [1], [2], [4], and [8]"
        },
        "zeuPwnI7ya": {
            "0": "It is an interesting to explore the connection and difference between LMs and human brain, and finetunes the LMs on a human brain datasets to show the improved alignment with brain responses on three phenomena.",
            "1": "So far No.",
            "2": "(Sorry, I am not an expert for this topic, though I try my best to read this paper)"
        }
    },
    "EDPxCjXzSb": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the problem of Compositional Image Retrieval (CIR) in a training-free manner, which is a significant advancement over existing methods that require extensive training on large datasets.",
            "1": "- The proposed CIReVL method leverages off-the-shelf vision-language models (VLMs) and large language models (LLMs) to achieve zero-shot CIR, which is novel and innovative.",
            "2": "- The approach demonstrates competitive and state-of-the-art performance on multiple benchmarks, highlighting its effectiveness and potential impact on the field.",
            "3": "Potential reasons for acceptance\n   - The method achieves state-of-the-art performance on several CIR benchmarks without requiring additional training, which is a significant contribution.",
            "4": "- The modularity and scalability of the approach allow for easy integration and improvement with different models, making it highly adaptable.",
            "5": "- The paper provides a thorough evaluation and ablation studies, demonstrating the robustness and versatility of the proposed method.",
            "6": "- The interpretability and human-understandability of the method enable user intervention, which is a valuable feature for practical applications.",
            "7": "Potential reasons for rejection\n   - **Limited novelty in individual components:**\n     - The method primarily combines existing pre-trained models rather than introducing new architectures or training techniques.",
            "8": "- The novelty lies in the integration and application rather than in the development of new models or algorithms.",
            "9": "- **Dependence on pre-trained models:**\n     - The performance heavily relies on the quality and capabilities of the pre-trained VLMs and LLMs used.",
            "10": "- If the pre-trained models are not available or are of lower quality, the method's performance may degrade significantly.",
            "11": "- **Scalability concerns:**\n     - While the method is scalable, the use of large models like GPT-4 may not be feasible for all users due to computational and resource constraints.",
            "12": "- The paper does not address the potential limitations and challenges of scaling the method in resource-constrained environments.",
            "13": "Suggestions for improvement\n   - **Highlighting the novelty:**\n     - Emphasize the unique aspects of the integration and the specific contributions of the proposed method in the context of existing work.",
            "14": "- Provide a clearer comparison with other zero-shot CIR methods to highlight the distinct advantages of CIReVL.",
            "15": "- **Addressing scalability and resource constraints:**\n     - Discuss potential strategies for optimizing the method for resource-constrained environments, such as using smaller models or efficient inference techniques.",
            "16": "- Provide insights into the trade-offs between model size, performance, and computational requirements.",
            "17": "- **Expanding the evaluation:**\n     - Include additional benchmarks and datasets to further validate the generalizability and robustness of the method.",
            "18": "- Provide more qualitative examples and case studies to illustrate the practical applications and benefits of the method in real-world scenarios.",
            "19": "- **User intervention and interpretability:**\n     - Explore and discuss more detailed use cases and scenarios where user intervention can significantly improve retrieval results.",
            "20": "- Provide guidelines or tools for users to effectively intervene and modify the retrieval process, enhancing the practical usability of the method."
        },
        "1jh3IV7b9O": {
            "0": "The proposed framework is training-free and achieves promising retrieval results.",
            "1": "The retrieval accuracy could be improved by adopting better off-the-shelf VLM and LLM, due to the modularity of the framework.",
            "2": "Since the query is mainly processed in language domain, the framework is human-interpretable and could also improve the retrieval accuracy by involving human intervention.",
            "3": "The method itself is very intuitive and not quite novel, which is more like an engineering extension of existing models like VLM and LLM.",
            "4": "Additionally, the proposed method does not achieve better results on all evaluation datasets.",
            "5": "In Table 1, CIReVL underperforms SEARLE on CIRR dataset in terms of CIRR, which shows the unstability.",
            "6": "This work only adopts datasets of everyday life and natural scenes (CIRR, CIRCO, GeneCIS) as evaluation.",
            "7": "It is necessary to include datasets of various domains, such as fashion domain datasets like fashioniq and fashion200k to evaluate the generalization ability.",
            "8": "Furthermore, for GeneCIS dataset, the comparison against other ZSCIR methods such as Pic2word and SEARLE should be included for complete comparison.",
            "9": "The proposed framework is not very time-efficient since for each query, there will be two auto-regressive processes: query image captioning and target caption generation, which are very time-consuming.",
            "10": "Both processes involve large models like VLM and LLM, which may further lead to low efficiency for image search.",
            "11": "Therefore, it is necessary to compare the retrieval efficiency (time) with the previous methods."
        },
        "r3DhoH5bBg": {
            "0": "The paper effectively integrates the VLM and LLM for ZS-CIR, offering a flexible and intervenable CIR system.",
            "1": "The authors have conducted thorough experiments to validate the effectiveness of the proposed method, making the evaluation rather reliable.",
            "2": "The figures in this paper are clear, effectively conveying the processing pipeline.",
            "3": "Limited contribution: The proposed method appears to be a combination of several foundation models and simply employs the basic modelling capacity of these models (e.g., BLIP-2 for image captioning, GPT for text editing and CLIP for image retrieval), presenting a naïve and straightforward solution for CIR task.",
            "4": "As a result, it is challenging to identify insightful and significant contributions to this field.",
            "5": "The authors should conduct further in-depth research to enhance their contributions.",
            "6": "Incorrect experimental results: The Recall@10 of CIReVL (ViT-G/14∗) on CIRR dataset in Table 1 is obviously incorrect.",
            "7": "Inconsistent formatting of table data: In some cases, the data is presented with two decimal places, while in others, only one decimal place is used.",
            "8": "Furthermore, there is a case with a mix of formatting in Table 3.",
            "9": "The authors may revise the paper more carefully.",
            "10": "The paper effectively combines the existing powerful VLM and LLM models.",
            "11": "However, it would be better to provide a more insightful analysis of the caption and reason processes.",
            "12": "For example, from Table 3, compared to utilizing different LLM models, the use of various state-of-the-art (SOTA) captioning models has a relatively minor impact on retrieval.",
            "13": "Are there any potential explanations for this phenomenon?",
            "14": "Does this mean the choice of caption model is not strict, as long as the model can catch the main object or attribute of images?",
            "15": "I think it would be better to provide more analysis.",
            "16": "The description about other works in Section 3.1 is a bit obscure for me who is not so familiar with the task but proficient other related tasks.",
            "17": "Missing references, such as in ‘Similar to existing ZS-CIR methods’ in the first paragraph of Section 3.2.",
            "18": "Experiments: (a) In Table 1, the author only presents the experimental results of ‘image only’ and ‘image+text’ methods for reference while missing that of the ‘text only’ method under the ViT-B/32 setting.",
            "19": "However, the authors mention that the results on CIRR benchmark are primarily dependent on the modifying instruction while the actual reference image has much less relation to the target image, which indicates that the ‘text-only’ method can be an important reference for measuring the performance of the proposed method.",
            "20": "Better to present the result of ‘text-only’ method and make a fair comparison and discussion.",
            "21": "(b) In Table 1, the authors miss the results of ‘image only’, ‘text only’ and ‘image+text’ method for ViT-L/14 and ViT-G/14 settings.",
            "22": "Better to include them for reference.",
            "23": "(c).",
            "24": "When evaluating on the GeneCIS benchmark, the authors do not specify the architecture of the vision backbone adopted in the experiment.",
            "25": "Better to specify it clearly for reference.",
            "26": "Discussions and evaluation on the potential limitations: The paper shows that the proposed method has several merits including free of training, good flexibility and scalability.",
            "27": "However, it may also have some potential limitations.",
            "28": "For example, (a) Inference costs and efficiency: The proposed method utilizes large VLMs and LLMs to conduct the image captioning, language reasoning and cross-modal retrieval during inference.",
            "29": "Will it take more computational costs and have longer inference time compared with the previous methods?",
            "30": "(b) Limitations of each module: Since the proposed method is composed of three different modules, the effect of each module plays an important role on the final retrieval results.",
            "31": "For example, if the image caption module generates partial or false descriptions for the given images, the reasoning and retrieval process will be misled.",
            "32": "Thus, it is necessary to analyze the potential negative impact brought by each module in an in-depth manner and quantify them if possible (which factor contributes more to the failure cases).",
            "33": "(c) Compatibility of different modules: Since the proposed method conducts cross-modal retrieval by cascading three separate modules, the compatibility of different modules seems to be an important factor.",
            "34": "For example, if the captions generated by LLMs have different styles with pretraining data of the VLMs, the VLMs used for cross-modal retrieval may produce some bias, which may hinder the final performance.",
            "35": "Overall, it will be appreciated if the authors can present more in-depth discussions and evaluation on the potential limitations to fully demonstrate the properties of the proposed method.",
            "36": "Writing: Some parts of the paper don’t flow well.",
            "37": "The overall writing requires further improvement."
        },
        "kKiMeQX6wj": {
            "0": "While there are a lot of works on compositional image retrieval, previous works typically require training several components like textual inversion and lack human interpretability.",
            "1": "The method proposed in this paper avoids all these problems by simply composing image and textual query in the language domain.",
            "2": "On the surface this is ingenious because you do it in a modular way and each module is a highly-generalisable large-scale pre-trained model.",
            "3": "For example, when training a textual inversion, we do not really guarantee it scales to open-set setups (given we train them in limited data and compute).",
            "4": "We just use a small-scale network and hope the rest of powerful models take care of it.",
            "5": "The proposed method works zero-shot and you know it works for open-set setups.",
            "6": "Additionally, unlike prior trainable compositional image retrievals, there is a lot of effort that goes into interpretability -- \"how was the image and query text was composed\".",
            "7": "Since the proposed method does this composition entirely in the language space, you know exactly what information was extracted from the image and you can see the generated caption from LLM for the desired target image.",
            "8": "This not only makes the retrieval process highly transparent, but also allows post-hoc edits -- you can literally make changes to adjust your retrieval results.",
            "9": "I encourage more works that reuses as much as large-scale models and combines them as modules with minimal or training-free way.",
            "10": "Despite its appeal, there are a few important drawbacks.",
            "11": "This entire process sticks on the underlying assumption that our image captioning module (e.g., BLIP-2 or CoCa) can provide a \"detailed caption\" that captures all information.",
            "12": "This, in my opinion, is a strong assumption.",
            "13": "A lot depends on your captioning module.",
            "14": "While the captioning module may be super accurate, it can miss some \"less important\" details that I want to change.",
            "15": "For example, given a photo, the sky was orange and my textual query is \"make the colour of the sky darker\".",
            "16": "If the image captioning module omits the colour of sky (i.e., orange) and focuses on the foreground (e.g., a person holding a flower, sitting in a bench near a park where kids are playing) -- the rest of the modules have no way of combining my textual query \"make the colour of sky darker\".",
            "17": "This is an example, where the entire pipeline fails -- due to no fault of the image captioning module.",
            "18": "It provided a detailed image description, but it is not possible to describe every little \"unimportant things\".",
            "19": "Looking at the same problem from a different direction, is the information bandwidth of an image the same as language?",
            "20": "(an image can be worth a thousand words)"
        },
        "xHnx7C2jfO": {
            "0": "- The paper is technically sound and widely applicable not only for text-image use case, but also text-video and probably other problem domains.",
            "1": "- The paper investigates three different datasets against multiple baselines and various additional ablations make the paper and results more appealing.",
            "2": "Limitations of the proposed method also gets discussed with examples.",
            "3": "- The paper is easy to follow / well-written.",
            "4": "- [Novelty & Literature review] There are a wider number of papers in the field since last year and some of those papers requires mention (at least the ones before the ICLR submission deadline).",
            "5": "The most similar paper ((https://arxiv.org/pdf/2310.05473.pdf) proposes the same approach but with an additional training method for merging the automatically generated image caption and the desired modification's text description.",
            "6": "This paper contains additional papers/baselines to cite/include.",
            "7": "It would be great if there is a way to understand whether the proposed training-free method would perform better or worse compared to the paper mentioned above.",
            "8": "- [Experiments] Baseline PALAVRA attack different set of problems, uses different datasets and metrics.",
            "9": "It does not mention CIR task in the text.",
            "10": "Not clear if PALAVRA serves as a baseline."
        }
    },
    "22pyNMuIoa": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant challenge in the field of prompt engineering for large language models (LLMs), specifically the automation of expert-level prompt generation.",
            "1": "- The novelty lies in the introduction of PromptAgent, which uses a strategic planning approach, specifically Monte Carlo Tree Search (MCTS), to navigate the complex space of expert-level prompts.",
            "2": "- The approach combines human-like trial-and-error exploration with machine efficiency, leveraging self-reflection capabilities of LLMs to generate insightful error feedback and refine prompts iteratively.",
            "3": "Potential reasons for acceptance\n   - The proposed method, PromptAgent, demonstrates significant improvements over existing prompt optimization methods, as evidenced by the experimental results across 12 diverse tasks.",
            "4": "- The use of MCTS for strategic planning in prompt optimization is a novel contribution that effectively balances exploration and exploitation in the prompt space.",
            "5": "- The paper provides extensive qualitative and quantitative analyses, showcasing the efficiency, generalizability, and superior performance of the optimized prompts.",
            "6": "- The framework's ability to integrate domain-specific knowledge and generate expert-level prompts autonomously is a valuable advancement in the field.",
            "7": "Potential reasons for rejection\n   - **Complexity and Scalability**\n     - The MCTS-based approach, while effective, may be computationally intensive and complex to implement, potentially limiting its scalability to very large datasets or more complex tasks.",
            "8": "- The paper does not provide a detailed analysis of the computational resources required for the proposed method, which could be a concern for practical deployment.",
            "9": "- **Generalizability to Other Domains**\n     - While the method shows strong performance in the tested domains, its applicability to highly specialized or less common domains where domain knowledge is sparse or protected is not thoroughly explored.",
            "10": "- The reliance on the capabilities of the optimizer LLM (e.g., GPT-4) may limit the method's effectiveness in domains where the LLM's pre-trained knowledge is insufficient.",
            "11": "- **Comparison with Baselines**\n     - The paper compares PromptAgent with several baselines, but it would benefit from a more comprehensive comparison with a wider range of state-of-the-art prompt optimization methods.",
            "12": "- The selection of baselines and the specific tasks chosen for evaluation may not fully represent the diversity of challenges in prompt optimization, potentially biasing the results.",
            "13": "Suggestions for improvement\n   - **Detailed Computational Analysis**\n     - Provide a detailed analysis of the computational resources required for the MCTS-based approach, including time complexity, memory usage, and scalability considerations.",
            "14": "- Discuss potential optimizations or approximations that could reduce the computational burden without significantly compromising performance.",
            "15": "- **Broader Domain Evaluation**\n     - Extend the evaluation to include more diverse and specialized domains, particularly those with limited or protected data, to better assess the generalizability of the method.",
            "16": "- Consider incorporating domain-specific LLMs or hybrid models that combine general and specialized knowledge to enhance performance in niche areas.",
            "17": "- **Enhanced Baseline Comparisons**\n     - Include a broader range of state-of-the-art prompt optimization methods in the comparison to provide a more comprehensive evaluation of PromptAgent's performance.",
            "18": "- Ensure that the selected tasks for evaluation cover a wide spectrum of prompt engineering challenges, including those that may not be well-represented in the current set of tasks.",
            "19": "- **User-Friendly Implementation**\n     - Provide a user-friendly implementation of PromptAgent, including detailed documentation and examples, to facilitate adoption by the research community and practitioners.",
            "20": "- Consider developing a web-based interface or tool that allows users to easily apply PromptAgent to their own tasks and datasets."
        },
        "jNNdTLgmRG": {
            "0": "- The paper is well written and easy to follow.",
            "1": "- Related literature is covered.",
            "2": "- Authors have conducted comprehensive experiments to evaluate the performance of their proposed method against a set of alternative methods and ablation studies shed further lights on the effectiveness of the proposed strategic exploration of the search space using MCTS.",
            "3": "- Appendix covers useful details about the hyper parameters and the evaluation data sets and details of the studied methods.",
            "4": "- **Novelty** of the proposed method is arguable.",
            "5": "Use of MCTS in prompt engineering is not novel.",
            "6": "However, the way authors have incorporated a base and an optimizer model in their implementation and the prompts used to incorporate the error feedback into the existing state (+ the empirical studies) can be considered as the main contributions of this work.",
            "7": "- **Contribution**: Based on the details provided in the Appendix section, it can be seen that the amount of details covered in prompts generated by APE is not comparable with that of the proposed method.",
            "8": "The prompts generated by the proposed method are significantly lengthier and cover more details compared which is hard to justify.",
            "9": "APE supposedly uses Monte Carlo Search to iteratively propose and select prompts, therefore, it is expected to see more details getting added to the original prompt over each iteration which is not reflected in the samples that I see in the Appendix section.",
            "10": "**This makes me conclude that the additional gain from the proposed method by authors come from the way they instruct the optimizer method to incorporate the feedback into the existing state rather than utilization of MCTS** which is the main claim of the paper.",
            "11": "However, \n- Clarity [minor]: It's ok that authors have covered the details of the selection, expansion, simulation, and back-propagation in MCTS and it can be very useful for general audience with less context, however, I was hoping to read more details about how authors have implemented the expansion stage.",
            "12": "Appendexi briefly touches base under \"Implementation details\" sub-section and mentions \"We sample 3 batches, and for each batch, we generate multiple new prompts\".",
            "13": "It would be good if authors can further explain how they generate new prompts for each batch."
        },
        "x1jyknFng7": {
            "0": "The idea of this work is clear and easy to follow.",
            "1": "The writing is in general clear.",
            "2": "This idea can be useful from the engineering/ deployment side.",
            "3": "Technical contribution is limited.",
            "4": "Comparing the performance of an average user with LLM in prompting is somewhat unfair.",
            "5": "Also, even human experts will be posited under an unfair setting where LLMs can do multiple-round prompting.",
            "6": "Some of the experiment settings are suspicious to be unfair (please see questions below)"
        },
        "13zVQ1rDdF": {
            "0": "This paper is well-written and easy to follow.",
            "1": "The method seems clean, straightforward, and promising.",
            "2": "There are several clarity issues in the experimental section regarding human prompts and reward functions.",
            "3": "See questions below."
        },
        "fWoXSxqtki": {
            "0": "- The proposed PromptAgent leveraged a Monte Carlo Tree Search framework to utilize errors and feedback identified by LLMs for the iterative refinement of prompts.",
            "1": "This approach is theoretically sound and can enhance navigation through the expansive search space of potential prompts.",
            "2": "- PromptAgent showed promising experimental results across 12 tasks, and the optimized prompt can be generalized to different LLMs.",
            "3": "- PromptAgent showed better performance and exploration efficiency than other prompt optimization methods, including Automatic Prompt Engineer (APE).",
            "4": "- PromptAgent relies on a key hypothesis: the optimizer LLM (GPT-4 in this study) possesses adequate domain knowledge to identify the errors in the response from the base LLM and give meaningful feedback.",
            "5": "However, this may not be a valid hypothesis, especially in some specialized areas such as medicine [1,2], where the data is relatively sparse due to strict data protection regularization like HIPAA.",
            "6": "- In order to refine the prompt, PromptAgent needs to concatenate the \"error_string\", \"error_summarization and \"trajectory_prompts\" as one input.",
            "7": "Challenges may arise in tasks demanding the interpretation of extensive contexts, such as the analysis of detailed medical documents, where the \"state_transit\" could become prohibitively large due to the number of training examples and the depth of the Monte Carlo Tree Search, potentially diminishing the LLMs' performance.",
            "8": "Reference\n\n[1] Bhayana, R., Krishna, S., & Bleakney, R. R. (2023).",
            "9": "Performance of ChatGPT on a radiology board-style examination: Insights into current strengths and limitations.",
            "10": "Radiology, 230582.",
            "11": "[2] Azizi, Z., Alipour, P., Gomez, S., Broadwin, C., Islam, S., Sarraju, A., ... & Rodriguez, F. (2023).",
            "12": "Evaluating Recommendations About Atrial Fibrillation for Patients and Clinicians Obtained From Chat-Based Artificial Intelligence Algorithms.",
            "13": "Circulation: Arrhythmia and Electrophysiology, e012015."
        }
    },
    "gisAooH2TG": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical challenge in robotics: enabling robots to adapt and replan in real-time for long-horizon tasks using a combination of Large Language Models (LLMs) and Vision Language Models (VLMs).",
            "1": "- The proposed REPLAN framework is novel in its integration of high-level planning, low-level reward generation, and real-time feedback from VLMs to handle unforeseen obstacles and adapt plans dynamically.",
            "2": "- The approach leverages recent advancements in LLMs and VLMs, pushing the boundaries of autonomous robotic systems by reducing the need for human intervention in complex, multi-stage tasks.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-structured and innovative framework that combines LLMs and VLMs for robotic replanning, which is a significant advancement in the field.",
            "4": "- The experimental results demonstrate the effectiveness of the REPLAN framework, showing a substantial improvement over existing methods in completing long-horizon tasks.",
            "5": "- The integration of a verifier to ensure the correctness of plans and the use of a perceiver for real-time feedback are well-justified and contribute to the robustness of the system.",
            "6": "- The paper is well-written, with clear explanations of the methodology, experiments, and results, making it accessible to a broad audience in the robotics and AI communities.",
            "7": "Potential reasons for rejection\n   - **Limited generalization to diverse environments:**\n     - The experiments are conducted in a limited set of environments, which may not fully represent the diversity of real-world scenarios.",
            "8": "- The framework's performance in more complex and dynamic environments is not thoroughly evaluated, raising questions about its scalability and robustness.",
            "9": "- **Dependence on VLM accuracy:**\n     - The framework heavily relies on the accuracy of VLMs for object recognition and state interpretation, which can be a limitation if the VLMs fail to provide accurate feedback.",
            "10": "- The paper does not provide a detailed analysis of the failure cases related to VLM inaccuracies and how they impact the overall system performance.",
            "11": "- **Lack of comparison with more diverse baselines:**\n     - The comparison is primarily with the Language to Rewards framework, and while some ablations are performed, the paper could benefit from a broader comparison with other state-of-the-art methods in robotic planning and control.",
            "12": "- Including more diverse baselines would strengthen the validation of the proposed approach and highlight its advantages more comprehensively.",
            "13": "Suggestions for improvement\n   - **Expand the experimental evaluation:**\n     - Test the REPLAN framework in a wider variety of environments, including more dynamic and unpredictable scenarios, to better demonstrate its generalization capabilities.",
            "14": "- Include additional metrics to evaluate the robustness and adaptability of the system in different conditions.",
            "15": "- **Detailed analysis of VLM performance:**\n     - Provide a more in-depth analysis of the VLM's performance, including common failure modes and their impact on the overall system.",
            "16": "- Discuss potential strategies to mitigate the limitations of VLMs, such as incorporating multiple VLMs or using additional sensory data.",
            "17": "- **Broaden the baseline comparisons:**\n     - Compare the REPLAN framework with a more diverse set of state-of-the-art methods in robotic planning and control to provide a more comprehensive evaluation.",
            "18": "- Highlight the specific advantages and limitations of REPLAN relative to these methods, providing a clearer picture of its contributions to the field.",
            "19": "- **Enhance the discussion on limitations and future work:**\n     - Provide a more detailed discussion on the limitations of the current approach and potential directions for future research.",
            "20": "- Explore the integration of additional components, such as advanced perception systems or more sophisticated planning algorithms, to further enhance the framework's capabilities."
        },
        "gJYXqJnk1T": {
            "0": "The core idea of using a VLM to generate textual feedback for a model makes sense and is a good addition to this research direction in robot learning.",
            "1": "The videos are very helpful for showcasing the resulting method.",
            "2": "Although the high level idea of the paper is reasonable, it is both not evaluated as extensively as would be helpful and is not put in context of recent literature very well.",
            "3": "As a minor note: please fix the references to surround the authors of citations in parentheses.",
            "4": "This would make the paper much easier to read.",
            "5": "On the experimental front:\n\n* the exact prompts used do not appear in the paper or appendix, which makes it hard to judge how the LLM was told to incorporate past feedback\n* The evaluation setting is 4 tasks, done for 3 trials each.",
            "6": "This is an incredibly small number of tasks and trials for the method.",
            "7": "As a point of comparison: the Language to Rewards paper used as a baseline was tested on 17 tasks with 10 generated rewards functions per task, run through MPC 50 times each.",
            "8": "My understanding is that in Table 1, RePLan without replanning is almost identical to Language to Rewards, but is only successful 1/3 times in the easiest Task 1 setting, compared to 3/3 from Language to Rewards.",
            "9": "To me this seems like it is just because of random noise, but if that's the case, why should we trust any of the other numbers in the table?",
            "10": "This is the point of issue I find most important about the paper.",
            "11": "On the prior literature front:\n\nThere are number of prior works based on providing LLM feedback from the environment, either ground truth or from VLMs.",
            "12": "Examples are Voyager by Wang et al, Inner Monologue by Huang et al, and Towards a Unified Agent with Foundation Models by Di Palo et al.",
            "13": "I would appreciate some discussion about such lines of work, since to me it is not so clear if this is doing anything very different from these works.",
            "14": "I believe at most you can argue that this paper is using MPC instead of RL or imitation learning, but otherwise prior work has used chain-of-thought style prompting to decompose high-level language to low level language, generate rewards from said low level language, provide feedback via VLMs, etc.",
            "15": "That is not to say that the combination of prior work cannot be novel, but in this instance, it does not feel like much is coming from said combination.",
            "16": "Especially given the weakness of the experimental results.",
            "17": "Edit: some more experimental results were provided and I have adjusted score from 3 -> 5."
        },
        "9G66vyiBLD": {
            "0": "- The presented idea is clear and well-motivated — leveraging LLMs in a hierarchical framework for both high-level task planning and low-level motion planning.",
            "1": "Compared to prior work, “Language to Reward”, it is clear that such hierarchical approach is needed for long-horizon tasks and can also offer additional robustness as the system can replan its high-level action.",
            "2": "- The literature review is also thorough, covering many recent works in this domain.",
            "3": "However, this part can be improved because it is now more like a laundry list instead of putting the work in the context of prior works.",
            "4": "- Currently the biggest limitation seems to be the lack of thorough experiments, which can use some improvement along two axes.",
            "5": "One is the breadth of the tasks: there are only four tasks investigated in this work while there are also quite some similarities between them.",
            "6": "An important advantage of using LLMs is that it is possible to apply to a wider set of tasks more easily.",
            "7": "The other axis is the quantitative evaluation: currrently only 3 runs are performed for each entry in Table 1, which makes the quantitative results not very convincing as it is also pointed out in the paper that there is “high variance of completion”.",
            "8": "In addition, the paper does not compare to prior methods that are not based on LLMs, e.g., task and motion planning methods or hierarchical RL methods.",
            "9": "- Another limitation lies in the use of the simulator ground-truth for MPC.",
            "10": "This raises the question whether the approach can be applicable to real-world settings.",
            "11": "However, the high-level planning part does use VLM for grounding image observations, but it’s unclear how this can be achieved for the attributes referred in MPC, e.g.",
            "12": "“block_r_side”, “cabinet_handle”.",
            "13": "- Currently the intro reads more like related works, where it may be confusing to readers what the actual motivation of the work is.",
            "14": "More care can be taken to improve the intro while appropriately contextualizing the work.",
            "15": "- The citation format in many places are currently incorrect — parenthesis often should be used."
        },
        "xr2EagjKkn": {
            "0": "- The paper is well-written and easy to follow\n- The motivation of the paper is topical since feedback and adaptive replanning is important for foundation models which may hallucinate or require grounding in physical interactions\n- The method does not require additional human input compared to the baseline method Language2Reward (just one human input at the beggining, the rest of the replanning and execution is autonomously completed by the foundation model submodules) - The VLM Perceiver is one of the most critical parts of the method, but it is not sufficiently explained.",
            "1": "Due to the lack of details, I can only assume how it is utilized based on Algorithm 1, in which case I have some major concerns.",
            "2": "Since the VLM is the bottleneck for providing feedback for grounding LLM plans and rewards for future LLM iterations.",
            "3": "However, details are not shared about how the Perceiver is used, even though it is mentioned that \"The High-Level Planner [is used] to decide what it wants to query from the Perceiver\".",
            "4": "From Algorithm 1, the VLM needs to be used for two use cases: #1 scene state generation `VLM(image_observation)` and #2 `VLM(image_observation, motion_error, language_instruction)`.",
            "5": "However, these seem to be quite challenging tasks to naively query for off-the-shelf VLMs.",
            "6": "While modern VLMs are fairly robust at narrow vision task domains like object detection or image captioning for internet images, more extended reasoning (such as failure explanation with multiple input contexts) or domain-specific understanding (like robotics reasoning from vision) is still an open problem.",
            "7": "- The evaluation complexity is very limited and does not justify the claims of a \"large-scale and long-horizon kitchen environment\": it is only in simulation, with relatively high-level and short-horizon tasks.",
            "8": "There are two issues: 1) the granularity of intermediate primitives (\"pick up the block\") is coarse, 2) the horizon length is short, going only up to 4 subtasks required.",
            "9": "Previous works in BEHAVIOR-1K, ALFRED, SayCan have studied robotic reasoning with much longer horizons with similar granularity of intermediate primitives.",
            "10": "- The evaluation has very few trials (3 seeds only), so it is hard to draw confident conclusions about the method's quantitative performance.",
            "11": "- The core claim of the work should clarified.",
            "12": "If the contribution is the incorporation of LLM and VLM feedback into high level planning, it needs to be compared/discussed against prior works that ground robot planning with additional foundation model feedback.",
            "13": "Specifically, [1] incorporates LLM and VLM feedback for closed-loop environment feedback for a High-Level LLM Planner for robot subtasks.",
            "14": "If the contribution is the integration of LLM and VLM feedback into code generation, it needs to compared/discussed with other replanning works from code generation [2] or LLM tool use.",
            "15": "If the core contribution is the verifier, this needs to be stated more clearly and more details about this module should be provided.",
            "16": "If the contribution is the admittedly impressive integration of existing modules, this should be clarified and more details about the bottlenecks of the system (VLM Perceiver, Verification) should be shared in the main text.",
            "17": "[1] \"Inner Monologue: Embodied Reasoning through Planning with Language Models\", Huang et al.",
            "18": "2022\n[2] \"Improving Code Generation by Training with Natural Language Feedback\", Chen et al.",
            "19": "2023"
        },
        "jbwFssE1h5": {
            "0": "The overall proposed method seems to be interesting and probably could work for a large variety of planning tasks.",
            "1": "The tasks are a bit more complex then simple pick and place and illustrate more complex reasoning.",
            "2": "==================\nI am increasing the score to 6 after rebuttal discussions.",
            "3": "1. the authors need to provide more environments or tasks to show the robustness of their method.",
            "4": "The authors can use LLMs to generate tasks which are long-horizon to come up with more varieties of task so that the proposed method could be more thoroughly tested.",
            "5": "I think this remains to be verified."
        }
    },
    "aN4Jf6Cx69": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the mechanistic basis of in-context learning (ICL) in transformer models, which is a significant and timely topic given the increasing importance of transformers in various AI applications.",
            "1": "- The study provides a detailed analysis of the factors influencing the trade-off between in-context learning and in-weights learning (IWL), contributing to a deeper understanding of how transformer models learn from data.",
            "2": "- The introduction of a minimal attention-only network to recapitulate the results observed in more complex models is a novel approach that simplifies the analysis and highlights the core mechanisms of ICL.",
            "3": "Potential reasons for acceptance\n   - The paper provides a thorough and well-structured analysis of the factors influencing ICL and IWL, supported by extensive experiments and clear visualizations.",
            "4": "- The introduction of a minimal model to study ICL is a valuable contribution that can serve as a foundation for future research in this area.",
            "5": "- The identification of progress measures and the construction of a two-parameter model of an induction head offer new insights into the dynamics of ICL, which could have implications for the design and training of transformer models.",
            "6": "- The paper's findings on the role of data distributional properties and the abrupt transitions in ICL are relevant and could inform the development of more efficient and effective training strategies for large language models (LLMs).",
            "7": "Potential reasons for rejection\n   - **Complexity and clarity of the model:**\n     - The paper introduces several new concepts and models, which might be challenging for readers to follow without a strong background in the field.",
            "8": "- The detailed mathematical formulations and the introduction of multiple progress measures could be overwhelming and may require further simplification or clearer explanations.",
            "9": "- **Generality of the findings:**\n     - The study is based on a minimal attention-only network and a simplified dataset, which may limit the generalizability of the findings to more complex and real-world scenarios.",
            "10": "- The paper does not provide empirical evidence that the identified mechanisms and models apply to larger and more complex transformer models used in practice.",
            "11": "- **Experimental validation:**\n     - While the paper presents extensive experiments, it lacks validation on a broader range of datasets and transformer architectures to demonstrate the robustness and applicability of the findings.",
            "12": "- The study could benefit from additional experiments comparing the minimal model's performance and dynamics with those of state-of-the-art transformer models on real-world tasks.",
            "13": "Suggestions for improvement\n   - **Simplify and clarify the presentation:**\n     - Consider simplifying the mathematical formulations and providing more intuitive explanations of the key concepts and models introduced in the paper.",
            "14": "- Use more visual aids and diagrams to illustrate the mechanisms and dynamics of ICL and IWL, making the content more accessible to a broader audience.",
            "15": "- **Expand experimental validation:**\n     - Conduct additional experiments on a wider range of datasets and transformer architectures to validate the generality and robustness of the findings.",
            "16": "- Compare the performance and dynamics of the minimal model with those of state-of-the-art transformer models on real-world tasks to demonstrate the practical relevance of the study.",
            "17": "- **Discuss implications and future work:**\n     - Provide a more detailed discussion of the implications of the findings for the design and training of transformer models, particularly in the context of LLMs.",
            "18": "- Suggest potential directions for future research, such as exploring the role of curricula in accelerating ICL and investigating other mechanisms that might contribute to ICL in larger models.",
            "19": "- **Address limitations:**\n     - Acknowledge the limitations of the study, particularly the use of a minimal model and simplified dataset, and discuss how these limitations might impact the generalizability of the findings.",
            "20": "- Propose ways to address these limitations in future work, such as incorporating more complex datasets and models into the analysis."
        },
        "2ZCcyMAYL9": {
            "0": "This paper tackles a timely and interesting topic, and contains several insights and useful contributions.",
            "1": "-- The synthetic task family introduced is a clean and intuitive way of exposing dependencies of learning strategies on pretraining data distributions.",
            "2": "Showing that key phenomena identified in (Chan et al., 2022) can be replicated in this setting is a useful contribution.",
            "3": "-- The characterization of the initial slow learning phase as driven by an increase in context-label accuracy is interesting\n\n-- The idea that a strategy that results in good context-label accuracy can facilitate (despite not being necessary for) learning of a true ICL strategy is very interesting, and some evidence is provided for this idea\n\n-- The evidence provided that the emergence of induction heads is strongly linked with the development of the ICL strategy, while not entirely novel, is nice to see I have the following concerns about this paper.",
            "4": "Many of them involve claims that I feel are made too strongly in the paper relative to the level of evidence provided.",
            "5": "-- The paper illustrates a set of phenomena in figure 2, and promises a mechanistic understanding of these phenomena.",
            "6": "But the mechanistic analysis provided later in the paper does not speak to most of the phenomenology -- for instance, the dependence of the ICL/IWL tradeoff on B, epsilon, K, and alpha.",
            "7": "In fact, the mechanistic analysis focuses on the p_C > 0 case, which is different from the p_c = 0 regime that gives rise to all the tradeoffs observed in Figure 2.",
            "8": "Thus, the connection between pages 1-4 of the paper and the rest is not entirely clear.",
            "9": "-- -- The following sentence, while intuitively reasonable, is written as a key strong claim and as far as I can tell is not really justified with evidence: \"Therefore, the relative rates at which the network acquires ICL and IWL control the fraction of loss explained by each mechanism after convergence.\"",
            "10": "-- The paper makes strong causal claims based only on correlational evidence.",
            "11": "For instance, \"Induction head formation drives the abrupt transition during ICL.\"",
            "12": "As far as I can tell no evidence is given for this claim, other than the (very suggestive, I agree!)",
            "13": "fact that they coincide in time.",
            "14": "-- The paper makes strong claims about the three-parameter model proving or ruling out certain hypotheses.",
            "15": "An example is the sentence \"This rules out hypothesis V as only the factors corresponding to the progress measures (a) through (d) have been included in the minimal model.\"",
            "16": "In my opinion, such claims are much too strong.",
            "17": "The three-parameter model is ultimately a different model from the original transformer architecture being used!",
            "18": "While the analysis of its behavior is suggestive of the learning strategies used by the original architecture, it is not conclusive.",
            "19": "The strength of the claims should be adjusted accordingly."
        },
        "VokzE19go0": {
            "0": "The paper is excellently written and fairly easy to follow despite the depth of insights proven.",
            "1": "The scientific investigation is very well conducted : of note is that it alternates particularly well between empirical elements, formulating subsequent hypotheses (section \"Induction head formation drives the abrupt transition during ICL\"), disproving some of those, and finally introducing a theory that accounts for those findings, replicating empirical stylized facts, whilst much simplifying the problem.",
            "2": "In particular, the phenomenological model Equation 10 (and its illustration Figure 7) is a standout novel contribution, and clearly worthy of publication, in our view.",
            "3": "In a sense, the paper is tantalizing, as it invites further work, for instance on the interplay of overlap difference \\xi and data Zipfianity parameter \\alpha."
        },
        "oxmXlHjg2U": {
            "0": "I have read several papers recently that try to provide some insights about the emergent capabilities of LLMs -- through abstract modeling and experiments with tasks such as linear regression (learned through ICL).",
            "1": "This paper is the best I have read so far in that direction.",
            "2": "The experiments are wisely designed, allowing us to understand the complex tradeoff between IWL and ICL -- as well the effect of some key data distributional parameters such as the rank-frequency exponent.",
            "3": "The simple model proposed in the second part of the paper is also intriguing, explaining how the abstract model of an induction head can explain mechanistically the ICL capabilities of an attention-based network.",
            "4": "The paper can be improved in terms of writing/presentation.",
            "5": "For example, you can explain early on in the paper what \"induction head\" means for readers that are less familiar with this area.",
            "6": "There are also several other parts of the paper in which the writing can be improved -- mostly by writing simpler/shorter/more clear sentences."
        },
        "zbDUCdPi5c": {
            "0": "The ability to use information in context to respond appropriately to later queries (called 'in context learning' or ICL), is central to the capabilities of AI systems like ChatGPT.",
            "1": "ICL was enabled by the attention mechanism in transformer-based neural networks.",
            "2": "ICL is exemplified by the simple item-label association task (introduced by others) that the authors have selected for the focus of their analysis.",
            "3": "By shedding light on how this task is solved (building on an earlier paper taking initial steps in this direction) the current paper deepens our understanding of this core property of today's performant AI systems.",
            "4": "The authors have created new variants of the task that further simplify it, and have introduced minimally-sufficient transformer architecture containing two attention layers, which together implement what they call an 'induction head', arguably the core emergent computational structure enabling ICL.",
            "5": "They have provided a insightful analysis of the (ultimately simple, but nevertheless important) computations performed by the network that allow the effective use of information in context in their task.",
            "6": "They gone on to attempt to understand how this attention head computation emerges as the network learns to solve the item-label association task.",
            "7": "They identify progress measures in both the network's input-output performance and of its attention head computations and establish clear alignments of several of these measures.",
            "8": "I consider these measures and their alignments enlightening contributions and consider them to be strengths of the paper.",
            "9": "They go on to further support their analysis by developing a three-parameter reduction of the induction head, and show that the learning dynamics of this reduction is sufficient to reproduce many of the features of the learning dynamics of their complete neural network; they then use the reduction to test hypotheses about the relationships between the progress measures, showing that a further reduction that eliminates one of the progress measures makes learning success initialization dependent.",
            "10": "Finally, they make an even further reduction in the form of a 2- or 3- parameter 'phenomenological model' whose loss landscape can be fully characterized.",
            "11": "The parameters now directly reflect the efficacies of the two attention layers making up the induction head and of their mapping to the correct label, and allow the loss landscape of each of the variants to be visualized.",
            "12": "This phenomenological model provides an abstract characterization of the emergent learning dynamics of the 2- and 3 parameter reduction models that allows a full explanation of why these models learn reliably under the condition that the number of possible labels is greater than the number of item-label pairs in context, and fails to learn reliably when the number of possible labels is equal to the number of item-label pairs in context.",
            "13": "All the paper provides us with important clues toward understanding the computations performed by transformers and of the processes that give rise to their learning dynamics.",
            "14": "Along the way the paper provides an approach to analysis of neural network learning dynamics that others could adopt to understand the learning dynamics as they arise in other setting, another valuable contribution to the effort to understand the complex computations performed by neural networks.",
            "15": "Although I consider the analysis presented a tour de force, possessing all the strengths describe above, it is not perfectly clear that the analysis of the 2-3 parameter reduction would carry over to the full 2-attention-head network of Figure 1c.",
            "16": "A hunch I have is that the L=N case might not be quite as susceptible to failure in the full network because the full network might have a more complex loss landscape with a lower likelihood of being initialized in a place in that landscape that doesn't allow a complete solution.",
            "17": "An important and simple step toward addressing this would be to repeat the L = N simulation in the full network.",
            "18": "If the full network fails to learn in that case, it would confirm the applicability of the analysis to the full network.",
            "19": "Success would not fully invalidate the analysis, but would leave something left to explain.",
            "20": "More generally, I believe more consideration of what will happen in a larger model will be useful for the field.",
            "21": "Clearly things will not work just in the way they do in these reductions when the task is learned in a larger transformer.",
            "22": "While fuller characterization of that will be a task for future work, noting this issue as a limitation of the present effort and pointing considering how these results inform us about what is happening in LLMs will be valuable.",
            "23": "There are two less important weaknesses I'd like to see addressed.",
            "24": "First, I don't feel I have an intuitive understanding of why the loss landscape of the 3 parameter model does not have a saddle point at the point were all three parameters are equal to 0.",
            "25": "Perhaps an understanding of this is latent in the equations and I could work it out with a bit of effort, but to help me (and possibly others) understand, it would be useful if the authors could work out such an intuitive understanding.",
            "26": "Such an understanding could help address reasons why the behaviors of the 2- and 3-parameter reduced models might or might not be applicable to the full model.",
            "27": "Second, paper is harder to read than it should be.",
            "28": "The main deficiency of the paper was its failure to take cognizance of the difficulty of extended chains of arbitrary associative bindings requiring long-distance leaps across context.",
            "29": "It is just such binding that lie at the heart of the mechanisms the authors are investigating, but they are hard for human readers when arbitrary as they often were in this paper.",
            "30": "As examples, we are treated to terms like the former vs the latter as referring expressions, arbitrary labels (a-d) for key phenomena, random ordering of the assignments of these labels to lines in graphs, arbitrary labels for hypotheses (I-V), and the unhelpful placement of figures (esp fig 4) on pages remote from the place in the paper where they are discussed.",
            "31": "Although ultimately the conclusions are stated in (what I find myself to be) conceptual terms, there should be engagement with this conceptual structure in the referential expressions used.",
            "32": "I know space is limited, but I'm sure it is possible to do a better job.",
            "33": "As examples, H3 could be abbreviated sCLA -> ILA+TILA (slow-learned context-label attention -> Item-label attention and Target-item-label association).",
            "34": "Just let a,b,c,d and I-V go.",
            "35": "H4 and H5 should each be expressed directly, or at the very least the order of defining the symbols x and 0/ should correspond to their order of appearance in these hypotheses.",
            "36": "I am also not sure that the difficulty of the L=N case in the"
        }
    },
    "b3LNKq6tfA": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenging problem of reverse engineering HTML/CSS code from UI screenshots, which has broad applications in web development and design.",
            "1": "- The proposed vision-code transformer (ViCT) architecture, which combines a vision encoder and a language decoder, is a novel approach to this problem.",
            "2": "- The introduction of a visual critic without rendering (ViCR) to predict visual discrepancy and the development of a new evaluation metric, htmlBLEU, are significant contributions.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and effective approach to a relevant and challenging problem in web development.",
            "4": "- The proposed ViCT model demonstrates significant improvements over baseline models in terms of various performance metrics.",
            "5": "- The introduction of the ViCR for actor-critic fine-tuning and the htmlBLEU metric for evaluation are innovative and valuable contributions.",
            "6": "- The creation of synthetic datasets for training and evaluation is a practical solution to the lack of existing datasets for this task.",
            "7": "Potential reasons for rejection\n   - **Limited real-world applicability**\n     - The synthetic datasets used for training and evaluation may not fully capture the complexity and diversity of real-world web pages.",
            "8": "- The generated code is limited to static text pages and small samples, which may not be sufficient for practical applications.",
            "9": "- **Evaluation and comparison limitations**\n     - The paper does not provide a detailed comparison with other state-of-the-art methods for similar tasks, such as those used in related works like pix2code and Sketch2code.",
            "10": "- The human evaluation component, while useful, may not be comprehensive enough to fully assess the model's performance in real-world scenarios.",
            "11": "- **Technical and methodological concerns**\n     - The training process for the visual critic (ViCR) involves tuning several hyperparameters, which may introduce overhead and complexity when adapting to new datasets and tasks.",
            "12": "- The reliance on synthetic datasets and the specific design choices for the ViCT and ViCR models may limit the generalizability of the approach.",
            "13": "Suggestions for improvement\n   - **Enhance dataset diversity and complexity**\n     - Incorporate more diverse and complex real-world web pages into the training and evaluation datasets to better capture the challenges of practical applications.",
            "14": "- Consider using or creating additional datasets that adhere to front-end development best practices to improve the relevance and applicability of the generated code.",
            "15": "- **Expand evaluation and comparison**\n     - Provide a more detailed comparison with other state-of-the-art methods for similar tasks, including both quantitative and qualitative assessments.",
            "16": "- Conduct more comprehensive human evaluations, possibly involving a larger and more diverse group of participants, to better assess the model's performance in real-world scenarios.",
            "17": "- **Address technical and methodological concerns**\n     - Explore ways to reduce the reliance on hyperparameter tuning for the visual critic (ViCR) to simplify the training process and improve adaptability to new datasets and tasks.",
            "18": "- Investigate methods to extend the generated code's length and improve the identification of text snippets in the input images, allowing for more complex and realistic examples."
        },
        "lpa1zDpHk2": {
            "0": "The paper works on an interesting problem of automated reverse engineering of HTML/CSS code from UI screenshots.",
            "1": "The authors propose to formulate the task as a Reinforcement Learning problem to tackle the problem of non-differentiable web rendering.",
            "2": "- The paper is not clearly written.",
            "3": "Some sections are hard to follow (e.g.",
            "4": "Section 3.3).",
            "5": "- Some parts of the paper are inconsistent.",
            "6": "- In Section 4.1, the authors claim to test InstructBLIP[1] as a baseline, but I could not find it in the experimental results.",
            "7": "- In Section 4.1, the authors mention an experiment \"identifying the number of distinct shapes\", which is absent in the paper.",
            "8": "- The main and only datasets the authors use for evaluation are fully synthesized.",
            "9": "The UIs in the dataset only contain three types of elements, Rectangle, Ellipse and Button.",
            "10": "From the examples in Figure 3, I find them quite unrealistic and do not resemble real-world web UIs, which shadows the effectiveness and practical applicability of the model in genuine scenarios.",
            "11": "- Important details on dataset construction and algorithm design are missing (see Questions).",
            "12": "- Experiments are limited.",
            "13": "- Missing baselines, e.g.",
            "14": "Pix2Struct [2].",
            "15": "- The models are only evaluated on two synthetic datasets.",
            "16": "Can you run experiments on other datasets, such as the dataset of pix2code [3]?",
            "17": "- \"DiT-LLaMA\" is missing in Figure 3.",
            "18": "(Minor)\n- In Section 1, \n> In this paper, we take the first step towards reverse-engineering a UI screenshot, i.e., generating an HTML/CSS code that can reproduce the image.",
            "19": "There are prior works on UI-to-Code tasks, such as Pix2Struct[2] and pix2code[3], as you mentioned in Related Works.",
            "20": "Do you mean you are the first to directly generate runable UI code without any postprocessing from images?",
            "21": "- Some typos, e.g.",
            "22": "a missing period at the end of Section 2.",
            "23": "[1] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung and Steven Hoi.",
            "24": "\"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.\"",
            "25": "arXiv preprint arXiv:2305.06500.",
            "26": "2023.",
            "27": "[2] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang and Kristina Toutanova.",
            "28": "\"Pix2struct: Screenshot parsing as pretraining for visual language understanding.\"",
            "29": "International Conference on Machine Learning.",
            "30": "PMLR.",
            "31": "2023.",
            "32": "[3] Tony Beltramelli.",
            "33": "\"pix2code: Generating code from a graphical user interface screenshot.\"",
            "34": "Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems.",
            "35": "2018."
        },
        "8qFNpBSf2r": {
            "0": "The paper is original, as it presents an interesrting problem that can be solved through transformers.",
            "1": "The state of the art is not able to re-create the same results as the proposal.",
            "2": "Interesting technique for generating HTML synthetic data.",
            "3": "**Why RL?",
            "4": "** I understand that it is not possible, given the render, to propagate gradients to the tokens.",
            "5": "However, for the same reason, it is not clear from the paper why this is not a problem when optimising the RL policy.",
            "6": "The authors should better explain the passage in 3.3, as now it is very confusing to understand.",
            "7": "**Missing ablation study.",
            "8": "** The RL algorithm is given some fixed rewards.",
            "9": "How the results changes by varying them?",
            "10": "And how these values have been chosen?",
            "11": "**Confusion around the htmlBLEU** While the authors write a generic description of the metric, it would be easier for readers to read an algorithm.",
            "12": "Also, the proposed metric does not score too different results with respect to BLEU.",
            "13": "**Synthetic data might be harder to parse than real webpages.",
            "14": "** While the introduction of the RUID dataset (and its creation) are very interesting and useful, I argue if the randomness of the approach could generate many samples that are very hard to transform to code, thus impeding the improvement of performance at training time."
        },
        "C4tj81Ru62": {
            "0": "The paper is well-written and easy to follow.",
            "1": "The experimental results are good, demonstrating the effectiveness of proposed method.",
            "2": "The method is incremental in terms of scientific research value, just simply modifying the normal pattern of inserting vision encoder into language models.",
            "3": "The proposed framework is effective in tackling the UI-to-code generation, but not such a fundamental research in representation learning from my perspective."
        },
        "Wq4dZAVupc": {
            "0": "- S1.",
            "1": "The main idea of fine-tuning an image-conditioned text generation model with a reward model and reinforcement learning is very interesting.",
            "2": "Even though the concept of an image-conditioned code generation was proposed before, using foundation models (DiT and Llama) and fine-tuning the model with RL (Policy Gradient method) seems novel.",
            "3": "- S2.",
            "4": "To demonstrate the proof of concept, this paper builds a new dataset for UI to code generation, which contains about 50K pairs of UI and HTML (RUID-Large, Random UI Dataset).",
            "5": "- W1.",
            "6": "Overall architecture of the proposed method (ViCT) seems reasonable.",
            "7": "However, I am not sure that the design choice for the reward modeling and RL fine-tuning is effective.",
            "8": "The overall method is similar to Reinforcement Learning with Human Feedback (RLHF), a recent prevailing method for LLM alignment.",
            "9": "In RLHF, the reward model (RM) is usually modeled by relative feedback (preference or superiority) over a pair of inputs.",
            "10": "Also, the prevalent RL algorithm is Proximal Policy Optimization (PPO) rather than vanilla Policy Gradient (PG).",
            "11": "It would be better to provide some considerations on these design choices.",
            "12": "And, it would be much better to provide a comparison between ViCR (absolute feedback + PG) and RLHF methods (relative feedback + PPO).",
            "13": "- W2.",
            "14": "I am not sure how effectively ViCR models an intermediate reward in Eq 2.",
            "15": "According to Eq 2., \\hat{q_theta}(w_t^s), a value function for the token w_t^s is used.",
            "16": "Can the reward model (ViCR) estimate the value for an intermediate token in partially generated code?"
        }
    },
    "wTJoOqxYUk": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant challenge in code representation learning by incorporating dynamic information from program executions, which is often overlooked in existing methods.",
            "1": "- The proposed FuzzPretrain method introduces a novel approach to leverage test cases generated through fuzzing to enhance code representations, providing a new perspective on integrating static and dynamic information.",
            "2": "Potential reasons for acceptance\n   - The paper presents a well-motivated and innovative solution to a relevant problem in the field of code representation learning.",
            "3": "- Extensive experimental results demonstrate the effectiveness of the proposed method, showing significant improvements over baseline models on multiple code understanding tasks.",
            "4": "- The approach is generic and can be integrated into various existing models, making it broadly applicable and valuable to the research community.",
            "5": "Potential reasons for rejection\n   - **Clarity and readability:**\n     - The paper contains dense technical details that may be challenging for readers to follow, especially those not familiar with the specific domain.",
            "6": "- Some sections, such as the explanation of the dynamic information distillation (DID) objective, could benefit from clearer and more concise descriptions.",
            "7": "- **Evaluation and comparison:**\n     - While the paper provides extensive experimental results, it would be beneficial to include more comparisons with state-of-the-art models that utilize different approaches to dynamic information modeling.",
            "8": "- The paper could further discuss the limitations and potential drawbacks of the proposed method, such as the dependency on the quality and diversity of the generated test cases.",
            "9": "- **Generalization and applicability:**\n     - The paper primarily focuses on code representation learning for specific programming languages (C/C++, Python, Java).",
            "10": "It would be valuable to discuss the generalizability of the proposed method to other programming languages and domains.",
            "11": "- The reliance on fuzzing for test case generation may limit the applicability of the method in scenarios where fuzzing is not feasible or effective.",
            "12": "Suggestions for improvement\n   - **Enhance clarity and readability:**\n     - Simplify and clarify the descriptions of key concepts and methods, particularly the dynamic information distillation (DID) objective and the overall training process.",
            "13": "- Include more visual aids, such as diagrams and flowcharts, to help readers understand the proposed method and its components.",
            "14": "- **Expand evaluation and comparison:**\n     - Include additional comparisons with state-of-the-art models that utilize different approaches to dynamic information modeling, providing a more comprehensive evaluation of the proposed method's performance.",
            "15": "- Discuss the limitations and potential drawbacks of the proposed method in more detail, including the dependency on the quality and diversity of the generated test cases.",
            "16": "- **Address generalization and applicability:**\n     - Explore the generalizability of the proposed method to other programming languages and domains, providing insights into its broader applicability.",
            "17": "- Discuss potential strategies to mitigate the reliance on fuzzing for test case generation, such as alternative methods for obtaining dynamic information or combining multiple sources of dynamic data."
        },
        "N6baaG1zNz": {
            "0": "incorporating program executions to pre-train LLMs is new despite a narrow and low-level contribution.",
            "1": "- My biggest concern is the key idea of using program execution to learn code models is just not new.",
            "2": "Authors seem to be completely unaware of the vast literature of neural code models based on dynamic executions.",
            "3": "Here are some papers for authors' reference:\n\n  1. dynamic neural program embedding for program repair\n  2. blended precise semantic program embeddings\n  3.",
            "4": "Improving Neural Program Synthesis with Inferred Execution Traces\n  4.",
            "5": "Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages\n  5.",
            "6": "Code vectors: understanding programs through embedded abstracted symbolic traces\n\n   Even though they do not directly target LLM rightfully so given that LLMs are not even around at the time those papers are published, their works already share the insight of how dynamic execution can benefit learning of code embeddings.",
            "7": "Therefore, it's entirely inappropriate for authors to totally ignore them.",
            "8": "- The evaluation task of clone detection is poorly handled.",
            "9": "First and foremost, clone detection is almost an entirely syntactic task as tools are asked to detect the syntactic similarity of code, however, incorporating execution traces are semantic information that totally ignores the syntactic features.",
            "10": "So this clone detection task does not even match the insight of the paper.",
            "11": "Of course, I am aware of the type 4 semantic clones, however, the question is, how many are those in POJ-104, authors provide no information in this regard, and it's more than reasonable to assume there are very few if any used in the evaluation.",
            "12": "- In ablation study, Fig 3 demonstrates that in Defect DIM and DID is not necessary because removing them actually yields a bigger gain over FuzzPretrain.",
            "13": "This casts doubt on the effectiveness of your technique."
        },
        "C0xywQ15Fm": {
            "0": "- This paper proposes leveraging fuzzing for pre-training which may inspire future techniques for building better pre-training datasets and objectives for code models.",
            "1": "- The approach proposed by this paper yields impressive improvements for code search and smaller improvements across three other code understanding tasks, relative to comparable models which use only static information.",
            "2": "- The pre-training tasks that the authors propose are quite interesting, and the extensive analyses and ablation studies that are included in the paper are helpful for understanding the contributions of these tasks.",
            "3": "- The paper is very well-written.",
            "4": "- The authors seem to suggest novelty in using dynamic program information for learning code representations through claims like \"To the best of our knowledge, this is the first attempt to unveil the benefits of dynamic program information on code representation pre-training\" and \"...we make the first attempt to derive the program functionality from the dynamic information collected during executions.\"",
            "5": "However, there is work that does similar things, one of which they have cited, and others that they have cited.",
            "6": "Namely, they have not cited \"TRACED: Execution-aware Pre-training for Source Code\" (https://arxiv.org/pdf/2306.07487.pdf) leverages dynamic information, specifically executable inputs and corresponding execution traces, for pre-training.",
            "7": "Though fuzzing is not used there, it is used for building code representations in a paper that is cited: \"Understanding Programs by Exploiting (Fuzzing) Test Cases\" (https://arxiv.org/pdf/2305.13592.pdf), though they are not actually using them for pretraining.",
            "8": "It seems that the contribution is more around using specifically fuzzing for pre-training.",
            "9": "I believe this should more clearly be conveyed.",
            "10": "- Related to the previous point, they present results for an approach that does use dynamic information for pre-training: CodeExecutor (https://arxiv.org/pdf/2305.05383.pdf).",
            "11": "However, they do not present this on the main code search task in Table 1 (which is also more aligned with what CodeExecutor was actually benchmarked on).",
            "12": "In fact, many of the \"state-of-the-art\" models listed in Table 3 are not included in Table 1 for code search.",
            "13": "It is not clear why these results were excluded from the paper.",
            "14": "The same goes for the analyses in Figures 4-5.",
            "15": "Since much of the focus was on code search, the ablations and analyses would be based on that.",
            "16": "- As the authors themselves acknowledge in the limitations section, this work is focused on code understanding tasks and no generative tasks.",
            "17": "I find this a bit troublesome because the underlying model that is used, UniXcoder, was originally designed to also handle generative tasks.",
            "18": "CodeExecutor was also benchmarked on code generation.",
            "19": "The authors do not report results for generative tasks like code generation or summarization."
        },
        "W26gahavxB": {
            "0": "+The idea to incorporate program dynamic behaviors in pre-training seems ok to supplement current code pre-trained models.",
            "1": "+This paper is easy to follow and understand.",
            "2": "-The technique novelty is lack.",
            "3": "I agree that the program dynamic information is important and can benefit code pre-trained models, however in this paper, the usage of dynamic information is too easy.",
            "4": "It just concatenates the test cases with the original code for model training.",
            "5": "I am not sure how much dynamic information is contained in the test cases.",
            "6": "Furthermore, I am confused that why test cases are enough for using dynamic information?",
            "7": "Lastly, is there any other way to use dynamic information rather than such a simple way?",
            "8": "-In terms of model design, the novelty is limited.",
            "9": "It uses BERT-style model as the model architecture for pre-training, why not use more powerful encoder-decoder model and there are some works such as CodeT5[1] and CodeT5++[2] have proved encoder-decoder is better than CodeBERT.",
            "10": "Furthermore, the designed pre-trained tasks DIM and DID are also simple, DID is similar to InfoNCE[3].",
            "11": "-In terms of downstream tasks, the evaluation tasks are also limited.",
            "12": "There are only four downstream understanding code tasks, more code-related tasks are need to evaluate to confirm the effectiveness of the proposed approach.",
            "13": "[1] Wang et al.",
            "14": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation.",
            "15": "[2] Want et al.",
            "16": "CodeT5+: Open Code Large Language Models\n\n[3] Liu et al.",
            "17": "Contrabert: enhancing code pre-trained models via contrastive learning"
        },
        "LbiNe1k7zO": {
            "0": "S1: This paper explores a very important domain, as a reliable, semantic-aware code embedding could be helpful for many code-related tasks;  \nS2: This paper proposed an interesting method that combines representation learning and code execution via fuzzing, and provides a simple way of incorporating the test cases into the pretraining tasks;  \nS3: The ablation studies are rather complete for the readers to understand the contribution of each part in the pretraining objectives.",
            "1": "W1: The presentation of this work is quite poor.",
            "2": "More specifically:\n* The notations are a bit (unnecessarily) complex (e.g., as a matter of fact, you can hardly find a symbol without any super/sub-script).",
            "3": "An example is with Equation 2, I don't think it is necessary to write down the formulation of a linear layer for binary classification, nor the cross-entropy loss;\n* Table 1 is a bit hard to parse, I assume they are some kind of \"transfer\" between different programming languages (nothing is mentioned in the caption), but even with that assumption, there are many questions needed to be answered;\n\nW2: Some of the experiment settings are questionable.",
            "4": "See the \"questions\" section for details;  \nW3: Compared with previous work, the improvements are quite marginal (i.e., 1 point or less) on 2 of the 3 reported tasks.",
            "5": "Also, it would be great to mark the parameter count in table 3."
        }
    },
    "gjfOL9z5Xr": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the evaluation of large language models (LLMs) by introducing a dynamic evaluation protocol, DYVAL, which aims to mitigate data contamination and static complexity in existing benchmarks.",
            "1": "- The novelty lies in the use of directed acyclic graphs (DAGs) to dynamically generate evaluation samples with controllable complexities, providing a more robust and evolving assessment of LLMs' reasoning capabilities.",
            "2": "Potential reasons for acceptance\n   - The introduction of a dynamic evaluation protocol is a significant advancement in the field, addressing the limitations of static benchmarks.",
            "3": "- The use of DAGs for generating evaluation samples is a novel approach that allows for fine-grained control over problem complexity.",
            "4": "- The extensive experiments and analysis provide valuable insights into the performance and limitations of current LLMs.",
            "5": "- The paper demonstrates that DYVAL-generated samples can be used not only for evaluation but also for fine-tuning LLMs, enhancing their performance on existing benchmarks.",
            "6": "Potential reasons for rejection\n   - **Limited scope of tasks:**\n     - The paper primarily focuses on reasoning tasks, which may limit the generalizability of the proposed protocol to other types of tasks.",
            "7": "- The application of DYVAL to natural language tasks is mentioned but not thoroughly explored, leaving a gap in understanding its broader applicability.",
            "8": "- **Sample size and evaluation:**\n     - The experiments are conducted on a limited set of test samples due to resource constraints, which may affect the robustness and reliability of the findings.",
            "9": "- Larger and more diverse test sets could provide a more comprehensive evaluation of the proposed protocol.",
            "10": "- **Fine-tuning experiments:**\n     - The fine-tuning experiments are limited to a few models and datasets, which may not fully capture the potential benefits and limitations of DYVAL-generated samples.",
            "11": "- More extensive fine-tuning experiments on a wider range of models and datasets could provide deeper insights into the effectiveness of the approach.",
            "12": "- **Prompt sensitivity:**\n     - The paper acknowledges that LLMs are highly sensitive to prompts, and the results are based on specific prompt designs.",
            "13": "This variability could affect the reproducibility and generalizability of the findings.",
            "14": "- A more systematic exploration of prompt designs and their impact on evaluation results could strengthen the conclusions.",
            "15": "Suggestions for improvement\n   - **Expand the scope of tasks:**\n     - Extend the application of DYVAL to a broader range of tasks beyond reasoning, such as natural language understanding, sentiment analysis, and other NLP tasks.",
            "16": "- Provide detailed examples and results for these additional tasks to demonstrate the versatility and robustness of the proposed protocol.",
            "17": "- **Increase sample size and diversity:**\n     - Conduct experiments on larger and more diverse test sets to ensure the robustness and reliability of the findings.",
            "18": "- Include a variety of datasets with different characteristics to evaluate the generalizability of DYVAL.",
            "19": "- **Enhance fine-tuning experiments:**\n     - Perform fine-tuning experiments on a wider range of models and datasets to gain deeper insights into the effectiveness of DYVAL-generated samples.",
            "20": "- Compare the performance of fine-tuned models on both in-distribution and out-of-distribution samples to assess their generalization capabilities.",
            "21": "- **Systematic exploration of prompt designs:**\n     - Investigate the impact of different prompt designs on the evaluation results to understand the sensitivity of LLMs to prompts.",
            "22": "- Develop guidelines or best practices for prompt design to improve the reproducibility and generalizability of the findings.",
            "23": "- **Detailed analysis of failure modes:**\n     - Provide a more comprehensive analysis of the failure modes observed in the experiments, including specific examples and potential reasons for the errors.",
            "24": "- Suggest strategies for addressing these failure modes to guide future improvements in LLMs."
        },
        "62SrNlTWFL": {
            "0": "DYVAL presents an innovative approach to evaluating LLMs by dynamically generating evaluation samples, mitigating concerns about data contamination and providing a more realistic assessment of LLMs' capabilities.",
            "1": "The paper conducts extensive experiments across various reasoning tasks and LLMs, offering valuable insights into LLM performance, failure patterns, and the impact of different prompt engineering methods.",
            "2": "DYVAL's ability to improve LLMs' performance on existing benchmarks through fine-tuning with DYVAL-generated data demonstrates its practical utility in enhancing LLM capabilities beyond evaluation 1.",
            "3": "The claim on \"co-evolution\" is not clear.",
            "4": "I do not quite understand what co-evolution means.",
            "5": "It seems that the evaluation process is not dependent on the LLM, then how they are correlated from each other.",
            "6": "The data contamination problem is not clear.",
            "7": "Notably, the data generated by the proposed method is rather limited type as it can not generate narrative generation tasks and others related to common sense.",
            "8": "I am wondering how the existing datasets have the contamination problem.",
            "9": "I think such a problem may not happen frequently in the logical reasoning and algorithm domains (especially, these abilities may be majorly from finetune from code and scientific papers).",
            "10": "However, they are much easier to happen on those storytelling data.",
            "11": "The potential bias may exist in the graph generation.",
            "12": "The paper focuses on how to conduct constraints for the graph to avoid illegal ones.",
            "13": "Nonetheless, there may be lacked of details on how the graph is generated to meet those constraints.",
            "14": "I am concerned that the graph generation algorithms remain biased.",
            "15": "Therefore, there will be bias in the generated text, leading to the potential issue."
        },
        "DG1xErIvIc": {
            "0": "- Extensive experiments are conducted.",
            "1": "- Graph-based notions of complexities can be used as a means to control the compositional complexity of the examples.",
            "2": "- Address data contamination and static complexity of the benchmarks.",
            "3": "- A common challenge associated with this framework is the need to manually specify a problem as a computation graph with valid constraints.",
            "4": "This requirement is only understandable if LLM is intended to acquire specific skills written in these formats.",
            "5": "- Before reading this paper, I believed that generating a large number of mathematical problems of specific types and evaluating LLMs on them was primarily for debugging specific LLM capabilities, such as compositionality, rather than as an evaluation framework.",
            "6": "I'm not sure if these types of problems are fundamental questions about LLMs.",
            "7": "In fact, prior studies, such as those by Dziri et al., have already highlighted the limitations of transformers in these settings, using a very similar setup for demonstration.",
            "8": "- It's not clear if LLMs are losing some skills when fine-tuned on DyVal as DyVal examples and the chosen existing benchmarks are from very similar domains.",
            "9": "The generalization of the fine-tuned model on DP is interesting though.",
            "10": "Recommendation:\nAs a person who has worked on dynamic adversarial data collection, or more broadly dynamic benchmarks, I think your review of this literature is underestimating their importance.",
            "11": "In fact, in dynamic adversarial data collection annotators can be provide interesting problem instances hard to find in static benchmarks and even hard to manually specify as a DyVal task.",
            "12": "So, I encourage you to include a better review of these works.",
            "13": "If you are concerned with the human-in-the-loop, I believe the recent theoretical frameworks of dynamic benchmarking are still valid if humans are replaced by generative tools which you may consider mentioning.",
            "14": "So, I encourage you to revisit page 2 paragraph 1 at your discretion.",
            "15": "Overall, I believe that in the era of LLMs, we should explore new methods of evaluation, and this paper's framework might be one of them.",
            "16": "The ICLR audience may find this work interesting, so I will maintain a positive rating despite the concerns I have."
        },
        "mkhIBGrSpw": {
            "0": "The motivation of this paper is clear.",
            "1": "As many LLMs tend to memorize static data for evaluation, this paper proposes a dynamic approach to avoid this kind of problem.",
            "2": "The idea of generating tasks with different difficulties in a DAG style sounds interesting.",
            "3": "The problem is clearly described with sufficient notations and examples.",
            "4": "Experiments are conducted in various aspects, including 7 reasoning tasks, 1 human evaluation, on about 8 well-known LLMs.",
            "5": "Fine-tuning experiments are also conducted to demonstrate that the LLMs' ability in learning to reason.",
            "6": "The title is somewhat misleading.",
            "7": "The evaluation tasks in this paper are mostly about reasoning on maths, logic, algorithms, etc.",
            "8": "However, the title reflects no information about this point.",
            "9": "The abstract could be also clearer if this point can be mentioned earlier.",
            "10": "For the fine-tuning results in Section 5, I wonder when these LLMs are fine-tuned for the reasoning tasks proposed in this method, will the general abilities be influenced?",
            "11": "Or to what extent will they be influenced?",
            "12": "As the samples for evaluation are dynamic, the comparison may be unfair when the generated data are different in different evaluation stages."
        },
        "9mU1CVhcbM": {
            "0": "S1.",
            "1": "Simple, yet flexible framework.",
            "2": "S2.",
            "3": "Dynamic task generation with controllable complexity\nS3.",
            "4": "Extensive evaluation of selected LLMs / prompting strategies for seven simple reasoning tasks.",
            "5": "On S1.",
            "6": "The general idea of the proposed benchmarking framework is to generate tasks that can be described by a directed acyclic graph.",
            "7": "This includes \"compute graphs\" (e.g., evaluate a numerical expression or perform logical reasoning) or \"data graphs\" (e.g., determine connectivity between vertices).",
            "8": "The framework takes care of graph generation, task implementations add contraints, labels, solutions, and verbalization.",
            "9": "This is a very natural approach and (most probably) how many of the existing benchmarks of this form are generated in the first place.",
            "10": "Such a framework may increase usability, especially when many tasks were implemented in it.",
            "11": "On S2.",
            "12": "Tasks are generated automatically and with varying complexity (mainly graph size).",
            "13": "Again, this is a simple, very natural approach.",
            "14": "Here the framework proposed by the paper may make comparative evaluation across a range of tasks more feasible, as all share the same notion of \"complexity\".",
            "15": "On S3.",
            "16": "The paper reports performance results on simple computational tasks (such as evaluating simple equations).",
            "17": "Generally, all models break down when complexity goes up so that the benchmark may be used as a way to evaluate progress.",
            "18": "Also, the performance reported on these simple tasks sometimes contradict performance results published on related, static benchmarks.",
            "19": "W1.",
            "20": "Certain computational tasks only\nW2.",
            "21": "Discussion of related work / results lacking\nW3.",
            "22": "Limitations in generated graphs\nW4.",
            "23": "Code/data availability unclear\nW5.",
            "24": "Limited insight of experimental study\n\nOn W1.",
            "25": "By the nature of the benchmark, it focuses on problems that can be expressed as (currently small) compute graphs or data graphs and are somewhat artificial.",
            "26": "It only tests a very limited field of LLM functionality.",
            "27": "On W2.",
            "28": "There are benchmarks for all of the tasks that are implemented in this framework already.",
            "29": "The paper states that its performance results contradict the ones on some of these benchmarks, but does not say which ones and, perhaps more importantly, does not provide any insight into why this is the case.",
            "30": "Also, the data generation strategies used by existing benchmarks are not discussed.",
            "31": "Finally, to what extent the benchmark can be used to really do new things (beyond existing benchmarks) is not discussed.",
            "32": "On W3.",
            "33": "First, the paper focuses solely on DAGs, but it's unclear why this is done for data graphs (e.g., reachability, max-sum).",
            "34": "Second, it's unclear whether graph size is the right complexity measure.",
            "35": "E.g., for reachability appears easier is source and target are neighbors, no matter how large the graph.",
            "36": "Finally, the system does not seem to generate balanced datasets.",
            "37": "For example, the paper reports in the appendix that the proportion of true answers for reachability is not controlled, leading to \"paradoxical\" results.",
            "38": "On W4.",
            "39": "It's important for benchmarking papers such as this one to make all code, datasets, prompts, results, etc.",
            "40": "public.",
            "41": "The paper currently does not provide any ressources (or, at least, I did not see them).",
            "42": "On W5.",
            "43": "The insight that can be drawn from the experiments is somewhat limited.",
            "44": "I do not count this against the paper, however.",
            "45": "It does show exposed limitations of LLMs and prompting strategies, and it does show that the generated tasks are useful for fine-tuning.",
            "46": "Minor points:\n\nI am not sure how useful the comparison to human performance is.",
            "47": "Clearly, all of the tasks can be solved \"easily\" by humans, it's just a pain to do so."
        }
    },
    "PhJUd3mbhP": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces AutoAgents, a novel framework for dynamically generating and coordinating multiple specialized agents to form an AI team tailored to different tasks.",
            "1": "- The framework addresses the limitations of existing LLM-based multi-agent systems that rely on predefined agents, enhancing adaptability and efficiency in task-solving.",
            "2": "- The incorporation of an observer role for self-refinement and collaborative refinement among agents is a significant contribution to improving the quality and coherence of solutions.",
            "3": "Potential reasons for acceptance\n   - The framework demonstrates significant improvements in task performance over existing methods, as evidenced by quantitative experiments and case studies.",
            "4": "- The dynamic generation of agents based on task content and the planning solutions for the current task showcase the adaptability and robustness of the proposed approach.",
            "5": "- The paper provides a comprehensive analysis and comparison with existing frameworks, highlighting the advantages of AutoAgents in handling complex tasks.",
            "6": "- The inclusion of detailed experimental results and case studies, such as the software development example, illustrates the practical applicability and effectiveness of the framework.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in certain sections:**\n     - The description of the agent generation process and the roles of predefined agents (Planner, Agent Observer, Plan Observer) could be more detailed and clearer.",
            "8": "- The explanation of the execution stage, particularly the communication and coordination among agents, may benefit from additional elaboration and examples.",
            "9": "- **Limited diversity in experimental tasks:**\n     - The experiments primarily focus on open-ended question answering and trivia creative writing tasks.",
            "10": "Including a wider variety of tasks could strengthen the validation of the framework.",
            "11": "- The case study on software development is insightful, but additional case studies in different domains could further demonstrate the versatility of AutoAgents.",
            "12": "- **Potential scalability issues:**\n     - The framework's performance and efficiency with a large number of agents and highly complex tasks are not thoroughly explored.",
            "13": "- The impact of token limitations in LLM models on the framework's performance, especially in scenarios with extensive historical information, needs further investigation.",
            "14": "Suggestions for improvement\n   - **Enhance clarity and detail:**\n     - Provide a more detailed and step-by-step explanation of the agent generation process, including specific examples of agent roles and their interactions.",
            "15": "- Elaborate on the execution stage, particularly the communication and coordination mechanisms among agents, with additional diagrams and examples.",
            "16": "- **Expand experimental validation:**\n     - Include a broader range of experimental tasks to validate the framework's effectiveness across different domains and scenarios.",
            "17": "- Conduct additional case studies in various fields, such as healthcare, finance, and education, to demonstrate the versatility and practical applicability of AutoAgents.",
            "18": "- **Address scalability and efficiency:**\n     - Investigate the framework's performance and efficiency with a larger number of agents and more complex tasks, providing insights into potential scalability issues.",
            "19": "- Explore strategies to mitigate the impact of token limitations in LLM models, such as optimizing memory management and information sharing among agents."
        },
        "XUrmOMqHzo": {
            "0": "This study provides a valuable clarification of its position,\nespecially in the context of LLM-based Agent frameworks.",
            "1": "In comparison\nto AgentVerse and SSP, this research stands out by highlighting the\nsignificance of Self-Refinement agents and Collaborative Refinement\nAction as key differentiators.",
            "2": "The paper is perceived as having low readability and insufficient\nreproducibility.",
            "3": "The reviewer kindly requests a more granular\ndescription of the methodology that enables readers to implement the\nprocedures step by step.",
            "4": "For instance, while Table 1 is highly\nbeneficial for positioning this research within the LLM-based Agent\nframework, in comparison to AgentVerse and SSP, it distinctly\nhighlights the significance of Self-Refinement agents and\nCollaborative Refinement Action.",
            "5": "Nevertheless, the two points\nmentioned above are not clearly articulated in Section 3.2, \"EXECUTION\nSTAGE.\"",
            "6": "They are mentioned in the text and Figure 2, but their\npresentation as steps is absent, making it challenging for readers to\ncomprehend and evaluate reproducibility.",
            "7": "The evaluation in the experiments lacks qualitative insights.",
            "8": "In the\nexperiments, it remains unclear how the introduction of\nSelf-Refinement agents and Collaborative Refinement Action has led to\ndifferential outcomes compared to SSP, and what specific effects these\ntwo points have had.",
            "9": "While accuracy has undeniably improved, it is\nessential to qualitatively demonstrate how these two aspects have\ncontributed to the observed results.",
            "10": "There are concerns regarding the reproducibility of the\nexperiments.",
            "11": "It is unclear whether the CASE STUDY has been practically\nrealized or if it serves as an imagined example for\napplication.",
            "12": "The paper lacks an ablation study to assess the impact of modifying or\nomitting certain components within the system, particularly in the\nDraft and Execution phases where multiple agents are involved, such as\nAgent Observer, Plan Observer, Researcher, Planner, Writer, Character\nDeveloper, and others.",
            "13": "This study could help elucidate the\nsignificance of each component and its contribution to the overall\nsystem.",
            "14": "Furthermore, the absence of an ablation study regarding\nShort-term memory, Long-term memory, and Dynamic memory raises\nconcerns.",
            "15": "Investigating the effects of altering or removing these\nmemory components could provide valuable insights into their\nrespective roles and importance within the framework.",
            "16": "Overall,\nconducting such ablation studies would enhance the paper's depth and\nprovide a more comprehensive understanding of the system's inner\nworkings and the role of its individual components."
        },
        "KUTJW9Ftwy": {
            "0": "- Clear presentation of high-level idea: the overall framework and process is clearly presented through well-drawn figures like Fig.",
            "1": "1 and 2.",
            "2": "- Strong reproducibility: the author provides source code and the temperature of LLM is set to 0, which makes it easy to reproduce the result in the paper.",
            "3": "- Limited novelty: according to Table 1, the main difference between the proposed framework and other existing methods like Social Simulacra, Epidemic Modeling, SSP, and AgentVerse is that this work uses self-refinement and collaborative refinement.",
            "4": "This difference is more of a prompting technique and has already been used in many existing works like [1, 2, 3]\n- Unclear presentation of detailed techniques: though the high-level idea is well-presented, the details of many technique are unclear.",
            "5": "For example, how to determine when and which agents should engage in collaborative refinement?",
            "6": "This is the main differnce from other methods but there is very little detailed description.",
            "7": "More questions are in the Question part.",
            "8": "- Insufficient evaluation: \n    1.",
            "9": "Lack of baselines: Table 1 lists 12 existing frameworks, but none is used as baseline in task 1, and only 1 is used in task 2.",
            "10": "Comparisons with existing methods are needed to show the effectiveness of the proposed methods.",
            "11": "Lack of ablation study: there is no quantitive ablations on different components of the framework like self-refinement, collaborative refinement, etc.",
            "12": "Unfair comparisons and potential problem in metric: in task 1, it is unfair to compare AutoAgents using GPT-4 with ChatGPT and Vicunna-13B.",
            "13": "In task 2, the metric only considers the QA quality, how about the quality of the story around the given topic?",
            "14": "Reference:\n\n[1] Noah Shinn, Beck Labash, and Ashwin Gopinath.",
            "15": "Reflexion: an autonomous agent with dynamic memory and self-reflection.",
            "16": "arXiv preprint arXiv:2303.11366, 2023.",
            "17": "[2] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.",
            "18": "Critic: Large language models can self-correct with tool-interactive critiquing, 2023.",
            "19": "[3] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.",
            "20": "Inner monologue: Embodied reasoning through planning with language models.",
            "21": "arXiv preprint arXiv:2207.05608, 2022."
        },
        "CZlayiAqtq": {
            "0": "This paper presents a framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks.",
            "1": "The paper is technically sound and the research question is clear.",
            "2": "The contribution of the paper is relevant for LLM-based multi-agent collaboration.",
            "3": "The results of this paper is interesting and significant in automatic agent generation.",
            "4": "The proposed AutoAgents framework generates more coherent and accurate solutions than the existing multi-agent methods.",
            "5": "How the proposed AutoAgents framework expands the scope of collaborative applications and reduces the consumption of resources should be elaborated.",
            "6": "The authors do not explain how to determine the number of agents in the section of the framework for automatic agent generation.",
            "7": "The section about automatic agent generation is too tedious to introduce too much related works\n4.",
            "8": "In addition to ChatGPT, Vicuna-13B and GPT4 in Table 2, it has not enough recent models to further show the superiority of the proposed framework-AutoAgents in open-ended question answer task in the experimental part.",
            "9": "In the experimental part, the performance on N=10 is better than N=5 in trivia creative writing task, but there is no explanations."
        },
        "XZLGgAbXIr": {
            "0": "The idea of dynamically generating agents who play different roles to solve team tasks is interesting and useful.",
            "1": "I found the idea to be novel.",
            "2": "It is easy for the reader to get a good overview of the idea of AutoAgents.",
            "3": "However, there was a need to look at supplementary materials to understand aspects of what the different predefined roles were supposed to do.",
            "4": "The visuals helped me understand the idea better.",
            "5": "The background was sufficient, in my opinion, and well-written.",
            "6": "This discussion and Table 1 made the contributions clear.",
            "7": "Section 3:\nFor the agent generation, the motivation for the format of the Prompt P is unclear.",
            "8": "Additionally, when we look at the supplementary material, the specific elements of the prompt are not explained -- are these taken from existing works?",
            "9": "Others:\nI also found details that needed to be included in a few other sections, such as the self-refinement process.",
            "10": "Furthermore, I had questions about specific choices of parameters during the evaluations.",
            "11": "I have included my questions in the next part to capture the specific places where I needed more information.",
            "12": "Minor typos:\nPage 2: effectiveness of AutoAgents.",
            "13": "[we] also conduct\nPage 7: at = lt ∪ pt ∪ ot, [where lt,] where lt denotes"
        }
    },
    "h1SSQ6Dekc": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper presents a novel approach to simultaneous machine translation (SiMT) using decoder-only large language models (LLMs), which is a significant departure from the traditional encoder-decoder transformer models.",
            "1": "- The introduction of a \"wait\" token to control input segmentation directly within the LLM is an innovative method that simplifies the translation process by eliminating the need for a separate policy.",
            "2": "Potential reasons for acceptance\n   - The proposed method demonstrates competitive BLEU scores for English-German and English-Russian SiMT tasks, showing that the approach is viable and effective.",
            "3": "- The paper provides a comprehensive evaluation of both open-source and closed-source models, including zero-shot performance, which adds robustness to the findings.",
            "4": "- The approach simplifies the SiMT system architecture by integrating input segmentation and translation within a single model, potentially reducing the complexity and computational overhead.",
            "5": "Potential reasons for rejection\n   - **Limited Dataset for Fine-Tuning**\n     - The fine-tuning was performed on a relatively small dataset (4000 sentences for training and 100 for validation), which may not be representative enough to generalize well across different domains and languages.",
            "6": "- The paper does not provide a detailed analysis of how the model performs on larger and more diverse datasets, which is crucial for real-world applications.",
            "7": "- **Evaluation Metrics and Comparisons**\n     - The paper primarily uses BLEU scores for evaluation, which, while standard, may not fully capture the nuances of translation quality, especially in simultaneous translation scenarios.",
            "8": "- There is a lack of detailed comparison with other state-of-the-art SiMT systems beyond BLEU scores, such as human evaluation or other qualitative metrics.",
            "9": "- **Real-Time Performance and Latency**\n     - The real-time factor (RTF) for the proposed system, especially in S2TT mode, is significantly higher compared to some baselines, which could be a critical issue for practical deployment.",
            "10": "- The paper does not thoroughly address the latency introduced by the ASR subsystem and how it impacts the overall performance of the SiMT system.",
            "11": "- **Generalization and Robustness**\n     - The zero-shot performance of the LLMs, particularly GPT-3.5, was poor, indicating that the model may not generalize well without fine-tuning.",
            "12": "- The paper does not explore the robustness of the model to different types of input noise or variations in speech, which is important for real-world applications.",
            "13": "Suggestions for improvement\n   - **Expand the Dataset for Fine-Tuning**\n     - Fine-tune the model on a larger and more diverse dataset to ensure better generalization and robustness across different domains and languages.",
            "14": "- Include additional language pairs to demonstrate the versatility of the proposed approach.",
            "15": "- **Enhance Evaluation Metrics**\n     - Incorporate additional evaluation metrics such as human evaluation, TER, or METEOR to provide a more comprehensive assessment of translation quality.",
            "16": "- Provide a detailed comparison with other state-of-the-art SiMT systems using these metrics.",
            "17": "- **Optimize Real-Time Performance**\n     - Investigate ways to reduce the real-time factor, particularly in S2TT mode, by optimizing the ASR subsystem and exploring end-to-end approaches.",
            "18": "- Consider hardware acceleration or more efficient quantization techniques to improve inference speed.",
            "19": "- **Improve Zero-Shot Performance**\n     - Explore advanced prompt engineering and more sophisticated generation strategies to enhance the zero-shot performance of the LLMs.",
            "20": "- Investigate the potential of self-instruct and human preference tuning to improve the model's ability to follow instructions and generate accurate translations without fine-tuning.",
            "21": "- **Robustness and Generalization**\n     - Conduct experiments to test the model's robustness to different types of input noise and variations in speech.",
            "22": "- Include a detailed analysis of the model's performance across different contexts and scenarios to ensure its reliability in real-world applications."
        },
        "GheNouW7iH": {
            "0": "- Though there are many papers about training LLMs to as decision-making agent, I consider doing so with Simultaneous translation, which is predominantly about speech, is novel and the task of SiMT can improved with the help from LLM.",
            "1": "- The results show comparable with existing high-quality SiMT baselines, though I highly doubt the actual computational cost is anywhere comparable (see weakness).",
            "2": "Future work should make up for this by achieve higher translation quality and latency, as well as in other lower-resource languages.",
            "3": "- Repeated inference of LLM is a huge computational cost, everytime a <wait> token is generated, the text prompt is updated and many tokens representations have to be recalculated without a theoretical room for caching.",
            "4": "As such, real-life inference, with a fixed physical hardware, will be much slower compared to existing lightweight translation model.",
            "5": "This is true for closed-source GPT models as well, as more tokens called to the API leads to more expensive bill.",
            "6": "- Therefore, I urge the authors to provide a real-life inference cost/wall-time comparison to have a better picture of the cost trade-off here and makes the paper complete.",
            "7": "I would appreciate and change scores if such report is produced."
        },
        "cWrZiwwm87": {
            "0": "The setup is described clearly and is very straightforward, which makes this work easily reproducible.",
            "1": "I also appreciate the results section, which includes the most natural ablations and is not overselling the results.",
            "2": "In fact, the most obvious concern of using LLMs for SiMT - inference time - is acknowledged in the paper.",
            "3": "The evaluation is based on (just) two language pairs and two LLM (sizes), which is definitely on the slim side, but it meets the minimum bar for me.",
            "4": "I don't think that this paper is particularly innovative.",
            "5": "On a high level, it strikes me as one of the \"we tried LLMs for task X and it worked\" papers that are very common these days.",
            "6": "That being said, I think that this is one of the better papers in that category due to the sober evaluation and clear writing.",
            "7": "So although I wasn't inspired by this work, there is still value in publishing it for the sake of completeness of the body of literature on LLMs.",
            "8": "Fig.",
            "9": "2 looks broken..",
            "10": "I guess the key point here is that \"away\" is aligned to \"befreite\", but the alignment link is not shown in the original en-de alignment."
        },
        "tw0pmS8qWr": {
            "0": "- The concept of employing large language models for simultaneous translation appears both novel and exciting.",
            "1": "- The paper is clearly written and easy to understand.",
            "2": "- The related work section provides a comprehensive summary of simultaneous translation research in the field of natural language processing.",
            "3": "- Figure 1 lacks clarity in terms of distinguishing when specific actions (READ/WRITE) occur.",
            "4": "It would be more reader-friendly if the figure illustrated a step-by-step walkthrough (e.g., t=1, t=2, t=3).",
            "5": "- In simultaneous translation, wall-clock time (actual speed) is a critical factor.",
            "6": "It would be important to report or at least mention how long it takes to generate translations in this setting.",
            "7": "- The experiment only presents BLEU scores; it lacks concrete examples of output, which would be beneficial for understanding the translation quality."
        },
        "jfebyH4La1": {
            "0": "The paper introduces a novel approach to improving SiMT systems by fine-tuning a pre-trained Language Model (LLM) with direct supervision on causally aligned source-target sentence pairs.",
            "1": "This approach differs from previous methods that rely on separate policies or incremental decoding.",
            "2": "By leveraging the capabilities of LLMs, the paper offers a fresh perspective on enhancing SiMT performance.",
            "3": "One of the main concerns regarding this paper is the reliance on a reference-based approach for the causal alignment introduced.",
            "4": "While the paper claims to propose a novel method, similar ideas have been studied in previous simultaneous translation literature (e.g.",
            "5": "[1]).",
            "6": "However, the paper lacks a comparative analysis with these existing approaches in the experiment section, making it difficult to assess the novelty and superiority of the proposed method.",
            "7": "Furthermore, a significant limitation of the reference-based approach is the potential mismatch between full sentence translation and simultaneous translation.",
            "8": "The references used to generate the causal alignment are derived from complete sentence translations, which may not be suitable for the dynamic nature of simultaneous translation.",
            "9": "Simultaneous translation requires the model to begin translation based on partial context, and the reference-based approach may not adequately capture the challenges and nuances specific to this task.",
            "10": "[1] Simultaneous translation policies: from fixed to adaptive.",
            "11": "ACL, 2020"
        }
    },
    "kvcbV8KQsi": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces the concept of \"successor heads\" in large language models (LLMs), which are attention heads that increment tokens in natural sequences such as days, months, and numbers.",
            "1": "- It provides a mechanistic interpretability approach to explain the behavior of these successor heads, contributing to the understanding of how LLMs process and generate sequential data.",
            "2": "- The study spans models of various sizes and architectures, from 31 million to 12 billion parameters, demonstrating the universality of successor heads across different LLMs.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a significant gap in the mechanistic interpretability of LLMs, particularly in understanding how these models handle sequential data.",
            "4": "- It introduces a novel concept (successor heads) and provides a detailed analysis of their behavior, including the identification of abstract numeric representations (mod-10 features).",
            "5": "- The findings are validated across multiple LLM architectures and sizes, suggesting a form of universality in the behavior of successor heads.",
            "6": "- The paper includes comprehensive experiments and analyses, such as vector arithmetic with mod-10 features and the role of successor heads in natural language datasets, enhancing the robustness of the results.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in some explanations:**\n     - The description of the effective OV circuit and its role in identifying successor heads might be too technical for readers unfamiliar with the specific terminology and methods used.",
            "8": "- The explanation of how mod-10 features are derived and their significance could be more detailed to ensure a broader understanding.",
            "9": "- **Limited scope of practical applications:**\n     - While the paper provides a thorough theoretical analysis, it may lack concrete examples of how the findings can be applied to improve LLM performance or interpretability in real-world scenarios.",
            "10": "- The practical implications of the greater-than bias and the polysemanticity of successor heads are not fully explored.",
            "11": "- **Insufficient comparison with existing methods:**\n     - The paper could benefit from a more detailed comparison with other interpretability methods and studies, highlighting the unique contributions and advantages of the proposed approach.",
            "12": "- There is limited discussion on how the findings relate to or differ from previous work on attention heads and numeric representations in LLMs.",
            "13": "Suggestions for improvement\n   - **Enhance clarity and accessibility:**\n     - Provide more background information and context for readers who may not be familiar with the specific technical terms and methods used in the study.",
            "14": "- Include more detailed explanations and visual aids to illustrate key concepts, such as the effective OV circuit and mod-10 features.",
            "15": "- **Expand on practical applications:**\n     - Discuss potential applications of the findings in improving LLM performance, such as enhancing the accuracy of sequential data predictions or developing more interpretable models.",
            "16": "- Explore how the identified greater-than bias and polysemanticity of successor heads can be leveraged or mitigated in practical scenarios.",
            "17": "- **Strengthen comparisons with existing work:**\n     - Include a more comprehensive review of related work in mechanistic interpretability and attention heads, highlighting the unique contributions of the current study.",
            "18": "- Provide a detailed comparison of the proposed approach with other interpretability methods, discussing the advantages and limitations of each.",
            "19": "- **Address limitations and future work:**\n     - Acknowledge the limitations of the current study, such as the focus on specific types of sequential data and the potential for other forms of numeric representations.",
            "20": "- Suggest directions for future research, such as exploring the behavior of successor heads in other types of LLMs or extending the analysis to more complex sequential tasks."
        },
        "3UmSZ4IwHn": {
            "0": "The empirical rigor of this work is high.",
            "1": "The authors provide several ablations to argue the existence of mod_$10$ features in transformers.",
            "2": "They moreover provide detailed additional information for experiments in the appendix.",
            "3": "The finding of successor heads is interesting and provides a good framework for understanding how transformers reason about incrementation.",
            "4": "The connection between incrementation and acronym prediction observed in successor heads is interesting.",
            "5": "The paper lacks a proper background section.",
            "6": "Terms like OV matrix are not introduced, and more generally, the notion of circuits, or what un enembedding is, are never properly defined.",
            "7": "This makes the paper very hard to digest without being familiar with the concepts of transformer circuits (Elhage '21).",
            "8": "Given that the paper is already quite dense, you can e.g.",
            "9": "move figure 7 to the appendix, and properly lay the appropriate terminology to understand this work."
        },
        "0rihrnLYVc": {
            "0": "Strengths:\n1.",
            "1": "The paper is clear and easy to read.",
            "2": "To show claims of a weak form of universality, the paper thoroughly tests for successor scores across several models for various numbers of parameters.",
            "3": "The experiments to find and verify the mod-10 features are also thoroughly performed.",
            "4": "These features were confirmed by several different methods: first by training a sparse autoencoder using reconstruction loss on the MLP0 outputs, and then by further comparisons to linear probing and ablation to reinforce these observations.",
            "5": "This is an interesting result uniting various tokens under mod-10 classes.",
            "6": "There are also interesting results with the natural language experiments that demonstrate interpretable polysemanticity.",
            "7": "Weaknesses:\n1.",
            "8": "In Section 4, direct effect mean ablation is used to show \"that when the successor head is contributing usefully, the prompts often required some kind of incrementation\".",
            "9": "Then in Appendix J, direct effect mean ablation is used again to show that the successor head is the most important head across 64 prompts.",
            "10": "Though this is stated with some evidence, not enough quantifiable evidence is shown here to justify the reach of these claims, such as the statement of \"mostly solved\".",
            "11": "More analysis can also be shown about the \"direct effect\" to separate it from \"indirect effects\".",
            "12": "The paper also did not clarify the details of the ablation, such as if it used resampling ablation (there may be issues if it used mean ablation from the same dataset, as there are known issues with mean ablation [1]), and/or path patching (to obtain direct effects).",
            "13": "[1] https://www.lesswrong.com/posts/kcZZAsEjwrbczxN2i/causal-scrubbing-appendix#3_Further_discussion_of_zero_and_mean_ablation\n\n2.",
            "14": "The paper mentions that vector addition was performed successfully for 89% of the cases for digits 20-29, and mentions how it was performed on token '35'.",
            "15": "It does not mention how this performed for other digits.",
            "16": "This is likewise the case for number words only showing ten to twenty.",
            "17": "Presumably, the performance is similar, but the paper should explicitly mention this to avoid criticism of cherry-picking.",
            "18": "In Figure 7, it's also unclear what \"target residues modulo 10\" means when referring to the column headers.",
            "19": "Presumably, this is stating something similar to how the vector arithmetic on MLP0(E('fifth') makes it \"behave more like MLP0(E('seventh')\".",
            "20": "The wording can be made clearer to avoid confusion that it means the number \"7\" rather than the word \"seventh\".",
            "21": "Additionally, the checkmarks are given when \"the max logits are on the successor token\".",
            "22": "This is an interesting result, but how big is the logit difference between logits for the successor token and other tokens?",
            "23": "Appendix D states that scaling was used on the additive feature terms.",
            "24": "A quick explanation of why a particular scaling factor was used would be helpful.",
            "25": "The paper states: \"to the best of our knowledge the presence of both successorship and acronym behavior in head L12H0 is the cleanest example of polysemantic behavior identified so far in an LLM.\"",
            "26": "Why is this the cleanest example of polysemantic behavior, compared to other studies on the topic such as in [2]?",
            "27": "Similarly for this statement, \"which to the best of our knowledge are the most closely studied components in LLMs that occur in both small and large models\", what other components are you comparing to that are not as closely studied?",
            "28": "[2] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas.",
            "29": "Finding neurons in a haystack: Case studies with sparse probing, 2023\n\n4.",
            "30": "This paper discovers novel and interesting observations, but it does not elaborate much on why this observation is impactful enough."
        },
        "6SwWiTBpXv": {
            "0": "Good informative figures such as Fig 1 and Fig 7, clear writing.",
            "1": "The use of OV circuits in the discovery and analysis seems smart and somewhat novel to standard methodology for these kinds of findings.",
            "2": "Interesting behavior and good multi-pronged analysis of it.",
            "3": "Somewhat overclaiming the contribution:\nFor example abstract says:  \"Existing research in this area has found interpretable language model components in small toy models.",
            "4": "However, results in toy models have not yet led to insights that explain the internals of frontier models and little is currently understood about the internal operations of large language models.\"",
            "5": "This makes it sound like existing work has only studied toy models which is not true, while also making it sound like this work would study frontier models which is not the case.",
            "6": "While they look at larger models than most related work, the wording makes it sound like difference is larger than it is.",
            "7": "Also the findings about mod 10 features are almost entirely based on the setting of incremental numbers which makes sense, while the writing makes it sound like they are behind successor head behavior on all tasks.",
            "8": "The only evidence of these being used on other task is a low success percentage on changing output month with vector arithmetic.",
            "9": "I would expect for tasks like months and days there would be other mod-12 or mod-7 features for example that could explain this behavior, was this studied?"
        },
        "XpaDH0136J": {
            "0": "The findings presented in this paper are significantly novel.",
            "1": "Authors have clearly described the functions of successor heads and designed multiple experiments to validate their hypothesis.",
            "2": "I especially appreciate Section 3.3 where the evidence in arithmetic is a strong proof that the activation of success attention indeed captures the natural ordering of words and is responsible for the LLM’s reasoning.",
            "3": "There are a few issues mostly in the presentation of the work.",
            "4": "I am getting really annoyed when the authors place all definitions, i.e.",
            "5": "the Glossary section, at the end of the appendix.",
            "6": "It is really inconvenient for the reader to go back and forth during the reading.",
            "7": "It has to have better ways to present the definitions in the context.",
            "8": "Please do not do this.",
            "9": "It lacks sufficient descriptions for the reader to understand the process that parses the original output of the attending heads to the sparse encoder’s output.",
            "10": "I understand that this is to make more room to present the findings; however, it makes the methodology part pretty unclear from reading the current version.",
            "11": "I have to go back and forth and spend a lot more time on Section 2 and 3 to make sure I understand the way each figure is plotted."
        }
    },
    "3d0OmYTNui": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the significant issue of privacy in the alignment of large language models (LLMs) with reinforcement learning (RL), a topic of growing importance as LLMs become more integrated into applications.",
            "1": "- The introduction of a differentially private (DP) framework for aligning LLMs with RL is novel and provides a new direction for research in the privacy community.",
            "2": "- The work builds on influential studies and adapts existing methodologies to ensure privacy, which is a crucial advancement in the field.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a relevant and timely problem, offering a solution that balances utility and privacy, which is critical for the deployment of LLMs in real-world applications.",
            "4": "- The proposed DP framework is well-defined and mathematically sound, with clear steps and privacy guarantees.",
            "5": "- The experimental results are robust, demonstrating the effectiveness of the approach on commonly studied tasks and showing competitive utility while ensuring strong privacy protections.",
            "6": "- The paper provides a comprehensive evaluation, including both qualitative and quantitative analyses, which strengthens the validity of the findings.",
            "7": "Potential reasons for rejection\n   - **Complexity and Clarity**\n     - The paper is dense with technical details, which might make it difficult for readers who are not experts in both RL and DP to fully grasp the contributions.",
            "8": "- Some sections, particularly the explanation of the DP adaptation of PPO, could benefit from clearer and more concise descriptions.",
            "9": "- **Experimental Limitations**\n     - The experiments are limited to specific datasets (IMDb and Reddit TL;DR), which may not fully represent the diversity of tasks LLMs are used for.",
            "10": "- The paper acknowledges the need for more extensive hyperparameter tuning and longer DP training, which suggests that the current results might not be fully optimized.",
            "11": "- **Generalization and Scalability**\n     - The scalability of the proposed framework to much larger models and more complex tasks is not thoroughly explored, which could be a concern for practical applications.",
            "12": "- The assumption that a single user can contribute to at most one of the three datasets (D1, D2, and D3) might limit the generalizability of the approach.",
            "13": "Suggestions for improvement\n   - **Enhance Clarity and Accessibility**\n     - Simplify and clarify the explanations of the DP adaptation of PPO and other technical sections to make the paper more accessible to a broader audience.",
            "14": "- Include more intuitive diagrams or flowcharts to illustrate the framework and the steps involved in the DP alignment process.",
            "15": "- **Expand Experimental Evaluation**\n     - Conduct experiments on a wider range of datasets and tasks to demonstrate the generalizability and robustness of the proposed framework.",
            "16": "- Perform more extensive hyperparameter tuning and longer DP training to optimize the results and provide a more comprehensive evaluation.",
            "17": "- **Address Scalability and Generalization**\n     - Explore the scalability of the framework to larger models and more complex tasks, providing insights into potential challenges and solutions.",
            "18": "- Consider alternative assumptions or frameworks that allow for more flexible contributions from users across different datasets, enhancing the generalizability of the approach.",
            "19": "- **Future Research Directions**\n     - Outline specific future research directions, such as improving DPPPO algorithms, tighter privacy guarantees, and adapting the algorithms to online settings, to guide further advancements in the field."
        },
        "dBfJTWyVRz": {
            "0": "The paper proposes a differentially private framework for aligning LLMs with RL, offering mathematical guarantees of privacy.",
            "1": "The paper empirically evaluates the framework on tasks like positive review generation and summarization, showing that it offers competitive utility while ensuring strong privacy protections.",
            "2": "The paper employs DPSGD to ensure privacy in the alignment of large language models through reinforcement learning.",
            "3": "While the use of DPSGD is well-established in the privacy literature.",
            "4": "Furthermore, the paper does not introduce significant modifications to the RLHF process.",
            "5": "The innovation seems to be more focused on engineering adjustments rather than novel theoretical contributions.",
            "6": "The paper discusses the trade-offs between privacy and utility but does not present these results in an intuitive manner.",
            "7": "A Pareto frontier could be more illustrative in showing how different levels of privacy (varying ε) impact the model's performance.",
            "8": "This would provide a clearer understanding of the trade-offs involved.",
            "9": "If the reward in step 2 is DP, is it necessary to use the DPPPO in step 3 as the learning reward is already DP?"
        },
        "ohPgFfwTif": {
            "0": "The paper combines two well-understood algorithms, DP-SGD and PPO in a well-motivated task of reinforcement with human feedback and supervised fine-tuning.",
            "1": "The experiments and the results are clear and the application of private model fine-tuning is reasonable.",
            "2": "The paper is well organized and the writing is clear.",
            "3": "The paper is well written and the algorithm is clearly explained.",
            "4": "Their claims are based on the GPT-2 family of models and run experiments on the ROUGE metrics and the TweetEval benchmark.",
            "5": "The authors offer a privacy-preserving technique to undertake reinforcement learning with human feedback.",
            "6": "They combine DP-SGD and PPO with a few adaptations and show utility benefits on NLP benchmarks.",
            "7": "While the examples are helpful, the overall motivation could be a bit stronger.",
            "8": "What are we protecting and why?",
            "9": "What is the threat model around incorporating human feedback?",
            "10": "Are their examples of memorization from human feedback?",
            "11": "The experiments in the main body do not include error or number of trials details.",
            "12": "It is unclear in Table 1 why models with less provacy should do worse than those with more privacy (GPT-2 Medium, eps 4->8, or GPT-2 Large eps 8->Inf).",
            "13": "Such results demand further study and/or ablations and are difficult to interpret without confidence intervals.",
            "14": "The use of corporate imagery (Reddit / OpenAI) weakens the overall presentation and the generality of the results.",
            "15": "Work in differential privacy and RL can be traced to differentially-private policy evaluation (Balle, Gomrockchi, Precup).",
            "16": "The paper touches on the privacy accounting implications when  $T_{\\text{PPO}} \\neq 1$\n, but does not offer evaluate the implication of fixing it to the default value of 4."
        },
        "HKjGsy9YeW": {
            "0": "(1) The approach proposed in this paper for aligning language models with PPO in a privacy-preserving way is original; (2) The paper is clearly written and well-organized.",
            "1": "(3) The paper gives a quite comprehensive analysis of the procedure and emphasize the difficult issues in the implementation.",
            "2": "(1)The DP part is too condensed to understand.",
            "3": "The authors used DP-SGD on several occasions but without a clear explanation of this algorithm.",
            "4": "And in the main text, I could not find a concrete DP algorithm and a clear procedure how it is combined with the alignment.",
            "5": "(2) Algorithm 1 is not original.",
            "6": "I don’t see the reason why it was presented in detail in the paper.",
            "7": "The PRIVATE alignment should be more interesting."
        }
    },
    "LZT9T57Bg0": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical challenge in AI research: logical reasoning over large, noisy, and incomplete knowledge graphs (KGs).",
            "1": "- It introduces a novel approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), which leverages large language models (LLMs) for complex logical reasoning.",
            "2": "- The proposed method demonstrates significant performance improvements over state-of-the-art KG reasoning methods, particularly for complex queries.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-motivated and novel approach to KG reasoning, combining the strengths of graph extraction algorithms and LLMs.",
            "4": "- The experimental results show substantial performance gains across multiple benchmark datasets and various logical query constructs.",
            "5": "- The approach is generalizable and scalable, with performance improvements proportional to the size of the underlying LLM.",
            "6": "- The paper provides a comprehensive evaluation, including comparisons with multiple baselines and analysis of different query types.",
            "7": "Potential reasons for rejection\n   - **Complexity and clarity of the methodology:**\n     - The methodology section is dense and may be challenging for readers to follow, particularly those not familiar with the specific techniques and terminologies used.",
            "8": "- The logical chain decomposition and neighborhood retrieval processes are intricate and may benefit from additional clarification and simplification.",
            "9": "- **Limited analysis of failure cases:**\n     - The paper does not provide a detailed analysis of scenarios where the proposed approach underperforms or fails, which could be valuable for understanding its limitations.",
            "10": "- Specific reasons for the performance degradation in certain query types (e.g., 3in and pni) are not thoroughly explored.",
            "11": "- **Dependence on LLM token limits:**\n     - The approach's performance is heavily dependent on the token limits of the underlying LLMs, which may restrict its applicability in certain contexts.",
            "12": "- The paper acknowledges this limitation but does not provide concrete solutions or alternatives for handling larger token sizes.",
            "13": "Suggestions for improvement\n   - **Enhance clarity and accessibility:**\n     - Simplify the presentation of the methodology, particularly the logical chain decomposition and neighborhood retrieval processes, to make it more accessible to a broader audience.",
            "14": "- Include more illustrative examples and visual aids to help readers understand the key concepts and steps involved in the proposed approach.",
            "15": "- **Provide a detailed analysis of failure cases:**\n     - Conduct a thorough analysis of scenarios where the proposed approach underperforms or fails, and discuss potential reasons and solutions for these issues.",
            "16": "- Include case studies or examples of complex queries where the approach struggles, and explore ways to address these challenges.",
            "17": "- **Address token limit dependency:**\n     - Investigate and propose alternative methods or optimizations to handle larger token sizes, beyond relying on future LLMs with increased token limits.",
            "18": "- Explore techniques such as hierarchical decomposition or context-aware token reduction to mitigate the impact of token limits on performance.",
            "19": "- **Expand evaluation and comparison:**\n     - Include additional baseline methods and datasets in the evaluation to provide a more comprehensive comparison and validation of the proposed approach.",
            "20": "- Conduct ablation studies to isolate and analyze the contributions of different components of the LARK model, such as query abstraction and chain decomposition."
        },
        "P06KaFZB2U": {
            "0": "The paper outlined an exciting application of LLMs to improve neural logic reasoning on knowledge graphs.",
            "1": "Figure 1 vividly illustrates the procedure of using query prompts to get single query results from LLM.",
            "2": "In most of the experiments, the performances indeed greatly outperform the SOTA level.",
            "3": "Authors assume that LLMs have the ability to reason and ascribe the improvement of their system's performances to this.",
            "4": "However, whether LLMs can reason is still an open question.",
            "5": "See below.",
            "6": "C. Biever, The easy intelligence tests that AI chatbots fails, Nature 619 (2023) 686–689\nM. Melanie, How do we know how smart AI systems are?, Science 381 (6654) (2023) adj5957\n\nIf we look carefully at the experiment results, it is not the case the proposed system consistently greatly outperforms SOTA level.",
            "7": "In the FB15K dataset, CQD system outperforms the authors' system in 4 out of 9 tasks.",
            "8": "In the NELL995 dataset (in ip task, CQD scores 70.0, while the authors' system scores 29.3), CQD system outperforms the authors' system in 2 out of 9 tasks.",
            "9": "In the case of the negation query, BetaE outperforms authors' system in 2 out of 5 tasks in the FB15K datasets."
        },
        "z65hRhU93p": {
            "0": "Impressive results on NELL dataset.",
            "1": "This approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs.",
            "2": "Integration of LLMs: The performance of the proposed approach improves proportionally to the increase in size of the underlying LLM, enabling the integration of the latest advancements in LLMs for logical reasoning over KGs.",
            "3": "The example query in Figure 1 equals to 3i.",
            "4": "It is not a simplest form.",
            "5": "Maybe another query structure is better.",
            "6": "RQ3.",
            "7": "Only two sizes of LLMs are not sufficient, though we do observe a significant performance improvement on 13B Llama2.",
            "8": "The experiment to answer RQ4 is not a good design.",
            "9": "GPT3.5 is different from Llama2-7B and Llama2-13B.",
            "10": "It’s hard to tell the importance of token limit.",
            "11": "Because GPT3.5 outperforms various open-source LLMs (including Llama2-7B and Llama2-13B) on many reasoning benchmarks.",
            "12": "A proper way to study how the token limit affects reasoning performance is to vary token limit on one LLM.",
            "13": "For example, we could set 100%, 75%, 50%, 25% on max token limit of Llama2.",
            "14": "Lack of comparison with other LLM-based approaches: The paper only compares the proposed approach with state-of-the-art KG reasoning methods and does not compare it with other LLM-based approaches for KG reasoning."
        },
        "BXD0d6MmEr": {
            "0": "The proposed method is simple yet effective.",
            "1": "The obtained improvement in performance is significant.",
            "2": "The presentations and figures are clear and easy to follow.",
            "3": "The limitations of the proposed method are preliminarily discussed.",
            "4": "The technical novelty is neutral, as the paper seems to use LLM for FOL queries directly.",
            "5": "The writing of the paper can be largely improved.",
            "6": "The running-time efficiency of LARK is not reported.",
            "7": "The paper is empirically driven and lacks in-depth analysis, whether from methodological or theoretical perspectives.",
            "8": "Besides, the paper does not provide satisfying insights or underlying properties of the LARK model.",
            "9": "As the paper uses entities and relations in queries to find pertinent subgraph contexts, it would be better to discuss some relevant subgraph sampling methods on KG, e.g., AStarNet (Zhaocheng Zhu et al., NeurIPS 2023) and AdaProp (Yongqi Zhang et al., KDD 2023)."
        },
        "HvebEZQzcg": {
            "0": "- The proposed approach based on LLMs only observers a subset of the full KG with anonymized entity and relation IDs.",
            "1": "This allows the system to:\n    - Easily generalize to new KGs\n    - Scale to larger KGs (since the LLM only observes a k-hop subgraph)\n    - Most existing approaches for KG reasoning need all or part (one-hop link prediction model) to be fine-tuned for each KG\n- Authors demonstrate that step-by-step query execution performs significantly better than directly attempting to execute the complex query\n- Authors show that other LLMs also seem to handle the task, demonstrating that the performance is not specific to Llama-2 - The presentation of the paper lacks details of the evaluation protocol\n    - The paper evaluates the system on the queries generated by [1].",
            "2": "[1] highlights 2 types of answer entities: trivial answers (query can be directly executed on the available KG to gather answers) and non-trivial answers (queries require reasoning about missing edges).",
            "3": "- It is unclear if this paper reports results on the trivial, non-trivial, or combined subsets of answers.",
            "4": "- While the paper reports strong results on all benchmarks, there is no indication of the mechanism by which an LLM \"could\" solve the task.",
            "5": "- The results are especially surprising given that the LLM only observes anonymized entity and relation names and cannot apply any semantic reasoning\n    - The LLM only sees a small subset of the KG, so it does not have sufficient information to learn the semantics of the anonymized relations \n- The paper misses some relevant work in query execution on KGs\n    - [2] is a strong baseline that uses learned one-hop link prediction models for each KG and uses them to execute complex queries.",
            "6": "The method provides guarantees that the trivial answers will be predicted exactly and shows strong performance on the non-trivial answer entities\n    - Other competitive baselines include [3] and [4].",
            "7": "[2] provides MRR results for these approaches on the same datasets\n    - The MRR for the CQD baseline in this paper do not match the corresponding results in [2]\n\nI have reframed these weaknesses as a series of questions in the next section.",
            "8": "---\n[1] Ren, H., Hu, W., and Leskovec, J. Query2box: reasoning over knowledge graphs in vector space using box embeddings.",
            "9": "In International Conference on Learning Representations, 2020.",
            "10": "[2] Yushi Bai, Xin Lv, Juanzi Li, and Lei Hou.",
            "11": "2023.",
            "12": "Answering complex logical queries on knowledge graphs via query computation tree optimization.",
            "13": "In Proceedings of the 40th International Conference on Machine Learning (ICML'23)\n\n[3] Chen, X., Hu, Z., and Sun, Y.",
            "14": "Fuzzy logic based logical query answering on knowledge graphs.",
            "15": "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp.",
            "16": "3939–3948, 2022.",
            "17": "[4] Zhu, Z., Galkin, M., Zhang, Z., and Tang, J. Neuralsymbolic models for logical queries on knowledge graphs.",
            "18": "In Proceedings of the 39th International Conference on Machine Learning, volume 162, pp.",
            "19": "27454–27478, 2022."
        }
    },
    "3KDbIWT26J": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant question in the field of natural language processing: how large language models (LLMs) acquire translation capabilities without explicit parallel data.",
            "1": "- The study explores the impact of different types of unintentional bilingualism (sentence alignment, word alignment, and code-switching) on the translation abilities of LLMs.",
            "2": "- The findings provide new insights into the mechanisms behind the translation capabilities of LLMs, which could inform future research and development in multilingual language models.",
            "3": "Potential reasons for acceptance\n   - The paper tackles an important and timely research question that is relevant to the NLP community.",
            "4": "- The methodology is thorough and well-executed, involving both post-training and pre-training experiments to assess the impact of different types of bilingual data.",
            "5": "- The results are significant and provide new insights into the role of word alignment data in enabling translation capabilities in LLMs.",
            "6": "- The paper is well-written and clearly presents the research question, methodology, results, and implications.",
            "7": "Potential reasons for rejection\n   - **Limited scope of languages**:\n     - The study focuses primarily on English and Chinese, which may limit the generalizability of the findings to other language pairs.",
            "8": "- The impact of unintentional bilingualism on low-resource languages is not thoroughly explored.",
            "9": "- **Computational constraints**:\n     - The pre-training experiments are conducted on a smaller-scale LLM (BLOOM-560m) due to computational constraints, which may not fully capture the behavior of larger models.",
            "10": "- The inability to fully train the BLOOM-560m model may affect the reliability of the pre-training results.",
            "11": "- **Evaluation metrics**:\n     - The reliance on perplexity as a metric for translation capability in pre-training experiments may not fully capture the quality of translations.",
            "12": "- The use of COMET and BLEURT for post-training evaluation is appropriate, but additional metrics or human evaluations could strengthen the findings.",
            "13": "- **Data contamination**:\n     - The study acknowledges the presence of unintentional bilingualism in the pre-training corpus, but the extent and impact of this contamination on the results are not fully quantified.",
            "14": "- The methodology for identifying and removing unintentional bilingualism may not be exhaustive, potentially affecting the purity of the monolingual data used in experiments.",
            "15": "Suggestions for improvement\n   - **Expand language coverage**:\n     - Include additional language pairs, especially low-resource languages, to assess the generalizability of the findings.",
            "16": "- Conduct experiments on a more diverse set of languages to better understand the impact of unintentional bilingualism across different linguistic contexts.",
            "17": "- **Enhance computational resources**:\n     - Attempt to secure more computational resources to fully train larger models, which would provide more robust and reliable results.",
            "18": "- Consider using cloud-based or distributed computing solutions to overcome computational constraints.",
            "19": "- **Improve evaluation metrics**:\n     - Incorporate additional evaluation metrics, such as human evaluations or other automated metrics, to provide a more comprehensive assessment of translation quality.",
            "20": "- Explore the use of task-specific metrics that better capture the nuances of translation performance.",
            "21": "- **Address data contamination**:\n     - Provide a more detailed analysis of the extent and impact of data contamination on the results.",
            "22": "- Develop more rigorous methods for identifying and removing unintentional bilingualism to ensure the purity of the monolingual data used in experiments.",
            "23": "- **Clarify methodology and results**:\n     - Include more detailed descriptions of the data collection and processing pipeline to enhance reproducibility.",
            "24": "- Provide additional analysis and discussion of the results, particularly regarding the unexpected findings related to word alignment data."
        },
        "efI5Unca3A": {
            "0": "The paper innovatively delved into the emergent translation abilities of LLM by analyzing its training data composition.",
            "1": "The evaluation methods have its limitations.",
            "2": "There are flaws in the methodology, suggesting that the prompts might influence the results.",
            "3": "At its core, it's an ablation study at the data level."
        },
        "soiLX4dfXh": {
            "0": "The underlying question has been addressed before, suggesting that the presence of incidental sentence alignment in \"supposedly\" monolingual data plays a large role in LLM performance on MT.",
            "1": "This submission pushes the investigation by looking at word-level alignment and code-switched data which as far as I know is novel.",
            "2": "The positive impact of word alignment is demonstrated in a number of experiments showing improvements in either translation quality or perplexity.",
            "3": "The hypothesis that the larger amount of word alignment data (as compared to sentence-aligned) allows it to be as useful as sentence-aligned data is well supported by the experiments.",
            "4": "MT quality is evaluated using neural metrics rather than BLEU, which aligns with recent results from the WMT22 metrics task (https://aclanthology.org/2022.wmt-1.2/).",
            "5": "Some results are presented with significance tests of the differences, yay!",
            "6": "The UBD (sentence & word aligned + code switched) is extracted using an automated methods with arbitrary parameters that seem fairly ad hoc (eg 10 BLEU poins threshold, appdx A).",
            "7": "This raises the question of how good that data is... Are sentence-aligned segments even aligned sentences?",
            "8": "At 10 BLEU point, this is not very clear.",
            "9": "This data is at the core of the argument of the paper, better quality control would make a more convincing case.",
            "10": "Experiments use mostly smaller models, as well as surrogate methods, and some models are not even converged.",
            "11": "Clearly the amount of computation is significant.",
            "12": "However, would one make conclusion on a chemical process from a reaction that has not completed?",
            "13": "These imperfect or incomplete experimental conditions make the conclusions less convincing.",
            "14": "The switch to perplexity does not help -- the claims in Section 5.3 are only mildly convincing if it is not possible to extract minimally useful translations from the models.",
            "15": "Section 5.4 would benefit from a comparison with a model using UBD data somehow -- this would allow to gauge whether the performance in Table 8 is getting remotely close to acceptable translation quality."
        },
        "8VSwKLyzGm": {
            "0": "- Extensive experiments are conducted, to address the main question of \"Why can multilingual large language model can learn to translate without parallel data?\"",
            "1": "- The authors provide detailed empirical analyses to identify the underlying mechanism of translation capability in multilingual large language model.",
            "2": "They also discuss the cases where single monolingual data are available with different sizes.",
            "3": "This part might be helpful on how to improve low-resource language translation quality in the language model\n- The paper is well organized and clearly described in most parts.",
            "4": "- The authors focus on English and Chinese data to identify unintentional bilingualism types, and I was wondering if any other bilingualism types exist when you check the other languages.",
            "5": "- This paper looks interesting in light of the empirical analyses of bilingualism's role in translation capabilities; however, the question still remains on how to effectively enhance translation capability further against the supervised translation models."
        },
        "vkLY9MCt1u": {
            "0": "* The findings from the paper are interesting and may shed light on the amazing translation ability from LLM.",
            "1": "I enjoy reading the work a lot.",
            "2": "My biggest concern about the work is that while it is true that maybe the presence of sentence alignment, word alignment and code-switching data contribute an important role in explaining this amazing ability of multilingual LLMs.",
            "3": "We just don't know how important they are and is there any other reason (e.g.",
            "4": "the presence of other stuff) that is actually even way more important than the three.",
            "5": "I don't think the paper provide any data points on this.",
            "6": "Another smaller concern is that I see some paragraphs are just half-baked from curiosity perspective.",
            "7": "For instance, section 5.4 shows that after eliminating unintentional bilingual data, the translation ability is still there, apparently.",
            "8": "But there is no data points on why that happens, just some postulating about the reasons and that is it.",
            "9": "The final weakness point of the paper to me is the presentation in Section 5.5.",
            "10": "I could not follow exactly what \"the shared transformer layers in the BLOOM-560m model\".",
            "11": "To make the work self-contained I think the paper should at least present some high level details about the shared transfomer layers before presenting how this may influence the effect of unintentional bilingualism."
        }
    },
    "gLARhFLE0F": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the field of NLP, specifically the memory wall problem during the generation phase of large-scale generative language models.",
            "1": "- The proposed LUT-GEMM kernel introduces a novel approach to quantized matrix multiplication that eliminates the need for resource-intensive dequantization processes, thereby reducing computational costs.",
            "2": "- The paper's contribution is significant as it offers a practical solution to accelerate token generation latency in large language models, which is a pressing need in the industry.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-defined problem and offers a clear and innovative solution.",
            "4": "- The experimental results demonstrate substantial improvements in token generation latency, providing strong evidence of the effectiveness of the proposed method.",
            "5": "- The authors provide a comprehensive evaluation of their method, including comparisons with existing approaches and detailed analysis of the trade-offs involved.",
            "6": "- The paper includes a reproducibility statement and supplementary materials, which enhance the credibility and transparency of the research.",
            "7": "Potential reasons for rejection\n   - **Limited focus on single-batch inference:**\n     - The method primarily focuses on single-batch inference, which may limit its applicability in scenarios where larger batch sizes are required.",
            "8": "- The performance gains may diminish as the batch size increases, which could be a significant limitation for some use cases.",
            "9": "- **Dependence on specific hardware:**\n     - The proposed method relies on GPU-specific hardware utilization, which may not be applicable or efficient on other types of hardware.",
            "10": "- The paper does not provide a detailed discussion on the potential limitations or challenges of implementing the method on different hardware platforms.",
            "11": "- **Complexity of implementation:**\n     - The implementation of the LUT-GEMM kernel may be complex and require significant modifications to existing systems.",
            "12": "- The paper does not provide sufficient details on the potential challenges and solutions for integrating the proposed method into existing NLP frameworks.",
            "13": "Suggestions for improvement\n   - **Expand the scope to include larger batch sizes:**\n     - Investigate and report the performance of the proposed method with larger batch sizes to provide a more comprehensive evaluation of its applicability.",
            "14": "- Explore potential optimizations or modifications to improve performance for larger batch sizes.",
            "15": "- **Discuss hardware limitations and alternatives:**\n     - Provide a detailed discussion on the potential limitations and challenges of implementing the method on different hardware platforms.",
            "16": "- Explore and report the performance of the proposed method on a variety of hardware configurations to enhance its generalizability.",
            "17": "- **Simplify implementation and integration:**\n     - Provide more detailed guidelines and best practices for implementing and integrating the LUT-GEMM kernel into existing NLP frameworks.",
            "18": "- Include case studies or examples of successful integration to demonstrate the practicality and ease of adoption of the proposed method."
        },
        "hWhffvaKQ8": {
            "0": "Clear ideas and descriptions of representing weights with scaling factor matrices and binary weight matrices.",
            "1": "The advantage of using LUT and grouped scaling factors is extensively evaluated.",
            "2": "The way of leveraging fine-grained scaling factors enabled a non-uniform quantization format.",
            "3": "Solid implementation.",
            "4": "The analysis and GPU implementation stand this idea out.",
            "5": "I understand the importance of having it work on GPUs but I still wonder how efficient is the implemented kernel as compared to the customized hardware accelerator.",
            "6": "Plausible ablation studies.",
            "7": "There are a lot of ablation studies to show the effectiveness of every proposed component.",
            "8": "One big issue in writing that prevents one from reimplementing the proposed method is how to construct the binary matrices and scaling factor matrices from the pre-trained weights.",
            "9": "The attached code assumes randomized matrices so that part cannot provide any useful information regarding this.",
            "10": "For more technical questions:\n\n1.",
            "11": "Regarding the LUT part, what is the repetition that we can exploit form?",
            "12": "The quantitative measurements or ablation studies on this will provide more insights into how many benefits we can get for such LUT-based GEMM instead of vanilla additions.",
            "13": "Any theoretical analysis on the upper bound of savings brought from LUT?",
            "14": "Regarding the scaling factor part, how to determine the group size?",
            "15": "In my understanding, if the group size is small, there are no benefits as you have as many matrices as used bits to represent weights.",
            "16": "At what group size, do the benefits compensate the cost of the scaling factor matrices?",
            "17": "Is there any formula to calculate this?",
            "18": "Regarding the experiments, why only consider the OPT model?",
            "19": "How about the speedups on other candidates like LLaMa at different scales?",
            "20": "In that way, we can also understand the scalability of the used kernel better."
        },
        "9ZSSNiVgpa": {
            "0": "The work of this paper is of great significance, especially for optimizing the inference performance of LLM, which is currently highly concerned.",
            "1": "Compared to the baseline cuBLAS implementation, the performance of this method has significantly improved.",
            "2": "The design, experiment, and analysis of this paper are relatively solid.",
            "3": "For the algorithm design of GPU, this paper provides detailed explanations and time complexity analysis.",
            "4": "This paper carefully considers the performance of different compression ratios under different bit and grouping parameters g. \n3.",
            "5": "CUDA kernel code and performance test code is provided.",
            "6": "The novelty of this paper is ordinary.",
            "7": "The main parts, including BCQ quantification and group-wise quantization optimization, both come from existing research.",
            "8": "This paper proposes to convert uniform quantization into BCQ format, which is not a complex transformation.",
            "9": "The author mentioned that the optimization here currently only focuses on the inference of a single batch, which may limit its use.",
            "10": "The LUT-Based method requires a significant amount of memory capacity in exchange for efficiency.",
            "11": "In Table 2, the 4-bit quantified LUT-GEMM storage footprint exceeds the 16 bit model of the cuBLAS baseline.",
            "12": "In fact, storage resources are also the main focus of quantization in large language models, not just performance.",
            "13": "This paper seems to focus mainly on computational efficiency, but lacks a comparison between memory resource usage.",
            "14": "The experimental data in this paper is not enough, and the main baseline for comparison is 16 bit cuBLAS, lacking a comprehensive comparison of other quantization methods.",
            "15": "The experiment only tested the quantization accuracy of PPL indicators using the OPT model on the LAMBADA dataset, which cannot fully demonstrate the effectiveness and universality of the method."
        },
        "Pk1RmtmQ32": {
            "0": "The core idea of the paper makes sense and in fact is more general and beyond the BCQ format used this paper.",
            "1": "The key question on rating the originality is the relation to prior works.",
            "2": "For example, how does this work relates to BiQGEMM (Jeon et al., 2020)?",
            "3": "Section 3.1 does acknowledge this and a few other prior works but it's unclear what are the distinctions.",
            "4": "If a satisfactory answer could be provided, then originality could be a big strength for this paper.",
            "5": "The experimental setup is good, particularly that perplexity test is included.",
            "6": "Including source code is a big plus, although I have not verified it.",
            "7": "As said above, the originality question is the key.",
            "8": "Please provide detailed narrative on the differentiation.",
            "9": "Including prior kernels in evaluations would be even stronger.",
            "10": "The last row of Table 2 suggests 4X speed up with 4-bit quantization at kernel level.",
            "11": "However this does not seems to translate to the 4-bit end2end latency in Tables 3 and 4, not anywhere close to 4X.",
            "12": "Can you explain why?",
            "13": "A critical property of the proposal is that the benefit gets larger as the matrix dimension increases.",
            "14": "Right now the results only show end2end latency for OPT 66B and 175B.",
            "15": "Could you add a plot of latency as a function of OPT model sizes, all the way from the smallest to 175B?"
        },
        "j3lGWF117I": {
            "0": "Extensive empirical evidence presented through various data visualizations confirms the efficiency of LUT-GEMM method.",
            "1": "The research goes further than most in addressing the real-world deployment challenges on standard hardware, which is a standout feature.",
            "2": "Impressive experiment section focusing on the largest open-source models and fitting them onto a single GPU.",
            "3": "Detailed latency insights provided in the paper lay a solid foundation for understanding the efficiency gains.",
            "4": "Well-written motivation and related work sections.",
            "5": "Discussions show a deep understanding of GPU programming.",
            "6": "The achievement of deploying huge models on a singular GPU is undermined by the lack of reported accuracy metrics at this scale.",
            "7": "The paper lacks a direct accuracy and latency comparison among different models, although the OPT model family is a good choice.",
            "8": "The paper does seem to have some limited technical novelty since it adapts and extends prior method in minor to moderate ways, but the analysis and implementation seem to make up for it."
        }
    },
    "BifeBRhikU": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical challenge in the deployment of Large Language Models (LLMs) on memory-constrained devices by proposing a novel approach to network binarization.",
            "1": "- The introduction of Partially-Binarized LLM (PB-LLM) is a significant advancement, as it aims to achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of LLMs.",
            "2": "- The paper's exploration of both post-training quantization (PTQ) and quantization-aware training (QAT) for PB-LLM is a novel contribution to the field of model compression.",
            "3": "Potential reasons for acceptance\n   - The proposed PB-LLM method demonstrates substantial improvements in the performance of low-bit quantized LLMs, as evidenced by the experimental results.",
            "4": "- The paper provides a comprehensive analysis of the ineffectiveness of existing binarization methods for LLMs and offers a well-justified solution.",
            "5": "- The methodology is well-documented, with clear explanations of the salient weight selection criteria, granularity, and storage costs.",
            "6": "- The experimental setup and results are robust, covering a range of tasks and models, including OPT-1.3B and LLaMA-7B.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in some sections:**\n     - The explanation of the scaling mechanism and its derivation could be more detailed to enhance understanding.",
            "8": "- The description of the experimental setup, particularly the training details, could be more explicit to ensure reproducibility.",
            "9": "- **Limited comparison with other state-of-the-art methods:**\n     - While the paper compares PB-LLM with some existing binarization methods, a more extensive comparison with other recent quantization techniques could strengthen the evaluation.",
            "10": "- The paper could benefit from a discussion on how PB-LLM compares with other advanced compression methods beyond binarization.",
            "11": "- **Scalability and generalization concerns:**\n     - The experiments are primarily conducted on OPT-1.3B and LLaMA-7B.",
            "12": "It would be beneficial to see results on even larger models to demonstrate scalability.",
            "13": "- The paper does not address potential challenges in generalizing the proposed method to other types of neural networks beyond LLMs.",
            "14": "Suggestions for improvement\n   - **Enhance clarity and detail in methodology:**\n     - Provide a more detailed explanation of the scaling mechanism and its derivation, including mathematical formulations and examples.",
            "15": "- Include more explicit details on the experimental setup, such as the specific hyperparameters used, to ensure reproducibility.",
            "16": "- **Expand comparison with state-of-the-art methods:**\n     - Conduct a more extensive comparison with other recent quantization techniques and advanced compression methods to provide a broader context for the proposed approach.",
            "17": "- Include a discussion on the advantages and limitations of PB-LLM compared to these methods.",
            "18": "- **Address scalability and generalization:**\n     - Conduct experiments on larger models and different types of neural networks to demonstrate the scalability and generalization of PB-LLM.",
            "19": "- Discuss potential challenges and solutions for applying PB-LLM to other neural network architectures.",
            "20": "- **Improve presentation and readability:**\n     - Ensure that all figures and tables are clearly labeled and referenced in the text.",
            "21": "- Consider adding more visual aids, such as flowcharts or diagrams, to illustrate complex concepts and processes.",
            "22": "- Proofread the manuscript for grammatical errors and improve the overall readability of the text."
        },
        "NqTeCSfDES": {
            "0": "+ Presents a practical approach that analytically identifies non-salient weights and applies binarization selectively for large models.",
            "1": "+ Achieves LLM binarization without sacrificing performance gains.",
            "2": "- The proposed approach section is comprehensive, but its complexity makes it challenging to navigate and comprehend throughout the entire section.",
            "3": "- The analysis of evaluation is limited to a single task.",
            "4": "It would be valuable to explore the potential limitations of PB-LLM in achieving comparable performance across various tasks."
        },
        "djmyVVuIaL": {
            "0": "- The paper is well-written and well-motivated.",
            "1": "- The proposed PB-LLM scheme is easy to follow and straightforward to understand.",
            "2": "- Exploring to improve the memory and/or storage efficiency using quantization (and algorithmic approximation in general) of LLMs is a promising research direction.",
            "3": "- Only Llama 7B is studied as the LLM for PB-LLM and all other baselines.",
            "4": "Thus, it's not clear how the PB-LLM method performs on larger-scale models.",
            "5": "- Only pre-trained base models are experimented with those models, however, are usually not deployed directly as applications.",
            "6": "- The LLM quantization scheme is motivated using an angle of GPU memory efficiency.",
            "7": "However, the actual GPU memory usage before and after binarization/quantization is not studied in this paper."
        },
        "0oBMzfPunQ": {
            "0": "- The paper provides a well-structured presentation of the preliminaries of binary quantization to the introduction of the proposed method, which made it easy to follow.",
            "1": "- Building on previous research such as AWQ and SparseGPT, this paper proposes a partial binarization technique through salient weight protection based on the hessian information and effectively demonstrates its efficacy in PTQ with ablation study (as shown in Table 1).",
            "2": "- The paper showcases empirical improvements in QAT optimization (higher accuracy with fewer training steps than LLM-QAT) across the CSQA tasks.",
            "3": "Major Concerns\n- Lack of novelty: The authors propose the optimal scaling factor as their primary contribution, but the core idea itself seems to have already been proposed in the previous work.",
            "4": "For example, [R1] proposed an optimal ternary function (eq.",
            "5": "(3) of [R1]), but it can be trivially reduced to the binary function when the threshold is zero; then the equation seems to be identical to the equation (8) and (9) proposed in this paper.",
            "6": "[R1] Li te tal., Ternary Weight Networks\n\n\n- Lack of Evaluation Task: This paper evaluates the reasoning capability of LLM only through the accuracy of the CSQA task.",
            "7": "In the CSQA task, tasks such as OBQA and ARC challenge were used for OPT-1.3B, where the FP performance did not reach even the random baseline (25%).",
            "8": "It raises questions about the suitability of these tasks for demonstrating the effectiveness of fine-tuning, and, hence, the superiority of PB-LLM.",
            "9": "To show the effects of fine-tuning more clearly, it would be advisable to carefully select reasoning tasks that are appropriate for the model capacity.",
            "10": "Reporting performance not just on CSQA, but also on multi-task accuracy like MMLU would be also beneficial for highlighting PB-LLM's efficacy.",
            "11": "- Inconsistent Salient Weight Methodology between PTQ and QAT: The absence of a consistent methodology for salient weight protection between PTQ and QAT is concerning.",
            "12": "While the effectiveness of using Hessian criteria for identifying salient weights in PTQ is demonstrated through performance comparisons, the rationale for using magnitude criteria to identify salient weights in QAT seems to be missing.",
            "13": "Understanding the disparity in the approach to salient weight protection across PTQ and QAT is crucial for a holistic appreciation of the proposed method.",
            "14": "- Insufficient evidence on PB-LLM efficiency: To claim that PB-LLM is more efficient in terms of training iteration number compared to LLM-QAT, a more thorough comparison seems necessary.",
            "15": "Specifically, it needs to be clear whether the LLM-QAT, being compared with PB-LLM, has been fine-tuned on the same dataset as PB-LLM.",
            "16": "Detailed experimental setup information regarding the LLM-QAT is required.",
            "17": "Moreover, verification is needed on whether the results through PB-LLM QAT have fully enhanced the reasoning capacity of the pre-trained model.",
            "18": "Essentially, it appears that the reasoning accuracy of the target model (OPT-1.3B) obtained through FP fine-tuning should be presented as the upper bound in Figure 7.",
            "19": "Additionally, there seems to be a lack of information in Table 2 regarding whether FP LLaMA-7B performance is pre-trained or fine-tuned.",
            "20": "Minor Concerns\n- Typo: Sec 3.3 bianrize -> binarize\n- Consistent notation should be used in Sec 4.1 -> LLaMA, LLaMa -> LLaMA\n- There may be an incorrect reference link in Sec 4.1, \"showing its fast convergence property (refer to 3.2)\" Should it possibly be corrected to \"refer to 3.4?\"",
            "21": "- There are spacing issues in the Figure 7 caption, \"LLMin\" should be \"LLM in\" and \"PM-LLMtriumphs\" should be \"PB-LLM triumphs\"."
        },
        "T5IFyC96el": {
            "0": "Compressing LLMs is an important question for today’s AI research, and the authors first introduce binarization into LLM compression pushing the quantized LLM into ultra-low bits.",
            "1": "The authors present a thorough exploration of network binarization techniques.",
            "2": "They effectively demonstrate the feasibility and potential of partially-binarized LLMs using post-training quantization and quantization-aware training methodologies.",
            "3": "The inclusion of source code with the submission is commendable, enabling reproducibility and verification of the reported results, which are impressive.",
            "4": "While the incorporation of Quantization-Aware Training (QAT) in LLM compression is an interesting proposal, its practicality is uncertain given the substantial costs associated with training LLMs.",
            "5": "Could the authors elaborate on the overhead implications of QAT for LLMs?",
            "6": "In regards to Table 2, it is unclear whether GPTQ-PB represents the method proposed by the authors.",
            "7": "Could you clarify the distinction between GPTQ-PB and PB-LLM within the context of your study?",
            "8": "The application of optimal scaling techniques appears to be confined to the specific case presented.",
            "9": "Could these techniques be generalized to other bit configurations, and if so, how might this affect the compression performance?"
        }
    },
    "EyDPfGy4Wh": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the field of large language models (LLMs) by proposing a novel method, Expert Projection Attention (EPA), to reduce the computational and memory requirements of Transformers.",
            "1": "- The novelty lies in the use of Mixture-of-Experts (MoE) layers for value and output projections, which significantly reduces the number of attention matrices needed, leading to substantial resource savings without compromising performance.",
            "2": "Potential reasons for acceptance\n   - The proposed method, EPA, demonstrates significant reductions in computational and memory requirements while maintaining or even improving performance compared to baseline Transformers.",
            "3": "- The paper provides extensive experimental validation across multiple datasets and model sizes, showing the robustness and generalizability of the proposed method.",
            "4": "- The combination of EPA with MoE MLP layers to create a \"Fast Transformer\" is a compelling advancement, offering a comprehensive solution for resource-efficient LLMs.",
            "5": "- The paper includes detailed analysis and comparisons with related methods, highlighting the advantages of EPA in terms of both performance and resource efficiency.",
            "6": "Potential reasons for rejection\n   - **Clarity and readability of the method description**\n     - The explanation of the EPA mechanism, particularly the mathematical formulations and the selection process for experts, could be more clearly articulated.",
            "7": "- The transition from the standard multi-head attention to the proposed EPA might be confusing for readers not deeply familiar with the underlying concepts.",
            "8": "- **Experimental setup and reproducibility**\n     - While the paper mentions the use of a public GitHub repository upon acceptance, the current lack of access to the code and detailed hyperparameters might hinder reproducibility.",
            "9": "- The parameter-matching procedure, although systematic, could benefit from a more detailed explanation to ensure that other researchers can replicate the experiments accurately.",
            "10": "- **Comparative analysis with other methods**\n     - The comparison with other MoE-based attention methods, such as MoA, could be expanded to include more detailed discussions on the trade-offs and specific scenarios where EPA outperforms or underperforms.",
            "11": "- The paper could include more ablation studies to isolate the impact of different components of EPA, such as the number of experts and the specific projections that benefit most from the MoE approach.",
            "12": "Suggestions for improvement\n   - **Enhance clarity and readability**\n     - Provide a more intuitive explanation of the EPA mechanism, possibly with additional diagrams or step-by-step illustrations to help readers grasp the concept more easily.",
            "13": "- Simplify the mathematical notations and ensure that all variables and terms are clearly defined and consistently used throughout the paper.",
            "14": "- **Improve experimental setup and reproducibility**\n     - Include a detailed appendix with all hyperparameters, training settings, and any specific implementation details that are crucial for reproducing the results.",
            "15": "- Ensure that the GitHub repository is well-documented and includes example scripts for running the experiments and reproducing the key results presented in the paper.",
            "16": "- **Expand comparative analysis**\n     - Conduct more comprehensive comparisons with other state-of-the-art methods, including both MoE-based and non-MoE-based approaches, to provide a clearer picture of where EPA stands in the broader context.",
            "17": "- Include additional ablation studies to explore the impact of different design choices within EPA, such as varying the number of experts, the selection mechanism, and the specific projections that are made conditional.",
            "18": "- **Address potential limitations and future work**\n     - Discuss any potential limitations of EPA, such as scenarios where it might not perform as well or specific challenges in scaling it to even larger models or datasets.",
            "19": "- Outline possible directions for future research, such as further optimizations, extensions to other types of attention mechanisms, or applications beyond language modeling."
        },
        "Y8SbFOYFze": {
            "0": "the method is clearly illustrated and the analysis in 2.3 is helpful for understanding the difference.",
            "1": "The author seems to misunderstand the position of flash-attention, see questions below.",
            "2": "2. the scale of experiment is small, how would this method generalize to larger models such as llama?",
            "3": "Some experiments have not been finished (Table 4)."
        },
        "ZdjmMUtzMD": {
            "0": "The idea at a high level looks decent.",
            "1": "However, the poor writing and underwhelming evaluation really makes it hard to appreciate it.",
            "2": "Please see the summary."
        },
        "HvwP4taCax": {
            "0": "The paper is well-written and effectively highlights the issues with the current attention architecture in terms of computational and memory demands.",
            "1": "The paper conducts experiments on various datasets and compares its results with existing baseline methods, including MOA.",
            "2": "The paper conducts a thorough analysis of attention maps to facilitate a qualitative study and comparisons with conventional attention matrices.",
            "3": "The paper refers to FlashAttention multiple times and compares against their CUDA kernel (SW designed to exploit HW efficiently) optimization vs algorithmic insight in this paper.",
            "4": "I am not sure if its an apple-to-apple comparison since there are tons of other literature for transformers which aim to reduce computation/memory cost (like quantization/sparsity methods) and the paper doesn’t compare against these.",
            "5": "While authors compare against FlashAttention custom kernel implementation and mention that as a drawback, EPA algorithm itself requires a custom CUDA kernel with its own set of restrictions (pointed in the results section).",
            "6": "For the EPA algorithm, the paper mentions that K/Q source experts are not necessary for good results and only output/value experts are required, which seems to contradict the disadvantages shown in 2.2 naive algorithm."
        }
    },
    "27YiINkhw3": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the use of large language models (LLMs) with external tools, specifically the generation of syntactically invalid tool calls.",
            "1": "- The proposed TOOLDEC algorithm introduces a novel finite-state machine-guided decoding approach to ensure syntactically correct tool calls, which is a significant advancement over existing methods.",
            "2": "- The approach is innovative in its ability to generalize to new tools without the need for additional training or in-context documentation, which is a notable improvement over current fine-tuning and in-context learning methods.",
            "3": "Potential reasons for acceptance\n   - The paper presents a clear and well-motivated problem statement, addressing a known limitation in the current use of LLMs with external tools.",
            "4": "- The proposed solution, TOOLDEC, is novel and demonstrates significant improvements in eliminating syntax errors and enhancing performance across various benchmarks.",
            "5": "- The experimental results are comprehensive and show that TOOLDEC outperforms existing methods in terms of accuracy, inference time, and generalization to unseen tools.",
            "6": "- The paper is well-structured, with detailed explanations of the methodology, experiments, and results, making it easy to follow and understand.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in some technical details:**\n     - The construction of the finite-state machine (FSM) from tool documentation and API signatures could be explained in more detail, particularly the automatic construction pipeline mentioned in Appendix A.9.",
            "8": "- The process of integrating TOOLDEC with different base models (ToolLLM and ToolkenGPT) could be elaborated further to provide a clearer understanding of the implementation.",
            "9": "- **Limited discussion on potential limitations and future work:**\n     - The paper does not thoroughly discuss the potential limitations of TOOLDEC, such as its applicability to a broader range of tools beyond those tested in the experiments.",
            "10": "- There is a lack of discussion on future work and how TOOLDEC could be further improved or extended to address any remaining challenges.",
            "11": "- **Insufficient comparison with other state-of-the-art methods:**\n     - While the paper compares TOOLDEC with ToolLLM and ToolkenGPT, it would benefit from a more extensive comparison with other state-of-the-art methods in tool-augmented LLMs.",
            "12": "- The inclusion of additional baselines and a more detailed analysis of the differences in performance would strengthen the evaluation.",
            "13": "Suggestions for improvement\n   - **Enhance clarity in technical details:**\n     - Provide a more detailed explanation of the FSM construction process, including specific examples and a step-by-step description of the automatic construction pipeline.",
            "14": "- Elaborate on the integration process of TOOLDEC with different base models, including any challenges faced and how they were addressed.",
            "15": "- **Discuss potential limitations and future work:**\n     - Include a section discussing the potential limitations of TOOLDEC, such as its scalability to a larger number of tools or its performance with more complex tool APIs.",
            "16": "- Outline possible future research directions, such as exploring the use of TOOLDEC with other types of tools or further optimizing the FSM construction process.",
            "17": "- **Expand the comparison with other methods:**\n     - Include additional state-of-the-art baselines in the experimental evaluation to provide a more comprehensive comparison of TOOLDEC's performance.",
            "18": "- Conduct a more detailed analysis of the differences in performance between TOOLDEC and other methods, highlighting the specific advantages and areas for improvement.",
            "19": "- **Improve the presentation of experimental results:**\n     - Provide more visual aids, such as graphs and charts, to better illustrate the performance improvements achieved by TOOLDEC.",
            "20": "- Include more qualitative examples of tool calls generated by TOOLDEC compared to other methods to highlight the practical benefits of the proposed approach."
        },
        "fUF6TEOv3u": {
            "0": "This paper shows that LLMs still require an external knowledge to constrain the search space for tool use and existing methods such as finetuning and in-context learning are not enough.",
            "1": "It shows that the type of errors (syntax errors) can be addressed by an adoption of a simple FSM.",
            "2": "It is shown that it is true for the settings the method was tested for.",
            "3": "Novelty\n\nIt is essentially about constraining the search space of a language model by a grammar.",
            "4": "It is definitely expected that the use of a grammar can reduce syntax errors if we know that the output needs to follow the grammar.",
            "5": "I feel that it is a known technique but not a novel finding although probably it has not been applied for LLMs yet.",
            "6": "It does not necessarily need to be theoretical, but I would probably at least want to see deeper discussions on why LLM has limitations without such external knowledge.",
            "7": "Does a much stronger LLM have the same problem?",
            "8": "Complexity\n\nA good thing about LLMs is that the input and output are both plain text and the mechanism is very simple.",
            "9": "This technique is against the simplicity.",
            "10": "It says the FSM can be automatically constructed, but I am not sure if it is always the case for more complex tools.",
            "11": "Defining FSM manually could be tedious and error-prone for complex ones.",
            "12": "Decoding with an external FSM will add additional complexity to the system although this could be standardized by for example open tools."
        },
        "WXy1b0neiK": {
            "0": "* Experimental validation that enforcing prefix-checkable constraints on generation can result in more effective tool use for a relatively large set of tools.",
            "1": "* The decoding approach is validated for several different LLM (ToolkenLLM, RestGPT, and ToolLLM), and appears to improve the in-context learning ability of the LLM (S4.3).",
            "2": "* Approach maintains levels of accuracy even with increasing numbers of unseen tools in the test set (Figure 5).",
            "3": "* Unclear why general machinery of FSM is necessary when the approach amounts to constrained decoding using prefix-checkable constraints on next token generation.",
            "4": "The paper states “Note that in practice, it’s not necessary to explicitly construct this FSM.",
            "5": "Any grammar checker that tells the set of valid next tokens suffice.” Perhaps there could be better motivation for using FSM?",
            "6": "* In some ways, the approach seems like a step backwards to expert-based AI, in that the improvements from the proposed approach appear largely to be the result of hand-crafting decoding constraints.",
            "7": "* Related to the above concern, it’s unclear how the proposed approach was validated.",
            "8": "Was the hand-crafted decoding approach tailored to perform on test data?"
        },
        "x2jWzW8eLW": {
            "0": "- The FSM guided decoding method is intuitive and suitable for solving the syntax errors.",
            "1": "- The proposed method is compatible with existing LLM tool-use schemes, i.e., both finetuning or in-context learning.",
            "2": "- The method has shown to be empirically effective in eliminating syntax error, and leads to performance improvements.",
            "3": "- The FSM construction may require careful curation.",
            "4": "For example, how does one decide what's the best naming for a tool?",
            "5": "Are LLMs robust to the name changes?",
            "6": "Also, what would the process be like for one to construct the FSMs for a large collection of tools?",
            "7": "Would it be done through parsing the tool documentations?",
            "8": "It'd be helpful if the authors provide more discussion here.",
            "9": "- It is not clear to me as to how ToolDec can enable generalization to new tools?",
            "10": "While adding new FSM (for the new tool) can ensure the LLM uses the new tool in a syntactically correct way, the FSM itself does not provide sufficient information on when the tool should be invoked.",
            "11": "Current generalization then seems to only depend on LLM's language prior, and thus related to above, it's tool use performance can largely depend on the proper naming of the tools.",
            "12": "- Following from above, it'd be interesting to see an experiment testing the robustness of ToolDec by assigning tool names that not are not semantically meaningful."
        },
        "vGMdDo1lcz": {
            "0": "Strengths:\nThis paper proposes the finite-state machine-guided decoding algorithm, which reduces the errors during calling tools.",
            "1": "It is a clear and simple method to restrict the decoding space.",
            "2": "The experimental results show that the method is effective in the tool learning task, which significantly reduces name errors.",
            "3": "Weaknesses:\nEven though the model achieves significant improvements, it is unclear the language and tool mechanism switching.",
            "4": "I think the switching effectiveness should be evaluated and whether the <T> token can be appropriately decoded.",
            "5": "The augment errors are zero.",
            "6": "However, this reason may lie in that the existing tool learning benchmark is a little easy.",
            "7": "If the input is a more complex problem and contains several numbers, the argument can also be wrong.",
            "8": "The zero error rate should be carefully claimed."
        }
    },
    "MNShbDSxKH": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces GENOME, a generative neuro-symbolic visual reasoning model that grows and reuses modules, which is a novel approach in the field of visual reasoning.",
            "1": "- The model aims to address inefficiencies in existing neuro-symbolic models by reusing and growing modules instead of generating new code snippets for each task instance.",
            "2": "- The approach is inspired by human learning processes, emphasizing knowledge accumulation and reuse, which is a significant step towards more efficient and scalable AI models.",
            "3": "Potential reasons for acceptance\n   - The proposed model demonstrates competitive performance on standard visual reasoning tasks such as visual question answering and referring expression comprehension.",
            "4": "- GENOME shows strong transfer learning capabilities, effectively applying learned modules to new tasks like image editing and knowledge tagging.",
            "5": "- The model exhibits the ability to adapt to new visual reasoning tasks with minimal training examples, showcasing its flexibility and robustness.",
            "6": "- The paper provides a comprehensive evaluation of the model across multiple benchmarks, supporting the claims with empirical evidence.",
            "7": "Potential reasons for rejection\n   - **Complexity and Scalability**\n     - The model's reliance on LLMs for module generation and execution may introduce significant computational overhead, potentially limiting its scalability.",
            "8": "- The process of module initialization, generation, and execution involves multiple stages and interactions with LLMs, which could be seen as overly complex and difficult to implement in practice.",
            "9": "- **Dependence on Pre-defined Modules**\n     - The initial set of pre-defined modules and APIs is crucial for the model's performance.",
            "10": "If these modules are not comprehensive enough, the model may struggle with certain tasks.",
            "11": "- The need for manually defining and implementing initial modules contradicts the goal of reducing labor-intensive processes, potentially limiting the model's applicability.",
            "12": "- **Generalization and Robustness**\n     - While the model shows promising results on specific benchmarks, its generalization to a wider range of visual reasoning tasks and real-world applications remains uncertain.",
            "13": "- The reliance on few-shot learning for new tasks may lead to overfitting or suboptimal performance if the training examples are not representative or sufficient.",
            "14": "Suggestions for improvement\n   - **Simplify the Model Architecture**\n     - Consider streamlining the module generation and execution process to reduce complexity and improve scalability.",
            "15": "This could involve optimizing the interactions with LLMs or exploring alternative methods for module creation.",
            "16": "- **Expand the Initial Module Library**\n     - Enhance the initial set of pre-defined modules and APIs to cover a broader range of visual reasoning tasks.",
            "17": "This could involve incorporating more diverse and complex functions to improve the model's versatility.",
            "18": "- **Improve Generalization Capabilities**\n     - Conduct additional experiments to evaluate the model's performance on a wider variety of visual reasoning tasks and real-world scenarios.",
            "19": "This could help identify potential limitations and areas for improvement.",
            "20": "- **Optimize Few-shot Learning**\n     - Investigate methods to enhance the effectiveness of few-shot learning, such as leveraging more sophisticated sampling techniques or incorporating additional contextual information to improve generalization.",
            "21": "- **Develop a Universal Prompt**\n     - Explore the possibility of creating a universal prompt that can be applied to all reasoning tasks, reducing the need for task-specific prompts and simplifying the model's application."
        },
        "lSWl1eYZmF": {
            "0": "I am appreciative of the idea of generating more modular and composable modules that are verified, to be used in a library of skills for future tasks.",
            "1": "I think the idea of continuously growing this library is innovative, and the method simple and elegant.",
            "2": "W1.",
            "3": "Is there anything to prevent overfitting to the small train set of a given task, and only creating specific modules tailored for that domain?",
            "4": "I can imagine that these overfitted modules may not be very useful in new tasks.",
            "5": "W2.",
            "6": "Isn’t it possible that the method creates a broad function signature, but during stage 2 verification with the small set of train examples, it overfits to a bad implementation that only works for those examples, and therefore actually harms performance from there on out?",
            "7": "I’m mainly concerned about the above two overfitting challenges.",
            "8": "W3.",
            "9": "There seems to be an assumption that the queries in the train set actually all require similar modules, for example, to verify the new modules, the selected set of test cases from the train set must actually use those modules.",
            "10": "I’m not sure if this is a reasonable assumption, and also, how is this sampling of train examples (both in stage 1 and stage 2) done?",
            "11": "W4.",
            "12": "In the experiments, are Visprog/Viper also given the same few-shot training set?",
            "13": "For example, I believe you can use the same method of error correction and correct both Visprog/Viper when it is incorrect.",
            "14": "In this way, you can include Visprog/Viper comparison in Tables 3 and 4.",
            "15": "Would be great to better disentangle what drives this improvement of performance -- modularity in the proposed modules, or the training examples given in the loop, etc."
        },
        "4iIhr5iJO1": {
            "0": "In general, I think the direction of the proposed method is sound and compelling; systems like VisProg and ViperGPT provide promising ways to push neuro-symbolic reasoning past toy-domains, but they are limited by fixed APIs.",
            "1": "Using LLMs to augment these APIs in a task-specific manner is an interesting idea with many potential avenues for future exploration.",
            "2": "From a methodological stand-point, I think the division of labor between the module initialization step and the module generation step is a neat insight.",
            "3": "This design decision allows the LLM to create it's own tests, that can then be used to help guarantee that LLM generated functions 'do what they are supposed to'.",
            "4": "The experimental results are largely positive for the proposed method, although I have some concerns about their design that I'll detail below.",
            "5": "That said, I think the experimental results do support the claim that the proposed system offers improvements over both VisProg and ViperGPT across tasks.",
            "6": "Perhaps the most compelling claim is that the modules that GNSVR finds from the GQA and RefCOCO tasks can \"transfer\" for related tasks of image editing and knowledge tagging, supported by the fact that GNSVR significantly outperforms VisProg in this setting.",
            "7": "The evaluations against fully-supervised methods on the Raven and MEWL tasks is also fairly impressive, considering GNSVR is operating in a few-shot learning paradigm.",
            "8": "# Main Concern\n\nFrom my perspective, the biggest current weakness of the paper is that from the experimental design its hard to parse out exactly how the discovered modules affect the system's performance.",
            "9": "Ostensibly, this can be gleaned from comparisons between GNSVR and VisProg/ViperGPT, but there are more differences between these systems beyond merging in discovered modules.",
            "10": "Specifically, GNSVR uses a \"base API\" that is a combination of VisProg and ViperGPT, so the \"fair\" comparison would be against an ablated version of GNSVR that removes steps 1 and 2, and just tries to solve test-problems with the original API functions.",
            "11": "This condition is considered in the ablation experiment (GNSVR w/o ML), but only a subset of the RefCOCO test-set.",
            "12": "To solidify the claim that the improvement GNSVR observes stems from its discovered modules, this base condition should be added to all of the experimental set-ups (tables 1-5), for example, from Table 2 its unclear how much of the delta improvement between VisProg and GNSVR can be attributed to improvements in the base API versus improvements to the API from the new modules.",
            "13": "Beyond this, I'm also slightly concerned about the design of the GNSVR w/o ML baseline.",
            "14": "At inference time, is this baseline allowed to invoke arbitrary python logic in the style of ViperGPT (e.g.",
            "15": "standard control flow constructs) or is it restricted to *only* using API function calls in the style of VisProg.",
            "16": "I would imagine that the first condition would be more fair to evaluate GNSVR.",
            "17": "Solving some tasks might require simple logic that the LLM knows how to express in python, but might not be directly expressible with a series of API calls.",
            "18": "In GNSVR, this logic is incorporated into modules, but in the baseline the LLM should still have the opportunity to invoke similar logic in its test-time solutions (otherwise its impossible to properly evaluate the usefulness of the discovered modules).",
            "19": "Please clarify which of these modes the baseline is operating in.",
            "20": "# Minor\n\nCompared to VisProg and ViperGPT this system seems to require more training data, as the I/O pairs are not only used to populate in-context examples, but also impact (i) what module concepts are proposed and (ii) how the correctness of each module concept is evaluated.",
            "21": "This point about reliance on training data is touched on in the ablation section, but it would be good to make this distinction explicit when comparing the pros/cons of the proposed system against past work."
        },
        "vlzGzJslie": {
            "0": "Firstly, the paper is very well written and the GNSVR method is very well explained, with a healthy split between using the main paper and supplementary for splitting the key information versus additional information.",
            "1": "The empirical performance of GNSVR for transfer tasks, as well as the examples of the modules generated is quite impressive - it would justify the overall approach to growing modules for reasoning and their effectiveness for few-shot  generalization.",
            "2": "There are several components to the GNSVR framework but the paper provides no detailed analysis in the main paper of the importance of each of the components of the GNSVR framework.",
            "3": "While I think the framework overall is useful, the components are not all equally important to solve the reasoning problem and hence it is important to understand for future research on modular reasoning to understand what works and what doesn't, and if so why not.",
            "4": "How big of a role does \"good initlialization\" of the neural module operators plays?How important is defining the correct input and output format for a new module?",
            "5": "How important is the selection of the few shot samples to evaluate a new module?",
            "6": "(the authors say \"We extracted 300 examples from GQA, 100 from RefCOCO, 10 from Raven, and 10 from MEWL based on experimental experience.\"",
            "7": "- is the experience here just cherry picking for results or something else?)",
            "8": "How important is the LLM capability to learn new modules?",
            "9": "What role does the prompt play for the LLM in evaluating existing modules and creating new ones?",
            "10": "Without detailed analysis to support answers to all these questions, the paper is limited in terms of explaining the method beyong just presenting a new method and showing empirical results.",
            "11": "Lastly, I would not suggest the authors not report results on the RAVEN dataset.",
            "12": "As shown independently in [1] and [2] the dataset contains flaws in choice design which enables models to learn shortcuts to solve the RPM reasoning task.",
            "13": "I would recommend the authors use the i-RAVEN dataset inroduced in [1] instead.",
            "14": "**References** \n\n1.",
            "15": "Hu, S., Ma, Y., Liu, X., Wei, Y. and Bai, S., 2021, May.",
            "16": "Stratified rule-aware network for abstract visual reasoning.",
            "17": "In Proceedings of the AAAI Conference on Artificial Intelligence (Vol.",
            "18": "35, No.",
            "19": "2, pp.",
            "20": "1567-1574).",
            "21": "Spratley, S., Ehinger, K. and Miller, T., 2020.",
            "22": "A closer look at generalisation in raven.",
            "23": "In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVII 16 (pp.",
            "24": "601-616).",
            "25": "Springer International Publishing."
        },
        "3Js79px0vh": {
            "0": "This paper makes a clear case for it's own contribution, and that contribution does appear to be valuable - visual learning is an important task, and the prohibitive cost of using SOTA large language models makes reuse of code appealing (although they explicitly don't use ChatGPT4 \"due to the prohibitive cost\", so maybe this is less of an argument than it would be otherwise).",
            "1": "The fact that they can show it's use on several domains and types of tasks is also appealing.",
            "2": "My main concern is that, based on the presentation, it seems that the authors took a lot of highly intricate API's for LLM's that large teams may have worked on and cobbled them together to solve a new task.",
            "3": "I refer to this section: \"The success of our GNSVR relies on a set of pre-defined modules and APIs as the starting point.",
            "4": "We utilize handcrafted modules from VisProg (Gupta & Kembhavi, 2022) as our initial components.",
            "5": "Additionally, we incorporate several new APIs from ViperGPT to enhance module creation.",
            "6": "We also include some new APIs from ViperGPT (Sur´ıs et al., 2023) for making new modules.\"",
            "7": "I appreciate their novelty in how they use these API's, but the ratio of insights of these authors vs of the authors of the API's appears insignificant.",
            "8": "I'm also not entirely convinced of the novelty of this paper.",
            "9": "I refer to \"Iterative Disambiguation: Towards LLM-Supported Programming and System Design\" (Pereira and Hartmann) and \"Self-planning Code Generation with Large Language Models\" (Jiang et al).",
            "10": "I don't think the fact that this is in the visual domain is enough to call it \"novel\", because there is virtually no engagement with visual modalities by the authors - as stated above, according to my understanding, they are using predefined modules which handle the interface between vision and language.",
            "11": "If I was given evidence against either of the two above claims - that all of the work (particularly the \"visual reasoning\" work) is being done by existing tools, or that the paper is not fundamentally using LLM's in a novel way - I would be happy to increase my score."
        }
    },
    "LIW88mwqgv": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the performance of large language models (LLMs) on reasoning tasks by introducing a novel method, Iter-CoT, which enhances chain-of-thought (CoT) prompting through iterative bootstrapping.",
            "1": "- The proposed method allows LLMs to autonomously rectify errors and select exemplars of appropriate difficulty, which is a significant advancement over existing CoT prompting techniques.",
            "2": "- The paper demonstrates the effectiveness of Iter-CoT across multiple reasoning tasks and datasets, showcasing its potential to improve LLM performance in a variety of contexts.",
            "3": "Potential reasons for acceptance\n   - The introduction of Iter-CoT represents a meaningful contribution to the field of natural language processing, particularly in improving the reasoning capabilities of LLMs.",
            "4": "- The experimental results are robust, showing significant performance improvements across ten datasets and three distinct reasoning tasks.",
            "5": "- The paper provides a comprehensive evaluation of Iter-CoT, including comparisons with several baseline methods and ablation studies to understand the impact of different components of the proposed approach.",
            "6": "- The method is versatile, working effectively in both labeled and unlabeled scenarios, which enhances its applicability in real-world settings.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability:**\n     - The paper is dense with technical details, which may make it challenging for readers to follow the main contributions and results.",
            "8": "- Some sections, particularly the experimental setup and results, could benefit from clearer explanations and more structured presentation.",
            "9": "- **Evaluation and comparison:**\n     - While the paper compares Iter-CoT with several baseline methods, it would be beneficial to include more recent and advanced techniques for a more comprehensive evaluation.",
            "10": "- The impact of the iterative bootstrapping process on computational efficiency and resource requirements is not thoroughly discussed, which could be a concern for practical implementation.",
            "11": "- **Generalizability:**\n     - The paper focuses on specific reasoning tasks and datasets.",
            "12": "It would be helpful to see evaluations on a broader range of tasks and more diverse datasets to confirm the generalizability of Iter-CoT.",
            "13": "- The reliance on GPT-4 as an evaluator in the absence of labels raises questions about the scalability and accessibility of the method for researchers without access to such powerful models.",
            "14": "Suggestions for improvement\n   - **Enhance clarity and readability:**\n     - Simplify the presentation of the methodology and results to make the paper more accessible to a broader audience.",
            "15": "- Use more visual aids, such as diagrams and flowcharts, to illustrate the iterative bootstrapping process and the construction of the demonstration pool.",
            "16": "- **Expand evaluation:**\n     - Include comparisons with more recent and advanced CoT prompting techniques to provide a more comprehensive evaluation of Iter-CoT.",
            "17": "- Discuss the computational efficiency and resource requirements of Iter-CoT, particularly the iterative bootstrapping process, to provide insights into its practical implementation.",
            "18": "- **Broaden generalizability:**\n     - Evaluate Iter-CoT on a wider range of reasoning tasks and more diverse datasets to confirm its generalizability and robustness.",
            "19": "- Explore alternative methods for evaluating the correctness of responses in the absence of labels, potentially using less resource-intensive models or heuristic approaches.",
            "20": "- **Detailed analysis:**\n     - Provide a more detailed analysis of the impact of different components of Iter-CoT, such as the number of iterations in the bootstrapping phase and the role of the summarization phase, to offer deeper insights into the method's effectiveness.",
            "21": "- Include a discussion on potential limitations and future work to address any identified weaknesses and suggest directions for further research."
        },
        "NXOoNofHP6": {
            "0": "This paper proposes iter-CoT, which uses the experiences with wrong cases to construct the demonstrations for CoT prompts.",
            "1": "It is a new CoT method that focuses on the demonstration selection to improve the in-context learning performance.",
            "2": "The evaluation is conducted on a wide range of benchmarks and various LLMs, showing advances in performance.",
            "3": "The idea of this paper is not very exciting.",
            "4": "a) The idea of self-correction is discussed in recent studies [1,2].",
            "5": "By constructing the correction demonstration pool, the samples that the LLM is prone to getting wrong are gathered and a correct CoT is prepared for each.",
            "6": "By sampling from these, the model can get stronger prompt.",
            "7": "b) The idea of ‘bootstrap’ is not very appealing.",
            "8": "First, LLM bootstrap has been proposed in [3], where the demos are totally generated by the proposed system.",
            "9": "However, in iter-CoT, both ‘w/ label’ setting and ‘wo label’ setting are under supervised, i.e., the golden label or another more powerful LLM.",
            "10": "(Minor) The analysis does not make this paper more convincing.",
            "11": "The settings of demo selection in the ablation can hardly uncover what the model learns from history errors and corrections.",
            "12": "The ‘rising and then falling’ trend in Figure 6 is not obvious.",
            "13": "Also, the comparisons are coarse-grained.",
            "14": "The demos, method, and reasoning steps are all different across all settings and may require in-depth analysis.",
            "15": "[1] Learning from Mistakes via Interactive Study Assistant for Large Language Models\n\n[2] Large Language Models Cannot Self-Correct Reasoning Yet\n\n[3] Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP"
        },
        "5xQmyzxdZf": {
            "0": "The motivation is strong and supported by two findings, i.e.",
            "1": "the self-correction ability of LLMs and the value of revised examples.",
            "2": "The Iter-CoT method is straightforward to follow.",
            "3": "Experimental results across three categories of reasoning tasks and ten datasets prove the effectiveness of this method.",
            "4": "A series of ablation experiments are conducted to investigate the two phases of the method.",
            "5": "The bootstrapping phase relies on GPT-4 to assess examples without labels, which does not address the issue of hallucinations and might lead to misjudgment of correct responses.",
            "6": "The summarization phase employs an LLM, which could also lead to hallucinations, especially when dealing with lengthy multi-turn conversation contexts, potentially impairing LLM performance and resulting in suboptimal summarizations.",
            "7": "Additionally, including the entire correction process in the summarization phase might lead to misleading summaries.",
            "8": "I found the innovativeness of the proposed method is limited, as the verify-and-correct approach has been previously explored in works such as ReAct and self-ask.",
            "9": "Additionally, more efficient methods for demonstration sampling, like k-nearest neighbor sampling, are available.",
            "10": "Reference:\nYao, Shunyu, et al.",
            "11": "\"React: Synergizing reasoning and acting in language models.\"",
            "12": "Press, Ofir, et al.",
            "13": "\"Measuring and narrowing the compositionality gap in language models.”\nLiu, Jiachang, et al.",
            "14": "\"What Makes Good In-Context Examples for GPT-3 ?.”"
        },
        "ekEFkkySeb": {
            "0": "The paper propose a new way of constructing few-shot chain-of-thought examples to use for in-context learning.",
            "1": "It is the first to apply self-refinement and LLM self-evaluation in construction demonstration examples, and the experimental results are strong with and without ground truth labels.",
            "2": "The authors evaluate the method with both proprietary GPT models and open source LLAMA models, and performs well in both cases.",
            "3": "The authors conduct thorough ablation to examine different stages of the construction pipeline and impact of LLM evaluator's performance on final accuracy.",
            "4": "These results are helpful for understanding the model performance and consistent with the main motivation to use iterative bootstrapping to improve exemplar quality.",
            "5": "It is mentioned that during the inference stage, a random N exemplars are samples as the fixed demonstrations for the entire test set.",
            "6": "I have a few questions on this choice:\n    1.",
            "7": "If only N random exemplars are used for the entire test set for inference, is it still necessary to construct the demonstration pool based on the entire training set?",
            "8": "Or what is the use of \"pool\" here since only N exemplars are used.",
            "9": "Is is possible to have comparison with the baselines use the same set of few-shot examples, just with different reasoning chain annotations.",
            "10": "Since the contribution of the paper is mainly on the construction of the reasoning chains, I feel it is better to have the selection of examples consistent for fair comparison.",
            "11": "Meanwhile it would be interesting to see if the example selection strategies used in Complex-CoT and Auto-CoT are helpful for Iter-CoT or not.",
            "12": "While the experiments are conducted over 10 different datasets, most of them are on the easier side and has some synthetic nature.",
            "13": "This also reflects on the results, where the LLMs generally achieve very high scores on these datasets already.",
            "14": "It would be better if the authors could evaluate on other more realistic and challenging datasets."
        },
        "lpmZITrm3E": {
            "0": "Overall, the writing is clear and easy to follow.",
            "1": "In addition, the organization of the main draft is well-established.",
            "2": "Improving the reasoning capability of LLMs is an interesting and important problem.",
            "3": "To this end, considering the quality of few-shot demonstrations is a reasonable and well-motivated direction.",
            "4": "The proposed method is simple and can be applicable regardless of the types of LLMs.",
            "5": "Also, it shows a significant improvement compared to the existing baselines, well-studied in the same problems.",
            "6": "While the improvements are significant, the explanation for such gains is not sufficient.",
            "7": "For example, the proposed Iter-CoT outperforms both Manual-CoT and Complex-CoT in Table 1, which use the ground-truth annotations of rationales.",
            "8": "Since Iter-CoT is a method to generate correct rationales of unannotated samples, both methods can be considered as the upper bound as they always use the ground-truth rationales.",
            "9": "Therefore, it’s not natural that Iter-CoT shows better performance than those ones.",
            "10": "What is the source of such improvement?",
            "11": "One of the major concerns is an increase in cost due to the iterative usage of LLMs, but there is no discussion regarding this.",
            "12": "How many costs are required for Iter-CoT, compared to other methods?",
            "13": "Also, it would be better if the authors could have a corresponding discussion in the draft.",
            "14": "While the proposed method is applied to generate rationales of the training examples for few-shot demonstrations, it can be applicable during the inference.",
            "15": "Also, the framework of bootstrapping the rationale has been widely explored [1,2].",
            "16": "Hence, I’m wondering if the improvement can be enlarged when Iter-CoT is also applied during inference.",
            "17": "### Minor\n\n1.",
            "18": "It would be better to change the order of the presented method in Table 3, with the decreasing order for better presentation.",
            "19": "Also, as the authors presented in the Appendix, STaR is highly relevant to the proposed Iter-CoT.",
            "20": "It seems to be better to add this baseline to the main table if possible.",
            "21": "Regarding Figure 6, the authors mention that “we utilize the best exemplars in this section”.",
            "22": "What is the meaning of best exemplars?",
            "23": "Also, how do you select them?",
            "24": "Do you utilize another validation set to choose them?",
            "25": "[1] Madaan et al., Self-Refine: Iterative Refinement with Self-Feedback., NeurIPS 23  \n[2] Shinn et al., Reflexion: Language Agents with Verbal Reinforcement Learning., NeurIPS23"
        }
    },
    "i5da6iedW8": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical challenge in federated learning, specifically the fine-tuning of large language models (LLMs) while ensuring data privacy and intellectual property protection.",
            "1": "- The proposed method, FedBiOT, introduces a bi-level optimization approach to reconcile the needs of data privacy for clients and intellectual property protection for LLM owners.",
            "2": "- The novelty lies in the formulation of the problem as a bi-level optimization task and the use of a public dataset for emulator distillation, which is a significant advancement over existing methods like offsite-tuning and FedOT.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a highly relevant and timely problem in the field of federated learning and LLM fine-tuning.",
            "4": "- The proposed method shows significant improvements over existing baselines in extensive experiments, demonstrating its effectiveness.",
            "5": "- The bi-level optimization approach is well-formulated and provides a clear framework for addressing the challenges of data privacy and intellectual property protection.",
            "6": "- The paper includes thorough experimental validation, including ablation studies, which strengthen the credibility of the proposed method.",
            "7": "Potential reasons for rejection\n   - **Complexity of the proposed method**\n     - The bi-level optimization approach may be challenging to implement and understand for practitioners who are not familiar with advanced optimization techniques.",
            "8": "- The paper could benefit from more detailed explanations and visualizations of the optimization process to make it more accessible.",
            "9": "- **Limited discussion on scalability**\n     - The paper does not provide a detailed analysis of the scalability of the proposed method, especially in scenarios with a large number of clients or very large LLMs.",
            "10": "- It would be beneficial to include experiments or discussions on how the method scales with increasing model size and number of clients.",
            "11": "- **Potential overfitting to specific tasks**\n     - The experiments are conducted on specific tasks (math problem-solving, code generation, and question answering), and it is unclear how well the method generalizes to other types of tasks or datasets.",
            "12": "- Including a broader range of tasks and datasets in the experiments would strengthen the generalizability claims of the method.",
            "13": "Suggestions for improvement\n   - **Simplify the explanation of the bi-level optimization approach**\n     - Provide more intuitive explanations and visual aids to help readers understand the bi-level optimization process.",
            "14": "- Include a step-by-step guide or pseudocode for the optimization process to make it easier for practitioners to implement.",
            "15": "- **Discuss scalability in more detail**\n     - Include experiments that test the scalability of the method with varying numbers of clients and different sizes of LLMs.",
            "16": "- Provide a detailed analysis of the computational and communication overhead associated with the proposed method.",
            "17": "- **Expand the range of experimental tasks**\n     - Conduct experiments on a wider variety of tasks and datasets to demonstrate the generalizability of the proposed method.",
            "18": "- Include tasks from different domains (e.g., medical, legal, etc.)",
            "19": "to show the versatility of the method.",
            "20": "- **Improve the clarity and organization of the paper**\n     - Ensure that the paper is well-organized and that each section flows logically from one to the next.",
            "21": "- Use clear and concise language to explain complex concepts, and avoid unnecessary jargon.",
            "22": "- Include a summary of key contributions and findings at the beginning and end of the paper to help readers quickly grasp the main points."
        },
        "tDOzx90LnE": {
            "0": "The combination of LLM and federated learning is interesting.",
            "1": "The problem formulation is well presented.",
            "2": "The layer selection and dropout mechanism is interesting.",
            "3": "However, there are some improvements for the paper:\n1.",
            "4": "The number of clients is very small.",
            "5": "In section 4.1, the number of clients is 4, which is relatively very small compared with that in real FL settings.",
            "6": "The idea is straightforward, which is presented in existing works, e.g., Yosinski et al., 2014.",
            "7": "The selection of dropout rate is not well elaborated.",
            "8": "Tables 1 and 2 are not clear.",
            "9": "The first line is not explained.",
            "10": "The unit can be added, and the meaning of the numbers can be explained.",
            "11": "The experimentation show that the performance of FedBiOT may be inferior than baselines.",
            "12": "The classic FL approaches can be added as baselines."
        },
        "0BprflfTDA": {
            "0": "- This paper considers a relatively new setting in federated learning and large language models.",
            "1": "- This paper proposes a new FL algorithm FedBiOT, which trains adapter over emulator to achieve parameter-efficient tuning.",
            "2": "- Experiments show the effectiveness of FedBiOT by comparing with two baselines.",
            "3": "- The contributions need to be clarified.",
            "4": "For me, I think the topic of this paper is interesting and worth exploring.",
            "5": "However, it is not so clear what are the main contributions of this paper since previous work [1] has considered such setting and proposed FedOT (federated learning with offsite-tuning).",
            "6": "Are the main contributions lying on improving FedOT via a bi-level optimization approach?",
            "7": "- The motivations need to be further clarified.",
            "8": "This paper claims that the clients cannot obtain the full model due to intellectual property of LLM.",
            "9": "However, I wonder if such claim still holds after the release of Llama2.",
            "10": "- Some meaningful experiments are missing.",
            "11": "- Some experiments for reference.",
            "12": "It would be more helpful if the authors can provide the results when clients can obtain the full model, such that we could see how large the gap is.",
            "13": "- Computation resources comparisons.",
            "14": "This method requires more training resources (e.g., more training steps) compared to baselines.",
            "15": "However, this paper does not show such comparisons, which would promote readers' understanding.",
            "16": "- Some confusions:\n  - \"Improvement 1\" at page 5.",
            "17": "What are the definations of bottom / first / last layers.",
            "18": "Suggest consistent expressions like first / last.",
            "19": "Currently, my rating is between 5 and 6.",
            "20": "I would consider re-rating if the authors can address the above concerns.",
            "21": "[1] Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou.",
            "22": "Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning."
        },
        "Ta9TatQnmd": {
            "0": "The topic is timely and interesting.",
            "1": "The experimental evaluation was only implemented in LLaMA-7B.",
            "2": "How does it work on other mainstream models such as ChatGPT2?",
            "3": "In the experiment, federated learning only considered 8 clients.",
            "4": "There is a lack of experiments that vary the number of clients and the number of training samples each client own."
        },
        "TnQupyYQ8Y": {
            "0": "The paper is built upon a relatively recent work so that it may offer modern insights into the related research fields.",
            "1": "Experimental results support the proposed improvements in the paper.",
            "2": "The proposed improvements in the paper are general and should be easy to adopt.",
            "3": "From a technical perspective, the two improvements proposed in the article may be incremental.",
            "4": "One involves changing the index of the fine-tuning layers (based on observation), and the other relies on the traditional distillation method.",
            "5": "Both methods are essentially at the level of tricks and are insufficient to serve as contributions to the paper.",
            "6": "I have doubts about the \"intellectual property protection\" aspect of the paper.",
            "7": "In this framework, although local clients can only obtain a portion of the model instead of the entire model, this sub-model can still be fine-tuned and used for inference, which implies that the majority of the model's functionality has been preserved.",
            "8": "Essentially, malicious users can still steal this intellectual property.",
            "9": "This framework does not seem to provide significant protection, so I do not consider the \"intellectual property protection\" mentioned in the title appropriate.",
            "10": "The paper should provide a detailed algorithm to help readers follow."
        }
    },
    "H49g8rRIiF": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a novel approach, LAMP, which leverages pretrained language models to process 3D point clouds.",
            "1": "- It explores the cross-modal strategy by aligning 3D point cloud data with language models, which is a relatively unexplored area.",
            "2": "- The approach has potential implications for both unimodal and multimodal tasks, particularly in 3D visual grounding.",
            "3": "Potential reasons for acceptance\n   - The proposed method demonstrates significant performance improvements on several benchmarks, including ModelNet-40, S3DIS, and ShapeNetPart.",
            "4": "- LAMP showcases the feasibility of using language models for 3D point cloud understanding, opening new avenues for research in this area.",
            "5": "- The paper provides extensive experimental validation, including comparisons with state-of-the-art methods and analysis of design choices.",
            "6": "- The approach is simple yet effective, requiring only a small portion of parameters to be trained, which is computationally efficient.",
            "7": "Potential reasons for rejection\n   - **Lack of detailed explanation on certain technical aspects:**\n     - The paper could provide more in-depth explanations of the CMSA and CMCA mechanisms, including mathematical formulations and theoretical justifications.",
            "8": "- The process of aligning point cloud data with language model parameters could be elaborated further to enhance understanding.",
            "9": "- **Limited discussion on potential limitations and challenges:**\n     - The paper does not thoroughly discuss the potential limitations of the proposed approach, such as scalability to larger datasets or real-time applications.",
            "10": "- There is a lack of discussion on the challenges faced during the implementation and how they were addressed.",
            "11": "- **Insufficient comparison with other cross-modal approaches:**\n     - The paper primarily compares LAMP with unimodal and vision-based multimodal methods but lacks a comprehensive comparison with other cross-modal approaches.",
            "12": "- It would be beneficial to include a discussion on how LAMP compares with other methods that leverage language models for non-language tasks.",
            "13": "Suggestions for improvement\n   - **Enhance technical explanations:**\n     - Provide more detailed explanations and mathematical formulations for the CMSA and CMCA mechanisms to improve clarity and understanding.",
            "14": "- Elaborate on the process of aligning point cloud data with language model parameters, including any preprocessing steps and challenges faced.",
            "15": "- **Discuss potential limitations and challenges:**\n     - Include a section discussing the potential limitations of the proposed approach, such as scalability, real-time application, and generalization to other datasets.",
            "16": "- Discuss any challenges encountered during the implementation and how they were addressed, providing insights for future research.",
            "17": "- **Expand comparison with cross-modal approaches:**\n     - Include a comprehensive comparison with other cross-modal approaches that leverage language models for non-language tasks, highlighting the unique contributions and advantages of LAMP.",
            "18": "- Discuss the potential for integrating LAMP with other cross-modal techniques to further enhance performance and applicability.",
            "19": "- **Provide additional experimental results:**\n     - Include more experimental results on diverse datasets and tasks to demonstrate the robustness and generalizability of LAMP.",
            "20": "- Conduct ablation studies to analyze the impact of different components and design choices on the overall performance of the proposed method."
        },
        "SfNwCOjE7r": {
            "0": "- The authors propose to adapt Language Models to tackle the point cloud perception problem, which has some originality.",
            "1": "- Their method outperforms the existing baseline approaches on several benchmarks, which demonstrates the effectiveness of their proposed method.",
            "2": "- The paper writing is clear and easy to follow.",
            "3": "This paper shares very similar spirits with many recent papers on Large (Vision) Language Models and Point Cloud Understanding:\n\n1.",
            "4": "Gao et al.",
            "5": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model.",
            "6": "Guo et al.",
            "7": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following.",
            "8": "Essentially, this paper and other relevant papers are trying to bind point cloud representations to Language Models via adaptation.",
            "9": "The attention-based adaptation has also been exploited in LLaMA-Adapter V2 (Note that this paper also supports point cloud inputs).",
            "10": "Hence, the authors need to discuss the differences with those works and also compare their method with those methods in the experiments.",
            "11": "From my understanding adapting Language Models to 3D point clouds with attention is straight-forward and not that novel considering the above-mentioned literature.",
            "12": "In addition to the 3D object datasets, the authors also need to evaluate their method on the more realistic indoor 3D datasets such as SUN-RGBD and ScanNet."
        },
        "jbkhHEKzwE": {
            "0": "The proposed LAMP approach demonstrates that by merely projecting point cloud features onto language models, while maintaining the language model in a frozen state, it is still possible for the model to process 3D data.",
            "1": "This underscores the versatility of language models as general-purpose functions, showcasing their capacity to handle data from unfamiliar modalities even without directly updating their parameters.",
            "2": "The experiments show that even with a few trainable parameters, the LAMP can still achieve reasonable performance.",
            "3": "The reviewer finds some performance in the paper somewhat unconvincing and also seems to lack a proper baseline to compare, particularly when referring to the results in Table 2.",
            "4": "For instance, when comparing point-MLP elite with LAMP, the performance appears quite similar, or even worse (considering Point-MLP elite has 90.9 as mAcc).",
            "5": "While the trainable parameters of the Point-MLP elite are also minimal at 0.68M, its inference speed is anticipated to be notably faster.",
            "6": "This is because, during inference, Point-MLP elite maintains its 0.68M parameters.",
            "7": "Conversely, LAMP, despite reducing only its trainable parameters, is expected to have a longer inference time, given it leverages a significantly larger frozen language model.",
            "8": "Thus, from a practical standpoint, the advantage of LAMP having a small number of trainable parameters but including an expansive language model seems to weaken its asserted advantages.",
            "9": "Additionally, given that ModelNet40 is somewhat of a saturated benchmark, it would enhance the paper's credibility if LAMP were evaluated on more challenging datasets, for example, the ScanObjectNN classification benchmark.",
            "10": "This would provide a clearer perspective on its efficacy and potential real-world applications.",
            "11": "Also, there are some other works like RepSurf[1], which is also lightweight (~1.5M) and exhibits very strong performance (94.7 OA) and at the same time fast at inference (3.1ms, 0.81GFLOPs, roughly 20 to 200 times faster).",
            "12": "[1] Ran, Haoxi, Jun Liu, and Chengjie Wang.",
            "13": "\"Surface representation for point clouds.\"",
            "14": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
            "15": "2022."
        },
        "IrXTupuHJX": {
            "0": "The starting point of the paper is very meaningful.",
            "1": "It uses the existing Language model to initialize the model and only trains a small part of the adapter, making the model have good performance capabilities.",
            "2": "The experimental results look pretty good.",
            "3": "Although using LLM (Language Learning Models) for 3D point analysis is a good starting point, I notice that the main experiments in the article still focus on pure point cloud experiments, such as 3D Object Classification and Part Segmentation, etc.",
            "4": "Tasks using Language Models typically focus on multimodal tasks (point cloud-text), like the 3D Visual Grounding mentioned in the paper.",
            "5": "However, it seems that most of the experiments in the article still conventionally utilize task-specific heads for 3D point cloud analysis.",
            "6": "The use of the LLM+adapter pipeline doesn't seem very suitable for traditional 3D point cloud analysis.",
            "7": "The reason why LLaVA and minigpt4 can effectively use the LLM+image adapter pipeline is that the final output space is still in the language space, so there's no issue with keeping the LLM fixed without further training.",
            "8": "In this article, the output space is a traditional perceptual space, such as 3D classification or segmentation.",
            "9": "To address these issues, one could either formulate traditional point analysis tasks as vision-language tasks with an output in the language space, or replace the LLM with a 2D image encoder to initialize parameters.",
            "10": "The approach in the article seems somewhat unreasonable and odd.",
            "11": "The author might want to reconsider it.",
            "12": "The experimental results don't seem to show a significant improvement.",
            "13": "The ModelNet dataset is too small.",
            "14": "Using a model the size of BERT might lead to overfitting?",
            "15": "It seems that even simpler models already achieve good results, such as the PointNet++ from six years ago.",
            "16": "Perhaps it's more appropriate to test on a larger dataset and then redefine all tasks as vision-language tasks.",
            "17": "The paper writing needs improvement; it looks a bit rushed."
        }
    },
    "1mjsP8RYAw": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of automated fact verification, which is increasingly important in the context of rising misinformation.",
            "1": "- It introduces SFA VEL, a novel unsupervised pretraining framework that leverages pre-trained language models for fact verification without the need for annotated data.",
            "2": "- The proposed method achieves state-of-the-art performance on benchmark datasets, demonstrating its effectiveness and potential impact on the field.",
            "3": "Potential reasons for acceptance\n   - The paper presents a significant advancement in the field of fact verification by eliminating the need for annotated data, which is often time-consuming and costly to obtain.",
            "4": "- The proposed method achieves impressive performance improvements on benchmark datasets, outperforming both supervised and unsupervised approaches.",
            "5": "- The use of a novel contrastive loss function and the distillation of pre-trained language model features into a compact and discrete structure are innovative contributions.",
            "6": "- The paper includes thorough experiments and ablation studies that justify the design decisions and demonstrate the effectiveness of the proposed approach.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability:**\n     - The paper is dense and may be difficult to follow for readers who are not already familiar with the technical details of language model distillation and contrastive learning.",
            "8": "- Some sections, such as the data processing pipeline and the generation of negative instances, could benefit from clearer explanations and more detailed examples.",
            "9": "- **Evaluation and comparison:**\n     - While the paper demonstrates state-of-the-art performance, it would be beneficial to include more detailed comparisons with a broader range of existing methods, including recent advancements in the field.",
            "10": "- The paper could provide more insights into the limitations and potential weaknesses of the proposed approach, as well as scenarios where it may not perform as well.",
            "11": "- **Reproducibility:**\n     - Although the authors provide a link to the source code, the paper could include more detailed information on the experimental setup, hyperparameters, and training procedures to ensure full reproducibility.",
            "12": "Suggestions for improvement\n   - **Improve clarity and readability:**\n     - Simplify and clarify the explanations of the data processing pipeline and the generation of negative instances, possibly with the help of visual aids or more detailed examples.",
            "13": "- Provide a high-level overview of the proposed method early in the paper to help readers grasp the main ideas before diving into technical details.",
            "14": "- **Expand evaluation and comparison:**\n     - Include comparisons with a wider range of existing methods, including recent advancements in fact verification and related fields.",
            "15": "- Discuss the limitations and potential weaknesses of the proposed approach, as well as scenarios where it may not perform as well.",
            "16": "- **Enhance reproducibility:**\n     - Provide more detailed information on the experimental setup, hyperparameters, and training procedures in the main text or as supplementary material.",
            "17": "- Include a step-by-step guide for reproducing the experiments, along with any necessary scripts or configuration files.",
            "18": "- **Additional experiments:**\n     - Conduct experiments on additional datasets to further validate the generalizability and robustness of the proposed method.",
            "19": "- Explore the impact of different pre-trained language models and their configurations on the performance of the proposed approach."
        },
        "IUmLC4Exat": {
            "0": "Experimental results on the FEVER dataset show the advantage of the proposed method.",
            "1": "The improvement seems very significant.",
            "2": "The unsupervised manner of conducting fact verification is encouraged and useful.",
            "3": "Though the experiments show the effectiveness of the method, I do not get how the framework solves the cold starting.",
            "4": "For the scoring module, the claim embeddings from the LM are very different from those of Knowledge from the knowledge model.",
            "5": "Then how does the framework pick the top-k evidence at the beginning?",
            "6": "How does $L_{distill}$ work at the early iterations?",
            "7": "This is an important prerequisite that should be clearly stated in the paper.",
            "8": "Why does the paper only evaluate the effect of each loss on a smaller T5 model?",
            "9": "Considering the best performance reported in the work is based on Transformer-XL, ablation studies based on it are desired.",
            "10": "The annotation in the methodology section makes me really confused.",
            "11": "For example but not limited to:\n\n    (1) In Section 3.1, what does the V and each $v_i$ mean?",
            "12": "I cannot get it until I read through the whole methodology section.",
            "13": "(2) The use of subscript and superscript is messed up.",
            "14": "I think embedding is presented as $X_F$ in Section 3.2, but it becomes $X^F$ in Section 3.3.",
            "15": "(3) Some annotations are not really necessary, e.g., $N_i$ in Equation 3 are not used anywhere else in the paper.",
            "16": "I encourage the authors to make Section 3 more concise and clear by better formula presenting."
        },
        "doSiX2dZiB": {
            "0": "The experimental result of SFEVEL on the FEVER dataset is remarkable.",
            "1": "First of all, the paper is confusing in using the term \"unsupervised\".",
            "2": "The proposed method SFAVEL is unsupervised because it is for learning a knowledge model.",
            "3": "However, the fact-verification model reported in section 4 is supervised.",
            "4": "The model uses SFAVEL for mapping fact / claim to vectors and then uses a classifier trained in a supervised learning manner.",
            "5": "Secondly, it is unclear about what is the used \"linear probe\".",
            "6": "Fig 4b shows that the linear probe takes top evidence as input.",
            "7": "But then how can we verify the input claim if we use only evidence (and their scores)?",
            "8": "E.g.",
            "9": "how knowing \"Obama was born in Hawaii\" and \"Hawaii is in the US\"  can reject a claim without knowing what the claim is?",
            "10": "Thirdly, although the performance of the proposed model is remarkable, it is unclear why there's such a big gap between it and the existing models in the literature.",
            "11": "What are cases that the proposed model can solve but the others can?",
            "12": "Does the model find some crucial factors that the others miss?",
            "13": "Last but not least, the proposed SFAVEL is for learning a knowledge model.",
            "14": "But it is unclear whether that knowledge model is useful for other fact-verification cases (like on other FEVER dataset -- FEVER 2.0 for example).",
            "15": "Also, whether that knowledge model is also useful for other downstream task requiring fact?"
        },
        "cqTCmzm6YX": {
            "0": "The paper describes a novel approach for fact-verification\n2.",
            "1": "Results show significant gains in comparison to state of the art approaches 1.",
            "2": "Generalizability of the approach given other knowledge bases\n2.",
            "3": "Self-supervision is an ambitious claim\n3.",
            "4": "Fever is the only dataset used"
        },
        "liaKzzBmaT": {
            "0": "* The paper is very well written and well motivated.",
            "1": "* The results presented in the paper are impressive, outperforming FEVER SOTA even for supervised approaches.",
            "2": "* The authors compare the approach on 7 different models, including a variety of small to medium size models.",
            "3": "* The paper contains good ablation experiments, in particular analysing the different components of the loss on a small model.",
            "4": "* No large models were included, the biggest model tested has 250M parameters.",
            "5": "There is no strict definition of LLM, but the authors may overpromise in their title/intro when no model with more than 1B parameters is included.",
            "6": "* The increase over the SOTA may be exaggerated, given that most of the systems the paper compares to are several years old, and do not include the latest generation of models.",
            "7": "(This is not strictly a weakness, but context worth mentioning.)"
        },
        "PKkxHxsx5P": {
            "0": "The author's idea of leveraging a knowledge graph to produce positive and negative examples of unlabeled claims to train a scoring model is creative.",
            "1": "The paper is very well-structured, and easy to follow.",
            "2": "The experiments presented promising results on FEVER (~8% improvement on accuracy), and the method can work on a broad set of language models.",
            "3": "The technique proposed in the paper does not seem to be generalizable.",
            "4": "Specifically, the positive and negative examples constructed through triples from knowledge graph are too simple, which makes this method difficult to generalize to more complicated claims.",
            "5": "Specifically, triples can only represent who did what, while in reality, a claim can be who did what at where on when for why.",
            "6": "Any wrong information about these factors can make a claim false.",
            "7": "While it is not clear to me how the current method can learn a model that can be effectively aware of some more fine-grained factual differences.",
            "8": "The experimental setup is limited.",
            "9": "The evaluations are only based on FEVER, which is not convincing.",
            "10": "FEVER is created through Wikipedia, and Wikipedia information is closer to triples, which is bias to author's method and training process.",
            "11": "At least, an experiment to show the effectiveness of this method on other fact verification dataset would be very helpful.",
            "12": "Ranking may not be the best problem formulation for fact verification.",
            "13": "For claim verification, it is important to help people decide whether they should believe the claim or not.",
            "14": "Now the author formulates this problem as a ranking problem, which is not very useful from a fact verification perspective.",
            "15": "it is not clear what does it mean to the user that a claim can find a piece of evidence with 0.9 score."
        }
    },
    "YH5w12OUuU": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces TEMPO, a novel framework for time series forecasting that leverages Generative Pre-trained Transformers (GPT) with prompt-based learning and time series decomposition.",
            "1": "- The approach is significant as it aims to bridge the gap between state-of-the-art natural language processing models and time series forecasting, potentially leading to more accurate and generalizable models.",
            "2": "- The novelty lies in the integration of prompt-based learning and decomposition of time series into trend, seasonal, and residual components, which is not commonly explored in existing time series forecasting models.",
            "3": "Potential reasons for acceptance\n   - The proposed method demonstrates superior performance over state-of-the-art methods in zero-shot settings across multiple benchmark datasets.",
            "4": "- The integration of prompt-based learning and time series decomposition is a novel approach that could inspire further research in the field.",
            "5": "- The paper provides extensive experimental results and ablation studies that validate the effectiveness of the proposed method.",
            "6": "- The introduction of the TETS dataset, which combines time series and contextual information, adds value to the research community.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability:**\n     - The paper is dense and may be difficult to follow for readers who are not familiar with the technical details of transformers and time series decomposition.",
            "8": "- Some sections, such as the methodology, could benefit from clearer explanations and more intuitive descriptions of the processes involved.",
            "9": "- **Experimental setup:**\n     - The paper does not provide detailed information on the hyperparameters used for training TEMPO, which could affect the reproducibility of the results.",
            "10": "- The comparison with baseline models might not be entirely fair if the baselines were not tuned to their optimal performance.",
            "11": "- **Generalizability:**\n     - While the paper demonstrates strong performance on benchmark datasets, it is unclear how well TEMPO would perform on real-world, noisy time series data that may not follow the same patterns as the benchmarks.",
            "12": "- The zero-shot setting is impressive, but the paper does not explore how the model performs with fine-tuning on specific datasets, which is a common scenario in practical applications.",
            "13": "Suggestions for improvement\n   - **Improve clarity and readability:**\n     - Simplify the explanations of the methodology, particularly the decomposition process and the prompt-based learning approach, to make it more accessible to a broader audience.",
            "14": "- Include more visual aids, such as diagrams and flowcharts, to illustrate the key components and processes of TEMPO.",
            "15": "- **Provide detailed experimental setup:**\n     - Include a comprehensive description of the hyperparameters and training procedures used for TEMPO and the baseline models to ensure reproducibility.",
            "16": "- Conduct additional experiments to compare TEMPO with baseline models that are tuned to their optimal performance.",
            "17": "- **Explore generalizability:**\n     - Test TEMPO on real-world, noisy time series data to evaluate its robustness and generalizability beyond benchmark datasets.",
            "18": "- Investigate the performance of TEMPO with fine-tuning on specific datasets to provide insights into its practical applicability in various domains.",
            "19": "- **Expand the discussion:**\n     - Provide a more in-depth discussion on the potential limitations of TEMPO and how they could be addressed in future work.",
            "20": "- Discuss the implications of the findings for the broader field of time series forecasting and potential applications in different industries."
        },
        "G9AdPXMHXn": {
            "0": "Writing: The paper is well-written for the most part.",
            "1": "Interpretability: The authors aim to shed some light on the time-series predictions made by the model.",
            "2": "Modeling time-series and text together: I really liked this key insight of the paper.",
            "3": "I think it is under-explored and valuable.",
            "4": "Theory: The authors provide some theoretical insight into the design decisions behind their model.",
            "5": "Claims:\n1.",
            "6": "While achieving strong prediction performance, the previous works on timeseries mostly benefit ... that captures temporal dependencies but overlooks a series of intricate patterns within timeseries data, such as seasonality, trend, and residual.\"",
            "7": "-- I do not agree with the authors, multiple recent approaches using deep learning for time-series forecasting have decomposed model inputs into trend and seasonal components.",
            "8": "See N-BEATS, N-HITS (stacked MLP based models), AutoFormer, as an example.",
            "9": "Theorem 3.1 -- I do not fully understand the implications of Theorem 3.1 and how that affects the design choices of the authors.",
            "10": "Prompt pool captures seasonal and trend components: The authors provide an example of 3 time-series from 1 dataset to demonstrate that the prompts capture season and trend components, but I am not sure this is sufficient evidence.",
            "11": "It would be interesting to look at the distribution of prompts for multiple (or all) time-series in one or more datasets, as time-series are clustered based on their trend and/or seasonality components.",
            "12": "I believe this would give a more, dataset level evidence for the authors' claims.",
            "13": "Interpretability: I am not sure how the GAM and SHAP provide interpretability, beyond confirming what is expected from these models, i.e.",
            "14": "the residuals do not have any pattern.",
            "15": "Experimentation: \n1.",
            "16": "\"Large-scale experiments/benchmarks\": The authors omit several benchmarks, and therefore I would argue that the experiments are not large-scale.",
            "17": "For e.g., for long horizon datasets, the authors do not use the Influenza-like Illnesses and Exchange-rate datasets which PatchTST and TimesNet, and other recent studies.",
            "18": "Secondly, there are multiple short-horizon benchmarks, like M3 or the M4 datasets, and the much larger and comprehensive Monash Forecasting archive, yet the authors do not confirm their methods on these datasets.",
            "19": "Multiple methods are omitted from the evaluation, for e.g.",
            "20": "statistical methods such as Auto-ARIMA, Auto-THETA, Auto-ETS etc., and deep learning methods such as N-HITS and N-BEATS.",
            "21": "Also the authors cite PromptCast but do not compare their method to this particular baseline.",
            "22": "The value of prompt pool-- The authors demonstrate in Table 9 that the prompt pool helps model prediction.",
            "23": "How would they explain the methods without prompt pooling doing better on some datasets for some forecasting horizons?",
            "24": "Clarity:\n1.",
            "25": "Insufficient details in model design and experimentation.",
            "26": "See Questions.",
            "27": "Minor: \n1.",
            "28": "Typos: inclduing, outpemforms ... etc.",
            "29": "References: I would encourage the authors to find references to accepted papers, instead of citing their ArXiv versions."
        },
        "I4gu39uxqe": {
            "0": "1) The paper is well-written.",
            "1": "2) The improvement of forecasting performance over benchmark methods is significant and consistent across datasets.",
            "2": "1) The idea of decomposition of trend, seasonality, and residuals is not that novel and has been used for time-series forecasting.",
            "3": "2) The theorem 3.1 does not directly prove the point “more importance in current transformer-based methods as the attention mechanism, in theory, may not disentangle the disorthogonal trend and season signals automatically\".",
            "4": "3) The result shown in Figure 2 seems to be obvious since the trend is easier to learn and may take a large portion of the data."
        },
        "tk1gvink0K": {
            "0": "This paper’s writing is clear and easy to follow.",
            "1": "For example, the methodology part gives a clear description on how to build the time series input representation and the design of the prompt pool.",
            "2": "The experiments on long-term time series forecasting, short-term forecasting and towards foundation model’s training are well organized to prove the model’s power from different aspects.",
            "3": "The proposed solution is well motivated: the motivation of decomposition is supported by both empirically and theoretically and the introduce of retrieval-based prompt selection can help the large pre-trained model handle complex non-stationary time series data with distribution shifts.",
            "4": "Utilizing the pre-trained transformer backbone, TEMPO give a state-of-the-art results on the popular time series research dataset.",
            "5": "The prompt pool’s improvement is limited: the prompt pool is supposed to have more contribution to the accuracy as the intuition is clear and convincing.",
            "6": "The collection of TETS dataset is not clear: a clear but simple discription in the main paper is necessary.",
            "7": "It seems only decoder-based pretrain model is considered in this paper.",
            "8": "The encoder based backbone (like Bert) and encoder-decoder based backbone (like T5) is also recommend in this stage."
        }
    },
    "64kSvC4iPg": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical challenge in online language model inference by proposing a novel context key/value compression method.",
            "1": "- The proposed method, Compressed Context Memory (CCM), offers a significant reduction in memory usage and computational overhead while maintaining performance levels.",
            "2": "- The integration of a lightweight conditional LoRA into the language model's forward pass without the need for fine-tuning the entire model is a novel approach.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-defined and innovative solution to the problem of expanding context in online language model scenarios.",
            "4": "- The proposed method demonstrates substantial improvements in memory efficiency and throughput, as evidenced by empirical evaluations.",
            "5": "- The approach is versatile and applicable to various applications, including conversation, personalization, and multi-task learning.",
            "6": "- The paper includes comprehensive experiments and comparisons with existing methods, showcasing the superiority of the proposed approach.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in certain sections**\n     - The explanation of the compression and memory update processes could be more detailed to enhance understanding.",
            "8": "- The description of the parallelized training strategy might benefit from additional clarification and examples.",
            "9": "- **Limited discussion on potential limitations**\n     - The paper does not thoroughly discuss potential limitations or edge cases where the proposed method might not perform as well.",
            "10": "- There is a lack of analysis on the impact of different compression token lengths and their effect on performance.",
            "11": "- **Insufficient exploration of alternative methods**\n     - The paper primarily focuses on comparisons with a few existing methods and does not explore a broader range of alternative approaches.",
            "12": "- There is limited discussion on how the proposed method compares to other state-of-the-art techniques in different contexts or applications.",
            "13": "Suggestions for improvement\n   - **Enhance clarity and detail in explanations**\n     - Provide more detailed explanations and visual aids for the compression and memory update processes to improve reader comprehension.",
            "14": "- Include additional examples and diagrams to illustrate the parallelized training strategy and its benefits.",
            "15": "- **Discuss potential limitations and edge cases**\n     - Include a section discussing potential limitations of the proposed method and scenarios where it might not perform optimally.",
            "16": "- Analyze the impact of different compression token lengths and provide guidelines on selecting appropriate lengths for various applications.",
            "17": "- **Expand comparisons with alternative methods**\n     - Conduct a broader comparison with a wider range of existing methods to provide a more comprehensive evaluation of the proposed approach.",
            "18": "- Discuss how the proposed method compares to other state-of-the-art techniques in different contexts or applications, highlighting its unique advantages and potential drawbacks."
        },
        "s2cM2Ff25b": {
            "0": "The exact method proposed is novel.",
            "1": "On the three presented datasets, it seems to work pretty well, judging from the small gap between compressed and uncompressed setups.",
            "2": "The problem this paper is trying to address is very important to the field, and I believe this work has significant contributions.",
            "3": "Finally, the paper is generally easy to read.",
            "4": "This method could be evaluated on more diverse datasets, such as those used for long context, by utilizing a sliding window for example.",
            "5": "From my perspective, this work has the potential to be applied more broadly beyond ICL, or dialog.",
            "6": "It'll also be nice to have the comparison with RMT and AutoCompressor in the main text, as they are very relevant for this problem."
        },
        "fR3Rsgvg3g": {
            "0": "- A interesting method to compress contexts in the few-shot learning setting.",
            "1": "- The results evaluated in the few-shot learning tasks show the effectiveness and superiority over the conventional approaches like RMT and Gist.",
            "2": "While this paper presents a seemingly promising solution to long contexts, I have significant concerns about several limitations.",
            "3": "Firstly, one of the main focuses of this paper is handling dynamic context for interaction.",
            "4": "Judging from its experimental design, it mainly conducts experiments with a fine-tuned LLM for few-shot learning scenarios, which are generally simpler tasks, all being multi-choice, or classification tasks.",
            "5": "The methods primarily compared in this paper are general context compression or long context handling methods (general and generative tasks).",
            "6": "This involves 3 issues.",
            "7": "The first issue: for simpler tasks like classification, compression is relatively easy (this is why previous models were easily distilled but GPT was not; I believe it's not because GPT is hard to distill, but because GPT is not for a specific task, but a general model.",
            "8": "The authors may better know what I said if reading the paper about information bottleneck: https://arxiv.org/pdf/1503.02406.pdf).",
            "9": "Therefore, it's not surprising that this method achieves good compression results (model size compression and context compression are similar, both reducing model capacity), but I believe it's hard for this paper's method and results to scale to general scenarios.",
            "10": "At least in this paper, I didn't see any general tests to prove its compression effect.",
            "11": "The second issue: for the few-shot learning setting, adding compression tokens after the demonstration makes sense, but for general scenarios, there is no definite boundary to limit compression tokens, making this method hard to generalize.",
            "12": "Even if a compression token can be added every K tokens, this approach would lead to inefficient training, as a large amount of sampling is required to ensure the model learns well for each position.",
            "13": "The third issue: the main setting of this paper is few-shot learning, and its main claim is online interaction.",
            "14": "But for few-shot learning, there doesn't seem to be any online interaction.",
            "15": "Users usually provide all demonstrations at once for the model to give an answer, and it's hard for me to imagine a setting where users incrementally provide demonstrations to the model.",
            "16": "Secondly, for dialogue tasks, this is a context compression for a specific task (more preciesely, for a specific dataset).",
            "17": "Similar to what I mentioned above, if it's for a specific task/benchmark, there is actually a lot of compression space, which has been discussed in many previous works, such as: https://arxiv.org/pdf/2301.12726.pdf.",
            "18": "For context compression of a specific task, even without this method, other methods should also achieve good compression results.",
            "19": "Thirdly, I am not entirely convinced by the results presented in the paper.",
            "20": "For example, in Table 15, the performance of RMT is almost the same as that of No context, which is hard to believe.",
            "21": "Moreover, Table 15 is an experiment conducted on OPT-2.7b.",
            "22": "The few-shot learning ability of the 2.7b OPT model, as far as I understand, should be very weak, and changes in the order of sample arrangement will significantly affect the results.",
            "23": "For a method like this paper's, which is similar to a recurrent method, it should be very easily influenced by later samples.",
            "24": "Unfortunately, I didn't see any discussion about this.",
            "25": "Some questions:\n1.",
            "26": "Figure 4 is a little confusing.",
            "27": "In (a), what do the blocks in different colors mean?",
            "28": "It seems that this work uses 1 single 80GB A100 for training.",
            "29": "Could the authors provide more details about the fine-tuning process?"
        },
        "720yspkERI": {
            "0": "The paper is overall sound.",
            "1": "The method design is concise, effective, and efficient.",
            "2": "Compared with retrieval-based method to re-compute the sentence embedding, the CCM can directly adopt the KV cache of introduced <COMP> token as the memory vector for one utterance and utilize them in further inference.",
            "3": "To engage the LLM to utilize such CCM, the parallel training and LoRA adapter are designed well for efficient adaptation.",
            "4": "The CCM is efficient in both training and inference.",
            "5": "Firstly, there is no need to re-compute the sentence embedding and instead caching the attention keys and values.",
            "6": "Secondly, the memory storage cost, the compression ratio, and the algorithmic complexity all demonstrate the efficiency of the method.",
            "7": "Thirdly, LoRA based adapter tuning brings a lot of efficiency in memory-engaged adaptation.",
            "8": "The evaluation is comprehensive and diverse.",
            "9": "Three important benchmarks, MetaICL, LaMP, and DailyDialog are selected for evaluation and MetaICL covers 26 tasks with high-diversity.",
            "10": "The CCM method is not that novel and has been explored well in some important early milestones before the creation of Transformer, i.e., Memory Networks, Fast Weights to Attend Recent Past.",
            "11": "The author should mention and discuss the relation with these methods.",
            "12": "Additionally, the Compress Transformer should be briefly introduced as it is not a universally known preliminary for readers.",
            "13": "In terms of the baselines, in the main tables, CCM is only compared with “no context\" and \"full context\" baselines on accuracy, which lacks sufficient comparison for demonstrating the effectiveness of the method.",
            "14": "As least, the retrieval-based method should be considered as an important baseline and it is now the universally-adopted method for memory compression.",
            "15": "If I understand the paper correctly, the token embedding produced by <COMP> token is the same as a sentence embedding.",
            "16": "The author can follow MemoryBank for adopting retrieval-based method to compress the long-context during online interactions.",
            "17": "Other baselines like LongMem and UnlimitedFormer can be also considered but not necessary."
        },
        "5tIU8XBC9Z": {
            "0": "- The problem of efficiently handling expanding contexts is highly relevant given the online nature of systems like ChatGPT.",
            "1": "The paper addresses an important open challenge.",
            "2": "- The method is flexible and broadly applicable to diverse online inference scenarios like multi-task learning, personalization and conversation.",
            "3": "- Empirical evaluations across three datasets substantiate the memory and computation advantages over baselines.",
            "4": "The method achieves slightly lower performance than the full context with 5x smaller memory.",
            "5": "- The parallel training strategy is effective in enabling large model optimization.",
            "6": "The conditional adapter improves compression capability.",
            "7": "- The complexity analysis clearly articulates the efficiency benefits, and ablation studies validate the design choices.",
            "8": "- The main limitation of the proposed compression framework is that it is task-specific.",
            "9": "The compression module must be trained for each task, which requires additional data, computation, and cannot generalize to new tasks.",
            "10": "This is a significant drawback in the context of foundation models which are trained on large datasets for general-purpose use.",
            "11": "- There is still a obvious gap in performance between the compressed and full context models.",
            "12": "The paper does not provide a clear explanation for this gap.",
            "13": "The authors should provide more analysis into why the compressed context is less effective.",
            "14": "- While the compression framework is novel, the proposed memory update functions are basic.",
            "15": "More sophisticated memory update mechanisms could further enhance capability.",
            "16": "- The comparison is primarily with simple adaptations of fixed-context compression methods.",
            "17": "A direct comparison to recurrent memory approaches, such as linear Transformers, would be more informative."
        }
    },
    "UX9lljSZqX": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a novel Unified Static and Dynamic Network (UniSDNet) for efficient video grounding, inspired by human visual perception biology.",
            "1": "- The approach integrates static and dynamic modeling to handle cross-modal semantic associations between text/audio queries and video content.",
            "2": "- The proposed method achieves state-of-the-art (SOTA) performance on multiple datasets for both Natural Language Video Grounding (NLVG) and Spoken Language Video Grounding (SLVG).",
            "3": "Potential reasons for acceptance\n   - The innovative combination of static and dynamic modeling inspired by human visual perception mechanisms.",
            "4": "- Significant performance improvements over existing methods on widely used datasets, demonstrating the effectiveness of the approach.",
            "5": "- The introduction of new datasets (Charades-STA Speech and TACoS Speech) for SLVG, which can facilitate further research in this area.",
            "6": "- The detailed experimental results and ablation studies that validate the contributions of different components of the proposed model.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability of the paper:**\n     - The paper is dense with technical details, which may make it difficult for readers to follow the key contributions and innovations.",
            "8": "- Some sections, such as the methodology, could benefit from clearer explanations and more intuitive descriptions of the proposed techniques.",
            "9": "- **Evaluation and comparison:**\n     - While the paper reports SOTA results, it would be beneficial to include more detailed comparisons with a broader range of baseline methods, especially those that are closely related to the proposed approach.",
            "10": "- The paper could provide more insights into the specific scenarios or types of queries where the proposed method excels or struggles compared to existing methods.",
            "11": "- **Generalization and applicability:**\n     - The paper focuses on specific datasets and tasks.",
            "12": "It would be valuable to discuss the generalizability of the proposed method to other video grounding tasks or domains.",
            "13": "- The applicability of the method to real-world scenarios and its potential limitations in practical settings are not thoroughly addressed.",
            "14": "Suggestions for improvement\n   - **Improve clarity and readability:**\n     - Simplify the presentation of the methodology by breaking down complex concepts into more digestible parts and using more intuitive explanations.",
            "15": "- Include more visual aids, such as diagrams and flowcharts, to illustrate the key components and processes of the proposed model.",
            "16": "- **Expand evaluation and comparison:**\n     - Include a more comprehensive set of baseline methods for comparison, highlighting the differences and advantages of the proposed approach.",
            "17": "- Provide detailed analysis and discussion of the results, including specific examples where the proposed method outperforms or underperforms compared to baselines.",
            "18": "- **Discuss generalization and applicability:**\n     - Explore the potential of the proposed method to generalize to other video grounding tasks or domains, and provide experimental results or case studies to support this.",
            "19": "- Address the practical applicability of the method, including potential challenges and limitations in real-world scenarios, and suggest possible solutions or future work to overcome these challenges.",
            "20": "- **Release code and datasets:**\n     - Ensure that the new datasets and source code are made publicly available as promised, to facilitate reproducibility and further research in the field."
        },
        "1L3ZND615P": {
            "0": "+ The manuscript is overall well-organized and easy to follow.",
            "1": "+ The motivation behind the static and dynamic interactions based on the brain activity is clear and compelling.",
            "2": "+ The two-stage information aggregation methods are shown to be effective, where each component is appropriately designed.",
            "3": "+ The experimental results are very strong, clearly outperforming the existing approaches for both grounding tasks.",
            "4": "+ The collected spoken language grounding datasets will significantly benefit the research community.",
            "5": "The authors are encouraged to publish the code and data after the review process.",
            "6": "I did not find major weaknesses in this paper, yet summarize some questions about the method below.",
            "7": "- What is the motivation behind the implementation of Static Semantic Supplement Network?",
            "8": "I am wondering how the cross-modal interaction is performed through the MLP layers.",
            "9": "To my understanding, the shared weights across different modalities would extract some common features spanning different modalities.",
            "10": "Some analytical experiments on this would be beneficial.",
            "11": "Also, the architecture design seems similar to that of Transformer blocks except for the self-attention.",
            "12": "What happens if we use the conventional Transformer layers?",
            "13": "- The proposed architecture exploits multiple queries at once, to facilitate the model learning.",
            "14": "However, how the number of queries affects the performance is not diagnosed.",
            "15": "An ablative study on the number of queries regarding performance and cost would be helpful.",
            "16": "- In Figure 5, the effectiveness of the proposed filtering GCN is clearly verified.",
            "17": "On the other hand, there are some interesting tendency differences between NLVG and SLVG.",
            "18": "That is, the graph convolution layer itself is important, yet different layer modeling brings insignificant performance gaps on NLVG.",
            "19": "In contrast, on SLVG, the graph modeling brings negligible gains alone, but the proposed filtering mechanism shows substantial improvements.",
            "20": "How can one interpret this phenomenon?",
            "21": "If you have, please share some insights.",
            "22": "- The proposed method is well validated in the datasets with one-to-one matching between queries and moments.",
            "23": "How would it perform for one-to-many matching datasets, such as QVHighlights [1]?",
            "24": "[1] Lei et al.",
            "25": "\"QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries\", Neurips, 2021.",
            "26": "(Minor)\n\nThe manuscript contains some formatting errors due to the excessively small margins between captions and the main text.",
            "27": "They should be handled appropriately to raise the quality of the paper."
        },
        "pi86s7onrw": {
            "0": "Good performance on both NLVG and SLVG benchmarks.",
            "1": "It is nice to see an extension from NLVG to SLVG with a newly proposed benchmark.",
            "2": "The proposed method proves effective on both tasks.",
            "3": "Detailed implementation details and prediction analysis in the appendix.",
            "4": "The inspiration from human visual perception biology is not very motivating.",
            "5": "Specifically, it is hard to see why a MLP with residual connection is the way to achieve the “global broadcast communication” of the brain.",
            "6": "Either bridge the gap or Simply drop the bio-inspiration and go straight into the technical method.",
            "7": "When expanding a single gaussian kernel to multi-kernel Gaussian, it seems that only the bias $z_i$ is sweeping?",
            "8": "Have you tried different $\\gamma$?",
            "9": "Ablation in Fig 5 shows mostly similar results especially on NLVG, indicating that the designs in Dynamic Filter Graph actually do not quite matter."
        },
        "OFRQYAs5FY": {
            "0": "Most parts of the paper is well-written, clearly demonstrating the motivation,  methodology and experiments.",
            "1": "The methodology part is kind of easy to follow.",
            "2": "The idea is motivated from the human visual perception biology, which formulates an interesting story for this paper.",
            "3": "Extensive experiments successfully demonstrate the effectiveness of each proposed component of this work, which is good.",
            "4": "The visualization and figures are plus to show more intuitions.",
            "5": "The final results of this paper achieves the state-of-the-art from both efficiency and effectiveness perspectives.",
            "6": "The introduction reads like a related work.",
            "7": "It will be great to make more comparison between this work and previous work.",
            "8": "Answering what is wrong with previous works?",
            "9": "and where the efficiency and performance gain come from in this paper?",
            "10": "This paper introduces some new/confusing terminologies with their own definition, which hurts the reading experience.",
            "11": "For example, 'static semantic supplement network' and 'activity-silent mechanism' are actually the global context interaction.",
            "12": "Although the motivation of static and dynamic network is demonstrated, the justification of specific design is not enough.",
            "13": "For example, in the static network, transformer architecture or the recent S4[1] architecture can also be used as long-range filter.",
            "14": "Some ablation studies regarding either the performance or efficiency would be great to include.",
            "15": "In the dynamic network, not sure why use Gaussian filter on the distance (d_{ij}).",
            "16": "Can you provide more insights?",
            "17": "why not directly use the distance.",
            "18": "No notation for the 'FNN'.",
            "19": "Is this the feedforward network?",
            "20": "In the Figure 5, no notation/description for 'D'.",
            "21": "[1]  Efficiently modeling long sequences with structured state spaces.",
            "22": "ICLR 2021"
        },
        "q6tRFCz9qI": {
            "0": "The proposed Dynamic Temporal Filter Network captures more fine-grained context correlations between video clips based on a well-desgined graph network.",
            "1": "The proposed method achieves state-of-the-art performance on NLVG and SLVG tasks.",
            "2": "In this work, two new SLVG datasets are collected based on existing NLVG datasets.",
            "3": "Compared with previous multi-queried methods, the proposed UniSDNet has less model parameters and is more efficient according to the average inference time per query.",
            "4": "In ResMLP, visual features and multiple query features are concatenated and fed into the network, largely leveraging the information leakage between different queries (because the features incorporate more accurate textual information that describes the video content).",
            "5": "If each query is individually input into the network, would this method exhibit a significant performance degradation?",
            "6": "In the ablation study, individually employing the static network and DTFNet yields significant improvements compared to the baseline.",
            "7": "However, the combination of both modules does not exhibit a notably large improvement compared to using either single module.",
            "8": "Is there a specific explanation for this phenomenon?",
            "9": "The authors should provide more details about the baseline models."
        }
    },
    "50P9TDPEsh": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical thinking and critique abilities of large language models (LLMs), which is an underexplored area in current LLM evaluation frameworks.",
            "1": "- The introduction of a unified evaluation framework and a new benchmark, CRITIC BENCH, to assess critique abilities across diverse tasks is a significant contribution.",
            "2": "- The study's findings on the scaling behavior of critique abilities and the challenges of self-critique provide valuable insights for future research and model development.",
            "3": "Potential reasons for acceptance\n   - The paper introduces a novel benchmark (CRITIC BENCH) that fills a gap in the current evaluation frameworks for LLMs by focusing on critique abilities.",
            "4": "- The study provides comprehensive analysis and insights into the critique abilities of various LLMs, including the scaling behavior and the challenges of self-critique.",
            "5": "- The proposed self-check method demonstrates a practical application of critique abilities to improve model performance, showcasing the potential utility of the research.",
            "6": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper could provide more detailed explanations of the data selection and annotation processes to ensure reproducibility.",
            "7": "- The criteria for selecting high-quality and differentiable examples from the initial pool of responses could be more explicitly defined.",
            "8": "- **Limited scope of evaluation:**\n     - The evaluation is primarily focused on a few specific tasks (math problem-solving, code completion, and question answering), which may not fully represent the critique abilities of LLMs across a broader range of tasks.",
            "9": "- The study could benefit from including more diverse tasks and scenarios to provide a more comprehensive assessment of critique abilities.",
            "10": "- **Insufficient comparison with existing methods:**\n     - The paper does not provide a thorough comparison with existing methods or benchmarks that assess similar capabilities, which could help contextualize the contributions and improvements made by CRITIC BENCH.",
            "11": "- The effectiveness of the self-check method could be compared with other potential approaches to leveraging critique abilities for model improvement.",
            "12": "Suggestions for improvement\n   - **Enhance clarity and detail in methodology:**\n     - Provide a more detailed description of the data selection and annotation processes, including specific criteria and examples, to ensure reproducibility and transparency.",
            "13": "- Clarify the complexity-based sample selection strategy and how it is applied to different tasks.",
            "14": "- **Expand the scope of evaluation:**\n     - Include a wider range of tasks and scenarios in the evaluation to provide a more comprehensive assessment of critique abilities.",
            "15": "- Consider incorporating tasks that involve more complex reasoning, creativity, or domain-specific knowledge to test the limits of LLMs' critique abilities.",
            "16": "- **Improve comparison with existing methods:**\n     - Conduct a thorough comparison with existing benchmarks and methods that assess similar capabilities to highlight the unique contributions and improvements made by CRITIC BENCH.",
            "17": "- Compare the self-check method with other potential approaches to leveraging critique abilities, such as fine-tuning the critic model or using critiques to supervise the policy model, to demonstrate its relative effectiveness.",
            "18": "- **Address limitations and future work:**\n     - Acknowledge the limitations of the current study, such as the focus on specific tasks and the use of prompting-based critic models, and discuss potential directions for future research.",
            "19": "- Explore the potential for fine-tuning critic models or integrating critique abilities into the training process to further enhance model performance and self-improvement capabilities."
        },
        "NW5KsbCn3m": {
            "0": "- To explore the critique ability of LLMs is interesting, and timely at this point.",
            "1": "- This paper provides a standardized way to evaluate the critique ability of LLMs on diverse tasks, \n- The paper offers several noteworthy insights, such as the challenges associated with self-critique in LLMs.",
            "2": "These findings can guide future research and model development.",
            "3": "- The evaluation is not comprehensive.",
            "4": "While it claims to evaluate the critique ability, it only evaluates this across three tasks: math, code, and commonsense.",
            "5": "A broader range of tasks should be tested.",
            "6": "- The paper does not discuss potential biases.",
            "7": "Without discussing these biases, it's unclear how they might influence the evaluation results, which could affect the validity of the findings.",
            "8": "- Authors could offer a more in-depth analysis of the utility of self-critique.",
            "9": "Understanding why self-critique could be better and its influence on critique capabilities would strengthen the paper's arguments.",
            "10": "- The paper's presentation appears disjointed.",
            "11": "The content seems pieced together without careful review.",
            "12": "Consistency in terminology is essential for clarity.",
            "13": "- The paper does not define key terms like the policy model and critic model.",
            "14": "- Lack of related work.",
            "15": "- Despite introducing a benchmark, the authors do not release it, limiting its utility and reproducibility for the research community."
        },
        "8EcckOIEUs": {
            "0": "The paper addresses an important and under-explored aspect of LLMs, which is their ability to critique their own outputs.",
            "1": "This is a valuable contribution as it moves beyond traditional evaluation metrics and looks at a model's ability to self-improve.",
            "2": "The paper presents a clear definition of critique ability and distinguishes between critique and self-critique, which helps in setting the scope and understanding the objectives of the study.",
            "3": "The paper could benefit from a more detailed discussion on the limitations of the current approach, particularly regarding the scalability of the self-check method and its applicability to real-world scenarios [1,2,3].",
            "4": "The study is limited to a few tasks and datasets.",
            "5": "Expanding the benchmark to include more diverse tasks and domains would make the findings more generalizable.",
            "6": "The evaluation of self-critique abilities shows that models struggle with certain tasks, but the paper does not delve deeply into why this is the case or propose potential solutions to improve self-critique performance.",
            "7": "The paper does not address the potential ethical implications of models that can self-critique and self-improve, especially in terms of reduced human oversight.",
            "8": "References \n\n[1] Madaan, Aman, et al.",
            "9": "\"Self-refine: Iterative refinement with self-feedback.\"",
            "10": "arXiv preprint arXiv:2303.17651 (2023).",
            "11": "[2] Krishna, Satyapriya.",
            "12": "“On the Intersection of Self-Correction and Trust in Language Models.” (2023).",
            "13": "[3] Huang, Jie, et al.",
            "14": "\"Large language models cannot self-correct reasoning yet.\"",
            "15": "arXiv preprint arXiv:2310.01798 (2023)."
        },
        "86snFxKYr8": {
            "0": "The paper is well-written and easy to follow.",
            "1": "The authors are very clear about all details in the data collection process and provided good motivation for the various design choices.",
            "2": "The evaluation is thorough and covers a wide range of models.",
            "3": "The proposed new heuristic is not particularly novel, but achieves solid improvement on the new benchmark.",
            "4": "A critique in this paper is defined as a language model assessment of another language model output on some underlying task.",
            "5": "A good critique model should be effective at identifying flaws in language model outputs.",
            "6": "The challenging examples to the task of critique are nuanced flaws, which would also require a detailed explanation by the critique model.",
            "7": "But the benchmark proposed by this paper use a simplistic quantitative metric that reduces the quality of a critique to a binary decision, which assumes that it’s appropriate to use a binary metric for the underlying task as well.",
            "8": "The benchmark offers very limited granularity.",
            "9": "Using a granular quantitative measure means that the qualitative questions that the benchmark can answer are also limited.",
            "10": "Outside of developing and evaluating self-refinement heuristics like the one proposed by the authors, the benchmark provides limited information for other uses of model-generated critique, such as informing human oversight.",
            "11": "Since the benchmark requires tasks with well-defined, fully-automated metrics for the underlying task, the problem of developing self-refinement critiques does not in fact depend on such a benchmark: even if the model critique doesn’t make sense to a human, as long as it improves subsequent prediction accuracy, it’s a good critique."
        }
    },
    "4qFIkOhq24": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the development and deployment of large language models (LLMs): the limitations of alignment processes in ensuring safe and desirable behavior.",
            "1": "- The introduction of the Behavior Expectation Bounds (BEB) framework provides a novel theoretical approach to understanding and analyzing the inherent limitations of alignment in LLMs.",
            "2": "- The paper's findings have significant implications for AI safety, particularly in the context of adversarial prompting attacks and the robustness of alignment methods like reinforcement learning from human feedback (RLHF).",
            "3": "Potential reasons for acceptance\n   - The paper tackles a highly relevant and timely problem in the field of AI and LLMs, contributing to the ongoing discourse on AI safety and alignment.",
            "4": "- The BEB framework is a novel and theoretically sound approach that offers new insights into the limitations of current alignment methods.",
            "5": "- The theoretical results are supported by empirical demonstrations, enhancing the credibility and applicability of the proposed framework.",
            "6": "- The paper is well-structured, with clear definitions, theorems, and proofs that are accessible to the reader.",
            "7": "Potential reasons for rejection\n   - **Lack of empirical validation:**\n     - The empirical demonstrations are limited to specific behaviors and models, which may not generalize to other LLMs or behaviors.",
            "8": "- The experiments rely on approximations and assumptions that may not fully capture the complexities of real-world LLM distributions.",
            "9": "- **Assumptions and limitations of the BEB framework:**\n     - The framework assumes ground truth behavior scores per sentence, which may not be realistic in practice due to the complexity and ambiguity of behavior scoring.",
            "10": "- The notion of decomposability into well-behaved and ill-behaved components may oversimplify the actual behavior of LLMs, which could be more nuanced and context-dependent.",
            "11": "- **Potential overemphasis on theoretical results:**\n     - The paper's focus on theoretical results may overshadow practical considerations and solutions for improving alignment in LLMs.",
            "12": "- The practical implications of the findings, such as the feasibility of implementing external filters and controllers, are not thoroughly explored.",
            "13": "Suggestions for improvement\n   - **Expand empirical validation:**\n     - Conduct more extensive experiments with a wider range of LLMs and behaviors to validate the generalizability of the BEB framework.",
            "14": "- Include comparisons with other alignment methods and frameworks to highlight the strengths and weaknesses of BEB.",
            "15": "- **Refine assumptions and definitions:**\n     - Provide a more detailed discussion on the limitations and assumptions of the BEB framework, including potential ways to address them.",
            "16": "- Explore alternative models of decomposability and behavior scoring that may better capture the complexities of LLM behavior.",
            "17": "- **Enhance practical relevance:**\n     - Discuss practical strategies for mitigating the limitations of alignment identified by the BEB framework, such as the development of more robust alignment methods or external control mechanisms.",
            "18": "- Include case studies or real-world examples to illustrate the practical implications of the theoretical findings and how they can inform the design of safer LLMs."
        },
        "C4g9zxTNlK": {
            "0": "This paper introduces and examines a framework for the theoretical study of LLM alignment.",
            "1": "While acknowledging potential limitations within the framework and its underlying assumptions, it presents an original perspective for the theoretical analysis of a complex empirical phenomenon.",
            "2": "The writing of this paper is clear and easy to follow, with most definitions and assumptions followed by high-level intuition.",
            "3": "My main comments are focused on three topics:\n\nThe mixture model seems to be a very strong assumption on what the models entail after pretraining.",
            "4": "Details are discussed in Questions.",
            "5": "Although empirical values for problem parameters are provided in the experiments, it is still hard to comprehend each assumption and their overall importance to the derived results.",
            "6": "Details are discussed in Questions.",
            "7": "Some experiment details are lacking.",
            "8": "See below."
        },
        "jT3gYLYaFc": {
            "0": "The paper attempts to offer a much needed theoretical base to the problem of aligning of LLMs.",
            "1": "The paper has a solid theoretical analysis that shows that under certain conditions, adversarial prompting can result in very low probability behaviours being exhibited with high probability.",
            "2": "The authors study these behaviours also in real-world models and show that adversarial prefixing can indeed be used to misalign a model.",
            "3": "The definition of γ-prompt-misalignment is extremely conservative: The existence of a single prompt resulting in misaligned behaviour is sufficient to label the whole model misaligned.",
            "4": "This makes this is a binary condition and it is not that surprising that there exists at least one prompt that will result in an undesirable behaviour.",
            "5": "However, this is not a realistic setting and in practice more nuanced measures of “misalignment” are needed.",
            "6": "The definition for β-distinguishability is very strict and, contrary to the claims in the paper, it is not clear to me whether $\\mathbb P_{-}$ and $\\mathbb P_{+}$ would be at all distinguishable in practice.",
            "7": "That is, because the definition requires that bound (5) holds *for any prefix* $s_0$.",
            "8": "However, while the components can be polar opposite in one sense, e.g.",
            "9": "“agreeableness”, the models are likely similar in many other ways.",
            "10": "E.g., “Which is the capital of France” is probably going to be completed with “Paris”, by both $\\mathbb P_{-}$ and $\\mathbb P_{+}$.",
            "11": "If that’s the case, then $\\beta=0$ and that’d invalidate the paper’s results.",
            "12": "The same issue seems to appear in the experimental estimation of $\\beta$.",
            "13": "It seem that the authors are not actually estimating $\\beta$.",
            "14": "The KL divergence is computed only for prefixes sampled from the unconditional negative distribution $\\mathbb P_{-}$ which of course has a bias.",
            "15": "This results in over-approximating $\\beta$, possibly by a lot.",
            "16": "However, if one considers all sentences $s_0$, there would be many for which the completion would be the same (e.g.",
            "17": "the Paris example), hence $\\beta$ would be 0.",
            "18": "Overall, Section 2.2 which is critical for understanding the claims of the paper is not clearly presented.",
            "19": "I would strongly recommend the authors to add examples of, e.g.",
            "20": "β-distinguishable and non-β-distinguishable distributions, as well as α,β,γ-negatively-distinguishable and non-α,β,γ-negatively-distinguishable factorizations.",
            "21": "The paper also fails to discuss the limitations of the analysis and the conditions under which it holds.",
            "22": "While the plausibility of the factorisation of the distribution is mentioned, I am missing the discussion on the other technical assumptions, as mentioned above."
        },
        "sgJMkMj5LQ": {
            "0": "Originality: The Behavior Expectation Bounds (BEB) framework offers a novel theoretical perspective on the alignment issues of LLMs.",
            "1": "Quality: The paper effectively combines theoretical insights with empirical results to support its claims.",
            "2": "The formalisms and theorems provide a solid foundation for the study.",
            "3": "Clarity: The paper is well-structured and the distinction between theoretical and empirical sections ensures the reader can follow the progression of ideas.",
            "4": "Significance: The problem of LLM alignment is pressing, and the paper's findings can influence future practices and methodologies in training and deploying these models.",
            "5": "Assumption Limitations: The framework is based on some strong assumptions, such as the decomposition of LLMs into distinct behavioral components.",
            "6": "This could be overly simplified or not universally applicable.",
            "7": "Overemphasis on Theoretical Aspects: While the theorems and formalizations are valuable, the balance between theoretical and practical aspects could be adjusted to appeal to a broader audience."
        },
        "m3GEG92kff": {
            "0": "+ The theory is presented clearly: the assumptions are presented well and the theorems explained nicely.",
            "1": "+ The potential impact of the work is quite large: this work presents fundamental limits on the ability of models to be correctly aligned.",
            "2": "If current trends continue and large models continue to increase in capability, this points towards important implications of an inability to avoid potentially very hazardous misalignment.",
            "3": "+ Experimental results go some way towards backing up the theoretical claims.",
            "4": "+ The analysis of the conversational and aligning prompt cases are interesting, and appropriate given the focus on conversational agents in the previous year.",
            "5": "The result that conversations can require longer adversarial input is counter-intuitive at first, but makes sense upon reading the proof and analysis.",
            "6": "+ The fact that all the results are asymptotic seems to be a limitation to the results.",
            "7": "Of course, developing finite-sample bounds is likely much harder than asymptotic results.",
            "8": "In principle, the results could be vacuous if the constants were large enough.",
            "9": "Given recent work on finding adversarial prompt injections, I don't think the results are actually vacuous, but I think a brief discussion of this is warranted in the paper.",
            "10": "+ The relevance of the experimental results is debatable, as investigating the fine-tuned models is not the same as investigating the different modes $P_-$, $P_+$.",
            "11": "I understand that direct examination of the modes is perhaps impossible, but I would like to see more discussion of the feasibility of this.",
            "12": "+ There is no discussion about the computational feasibility of finding adversarial prompts.",
            "13": "In light of the combinatorially large search space of all possible contexts of length $n$ of size $V^n$ for vocab size $n$, the main result is less impressive unless it is computationally tractable to find these adversarial injections.",
            "14": "Again, I think a discussion of recent injection techniques should address this concern in the paper."
        }
    },
    "3EWTEy9MTM": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical gap in understanding the mechanism behind Chain of Thought (CoT) in large language models (LLMs).",
            "1": "- It provides a theoretical framework to explain how CoT enhances the expressiveness of decoder-only transformers, particularly for inherently serial problems.",
            "2": "- The work bridges the gap between empirical observations and theoretical foundations, offering a new perspective on the capabilities of LLMs.",
            "3": "Potential reasons for acceptance\n   - The paper introduces a novel theoretical framework that explains the power of CoT in LLMs, which is a significant contribution to the field.",
            "4": "- It provides rigorous proofs and empirical evidence to support the claims, demonstrating the practical implications of the theoretical findings.",
            "5": "- The work is well-structured, with clear definitions, theorems, and proofs, making it accessible to readers with a background in machine learning and computational complexity.",
            "6": "- The empirical results corroborate the theoretical analysis, showing substantial improvements in solving inherently serial problems with CoT.",
            "7": "Potential reasons for rejection\n   - **Complexity and Accessibility**\n     - The paper's theoretical content is highly complex and may be challenging for readers without a strong background in computational complexity and automata theory.",
            "8": "- The notation and formalism used throughout the paper could be overwhelming for a broader audience, potentially limiting its impact.",
            "9": "- **Empirical Validation**\n     - The empirical validation is based on synthetic data and specific tasks, which may not fully represent the diverse range of real-world problems LLMs encounter.",
            "10": "- The experiments focus on a limited set of problems, and it is unclear how well the findings generalize to other tasks or domains.",
            "11": "- **Assumptions and Limitations**\n     - The paper assumes a non-uniform model of computation, which may not align with practical implementations of LLMs that require uniformity for scalability.",
            "12": "- The reliance on specific architectural choices (e.g., decoder-only transformers) may limit the applicability of the findings to other types of neural network architectures.",
            "13": "Suggestions for improvement\n   - **Simplify Theoretical Explanations**\n     - Consider simplifying the theoretical explanations and providing more intuitive insights to make the content more accessible to a broader audience.",
            "14": "- Include more diagrams and visual aids to illustrate key concepts and theorems, helping readers grasp the complex ideas more easily.",
            "15": "- **Expand Empirical Validation**\n     - Conduct additional experiments on a wider range of tasks, including real-world datasets, to demonstrate the generalizability of the findings.",
            "16": "- Compare the performance of CoT-enabled transformers with other state-of-the-art models on these tasks to provide a more comprehensive evaluation.",
            "17": "- **Address Practical Considerations**\n     - Discuss the implications of the non-uniform model assumption and explore potential ways to achieve similar results with uniform models.",
            "18": "- Provide insights into how the findings can be applied to other neural network architectures, such as encoder-decoder models or recurrent neural networks.",
            "19": "- **Clarify Assumptions and Limitations**\n     - Clearly state the assumptions and limitations of the theoretical framework and empirical validation, and discuss potential avenues for future research to address these limitations.",
            "20": "- Include a section on the practical challenges of implementing CoT in real-world LLMs and suggest possible solutions or workarounds."
        },
        "ofubcIbvxL": {
            "0": "The first contribution (power of hard attention) may not be the most realistic model of practical transformers (see Weaknesses), but it is still potentially valuable for filling out the theory of transformers of different types.",
            "1": "The second contribution showing log-precision transformers with rounding are still in TC0 is quite interesting and solves an technical problem unresolved in prior work.",
            "2": "Even though the result is not fully general (requires zero exponent), the progress here is quite exciting and perhaps could be extended in future work.",
            "3": "I would like some of the assumptions for the P/poly result to be better discussed (see Weaknesses), but it is a valid and potentially useful result for formalizing the power of CoT.",
            "4": "The paper is generally well-written and organized, and I appreciate the inclusion of a neat empirical study.",
            "5": "## Limitations of Constant Precision\n\nConstant precision is not necessarily a realistic setting, since it means transformers cannot attend based on positional encodings or compute uniform attention (which require $\\log n$ bits).",
            "6": "In the practical regime, transformers have enough precision to express uniform attention over their input length and can use uniform attention to recognize majority ([Merrill et al., 2021](https://aclanthology.org/2022.tacl-1.49/)), which is outside your upper bound of AC0.",
            "7": "Presumably, if we wanted to apply transformers on very long input lengths, we would scale up the precision of attention logarithmically so that uniform attention and positional embeddings would remain expressible.",
            "8": "For this reason, [Merrill & Sabharwal (2023)](https://neurips.cc/virtual/2023/poster/70153) propose studying the log-precision model instead of the constant precision one.",
            "9": "To put it another way, let's say you ran the same experimental setup as Figure 3 but with Majority instead of Iterated Squaring.",
            "10": "If transformers are in AC0, we'd expect a similar qualitative pattern where models without CoT struggle without sufficient depth but models with CoT can succeed.",
            "11": "But I think that's not what you would find: instead, even models of one layer could do well using a single uniform attention head.",
            "12": "To be clear, even though I think log-precision is more realistic, I think it is still potentially interesting to analyze the constant-precision case to fill out the overall theory and understand the value of log precision.",
            "13": "However, the authors should discuss the differences between constant-precision and log-precision and specifically mention or respond to the argument for log-precision from [Merrill & Sabharwal (2023)](https://neurips.cc/virtual/2023/poster/70153).",
            "14": "It could also be helpful to run the experiment I described above with Majority and potentially include the results in the appendix.",
            "15": "## Nonuniform Embeddings\n\nThe paper characterizes transformers with T steps as P/poly.",
            "16": "There is something weird about this result, in that it characterizes transformers by a nonuniform complexity class that contains undecidable problems (e.g., the unary encoding of the halting problem)!",
            "17": "In contrast, we know that transformers cannot compute undecidable problems since they can be implemented on standard computers.",
            "18": "This disconnect comes from the fact that the embeddings are assumed to be **nonuniform**: i.e., they can be any sequence of $O(\\log n)$ bits on inputs of size $n$.",
            "19": "This enables the embeddings to be able to encode advice for solving undecidable problems, which standard positional transformers cannot do because they are computable.",
            "20": "This assumption of nonuniform embeddings should be better highlighted in Section 3.4: right now it's not even visible in the theorem statement.",
            "21": "It would also be good to add some discussion of the assumption in the introduction or other high-level parts of the paper so readers don't miss it."
        },
        "2WoJs9OwlX": {
            "0": "The main strengths of this paper are:\n\n1. its rigorousness in making clear, concise statements of its findings (except for one important \"detail\" discuss under weaknesses);\n\n2. a formal characterization of the power of transformers with CoT, for which results have come out only very, very recently (after the ICLR submission deadline);\n\n3. carefully crafted arguments and proofs around rounding of numbers when performing addition of $n$ numbers, a key step used in multiple prior papers unless less realistic assumptions; and\n\n4. empirical evaluation to support the theory, which is often missing in similar theoretical characterizations of transformer variants in the past.",
            "1": "I have not read the proofs in detail (esp.",
            "2": "the material in the appendix), but the results and approach intuitively appear plausible.",
            "3": "I really have only one, albeit big, concern about the paper, namely **the formal model of transformers being studied is different not only from all prior theoretical works but also from practical use of transformers**.",
            "4": "This is made worse by the lack of a discussion of this difference.",
            "5": "Consequently, while the results appear to tighten prior upper bounds and provide novel lower bounds, they really are applicable to a different model.",
            "6": "Specifically, the authors assume a model of transformers that is **non-uniform** (in the sense it is used in circuit complexity), namely, for each $n$, there is a **different** transformer.",
            "7": "As they state in Defn 3.4, \"for every ... $n$, there is a $L$-layer ... transformer\".",
            "8": "This means that one needs a *family* of transformers, one for each input size $n$, to solve a given problem, and the weights of the transformer for input length $n$ may have nothing to do with that of the transformer for input lengths $n+1$.",
            "9": "To specify such a family, one thus needs to specify an infinite family of unrelated weights, which obviously is unrealistic.",
            "10": "In contrast, in practice, one trains a transformer on inputs of certain lengths, freezes those weights, and then uses the same frozen-weights transformers for inputs (and chains-of-thought) of arbitrary lengths.",
            "11": "In fact, inspired by this, the theoretical research on the representation power of transformers in the past 2-3 years has gone in the opposite direction---from non-uniform upper bounds, to tighter and tigher uniform bounds (e.g., log-space uniform, log-time uniform, FO-uniform, etc.).",
            "12": "Importantly, all along, the model of transformers used was uniform.",
            "13": "The *non-uniformity* of the model assumed here has other, well-known undesirable consequences also seen in non-uniform circuits---it allows transformers to **trivially solve certain undecidable problems**, namely any undecidable unary problem, over the alphabet $\\{1\\}$.",
            "14": "E.g., it can solve the halting problem expressed in unary, just like all non-uniform circuit classes (including P/poly) can.",
            "15": "Besides being unrealistic, this raises a question about the meaningfulness of the technique used to prove the main result (Theorem 3.3).",
            "16": "In order to simulate a circuit of size $T(n)$, one must somehow *embed* the circuit into the transformer as the transformer needs to know what circuit to compute.",
            "17": "As seen from the proof of it in the appendix, the authors have a creative solution: they put the description of the circuit in the *positional embedding* of the transformer.",
            "18": "While interesting and unique, this has two undesirable implications:\n\n1.",
            "19": "The positional embedding for inputs of length $n$ is allowed to be arbitrarily different from the positional embeding for inputs of length, say, $n+1$ (because there is no uniformity constraint between the circuits for the two respective sizes, $n$ and $n+1$), which departs heavily from practice.",
            "20": "It means that the proposed construction must include some **uncomputable / undecidable positional embeddings**!",
            "21": "To see this, consider any undecidable unary problem $P$ in P/poly.",
            "22": "The typical polysize circuit construction for $P$ is to have, for each $n$, a trivial circuit $C_n$ that outputs a $1$ if and only if $1^n$ is in that undecidable language (e.g., $1^n$ is the unary encoding of the $n$-th Halting problem).",
            "23": "Thus, by the authors' construction, there is a trivial transformer that decides the same language---and its embedding of the first position computes membership in that undecidable language!",
            "24": "In other words, the embedding itself in not computable by any reasonable model of computation.",
            "25": "To summarize this, while the construction is correct to my understanding, it is for a formal model that departs from practice and assumes a lot of power (e.g., that of having access to potentially uncomputable embeddings).",
            "26": "At the very least, these limitations and their implications should be clearly discussed in the paper.",
            "27": "**Minor points**\n\n* In the abstract, the statement \"with $T$ steps of CoT, constant-depth transformers ... can solve ...\" should be qualified with T being at most a polynomial in $n$.",
            "28": "* In the 2nd last line of page one, I think you mean \"encoder-only\" rather than \"decoder-only\"; or single-step decoder.",
            "29": "* In line 3 of section 2, do you mean $\\phi(bin_k(x)) = x$ rather than $\\ldots = 0$?",
            "30": "* In the 2nd paragraph of section 2, where is the input length limited to $n_\\max$?",
            "31": "* page 3, two lines before defn 3: \"over more two\" => \"over more than two\""
        },
        "BHBs4nGhdi": {
            "0": "Overall i felt it was a quite well written paper and the case was made well.",
            "1": "The paper formalizes the problem precisely, which is not only critical for answering the question considered in the paper, but is also of independent interest.",
            "2": "I think the parameterisation of CoT  in terms of embedding size and depth, is novel and interesting.",
            "3": "There are several well chosen examples that made comprehension easier.",
            "4": "The empirical evaluation is fairly convincing but it does not really reveal anything new insights not already covered by the proofs.",
            "5": "Minor  errors:\nSection 1: poewrful -> powerful"
        },
        "dCCNzcot8V": {
            "0": "To my mind, there were three main strengths to the paper.",
            "1": "Its theoretical approach: I greatly appreciated the theoretical bend to the paper.",
            "2": "Phrasing things in terms of problem classes that a particular model can solve is the kind of I'd like to see more of.",
            "3": "Interesting examples: The problem classes they used were interesting, novel, and nicely targeted to the theoretical results\n\n3.",
            "4": "A detailed and rigorous approach to a intuitive idea.",
            "5": "The supporting information section that includes very detailed proofs and ample details for the interested reader.",
            "6": "In terms of the primary dimensions, this paper was quite strong in two of them: Originality and Significance\n\nOriginality: As previously noted, phrasing the CoT problem in terms of specific problem classes for particular families of models is a fantastic idea, something we don't see nearly enough of in the field.",
            "7": "I also thought the experiments performed were quite original - finding problems in particular problem classes takes a great deal of effort and creativity, the problems they used were not brand new, but were new to the area of CoT and transformer models\n\nQuality: The experiments performed were well chosen and did a good job supporting the theory\n\nSignificance: This paper does make a significant claim and provides good evidence to back it up.",
            "8": "Understanding the power of transformer models, where they face challenges, and what problem classes they excel at, is a very significant result and exactly the kind of result that should be featured at an ICLR tier conference.",
            "9": "For all the strengths of this paper, there were a number of weaknesses as well.",
            "10": "The performance evaluation - data was trained and evaluated using \"freshly sampled synthetic data\".",
            "11": "I think this is problematic at several levels.",
            "12": "First, without a carefully though out test/train split, the model is at risk of overfitting.",
            "13": "I think that is fine here - because overfitting is still telling you information about the expressive power of your model class, however this goes against the grain of standard model training practice, and at least deserves comment.",
            "14": "Second, the problems used were all discrete problems.",
            "15": "Discrete problems are great test beds for theoretical arguments - but in most of these examples, the problem space is finite for a fixed size, and randomly sampling from a discrete space gives the artificial impressing of having more data than is actually available.",
            "16": "Finally, how you sample from these discrete spaces seems like it would make a big difference on the model performance\n\nThe model discussion was also significantly lacking.",
            "17": "At the very least, a discussion of what the class of transformer models looks like and how you are bounding it belong in the main body of the text, not banished to appendix G on the final page of the supporting information.",
            "18": "The details that were present, were in the form of Algorithm 1, defining an implementation, but not the class itsself.",
            "19": "That made it difficult to tell what was the result of the model class and what was the result of the training process.",
            "20": "Finally, this paper would be stronger if the presentation was as crisp and focused as the data classes and results.",
            "21": "In particular, there was an odd mix of too much detail, too little detail, and superfluous detail.",
            "22": "For example, Definitions 3.1 and 3.2 were primarily used in appendix C so their presentation on page 3 distracted from the main arguments of the paper."
        },
        "jTocaS1d05": {
            "0": "(+) Important result regarding the expressiveness of transformers.",
            "1": "The finding seems very obvious intuitively, but the novelty seems to be in a rigorous proof.",
            "2": "(+) Realistic modeling of finite precision computations\n\n(+) The theoretical finding is matched with empirical results on four tasks in arithmetic.",
            "3": "(1) I'm not sure I grasp the significance of the finding.",
            "4": "It seems obvious that any model can perform serial computations if it is allowed to store intermediate results somewhere (in this case, in the output sequence, which can be written to and subsequently access with self-attention).",
            "5": "Therefore, producing such intermediate output seems as expressive as a chain of multiple instances of the model (this does not even specific to transformers).",
            "6": "Is this intuition misguided?",
            "7": "Perhaps the important finding is rather that a transformer *without* CoT could not solve serial problems?",
            "8": "(but this was covered in previous work).",
            "9": "Note that this topic is not exactly my area of expertise (I was called as an emergency reviewer), but this should be addressed since it is likely something that other readers will wonder about.",
            "10": "---------------------\n\n(2) Presentation could be improved.",
            "11": "These are details though that are easy to fix.",
            "12": "The abstract could be clearer about what form of CoT is studied in the paper (whether it's about the training data, the test phase, the prompting, ...).",
            "13": "The very first sentence seemed clear in retrospect to me, not on the first read.",
            "14": "So it's probably good to be even more explicit and specify that it's just about instructing the model to generate intermediate steps.",
            "15": "There are some informal shortcuts in the technical language that should be fixed.",
            "16": "Examples:\n\"transformers with polynomial intermediate steps\" -> \"with a number of intermediate steps polynomial in ...\"\n\"transformers with linear intermediate steps\"\n\"poly width and log precision\"\netc.",
            "17": "Typos: poewrful, circuit valuation, \"because every regular languages\" (should be singular)\n\nSec.",
            "18": "4\"we find that cotis always\" (missing space, also in multiple other places in this section)\n\nWeird grammar in the abstract: \"previous works have ...\"\nI suppose you mean \"previous works show that ...\""
        },
        "GbA6LcPI3e": {
            "0": "- Analyzing the effect of the presence or absence of CoT on model expressivity from a theoretical perspective.",
            "1": "- Proposing a tighter upper bound for constant depth transformers' expressive power.",
            "2": "- The motivation of this paper is clear.",
            "3": "- Symbols are not clearly described.",
            "4": "The authors  don't make clear instructions when using symbols like NC^1, AC^0, etc.",
            "5": "It is recommended that these symbols be explained when they are first mentioned.",
            "6": "- The conclusions of Theorem 3.8 and Theorem 3.9 presented in the paper seem to be contradictory.",
            "7": "Theorem 3.8 indicates that enlarging the embedding size to poly(n) doesn't improve the model's expressiveness of T(n) = poly(n) step CoT.",
            "8": "On the other hand, Theorem 3.9 demonstrates that broadening the embedding width strengthens the model's power for any particular polynomial T(n) = n^k step CoT.",
            "9": "The conclusions of these two theorems are confusing and require further clarification."
        }
    },
    "ARP0xaE6od": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of content moderation using Large Language Models (LLMs), which is highly relevant given the increasing volume of online communication.",
            "1": "- It explores the novel approach of incorporating reasoning processes during fine-tuning to enhance model robustness and interpretability.",
            "2": "- The study provides a detailed examination of data engineering and supervised fine-tuning (SFT) techniques, which are crucial for developing effective content moderation systems.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a significant and timely problem in the field of content moderation, which is essential for maintaining safe online environments.",
            "4": "- It introduces innovative methods for improving the performance and robustness of LLMs in content moderation tasks, including the use of reasoning processes and weak supervision.",
            "5": "- The experimental results demonstrate the effectiveness of the proposed methods, showing improvements over baseline models and competitive performance compared to GPT-3.5.",
            "6": "- The paper provides a comprehensive analysis of different fine-tuning strategies and their impact on model performance, offering valuable insights for future research and practical applications.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper could benefit from a more detailed explanation of the data engineering and fine-tuning processes, including specific steps and parameters used.",
            "8": "- The description of the weak supervision approach and how it was implemented could be more explicit to ensure reproducibility.",
            "9": "- **Insufficient comparison with existing methods:**\n     - The paper primarily compares the proposed methods with GPT-3.5 and GPT-4 but lacks a thorough comparison with other state-of-the-art content moderation techniques.",
            "10": "- Including a broader range of baseline models and methods would strengthen the validity of the findings.",
            "11": "- **Limited discussion on limitations and future work:**\n     - The paper does not adequately address the limitations of the proposed methods, such as potential biases in the training data or the scalability of the approach.",
            "12": "- A more detailed discussion on future research directions and how to address the identified limitations would enhance the paper's contribution to the field.",
            "13": "Suggestions for improvement\n   - **Enhance methodological clarity:**\n     - Provide a step-by-step description of the data engineering and fine-tuning processes, including specific parameters and settings used in the experiments.",
            "14": "- Clarify the implementation details of the weak supervision approach, including how the reasoning processes were generated and validated.",
            "15": "- **Expand comparative analysis:**\n     - Include a broader range of baseline models and state-of-the-art content moderation techniques in the experimental comparison to provide a more comprehensive evaluation of the proposed methods.",
            "16": "- Discuss the performance of the proposed methods in relation to these additional baselines to highlight their strengths and weaknesses.",
            "17": "- **Address limitations and future work:**\n     - Identify and discuss the limitations of the proposed methods, such as potential biases in the training data or challenges in scaling the approach to larger datasets.",
            "18": "- Provide suggestions for future research directions, including how to address the identified limitations and potential areas for further improvement.",
            "19": "- **Improve presentation and readability:**\n     - Ensure that all figures and tables are clearly labeled and referenced in the text, with sufficient explanation provided for their interpretation.",
            "20": "- Consider restructuring the paper to improve the flow of information and make it easier for readers to follow the key points and findings."
        },
        "9hOTxLJk2E": {
            "0": "clear implementation details - the first point I like the paper most is it provide details to the extent that it is easy for others to reimplement their method.",
            "1": "The prompts, the process, and fine tuning methods, make it a hands on guide to fine tuning domain specific task using llm.",
            "2": "improvement over baseline model - from the experiment I find the purposed reasoning prompt method has significantly improved the two baseline llm's performance.",
            "3": "this is useful in that not all scenarios is suitable for cloud llm providers, and it is reasonable self trained or open source models could perform bad on a domain specific task.",
            "4": "in general I think the authors of the papers are resolving a real world issue\n\nmiscs - ablation studies, discussions, etc are meaningful 1. from the comparison the purposed methods do not seem to beat gpt 3.5 model by a large margin, and in OOD test CInsult dataset seems the improvement is trivial.",
            "5": "although this does not negates the usefulness of the reasoning, it seems to me cloud llm provider still has advantage in most of the tasks\n\n2. some of the tables/figures are not annotated/referenced in the paper like table 2, which is kind of confusing at first glimpse (the first row of table 2 is the setting A in table 1)\n\n3. table 3 is kind of confusing - what do you want to express with the figure?",
            "6": "training set performance is similar for reasoning and expansion?",
            "7": "how is it related to the main claim of the paper?"
        },
        "k8boqC9Wkc": {
            "0": "The paper discusses the suitability of LLMs to serve as content moderation tools, specifically using open-source models.",
            "1": "This is an important area of research since it provides insights into how open-source models can be used in that context and how they compare to commercial models (and hence has potential widespread applicability).",
            "2": "* The paper claims that their approach results in performances which are \"not weaker than\" those of GPT-3.5.",
            "3": "However, the results in Table 1 do not necessarily support that claim, since performance results are still lower than GPT-3.5 across most of the investigated categories.",
            "4": "* No details on datasets provided: the paper uses a privately collected and annotated dataset for experimentation (in-distribution), however no details on the number of examples and label distributions are provided.",
            "5": "* Related to the above, it would also be helpful to show how many examples have been incorrectly labeled by GPT-3.5 in Setting B and Setting C. How reliably can GPT-3.5 be used in that context?",
            "6": "* The explanations generated by GPT-3.5 to enhance the training datasets have not been manually verified.",
            "7": "While I acknowledge that an increase in performance hints at their usefulness, it would be interesting to see how such reasoning samples look like, and whether they truly represent what they are intended to.",
            "8": "Furthermore, which prompt formulations were used to obtain the reasoning and label?",
            "9": "Did you ask the model to first label and then generate reasoning, or vice versa?",
            "10": "Analyzing such details can have a notable impact on performance and should be discussed in more detail.",
            "11": "* Table 2 in the paper is not referred to in the manuscript?",
            "12": "* In Table 3, there are no details on which metrics are being reported, making it difficult to understand the shown results.",
            "13": "* Re.",
            "14": "Table 4: The paper assumes that length serves as an indicator for detailedness, which would need to be verified with manual annotations."
        },
        "g4CsnLldOF": {
            "0": "+ content moderation is an important research topic\n+ exploiting large language models for content moderation is a promising direction - I wonder if the authors could better clarify which part of the methodology corresponds to addressing the three challenges proposed in the introduction.",
            "1": "In addition, how does the proposed approach alleviate the need for quality annotated data?",
            "2": "- What is the unique technical contribution of the proposed pipeline?",
            "3": "Data deduplication is not new, and fine-tuning with chain-or-thought paths neither.",
            "4": "I'm also not exactly sure if the \"weak supervision\" part claimed in the methodology is very accurate.",
            "5": "It would be great to better highlight in the methodology the parts taken from existing works and the parts uniquely proposed by this work.",
            "6": "- How is self-recheck actually performed in step 2.b?",
            "7": "Is this a prompting-based approach?",
            "8": "Please provide more details as there is much unclear about this important step.",
            "9": "- I'm not sure about the contributions of Table 1 and Section 3.2.",
            "10": "Yes, fine-tuning a smaller model for a specific task outperforms ChatGPT, which might be considered already established.",
            "11": "I wonder if the authors could provide more analysis/insights as to this gigantic table.",
            "12": "- One of the major concerns is that there isn't any supervised baseline in this work.",
            "13": "While the authors proposed an approach based on supervised fine-tuning and claims for performance gains, it would be better to support it with supervised baselines proposed in recent literature.",
            "14": "- Maybe this is a minor point, but the taxonomy of \"content moderation\" could be better supported by related literature.",
            "15": "There is much research and discussion on what constitutes \"content that should be moderated\" and the ethical considerations associated with it.",
            "16": "- The related work on content moderation is inadequate.",
            "17": "It mostly discusses 2023 works with large language models, potentially overlooking a decade of research on related topics such as misinformation, hate speech, Twitter bots, and more.",
            "18": "It is suggested to greatly enrich related work discussions and baselines to better position this work in the context of the research landscape."
        }
    },
    "MiPacpmtmy": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the underexplored area of compositional generalization in multimodal models, which is crucial for advancing AI's ability to handle complex, real-world tasks.",
            "1": "- The introduction of the COMPACT dataset, which is carefully constructed to test compositional generalization in a multimodal setting, represents a significant contribution to the field.",
            "2": "- The study's focus on sequential compositional generalization using real-world egocentric kitchen activity videos adds a novel dimension to existing research.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a relevant and timely problem in AI research, contributing to the understanding of multimodal compositional generalization.",
            "4": "- The introduction of the COMPACT dataset provides a valuable resource for future research in this area.",
            "5": "- The comprehensive evaluation of various unimodal and multimodal models offers insightful findings that highlight the importance of multimodal inputs.",
            "6": "- The experimental results are well-presented and demonstrate the challenges and potential of multimodal models in achieving compositional generalization.",
            "7": "Potential reasons for rejection\n   - **Limited scope of the dataset:**\n     - The COMPACT dataset is derived from the EK-100 dataset, which focuses on kitchen activities.",
            "8": "This domain-specific focus may limit the generalizability of the findings to other contexts.",
            "9": "- The dataset's reliance on egocentric videos may not capture the full range of multimodal interactions present in other real-world scenarios.",
            "10": "- **Evaluation metrics and analysis:**\n     - The paper primarily uses standard metrics like BLEU, Exact Match, Categorical Accuracy, and BERTScore, which may not fully capture the nuances of compositional generalization.",
            "11": "- The analysis of model performance could benefit from more detailed error analysis to understand the specific challenges faced by the models.",
            "12": "- **Model selection and fusion strategies:**\n     - The paper evaluates a limited set of models and fusion strategies, which may not represent the full spectrum of possible approaches in multimodal learning.",
            "13": "- The chosen fusion strategies may not be the most effective for all types of multimodal data, and alternative approaches could yield different insights.",
            "14": "Suggestions for improvement\n   - **Expand the dataset:**\n     - Consider extending the COMPACT dataset to include a wider variety of activities and domains beyond kitchen tasks to enhance the generalizability of the findings.",
            "15": "- Incorporate additional sources of multimodal data, such as social interactions or outdoor activities, to provide a more comprehensive test bed for compositional generalization.",
            "16": "- **Enhance evaluation metrics and analysis:**\n     - Introduce more sophisticated evaluation metrics that can better capture the nuances of compositional generalization, such as metrics that account for semantic similarity or context-specific accuracy.",
            "17": "- Conduct a detailed error analysis to identify specific types of compositional errors and provide insights into the limitations of current models.",
            "18": "- **Explore alternative models and fusion strategies:**\n     - Evaluate a broader range of multimodal models, including those that use different architectures or training paradigms, to provide a more comprehensive understanding of the field.",
            "19": "- Experiment with alternative fusion strategies, such as late fusion or attention-based mechanisms, to determine the most effective approaches for integrating multimodal data.",
            "20": "- **Address limitations and future work:**\n     - Acknowledge the limitations of the current study, such as the domain-specific focus and the simplicity of the textual utterances, and outline potential directions for future research.",
            "21": "- Propose extensions to the current work, such as incorporating more complex language data or exploring the impact of different types of multimodal interactions on compositional generalization."
        },
        "7ZaSdQcnLR": {
            "0": "The paper is clear and well-written.",
            "1": "The study of multimodal models on natural video datasets is quite timely.",
            "2": "The idea of curating a subset of an existing dataset, rather than creating one from scratch, is a sound idea, especially since there are many similar datasets out there.",
            "3": "The paper is essentially an evaluation of well-defined tasks using an existing dataset and pre-trained models.",
            "4": "Out of the three tasks, the problem of action classification is only slightly different from next-utterance noun prediction and verb prediction (i.e., action is a combination of noun + verb, and the other two are predicted separately.)",
            "5": "Various pre-trained models are run in a zero-shot manner to predict the next entity in the sequence.",
            "6": "While the evaluation results could be of interest to someone who is looking to build these models, the paper main content offers little beyond this evaluation."
        },
        "VunhYePF1j": {
            "0": "The data distributions between training and evaluation in the CompAct benchmark are carefully controlled, allowing for the diagnosis of models' compositional generalization capabilities, which could be useful to the research community.",
            "1": "The authors present experimental results for approximately ten different unimodal or multimodal models.",
            "2": "Some of the results are intriguing; for example, the language-only method outperforms the multimodal method in noun classification.",
            "3": "However, for verb classification or next utterance prediction, the multimodal methods demonstrate superior performance.",
            "4": "The proposed method for curating train/eval splits to diagnose compositional generalization appears to be applicable to many other existing video datasets.",
            "5": "The authors have overlooked several works and benchmarks that are highly similar (see Questions below).",
            "6": "Compared to these existing works, the contribution of this paper does not seem to be very significant.",
            "7": "Additionally, the conclusions drawn from the experiments (e.g., recognition that compositional generalization is an area requiring improvement or that multi-modality could be more important than single-modality for certain challenging tasks) lack depth and insight."
        },
        "UrA4si1dpH": {
            "0": "The detailed strengths are as follows:\n1.",
            "1": "This paper is interesting because it is trying to understand the compositional generalization capabilities of foundation models.",
            "2": "This is a crucial skill for intelligent agents and yet there are limited work and benchmarks proposed to investigate the question.",
            "3": "Paper in this topic should be encouraged.",
            "4": "It investigates the important topic of compositional generalization capabilities in foundational models.",
            "5": "This is a crucial skill for intelligent agents and yet there are limited research and benchmarks in this domain.",
            "6": "Studies like this should be encouraged.",
            "7": "- However, the paper appears to have limitations in addressing this issue for large-scale pre-trained foundational models.",
            "8": "See weaknesses for details.",
            "9": "To answer this question, the paper presents a carefully curated novel dataset from real-world videos which could be much useful for future studies.",
            "10": "The paper also designs a set of multimodal models use different combinations of modalities (including unimodal) and different ways of fusing the multi-modal information.",
            "11": "This investigation provides valuable insight on how multi-modality inputs could influence the performance of models' compositional generalization ability.",
            "12": "The paper does not sufficiently investigate the compositional generalization ability of **foundation** models.",
            "13": "Addressing this is challenging due to the potential distributional discrepancies between training and testing splits during their pretraining, as noted in the paper.",
            "14": "Consequently, emphasizing \"foundation models\" in the title may be somewhat overstated.",
            "15": "- Could incorporating domain-specific fine-tuning offer additional insights?",
            "16": "The dataset's domain-specific nature results in text descriptions that lack diversity.",
            "17": "As a result, unlike foundation LLM, language models trained on these specific tests might be prone to overfitting and lack reasoning skills.",
            "18": "On the other hand, other modalities, such as the vision input processed by a pretrained ResNet model, inherently resist overfitting, potentially leading to enhanced generalization.",
            "19": "Thus, the conclusion that multi-modality contributes to improvements and that visual features consistently enhance results could potentially be invalid."
        },
        "M2jEVfY3qn": {
            "0": "The paper focus on studying the composition of foundation models on many variants, on Epic kitchen dataset that is tailored for composition evaluation.",
            "1": "The paper draws a conclusion that multimodal helps composition, yet from Table 1, the trend is not very clear."
        }
    },
    "AL1fq05o7H": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Mamba, a new class of selective state space models (SSMs) that aim to combine the modeling power of Transformers with the computational efficiency of linear-time sequence models.",
            "1": "- The novelty lies in the integration of selective mechanisms into SSMs, allowing the model to perform content-based reasoning and selectively propagate or forget information based on the input.",
            "2": "- The proposed architecture achieves state-of-the-art performance across various modalities, including language, audio, and genomics, while maintaining linear scaling in sequence length.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a significant challenge in deep learning: the computational inefficiency of Transformers on long sequences.",
            "4": "- The proposed Mamba architecture demonstrates substantial improvements in both performance and efficiency, achieving state-of-the-art results in multiple domains.",
            "5": "- The integration of selective mechanisms into SSMs is a novel and well-motivated approach, supported by thorough empirical validation.",
            "6": "- The paper provides detailed explanations and technical contributions, including a hardware-aware parallel algorithm for efficient computation.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability:**\n     - The paper is dense with technical details, which may make it difficult for readers to follow the key contributions and innovations.",
            "8": "- Some sections, such as the explanation of the selection mechanism and hardware-aware algorithm, could benefit from clearer and more concise descriptions.",
            "9": "- **Empirical validation:**\n     - While the paper presents impressive results, it would be beneficial to include more comparisons with a broader range of baseline models, especially recent advancements in efficient sequence modeling.",
            "10": "- The evaluation on real-world tasks could be expanded to provide a more comprehensive assessment of the model's generalizability and robustness.",
            "11": "- **Ablation studies:**\n     - The paper includes some ablation studies, but additional experiments could further elucidate the impact of different components and design choices on the model's performance.",
            "12": "- More detailed analysis of the trade-offs between model complexity, performance, and computational efficiency would strengthen the paper's contributions.",
            "13": "Suggestions for improvement\n   - **Improve clarity and readability:**\n     - Simplify and streamline the explanations of the selection mechanism and hardware-aware algorithm to make them more accessible to a broader audience.",
            "14": "- Use visual aids, such as diagrams and flowcharts, to illustrate complex concepts and processes.",
            "15": "- **Expand empirical validation:**\n     - Include more comparisons with a diverse set of baseline models, including recent advancements in efficient sequence modeling.",
            "16": "- Evaluate the model on additional real-world tasks and datasets to demonstrate its generalizability and robustness.",
            "17": "- **Enhance ablation studies:**\n     - Conduct more comprehensive ablation studies to analyze the impact of different components and design choices on the model's performance.",
            "18": "- Provide detailed analysis of the trade-offs between model complexity, performance, and computational efficiency.",
            "19": "- **Broaden the scope of applications:**\n     - Explore the application of Mamba to other emerging modalities, such as video and multimodal tasks, to further demonstrate its versatility and potential as a general sequence model backbone."
        },
        "ZJCXkIMKQY": {
            "0": "S1: The paper addresses very efficiently and effectively pressing problems in sequential modeling.",
            "1": "S2: The authors have identified simple toy tasks, such as selective copying and associative recall, that enable them to make design choices which state-of-the-art impact on real-world data.",
            "2": "S3: The connection to the role of gating mechanisms in RNNs is well-appreciated.",
            "3": "S4: The empirical part of the paper is very thorough, and the results are strong.",
            "4": "I do not identify any major weaknesses of the paper."
        },
        "t7w5AlzJfq": {
            "0": "The paper is written in a clear and understandable manner, with a well-defined approach and simple yet effective improvement strategies.",
            "1": "The paper lacks references to some relevant works, such as [1], [2], [3], [4] which discusses some Linear Attention methods, and [5], which is also a LongConv method.",
            "2": "However, these references are completely absent in the paper.",
            "3": "I suggest that the authors consider adding these citations to provide a more comprehensive review of related work.",
            "4": "[1] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong.",
            "5": "cosformer: Rethinking softmax in attention.",
            "6": "In ICLR, 2022.",
            "7": "[2] Efficient Attention via Control Variates, Lin Zheng, Jianbo Yuan, Chong Wang, and Lingpeng Kong In International Conference on Learning Representations (ICLR), 2023\n\n[3] Linear Complexity Randomized Self-attention Mechanism, Lin Zheng, Chong Wang, and Lingpeng Kong In International Conference on Machine Learning (ICML), 2022\n\n[4] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong.",
            "8": "The devil in linear transformer.",
            "9": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7025–7041, Abu Dhabi, United Arab Emirates, Dec. 2022.",
            "10": "Association for Computational Linguistics.",
            "11": "[5] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong.",
            "12": "Toeplitz neural network for sequence modeling.",
            "13": "In The Eleventh International Conference on Learning Representations (ICLR), 2023."
        },
        "bq8XeiBhwY": {
            "0": "+ A key limitation of prior SSMs is the inability to efficiently select data in an input-dependent manner.",
            "1": "The paper introduces a key mechanism by parameterizing the SSM parameters based on the input, allowing the model to filter out irrelevant information and remember relevant information indefinitely.",
            "2": "+ The results as compared to Pythia, and Transforms on many benchmarks are impressive.",
            "3": "- The model still has a quadratic memory requirement during training like Transformers."
        },
        "eIarHE3vGs": {
            "0": "* The proposed Mamba method includes a simple modification to the conventional SSM model: add additional models to make SSM models dependent on the inputs.",
            "1": "SSMs are known for their computational difficulties, and the authors address this issue by several performance optimization techniques.",
            "2": "* The authors pre-train several variants of Mamba, ranging from 130M parameters to 1.4B parameters.",
            "3": "These pre-trained models show performance improvements compared with the baselines in the paper.",
            "4": "Concerns about model design:\n\n* The motivation of Mamba is to address the drawbacks of recurrent models while improving the efficiency of attention-based models.",
            "5": "There are many works following the same direction: S4-diagonal [1], SGConv [2], MEGA [3], SPADE [4], and many efficient Transformer models (e.g., [5]).",
            "6": "All of these models achieve near linear complexity, and the authors need to compare Mamba with these works in terms of both model performance and efficiency.",
            "7": "For model performance, some simple experiments such as language modeling on Wikitext-103 should suffice.",
            "8": "* Many attention-based Transformer models show length generalization ability, i.e., models can be trained on a shorter sequence length and tested on a longer sequence length.",
            "9": "Some examples include relative positional encoding (T5) and Alibi [6].",
            "10": "Because SSMs are in general sequential, does Mamba have this length generalization ability?",
            "11": "Concerns about experiments:\n\n* The authors need to compare with stronger baselines.",
            "12": "The authors acknowledge that H3 was used as a motivation for the model architecture.",
            "13": "However, they did not compare with H3 in the experiments.",
            "14": "From Table 4 in [7], ppl of H3 is 8.8 (125M), 7.1 (355M), and 6.0 (1.3B) on the Pile dataset, which are considerably better than Mamba.",
            "15": "The authors need to show comparisons with H3.",
            "16": "* For the pre-trained models, the authors only show results on zero-shot inference.",
            "17": "This setting is quite limited and the results cannot support the effectiveness of Mamba well.",
            "18": "I suggest the authors run more long-sequence experiments such as document summarization, where the input sequence is naturally long (e.g., the average sequence length of the arXiv dataset is greater than 8k).",
            "19": "* One of the main contributions that the authors claim is long sequence modeling.",
            "20": "The authors should compare with more baselines on LRA (Long Range Arena), which is essentially the standard benchmark for long sequence understanding.",
            "21": "* Memory benchmarking is missing.",
            "22": "Even though Section 4.5 is titled “speed and memory benchmark”, only speed comparisons are presented.",
            "23": "Also, the authors should provide more detailed setups of Figure 8 left, e.g., model layers, model sizes, details of the convolution, etc.",
            "24": "Could the authors provide some intuitions why FlashAttention is the slowest when the sequence length is very large (Figure 8 left)?",
            "25": "[1] https://arxiv.org/pdf/2203.14343.pdf \\\n[2] https://arxiv.org/pdf/2210.09298.pdf \\\n[3] https://arxiv.org/pdf/2209.10655.pdf \\\n[4] https://arxiv.org/pdf/2212.08136.pdf \\\n[5] https://arxiv.org/pdf/2202.10447.pdf \\\n[6] https://arxiv.org/pdf/2108.12409.pdf \\\n[7] https://arxiv.org/pdf/2212.14052.pdf"
        }
    },
    "6PmJoRfdaK": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces LongLoRA, a novel and efficient fine-tuning approach for extending the context sizes of pre-trained large language models (LLMs) with limited computational cost.",
            "1": "- The proposed method, Shifted Sparse Attention (S2-Attn), allows for efficient context extension during training while maintaining the original attention mechanism during inference.",
            "2": "- The combination of S2-Attn with an improved version of LoRA (LoRA+) demonstrates strong empirical results on various tasks and models, significantly reducing memory and computational costs.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a significant challenge in the field of LLMs by providing a method to extend context sizes efficiently, which is crucial for applications requiring long-context understanding.",
            "4": "- The proposed S2-Attn method is simple to implement and integrates seamlessly with existing architectures and optimization techniques.",
            "5": "- The empirical results show that LongLoRA achieves comparable performance to full fine-tuning with significantly lower computational costs, making it accessible to a broader range of researchers.",
            "6": "- The paper provides comprehensive experimental validation, including comparisons with state-of-the-art methods and ablation studies to demonstrate the effectiveness of the proposed approach.",
            "7": "Potential reasons for rejection\n   - **Limited novelty in the combination of existing techniques:**\n     - The paper primarily combines existing techniques (sparse attention and LoRA) rather than introducing entirely new concepts.",
            "8": "- The novelty might be perceived as incremental rather than groundbreaking.",
            "9": "- **Insufficient theoretical analysis:**\n     - The paper lacks a detailed theoretical analysis of why S2-Attn and the improved LoRA work well together for long-context fine-tuning.",
            "10": "- A deeper theoretical understanding could strengthen the paper's contributions and provide more insights into the method's effectiveness.",
            "11": "- **Potential overfitting to specific datasets:**\n     - The empirical results are primarily based on specific datasets (RedPajama, PG19, proof-pile).",
            "12": "It is unclear how well the method generalizes to other datasets or real-world applications.",
            "13": "- Additional experiments on diverse datasets could provide a more comprehensive evaluation of the method's robustness.",
            "14": "- **Scalability concerns for extremely large models:**\n     - While the paper demonstrates the method's effectiveness on models up to 70B parameters, it is unclear how well it scales to even larger models or more complex architectures.",
            "15": "- Further experiments on larger models or different architectures could address potential scalability concerns.",
            "16": "Suggestions for improvement\n   - **Enhance theoretical analysis:**\n     - Provide a more detailed theoretical analysis of the proposed S2-Attn and improved LoRA methods, explaining why they work well together for long-context fine-tuning.",
            "17": "- Include mathematical formulations and proofs to support the empirical findings.",
            "18": "- **Expand experimental validation:**\n     - Conduct experiments on a wider range of datasets, including real-world applications, to demonstrate the method's generalizability and robustness.",
            "19": "- Include additional benchmarks and comparisons with more state-of-the-art methods to strengthen the empirical validation.",
            "20": "- **Investigate scalability:**\n     - Perform experiments on even larger models and different architectures to evaluate the scalability of LongLoRA.",
            "21": "- Analyze the computational and memory requirements for these larger models to provide insights into potential limitations and optimizations.",
            "22": "- **Provide practical guidelines:**\n     - Offer practical guidelines and best practices for implementing LongLoRA in different scenarios, including recommendations for hyperparameter settings and training procedures.",
            "23": "- Include a detailed discussion on potential challenges and solutions when applying LongLoRA to various LLMs and tasks."
        },
        "nxauXgEcUM": {
            "0": "1. the paper is well written and easy to follow.",
            "1": "the proposed approach is a simple method that can adapt LLM for longer context without too much compute.",
            "2": "2. the paper has good ablation to show that LoRA on embedding and normalization is important for long-context adaptation.",
            "3": "1. the paper only evaluated on retrieval and perplexity.",
            "4": "It would be good to evaluate on other generative tasks that require longer context.",
            "5": "2. the improvement on perplexity doesn't seem super consistent in Table.",
            "6": "4"
        },
        "Ke4BKU68Fr": {
            "0": "(1) The method seems useful and impactful, and the evaluation is thorough with strong results.",
            "1": "(2) The authors perform very thorough ablations and isolate key design decisions (attention shift, modifying the norm & embedding layers) that enable the method to match full fine-tuning.",
            "2": "(3) The paper is well-written.",
            "3": "No major weaknesses."
        },
        "LhmjbCk8CO": {
            "0": "- The proposed method builds on previous work and shows strong empirical results on long lange language modelling and a retrieval task\n- The proposed approach is conceptually simple and can be implemented in a few lines of code (as demonstrated by the authors)\n- The proposed approach can be combined with existing approaches for context extension such as positional interpolation \n- The authors provide a detailed discussion of related work - The efficiency aspect of the could could be more prominently discussed in the main body of the paper\n- The presentation of the work could be improved.",
            "1": "See below for suggestions"
        },
        "yHUD09zCLP": {
            "0": "- The authors propose an extremely simple method, that performs well and is applicable to existing pretrained models - The authors only evaluate perplexity and retrieval setting"
        }
    },
    "c8McWs4Av0": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant challenge in the field of AI and natural language processing: solving complex mathematical word problems using large language models (LLMs).",
            "1": "- The introduction of the GPT-4 Code Interpreter and the novel explicit code-based self-verification (CSV) method represents a notable advancement in enhancing the reasoning capabilities of LLMs.",
            "2": "- The study provides a systematic analysis of code generation, execution, and self-debugging mechanisms, which are crucial for improving the accuracy of mathematical problem-solving.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and effective method (CSV) that significantly improves the performance of GPT-4 Code Interpreter on challenging mathematical datasets.",
            "4": "- The empirical results demonstrate substantial improvements in accuracy across multiple benchmarks, including MATH, GSM8K, and MMLU-Math datasets.",
            "5": "- The proposed verification-guided weighted majority voting (VW-voting) further enhances the reliability of the solutions, showcasing the robustness of the approach.",
            "6": "- The study offers a comprehensive analysis of the impact of code usage frequency on the performance of LLMs, providing valuable insights for future research.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper could benefit from a more detailed explanation of the CSV prompting technique and how it is implemented in practice.",
            "8": "- The description of the verification-guided weighted majority voting (VW-voting) method could be more explicit, particularly in terms of how the weights are determined and applied.",
            "9": "- **Limited comparison with other state-of-the-art methods:**\n     - While the paper compares the proposed method with a few existing approaches, a more extensive comparison with a broader range of state-of-the-art techniques would strengthen the validity of the results.",
            "10": "- The paper could include more detailed discussions on the limitations of the proposed method and how it compares to other verification techniques in terms of computational efficiency and scalability.",
            "11": "- **Insufficient analysis of failure cases:**\n     - The paper lacks a thorough analysis of the failure cases where the proposed method does not perform well.",
            "12": "Understanding these limitations is crucial for further improving the approach.",
            "13": "- Providing examples of specific mathematical problems that the method struggles with and discussing potential reasons for these failures would add depth to the evaluation.",
            "14": "Suggestions for improvement\n   - **Enhance methodological clarity:**\n     - Provide a more detailed step-by-step explanation of the CSV prompting technique, including specific examples of prompts and how the model responds to them.",
            "15": "- Clarify the process of determining and applying weights in the VW-voting method, possibly with illustrative examples to aid understanding.",
            "16": "- **Expand comparative analysis:**\n     - Include a broader range of state-of-the-art methods in the comparative analysis to provide a more comprehensive evaluation of the proposed approach.",
            "17": "- Discuss the computational efficiency and scalability of the proposed method compared to other verification techniques.",
            "18": "- **Analyze failure cases:**\n     - Conduct a detailed analysis of the failure cases where the proposed method does not perform well, providing specific examples and discussing potential reasons for these failures.",
            "19": "- Suggest possible improvements or alternative approaches to address the identified limitations and enhance the robustness of the method.",
            "20": "- **Improve presentation and readability:**\n     - Ensure that all figures and tables are clearly labeled and referenced in the text, with sufficient explanations provided for each.",
            "21": "- Consider adding a summary table that highlights the key contributions and findings of the paper, making it easier for readers to grasp the main points."
        },
        "Q933vlNNOH": {
            "0": "A good demonstration of OpenAI's new GPT4 with Python Interpreter version.",
            "1": "The experiments show the great improvement of the new model comparing w/ the traditional GPT4.",
            "2": "It also shows the potential of using code interpreter as an external tool to enhance performance on math reasoning tasks.",
            "3": "They authors push the SoTA of MATH to 84.3%, which is a very high number considering its complexity.",
            "4": "Most of the methods discussed in the paper are proposed by existing works and the main contribution of this paper is to try them out using the new OpenAI model.",
            "5": "The contribution and novelty could be a weakness of the paper.",
            "6": "Most credit of the huge improvement on MATH should be given to the better capability of GPT4-code itself.",
            "7": "The authors seem to over claim their own contribution throughout the paper.",
            "8": "For example in Page 6, the authors said \"Before the advent of GPT4-Code, prior frameworks (Lightman et al., 2023; Cobbe et al., 2021) depended on an external LLM to use natural language for verification and well-designed fewshot example prompts.",
            "9": "In contrast, our approach simplifies the process by relying solely on a straightforward prompt for GPT4-Code, all in a **zero-shot** manner.\"",
            "10": "But the root reason that zero-shot is applicable is because you are using OpenAI's instruction-tuned model.",
            "11": "In addition, it is very likely that OpenAI has already used PRM (Lightman et al., 2023) to RL tune it's new model, therefore the API you used could be based on top of Lightman et al., 2023."
        },
        "7uD78Lo0gM": {
            "0": "This paper is timely.",
            "1": "It's a novel approach of leveraging code self-verification to improve math reasoning ability.",
            "2": "It's simplicity means it can be easily used by GPT-4 users, and the thorough study provides solid evidence of adopting this technique.",
            "3": "Concretely, the paper's strengths include:\n1.",
            "4": "Novel technique that combines self-debugging and test generation for math reasoning tasks.",
            "5": "Quite complete study that highlights effectiveness of the technique.",
            "6": "Weighted voting technique is quite unique, and it can possibly used in other self-debugging techniques beyond Math reasoning tasks.",
            "7": "The paper lacks some insights into the quality of self-verification results, and how that matters for model performance.",
            "8": "The paper can potentially dive deeper into analysis of consistency between verification process and NL reasoning process as well as output correctness.",
            "9": "As shown in prior work like CodeT, some self-generated test-cases or verification code can either be wrong or inconsistent, but it may or may not affect model output quality.",
            "10": "Guessing from the paper's results on weighted voting versus simple voting, such inconsistency exists and they could benefit model performance (or affect model performance if we simply reject such answers).",
            "11": "I would suggest the authors perform some qualitative analysis to dive into this problem.",
            "12": "1b.",
            "13": "Some deeper qualitative analysis into what types of verification code are generated would also be helpful.",
            "14": "2.The paper considers both sampling and sequential self-repair.",
            "15": "It would be great if the authors can analyze the tradeoff between the depths of self-repair (e.g., if self-verification continues to fail, how much can the model benefit from continuing self-repair until verification succeed) verse breaths of self-verification (e.g., simply repair once, but generate multiple samples to do weighted sampling).",
            "16": "The paper plots are visually misleading: Figure 2 accuracy numbers should all start from 0 as opposed to 60 in figure 2a and 40 in figure 2b.",
            "17": "Otherwise the improvement looks like 10x as opposed to 6% comparing prompts 1 and 2.",
            "18": "Similarly for other figures."
        },
        "LAj677FfFc": {
            "0": "- Like many recent works, this work provides strong evidence that prompting LLMs in a proper way could significantly influence the performance\n- This work also makes another natural but still interesting finding -- the frequency of code usage has a strong correlation with the accuracy of the final answer.",
            "1": "The rationality is that executing code is more accurate/reliable than performing reasoning through natural language text.",
            "2": "- This work presents the new state-of-the-art results on three datasets (i.e., MATH, GSM8K, MMLU-Math).",
            "3": "- In terms of improving LLMs, the ideas of code writing, self-validating, and majority voting have been already explored in recent literature.",
            "4": "The novelty of this work seems a simple combination of all three ideas together.",
            "5": "- Only one particular and proprietary LLM (i.e., GPT-4) is used for evaluation.",
            "6": "Whether the results and findings reported in this work may generalize to other publicly available LLMs (e.g., Llama 2) or relatively smaller language models is unclear.",
            "7": "- Another similar concern is that only one specific kind of benchmark (i.e., grade school math problems) is used, thus the proposed prompt might be overfitting to simple math problems.",
            "8": "For the chosen benchmarks, improvements due to coding and CSV on GSM8K and MMLU-Math are already fairly small (i.e., 2-4%).",
            "9": "- The improvement seems largely due to the superior capability of GPT-4.",
            "10": "For instance, GPT-4 itself can outperform the state-of-the-art by a large margin without any sophisticated prompting and majority voting.",
            "11": "It is already well-known that even a simple prompt could dramatically influence performance (see ).",
            "12": "The particular finding of GPT-4 on the MATH benchmark is not very surprising."
        },
        "vlsGuwEwdw": {
            "0": "- The results on correlations between code usage frequency and accuracy are a nice analysis, and breaking it down by difficulty level of problem is useful (e.g.",
            "1": "at higher difficulty levels amount of code usage seems to matter more)\n- The CSV setup itself is quite simple, which is a good thing – \"Solve the problem using code interpreter step by step, even in every sub-step.",
            "2": "And following your answer, please verify it using code interpreter by yourself.\"",
            "3": "A simple way of getting a decent boost in performance from zero-shot prompting is a fairly useful contribution.",
            "4": "The boost of 3.85% accuracy on the MATH dataset from using CSV over the base prompt is solid – that's a reasonable gain.",
            "5": "- The Verification-guided weighted majority voting setup is also quite simple (just a weighted majority vote with human-picked parameters) which is good.",
            "6": "The weighting seems to provide a boost of around +1% accuracy over a standard majority voting approach (at 16 samples, MATH dataset, per Fig 6b).",
            "7": "- The precision-recall-accuracy analysis and also analysis of weighted majority vs naive majority are good to see, I was preparing to suggest that, and I'm glad it was done.",
            "8": "Overall I'm marginally below acceptance on this paper, but would certainly consider raising my score if the following points are well addressed in revisions/rebuttal.",
            "9": "- As-is the current paper has only one row in Table 1 that uses 16 samples with GPT4-Code, so the +14.63% improvement is a comparison between a 1 sample and 16 sample method.",
            "10": "This can be a bit misleading to readers (or at least it was to me) since much of that improvement comes from using additional samples, which is something that could also be done through prior work via naive majority voting (which is actually evaluated later in the paper, in Fig 6b).",
            "11": "Therefore, Table 1 should include a line for **GPT4-Code + CSV + Majority Voting** (which does about 1% worse than weighted voting, per Fig 6b).",
            "12": "- For the same reasons as in the previous comment, adding a line to Table 1 with **GPT4-Code + Majority Voting (no CSV)**, an ablation that done with neither CSV nor weighted voting, would be an informative baseline to include.",
            "13": "These two baselines would considerably strengthen the analysis.",
            "14": "- There doesn't seem to be a **GPT4-Code + CSV** row in Table 2, nor a **GPT4-Code + CSV + Voting** row in Table 3.",
            "15": "This use of different setups on different datasets is confusing – an explanation should be given, or preferably both should be added.",
            "16": "Having both of these novel methods from the paper evaluated on all 3 datasets would strengthen the paper.",
            "17": "- Figure 6a is quite confusing to me:\n    - Which dataset is used?",
            "18": "I believe it's MATH overall dataset, but this should be in the caption and optionally also main text\n    - I don't understand why there are 5 separate paths graphed, given that there's nothing special differentiating path 0 from path 1, etc.",
            "19": "The different path indices are just IID samples, so of course when you calculate precision/accuracy/recall averaged over a whole dataset of problems, all the histogram bars will look the same for different sample path indices.",
            "20": "So having 5 separate paths doesn't seem to add anything – why not just collapse them all into a single graph with three bars (accuracy, precision, recall)?",
            "21": "minor:\n- Consider weakening the statement \"Each line in Fig.",
            "22": "5 has an obvious trend of going upwards, proving that the increase of Code Usage Frequency induces a general improvement in accuracy\": *Proves* is a strong word there, when it's just a nice correlation (not causal – there are many ways of including meaningless code in an output that wouldn't not induce an improvement in accuracy).",
            "23": "Just state it as a correlation and that's good."
        }
    },
    "Pj52xO5ysY": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces an attention-based multiple instance classification model (AMIC) for interpretable word-level sentiment analysis using only document sentiment labels.",
            "1": "- The novelty lies in the model's ability to provide interpretable outputs that offer insights into context weighting, word neutrality, and negation, which are often opaque in traditional transformer-based models.",
            "2": "Potential reasons for acceptance\n   - The model achieves competitive performance at the document level while providing word-level interpretability, addressing a significant gap in sentiment analysis research.",
            "3": "- The use of multiple instance classification combined with self-attention and positional encoded self-attention is a novel approach that enhances both transparency and effectiveness.",
            "4": "- The extensive analysis and comparison with other models demonstrate the robustness and applicability of AMIC across different datasets.",
            "5": "Potential reasons for rejection\n   - **Lack of clarity in model explanation**\n     - The paper could benefit from a more detailed and clearer explanation of the model components and their interactions.",
            "6": "- Some technical terms and concepts are not adequately explained, which may hinder understanding for readers not familiar with the specific methodologies used.",
            "7": "- **Limited evaluation on diverse datasets**\n     - The model is evaluated on only two datasets (wine reviews and Twitter Sentiment140), which may not be sufficient to generalize its effectiveness across other types of text data.",
            "8": "- Additional datasets with varying characteristics could provide a more comprehensive evaluation of the model's performance and robustness.",
            "9": "- **Insufficient comparison with state-of-the-art models**\n     - While the paper compares AMIC with several models, it lacks a thorough comparison with the latest state-of-the-art models in sentiment analysis.",
            "10": "- Including more recent and advanced models in the comparison could strengthen the validation of AMIC's performance claims.",
            "11": "- **Potential overfitting concerns**\n     - The paper does not address potential overfitting issues, especially given the model's complexity and the relatively small size of the wine review dataset.",
            "12": "- Providing details on how overfitting is mitigated or avoided would enhance the credibility of the results.",
            "13": "Suggestions for improvement\n   - **Enhance clarity and detail in model description**\n     - Provide a more detailed and step-by-step explanation of the model components, their roles, and how they interact within the AMIC framework.",
            "14": "- Include visual aids or diagrams to illustrate the model architecture and the flow of data through the different components.",
            "15": "- **Expand evaluation to more diverse datasets**\n     - Evaluate the model on additional datasets with different characteristics, such as longer texts, different domains, and varying levels of sentiment complexity.",
            "16": "- This would help demonstrate the model's generalizability and robustness across different types of text data.",
            "17": "- **Include more state-of-the-art comparisons**\n     - Compare AMIC with a broader range of recent and advanced sentiment analysis models to provide a more comprehensive validation of its performance.",
            "18": "- Highlight specific scenarios or cases where AMIC outperforms these models to emphasize its unique strengths.",
            "19": "- **Address potential overfitting concerns**\n     - Discuss the measures taken to prevent overfitting, such as regularization techniques, cross-validation, or data augmentation.",
            "20": "- Provide additional analysis or experiments to show that the model's performance is not due to overfitting on the training data.",
            "21": "- **Improve interpretability examples**\n     - Include more diverse and complex examples to showcase the model's interpretability capabilities, especially in handling nuanced linguistic phenomena.",
            "22": "- Provide a more detailed analysis of how the model's interpretability features contribute to understanding the sentiment analysis process."
        },
        "ibqCN1mgRD": {
            "0": "The proposed model improves the interpretability from the word-level perspective and uses the MIC module.",
            "1": "The overall structure is well organized, and the experimental results illustrate the effectiveness of the proposed model on two public SA datasets.",
            "2": "The novelty and contributions are limited.",
            "3": "There are some existing methods using the MIC and self-attention mechanism on SA tasks.",
            "4": "The authors should introduce them and explain the main differences as well as advantages.",
            "5": "Chapter 4 in \"Sentiment Lexicon Induction and Interpretable Multiple-instance Learning in Financial Markets\".",
            "6": "In addition, for the word level interpretability of the SA model, does the author consider combining other level information in the model as illustrated in \"A Multi-Grained Self-Interpretable Symbolic-Neural Model For Single/Multi-Labeled Text Classification\" -- \"there is only word-level attribution but no high-level attribution such as those over phrases and clauses.",
            "7": "Take sentiment analysis as an example, in addition to the ability to recognize the sentiment of sentences, an ideal interpretable model should be able to identify the sentiment and polarity reversal at the levels of words, phrases, and clauses.\"",
            "8": "For the AMIC architecture, the reason why using a three-layer feedforward operation should be introduced.",
            "9": "And why not use a Transformer-based model and only use a self-attention mechanism?",
            "10": "For the word embedding, what are the differences between Glove-300-Wiki  and  word2vec?",
            "11": "The author should add more comparative experiments to show the different performances and BERT embedding.",
            "12": "For Table 2, it is better to add more ablation studies on different datasets.",
            "13": "Moreover, sentiment analysis is a common NLP task, so the authors should add other baselines.",
            "14": "For interpretability, do the authors consider an attention map to show the performance of the attention mechanism?",
            "15": "to illustrate the effectiveness of the proposed model?",
            "16": "More sentence case studies could be added in the appendix."
        },
        "r7e9hspBA9": {
            "0": "-\tReasonable method.",
            "1": "-\tEvaluation of two datasets with relatively detailed experimental analysis.",
            "2": "-\tThe techniques employed in this study are rather conventional, and in terms of text interpretability, while they do offer some assistance, their significance is limited.",
            "3": "Furthermore, for longer sentences or sentence-level tasks (commonly addressed using BERT-based models for text encoding), they lack scalability.",
            "4": "-\tThe work presented in this paper is quite mundane, and employing large models like ChatGPT for word-level interpretability might even yield better results, enhancing its overall scalability as well.",
            "5": "**Text:** \\\nThe service of the restaurant is good, the overall experience is not bad\n**Prompt:** \\\nWhat is the emotion of this sentence, analyzed at the word level\n**ChatGPT 3.5:** \\\nAt the word level, the emotion of this sentence can be broken down as follows:\n\n\"good\" implies a positive emotion.",
            "6": "\"not bad\" implies a somewhat positive or neutral emotion.",
            "7": "While it contains a negation (\"not\"), it's a double negative, and when used colloquially, it often means \"quite good\" or \"satisfactory.\"",
            "8": "So, the overall emotion at the word level is generally positive, with a touch of reservation or neutrality.",
            "9": "-\tThe related work should be updated with more recent related works.",
            "10": "-\tFigure 1 appears quite blurry.",
            "11": "I recommend redrawing the figure to ensure it meets the required resolution of 300 DPI.",
            "12": "-\tThe formulas in the article appear quite unusual and are not conducive to understanding, especially when there are three formulas in a single line.",
            "13": "It's not recommended.",
            "14": "-\tThe format of the references in the paper needs to be consistent.",
            "15": "If a referenced paper has already been published, it should not be cited in the arXiv format.",
            "16": "This should be updated to reflect the appropriate citation style for the published version.",
            "17": "-\tWriting errors are common across the overall paper.",
            "18": "Examples could be found in “Typos, Grammar, Style, and Presentation Improvements”.",
            "19": "**Typos, Grammar, Style, and Presentation Improvements:**\n\n-\tOn page 1, “… weighting, indication of …” => “… weighting, an indication of …”\n-\tOn page 1, “… insights on how context influences …” => “… insights into how context influences …”\n-\tOn page 1, “… multiple instance classification model …” => “… multiple instance classification models …”\n-\tOn page 1, “… methods which focus …” => “… methods that focus …”\n-\tOn page 2, “… support vector machine …” => “… support vector machines …”\n-\tOn page 2 “… long-short term …” => “… long-short-term …”\n-\tOn page 2 “… shown BERT …” => “… shown that BERT …”\n-\tOn page 2 “… in text can …” => “… in the text can …”\n-\tOn page 3 “… sentiment score at …” => “… sentiment scores at …”\n-\tOn page 3 “… help a SA model …” => “… help an SA model …”\n-\tOn page 3 “… its ignorance to the …” => “… its ignorance of the …”\n-\tOn page 6 “… the sentiment of a wine …” => “… the sentiment of wine …”\n-\tOn page 6 “… sentiment and the other …” => “… sentiment, and the other …”\n-\tOn page 6 “… follows a 18:1:1 ratio …” => “… follows an 18:1:1 ratio …”\n-\tOn page 7 “… has similar performance …” => “… has a similar performance …”\n-\tOn page 7 “… is able to also capture …” => “… is able to capture …”\n-\tOn page 7 “… various impact of …” => “… various impacts of …”\n-\tOn page 7 “… local patterns recognition.” => “… local pattern recognition.”\n-\tOn page 7 “… aspect to the …” => “… aspect of the …”\n-\tOn page 7 “… delicate lingistic complexities …” => “… delicate linguistic complexities …”\n-\tOn page 7 “… clauses seperated by …” => “… clauses separated by …”\n-\tOn page 8 “… the capability AMIC in provid …” => “… has a similar \n-\tOn page 9 “… providing detailed interpretation …” => “… providing a detailed interpretation …”\n-\tOn page 9 “… of analysis process …” => “… of the analysis process …”"
        },
        "Qh9ZldEtAj": {
            "0": "The model's integration of the MIC module enhances word-level interpretability, and its efficacy is demonstrated through experiments conducted on two public sentiment analysis datasets.",
            "1": "The critique highlights the perceived lack of innovation and the limited contributions of the study, suggesting that the methods used are standard and offer minimal advancements in text interpretability.",
            "2": "It points out a scalability issue with longer text sequences.",
            "3": "It suggests that utilizing more advanced language models for sentence-level tasks or even LLMs (like ChatGPT or LLaMa) for word-level interpretability could improve performance and scalability significantly.",
            "4": "In addressing the use of different word embeddings, the question is about the rationale behind using GloVe-300-Wiki for the wine dataset and word2vec for the Sentiment140 dataset.",
            "5": "The author is encouraged to explain the reasons for this choice and to provide a comparison between the two embeddings to elucidate their differences and justify their specific applications within the study.",
            "6": "The related work section is weak and does not cover SOTA models.",
            "7": "It needs to be updated with more recent studies.",
            "8": "Figure 1 is noticeably unclear and requires enhancement for better visibility.",
            "9": "The article has many writing errors throughout and needs to be thoroughly corrected."
        }
    },
    "hgDDyoWQt3": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenging task of Open-World Compositional Zero-Shot Learning (OW-CZSL) by leveraging Large Language Models (LLMs) to predict the feasibility of state-object pairs.",
            "1": "- The proposed Feasibility with Language Model (FLM) method is novel in its approach to use LLMs for feasibility prediction, which has not been explored in the context of OW-CZSL before.",
            "2": "- The study provides a comprehensive evaluation of various LLMs and prompt designs, contributing valuable insights into the application of LLMs in vision tasks.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and effective approach to improve OW-CZSL performance by leveraging LLMs, which is a significant contribution to the field.",
            "4": "- The extensive experiments and ablation studies provide strong evidence of the effectiveness of the proposed method across multiple datasets and scenarios.",
            "5": "- The paper is well-structured and clearly explains the methodology, making it easy to follow and understand the contributions.",
            "6": "- The integration of in-context learning with LLMs to enhance feasibility prediction is a noteworthy innovation that could inspire further research in related areas.",
            "7": "Potential reasons for rejection\n   - **Limited evaluation on proprietary LLMs:**\n     - The evaluation of proprietary LLMs like GPT-4, PaLM-2, and Claude-2 is limited to binary responses due to API constraints, which may not fully capture their potential.",
            "8": "- The paper does not explore the possibility of obtaining logit values from these proprietary models, which could provide a more comprehensive comparison.",
            "9": "- **Dependence on specific LLMs:**\n     - The proposed method heavily relies on the performance of specific LLMs (e.g., Vicuna and ChatGPT), which may not be accessible or practical for all researchers or applications.",
            "10": "- The paper does not discuss the potential limitations or challenges of using LLMs in resource-constrained environments.",
            "11": "- **Generalization to other tasks:**\n     - While the method shows promising results for OW-CZSL, the paper does not provide evidence of its applicability to other zero-shot learning tasks or related vision problems.",
            "12": "- The generalizability of the proposed approach to different domains or datasets beyond the ones evaluated is not thoroughly explored.",
            "13": "Suggestions for improvement\n   - **Expand evaluation on proprietary LLMs:**\n     - Investigate ways to obtain logit values or more detailed outputs from proprietary LLMs to provide a more comprehensive comparison with open-source models.",
            "14": "- Include additional experiments with other state-of-the-art LLMs to further validate the robustness and generalizability of the proposed method.",
            "15": "- **Address resource constraints:**\n     - Discuss the potential challenges and limitations of using LLMs in resource-constrained environments, and explore possible solutions or alternatives.",
            "16": "- Provide insights into the computational requirements and scalability of the proposed method to help researchers understand its practical implications.",
            "17": "- **Broaden applicability:**\n     - Conduct experiments on other zero-shot learning tasks or related vision problems to demonstrate the broader applicability of the proposed approach.",
            "18": "- Explore the potential of the FLM method in different domains or datasets to validate its generalizability and effectiveness in diverse scenarios.",
            "19": "- **Enhance clarity and detail:**\n     - Provide more detailed explanations and examples of the prompt designs and in-context learning strategies used in the experiments.",
            "20": "- Include additional qualitative results and visualizations to illustrate the effectiveness of the proposed method in different contexts."
        },
        "cs3fKTCUdZ": {
            "0": "1.The paper is well-written and easy to follow.",
            "1": "2.According to the experiments, FLM achieves noteworthy improvement in performance.",
            "2": "1.Time costs need to be taken into account.",
            "3": "As is known to all, the open-word setting will produce a large number of virtual compositions, which will bring a huge amount of calculation to the model (the proposed model process all possible pairs once and predict the score).",
            "4": "2.According to the paper, the In-context Learning seems not to be trained in the process, which means that the performance relies on the LLMs.",
            "5": "However, there existing some objects and states that are totally unknown to LLMs.",
            "6": "In this situation, the proposed model cannot transfer the knowledge to the unknown compositions.",
            "7": "3.The proposed method relies much on the quality of LLMs, and the transferability of the model is not reflected in the paper."
        },
        "VK7PeSRalx": {
            "0": "To the best of the reviewer’s knowledge, this method proposed in this paper is novel.",
            "1": "This paper is clearly motivated and the intuition behind the proposed methods are also very clear.",
            "2": "The idea of using LLMs for solving feasibility conflicts is simple yet quite effective.",
            "3": "The authors also show that as an orthogonal component to existing compositional zero shot learning methods, LLM-guided feasibility calibration can clearly boost the performance for most of the scenarios.",
            "4": "Despite the work’s obvious merit, the idea itself is very simple.",
            "5": "Within the ablations, it would be helpful if the authors are to thoroughly examine more variants of prompts since LLMs output can vary a lot.",
            "6": "The performance variations under such scenarios would be very informative to the community."
        },
        "Mb6pFxVPVl": {
            "0": "Figure 1 is well designed and helps the reader to understand the content.",
            "1": "The proposed method is simple and easy to understand.",
            "2": "The main concern of this work is its contribution.",
            "3": "The paper basically uses the existing LLM to determine the feasibility of a state-object combination.",
            "4": "This only shows that the existing LLM is able to determine the feasibility of a state-object combination, but what is the author’s contribution throughout the process?",
            "5": "Since different threshold will affect the binary classification performance, wouldn’t a metric like ROC curve suits the tasks better?",
            "6": "For Figure 2, it seems that both green and red block only show the feasible (s,o) pairs.",
            "7": "The author is suggested to show some infeasible (s,o) pairs and the model prediction on those infeasible (s,o) pairs.",
            "8": "It is challenging to tell whether GloVe or the proposed FLM separates the feasible and infeasible better by only looking at the figures.",
            "9": "The author is suggested to show some numerical results to support the claim.",
            "10": "In the evaluation metric, the author mentioned that the calibration bias is varied.",
            "11": "Does it mean that different calibration bias is used for different metric?",
            "12": "Typo: such “at” ChatGPT"
        },
        "aQBQ35PlOu": {
            "0": "S1: The studied problem about open-world compositional zero-shot learning is significant important and can apply to the real-world scene.",
            "1": "S2: The large-language models are used to reduce the gap between machines and humans.",
            "2": "S3: Extensive experiments on many prompt variants and six LLMs shows the best performence.",
            "3": "W1: Is this the first paper to solve the CZSL problem by using  the LLMs?",
            "4": "If yes, I am curious about the motivation or some motivation experiments to demonstrate the effectiveness of LLMs?",
            "5": "If no, I tend to see some differents compared with other published related works.",
            "6": "W2: This method in this paper is not novel and performance improvement depends entirely on the language model.",
            "7": "If the language model introduces biases, such as racial discrimination, during training, will this also affect downstream tasks?",
            "8": "W3: Does a more powerful language model perform best in this paper?",
            "9": "In my opinion, simply introducing a language model to solve downstream tasks does not reach the upper limit of ICLR acceptance."
        }
    },
    "71mqtQdKB9": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant gap in the application of diffusion models to discrete data domains, particularly natural language.",
            "1": "- It introduces a novel training objective, score entropy, which is more stable and forms an ELBO for maximum likelihood training.",
            "2": "- The proposed Score Entropy Discrete Diffusion models (SEDD) achieve competitive performance with GPT-2, a well-established autoregressive model, and offer distinct algorithmic advantages.",
            "3": "Potential reasons for acceptance\n   - The introduction of score entropy as a novel discrete score matching loss is a significant theoretical contribution.",
            "4": "- The empirical results demonstrate that SEDD models can achieve comparable perplexities to GPT-2, which is a notable achievement in the field of language modeling.",
            "5": "- The paper provides a comprehensive theoretical foundation and practical implementation details, making it a valuable resource for researchers in the field.",
            "6": "- The ability of SEDD models to trade off compute for generation quality and enable arbitrary infilling is a practical advantage over existing models.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability of the paper**\n     - The paper is dense with theoretical content, which may be challenging for readers to follow without a strong background in diffusion models and score matching.",
            "8": "- Some sections, particularly the mathematical derivations, could benefit from additional explanations and examples to improve accessibility.",
            "9": "- **Empirical validation and comparison**\n     - While the paper demonstrates competitive performance with GPT-2, it would be beneficial to include comparisons with other state-of-the-art models beyond GPT-2 to provide a broader context.",
            "10": "- The experiments primarily focus on perplexity as a metric.",
            "11": "Including additional metrics such as BLEU scores or human evaluations of generated text could provide a more comprehensive assessment of model performance.",
            "12": "- **Scalability and efficiency**\n     - The paper mentions that the score entropy training iterations are about as quick and use a similar amount of memory as standard autoregressive model training iterations.",
            "13": "However, more detailed analysis and benchmarks on training time and resource usage would strengthen the claims of scalability and efficiency.",
            "14": "- The practical implementation section highlights some challenges with memory overhead and computation of transition matrices.",
            "15": "Addressing these challenges more explicitly and providing potential solutions would be beneficial.",
            "16": "Suggestions for improvement\n   - **Enhance clarity and readability**\n     - Simplify and clarify the mathematical derivations where possible, and provide more intuitive explanations and examples to aid understanding.",
            "17": "- Consider adding a glossary of key terms and concepts to help readers unfamiliar with the specific terminology used in the paper.",
            "18": "- **Expand empirical validation**\n     - Include comparisons with a wider range of state-of-the-art models to provide a more comprehensive evaluation of the proposed method.",
            "19": "- Incorporate additional evaluation metrics, such as BLEU scores or human evaluations, to assess the quality of generated text more holistically.",
            "20": "- **Address scalability and efficiency concerns**\n     - Provide detailed benchmarks on training time, memory usage, and computational efficiency to support claims of scalability and efficiency.",
            "21": "- Discuss potential solutions to the challenges related to memory overhead and computation of transition matrices, and explore alternative approaches that could mitigate these issues.",
            "22": "- **Improve practical implementation details**\n     - Offer more detailed guidelines and best practices for implementing the proposed method in real-world scenarios.",
            "23": "- Include a discussion on the potential limitations and trade-offs of the proposed method, and suggest areas for future research to address these limitations."
        },
        "YxrqzpwA9i": {
            "0": "**[S1]** The proposed loss seems intuitively reasonable.",
            "1": "The authors motivate it based on limitations of previous score-network-inspired discrete diffusion methods, and argue why their approach should work better.",
            "2": "~~(Although, a lot of this argument seems to rely on unproven theorems; see [W1] below.",
            "3": ")~~\n\n**[S2]** The generative perplexity results are quite impressive.",
            "4": "It seems that the proposed sampling strategy is Pareto-optimal relative to the fixed GPT-2-small model, when evaluating based on GPT-2-large's perplexity v.s.",
            "5": "number of sampling iterations.",
            "6": "**[S3]** The problem of building better non-autoregressive probabilistically-sound generative models is an important one, and the claimed improvements represent an important step in this direction ~~(although due to [W1], [W3], and [W4] below I'm not convinced they've justified their claims sufficiently in this regard)~~\n\n**[S4]** The authors do a good job connecting this work to previous work on diffusion models, and in particular on drawing connections between their score-matching objective and previous work on continuous score-matching diffusion models.",
            "7": "~~**[W1]**~~ *(addressed in current revision)* The central theoretical claims of this work are incomplete and unsupported, and I am not convinced they are correct.",
            "8": "In particular, although much of the paper is devoted to statements about the new \"score entropy\" and its properties, the proofs are either omitted, incorrect, or only provided in a sketch form.",
            "9": "- Proposition 3.2, which states that their score entropy loss has the right minimum, is never proven.",
            "10": "- Proposition 3.3 and Theorem 3.4, which give alternative forms of the score entropy, have \"proofs\" that are very handwavey and informal.",
            "11": "And I believe these proofs are also incorrect!",
            "12": "The derivations ignore the weights $w_{xy}$ and thus end up proving something different than the intended proposition/theorem.",
            "13": "- The central result, Theorem 3.6, is justified only with a sketch which says to apply the (likely incorrect) trick from the \"proof\" of 3.3 to some unstated result of Campbell et al.",
            "14": "(2022).",
            "15": "This is nowhere near enough detail to reconstruct an adequate proof.",
            "16": "- Theorem 4.2's proof is also a sketch which does not include enough detail for me to verify its correctness.",
            "17": "Additionally, although the introduction claims that one contribution of the work is a \"Langevin corrector framework\", this never appears in the paper.",
            "18": "*Edit: The authors have corrected some small errors in their theorems and added detailed proofs for all of them.",
            "19": "I believe the theoretical claims are justified now, although I'm not familiar enough with stochastic processes to check everything in detail.",
            "20": "*\n\n---\n\n**[W2]** The provided experiments appear to be only preliminary results.",
            "21": "For their SEDD-small model, they \"emphasize that it is still improving\", and for their SEDD-medium model, they state that it \"has not started converging\".",
            "22": "The authors say they will \"update our model results as training progresses\".",
            "23": "My understanding is that work submitted to ICLR is supposed to be feature-complete at the time of submission.",
            "24": "I'm not sure it's appropriate to plan on updating the central results of the submission during the review process.",
            "25": "*Edit: The authors have explained their reasoning below (they meant to pre-emptively reassure reviewers that they could add more comparisons if asked, not to do so unasked).",
            "26": "It still seems a bit strange to include comments directed at the reviewers in a paper submission, especially with results that the paper calls \"preliminary\", since presumably these would always be removed in the final version.",
            "27": "On the other hand, if these statements had simply not been added in the first place, I think the initial results would have still supported the main empirical claims, so perhaps this isn't a big deal.",
            "28": "*\n\n---\n\n~~**[W3]**~~ *(addressed in current revision)* I found the evaluation criteria to be somewhat imprecise, especially in regards to the authors claims that the demonstrate \"for the first time, a non-autoregressive modeling technique that is able to achieve similar perplexity scores as autoregressive modeling\".",
            "29": "The authors claim performance is \"competitive\" with GPT-2-small, but this seems like a subjective statement; the perplexity of their SEDD-small models seems to be a few points higher for everything except the PTB dataset.",
            "30": "They also present results for SEDD-medium, a larger model, which outperform the smaller GPT-2 model.",
            "31": "However, it's not clear that comparing perplexity across model sizes is fair without controlling for the amount of training compute.",
            "32": "The authors additionally reference the Plaid 1B model from Gulrajani & Hashimoto (2023), which had previously shown strong non-autoregressive performance relative to GPT-2-small (albeit with a larger model and more training compute than GPT-2-medium).",
            "33": "That seems to contradict the claim that this work is the \"first time\" non-autoregressive modeling has been competitive with autoregressive modeling.",
            "34": "I would have hoped for a more rigorous set of experimental results here.",
            "35": "For instance, Gulrajani & Hashimoto (2023) give a thorough study of different model scaling law behavior while controlling for training compute; this kind of thing seems necessary to fairly compare with autoregressive methods.",
            "36": "(Perhaps much of the performance of the SEDD models here is due to them being overtrained relative to the GPT-2 models.)",
            "37": "*Edit: The authors have added context for their 10% perplexity gap based on existing continuous diffusion results, added comparisions between their medium model and GPT-2 medium, and clarified that their experimental results are not overtrained with respect to GPT-2.",
            "38": "The new baselines also provide additional supporting evidence.",
            "39": "*\n\n---\n\n~~**[W4]**~~ *(addressed in current revision)* Although motivated as a way to improve upon previously-proposed discrete diffusion approaches, the experiments do not include any discrete diffusion model baselines.",
            "40": "Additionally, the perplexity experiments use different evaluation splits and different evaluation methods from previous works, so the numbers cannot be directly compared to previous works.",
            "41": "The GPT-2-small comparisons may also be confounded by differences in the dataset or number of training iterations used for GPT-2-small.",
            "42": "It is thus difficult to tell how much of the observed gains are due to the new contributions in this work, rather than being due to the training procedure, base model architecture, or evaluation method.",
            "43": "*Edit: Diffusion baselines have been added, using consistent training, architecture, and evaluation setups.",
            "44": "*\n\n---\n\n~~**[W5]**~~ *(addressed in current revision)* The generated samples still seem somewhat incoherent in a qualitative sense.",
            "45": "In particular, I found the \"infilling\" samples in Table 2 to be unimpressive; none of them appear to be meaningful or consistent with the provided prompt tokens.",
            "46": "*Edit: The newer SEDD-medium samples and long-form samples are much more coherent than those in the initial submission.",
            "47": "*"
        },
        "hjuengPMYp": {
            "0": "Discrete diffusion models and especially text-diffusion models are difficult but exiting research topics: as mentioned by the authors much work remains to be done before discrete diffusion models can truly rival state-of-the-art autoregressive models on text generation.",
            "1": "The main weakness of text-diffusion models is their extremely slow training time when compared to (equivalent) autoregressive models.",
            "2": "However their future potential is huge, especially regarding the ability of control they provide.",
            "3": "- I found the paper easy to follow and interesting\n- I find the idea of trying a better -- numerically more stable -- score-matching criterion as proposed in this article interesting\n- The authors also extend the study of (Meng et al.",
            "4": "2022) and provide an ELBO and a denoising variant of their criterion\n- This article may provides a real step toward an improvement of discrete diffusion models - There is undoubtedly a lot of work in this article, but I felt that the scientific impact of this contribution is unclear: the main contribution is to propose a new score-matching loss but I see no theoretical evidence and no experiment, be it on a toy example, showing that a simple \"quadratic score-matching loss\" as in (Meng et al.",
            "5": "2022) would be less efficient than its new \"score entropy loss\" counterpart.",
            "6": "- The paper lacks of a proper ablation study (be it on small datasets)\n- The experiments are only provided on text generation and seem unfinished at submission time (due, I guess, to the huge amount of compute time required to train a medium-size GPT2-like diffusion model)\n- On Table 1, the SEDD-medium results are provided, but the equivalent results for medium-size GPT2 must be provided as well otherwise it could be misleading (I hope this will be fixed at the rebuttal time).",
            "7": "Minor remarks:\n- typo on page 3 equation 7 : \"k\\neq i\" -> \"z \\neq x\"\n- the indices used to write score functions can be confusing to the reader e.g.",
            "8": "$s_\\theta(x)_y$, $s_\\theta(x,t)_j$"
        },
        "GrTzVksQGz": {
            "0": "The proposed criterion is extremely well justified from a theoretical point of view.",
            "1": "The simplified criteria for model scaling are well justified.",
            "2": "The derivations were fun to read and follow.",
            "3": "The experimental results are compelling.",
            "4": "The only weakness is that, while presenting so much detail about the scaling properties of the proposed criterion, the paper omits to explain the unusually complicated form of the criterion itself.",
            "5": "The derivations give wonderful consequences of Eq.",
            "6": "(9), but don't really explain where Eq.",
            "7": "(9) comes from!",
            "8": "This might be relevant because I think there might be a small typo in Eq.",
            "9": "(9).",
            "10": "I am almost able to derive Eq.",
            "11": "(9) by making the assumption that it is a Bregman divergence between s(x,y) and p(y)/p(x), using -log as the convex function, which would totally make sense, because it would guarantee that your score divergence is non-negative, reflexive, and convex in s(x,y); these properties are stated in the paper, but are not proven in the paper, perhaps because they follow naturally from the Bregman divergence.",
            "12": "However, if I derive it in that way, I find one typo in the equation: by that derivation, the last term should not be  (p(y)/p(x))\\log(p(y)/p(x)-1), it should be (p(y)/p(x))(log(p(y)/p(x))-1).",
            "13": "Indeed, my correction seems necessary, because log(p(y)/p(x)-1) will often be taking the logarithm of a negative number, which would be avoided if you instead calculated log(p(y)/p(x))-1.",
            "14": "Notably, this last term in Eq.",
            "15": "(9) is ignored for most of the rest of the paper, since it does not involve s_\\theta(x); it seems to be necessary only for the purpose of shifting the criterion upward so that it is strictly non-negative."
        },
        "NmacgIB5lf": {
            "0": "* The paper is well-motivated and addresses an important area of research that is of interest to the larger community.",
            "1": "* SEDD generalizes score to the discrete domain and improves upon CSM by addressing its limitations (i.e., infinite KL divergence) and satisfies a number of desirable properties that make it suitable for score matching.",
            "2": "* SEDD models achieve competitive metrics compared to GPT-2 on a variety of standard datasets, which suggests the robustness and generalizability of the method.",
            "3": "* The experiment lacks good baselines.",
            "4": "Although the paper claims to improve over concrete score matching, they do not consider CSM in their baseline and only compare the proposed SEDD with an autoregressive model (GPT-2 small).",
            "5": "Moreover, SEDD-medium is compared with GPT-2 small.",
            "6": "* The experiment appears inconclusive or incomplete.",
            "7": "The model is still being trained, and the authors claim that it has not converged yet; the experiment on the 1 billion-word dataset is said to have encountered unexpected errors, without elaboration.",
            "8": "* Certain design choices lack justification.",
            "9": "The SEDD model uses rotary embeddings instead of learned positional embeddings, as in the GPT-2 baseline.",
            "10": "In the absence of ablations, it is unclear how much this decision impacted obtained results.",
            "11": "* Not all variables and notations are clearly specified, making the paper difficult to follow at times."
        },
        "WVXpQ1TFFA": {
            "0": "- Thorough theoretical analysis about the diffusion weighted denoising score entropy.",
            "1": "- Better generation quality than same-scaled GPT-2 - Evaluation is a little bit weak.",
            "2": "Like, lack of comparision between previous discrete or continous diffusion mdoels, including the sampling speed and generation quality.",
            "3": "No specific numbers of  sampling speed (only the caption of Fig 2 mentioned once).",
            "4": "No quantitative evaluation for infilling tasks, just showing some examples.",
            "5": "- Some motivations are not clear.",
            "6": "Section 4 is not well presented.",
            "7": "Why the design of this denoising scheme is needed?",
            "8": "If it is designed for speedup, you need to explain two things: (1) no detailed experiments or ablation study about this strategy (2) some discrete diffusion models can sampling within several steps (like~10), and in such condition, discrete diffusion models already have the advantages over generation speed, so what's the difference between theirs and yours?",
            "9": "- Writing: cictation format (citep and citet) is mixed up."
        }
    },
    "RTL8fWgJaS": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces the concept of self-specialization for large language models (LLMs), focusing on domain-specific expertise rather than general alignment.",
            "1": "- It demonstrates the effectiveness of self-specialization in improving zero-shot and few-shot performance in specialized domains such as biomedicine and finance.",
            "2": "- The approach leverages domain-specific unlabelled data and a few labeled seeds, augmented with retrieval mechanisms to reduce hallucination and enhance concurrency.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a significant challenge in the field of LLMs by proposing a novel method for domain-specific specialization.",
            "4": "- The experimental results show substantial improvements in performance for specialized domains, highlighting the practical applicability of the proposed method.",
            "5": "- The approach is efficient in terms of data and parameters, making it a practical solution for real-world applications.",
            "6": "- The paper provides a comprehensive evaluation and benchmarking of the proposed method against existing models, demonstrating its superiority.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper could benefit from a more detailed explanation of the self-specialization process, including the specific steps and algorithms used.",
            "8": "- The description of the retrieval mechanism and its integration with the base model could be more explicit to ensure reproducibility.",
            "9": "- **Limited scope of evaluation:**\n     - The experiments are primarily focused on the biomedical and financial domains, which may limit the generalizability of the findings to other specialized domains.",
            "10": "- Additional experiments on a wider range of domains could strengthen the claims of the paper.",
            "11": "- **Insufficient comparison with state-of-the-art models:**\n     - While the paper compares the proposed method with some existing models, it lacks a thorough comparison with the latest state-of-the-art models in the field.",
            "12": "- Including more recent and competitive baselines could provide a clearer picture of the relative performance of the proposed method.",
            "13": "- **Potential over-reliance on specific datasets:**\n     - The paper heavily relies on specific datasets for evaluation, which may introduce biases and limit the generalizability of the results.",
            "14": "- Evaluating the method on a more diverse set of datasets could address this concern.",
            "15": "Suggestions for improvement\n   - **Enhance methodological clarity:**\n     - Provide a more detailed and step-by-step explanation of the self-specialization process, including the algorithms and techniques used.",
            "16": "- Clarify the role and implementation of the retrieval mechanism in the response generation phase.",
            "17": "- **Expand the scope of evaluation:**\n     - Conduct experiments on a broader range of specialized domains to demonstrate the generalizability of the proposed method.",
            "18": "- Include additional datasets from different domains to provide a more comprehensive evaluation.",
            "19": "- **Include more competitive baselines:**\n     - Compare the proposed method with a wider range of state-of-the-art models to provide a clearer understanding of its relative performance.",
            "20": "- Consider including recent models that have shown strong performance in similar tasks.",
            "21": "- **Address potential biases:**\n     - Evaluate the method on a more diverse set of datasets to mitigate potential biases introduced by relying on specific datasets.",
            "22": "- Discuss the limitations and potential biases of the chosen datasets and how they might affect the results.",
            "23": "- **Provide code and implementation details:**\n     - Ensure that the code and implementation details are made available upon acceptance to facilitate reproducibility and further research.",
            "24": "- Include detailed instructions and documentation to help other researchers replicate the experiments and build upon the work."
        },
        "hu3hs512uX": {
            "0": "- Methodology: The self-specialization technique introduced is both straightforward and powerful.",
            "1": "The authors effectively demonstrate that, by refining the instruction dataset for a target domain, one can substantially elevate the performance of an LLM.",
            "2": "- Results: The outcomes are compelling.",
            "3": "The self-specialized technique not only outpaces the baseline LLM with general instruction tuning across various NLP tasks, but even a 30B self-specialized model can occasionally surpass larger models of 65B capacity.",
            "4": "- Structured Presentation & Clear Communication: This research is characterized by its lucid motivation and coherent narrative, seamlessly bridging the gap between identified issues and the proposed solutions.",
            "5": "The results are presented with clarity, reinforcing the dominance of the introduced methods.",
            "6": "At its core, the proposed method is somewhat an amalgamation of existing concepts.",
            "7": "While termed \"self-specialization\", it essentially contrasts with \"self-alignment\" (for instance, as seen in Alpaca) and incorporates \"domain-specific knowledge-guided generation\".",
            "8": "The enhancements observed are not surprising since the retrieval-augmented generation, grounded on specific knowledge retrieval, naturally offers a more thorough and targeted knowledge base for instruction tuning.",
            "9": "Thus, one could argue that self-specialization is essentially self-alignment augmented with RAG, somewhat constraining its novelty.",
            "10": "Furthermore, rather than relying on RAG to generate the instruction dataset with the foundational model, the authors might have delved deeper into refining the dataset construction process.",
            "11": "Several uncharted avenues remain: (1) devising superior methods for seed generation, considering aspects like topic ratios or instruction diversity; (2) optimizing answer generation for the instructions, with focus on enhancing and validating answer quality.",
            "12": "Regrettably, this research offers only a cursory exploration in these dimensions."
        },
        "TYt3sx7sOB": {
            "0": "The paper is well-motivated, as specializing the language model in a specific domain is an area of interest.",
            "1": "Additionally, the paper is well-structured and easy to follow.",
            "2": "Despite the promising results reported in this paper within the biomedical domain, I still feel uncertain about its contribution for the following reasons:\n1) The experimental results are not convincing.",
            "3": "Table 1 reports the comparative results of the base model and the self-specialized model, with the authors stating \"the scores (F1) witness a rise from 25.15 to 36.63 in a zero-shot setting.\"",
            "4": "However, this is the average result, and the datasets \"BioASQ-Yesno\" and \"Medical Drugs\" contribute the most differences, with a difference of around 60-70 in \"BioASQ-Yesno.",
            "5": "I highly doubt the correctness of the results for this dataset.",
            "6": "This is a binary classification dataset, and the base model performs much worse than random guessing (50.0 if labels are balanced), which is the first odd point.",
            "7": "Secondly, as the number of demonstrations increases, the F1 score of the base model surprisingly decreases.",
            "8": "This implies that the base model must learn information from the prompted demonstrations.",
            "9": "If the model is not performing well on a dataset, the F1 score should remain almost the same or should not exhibit such consistent declines.",
            "10": "I am considering if the authors mistakenly reversed the labels of \"yes\" and \"no,\" leading to these counterintuitive results.",
            "11": "If we exclude the \"BioASQ-Yesno\" and \"Medical Drugs\" datasets, the average F1 score improvement is down to around 2.5, far less than the reported 11.48 (36.63 - 25.15).",
            "12": "This makes doubt not only on the reported results in Table 1 but also on Fig.",
            "13": "4 and its claims of the model's superiority over all 65B models despite its ≈2.2x smaller size.",
            "14": "2) The Gouge-L scores for the \"DDI\" and \"Medical\" datasets are identical to their respective F1 scores.",
            "15": "I haven't investigated if this is possible, but please check the reported results carefully.",
            "16": "3) During the seed generation phase, the proposed method requires the NLP benchmarks in the target domain.",
            "17": "This limits the proposed approach to extend to other domains, as not all domains have this NLP benchmark accessible.",
            "18": "With the paper using 80 seeds, one possible solution could be manually generating them; however, the paper didn't mention this point.",
            "19": "4) How about the results if applying the proposed scheme to a larger model, such as LLaMA-65B?",
            "20": "The base model used in the paper initially shows not good results (20-30 F1 scores), which are relatively easy to improve.",
            "21": "If a larger model with better initial results is used, is it still feasible to have improvement through the proposed scheme?",
            "22": "5) Some places lack professional writing.",
            "23": "For example, in Eq.",
            "24": "(3), $p_{lm}$ is introduced without prior definition.",
            "25": "While the intended meaning may be inferred, a scientific paper should maintain consistency and rigor in its notation and explanations."
        },
        "msMvr8IOkK": {
            "0": "The topic of specialization is important for deploying LLM to a specified domain.",
            "1": "The paper is well-written and easy to follow.",
            "2": "The idea of self-specialization is interesting, which utilizes both seed instructions and generated ones together.",
            "3": "Several experiments are conducted to evaluate the proposed method.",
            "4": "The proposed self-specialization method outperforms the base model and larger LLaMA variants.",
            "5": "The benchmark only includes two aligned models.",
            "6": "Better to include more aligned models for comprehensive benchmarking.",
            "7": "It claims that 'we hypothesize that the model expertise in different domains resides in “superposition” in the model’s parameters and hidden states'.",
            "8": "But there are no further theoretical explanations or experimental results to support this claim.",
            "9": "If the \"superposition\" could be explained in details, the application scope of the proposed method may be more clear.",
            "10": "The metrics of the y-axes should be added in figure 2 and 5."
        },
        "ANdAjxWhid": {
            "0": "- The paper is well-written and easy to follow, providing sufficient technical details.",
            "1": "The authors also mention that the data, code, and trained model will be open source.",
            "2": "- The empirical findings demonstrate that incorporating unlabeled data positively impacts the model's ability to effectively respond to queries within a specialized domain, particularly in the challenging context of biomedical research.",
            "3": "- The proposed method, self-specialization, is an extension of the self-instruct [1] work to recover certain expertise in the LLMs.",
            "4": "However, the method is only designed and evaluated for one specific domain without showcasing its generalization ability to other domains.",
            "5": "- The compared methods in the paper are based on zero-shot/few-shot settings, while the proposed method uses LoRA for finetuning.",
            "6": "It would be beneficial to include stronger task-specific comparison methods to better illustrate the effectiveness of the proposed approach.",
            "7": "- Since the method has multiple components, it would be helpful to show the contribution of each component to the overall performance.",
            "8": "For example, how does domain-specific response generation impact the final results?",
            "9": "- The performance in the knowledge sparse domain is uncertain.",
            "10": "It is understood that domain response generation involving harnessing knowledge is one of the main contributors to the specification, but it is important to understand the method's bottleneck when existing knowledge is limited for certain domains.",
            "11": "- The paper should discuss potential data contamination and address how the authors ensure that the data for downstream testing does not overlap with the generated data.",
            "12": "[1] Wang, Yizhong, et al.",
            "13": "\"Self-instruct: Aligning language model with self generated instructions.\"",
            "14": "arXiv preprint arXiv:2212.10560 (2022)."
        }
    },
    "S1hTxWE7cJ": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a novel classification challenge, X-Shot, which aims to handle frequent-shot, few-shot, and zero-shot labels within a single system.",
            "1": "- The proposed approach leverages instruction learning and data generated by pre-trained language models (PLMs) to create a unified system that surpasses state-of-the-art techniques on multiple benchmark datasets.",
            "2": "- This is the first work addressing the X-Shot learning problem, where the variable X can range from 0 to +∞, reflecting real-world scenarios with varying label frequencies.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a significant and practical problem in the field of text classification, providing a solution that can handle a wide range of label frequencies.",
            "4": "- The proposed X-Shot system demonstrates superior performance compared to existing methods on three benchmark datasets, showcasing its effectiveness and versatility.",
            "5": "- The use of instruction learning and PLMs for generating data is a novel approach that contributes to the advancement of few-shot and zero-shot learning techniques.",
            "6": "- The paper provides a comprehensive evaluation and analysis of the proposed method, including error analysis and the impact of task type overlap, which adds to the robustness of the findings.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper could benefit from a more detailed explanation of the system architecture and the process of transforming classification problems into a unified binary classification framework.",
            "8": "- The description of the supervision acquisition process, particularly the use of GPT for generating weakly labeled examples, could be more explicit and detailed.",
            "9": "- **Limited comparison with other state-of-the-art methods:**\n     - While the paper compares the proposed method with several baselines, it would be beneficial to include more recent and advanced methods in the comparison to provide a more comprehensive evaluation.",
            "10": "- The paper could also explore the performance of the proposed method on additional datasets to further validate its generalizability and robustness.",
            "11": "- **Insufficient discussion on limitations and future work:**\n     - The paper does not thoroughly discuss the limitations of the proposed approach, such as potential challenges in scaling the system to larger datasets or more complex classification tasks.",
            "12": "- Suggestions for future work are limited, and the paper could benefit from a more detailed discussion on potential directions for further research and improvement.",
            "13": "Suggestions for improvement\n   - **Enhance clarity and detail in methodology:**\n     - Provide a more detailed explanation of the system architecture, including a step-by-step description of how classification problems are transformed into a unified binary classification framework.",
            "14": "- Elaborate on the supervision acquisition process, particularly the use of GPT for generating weakly labeled examples, and provide more examples to illustrate the process.",
            "15": "- **Expand comparison with other methods:**\n     - Include more recent and advanced state-of-the-art methods in the comparison to provide a more comprehensive evaluation of the proposed method's performance.",
            "16": "- Evaluate the proposed method on additional datasets to further validate its generalizability and robustness.",
            "17": "- **Discuss limitations and future work:**\n     - Provide a thorough discussion of the limitations of the proposed approach, including potential challenges in scaling the system to larger datasets or more complex classification tasks.",
            "18": "- Offer detailed suggestions for future research and improvement, such as exploring alternative approaches for instruction learning or investigating the impact of different types of weak supervision on the system's performance."
        },
        "UBkHKcocSx": {
            "0": "-- The problem X-shot learning is a novel one and it is practically useful.",
            "1": "-- The presentation is generally good with comprehensive introduction of background and related works.",
            "2": "-- In section 4.1, it is unclear what the instruction term stand for.",
            "3": "It would be helpful to give some examples in the text or Figure 1 for clarity.",
            "4": "-- It is also unclear how the \"indirect supervision\" works.",
            "5": "Again, including concrete examples in Figure 2 would be helpful.",
            "6": "-- The experimental results do not provide convincing proof that the proposed approach outperform its counterparts.",
            "7": "The comparison may be unfair since varying resources have been used in different methods."
        },
        "t7bJGld9oB": {
            "0": "Originality: The paper introduces a new problem setting for text classification, X-SHOT, which can handle any label occurrence, whether it be frequent-shot, few-shot, or zero-shot.",
            "1": "This is a novel problem formulation according to the authors.",
            "2": "But since I am not well-versed in this line of work, I am not confident about the authors's claim on the originality.",
            "3": "I encourage other reviewers comment on this point.",
            "4": "The paper reframes text classification tasks into a binary classification task, adaptable to any number of labels and occurrences.",
            "5": "I think it is a neat approach that unifies various text classification tasks into a single framework.",
            "6": "It is something I have not read about before, but again I am not an expert in this area, so please refer to other reviewers' comments on this point.",
            "7": "Quality: The paper provides a thorough evaluation of X-SHOT on three benchmark datasets across diverse domains in both single-label and multi-label classifications.",
            "8": "The evaluation demonstrates that X-SHOT outperforms preceding state-of-the-art techniques in FewRel and UFET.",
            "9": "Clarity: The paper is well-organized and easy to follow, with clear section headings and subheadings.",
            "10": "The paper provides detailed explanations of technical terms and concepts, making it accessible to readers with varying levels of expertise.",
            "11": "Significance: The paper's contributions seems to have significant implications for the field of text classification, as X-SHOT provides a unified system that can handle any label occurrence, making it a versatile solution for real-world applications.",
            "12": "* May be over claiming contributions: I have to admit that I have not read a lot of papers in few-shot/zero-shot/one-shot domain.",
            "13": "It seems quite unexpected to me that no one had ever attempted to develop a system that can utilize labels of all frequencies.",
            "14": "After reading this paper, I did some simple google search.",
            "15": "Without going into too much details, I found some articles discussing models that can handle \"N-shot\" labels with N ranging from 0 to some N, albeit N is not claimed to be set to infinity, which seems to be very similar to the \"X-shot\" problem formation.",
            "16": "Since I am no expert in this topic and I certainly do not intend to become on for the purpose of this review, I encourage other reviewers who have more domain knowledge than me to think about this point.",
            "17": "* The fact that the proposed model does not do well on the Situation dataset can be problematic: From table 2 we can see that the proposed model does not do well on the Situation dataset and it certainly did not \"still exceed[s] all other baselines significantly.\"",
            "18": "In particular, I find it hard to be persuaded that the proposed model exceeds NLI (Li et al., 2022) **significantly**.",
            "19": "Can authors elaborate on this?",
            "20": "* Other aspect might worth considerations yet unexplored: For example, for the Situation dataset, GPT3.5 is able to outperform all other models.",
            "21": "However, the authors could have argue that GPT3.5 might be less advantageous because of computational efficiency.",
            "22": "The paper does not provide a detailed comparison of X-SHOT with other state-of-the-art techniques in terms of computational efficiency.",
            "23": "It is unclear how X-SHOT compares in terms of training time and computational resources required."
        },
        "m6JmKANL4g": {
            "0": "The authors propose an interesting challenge that the zero-shot, few-shot and frequent-shot tasks can be trained within one unified framework, which has never been studied before.",
            "1": "The way to leverage the indirect supervision by enabling the model to distinguish the positive and negative classes from marginally tweaked inputs is worth thinking.",
            "2": "The proposed method lacks novelty.",
            "3": "It is unclear which part is more important.",
            "4": "If the pretraining stage on Super-NaturalInstruction Dataset is more critical, it seems unfair to directly compare with those competitor methods which adopts no pretraining stage.",
            "5": "The results seems not good enough, especially on the Situation benchmark.",
            "6": "In order to prove the practical value of the proposed X-shot that the zero-shot, few-shot, and frequent-shot problems can be well-solved by the unified training, I think the final results should be compared with the additional results retrieved by disjoint training.",
            "7": "For example, the zero-shot results retrieved by unify training should be better than that retrieved by only training on all zero-shot tasks."
        },
        "ftPbtDBKKN": {
            "0": "The paper is well-written and easy to follow.",
            "1": "The paper unifies freq-shot, few-shot, and zero-shot labels in the proposed *X-shot* setting.",
            "2": "The ideas of indirect supervision and GPT-supervision are reasonable.",
            "3": "The solution for *X-shot* setting is not new (Sec 4.1).",
            "4": "Converting multi-label classification into binary classifications is a conventional approach in ML algorithms.",
            "5": "While GPT supervision is proposed for zero-shot labels (Sec 4.2), there is no ablation study on the influence of generated instances on freq-shot, few-shot, and zero-shot labels.",
            "6": "The improvement looks marginal compared to previous works (Tab.",
            "7": "2)."
        }
    },
    "4L0xnS4GQM": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenge of table-based reasoning with large language models (LLMs), which is crucial for tasks like table-based question answering and fact verification.",
            "1": "- The proposed CHAIN-OF-TABLE framework introduces a novel approach by explicitly using tabular data in the reasoning chain, which is a significant advancement over existing methods that rely on free-form text or code for intermediate reasoning steps.",
            "2": "- The framework's ability to iteratively generate operations and update tables to represent a reasoning chain is a novel contribution that enhances the accuracy and reliability of predictions.",
            "3": "Potential reasons for acceptance\n   - The paper presents a clear and well-motivated problem statement, addressing the limitations of current table-based reasoning methods.",
            "4": "- The proposed CHAIN-OF-TABLE framework demonstrates state-of-the-art performance on multiple benchmarks (WikiTQ, FeTaQA, and TabFact) across different LLMs, showcasing its effectiveness and generalizability.",
            "5": "- The experimental results are comprehensive and show significant improvements over existing methods, highlighting the practical impact of the proposed approach.",
            "6": "- The paper includes detailed explanations of the methodology, including dynamic planning and argument generation, which are crucial for understanding the framework's operation.",
            "7": "Potential reasons for rejection\n   - **Complexity and clarity of the methodology**\n     - The iterative process of dynamic planning and argument generation might be complex for readers to fully grasp without additional visual aids or step-by-step examples.",
            "8": "- The explanation of how the operations are dynamically selected and executed could be more detailed to ensure clarity.",
            "9": "- **Evaluation and comparison with baselines**\n     - While the paper compares CHAIN-OF-TABLE with several baselines, it would benefit from a more in-depth analysis of why certain baselines perform poorly in specific scenarios.",
            "10": "- The paper could include more qualitative examples to illustrate the differences in reasoning chains generated by CHAIN-OF-TABLE versus other methods.",
            "11": "- **Scalability and efficiency concerns**\n     - The iterative nature of the framework might raise concerns about its scalability and efficiency, especially for very large tables or highly complex queries.",
            "12": "- The paper should provide more insights into the computational overhead and potential bottlenecks of the proposed approach.",
            "13": "Suggestions for improvement\n   - **Enhance clarity and detail in methodology**\n     - Include more visual aids, such as flowcharts or diagrams, to illustrate the dynamic planning and argument generation processes.",
            "14": "- Provide step-by-step examples of how the framework handles specific queries, highlighting the intermediate tables and operations.",
            "15": "- **Expand evaluation and analysis**\n     - Conduct a more detailed analysis of the performance of different baselines, explaining why CHAIN-OF-TABLE outperforms them in specific cases.",
            "16": "- Include more qualitative examples and case studies to demonstrate the practical advantages of the proposed approach.",
            "17": "- **Address scalability and efficiency**\n     - Provide a detailed analysis of the computational complexity and efficiency of the framework, including potential optimizations for handling large tables.",
            "18": "- Discuss the scalability of the approach and any limitations or challenges that might arise when dealing with very large datasets or highly complex queries.",
            "19": "- **Broaden the scope of experiments**\n     - Test the framework on additional datasets or real-world scenarios to further validate its generalizability and robustness.",
            "20": "- Explore the impact of different LLMs and configurations on the performance of CHAIN-OF-TABLE, providing insights into its adaptability to various settings."
        },
        "t9AbVC8TSd": {
            "0": "The proposed method is new in that it adapts the previous chain-of-thought prompting to the table-related tasks, and it improves the understanding of intermediate reasoning results.",
            "1": "Experiments on three benchmarks show the advantages of proposed method over a broad range of baselines including end-to-end methods and other program-aided methods.",
            "2": "The proposed method requires significantly more number of queries to LLMs compared to baselines such as chain-of-thought and Binder, since the method needs to query LLMs twice (one for operation generation and one for arguments generation) for each operation.",
            "3": "However, no comparison is provided for performance under the same number of queries.",
            "4": "For example, what is the performance of baselines when they adopt the self-consistency idea (which is shown to be beneficial in the Binder paper)?",
            "5": "Does the proposed method still have advantage when using the same number of queries?",
            "6": "The pre-defined five atomic operations seem to significantly limit the available operations.",
            "7": "For example, aggregations such as SUM, MAX, MIN, etc.",
            "8": "can be easily done in SQL.",
            "9": "How are these aggregations done in the proposed method?",
            "10": "No explanation is provided for why these five operations are used in the paper and why they cover the complete set of operations on table.",
            "11": "The presentation of argument generation process is not clear.",
            "12": "Based on the prompts in the Appendix, it seems add_column operation directly uses LLMs to generate the new column, whereas other four operations only prompt LLMs to generate arguments that will be fed to the programming language."
        },
        "dTZc7RsELr": {
            "0": "The investigated problem of leveraging and understanding structural data like tables is important and practical while existing LLMs can not solve it well.",
            "1": "The proposed method does not require training (or fine-tuning) of the existing LLMs.",
            "2": "The design of atomic operations is novel and also reasonable.",
            "3": "The overall reasoning procedure of chain-of-table is step-by-step, explainable, and effective.",
            "4": "The paper is solid from a technical perspective, and extensive experiments are conducted.",
            "5": "The presentation and drawn figures are generally clear and easy to understand.",
            "6": "Several case studies are also elaborated on in the Appendix.",
            "7": "The proposed method only achieves marginal improvements in some cases, e.g., TabFact and ROUGE-1/2/L datasets.",
            "8": "I would suggest the paper discuss the potential reasons.",
            "9": "The observations in Figure 4 are quite interesting.",
            "10": "It seems that a longer chain does not consistently bring more accurate results.",
            "11": "What are the underlying reasons for this?",
            "12": "Dater (Ye et al., 2023) should be the most important baseline for comparison.",
            "13": "I would suggest the paper make a further comparison with in-depth analysis from the perspective of methodology, e.g., a comparison between one-step and multi-step reasoning on tabular data.",
            "14": "Besides, how efficient is chain-of-tables when dealing with large-scale data?",
            "15": "It seems that the running-time efficiency is known from the current draft."
        },
        "xyegwnMopa": {
            "0": "The proposed chain-of-table is simple and effective.",
            "1": "This highlights the value of decomposing reasoning in tabular tasks, as opposed to employing single-step table reasoning methods.",
            "2": "The good performance underscores its effectiveness compared to baseline methods across multiple tabular reasoning datasets.",
            "3": "The proposed method is an extension of chain-of-thought to tabular data.",
            "4": "Each reasoning step is constrained by predefined operations on the table.",
            "5": "However, it raises questions about the adaptability of the chain-of-table framework to incorporate new operations or external knowledge, such as contextual information related to the table.",
            "6": "Chain-of-table requires a table, which could be large, in each reasoning step.",
            "7": "This may significantly increase the computational cost.",
            "8": "(See my questions below.)",
            "9": "While it is not necessarily a weakness, it would be beneficial to evaluate the proposed method with an open-sourced model (e.g.",
            "10": "Llama-2) to understand whether the framework can be easily adopted by other models.",
            "11": "One concern is that chain-of-table relies on LLMs' ability to comprehend the defined operations and reasoning chains, and it is uncertain whether other LLMs can seamlessly adapt to these requirements."
        },
        "7z70rzmbzR": {
            "0": "Recently, there has been a lot of interest in developing methods to improve the performance of LLMs on tabular data.",
            "1": "- The paper addresses one of the key stumbling blocks in improving the performance of LLMs on reasoning over tables.",
            "2": "- The paper is well-written and easy to follow.",
            "3": "- Empirical results show that the proposed approach outperforms the baselines on real-world datasets.",
            "4": "One of the issues with the paper is how they evaluate, especially the choice of benchmarks.",
            "5": "I encourage the author to evaluate their approach over a diverse class of tasks, especially tasks like Table summarization, Column type annotation, Row augmentation, etc.",
            "6": "In addition, I also encourage the authors to include open models too."
        }
    },
    "2ov9RiAkxE": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical and emerging issue in the deployment of large language models (LLMs) within integrated applications, specifically focusing on identifying and mitigating vulnerabilities.",
            "1": "- The novelty lies in the comprehensive analysis of both insider and outsider threats in LLM-integrated applications and the proposal of a novel defense mechanism, Shield, which is lightweight and does not require retraining the LLM.",
            "2": "Potential reasons for acceptance\n   - The paper tackles a highly relevant and timely problem as LLMs are increasingly integrated into various applications, making the identification of vulnerabilities crucial.",
            "3": "- The proposed defense mechanism, Shield, is innovative and practical, offering a solution that is compatible with existing LLM deployments without requiring significant modifications.",
            "4": "- The empirical evaluations are thorough, demonstrating the effectiveness of the proposed attacks and the efficacy of the defense mechanism across multiple scenarios and risks.",
            "5": "- The paper provides a clear and structured analysis of the attack surfaces and threat models, contributing valuable insights to the field of AI security.",
            "6": "Potential reasons for rejection\n   - **Lack of detailed comparison with existing defenses:**\n     - The paper mentions existing mitigation strategies but does not provide a detailed comparison of Shield with these methods in terms of performance, effectiveness, and practicality.",
            "7": "- A more comprehensive evaluation against state-of-the-art defenses would strengthen the argument for the novelty and superiority of Shield.",
            "8": "- **Limited scope of threat models:**\n     - The paper assumes that both users and LLM service providers are benign, which may not cover all potential real-world scenarios.",
            "9": "- Additional threat models, such as malicious users or compromised LLMs, should be considered to provide a more holistic view of the vulnerabilities and defenses.",
            "10": "- **Evaluation metrics and benchmarks:**\n     - The evaluation relies heavily on specific metrics like TSR and benchmarks like TruthfulQA, which may not fully capture the complexity and variety of real-world applications.",
            "11": "- Including a broader range of metrics and benchmarks would provide a more robust validation of the proposed defense mechanism.",
            "12": "- **Scalability and performance overhead:**\n     - The paper does not thoroughly discuss the scalability of Shield and the potential performance overhead introduced by the additional API.",
            "13": "- Detailed analysis and experiments on the impact of Shield on system performance and user experience are necessary to ensure its practical applicability.",
            "14": "Suggestions for improvement\n   - **Expand the comparison with existing defenses:**\n     - Provide a detailed comparison of Shield with other state-of-the-art defense mechanisms in terms of effectiveness, performance, and practicality.",
            "15": "- Include quantitative and qualitative analyses to highlight the advantages and potential limitations of Shield.",
            "16": "- **Broaden the scope of threat models:**\n     - Consider additional threat models, such as malicious users or compromised LLMs, to provide a more comprehensive analysis of vulnerabilities and defenses.",
            "17": "- Evaluate the proposed defense mechanism against these extended threat models to demonstrate its robustness.",
            "18": "- **Enhance evaluation metrics and benchmarks:**\n     - Incorporate a wider range of evaluation metrics and benchmarks to capture the complexity and variety of real-world applications.",
            "19": "- Include user studies or real-world deployment scenarios to validate the practical effectiveness of Shield.",
            "20": "- **Analyze scalability and performance overhead:**\n     - Conduct detailed experiments to analyze the scalability of Shield and its impact on system performance and user experience.",
            "21": "- Provide insights into the trade-offs between security and performance, and suggest potential optimizations to minimize overhead.",
            "22": "- **Clarify implementation details:**\n     - Provide more detailed implementation guidelines and examples to facilitate the adoption of Shield by practitioners.",
            "23": "- Include a discussion on the integration of Shield with different LLM service providers and application domains."
        },
        "OFXG6uXvwH": {
            "0": "+ The study focuses on an interesting and important topic, the potential vulnerabilities in LLM-integrated applications.",
            "1": "+ The service scheme of LLM-integrated applications is clear presented.",
            "2": "- Lack of real-world case analysis\n\nMy first concern is related to threat evaluation.",
            "3": "In my opinion, it would be better and necessary to provide a set of real-world cases for this threat evaluation, rather than simply mentioning \"consider an online shopping application whose chatbot uses GPT-3.5 and GPT-4 from OpenAI\".",
            "4": "Since there is no detailed information about this shopping application provided, I doubt whether it represents a real-world application.",
            "5": "Even if it is, to present the potential threats more effectively, it would be beneficial to involve multiple real-world applications in the evaluation.",
            "6": "- Sending message directly to LLM may break the business model\n\nIn the proposed mitigation, it is mentioned that \"queries from users are also sent to an LLM along with queries refined by the application\".",
            "7": "If I understand this correctly, this approach may break the business model of LLM-integrated applications, as illustrated in Figure 1.",
            "8": "Additionally, it would be helpful to clarify how directly sending messages to the LLM model can prevent the attacks discussed in the threat model, as transmitting more information may increase the attack surface.",
            "9": "- Not clear what is verified in the proposed Shield\n\nDespite the security concerns that may arise with the proposed Shield, it is not clear what exactly the Shield verifies in the proposed defense.",
            "10": "It appears that the Shield only verifies whether the message originates from a user, rather than conducting semantic analysis.",
            "11": "As described in the threat model and shown in Figure 4, an attacker can manipulate the output of the LLM by sending a malicious system prompt, rather than altering the information in the user's message.",
            "12": "Please clarify how such signature verification can effectively address the potential threats described in Figure 4."
        },
        "2vcihgWc09": {
            "0": "The paper proposes an analysis over vulnerability of LLMs 1.",
            "1": "Assessing the vulnerability of LLMs is an important topic.",
            "2": "However, the analysis presented in the paper and the results obtained from those analysis are already  widely known.",
            "3": "The paper is poorly written.",
            "4": "It is extremely difficult to follow.",
            "5": "The problem setting and the proposed attack surfaces are not  well-defined and it is not clear how these attacks are different from the existing attacks proposed for LLMs (e.g., [1]) .",
            "6": "At the very end of the paper, it proposes a defense mechanism which is not talked about at all throughout the paper.",
            "7": "It is also not clear how the proposed defense mechanism is different form existing defenses proposed for LLMs.",
            "8": "[1]Wei, Alexander, Nika Haghtalab, and Jacob Steinhardt.",
            "9": "\"Jailbroken: How does llm safety training fail?.\"",
            "10": "arXiv preprint arXiv:2307.02483 (2023)."
        },
        "mJairL26iQ": {
            "0": "Paper discusses a relevant area of research which might become very important in the near future.",
            "1": "Because of the recent success of LLMs there is a keen interest in integrating all sorts of applications (including chatbots) with LLMs using APIs.",
            "2": "However most people in the industry are still unaware of the potential risks and security threats involved in doing this although they fear that if they are not doing this they might fall behind.",
            "3": "This work can help identify some of these risks and the mitigation steps and as such will be very useful for the industry practitioners to read and implement.",
            "4": "The contribution of the paper is very well articulated.",
            "5": "For example, it is clear that the authors are not focused on the typical risks like hallucination, unwanted content, privacy and bias associated with the LLM response.",
            "6": "These risks have been well studied and also the industry is more aware of these kind of risks.",
            "7": "The authors here are instead focused on insider and outsider threats associated with LLM integration by which  restrictions and policies imposed by OpenAI can be bypassed to achieve an undesired objective.",
            "8": "The paper proposes a simple yet effective method for guarding against upstream and downstream manipulation of user queries using a signing an verification process which ensures that the correct user query is used for prompting and the correct response is received at the user end.",
            "9": "Any semantic perturbations of the user query or LLM response are detected by the Shield system.",
            "10": "This appears to be a novel contribution and can be easily adopted in the industry.",
            "11": "The scientific contribution of this paper is limited except for the defence detection strategy.",
            "12": "However this method also does not involve any ML/DL and uses cryptographic techniques (RSA based).",
            "13": "Having said that, the overall contribution is valuable as it exposes the weakest of an AI based system and helps in defending against attacks on such systems by malicious users.",
            "14": "2.Some of the contributions of the paper like cost analysis are not mentioned in the paper and is available only in the supplemental information.",
            "15": "Not sure if this can be used in the evaluation of the paper as then the paper itself will exceed the content limit.",
            "16": "However a lot of questions which I had after reading the paper was actually answered satisfactorily by the supplemental material."
        },
        "uvLYKMSwIF": {
            "0": "- This paper provides extensive experimental results on various vulnerabilities in LLM-intergrated applcations.",
            "1": "Considering the rapid expansion of such applications, this work focuses on an important problem.",
            "2": "These results could be valuable for the community for building more secure applications using LLMs.",
            "3": "- It characterizes key properties required for reducing vulnerabilties in LLM-integrated applications.",
            "4": "This characterization could potentially be useful for developing solutions in this domain.",
            "5": "- Experimental results shows that the proposed API,  Sheild, provides effective defense to counter the presented threat models in LLM-integrated applications that use GPT-based models.",
            "6": "- While this work provides extensive empirical results on potential vulnerabilities, the novelty of this work on showing the risks in the query-response protocol with LLM compared to existing works on prompt injection is not clear.",
            "7": "- For attack detection, Shield relies on LLM's capability in detecting maliciousness.",
            "8": "It would be interesting to see how this dependency impacts the overall effectiveness of Shield.",
            "9": "Results from different LLMs may provide some insights."
        }
    },
    "3ucOvX8WVu": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the field of large language models (LLMs) by proposing a novel method, Local Fine-Tuning (LoFT), to improve the transferability of adversarial attacks.",
            "1": "- The approach focuses on fine-tuning proxy models in the local neighborhood of harmful queries, which is a novel idea that could significantly enhance the effectiveness of adversarial attacks on private target models.",
            "2": "- The paper's contributions are relevant to the ongoing research on LLM safety and robustness, making it a significant addition to the field.",
            "3": "Potential reasons for acceptance\n   - The proposed LoFT method is innovative and addresses a well-defined problem in the domain of LLMs.",
            "4": "- The experimental results demonstrate substantial improvements in attack success rates, validating the effectiveness of the proposed approach.",
            "5": "- The paper provides a comprehensive evaluation, including human studies, to assess the real-world impact of the attacks, which adds credibility to the findings.",
            "6": "- The methodology is clearly explained, and the experiments are well-designed, making the research reproducible and transparent.",
            "7": "Potential reasons for rejection\n   - **Limited generalizability of results:**\n     - The experiments are conducted on a specific set of target models (ChatGPT, GPT-4, and Claude), which may not represent the entire spectrum of LLMs.",
            "8": "- The effectiveness of LoFT on other LLMs or in different contexts is not explored, limiting the generalizability of the findings.",
            "9": "- **Insufficient comparison with existing methods:**\n     - The paper primarily compares LoFT with a baseline of no fine-tuning, but does not provide a detailed comparison with other state-of-the-art adversarial attack methods.",
            "10": "- A more thorough comparison with existing techniques could strengthen the argument for the superiority of LoFT.",
            "11": "- **Potential ethical concerns:**\n     - The research focuses on improving the effectiveness of adversarial attacks, which could be misused for malicious purposes.",
            "12": "- While the authors acknowledge the ethical implications, the paper could benefit from a more detailed discussion on the potential risks and mitigation strategies.",
            "13": "Suggestions for improvement\n   - **Expand the evaluation to more diverse LLMs:**\n     - Conduct experiments on a wider range of LLMs to demonstrate the generalizability and robustness of the LoFT method.",
            "14": "- Include models from different providers and with varying architectures to provide a more comprehensive evaluation.",
            "15": "- **Compare with other state-of-the-art methods:**\n     - Include a detailed comparison with other existing adversarial attack methods to highlight the advantages and potential limitations of LoFT.",
            "16": "- Provide quantitative and qualitative analyses to support the claims of improved transferability and attack success rates.",
            "17": "- **Enhance the discussion on ethical implications:**\n     - Provide a more in-depth discussion on the ethical considerations and potential risks associated with the research.",
            "18": "- Suggest concrete measures and guidelines for responsible use and dissemination of the findings to prevent misuse.",
            "19": "- **Improve clarity and organization:**\n     - Ensure that the methodology and experimental setup are described in a clear and concise manner, with sufficient details for reproducibility.",
            "20": "- Organize the paper in a way that highlights the key contributions and findings, making it easier for readers to follow the narrative."
        },
        "8jptr6UQoP": {
            "0": "- Trendy topic\n- Interesting idea\n- Good performance on specific models\n- Convinced evaluation method (human evaluation) - Limited technical contribution\n- Unclear description\n- More explanations are needed"
        },
        "P3rbUeNCOJ": {
            "0": "This paper studies an important research topic of how to attack LLM without knowing the characterization.",
            "1": "The idea of fine-tuning a proxy model to approximate the target model is easy to understand and technically sound.",
            "2": "The authors provide multiple options for similar query generation and conduct a study of the performance of each method.",
            "3": "Some design details of LOFT are not clearly explained.",
            "4": "For example, what is the attack algorithm in Fig.2 and how would it affect the performance of attack transferability?",
            "5": "Some claims are not supported with convincing results and solutions.",
            "6": "For example, the authors claim they “discover that even when an adversarial attack induces the target LLM to respond to a harmful query, the response does not necessarily contain harmful content”.",
            "7": "However, they don’t provide enough results to support the claim.",
            "8": "Also, it would be nice to dive deep and provide a solution (e.g., a new metric) for the observation.",
            "9": "The performance of LOFT is inconsistent across different models (e.g., the improvement on Claude is only 0.5%), which indicates that the utility of LOFT might be limited.",
            "10": "There seems to be negative impact of fine-tuning the proxy model.",
            "11": "For example, in Table 3, the response rate of Claude and GPT-3.5 generated data on GPT-4 are even worse than the baseline without fine-tuning."
        },
        "TusvvCpKar": {
            "0": "- Jailbreak attacks represent a major threat to the safety of LLMs.",
            "1": "The paper studies an important and challenging problem.",
            "2": "- By assuming a black-box setting, the paper considers a more practical threat model than prior work (e.g., Zou et al.",
            "3": "2023).",
            "4": "- The paper considers and evaluates various designs, including the ways of generating semantically similar prompts and transferring prompts generated based on one LLM to another.",
            "5": "- The proposed attack requires fine-tuning the surrogate LLM for each harmful prompt.",
            "6": "It is unclear how this approach scales up to a large number of prompts.",
            "7": "- The evaluation mainly compares LoFT with the case of no fine-tuning, which seems not very meaningful: as the initial prompts are not optimized, they are likely to have low response rate and attack success rate.",
            "8": "I think a more meaningful comparison would be to compare LoFT with other blackbox jailbreak attacks, and show the number of queries needed to generate successful adversarial prompts.",
            "9": "- The number of harmful prompts used in the evaluation is fairly small (25).",
            "10": "It is suggested to conduct a larger-scale experiments using more diverse prompts."
        },
        "vJlH3VoUX9": {
            "0": "==*== Strengths\n\n+ An efficient Local Fine-Tuning (LoFT) method is proposed to effectively adversarially attack large closed-source commercial models.",
            "1": "+ The research problem is well defined and valuable to the research community.",
            "2": "==*== Weaknesses\n\n- The outcomes of the experiment need to be made more convincing.",
            "3": "- Limited in-depth comparison with state-of-the-art solutions."
        }
    },
    "GDdxmymrwL": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Corex, a novel suite of strategies that transform Large Language Models (LLMs) into autonomous agents for complex reasoning tasks.",
            "1": "- Corex leverages multi-model collaboration through Debate, Review, and Retrieve modes, inspired by human behaviors, to enhance the factuality, faithfulness, and reliability of the reasoning process.",
            "2": "- The approach is task-agnostic and aims to overcome the limitations of existing methods by enabling LLMs to \"think outside the box.\"",
            "3": "Potential reasons for acceptance\n   - The paper addresses a significant challenge in the field of NLP by improving the reasoning capabilities of LLMs through multi-model collaboration.",
            "4": "- The proposed Corex framework demonstrates substantial performance improvements across various reasoning tasks compared to existing methods.",
            "5": "- The approach is cost-effective and promotes annotation efficiency, making it a practical solution for real-world applications.",
            "6": "- The paper provides extensive experimental results and in-depth analysis, showcasing the effectiveness and versatility of Corex.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper may not provide sufficient details on the implementation of the Debate, Review, and Retrieve modes, making it difficult for readers to fully understand and replicate the approach.",
            "8": "- The description of the interaction rounds and the decision-making process in the Debate mode could be more detailed to enhance transparency.",
            "9": "- **Limited comparison with state-of-the-art methods:**\n     - While the paper compares Corex with several strong baselines, it may not include comparisons with the latest state-of-the-art methods in the field, potentially limiting the perceived impact of the proposed approach.",
            "10": "- The paper could benefit from a more comprehensive evaluation against a broader range of recent techniques to strengthen its claims.",
            "11": "- **Potential scalability issues:**\n     - The paper does not thoroughly address the scalability of Corex when applied to larger datasets or more complex tasks, which could be a concern for practical deployment.",
            "12": "- The computational overhead and resource requirements for multi-model collaboration might be significant, and the paper should provide more insights into how these challenges are managed.",
            "13": "Suggestions for improvement\n   - **Enhance methodological clarity:**\n     - Provide more detailed explanations and step-by-step descriptions of the Debate, Review, and Retrieve modes, including the specific algorithms and processes involved.",
            "14": "- Include visual aids or flowcharts to illustrate the interaction rounds and decision-making processes in the Debate mode for better understanding.",
            "15": "- **Expand comparative analysis:**\n     - Include comparisons with the latest state-of-the-art methods in the field to provide a more comprehensive evaluation of Corex's performance.",
            "16": "- Conduct additional experiments on a wider range of datasets and tasks to demonstrate the versatility and robustness of the proposed approach.",
            "17": "- **Address scalability and resource management:**\n     - Discuss the scalability of Corex in more detail, including potential challenges and solutions for handling larger datasets and more complex tasks.",
            "18": "- Provide insights into the computational overhead and resource requirements for multi-model collaboration, and suggest strategies for optimizing performance and efficiency.",
            "19": "- **Improve presentation and readability:**\n     - Ensure that the paper is well-organized and clearly written, with a logical flow of ideas and arguments.",
            "20": "- Use consistent terminology and avoid jargon to make the paper accessible to a broader audience, including those who may not be experts in the field."
        },
        "2LlrFhQ7PN": {
            "0": "- This paper proposes multi-model collaborations for solving complex reasoning problems, which is an interesting and attractive direction for the current community.",
            "1": "- A fairly diverse set of results across several benchmarks are provided, with several LLMs explored.",
            "2": "The analysis is detailed and provides some insights into the proposed method.",
            "3": "I especially like the analysis for cost-effectiveness.",
            "4": "Code is provided for reproducibility.",
            "5": "- The paper is generally well-written and organized, and most of the content is clear to me.",
            "6": "Code is also provided for reproducibility.",
            "7": "- The proposed three modes are not that novel to me.",
            "8": "There are many existing works that have already explored or at least share similar ideas to the three modes.",
            "9": "I only list several representative works here.",
            "10": "For Debate mode, [1] already explored this setting.",
            "11": "For Review mode, [2] has a similar method.",
            "12": "And finally for Retrieve mode, [3] also shares similar ideas.",
            "13": "I don't think Section 2 Related Works is well-written either.",
            "14": "It only lists related works without thoroughly discussing the relatedness and differences.",
            "15": "Also, I don't see why \"External knowledge and tool utilization\" is that related to this work.",
            "16": "- The performance improvement is not that consistent.",
            "17": "It seems that in most cases, *Corex-Debate* and *Corex-Review-NL* do not perform that well.",
            "18": "Instead, *Corex-Review-Code* and *Corex-Retrieve* seems to be better.",
            "19": "I think it demonstrates the advantage of using PL, which is a well-acknowledged fact in the community.",
            "20": "The paper also does not provide explanations or understandings of why these methods work well or not.",
            "21": "I think it is important to have [1,2,3] as baselines and better explain why *Corex* works as a paradigm of multi-model collaboration.",
            "22": "[1] Du, Yilun, et al.",
            "23": "\"Improving Factuality and Reasoning in Language Models through Multiagent Debate.\"",
            "24": "arXiv preprint arXiv:2305.14325 (2023).",
            "25": "[2] Madaan, Aman, et al.",
            "26": "\"Self-refine: Iterative refinement with self-feedback.\"",
            "27": "arXiv preprint arXiv:2303.17651 (2023).",
            "28": "[3] Yang, Chengrun, et al.",
            "29": "\"Large language models as optimizers.\"",
            "30": "arXiv preprint arXiv:2309.03409 (2023)."
        },
        "Yqcmh3ZuTU": {
            "0": "S1) The coverage of the related work in the paper is very extensive and seems complete.",
            "1": "S2) The paper lists a lot of challenges, which are indeed relevant, including misunderstanding the question, or generating a faulty reasoning process.",
            "2": "Using multiple LLMs, treated as agents, is an exciting direction to explore to address these challenges.",
            "3": "S3) The paper is overall relatively well-written and easy to follow.",
            "4": "S4) The method should be relatively easy to reproduce based on the details provided in the paper.",
            "5": "S5) The results are informative as they cover a large set of tasks and categories.",
            "6": "The additional analysis sheds light on the method's behavior and computational efficiency.",
            "7": "W1) Contribution unclear - It is clear that the Corex variants fare better against the baselines, but it is less clear what is the main contribution of Corex.",
            "8": "All three components are based on ideas that are present in prior work, and Corex does not integrate the components.",
            "9": "So methodologically, it is indeed only a suite of what has been done before.",
            "10": "To make the contribution further complicated, the abstract and the introduction mention (often vaguely) a number of issues, including \"the limitations of its internal representation\", \"limitations in solving reasoning tasks\", \"unreliable answers\", \"think outside the box\", \"prevalent obstacles\".",
            "11": "The three issues that are specifically listed in Figure 1: wrong calculation, misunderstanding the relationship between variables, and codes fail to accurately reflect the problem statement - are actionable, but there is no analysis on whether the improved performance of Corex has a qualitative impact on any of these issues.",
            "12": "The desired features reviewed in Table 1 (e.g., reference free, multiple LLMs) are again different from what the introduction was arguing.",
            "13": "In the other sections, aspects like factuality, task-agnosticity, and reliability are argued for, but again, there was no experiment to validate these claims.",
            "14": "W2) Comparison to baselines - the paper compares against one set of baselines in Table 1, then another set of baselines is referred to in the method section (e.g., Du et al.",
            "15": "2023 for the Debate module), and then the results and the analysis focus on general approaches like CoT and SC.",
            "16": "It is unclear why the other baselines from Table 1 and prior works that designed the individual components are not included in the evaluation.",
            "17": "This is even more important because Corex diverges from prior works to design these components differently (e.g., the Debate component), and it is important to know if this different approach fares better or worse, and why.",
            "18": "W3) Originality - the proposed method reads like a more complete combination of heuristics compared to prior work, but these heuristics are already present in recent methods.",
            "19": "In that sense, it is unclear what is the methodological delta between this method and prior work.",
            "20": "This novelty gap is further blurred by the absence of a clear problem statement in section 1, and the lack of direct comparison to related work in section 2.",
            "21": "W4) Premise - The overall premise of Corex is also confusing.",
            "22": "If LLMs cannot reason reliably (as stated in section 1), then what makes these same LLMs suddenly able to reason in Corex?",
            "23": "Moreover, in some of the Corex variants, like the Debate module, it is unclear what reasoning means exactly - because the design here explicitly opts for a majority voting to suppress reasoning.",
            "24": "The Retriever compares different chains of thought, but whether these are scored based on their reasoning soundness, is not clear.",
            "25": "W5) Result takeaways - The results often say \"our method\" but in fact Corex is a multitude of methods, whose probability of outperforming the baselines is generally around 50% (e.g., table 5 has 4 Corex variants and three baselines).",
            "26": "Moreover, the best Corex variant is largely unstable over the tasks, though the Debate one is typically the weakest, while the Retrieve and Review-code are usually performing better.",
            "27": "The results need a discussion that dives deeper into these distinctions in performance across tasks.",
            "28": "Minor:\n* Retrieve is not a paradigm\n* Footnote 3 - not clear what is meant by the \"nature of commonsense tasks\" - prior works have used code representations to also address these"
        },
        "i2ncMSJAPg": {
            "0": "The idea of using multi-agent collaboration to solve complex tasks is well-motivated and is a promising direction.",
            "1": "The paper is in general well written and easy to follow.",
            "2": "The experimental results show some improvement upon the compared baselines.",
            "3": "The paper lacks technical novelty.",
            "4": "Multi-agent debate for reasoning and complex task-solving, the \"review\" method is also quite similar to recent work on self-reflection and self-refinement, the \"retrieve\" method is intuitively very similar to RAG (retrieval-augmented generation) and the main difference is incorporating it in the multi-agent framework using one agent as the retriever.",
            "5": "The proposed components including debate / review / retrieve are not conceptually very much related.",
            "6": "Instead they seem to be distinct methods.",
            "7": "The authors also use them separately in the experiments without combining or integrating them into a single framework.",
            "8": "The performance improvement over stronger baselines such as CoT-SC(10) is not very significant.",
            "9": "Also, It is unshown whether variants such as CoT-SC(20/30) will lead to different conclusions.",
            "10": "Maybe it's because the CoT-SC(10) baseline consumes similar number of tokens with OpenAI's API?",
            "11": "But the authors did not show the total tokens consumed by different methods.",
            "12": "And if so the comparison with CoT would not be fair enough.",
            "13": "The manuscript lacks analysis of when (or for which kind of tasks) one of the methods among debate / review / retrieve outperform others and why it is the case.",
            "14": "Adding some analysis about this question will bring more insights for the manuscript."
        },
        "eLOyNp1T9B": {
            "0": "--The idea of collaborative language models is very interesting and fairly novel.",
            "1": "--Using the debate structure to guide the interactions is novel and effective.",
            "2": "--The experiments are conducted on many and a variety of benchmarks with different tasks.",
            "3": "--The analysis is interesting and insightful.",
            "4": "--The notation is a bit unclear in some places.",
            "5": "In the very beginning explaining the debate,  what is k?",
            "6": "Please denote this when you explain c^i_t.",
            "7": "c_i is the viewpoint or one step of the reasoning chain?",
            "8": "Or both?",
            "9": "Please make it explicit and use one term consistently.",
            "10": "--I expected the collaboration of multiple models to be based on 5 different LLMs.",
            "11": "The paper uses GPT 3 and 4 and Claude.",
            "12": "It was not clear in the paper how the 5 different opinions were solicited.",
            "13": "Are you using different temperatures or obtaining multiple samples from one LLM and looking at them as different heterogeneous agents?",
            "14": "I see in the experiments that you use different LLMs to play the judge roles but was not sure if that is enough to have a real heterogeneous setting with multiple agents."
        }
    },
    "RIbH5ekQpr": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical gap in vision-language models by introducing the concept of image polysemy, which is the phenomenon where a single image can convey multiple meanings.",
            "1": "- The introduction of the IMP dataset is a novel contribution that challenges existing models to handle diverse and conceptual captions, moving beyond purely descriptive paradigms.",
            "2": "- The study provides empirical evidence that current vision-language models fall short in recognizing multiple semantic dimensions of images, highlighting the need for more robust approaches.",
            "3": "Potential reasons for acceptance\n   - The paper introduces a novel and significant dataset (IMP) that addresses a critical gap in vision-language research.",
            "4": "- The empirical results are well-documented and provide clear evidence of the limitations of current models in handling image polysemy.",
            "5": "- The paper is well-structured, with a comprehensive review of related work, detailed methodology, and thorough experimental evaluation.",
            "6": "- The proposed benchmark can serve as a valuable resource for the research community, encouraging further exploration and improvement of vision-language models.",
            "7": "Potential reasons for rejection\n   - **Lack of detailed analysis on the dataset creation process:**\n     - The paper briefly mentions the data curation pipeline but lacks detailed information on the criteria used for selecting and annotating captions.",
            "8": "- More transparency on the annotation process and the diversity of the annotators could strengthen the validity of the dataset.",
            "9": "- **Limited exploration of alternative approaches:**\n     - The paper primarily focuses on contrastive learning and multi-view models but does not explore other potential approaches that could address image polysemy.",
            "10": "- Including a broader range of models and techniques could provide a more comprehensive evaluation of the problem.",
            "11": "- **Insufficient discussion on the implications of findings:**\n     - The paper presents empirical results but lacks a deep discussion on the broader implications of these findings for the field of vision-language research.",
            "12": "- More insights into how these results could inform future model development and training strategies would be beneficial.",
            "13": "Suggestions for improvement\n   - **Provide more details on the dataset creation process:**\n     - Include a more detailed description of the criteria used for selecting and annotating captions, as well as the diversity and expertise of the annotators.",
            "14": "- Discuss any potential biases in the dataset and how they were addressed.",
            "15": "- **Explore a wider range of models and techniques:**\n     - Evaluate additional models and techniques beyond contrastive learning and multi-view models to provide a more comprehensive assessment of the problem.",
            "16": "- Consider incorporating models that leverage different types of supervision or incorporate external knowledge.",
            "17": "- **Enhance the discussion on the implications of findings:**\n     - Provide a deeper analysis of the broader implications of the empirical results for the field of vision-language research.",
            "18": "- Discuss how these findings could inform future model development, training strategies, and the design of new benchmarks.",
            "19": "- **Include more qualitative analysis:**\n     - Provide more examples of hard captions and a detailed analysis of why certain captions are challenging for the models.",
            "20": "- Discuss potential strategies for improving model performance on these challenging cases."
        },
        "7UjeP7NiIq": {
            "0": "The dataset is novel, and is the first to study image polysemy in the context of image-text matching / retrieval.",
            "1": "The experiment evaluation covers a wide range of models and is sufficient.",
            "2": "I am concerned about the difficulty of the task.",
            "3": "\"Vanilla\" image-text retrieval itself is not straightforward to evaluate, given the high rate of false negative caused by plausible but unrecorded matches [1,2].",
            "4": "The subjectivity of image polysemy amplifies all these difficulties.",
            "5": "I wonder to what degree this task is even _possible_ to evaluate objectively.",
            "6": "For example, it was not obvious to me that the caption and images in Fig 4.",
            "7": "Col1 & Col 4 go together.",
            "8": "Similarly for many of pairs in Fig 3.",
            "9": "The subjectiveness of this benchmark calls for a human accuracy check, given that this benchmark is so heavily based on human judgements of image meaning.",
            "10": "The human judgements of similarity can then be used to create similarity judgements similar to [1,2] or correct false negatives.",
            "11": "Given the dataset construction technique, I'm skeptical whether the evaluations are meaningful given that the captions are abstract enough that many of the might match plausibly match to other images.",
            "12": "This could be corrected by verifying human accuracy on the dataset and modifying the benchmark so that correlation with human accuracy is being assessed instead.",
            "13": "Note that the annotation process _does not_ guarantee this, since the annotation procedure does not exclude the possibility that there are 10-15 other captions somewhere in the dataset that describe the image equally well, and vice versa for any image.",
            "14": "You could do this by using CLIP to rank _all_ the captions in the dataset w.r.t to a particular image (and vice versa), and having humans rerank the top 50 from each cluster.",
            "15": "This could be used to measure both human accuracy, and capture more plausible measurements of human similarity.",
            "16": "[1] ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO\n[2]  Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for ms-coco"
        },
        "2s6D7OA5jw": {
            "0": "- Tackles an important barrier in higher level image-text comprehension\n- Contributes another fairly large scale dataset that can be of use to community - The claim for having higher diversity in captions in this work is slightly problematic because diversity is never clearly defined.",
            "1": "Based on their method, it seems to refer specifically to whatever euclidean distance can be associated with in sentenceBERT's embedding representation, which I am not entirely sure how to interpret.",
            "2": "- Some concerns regarding methodology which I elaborate on in the next section.",
            "3": "- While this work certainly provides a useful benchmark for polysemy, I don't think the contributions of this work sufficiently shed light on any new aspects of the problem that the community was already aware of."
        },
        "qesICnTjcU": {
            "0": "Extensive zero-shot experiments across a large collection of models.",
            "1": "Should prove to be a useful resource for evaluating VLMs.",
            "2": "I didn't feel like both Figure 1 and Figure 3 needed to exist in the main body of the paper because it feels like they are duplicating information.",
            "3": "Not entirely clear why the dataset needed to be based on \"high-quality stock photography from Unsplash\"."
        },
        "QJ7gqzGpuB": {
            "0": "- S1: I think that the research question studied in this paper is significant and interesting.",
            "1": "This benchmark would be really beneficial to the community.",
            "2": "As far as I can tell, no benchmark like this exists.",
            "3": "Previous benchmarks also suffer from losing images over time.",
            "4": "- S2: Overall, both the approach for collecting the dataset and experimental design are sound.",
            "5": "- W1: Analysis of this dataset can be improved significantly from what is reported in Table 1.",
            "6": "My major concern is in the characterization of polysemy, which is not much beyond “non-descriptive”.",
            "7": "For instance, does any kind of image-text correspondence ontology emerge?",
            "8": "Perhaps this needs human classification of (a subset) captions into a pre-defined categories, or automatically clustering captions, etc.",
            "9": "This is especially important since quantitatively MPL2D cannot distinguish noise from diversity.",
            "10": "Finally, I would also like to see further statistics beyond word lengths (word clouds, token/type ratio, etc.",
            "11": "that would lead us to have a clearer picture of how this dataset is more diverse than existing ones.",
            "12": "- W2: The tasks that perform on this dataset are standard image- and text- retrieval tasks with zero-shot and fine-tuning evaluation.",
            "13": "Further, an adaptation of models that address image polysemy does not lead to improved performance (see, e.g., SE models in Table 4).",
            "14": "These experiments are a good start but the paper would be stronger with additional tasks that focus on measuring model’s capability in addressing polysemy such as image captioning generation given a “caption sense”.",
            "15": "- W3: Besides improving the analysis in W1 above, the discussion of image-text datasets can also be further expanded to include additional datasets in the analysis, including RedCaps and SBU captions.",
            "16": "- W4: Clarity on the data collection process is quite obscure (under Table 4).",
            "17": "IMO, it is important to include the details on the database used to retrieve captions (CC3M and CC12M) in the main text so the reader is well-informed about the bias of these captions.",
            "18": "In addition, it is important to include the details on how the authors optimize for diversity and quality control."
        },
        "pf1gBanuVp": {
            "0": "Most prior work seeks to have cleaner and more descriptive captions; it is interesting to see efforts on including more conceptual captions.",
            "1": "The evaluation shows that current models struggle with retrieving more abstract and conceptual captions, which raises an interesting and challenging problem.",
            "2": "**A.",
            "3": "** It seems that the “image polysemy” considered in this paper mainly means: the model should have the ability to match images to both “descriptive” and “conceptual” captions; previous datasets such as COCO contains mainly descriptive captions.",
            "4": "However, I am not fully convinced that the collected datasets have more “conceptual” captions than the web image-text data such as CC3M, since many of the captions in the dataset come from web data.",
            "5": "The MPL2D also does not indicate that the collected dataset is more “conceptual”.",
            "6": "The only advantage of the dataset seems to be that it has multiple captions per image while CC3M/12M does not.",
            "7": "But if the final goal is to teach a model to match an image to descriptive / conceptual captions, then it is not clear why it is necessary to have multiple captions per image for training; as long as CC3M has a lot of conceptual & descriptive captions, then the model can learn to retrieve both types captions.",
            "8": "E.g., say there is a dataset A with 1K images each with 5 captions; suppose a dataset B with 5K images each with 1 caption.",
            "9": "The captions in A and B are identical.",
            "10": "Then I do not see the necessity of training on A if we have dataset B.",
            "11": "In sum, it would be better if the paper could illustrate either a) why / how the dataset has more diverse / conceptual captions than CC3M or b) the importance of having multiple captions per image.",
            "12": "**B.",
            "13": "** The experiments are not very insightful.",
            "14": "While the problem of image polysemy is interesting, the paper simply evaluates/fine-tunes current models with image-text retrieval on the collected datasets.",
            "15": "The take-away conclusion seems to be that the task is hard and larger models perform better.",
            "16": "I would expect more analysis and discussions on why and how studying image polysemy could benefit future vision-language models and how to model such a phenomena.",
            "17": "For example, does explicitly modeling image polysemy benefit other tasks that could require high-level conceptual understanding (e.g., understanding actions, events, memes, etc)?",
            "18": "**C.** For the image-to-text retrieval evaluation, how does this test model’s ability to handle image polysemy?",
            "19": "If I understand it correctly, as each image has 5 matching captions, as long as the model retrieves 1 of the matching captions, then it is counted as correctly retrieved?",
            "20": "Then the evaluation protocol does not test whether the model handles polysemy; if for most images, at least 1 of the captions is “descriptive”, then a model that only “understands” descriptive captions will still score high on image-to-text retrieval.",
            "21": "**D.** For comparing the dataset with CC3M/CC12M on MPL2D, why not treat the collected dataset as a single caption dataset (either downsample to one caption per image or just “duplicate” the images)?",
            "22": "In addition, I am not sure about the takeaway message by comparing MPL2D: is higher MPL2D score better or lower score better?",
            "23": "On the one hand, if there are more diverse / conceptual captions, the score is higher; on the other hand, if the captions are noisier, the score is also higher.",
            "24": "Thus, a lower/higher score could be attributed to these two possible factors and we cannot make a conclusion."
        }
    },
    "dONpC9GL1o": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in neural text generation, specifically the effectiveness of truncation sampling methods like nucleus sampling.",
            "1": "- It provides a theoretical explanation for why these methods work and introduces a new sampling strategy that leverages the softmax bottleneck to improve text generation quality.",
            "2": "- The proposed method, Basis-Aware Threshold (BAT) sampling, is shown to outperform traditional threshold-based methods in certain scenarios, particularly in low-entropy text generation.",
            "3": "Potential reasons for acceptance\n   - The paper offers a novel theoretical insight into the effectiveness of truncation sampling methods, which is a significant contribution to the field of natural language processing.",
            "4": "- The proposed BAT sampling method is innovative and addresses a known limitation of current sampling strategies by leveraging the softmax bottleneck.",
            "5": "- The experimental results, including both automatic and human evaluations, demonstrate the practical benefits of the proposed method, particularly in low-entropy generation tasks.",
            "6": "- The paper is well-structured and provides a clear explanation of the theoretical foundations, the proposed method, and the experimental results.",
            "7": "Potential reasons for rejection\n   - **Computational Complexity**:\n     - The proposed BAT sampling method involves solving large linear programs, which can be computationally expensive and impractical for large-scale text generation tasks.",
            "8": "- The paper mentions that even with optimizations, the method can take about 0.1 seconds per token, which may still be too slow for real-time applications.",
            "9": "- **Limited Scope of Experiments**:\n     - The experiments are conducted primarily on GPT-2 models, and it is unclear how well the proposed method generalizes to other language models, especially larger and more complex ones.",
            "10": "- The evaluation focuses on low-entropy generation tasks, and the performance of BAT sampling in high-entropy or more diverse generation scenarios is not thoroughly explored.",
            "11": "- **Inconsistent Performance**:\n     - The results show that BAT sampling does not consistently outperform traditional methods across all model sizes and scenarios, which raises questions about its robustness and general applicability.",
            "12": "- The method does not pair well with nucleus sampling, which is a widely used truncation strategy, limiting its practical utility.",
            "13": "Suggestions for improvement\n   - **Address Computational Complexity**:\n     - Explore more efficient optimization techniques or approximations to reduce the computational overhead of BAT sampling.",
            "14": "- Investigate the feasibility of parallelizing the linear program solving process to speed up the sampling method.",
            "15": "- **Expand Experimental Scope**:\n     - Conduct experiments on a wider range of language models, including larger and more recent models, to validate the generalizability of the proposed method.",
            "16": "- Include evaluations on high-entropy generation tasks and other diverse text generation scenarios to provide a more comprehensive assessment of BAT sampling.",
            "17": "- **Improve Consistency and Robustness**:\n     - Investigate the reasons behind the inconsistent performance of BAT sampling across different model sizes and scenarios, and propose modifications to enhance its robustness.",
            "18": "- Explore hybrid approaches that combine BAT sampling with other truncation strategies, such as nucleus sampling, to leverage the strengths of both methods.",
            "19": "- **Provide Practical Guidelines**:\n     - Offer practical guidelines and recommendations for choosing the appropriate parameters for BAT sampling in different text generation tasks.",
            "20": "- Include a detailed analysis of the trade-offs between computational cost and generation quality to help practitioners make informed decisions."
        },
        "U1ZdwK842i": {
            "0": "The proposed approach is innovative and theoretically-grounded.",
            "1": "This paper brings new insights to the community.",
            "2": "The theoretical concepts discussed in this paper are previously ignored but seems to be important.",
            "3": "Basis-aware sampling is specifically designed for models trained using cross-entropy loss.",
            "4": "However, not all language models meet this criterion.",
            "5": "For instance, LLMs fine-tuned with RLHF do not adhere to this condition.",
            "6": "Theorem 2 and Corollary 2 provide sufficient but unnecessary conditions for proving tokens are in the true support.",
            "7": "Therefore, the induced sampling algorithm may also discard tokens in the support of true distribution, leading to biased sampling.",
            "8": "In Corollary 1, the statement \"threshold-based truncation sampling correctly discards all tokens that are not in the support of p*\" is not precise, as it may also incorrectly discard tokens that are in the support of p*.",
            "9": "A more precise phrasing would be: \"all tokens that are not in the support of p* will be discarded by threshold-based truncation sampling.\"",
            "10": "The relationship between the softmax bottleneck and text degeneration phenomena has not been verified.",
            "11": "It remains unclear whether text degeneration is directly caused by the softmax bottleneck, or if increasing the dimensionality (d) beyond the vocabulary size (v) would effectively resolve the issue."
        },
        "0EYFL0hNoA": {
            "0": "This is a great analysis paper, providing an interesting explanation for why truncation sampling works so well in language model decoding.",
            "1": "The paper's motivation is clear and well-written.",
            "2": "The fact that BAT can determine that some tokens have nonzero true support, even though they are assigned less probability than others which are not in the support of the true distribution, is a surprising and compelling result.",
            "3": "Leveraging the softmax bottleneck is a clever trick here and one that will be unexpected to most readers in NLP.",
            "4": "I expected BAT to be computationally infeasible to run in practice due to its dependence on an LP-solver at each tilmestep of decoding.",
            "5": "However, the speedups in the \"Basis-aware threshold sampling in practice\" (namely, using a decomposition of the softmax matrix and only relying on BAT when a token under the threshold probability is chosen) seem reasonable and compelling, and the amortized cost of 0.1s/token, while slow, is not infeasible for certain classes of applications.",
            "6": "The experiments, although not particularly compelling as a reason to start using BAT sampling in practice, seem reasonable and sufficiently thorough.",
            "7": "In particular, the analysis of performance as more constraints are added back (after the SVD) is very clear.",
            "8": "In contrast, I did not find the \"BAT outperforms all other methods for GPT-2-Large\" paragraph very compelling given that BAT is not the best-performing model on any other model size.",
            "9": "The primary weakness seems to be the performance of BAT compared to other methods.",
            "10": "Despite its theoretical justification, it does not clearly outperform other sampling approaches (Figure 5).",
            "11": "Although there is a preference for BAT to eta-sampling shown in Figure 6 and Table 1, this preference is very slight and the comparison is only between two sampling methods.",
            "12": "However, I do not see this weakness as a legitimate reason to reject the paper, since its main contribution seems to be analysis and theoretical understanding of existing decoding algorithms."
        },
        "rrgW7V8GfO": {
            "0": "- Nice idea and analysis\n- Well written / clear\n- Shines new insights on a well-studied problem and could lead to more promising sampling methods - Results are rather weak, efficacy of the method still remains to be demonstrated (minor)\n- An pseudo-code / algorithm box with the practical implementation of BA is needed in the main paper (minor)\n- Unclear whether the method will help for larger models or for models where the approximation errors (under-estimation / over-estimation) are small (kinda major)."
        },
        "K1uEotYN5O": {
            "0": "* The work offers a theoretical explanation for why certain ad-hoc methods used during language generator decoding work well.",
            "1": "This is a valuable insight to the NLG community\n* The work then develops a sampling algorithm based on this theoretical explanation * The theoretical portion of the paper is at times difficult to understand due to notational choices and lack of specificity (for example, switching between individual token probabilities ).",
            "2": "This is particularly important since the theoretical portion is the main contribution of the work \n* The method does not appear to have empirical performance benefits and is computationally expensive, making it impractical\n* There lacks robust empirical justification of the hypothesis.",
            "3": "Figure 7, which is intended to show that including more of the original optimization problem constraints lead to better results, only consists of 3 points, which hardly feels like enough evidence to claim a “trend”.",
            "4": "* The terminology of the “true” distribution is perhaps misleading.",
            "5": "I personally think that something like the “aggregate” distribution or the “data-generating” distribution would be more accurate\n* A small point: the intro of section 4 has some grammatical errors"
        }
    },
    "6J7WG1jyog": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of localization in large language models (LLMs) for the Arabic language, which is often overlooked in mainstream models.",
            "1": "- It introduces AceGPT, a localized LLM for Arabic, which incorporates cultural sensitivity and local values through a comprehensive approach including pre-training, supervised fine-tuning, and reinforcement learning with AI feedback.",
            "2": "- The paper presents a new benchmark, the Arabic Cultural and Value Alignment (ACVA) dataset, to evaluate the cultural and value alignment of LLMs.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a significant and underexplored problem in the field of LLMs, specifically the localization issue for non-English languages.",
            "4": "- The methodology is comprehensive and well-structured, combining multiple techniques to ensure cultural and value alignment in the Arabic context.",
            "5": "- The results demonstrate that AceGPT achieves state-of-the-art performance across various benchmarks, including instruction-following, knowledge, and cultural alignment.",
            "6": "- The introduction of the ACVA dataset provides a valuable resource for future research in the localization of LLMs.",
            "7": "Potential reasons for rejection\n   - **Limited pre-training data and resources**\n     - The pre-training phase was constrained by limited machine resources, which may have restricted the model's potential in handling Arabic content.",
            "8": "- The vocabulary derived from LLaMA2 was not expanded, potentially reducing the efficiency of Arabic text encoding tasks.",
            "9": "- **Evaluation limitations**\n     - The paper does not conduct reasoning, misinformation, and bias testing, which are critical aspects of evaluating LLMs.",
            "10": "- The safety alignment of the model is not thoroughly addressed, raising concerns about its suitability for online deployment.",
            "11": "- **Quality and quantity of cultural dataset**\n     - Although manual verification was conducted on the cultural dataset, there is room for improvement in both the quality and quantity of the questions.",
            "12": "- The limited scope of the cultural dataset may impact the model's practical application and adoption.",
            "13": "Suggestions for improvement\n   - **Expand pre-training data and resources**\n     - Increase the amount of Arabic text data used for pre-training to fully realize the model's potential in handling Arabic content.",
            "14": "- Consider expanding the vocabulary to include more Arabic-specific tokens to improve text encoding efficiency.",
            "15": "- **Comprehensive evaluation**\n     - Conduct additional evaluations focusing on reasoning, misinformation, and bias to provide a more holistic assessment of the model's performance.",
            "16": "- Address safety alignment concerns by implementing and testing safety measures to ensure the model's suitability for online deployment.",
            "17": "- **Enhance cultural dataset**\n     - Improve the quality and quantity of the cultural dataset by incorporating a wider range of questions and ensuring thorough manual verification.",
            "18": "- Collaborate with native Arabic speakers and cultural experts to refine the dataset and ensure it accurately reflects the diverse cultural and value norms of Arabic-speaking communities."
        },
        "cm8ykwwI0y": {
            "0": "The authors organize the paper clearly and describe in details on how to tune an LLM to work better on a specific language Arabic.",
            "1": "From the evaluation, tuning improves metrics on both human and auto evaluations.",
            "2": "I think the main weakness is novelty.",
            "3": "Though novel application should be also considered an contribution, I do not think the paper provides many insights on LLM in Arabic.",
            "4": "It seems to just follow the common techniques continue training/SFT/RLAIF."
        },
        "qzS5qBWX8e": {
            "0": "This paper is well-written and easy to follow.",
            "1": "Localization on specific culture is an important topic in LLMs.",
            "2": "This paper mentioned Arabic-related dataset and models, which can be useful for the people in related fields.",
            "3": "The theoretical and technical contributions are poor.",
            "4": "This paper is more like a engineering report to introduce how to localize a public LLM on Arabic, illustrating the operation and dataset during pre-training, instruction tunning and RLHF stage.",
            "5": "All the methods are well-known.",
            "6": "The findings are intuitive, using localize data to pre-train, instruction tuning and training RLHF can be helpful for better localization.",
            "7": "It seems more suitable for an empirical NLP conference rather than a learning conference.",
            "8": "The newly introduced Arabic Cultural and Value Alignment benchmark is not novel.",
            "9": "Previous work [1] also introduce a benchmark to measure the culture bias in Arabic LLMs.",
            "10": "It seems the authors didn’t mention and cite this paper.",
            "11": "The analyses are insufficient.",
            "12": "I will be more excited to see a comprehensive and systematic evaluation on the contribution of different operation, i.e., pre-training, instruction tuning and RLHF, on different evaluation perspective, such as instruction-following, knowledge accuracy, culture alignment, etc.",
            "13": "[1] Having Beer after Prayer?",
            "14": "Measuring Cultural Bias in Large Language Models, https://arxiv.org/abs/2305.14456"
        },
        "2pdheArqie": {
            "0": "* Pre-training with Arabic data seems to improve performance on MMLU and ACVA benchmarks, proving the utility of native language data.",
            "1": "* An interesting analysis of preference for certain cultural contexts in Language model responses.",
            "2": "A dataset for studying cultural alignment the same has been created, which is a novel and useful contribution.",
            "3": "* Better performance than JAIS, comparable to ChaptGPT.",
            "4": "* Ablation studies show the utility of pre-training with Arabic data and RLAIF (which improves both instruction following and localization) * While the paper makes an interesting contribution to an improved Arabic LLM, it does little to advance the study of building/adapting LLMs for non-English languages.",
            "5": "Most of the methods are well known.",
            "6": "A few studies can help draw broader lessons on the localization of LLMs: \n    * How much pre-training data is required?",
            "7": "What is the best data balance between English and other languages?",
            "8": "* Does the English performance get impacted due to the finetuning?",
            "9": "* How objective is the ACVA benchmark?",
            "10": "Different annotators (even amongst Arabic annotators) themselves might have different viewpoints on Arabic culture.",
            "11": "On  subjective questions, what is the inter-annotator agreement on the answers?",
            "12": "* Evaluation using automatically translated datasets can be problematic (MMLU and EXAM).",
            "13": "* Some human judgment on translation quality should be provided to understand the quality of the benchmark.",
            "14": "* You could consider other translation models: GPT4 vs NLLB or any available Arabic-specific NMT model that might be better than GPT4."
        }
    },
    "3TO3TtnOFl": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the deployment of retrieval-augmented language models (LMs), specifically the inefficiency and high storage costs associated with processing large amounts of retrieved text.",
            "1": "- The introduction of Binary Token Representations (BTR) is a novel approach that significantly reduces computation during inference by using 1-bit vectors to precompute every token in passages.",
            "2": "- The paper presents new calibration techniques and training objectives to restore performance despite the potential loss of accuracy due to binarization.",
            "3": "Potential reasons for acceptance\n   - The proposed BTR method demonstrates substantial improvements in inference speed (up to 4x) and storage reduction (over 100x) while maintaining over 95% task performance.",
            "4": "- The paper provides a comprehensive evaluation of BTR on five knowledge-intensive NLP tasks, showing its effectiveness across different scenarios.",
            "5": "- The introduction of offline and runtime compression techniques further enhances the efficiency of BTR, making it a practical solution for real-world applications.",
            "6": "- The paper includes detailed ablation studies and comparisons with state-of-the-art baselines, providing strong evidence of the benefits of BTR.",
            "7": "Potential reasons for rejection\n   - **Limited applicability to decoder-only models:**\n     - The paper acknowledges that extending BTR to decoder-only readers is non-trivial due to the sequential nature of passage representation computation.",
            "8": "- The challenges associated with breaking computational dependencies and caching passage representations in decoder models are not addressed in this work.",
            "9": "- **Potential accuracy degradation:**\n     - Although the paper claims that BTR maintains over 95% task performance, there is still a noticeable drop in accuracy compared to the original models.",
            "10": "- The effectiveness of the proposed training regularization techniques in mitigating accuracy loss may vary across different tasks and datasets.",
            "11": "- **Scalability concerns:**\n     - The paper mentions the need for further research to scale BTR for larger models with bigger representation sizes.",
            "12": "- The current implementation may not be directly applicable to extremely large models or datasets without additional modifications.",
            "13": "- **Complexity of implementation:**\n     - The introduction of multiple components such as calibrated binarization, offline compression, and runtime compression adds complexity to the implementation of BTR.",
            "14": "- Practitioners may find it challenging to integrate BTR into existing systems without significant effort and expertise.",
            "15": "Suggestions for improvement\n   - **Addressing decoder-only models:**\n     - Provide a more detailed discussion on potential approaches to extend BTR to decoder-only models, including possible solutions to the challenges mentioned.",
            "16": "- Conduct preliminary experiments or simulations to demonstrate the feasibility of these approaches.",
            "17": "- **Improving accuracy:**\n     - Explore additional training techniques or model architectures that could further mitigate the accuracy loss due to binarization.",
            "18": "- Investigate the impact of BTR on a wider range of tasks and datasets to ensure its robustness and generalizability.",
            "19": "- **Scalability enhancements:**\n     - Develop methods to efficiently scale BTR for larger models and datasets, such as using autoencoders or other dimensionality reduction techniques.",
            "20": "- Provide empirical results on the performance of BTR with larger models to validate its scalability.",
            "21": "- **Simplifying implementation:**\n     - Offer detailed implementation guidelines and best practices to help practitioners integrate BTR into their systems.",
            "22": "- Consider releasing a comprehensive toolkit or library that encapsulates the various components of BTR, making it easier for users to adopt the method."
        },
        "V3uNytBy7T": {
            "0": "For the first time, the authors construct binary token representations to improve the efficiency of retrieval augmentation models, an approach that has never been explored before.",
            "1": "The paper is very well-written and well-organized and provides clear motivation, background, and some technical details for the proposed model, including model quantization and binarization.",
            "2": "Experiments are conducted on 5 datasets and provide meaningful comparisons with existing retrieval-enhanced language models such as Atlas and DensePhrase and large language models such as LLaMA2-7B.",
            "3": "The results show that the proposed model can maintain 95% of the above task performance while improving the inference speed and greatly reducing the storage space.",
            "4": "The paper also includes ablation studies to analyze the performance further.",
            "5": "As the paper mentions, BTR is difficult to apply to decoder-only models, and most current large language models utilize a decoder-only structure.",
            "6": "The paper only discusses FiD, which is the SOTA model in the KiLT ranking.",
            "7": "However, it is also important to discuss other commonly used retrieval-augmented structures, such as RAG, particularly when dealing with black-box LLM.",
            "8": "For retrieval augmented language models, the number of retrieved passages is an important factor for performance and efficiency.",
            "9": "While \nauthors only set this factor to a fixed number (40 or 30 for different datasets), thus additional experiments should be involved."
        },
        "RWNoVwdv9g": {
            "0": "Storage savings of 100x.",
            "1": "This will make retrieval more available to many, since storage can often become a bottleneck in retrieval-augmented ML.",
            "2": "Speed savings of 2-4x.",
            "3": "Although this is similar speed improvement as Deformer (Cao et al), it leads to 5 percent accuracy loss which is worse than Deformer's 1 percent accuracy loss.",
            "4": "Extensive experiments and analysis.",
            "5": "There are some confusing or missing details, but these can be probably be easily fixed.",
            "6": "There are many steps required to make BTR work well.",
            "7": "Of course, that is also why this is a valuable paper since it outlines what these steps are.",
            "8": "There is a substantial (about 5 percent) drop in model accuracy.",
            "9": "It's not clear whether 2-4x speed boost is enough value to make up for this, although the storage improvements are definitely very valuable.",
            "10": "The paper is not very self-contained.",
            "11": "It seems like the reader is expected to have read Cao et al and Atlas papers very closely.",
            "12": "This is not ideal.",
            "13": "For example, it is hard to understand what it means by \"decomposed model\" or \"decomposed reader\" in sec 3.2.",
            "14": "Also, the reader is left to infer that retrieval is done at the passage level, but passages are incorporated at the token-level.",
            "15": "(minor) The paper is hard to read.",
            "16": "In more than one instance, a method is used but is \"defined below\", so we must constantly revisit parts of the reading to get a full understanding.",
            "17": "(medium) \"BTR is more accurate and efficient than other baseline methods\" This claim can easily be interpreted as \"all other baselines\", which is not accurate.",
            "18": "There are baselines that are more accurate or more efficient.",
            "19": "This should probably be revised to be more accurate/specific.",
            "20": "Also, I am not sure why BTR base is constantly bolded in Sp column when it is not the best value.",
            "21": "There is not a good breakdown of efficiency between retrieval + inference.",
            "22": "Perhaps alternative methods akin to DistilBERT will give a substantial speed boost plus keep good performance.",
            "23": "(minor) Similarly, given the emphasis on storage it would have been helpful to see some basic baselines to improve storage.",
            "24": "Although I am not sure what else can easily achieve 100x savings without larger tradeoff in performance.",
            "25": "(very minor) It was confusing whether the token distillation is taken directly from Cao et al, or is something new."
        },
        "9ZlXTDc5zn": {
            "0": "* Importance and relevance of topic: LLMs are everywhere and retrieval augmentation LMs addresses critical problems in LLMs such as hallucination, staleness, and privacy leaks, but suffer from low inference speed and huge storage requirements.",
            "1": "* BTR has much lower storage footprint than other approaches, is more scalable and has lower inference speed at the expense of a \"modest\" loss in performance.",
            "2": "* The paper reports numbers for actual throughput for a more realistic comparison of efficiency across different systems.",
            "3": "* The code will be publicly available.",
            "4": "* \"BTR is more accurate and efficient than other baseline methods.",
            "5": "\": According to Figure 3, the proposed BTR appears to be on the Pareto front.",
            "6": "I.e., it isn't substantially better than existing methods but provides a different tradeoff between speed and accuracy.",
            "7": "This makes it hard to assess the merits of BTR.",
            "8": "To make the result less sensitive to a specific operating point and better comparability, it would be interesting to show, for example, how the tradeoff changes with the resolution of the representation (1-bit vs b-bit representations).",
            "9": "Furthermore, adding sub-optimal points to the plots would give a more comprehensive picture (for example, LLaMA at different size/speed/accuracy).",
            "10": "* The benefits at runtime come with a clearly more complicated training pipeline and increased training time.",
            "11": "How much?",
            "12": "* Figure 3: Why do the plots include Atlas-Q but not the original model (Atlas)?",
            "13": "Also, I can't see any small points for Atlas-Q and LLaMA2-7B.",
            "14": "* Different spelling in title and text: \"retrieval augmented language model\" vs \"retrieval-augmented language model\".",
            "15": "* Funsion-in-Decoder → Fusion-in-Decoder"
        },
        "LOpL1iFVYo": {
            "0": "The strengths are as follows:\n* Retrieval augment language models are becoming increasingly popular.",
            "1": "Increasing the inference speed and reducing the memory footprint of such methods will be quite useful.",
            "2": "The authors demonstrate that they can achieve a 4x speedup in inference speed and a 100x reduction in storage space.",
            "3": "* The authors present results across multiple datasets.",
            "4": "* The authors ablate all of their modifications and show how each one affects performance, speed and memory.",
            "5": "The weaknesses are as follows:\n* Some things like passage representation regularization are mentioned in passing and it would be helpful if the authors added a couple sentences providing context and explaining what this is.",
            "6": "* No motivation or explanation is given for why distillation is required/helpful.",
            "7": "* It is not clear if the linear projection layer for passage representation recovery is a learnable layer.",
            "8": "Also, the $L_{recovery}$ term is confusing; $b_i$ is the binary passage representation and $h_i$ is the original passage representation, where is the projection layer used?",
            "9": "* No comparison to newer methods like Lumen."
        }
    },
    "KzMMv0OygD": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the field of NLP, specifically the collection of high-quality instruction-tuning data for large language models (LLMs).",
            "1": "- The proposed method, TEG-INSTRUCT, introduces a novel approach by training LLMs to autonomously design tasks using human-written texts, which is a significant advancement over existing methods that either rely on manual annotation or suffer from hallucination issues.",
            "2": "- The inclusion of a task generator and a task discriminator to ensure the quality of the generated tasks is a novel contribution that enhances the reliability of the collected data.",
            "3": "Potential reasons for acceptance\n   - The paper presents a scalable and efficient solution for generating high-quality instruction-tuning data, which is a valuable contribution to the NLP community.",
            "4": "- The proposed method demonstrates significant improvements in reducing hallucinations and noise in the generated tasks, as evidenced by both automated and manual evaluations.",
            "5": "- The release of a large dataset (270k examples) and the detailed experimental results provide a strong foundation for future research and practical applications in instruction-tuning.",
            "6": "Potential reasons for rejection\n   - **Clarity and readability of the paper**\n     - The paper contains several sections with dense and complex explanations that may be difficult for readers to follow.",
            "7": "- Some parts of the methodology, particularly the post-processing steps and the training of the task discriminator, could benefit from clearer and more detailed descriptions.",
            "8": "- **Evaluation and comparison with existing methods**\n     - While the paper compares TEG-INSTRUCT with several existing datasets, the evaluation could be more comprehensive by including additional metrics or benchmarks.",
            "9": "- The comparison with other text-grounded methods, such as LONG FORM and HUMPBACK, could be expanded to provide a more thorough analysis of the differences and advantages of TEG-INSTRUCT.",
            "10": "- **Generalization and applicability**\n     - The paper focuses primarily on the in-domain and out-of-domain performance of the generated tasks, but it would be beneficial to include more discussion on the generalization capabilities of the proposed method across different types of texts and domains.",
            "11": "- The potential limitations and challenges of applying TEG-INSTRUCT in real-world scenarios, such as handling highly specialized or technical texts, are not sufficiently addressed.",
            "12": "Suggestions for improvement\n   - **Improve clarity and readability**\n     - Simplify and clarify the explanations of the methodology, particularly the post-processing steps and the training of the task discriminator.",
            "13": "- Include more visual aids, such as diagrams or flowcharts, to illustrate the key steps and components of the proposed method.",
            "14": "- **Expand evaluation and comparison**\n     - Include additional evaluation metrics, such as precision, recall, and F1-score, to provide a more comprehensive assessment of the quality of the generated tasks.",
            "15": "- Conduct a more detailed comparison with other text-grounded methods, highlighting the specific advantages and limitations of TEG-INSTRUCT.",
            "16": "- **Address generalization and applicability**\n     - Discuss the potential challenges and limitations of applying TEG-INSTRUCT to different types of texts and domains, and propose possible solutions or future research directions to address these issues.",
            "17": "- Include case studies or examples of how TEG-INSTRUCT can be applied in real-world scenarios, such as generating tasks for specialized technical documents or legal texts.",
            "18": "- **Enhance dataset release and documentation**\n     - Provide detailed documentation and guidelines for using the released dataset, including examples of how to integrate it with existing LLMs and instruction-tuning frameworks.",
            "19": "- Consider releasing additional tools or scripts to facilitate the use and evaluation of the dataset, such as pre-processing scripts or evaluation metrics."
        },
        "DFn5hpKkwq": {
            "0": "- The proposed method is scalable and the released dataset is large-scale.",
            "1": "- The paper is generally well-written and organized.",
            "2": "Almost everything is clear to me.",
            "3": "- Thorough automatic and human evaluations over a range of instruction-tuning datasets on various downstream tasks.",
            "4": "The analysis for the curated *Teg-Instruct* is also comprehensive.",
            "5": "- I'm not so convinced by the experiment results.",
            "6": "It is not surprising that *Teg-Instruct* achieves the best performance for in-domain tests.",
            "7": "For out-of-domain experiment results, the performance is not that satisfactory.",
            "8": "It.",
            "9": "is understandable for different output distributions.",
            "10": "Instead, what is the performance on some general benchmarks aside from the in-domain and out-domain evaluations, such as LongForm and Alpaca Eval?",
            "11": "- Not a strict weakness, just wondering why it is called \"text-grounded task design\"?",
            "12": "I'm a little confused by the term \"task design\".",
            "13": "In my view, \"task\" usually refers to well-defined task formats, such as QA, IE, etc."
        },
        "J8IzE2Eer6": {
            "0": "- This paper proposes a text-grounded instruction-tuning dataset with up to 270k examples, and the method is scalable.",
            "1": "- The method trains a discriminator to filter out low-quality data, which can alleviate the problem of hallucination.",
            "2": "- The motivation of this paper is intuitive.",
            "3": "- The contribution of this paper is a little bit limited, and the differences compared to [1] and [2] are not very distinct.",
            "4": "- There are some concerns regarding the evaluation of the proposed method and dataset.",
            "5": "In terms of automated evaluation, using the test set generated in this paper, TEG-Instruct, as the testing dataset might be somewhat unfair.",
            "6": "Additionally, including evaluation results on more benchmarks can enhance the robustness of the article.",
            "7": "[1] \"Self-Alignment with Instruction Backtranslation\".",
            "8": "Li et al.",
            "9": "[2] \"Longform: Optimizing instruction tuning for long text generation with corpus extraction.\"",
            "10": "Köksal et al."
        },
        "HF4KaKw9R3": {
            "0": "+ The paper provides extensive and thorough experimentation on the data quality.",
            "1": "Particularly, The human evaluation on the intrinsic quality of the instruction tuning datasets is valuable.",
            "2": "+ The instruction tuning datasets will be released and will be valuable to the community.",
            "3": "- The improvement from the dataset seems incremental compared with previous datasets and methods especially in terms of downstream evaluation with out-of-domain data.",
            "4": "- The data creation process is complicated with many rules not clearly specified."
        },
        "BI0UVD8N77": {
            "0": "The introduced method can generate more Instruction tuning samples at a lower cost.",
            "1": "The paper states in Figure 1 that human-written text contains noise.",
            "2": "However, this is a rare case.",
            "3": "If the authors think this is a common problem, then all the human-annotated tasks are pointless.",
            "4": "The author uses examples generated by ChatGPT as training samples to train a Llama-2-7B model to generate new training samples.",
            "5": "I think the training samples generated in this case will have more noise.",
            "6": "The paper states in Figure 1 that the lack of input field is detrimental to LLM’s generalization capability in diversity tasks.",
            "7": "There is no verification of it, no explanation in other papers, and no ablation experiments.",
            "8": "This method has almost no improvement in out-of-domain datasets compared to other baselines, and the in-domain experiment is almost meaningless because other baselines are all in the setting of out-of-domain when dealing with the dataset proposed in this paper.",
            "9": "This is an unfair comparison.",
            "10": "In addition, the article lacks the number of training data for other baselines."
        }
    },
    "rhp5PDNOgf": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a novel adaptive scheduling strategy called Spaced Scheduling Training (SST) for instruction-tuning large language models (LLMs).",
            "1": "- The approach is inspired by the spaced repetition learning method used by humans, aiming to optimize the curriculum of training examples dynamically during training.",
            "2": "- The proposed method addresses the issue of performance degradation in instruction-tuned LLMs by reducing the need for extensive studies over different compositions of training data.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and innovative approach to improving the performance of instruction-tuned LLMs, which is a significant contribution to the field.",
            "4": "- The empirical results demonstrate that SST yields better performance than random sampling and other pruning and scheduling baselines, using less training data and minimizing catastrophic forgetting.",
            "5": "- The approach is well-motivated by cognitive science principles and provides a clear and detailed explanation of the methodology.",
            "6": "- The paper includes extensive empirical evaluation and careful statistical analysis, which strengthens the validity of the findings.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in some sections**\n     - The explanation of the SST algorithm and its components could be more concise and clearer, as some parts are overly detailed and may confuse readers.",
            "8": "- The description of the experimental setup and baselines could be more structured to improve readability and understanding.",
            "9": "- **Limited scope of evaluation**\n     - The experiments are conducted only on LLaMA-2 models with up to 13B parameters, which may not generalize to larger models or other architectures.",
            "10": "- The evaluation focuses primarily on reasoning tasks, and it would be beneficial to see results on a broader range of tasks and datasets.",
            "11": "- **Compute overhead concerns**\n     - The paper acknowledges the potential compute overhead induced by the evaluation process before each learning epoch, but it does not provide a detailed analysis of the trade-offs involved.",
            "12": "- The efficiency of SST in scenarios where all training data is useful is not thoroughly explored, which could limit the applicability of the approach.",
            "13": "Suggestions for improvement\n   - **Improve clarity and conciseness**\n     - Simplify the explanation of the SST algorithm and its components, focusing on the key steps and their motivations.",
            "14": "- Structure the experimental setup and baselines section more clearly, providing a concise summary of each baseline and its relevance to the study.",
            "15": "- **Expand the scope of evaluation**\n     - Conduct experiments on larger models (e.g., LLaMA-2 70B) and other architectures to assess the generalizability of the proposed approach.",
            "16": "- Include a wider range of tasks and datasets in the evaluation to demonstrate the versatility and robustness of SST.",
            "17": "- **Analyze compute overhead trade-offs**\n     - Provide a detailed analysis of the compute overhead induced by the evaluation process and the conditions under which SST remains efficient.",
            "18": "- Explore scenarios where all training data is useful and discuss the potential limitations and adaptations of SST in such cases.",
            "19": "- **Incorporate additional evaluation signals**\n     - Consider including evaluation signals in the dropping mechanism to address the issue of negative impact from some datasets and improve the overall effectiveness of SST."
        },
        "QhelRsXvHS": {
            "0": "Motivating the technique with relevant psychology literature.",
            "1": "Applying the technique to instruction tuning LLMs, which is a research\n  topic that is attracting considerable attention.",
            "2": "Conducting an ablation analysis on the components of the proposed algorithm.",
            "3": "* Discussion of related work.",
            "4": "For example, spaced scheduling for deep\n  learning has been considered in **Hadi Amiri et.",
            "5": "al**:\n  *Repeat before Forgetting: Spaced Repetition for Efficient and Effective\n  Training of Neural Networks (ACL 2017, see page 2404)*\n\n* The proposed method does not appear to be motivated with a memory model; compare\n  **Amiri et al.",
            "6": "** or **https://arxiv.org/pdf/1602.07032.pdf**, both works seem to motivate\n  their proposals based on a memory model.",
            "7": "* In my opinion, the empirical part should have at least a comparison to\n  another spaced scheduling method (compare **Amiri et al.**).",
            "8": "* In my opinion, it is hard to conclude if one should use the proposed method\n  or some other online scheduling approach.",
            "9": "For example, there is prior\n  relevant work on automated curriculum learning, see for example **Kreutzer et.",
            "10": "al**: *Bandits Don’t Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits (ACL 2021)*.",
            "11": "While I am **not necessarily** advocating direct comparison\n  to the algorithm of **Kreutzer et al.",
            "12": "**, I think the empirical part would be\n  more solid by having a comparison to one or two additional approaches\n  that schedule the data dynamically."
        },
        "IGUKQw8dfa": {
            "0": "- The introduction seems interesting.",
            "1": "The analogy drawn to human learning processes is quite inspiring.",
            "2": "- The author performed an ablation study to show the benefit of each main component of the proposed algorithm.",
            "3": "- Some parts of the writing are ambiguous.",
            "4": "I think it is better to provide a representative concept figure.",
            "5": "- The experimental results are not so good.",
            "6": "Stating that there is an overall performance improvement seems risky because there was a performance gain in four out of seven evaluations.",
            "7": "- While there are numerous hyperparameters within the proposed algorithm, the impact of their variations on performance has not been analyzed.",
            "8": "Some ablation studies on hyperparameters, e.g., $s_t$ and $\\rho_0$, seem necessary.",
            "9": "This is necessary to determine whether this method is insensitive to hyperparameters.",
            "10": "- There are often instances where explanations are missing.",
            "11": "For instance, it would be advisable for the authors to explain why the formula for the minimum score threshold based on competency is $z_t \\leftarrow z_{max} - \\kappa -1$."
        },
        "nrQrogXnoS": {
            "0": "- The paper is well-written and clearly presented; \n- The paper tackled the important problem of data mixture & curriculum  learning of instruction tuning for large language model and proposed a novel method, \"space scheduling\", quantitive results on 4 benchmark suite show the effectiveness of the proposed methods versus baseline or random mixture methods; \n- Detailed ablation and qualitative examples w.r.t other scheduling variants have been presented to show the effectiveness of the proposed \n- It is great to show the proposed methods are based on LoRA, which offers extra accessibility to large research community; - There is a missing comparison in Table 1 versus Tulu as mentioned in 4.2 for the effectiveness of MERCURY versus original Tulu paper.",
            "1": "Besides, MT-Bench or other human-involved evaluations might also be good to show the comprehensive effectiveness of the proposed methods; \n- Another concern of the proposed method is that it seems the benefits are enlarged for Math/Code with both MERCURY / Space Scheduling.",
            "2": "However, when adding OpenOrca dataset only will contribute to that effect should be ablated; \n- The scalability of the proposed method is questionable but it is great to show the purposed methods are based on LoRA;"
        },
        "pf64plMlP5": {
            "0": "Spaced repetition consistently improves accuracy across all 5 benchmarks and 2 models, demonstrating robust gains.",
            "1": "The paper provides an ablation study for three components of the algorithm, and it seems that all 3 improve performance\n3.",
            "2": "The paper spends time clearly outlining their precise algorithm I overall would appreciate better contextualization/analysis of the method to prove that it's improving upon our current understanding/practice of fine-tuning.",
            "3": "Baselines: Though it is encouraging that this model improves over vanilla fine-tuning, this is not the only work in improving data quality/curriculum for fine-tuning.",
            "4": "In terms of static pruning methods, one naive baseline is to filter sentences with too high or low perplexity (similar to high/low difficulty), as done [for pretraining](https://arxiv.org/abs/2309.04564).",
            "5": "In terms of active learning methods, [Data diets](https://arxiv.org/abs/2306.03208) performs a very similar algorithm to the one in this work, dynamically pruning based on a notion of sample importance (the related work of this paper also provides other references).",
            "6": "I believe the paper should reflect this in two ways.",
            "7": "- The related work and contextualization of the current paper make it seem that this problem has not been studied before, which can be misleading.",
            "8": "Better contextualizing the work in terms of prior research in this area will help highlight the novel contributions by this paper.",
            "9": "- Though the results provide an ablation study, it is unclear where prior work lies in this spectrum.",
            "10": "Regardless of the motivation, this paper is providing a new method, and its important to contextualize its gains with respect to prior work.",
            "11": "Though there are too many baselines to evaluate all, I would appreciate seeing a reasonable baseline to confirm that along some axis, this work pushes along various fine-tuning tradeoffs.",
            "12": "Overhead: This algorithm should induce extra time overhead since examples have to be scored, and \"currently difficult\" examples may have to go through the model multiple times.",
            "13": "The authors should report the time taken by both algorithms to evaluate this slowdown so one can evaluate whether this accuracy improvement is worth the additional cost.",
            "14": "Connection to spaced repetition in psychology: From reading the paper, it is not clear to me how connected the algorithm is to spaced repetition learning for humans.",
            "15": "According to the paper, spaced repetition says that \"brains retain information more effectively when we learn in multiple, spread-out sessions\".",
            "16": "However, the actual algorithm proposed does not do this, and frames example selection under dynamic filtering based on example difficulty.",
            "17": "Even if the analysis in Section 5.3 implies that the method is implicitly setting a curriculum, it is not clear to me how this connects to spaced repetition.",
            "18": "I wonder what value the psychological motivation provides in the context of this work, and if there is a connection I'm missing in this regard.",
            "19": "I am happy to adjust my score provided some further analysis along these axes."
        }
    },
    "NddKiWtdUm": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the critical issue of social alignment in AI systems, which is essential for ensuring that language models behave according to societal values.",
            "1": "- It introduces a novel training paradigm that allows language models to learn from simulated social interactions, which is a significant departure from traditional training methods that rely on static datasets.",
            "2": "- The proposed approach, Stable Alignment, demonstrates superior performance in alignment benchmarks and human evaluations, indicating its potential impact on the development of socially aligned AI systems.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and scalable approach to training language models for social alignment, which is a significant contribution to the field.",
            "4": "- The experimental results show that Stable Alignment outperforms existing methods in various alignment benchmarks, demonstrating its effectiveness.",
            "5": "- The introduction of SANDBOX, an open-source platform for simulating human society, provides a valuable tool for future research in AI alignment and social simulation.",
            "6": "- The comprehensive evaluation, including human preference studies and benchmarking against multiple datasets, strengthens the validity of the proposed approach.",
            "7": "Potential reasons for rejection\n   - **Limited scope of social interactions:**\n     - The current implementation of Stable Alignment is confined to text-based interactions, which may not fully capture the complexity of human communication that includes non-verbal cues.",
            "8": "- The static view of societal norms in the simulation may not accurately reflect the dynamic and evolving nature of real-world societal values.",
            "9": "- **Language and cultural limitations:**\n     - The empirical analysis is conducted primarily in English, which limits the generalizability of the findings to other languages and cultural contexts.",
            "10": "- The model may predominantly reflect mainstream value judgments, potentially overlooking the perspectives of underrepresented social groups.",
            "11": "- **Potential ethical concerns:**\n     - The simulations using publicly available language models might not accurately represent the judgments of certain communities, raising concerns about bias and fairness.",
            "12": "- The temporal relevance of the social values derived from the simulations may not align with current societal norms, necessitating continuous updates and real-time information sources.",
            "13": "Suggestions for improvement\n   - **Expand the scope of social interactions:**\n     - Incorporate non-verbal cues and multimodal interactions in the simulation to better capture the complexity of human communication.",
            "14": "- Introduce mechanisms to dynamically update societal norms and values in the simulation to reflect their evolving nature.",
            "15": "- **Enhance language and cultural diversity:**\n     - Conduct experiments in multiple languages and cultural contexts to validate the generalizability of the findings.",
            "16": "- Train language models on data from diverse communities to ensure that the simulations accurately represent a wide range of perspectives.",
            "17": "- **Address ethical concerns:**\n     - Implement measures to mitigate bias and ensure fairness in the simulations, such as using diverse training data and involving stakeholders from underrepresented groups.",
            "18": "- Equip language model agents with access to real-time information sources to maintain the temporal relevance of the social values they learn.",
            "19": "- **Provide more detailed analysis:**\n     - Include a more detailed analysis of the limitations and potential biases of the proposed approach.",
            "20": "- Offer a thorough discussion on the ethical implications of using simulated social interactions for training language models."
        },
        "ESYYxy8Xlf": {
            "0": "The paper proposes a quite original approach to alignment, using social interactions between multiple LMs as a data-generating process for finetuning.",
            "1": "It’s ambitious and inspiring.",
            "2": "I think multi-agent experiments with LMs studying norm establishing, norm following and social learning are interesting on their own and the paper (specifically, the Sandbox environment) is a great contribution to this area.",
            "3": "The paper is well-written.",
            "4": "Despite complexity of the method, it’s relatively easy to understand.",
            "5": "The experiments are quite extensive and well-designed.",
            "6": "The results look promising: significant improvements can be seen on multiple relevant benchmarks.",
            "7": "I don’t think the authors have enough evidence to claim that “their approach is considerably more scalable” (from the abstract).",
            "8": "I’m a bit concerned that the method does not scale with model size (page 4) and I didn’t see experiments showing scaling wrt society size.",
            "9": "If it doesn’t scale, Stable Alignment is significantly less promising as an alignment method.",
            "10": "(It’s still interesting as an LLM capabilities analysis though.)",
            "11": "I’m not entirely convinced how important is the social aspect of the data-generating process.",
            "12": "It seems to be just an extension of [language model cascades](https://arxiv.org/abs/2207.10342) line of work, including [imitational learning from language feedback](https://arxiv.org/abs/2303.16755) (ILF) or [constitutional AI](https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback) (CAI), where LMs give feedback on LMs’ initial responses which are then refined.",
            "13": "The agents are weakly individuated in the proposes framework, i.e.",
            "14": "(as far as I understand) they start the same and and only differ in their conversation histories.",
            "15": "I wonder how important is treating multiple calls to the same LM as simulating different agents.",
            "16": "Does it improve alignment compared with a single-agent society, more similar to ILF or CAI?",
            "17": "Is there a scaling plot?",
            "18": "(I might’ve missed this, I’m happy to corrected by the authors.)",
            "19": "Relatedly, I think [language model cascades](https://arxiv.org/abs/2207.10342) line of work (including [imitational learning from language feedback](https://arxiv.org/abs/2303.16755) (ILF), [constitutional AI](https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback) (CAI), [critiques](https://arxiv.org/abs/2206.05802), [Reflexion](https://arxiv.org/abs/2303.11366), [STaR](https://arxiv.org/abs/2203.14465)) could be discussed briefly in Related work.",
            "20": "I’m also happy to be corrected if the authors think I’m wrong seeing their paper as a multi-agent extension of this line of work."
        },
        "of63zCaAER": {
            "0": "The proposed method is interesting and conceptually novel, especially the sandbox idea.",
            "1": "The experiment is extensive (although contains some issues that I am concerned with, which I will discuss later)\n\n3.",
            "2": "The writing is clear.",
            "3": "The main issue that I am concerned with is the relation between this work and RLAIF[1].",
            "4": "It is also trying to use AI feedbacks to align the model.",
            "5": "The main difference between the proposed method and RLAIF is that RLAIF still follows the paradigm of RLHF and only replace human feedback with AI feedback.",
            "6": "And the proposed method is building up a sandbox of multiple AI.",
            "7": "I am wondering how much difference in performance will be brought by this difference.",
            "8": "[1]Lee, Harrison, et al.",
            "9": "\"Rlaif: Scaling reinforcement learning from human feedback with ai feedback.\"",
            "10": "arXiv preprint arXiv:2309.00267 (2023)."
        },
        "RszNx5d8Re": {
            "0": "-The proposed alignment method is novel, and takes inspiration from recently published research.",
            "1": "The human evaluation results confirm experimental results on relevant benchmarks.",
            "2": "Relevant related work is cited.",
            "3": "The paper is well written, and graphics help with understanding.",
            "4": "-The proposed methodology is complicated compared to existing alternatives, but ablation shows that each component is critical in the effectiveness.",
            "5": "I would assume the proposed method generalizes to other (larger) LMs, but additional experimental results would be helpful.",
            "6": "-I might overlook this but an analysis of the results on chatgpt will be helpful to identify future improvements.",
            "7": "-Experiments can be extended to more human value relevant datasets such as mmlu and civil comments.",
            "8": "-See questions below."
        },
        "AyJOmWekX0": {
            "0": "The proposed Contrastive Preference Optimization is new, and is shown to be able to effectively train LMs to learn from both positive samples (well-aligned responses) and negative ones.",
            "1": "This hints at how data collection procedure in the future may not always need to focus on collecting high-quality, socially aligned data only, which can be costly and time-consuming.",
            "2": "The authors provided extensive analysis across six benchmark tasks with seven related baselines to show that Stable Alignment can generate/choose more socially aligned responses.",
            "3": "This is helpful especially because the pipeline is autonomous: it does not require human involvement for additional annotation.",
            "4": "The authors claim the proposed Stable Alignment addresses limitations from (e.g.)",
            "5": "RL from a reward model, which \"may be inherently imperfect and not fully capture nuances of human judgment\".",
            "6": "However, since the data collection process (i.e., the sandbox) mainly prompts LLMs to gather feedback and ratings, how are these not imperfect or can be guaranteed to fully capture nuances of human judgement?",
            "7": "There is a lack of analysis or discussion on the noises that could come from gathering data from LLMs, which is arguably critical for the proposed method to succeed and can be seen as limitation of how far this approach can go.",
            "8": "2. the sandbox data creation procedure appears to be an extension to prompt *multiple* LLM agents to provide rating and feedback for a single agent, instead of just prompting a single LLM teacher to gather data.",
            "9": "There is a lack of experiments showing the advantage of using \"multiple\" LLM agents, and whether if this is even necessary.",
            "10": "If not, many relevant work ([1]-[3]), which also did learning to generate feedback/critiques/improvement from a single teacher model, and is not discussed in this work.",
            "11": "This limits the novelty and usefulness of the proposed \"sandbox\" construction and the following supervised training procedure."
        },
        "D6fIS8mYzn": {
            "0": "The language in the paper is mostly clear and easy to read.",
            "1": "The general concept of being inspired by social interactions is interesting, and there is the possibility of being able to train an aligned model without the need to host an online reward model.",
            "2": "However, there are many details in the execution that cast doubt on the learning method’s efficacy.",
            "3": "Additionally, the claim of simulating human society with no supporting references from the social science literature is too grandiose.",
            "4": "There are extensive experiments on multiple datasets.",
            "5": "Human annotators were used for evaluating the final output rather than simply using a language model as a shortcut.",
            "6": "> However, this method often yields models susceptible to adversarial attacks, like “jailbreaking prompting” (Subhash, 2023; Xu et al., 2021), due to limited exposure to misaligned data during training (Amodei et al., 2016).",
            "7": "To address this, a more advanced technique, “reward modeling” has been proposed (Leike et al., 2018; Christiano et al., 2017).",
            "8": "Subhash, 2023 uses ChatGPT, a model that has likely been trained with a reward model, as an example of a vulnerable model and appears to contradict the second sentence stating that reward modeling addresses the vulnerabilities of SFT-trained models.",
            "9": "The claim of simulating human society with no supporting references from the social science literature is way too grandiose.",
            "10": "The authors should improve the accuracy of their claim.",
            "11": "It is somewhat unclear how the OpenAI models fit into the picture; are 99 of the 100 models all simply one of the 3 OpenAI models and one of them the Stable Alignment model?",
            "12": "If so, this seems like a form of distillation of the preferences/values from the OpenAI model, which was purportedly trained with SFT and RLHF.",
            "13": "Given that this method claims to do away with the need for RLHF/RMs, experiments showing its efficacy should not rely on models trained with such methods.",
            "14": "In other words, the current experiments do not convince me that Stable Alignment can replace RLHF on its own.",
            "15": "For example, I would be much more convinced if all the agents were initialized from non-RM based models.",
            "16": "Relatedly, while convenient, using the OpenAI API models for critical parts of the experiments makes them essentially non-replicable and rather non-reproducible, given the constantly changing model in the backend.",
            "17": "An additional nitpick is that the method claims to deviate from SFT-like approaches but uses an SFT-trained model as the starting point.",
            "18": "I would like to see this addressed as well; what is the performance of Stable Alignment without using an SFT-trained model as the initialization?",
            "19": "Regarding CPO, since it is a new technique that claims to improve the efficacy of the overall method, there should be an ablation where it is replaced with the regular SFT loss or other alignment algorithms, such as perhaps DPO.",
            "20": "More details on the procedure and exactly which models and types of social agents were used to generate data for each learning stage are needed; this is still unclear to me after reading through the details and appendices multiple times.",
            "21": "Are the numbers of examples for each stage in Fig 2 the total over all iterations, or per iteration?",
            "22": "How many iterations were needed to arrive at the final model?",
            "23": "Was the model trained on data from all three societies at each stage or were 3 separate models trained?",
            "24": "If the latter, which model was used in the final evaluation?",
            "25": "Use of HH-A as an estimate of adversarial prompt robustness:\nI went through the examples in the HHH dataset (there are typos where the last H is missing throughout the paper) and disagree with the use of the appended “misaligned” response as a dataset good enough for adversarial robustness evaluation.",
            "26": "A better evaluation of adversarial prompt robustness would be to use a library such as [garak](https://github.com/leondz/garak) with known adversarial prompts used by the community."
        }
    },
    "ygxTuVz9eU": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in data-centric AI by proposing a versatile data cleanser (VDC) that detects various types of dirty samples, including poisoned samples and noisy labels.",
            "1": "- The novelty lies in leveraging multimodal large language models (MLLM) to capture visual-linguistic inconsistencies, which is a unique approach compared to existing methods that focus on specific types of dirty samples.",
            "2": "Potential reasons for acceptance\n   - The proposed VDC framework demonstrates superior performance in detecting both poisoned samples and noisy labels across multiple datasets, as evidenced by extensive experimental results.",
            "3": "- The approach is generalizable and robust, showing consistent effectiveness across different types of dirty samples, datasets, and poisoning ratios.",
            "4": "- The use of MLLM for cross-modal alignment and reasoning is innovative and leverages the latest advancements in AI, making the method state-of-the-art.",
            "5": "- The paper provides a comprehensive evaluation, including ablation studies and comparisons with multiple baselines, which strengthens the validity of the proposed method.",
            "6": "Potential reasons for rejection\n   - **Dependence on LLM and MLLM:**\n     - The performance of VDC heavily relies on the capabilities of LLM and MLLM, which may not be accessible or feasible for all researchers due to computational and resource constraints.",
            "7": "- The accuracy of VDC is contingent on the correctness of responses from LLM and MLLM, which can still yield incorrect replies, potentially affecting the detection results.",
            "8": "- **Limited scope of detection:**\n     - VDC is designed to detect dirty samples with corrupted labels, making it unsuitable for detecting clean-label backdoor attacks or other types of data corruption that do not involve label inconsistencies.",
            "9": "- The framework may not perform well in scenarios where the visual content and labels are inherently ambiguous or subjective, leading to potential false positives or negatives.",
            "10": "- **Computational complexity:**\n     - The inference process involving LLM and MLLM can be computationally expensive, especially for large datasets with numerous classes and samples, which may limit the practical applicability of VDC in real-world scenarios.",
            "11": "- The time complexity of the method, although detailed, may still pose challenges for large-scale deployment without further optimization or advancements in LLM efficiency.",
            "12": "Suggestions for improvement\n   - **Enhance accessibility and efficiency:**\n     - Explore ways to optimize the inference process of LLM and MLLM, such as using lightweight models or quantization techniques, to reduce computational costs and make the method more accessible.",
            "13": "- Consider providing a detailed analysis of the computational requirements and potential optimizations to help researchers and practitioners better understand the feasibility of implementing VDC.",
            "14": "- **Expand detection capabilities:**\n     - Investigate the potential of extending VDC to detect other types of data corruption, such as clean-label backdoor attacks or subtle data manipulations that do not involve label inconsistencies.",
            "15": "- Incorporate additional modules or techniques that can handle ambiguous or subjective visual content and labels, improving the robustness of the framework in diverse scenarios.",
            "16": "- **Improve evaluation and validation:**\n     - Conduct further experiments on a wider range of datasets, including those with more complex and varied visual content, to validate the generalizability and robustness of VDC.",
            "17": "- Provide more detailed case studies or qualitative analyses of the detected dirty samples to illustrate the practical impact and effectiveness of the proposed method in real-world applications."
        },
        "oa4JbDyZ22": {
            "0": "- The proposed method leverages the recent trend of MLLM to the label error detection literature\n- The proposed method is training-free - The empirical comparison is not fair.",
            "1": "- The propose approach is using instruct-BLIP (larger network trained on larger dataset), while the baseline is usually using less-expressive network trained on smaller datasets, e.g., CL is using ResNet-18 and trained on CIFAR dataset.",
            "2": "- The claim that the proposed method mitigates all three types of label noises is too strong.",
            "3": "- The proposed approach is general, but so do other baselines.",
            "4": "For example, SimiFeat-V leverages the feature similarity to detect noisy labels, which is also applicable to the scenario of “poisoned samples” as long as the feature extractor is trained on a poison-free dataset."
        },
        "LpFKkIcfAp": {
            "0": "+ The paper's approach is innovative, using popular multimodal large models to replace manual data cleaning work.",
            "1": "+ The paper unified the treatment of noisy data and poisoned data from backdoor attacks, which is relatively rare in previous research.",
            "2": "+ The paper's extensive experiments demonstrate the effectiveness of VDC in handling dirty data.",
            "3": "+ The overall content of the paper seems to be more about using out-of-distribution methods to filter data, and perhaps this should be reflected in the accuracy of sample selection.",
            "4": "+ While the paper effectively addresses the issue of dirty data using MLLM, it seems to have a bias towards reporting the application of MLLM."
        },
        "VV87JWnLKE": {
            "0": "Noisy or dirty data detection and cleaning is an important research topic.",
            "1": "It is becoming even more critical for recent machine learning research since the data scale is always getting larger.",
            "2": "The proposed framework wisely utilize the advantage of current large-scale model to benefit the dirty data detection task.",
            "3": "Comprehensive empirical results show the framework superiority compared with other baselines.",
            "4": "The whole draft is in a good format for readers.",
            "5": "I mainly concern about the technical contribution in this draft.",
            "6": "The wise combination of prompting and dirty data detection is interesting.",
            "7": "However, it still based on the visual-language understanding from large-scale pretrained model.",
            "8": "Only based on such powerful tools relatively diminish this paper novelty.",
            "9": "In addition, the key point of this paper is proposing an integrated detection pipeline instead of only focusing on sample or label.",
            "10": "This point looks like a trivial combination which is incremental compared with previous settings.",
            "11": "Is this setting practical and necessary for real-world scenarios?",
            "12": "Adding more recent published works for comparison may further help to support the paper contribution.",
            "13": "Currently, only one or two baselines are published within past one year."
        }
    },
    "farT6XXntP": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant gap in the performance of large language models (LLMs) in machine translation, particularly for models with moderate sizes (7B or 13B parameters).",
            "1": "- It introduces a novel two-stage fine-tuning approach that leverages monolingual data followed by high-quality parallel data, which is a departure from traditional methods that rely heavily on large amounts of parallel data.",
            "2": "- The proposed method, ALMA, demonstrates substantial improvements in translation performance, surpassing state-of-the-art models like NLLB-54B and GPT-3.5-text-davinci-003.",
            "3": "Potential reasons for acceptance\n   - The paper presents a clear and well-motivated problem statement, addressing the limitations of current LLMs in translation tasks.",
            "4": "- The proposed two-stage fine-tuning approach is innovative and effectively improves translation performance without the need for extensive parallel data.",
            "5": "- The experimental results are robust, showing significant improvements in BLEU and COMET scores across multiple language pairs and test datasets.",
            "6": "- The paper provides a comprehensive analysis of the impact of monolingual data and parallel data quality on translation performance, adding depth to the findings.",
            "7": "- The release of code and models enhances the reproducibility and practical impact of the research.",
            "8": "Potential reasons for rejection\n   - **Limited evaluation on low-resource languages:**\n     - The paper primarily focuses on high-resource language pairs, with limited evaluation on truly low-resource languages.",
            "9": "- The effectiveness of the proposed method on low-resource languages remains unclear, which could be a critical aspect for broader applicability.",
            "10": "- **Comparative analysis with other fine-tuning methods:**\n     - The paper does not provide a detailed comparative analysis with other recent fine-tuning methods beyond the ones mentioned.",
            "11": "- Including a broader range of baseline methods could strengthen the claims of superiority.",
            "12": "- **Scalability and computational cost:**\n     - While the paper mentions the computational efficiency of the proposed method, a more detailed analysis of scalability and cost, especially for larger models, would be beneficial.",
            "13": "- The practical feasibility of applying this method to even larger models or more diverse datasets is not thoroughly discussed.",
            "14": "- **Generalization to other NLP tasks:**\n     - The paper focuses solely on translation tasks, and it is unclear how well the proposed fine-tuning approach generalizes to other NLP tasks.",
            "15": "- Including experiments or discussions on the applicability of the method to other tasks could enhance the paper's impact.",
            "16": "Suggestions for improvement\n   - **Expand evaluation to low-resource languages:**\n     - Include additional experiments on low-resource language pairs to demonstrate the effectiveness of the proposed method in more challenging scenarios.",
            "17": "- Provide a detailed analysis of the performance on these languages to highlight any potential limitations or necessary adjustments.",
            "18": "- **Broaden comparative analysis:**\n     - Compare the proposed method with a wider range of recent fine-tuning techniques to provide a more comprehensive evaluation of its relative performance.",
            "19": "- Include ablation studies that isolate the impact of each component of the fine-tuning approach.",
            "20": "- **Detailed scalability and cost analysis:**\n     - Provide a more detailed analysis of the computational cost and scalability of the proposed method, especially for larger models and more extensive datasets.",
            "21": "- Discuss the practical implications of applying this method in real-world scenarios, including potential limitations and solutions.",
            "22": "- **Generalization to other NLP tasks:**\n     - Conduct experiments to evaluate the applicability of the proposed fine-tuning approach to other NLP tasks beyond translation.",
            "23": "- Include discussions on how the method could be adapted or extended to benefit other areas of NLP, providing a broader context for its impact."
        },
        "VezMUEs1o4": {
            "0": "The results of the paper indicated that smaller models could achieve SOTA translation levels through specialized fine-tuning, suggesting that there might not be a continuous need to expand datasets and models for better performance.",
            "1": "Through compact, specialized fine-tuning, smaller LLMs could achieve SOTA translation quality without billions of parameters.",
            "2": "The focus of this research was on tailored fine-tuning methods that unleashed the potential of LLM's multilingual capabilities on a broader scale.",
            "3": "The paper demonstrated that instead of increasing data scale, intentional fine-tuning targeting key language capabilities might be the key to maximizing LLM performance.",
            "4": "By revealing the potential of smaller LLMs for efficient and accurate machine translation, this work laid the foundation for developing more user-friendly and scalable machine translation systems.",
            "5": "This training approach offered more possibilities for deploying capable multilingual LLMs in real-world applications.",
            "6": "There were certain flaws in the method, and prompts affected the results.",
            "7": "The evaluation methods had its limitations.",
            "8": "The stability of the proposed method was not verified."
        },
        "RzDrUF628F": {
            "0": "They show large improvements in the translation capabilities of the most useful size of models (7B,13B) with very affordable limited fine-tuning and data.",
            "1": "This is a useful paper for people working in machine translation to see what works in fine-tuning large language models for the translation task.",
            "2": "There is not a lot of novelty in the approach - either in training or modelling.",
            "3": "I am not sure that the \"New paradigm\" title is justified.",
            "4": "I have not learned much from reading the paper - it is still not clear what the contribution of the monolingual vs parallel training data is.",
            "5": "It is also not clear whether the good performance of the trained models is due to the reduced number of non English languages (5) vs other models (NLLB, GPT3.5,4).",
            "6": "I am also not sure these results (improvement over LLaMa7B with fine-tuning) would hold if you used few-shot - and it would have been a very easy experiment to conduct.",
            "7": "The paper writing is not particularly clear (see questions for details)."
        },
        "XozJtG57x1": {
            "0": "1) Propose a simple training recipe for LLMs for translation tasks: finetuning first on monolingual data and then on small high-quality parallel data.",
            "1": "2) Demonstrate impressive performance across 5 language pairs with LLAMA-2.",
            "2": "1) The statement of \"paradigm shift\" is somehow overestimated.",
            "3": "2) The few-shot prompting results are highly undervalued.",
            "4": "3) The proposed recipe might not apply to other LLMs and languages."
        },
        "Ls28hhc4xO": {
            "0": "The paper is clearly written and provides many insights.",
            "1": "Using LLM to boost the translation quality is an interesting and important topic.",
            "2": "It proposes a novel fine-tuning paradigm to let the moderate size LLMs better at translation.",
            "3": "Many analyses should be very helpful to the NLP and ML community.",
            "4": "The paper is mainly focusing on improve the translation quality of LLMs.",
            "5": "It'd be better to compare more with the encoder-decoder translation models and shed light on the best practice of translation itself."
        }
    },
    "oWENFj7583": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant challenge in the field of Synthetic Aperture Radar (SAR) imagery by proposing a novel method to synthesize SAR data using large-scale pre-trained image generation models.",
            "1": "- The introduction of the 2-stage low-rank adaptation method (2LoRA) and its improved version (pLoRA) to adapt models from regular images to SAR images is a novel contribution.",
            "2": "- The work has potential applications in marine safety, environmental protection, and climate studies, making it highly relevant and impactful.",
            "3": "Potential reasons for acceptance\n   - The proposed method effectively addresses the data scarcity issue in SAR imagery by leveraging pre-trained models to generate high-quality synthetic data.",
            "4": "- The introduction of pLoRA to handle class imbalance in SAR datasets is a significant improvement over existing methods.",
            "5": "- The experimental results demonstrate notable improvements in SAR recognition performance, particularly for minor classes, validating the effectiveness of the proposed approach.",
            "6": "- The paper provides a comprehensive evaluation of the proposed methods, including both quantitative and qualitative analyses.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability:**\n     - The paper contains several complex concepts and technical details that may be difficult for readers to follow without additional explanations or visual aids.",
            "8": "- Some sections, particularly the methodology, could benefit from clearer structuring and more concise descriptions to improve readability.",
            "9": "- **Evaluation and comparison:**\n     - The paper primarily focuses on the FU-SRS dataset for evaluation.",
            "10": "It would be beneficial to include additional datasets or benchmarks to demonstrate the generalizability of the proposed methods.",
            "11": "- The comparison with other state-of-the-art methods is limited.",
            "12": "Including more baseline methods and a detailed discussion of their performance relative to the proposed methods would strengthen the evaluation.",
            "13": "- **Implementation details:**\n     - The paper lacks detailed information on the implementation of the proposed methods, such as hyperparameters, training procedures, and computational resources used.",
            "14": "Providing these details would enhance reproducibility.",
            "15": "- The process of generating text prompts using GPT-4 is mentioned but not thoroughly explained.",
            "16": "A more detailed description of this process and its impact on the results would be helpful.",
            "17": "Suggestions for improvement\n   - **Improve clarity and readability:**\n     - Simplify the language and structure of the methodology section to make it more accessible to a broader audience.",
            "18": "- Include more visual aids, such as diagrams and flowcharts, to illustrate the key concepts and steps of the proposed methods.",
            "19": "- **Expand evaluation and comparison:**\n     - Evaluate the proposed methods on additional SAR datasets or other non-visible light domains to demonstrate their generalizability and robustness.",
            "20": "- Include more baseline methods in the comparison and provide a detailed discussion of their performance relative to the proposed methods.",
            "21": "- **Provide detailed implementation information:**\n     - Include comprehensive details on the implementation of the proposed methods, such as hyperparameters, training procedures, and computational resources used.",
            "22": "- Elaborate on the process of generating text prompts using GPT-4 and discuss its impact on the results.",
            "23": "- **Address potential limitations:**\n     - Discuss potential limitations of the proposed methods, such as scalability to larger datasets or other non-visible light domains, and suggest directions for future research to address these limitations."
        },
        "CC75OFaXA3": {
            "0": "The paper addresses an important problem in the context of using deep learning in remote sensing, which is the scarcity of annotated data, especially for certain acquisition modalities like SAR.",
            "1": "Using synthetically generated images can be a sensible way of performing data augmentation, that has already been shown effective in other domains.",
            "2": "The overall novelty of work is limited.",
            "3": "The methodology is based on well-known concepts such as LoRA and ControlNet.",
            "4": "The two-stage approach to adaptation, while important for problem, is minor and of interest only for a niche audience.",
            "5": "Overall, the clarity of the paper can be improved.",
            "6": "For example, the presentation of the method is often intertwined with experimental details.",
            "7": "Experimental evaluation is also problematic.",
            "8": "The FID metric is not suitable for SAR images, since it is a distance in the latent space of an ImageNet-trained neural network, thus having a large domain gap (some works point out that the ImageNet prior affects evaluation even of RGB images, nevermind SAR, see Kynkäänniemi et al.",
            "9": "\"The Role of ImageNet Classes in Fréchet Inception Distance\").",
            "10": "The F1 score of the downstream classification task is a relevant metric but the results are difficult to understand.",
            "11": "For instance, Table 2 does not report the F1 performance of the baseline without augmentations."
        },
        "InXRMdfcrX": {
            "0": "The strengths of this research lie in its innovative approach to addressing the challenges of adapting semantic knowledge from regular imagery to synthetic aperture radar (SAR) imagery.",
            "1": "The 2LoRA method and its enhanced version, pLoRA, offer a novel solution to overcome class imbalance issues in SAR datasets, particularly for minor classes.",
            "2": "The paper's pioneering use of large-scale pre-trained generation models for synthesizing non-visible light images is a notable technical contribution, allowing the transfer of pre-learned semantic knowledge from regular images to SAR data despite significant domain gaps.",
            "3": "A comprehensive comparison is essential to substantiate the efficacy of the proposed approach.",
            "4": "Additionally, the authors' reliance on a custom dataset limits the ability to assess the method's effectiveness without validation on publicly available datasets.",
            "5": "Furthermore, there are concerns regarding stability, including potential dependencies in prompt construction and the generation of detailed visual descriptions by GPT-4."
        },
        "ps88JVnIHT": {
            "0": "Data synthesis is important and valuable for non-visible light data, which is hard to collect large-scale dataset.",
            "1": "The proposed methods do not show significant performance improvement when using the full traning dataset.",
            "2": "In practical, we always will employ all data to train a model rather than only employ 10% data.",
            "3": "However, when employing the full training data, the resample strategy is simple and comparable with the proposed method.",
            "4": "The pLoRA is not effective.",
            "5": "According to the experiments, pLoRA performs worse than 2LoRA in most cases."
        },
        "ugWtjB1yrH": {
            "0": "The paper is well written and the background sound.",
            "1": "A large amount of content has been presented in a short number of pages very well.",
            "2": "I find no plagiarism either.",
            "3": "The paper may not be easily accessible to non-experts as the paper is written very high-level.",
            "4": "Due to space limitations, however, this is ok."
        }
    },
    "tcFcKyJgRM": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant challenge in teaching large language models (LLMs) to perform tasks on the web, which involves combinatorially large open-world tasks and variations across web interfaces.",
            "1": "- The proposed framework, Hierarchical Policies for Web Actions using LLMs (HeaP), introduces a novel approach by leveraging LLMs to decompose web tasks into modular sub-tasks, each solved by a low-level, closed-loop policy.",
            "2": "- The framework's ability to generalize across tasks and interfaces from few-shot demonstrations is a notable advancement over prior works that require large amounts of training data.",
            "3": "Potential reasons for acceptance\n   - The hierarchical approach of HeaP is innovative and addresses the complexity of web tasks by breaking them down into manageable sub-tasks.",
            "4": "- The experimental results demonstrate that HeaP outperforms prior works with significantly fewer training examples, showcasing its efficiency and effectiveness.",
            "5": "- The paper provides a comprehensive evaluation across multiple benchmarks, including MiniWoB++, WebArena, a mock airline CRM, and live website interactions, which strengthens the validity of the proposed method.",
            "6": "- The authors plan to open-source the code, simulator, and data, which will benefit the research community and facilitate further advancements in this area.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper could benefit from a more detailed explanation of the hierarchical task planner and the specific low-level policies used.",
            "8": "- The process of auto-labeling demonstrations and generating prompts could be elaborated further to enhance reproducibility.",
            "9": "- **Limited discussion on limitations and future work:**\n     - The paper briefly mentions limitations related to complex webpages and error recovery but does not provide in-depth analysis or potential solutions.",
            "10": "- There is a lack of discussion on how the framework could be extended to handle visual-only components or multi-modal tasks.",
            "11": "- **Insufficient comparison with state-of-the-art models:**\n     - While the paper compares HeaP with several baselines, it would be beneficial to include more recent state-of-the-art models and provide a more detailed analysis of the differences in performance.",
            "12": "- The comparison with models that use reinforcement learning or fine-tuning approaches could be expanded to highlight the advantages and disadvantages of each method.",
            "13": "Suggestions for improvement\n   - **Enhance methodological clarity:**\n     - Provide a more detailed description of the hierarchical task planner, including specific examples of high-level tasks and their corresponding low-level policies.",
            "14": "- Elaborate on the process of auto-labeling demonstrations and generating prompts, possibly with visual aids or flowcharts to illustrate the steps involved.",
            "15": "- **Expand discussion on limitations and future work:**\n     - Include a more thorough analysis of the limitations of HeaP, particularly in handling complex webpages and error recovery.",
            "16": "- Discuss potential extensions of the framework to incorporate visual information and multi-modal tasks, and outline future research directions to address these challenges.",
            "17": "- **Improve comparison with state-of-the-art models:**\n     - Include additional comparisons with recent state-of-the-art models, particularly those that use reinforcement learning or fine-tuning approaches.",
            "18": "- Provide a more detailed analysis of the performance differences between HeaP and these models, highlighting the strengths and weaknesses of each approach.",
            "19": "- **Provide more qualitative examples:**\n     - Include more qualitative examples of HeaP's performance on various web tasks, with detailed explanations of the reasoning and actions taken by the model.",
            "20": "- Use visual aids, such as screenshots or diagrams, to illustrate the model's interactions with web interfaces and the hierarchical decomposition of tasks."
        },
        "xmk9BvbTI7": {
            "0": "The design of this method is sound and reasonable.",
            "1": "Exhaustive details of the prompt and results analysis are presented.",
            "2": "The evaluation is based on too few samples: 45 tasks on MIniWob++, 125 examples of two domains on WebArena, 5 distinct tasks with 20 scenarios on Ariline CRM, and 3 website with 10 searches per site on Live Websites.",
            "3": "Given the fact that human demonstration is collected to form the prompts to the LLM in HEAP, it should be actually evaluated on more diverse websites instead of fewer websites."
        },
        "xEIR7Ij9Wt": {
            "0": "The approach introduces a novel hierarchical approach to prompt LLMs to perform web tasks.",
            "1": "Experimental results on various complex web benchmarking datasets show the superiority of the proposed approach.",
            "2": "I recommend moving some implementation details like prompts into the main body to help the reader better understand the work."
        },
        "fgGmSXNhF6": {
            "0": "- The paper is overall easy to read, although some important methodological details like autolabeling and prompt construction are in appendix which makes it hard to read.",
            "1": "- The experiments are extensive over 4 datasets with many tasks.",
            "2": "The gain demonstrated is substantial.",
            "3": "- The idea of hierarchical planning with a high-level planner and low-level policies using LLM has been explored by many previous robotics works e.g.",
            "4": "LLM-planner (https://arxiv.org/pdf/2212.04088.pdf and the line of works they cited).",
            "5": "Additionally, PaP (https://aclanthology.org/2022.suki-1.8.pdf) and Parsel (https://arxiv.org/pdf/2212.10561.pdf) have also explored similar ideas of prompting LLM to generate a hierarchical plan but implementing low-level planners with programs.",
            "6": "Implementing both high-level and low-level planners with LLM prompting has been explored in Decomposed Prompting (https://openreview.net/pdf?id=_nGgzQjzaRy).",
            "7": "Considering these previous works, the novelty of this paper is limited to applying existing ideas to web datasets and potentially the technical details of autolabeling from human demonstrations.",
            "8": "- The low-level policies are manually defined during autolabeling, making the framework limited in flexibility comparing to previous works that allow LLM to generate decompositions freely.",
            "9": "- The only LLM prompting baseline compared against is ReAct, which demonstrates the benefits of hierarchical planning.",
            "10": "However, such benefits have been demonstrated with the prior works mentioned above."
        },
        "ZMHbTgCDhJ": {
            "0": "-Originality: The idea is interesting in the way HeaP leverages hierarchical policies to decompose complex web tasks using a high-level task planner \n into modular  low-level web policies.",
            "1": "-Quality: The paper is quite thorough in its experimental setup as it tests on 4 interesting datasets, including simulated and live websites, to assess the performance of the proposed approach.",
            "2": "-Clarity: The paper is well-written and structured, making it easy for readers to follow and understand the proposed approach\n\n-Significance: The paper addresses a significant challenge in the field of natural language processing and machine learning, which is teaching LLMs to perform web-based tasks which can lead to a huge set of applications - The tasks are not that challenging and the results are very weak relative to how powerful the LLM model used here which is GPT-3.5.",
            "3": "For example, it seems that the proposed method struggles with book-flight which is a basic constrained task and therefore this method is very far from being deployed in the real world\n\n- Using closed source methods like GPT-3.5 is expensive.",
            "4": "I'd be curious to see how this method would perform with open source methods like Llama and Mistral.",
            "5": "- No code was provided to asses and verify the results as well as understand the low level details of how the method is implemented"
        }
    },
    "uLOFyiruin": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Babel-ImageNet, a massively multilingual benchmark for evaluating vision-and-language (VL) models, which is a significant contribution to the field.",
            "1": "- The benchmark includes translations of 1000 ImageNet labels into 92 languages, addressing the lack of multilingual evaluation datasets for VL models.",
            "2": "- The approach of leveraging BabelNet to obtain translations without relying on machine translation or manual annotation is novel and cost-effective.",
            "3": "Potential reasons for acceptance\n   - The creation of Babel-ImageNet fills a critical gap in the evaluation of multilingual VL models, providing a valuable resource for the research community.",
            "4": "- The paper demonstrates a thorough evaluation of multiple multilingual CLIP models, highlighting performance disparities across languages and validating the benchmark's effectiveness.",
            "5": "- The proposed parameter-efficient language-specific training method shows significant improvements for low-resource languages, offering practical solutions for enhancing VL model performance.",
            "6": "Potential reasons for rejection\n   - **Limited evaluation scope**\n     - The evaluation focuses primarily on zero-shot image classification (ZS-IC) and image-text retrieval, potentially overlooking other important VL tasks.",
            "7": "- The benchmark's reliance on ImageNet classes may not fully capture the diversity of real-world multilingual applications.",
            "8": "- **Potential biases in dataset**\n     - The use of ImageNet classes, which are known to have Anglo-centric biases, may not accurately reflect the cultural and linguistic diversity of the target languages.",
            "9": "- The selection of 92 languages based on XLM-R pretraining may exclude extremely low-resource languages, limiting the benchmark's inclusivity.",
            "10": "- **Translation accuracy concerns**\n     - The automatic translation process, despite leveraging BabelNet, may still introduce errors or inaccuracies, particularly for low-resource languages.",
            "11": "- The paper does not provide a detailed analysis of the translation quality or potential impact of translation errors on model evaluation.",
            "12": "Suggestions for improvement\n   - **Expand evaluation scope**\n     - Include additional VL tasks, such as visual question answering (VQA) or cross-modal reasoning, to provide a more comprehensive evaluation of multilingual VL models.",
            "13": "- Consider incorporating datasets with more diverse and culturally relevant classes to better reflect real-world multilingual applications.",
            "14": "- **Address potential biases**\n     - Conduct a thorough analysis of potential biases in the ImageNet classes and their impact on the evaluation of different languages.",
            "15": "- Explore methods to include extremely low-resource languages in the benchmark, ensuring broader language coverage and inclusivity.",
            "16": "- **Enhance translation quality**\n     - Provide a detailed analysis of the translation quality, including error rates and examples of potential inaccuracies.",
            "17": "- Consider incorporating human validation or post-editing for a subset of translations to assess and improve the overall quality of the benchmark.",
            "18": "- **Broaden language selection criteria**\n     - Reevaluate the criteria for selecting languages to ensure a more balanced representation of high-, mid-, and low-resource languages.",
            "19": "- Explore the possibility of including additional languages beyond those covered by XLM-R pretraining, leveraging other multilingual resources if necessary."
        },
        "tz7HIXzzdC": {
            "0": "- The study goes beyond traditional monolingual evaluation, offering a comprehensive analysis of 8 multilingual CLIP models across 92 languages.",
            "1": "- The paper is well-motivated and, in general, clear enough to follow through;\n- It provides a practical and parameter-efficient approach that significantly improves model performance, making multilingual models more relevant and accessible for underrepresented linguistic communities;\n- The dataset/benchmark contribution targets a relevant issue (the overall imbalance between high and low-resourced languages);\n- The authors already provide the code for reproducibility purposes; - BabelNet reliance.",
            "2": "This work relies entirely on BabelNet and assumes that the mapping between WordNet and other resources is high quality.",
            "3": "However, BabelNet is automated and has a known percentage of error, potentially affecting the label mapping [1];\n- Using WordNet synsets for translations may introduce limitations, as not all concepts or words have direct equivalents in WordNet or BabelNet, potentially impacting the completeness of translations for some languages.",
            "4": "- While the paper emphasizes the creation of the benchmark and model evaluation, it could benefit from a deeper analysis of why certain languages perform poorly according to the chosen metrics and explore potential solutions to address these disparities;\n- Considering that the paper belongs to the \"datasets and benchmarks\" area, the methodology employed (mapping from ImageNet to WordNet and then to BabelNet) is expectedly straightforward.",
            "5": "However, I think there's also some weakness in the data cleaning and validation since the obtained multilingual data is used to evaluate models, but those same benchmarks cannot be used to assess the quality of the data itself;\n- The paper's process of removing words with identical English counterparts in the class label translation and cleaning may not be fully justified, as there can be legitimate shared words between the English and language-specific vocabulary;\n\n[1] Ten Years of BabelNet: A Survey.",
            "6": "Roberto Navigli, Michele Bevilacqua, Simone Conia, Dario Montagnini, Francesco Cecconi.",
            "7": "IJCAI 2021"
        },
        "CgAA4ceifX": {
            "0": "This paper introduces an extensive image-text evaluation benchmark on a large set of languages which motivates research in the largely unexplored multilingual VL representation learning space.",
            "1": "Also, the technique is free from any machine translation or similar techniques that can introduce errors in the evaluation data.",
            "2": "This makes it more robust and suitable for adoption.",
            "3": "This evaluation corpus should be extremely helpful for furthering research in this area.",
            "4": "None."
        },
        "90Fx0BorTV": {
            "0": "- The proposed translation method is robust and the claimed error rate from manual inspection is low.",
            "1": "- The translation covers 92 languages, including many medium and low resource languages.",
            "2": "- When evaluating multilingual models, the performance on Babel-ImageNet correlates well with the text to image retrieval performance on multilingual image-text datasets, suggesting the usefulness of this dataset as an alternative evaluation method for multilingual models From a significance and usefulness perspective, the unique advantage of this dataset over the multilingual image-text datasets for model evaluation is unclear.",
            "3": "It is not surprising that the performance of models on multilingual ImageNet classification is correlated with multilingual text to image retrieval.",
            "4": "My concern is that Babel-ImageNet might not be as good as the multilingual image-text datasets as the former contains much less detailed description for the image, and that other image-text datasets support image-to-text retrieval as well for which Babel-ImageNet could not cover.",
            "5": "The section 6 discussion might be a good opportunity to set up such a comparison if the models there could be evaluated on the multilingual image-text datasets as well.",
            "6": "If the authors can show that Babel-ImageNet better reflects the model quality improvement, that would make a strong argument."
        },
        "cQqPKvNbKj": {
            "0": "1) The multilingual ImageNet benchmark, which supports 92 languages, serves as an excellent platform for evaluating multilingual CLIP models, particularly for those languages that are under-resourced.",
            "1": "2) The assessment of eight different multilingual CLIP models also provides valuable insights.",
            "2": "My concern is about the simplicity of the method, which merely translates English ImageNet labels using BabelNet.",
            "3": "While the resulting benchmark proves useful, the method's contribution appears to be limited."
        }
    },
    "V8aD5pUcVX": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses an important aspect of enhancing Large Language Models (LLMs) to become powerful Multimodal Large Language Models (MLLMs) by investigating the pre-training supervision required for building effective visual tokenizers.",
            "1": "- The study introduces a new benchmark (GVTBench) to evaluate MLLM’s visual semantic understanding and fine-grained perception capabilities, which is a novel contribution to the field.",
            "2": "- The findings provide insights into the performance of different visual tokenizers and propose a new visual tokenizer (GVT) that achieves superior performance without introducing extra parameters or task-specific fine-tuning.",
            "3": "Potential reasons for acceptance\n   - The paper provides a comprehensive empirical evaluation of various visual tokenizers, which is valuable for the research community.",
            "4": "- The introduction of GVTBench as a benchmark for evaluating visual tokenizers in MLLMs is a significant contribution.",
            "5": "- The proposed GVT model demonstrates strong performance across multiple vision-language tasks, showcasing the practical applicability of the research.",
            "6": "- The paper offers detailed insights and observations that can guide future research in the development of visual tokenizers for MLLMs.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper could benefit from a more detailed explanation of the experimental setup and the specific steps taken during the pre-training and evaluation of the visual tokenizers.",
            "8": "- The process of feature distillation and the specific implementation details of the GVT model could be elaborated further to enhance reproducibility.",
            "9": "- **Limited comparison with state-of-the-art models:**\n     - While the paper compares GVT with several existing MLLMs, it would be beneficial to include a more extensive comparison with other state-of-the-art models to provide a clearer context of the improvements achieved.",
            "10": "- The paper could also discuss the limitations and potential weaknesses of the proposed GVT model in comparison to other approaches.",
            "11": "- **Insufficient analysis of results:**\n     - The paper presents a lot of quantitative data, but the analysis of these results could be more thorough.",
            "12": "For example, discussing why certain models perform better or worse in specific tasks would provide deeper insights.",
            "13": "- The qualitative results and visualizations are helpful, but more examples and a detailed discussion of these results would strengthen the paper.",
            "14": "Suggestions for improvement\n   - **Enhance clarity and detail in methodology:**\n     - Provide a more detailed description of the experimental setup, including the specific datasets used, the training process, and the evaluation metrics.",
            "15": "- Elaborate on the feature distillation process and the implementation details of the GVT model to ensure that the methodology is clear and reproducible.",
            "16": "- **Expand comparison with state-of-the-art models:**\n     - Include a broader comparison with other state-of-the-art MLLMs and visual tokenizers to provide a more comprehensive evaluation of the proposed GVT model.",
            "17": "- Discuss the limitations and potential weaknesses of the GVT model in comparison to other approaches to provide a balanced perspective.",
            "18": "- **Deepen analysis of results:**\n     - Provide a more thorough analysis of the quantitative results, discussing the reasons behind the performance differences observed across different models and tasks.",
            "19": "- Include more qualitative examples and a detailed discussion of these results to illustrate the strengths and weaknesses of the GVT model in practical scenarios.",
            "20": "- **Improve presentation and readability:**\n     - Ensure that the figures and tables are clearly labeled and easy to interpret, with sufficient explanation provided in the text.",
            "21": "- Consider restructuring the paper to improve the flow of information and make it easier for readers to follow the key points and findings."
        },
        "aYRvrqJMqA": {
            "0": "This paper shows the following strengths:\n\n1.",
            "1": "The authors have conveyed their design choices and insights via extensive experiments.",
            "2": "For example, the insights of the visual tokenizers are derived from the experiments of various models and datasets.",
            "3": "The benchmark constructed could be useful for future studies on multi-modal models with LLMs.",
            "4": "The final approach of feature distillation is straightforward and clearly improves the performance on the additional datasets of object counting and multi-class identification.",
            "5": "I think the authors may want to pay attention to the following weaknesses.",
            "6": "I will include the specific questions in the next section.",
            "7": "I think the authors need to **better define the important notions** and consequently **clarify the difference to previous works**.",
            "8": "* Describing the difference between \"overall semantics\" and \"fine-grained perception\" is especially critical.",
            "9": "Both terms seem vague to the readers but are the foundation of the evaluation benchmarks and insights in the paper.",
            "10": "* I understand the intuition of adding object counting and multi-class identification.",
            "11": "However, their separation from VQA is unclear.",
            "12": "I suggest improving the current clarification in Sec.",
            "13": "2.1 according to my questions below.",
            "14": "The method of this paper is quite simple (in a good way) by combining the techniques of feature distillation with patch-level supervision.",
            "15": "However, I found the following things need improvement:\n* Writing.",
            "16": "Since the method is simple, it is always better to clearly explain the method.",
            "17": "Sadly, I could hardly find a grounded description of the method in both Sec.",
            "18": "2.1 and Sec.",
            "19": "3.2.",
            "20": "Adding proper equations and detailed clarifications will help the reader to fully understand the approach.",
            "21": "I personally think that the title of the paper is a little **overclaiming**.",
            "22": "I will give two perspectives:\n* I know that it is hard to define \"good,\" but what the paper achieves is only **making MLLMs better at object counting multi-class identification** using patch-level feature distillation, without improving the overall semantic understanding.",
            "23": "So concentrating on what is achieved is more precise, in my own opinion.",
            "24": "* When we talk about visual tokenizers, a lot of variables also need to be covered, such as the architecture, training techniques, etc.",
            "25": "The paper mainly focuses on the types of supervision without investigating the other variables, which might not be sufficient for a grand title or claim.",
            "26": "For example, will switching the claim of the paper to concentrate on \"supervision\" be better?"
        },
        "FQLw00f2UA": {
            "0": "The analysis of different visual tokenizers (e.g., Table 1) is very informative.",
            "1": "The writing is clear and easy to follow.",
            "2": "Compared to the baselines, there are consistent improvements on OC and MCI tasks.",
            "3": "The main concern is about the experimental setup.",
            "4": "Besides CC3M and SBU, authors use Visual Genome, MS-COCO, Object365 and OpenImages V6 to train the visual tokenizer.",
            "5": "Based on the descriptions in Section 2.1, questions in OC and MCI tasks are object-centric.",
            "6": "Such pretraining datasets provide a privilege for the proposed model.",
            "7": "As shown in table 4, most of the improvements come from OC and MCI.",
            "8": "In table 1 and 2, authors use CLIP for ablation study but switch to EVA-CLIP for the remaining experiments.",
            "9": "Why is there a inconsistency?",
            "10": "Based on table 5 and 6, it seems that there are consistent improvements by explicitly utilizing object-level annotations during pretraining.",
            "11": "This does not necessarily mean that the proposed tokenizer is a good visual tokenizer, especially considering the performance drop on VQA and image captioning tasks."
        },
        "EOMw1H1eJy": {
            "0": "I appreciate the authors for conducting a good experimental study on the effect of different visual tokenizers for large multimodal models.",
            "1": "It has been a routine that almost all LMMs use CLIP vision encoder as the visual tokenizer.",
            "2": "Few people pay enough attention to the vision encoder itself, though it is a very important factor in LMMs.",
            "3": "The study on different visual tokenizers suggests that CLIP is still one of the best visual tokenizers for LMMs, despite it is not necessarily able to provide fine-grained representations.",
            "4": "This is somehow surprising but also expected because all of the other used visual tokenizers were trained either with much fewer image-text pairs or purely image data at similar scales.",
            "5": "The authors further proposed a new benchmark and a simple distillation method to investigate and enhance the fine-grained visual understanding capability for LMMs.",
            "6": "The experimental results demonstrate the superiority of the proposed method to previous ones.",
            "7": "As mentioned earlier, I like the first part of this work studying the effectiveness of different visual tokenizers for LMMs.",
            "8": "The argument of LMMs needing both semantic-rich and fine-grained representations is valid and shared by many but has never been demonstrated experimentally before.",
            "9": "However, I do not see a clear motivation to propose a distillation method to address the problem derived from the observation at the first part.",
            "10": "The authors simply distill the visual representations from EVA-CLIP to another randomly initialized vision encoder and presume that the new model has better semantic and fine-grained representations.",
            "11": "I do not buy that the proposed method learns better fine-grained representations in that the input images for distillation are still 224x224, and the distillation is simply copying one network to another based on a limited number of images from ImageNet-1K.",
            "12": "Despite the better performance in Table 4, I can see the unfairness when comparing the proposed method with previous ones -- e.g.",
            "13": "different models have different settings and pretraining data.",
            "14": "In the ablation study, the authors did show some improvements after distillation on the new benchmarks.",
            "15": "Two question marks arise.",
            "16": "First, why the authors use EVA-CLIP which proved to be defective in Table 3.",
            "17": "Second, is it because the distillation pulls the feature space closer to the target domain, e.g., web images->imagenet->COCO?",
            "18": "In Table 6, the gap actually is very marginal (seems the averaged gap does not match the individual gaps for Flant5-xxl).",
            "19": "The experimental study is not enough to demonstrate the effectiveness of the distilled visual tokenizer.",
            "20": "The authors should examine the visual tokenizers in a wider range of vision and vision-language tasks.",
            "21": "That being said, the authors attempt to find a more generic visual tokenizer that can provide semantic-rich and fine-grained representations but fail to demonstrate the generality of the proposed visual tokenizer by itself."
        }
    },
    "bRLed9prWC": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces a novel task of future language modeling, which aims to predict future textual content based on a temporal history of documents.",
            "1": "- This task is significant as it addresses a gap in the machine learning and natural language processing communities, where there has been relatively little work on automatically predicting textual data.",
            "2": "- The proposed models incorporate temporal information into pre-trained language models, which is a novel approach to improving the generation of future textual content.",
            "3": "Potential reasons for acceptance\n   - The paper addresses a novel and important problem that has not been extensively studied in the field of natural language processing.",
            "4": "- The proposed models show significant improvements over strong non-temporal language model baselines in both automatic and human evaluations.",
            "5": "- The paper provides a comprehensive evaluation of the proposed models, including detailed experiments and analysis.",
            "6": "- The authors have made their code available, which promotes transparency and reproducibility of their work.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in the methodology:**\n     - The paper could benefit from a more detailed explanation of the proposed models, particularly the mathematical formulations and the intuition behind them.",
            "8": "- Some of the notations and equations used in the paper are not clearly defined, which may make it difficult for readers to fully understand the proposed approaches.",
            "9": "- **Limited scope of evaluation:**\n     - The evaluation is limited to the domain of NLP abstracts, which may not fully demonstrate the generalizability of the proposed models to other domains.",
            "10": "- The paper could include more diverse datasets and tasks to show the broader applicability of the future language modeling task.",
            "11": "- **Insufficient comparison with related work:**\n     - While the paper mentions related work on temporal language models, it does not provide a thorough comparison with these approaches.",
            "12": "- A more detailed discussion on how the proposed models differ from and improve upon existing temporal language models would strengthen the paper.",
            "13": "Suggestions for improvement\n   - **Enhance clarity and detail in the methodology:**\n     - Provide a more detailed explanation of the proposed models, including the mathematical formulations and the intuition behind them.",
            "14": "- Clearly define all notations and equations used in the paper to ensure that readers can fully understand the proposed approaches.",
            "15": "- **Expand the scope of evaluation:**\n     - Include additional datasets and tasks from different domains to demonstrate the generalizability of the proposed models.",
            "16": "- Provide a more comprehensive evaluation that includes both automatic and human evaluations across diverse datasets.",
            "17": "- **Improve comparison with related work:**\n     - Provide a more detailed discussion on how the proposed models differ from and improve upon existing temporal language models.",
            "18": "- Include a thorough comparison with related work, highlighting the strengths and weaknesses of the proposed approaches relative to existing methods.",
            "19": "- **Address potential limitations and future work:**\n     - Discuss potential limitations of the proposed models and suggest directions for future research.",
            "20": "- Explore the possibility of combining the proposed models with other state-of-the-art techniques to further improve performance."
        },
        "4ulqHTN5pb": {
            "0": "To the best of my knowledge, the application and task are novel and interesting to study.",
            "1": "Three architectures are introduced, each building on each other.",
            "2": "The effects of each component are studied and give us a sense of what is working and what is needed to improve performance.",
            "3": "More motivation and applications for this task could be provided or speculated.",
            "4": "The paper uses GPT-2 as the base model for all experiments.",
            "5": "It would be better for testing robustness to potentially include a few more base models, perhaps with larger sizes or an encoder-decoder architecture for diversity.",
            "6": "Additional experiments and evaluation on downstream tasks can be performed using conditional generation or prompting on existing data sets with temporal dimensions e.g.",
            "7": "the temporal NER data set (Rijhwani & Preotiuc-Pietro, ACL 2019; Luu et al NAACL 2022) or generating hashtags in future tweets using just the tweet text (similar to Preotiuc-Pietro & Cohn, EMNLP 2013).",
            "8": "These would avoid the issues associated with evaluating generations.",
            "9": "Another interesting experiment to conduct would be to study prediction more into the future and quantify the expected degradation as the time window increases."
        },
        "akEfqFBZp8": {
            "0": "Originality:\n- First to formalize future textual data prediction using temporal information.",
            "1": "Develops novel methods for measuring temporal dynamics in language modeling.",
            "2": "Quality:\n- Presents a thorough structure, comparing three new models against multiple baselines.",
            "3": "- Demonstrates model effectiveness through careful data handling, especially distinguishing between content and non-content words during evaluation.",
            "4": "Clarity:\n- Clearly articulates research motivations, background literature, methodology, and findings.",
            "5": "Significance:\n- Offers significant research outcomes with implications for various applications.",
            "6": "- Discusses potential future applications and necessary adaptations.",
            "7": "To improve readability, you can align the organization of tables and figures more closely with their corresponding text."
        },
        "0Rk7OGoR6p": {
            "0": "Modeling of temporal patterns in LLM has not captured much attention from the community.",
            "1": "Yet it is an important problem to look into.",
            "2": "The paper is very easy to follow.",
            "3": "The authors did a good job of describing their ideas and approaches in simple yet accurate terms and notations.",
            "4": "The proposed models look reasonable and are proven to be effective in generating future text based on historical documents.",
            "5": "The proposed models are relatively simple and don't leverage the power of the most advanced LLM.",
            "6": "Some problems the authors tried to solve, such as the gating problem in Sec 3.4 look like sth that would not be an issue to GPT-3 or other recently developed LLM as they are very effective in generating readable and coherent text.",
            "7": "Finetuning a more powerful LLM with the latest text data seems to be a very effective way to model temporal patterns.",
            "8": "Some details and questions from the experiment were not well discussed.",
            "9": "For example, how many raters participated in the human evaluation, what are their agreements, and how subjective are their ratings?",
            "10": "Besides, the results in Table 4. are worth more analysis and discussion.",
            "11": "Why do the baselines not perform well in Problem and Method?",
            "12": "Intuitively, they should be good at generating coherent and readable content."
        }
    },
    "2PKZtPMyvI": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the intrinsic dimension (ID) of data representations learned by Transformers, a topic that has not been extensively explored in the context of sequential modeling and NLP tasks.",
            "1": "- The study provides empirical evidence on how embedding dimension, layer depth, and sequence length impact the ID and task performance, offering new insights into the geometric properties of data representations in Transformers.",
            "2": "- The findings could guide the selection of hyper-parameters and the application of dimension/data reduction techniques, potentially improving the efficiency and performance of Transformer models in NLP tasks.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a novel and significant aspect of Transformer models, contributing to a deeper understanding of their internal mechanics.",
            "4": "- The empirical results are well-supported by a series of experiments on multiple datasets, enhancing the credibility and generalizability of the findings.",
            "5": "- The insights provided could have practical implications for optimizing Transformer models, making the research valuable for both academic and industry applications.",
            "6": "- The paper is well-structured and clearly written, making it accessible to a broad audience within the machine learning and NLP communities.",
            "7": "Potential reasons for rejection\n   - **Lack of theoretical grounding:**\n     - The paper primarily relies on empirical observations without providing a strong theoretical framework to explain the observed phenomena.",
            "8": "- The connection between intrinsic dimension and task performance is not deeply explored from a theoretical perspective, which could limit the impact of the findings.",
            "9": "- **Limited scope of experiments:**\n     - The experiments are focused on text classification tasks, which may not fully capture the behavior of Transformers in other NLP tasks or domains.",
            "10": "- The study does not include comparisons with other sequential models beyond CNNs, which could provide a more comprehensive understanding of the intrinsic dimension in different architectures.",
            "11": "- **Insufficient exploration of practical implications:**\n     - While the paper suggests that the findings could guide hyper-parameter selection and data reduction, it does not provide concrete guidelines or examples of how to apply these insights in practice.",
            "12": "- The potential benefits of data reduction are mentioned but not thoroughly investigated, leaving questions about the practical feasibility and impact of this approach.",
            "13": "Suggestions for improvement\n   - **Strengthen the theoretical foundation:**\n     - Provide a more detailed theoretical analysis of the relationship between intrinsic dimension and task performance, possibly drawing on existing literature in geometry and deep learning.",
            "14": "- Explore the mathematical properties of intrinsic dimension in the context of Transformers, offering a more rigorous explanation of the observed trends.",
            "15": "- **Expand the scope of experiments:**\n     - Include additional NLP tasks, such as text generation or machine translation, to validate the findings across different types of sequential modeling tasks.",
            "16": "- Compare the intrinsic dimension behavior of Transformers with other sequential models, such as RNNs or LSTMs, to provide a broader context for the results.",
            "17": "- **Enhance practical applicability:**\n     - Develop concrete guidelines or algorithms for selecting hyper-parameters based on intrinsic dimension, supported by experimental validation.",
            "18": "- Investigate the practical implications of data reduction in more detail, including potential trade-offs and strategies for implementation in real-world scenarios.",
            "19": "- **Improve clarity and presentation:**\n     - Provide more detailed explanations of the experimental setup and methodology, ensuring that readers can easily replicate the experiments.",
            "20": "- Include additional visualizations or tables to summarize key findings, making it easier for readers to grasp the main insights and their implications."
        },
        "VdynIaqIXr": {
            "0": "Studying the internal representation of deep networks, and in particular of transformers, is a very timely and important research line, and the ID is one of the tools which allows addressing this task.",
            "1": "The monotonic dependence of the ID on the depth is at odds with what observed in convolutional NN for image classification, and also with a similar analysis performed on transformers trained by self-supervision (https://arxiv.org/abs/2302.00294).",
            "2": "Also the focus on the impact of sequence length on learning  (section 3.6 and 3.7) is interesting.",
            "3": "The observation that the ID grows with the embedding dimension  (ED), while the generalisation error decreases (bullet point number 3) seems to me pretty trivial.",
            "4": "Of course by enlarging the ED one recovers a richer presentation, whose ID will be larger, and which will provide better models.",
            "5": "I was not able to understand if the ID is computed by performing a prior average pooling over the sequence, as done in other works on transformers.",
            "6": "If this pooling is not performed, the dimension of the representation is equal to the number of tokens times ED, and not to ED.",
            "7": "The analysis performed in 3.6 is in principle interesting, but inconclusive (the results in table 5 seem to show that the ID cannot be used as a quality proxy to decide if the learning set can be reduced)\nThe analysis presented in 3.4 is very similar to the one presented in [Ansuini 2019]"
        },
        "0F1WTVzYZq": {
            "0": "1.The understanding of Transformer is crucial in field of natural language process, the paper supplied a viewpoint from intrinsic dimension analysis to uncover the behavior of Transformer in sequential classification.",
            "1": "1.Several important related works on analysis of Transformers are missed (such as Revisiting over-smoothing in BERT from the perspective of graph), and the observations of decreasing intrinsic dimension can be viewed as a showcase of over-smoothing.",
            "2": "2.The experiments should be further improved since the paper resorts to experimental study.",
            "3": "For example, more sequential learning tasks should be includes, only text classification seems insufficient, such as machine translation which is a typical sequential learning task for Transformer understanding (see more in question part).",
            "4": "3.Lack of deep understanding of the observation, for example, increasing the embedding dimension will increase (intrinsic dimension).",
            "5": "However, large embedding dimension may suffer from over-fitting even with fixed depth (see more in question part)."
        },
        "BHRasoPBbo": {
            "0": "The analysis of the intrinsic dimension of the hidden representations in transformers trained on NLP tasks is an interesting and relevant topic that has only recently started to be addressed by some studies.",
            "1": "The experimental tests in support of some claims are not solid enough (see my concern regarding the ID vs. accuracy analysis).",
            "2": "Some parts contain technical flaws (see the concern about the PCA-ID computation), and the sentences are phrased in a way that is sometimes hard to follow."
        },
        "IsCaiV5VIS": {
            "0": "originality: Although the intrinsic dimension (ID) is an established concept and the codes have been provided in prior work [1], this paper under review is the first to carefully examine ID (under this particular definition) in Transformers in the text classification setting.",
            "1": "clarity: The methodology and findings are clearly described.",
            "2": "[1] Intrinsic Dimension of Data Representations in Deep Neural Networks, NeurIPS 2019, https://github.com/ansuini/IntrinsicDimDeep quality: a key underlying assumption is questionable, making the observations less convincing\n\nIntrinsic dimension (ID) is defined in the following way: assuming a set of vectors “locally uniformly” lie on a d-dimensional subspace of the full D-dimensional vector space (d <= D), then the ratios of close pairwise distances follow the Pareto distribution parametrized by d. Then, the intrinsic dimension (ID) is the max likelihood estimation of d. (For a formal definition, see Section 2, paragraph “TwoNN Method” on page 3.)",
            "3": "Thus, the ID estimation is only meaningful if the following Assumption 1 holds: \n\nAssumption 1: The ratios of close pairwise distances of Transformer representations (approximately) follow the Pareto distribution.",
            "4": "However, without further justification, the validity of Assumption 1 is questionable.",
            "5": "According to [2], a sufficient condition for Assumption 1 is that the representations are “locally uniform” in density, where \"locally\" means “within the range of the second neighbor for each data point”.",
            "6": "It is unclear whether this sufficient condition holds for Transformer representations.",
            "7": "Nevertheless, note that this is likely not a necessary condition, so there could be other ways to achieve Assumption 1.",
            "8": "However, the authors did not check (or even explicitly mention) Assumption 1, which calls into question the validity of the method and the results.",
            "9": "Moreover, [1] states that for ID estimation for convolutional neural networks (CNNs) representations on image data,  \n\n“the estimated values remain very close to the ground truth ID, when this is smaller than ~20.",
            "10": "For larger IDs and finite sample size, the approach moderately underestimates the correct value, especially if the density of data is non-uniform.",
            "11": "Therefore, the values reported in the following figures, when larger ~20, should be considered as lower bounds.”\n\nNote that in the current paper under review, most of the estimated IDs are greater than 20 (Figures 1-2, Tables 1-5).",
            "12": "Can we trust the estimated IDs by this approach?",
            "13": "Do we have evidence that the estimated values remain very close to the ground truth ID for a larger range of IDs (> 20) in the setting of this paper?",
            "14": "originality: it seems that the current paper under review mostly used the approach of [1] (which was for convolutional neural networks (CNNs) representations on image data), applying it to Transformers on text classification data.",
            "15": "While some observations could potentially be interesting, the main technical contribution of the methodology should be primarily attributed to prior works.",
            "16": "[1] Intrinsic Dimension of Data Representations in Deep Neural Networks.",
            "17": "NeurIPS 2019\n\n[2] Estimating the intrinsic dimension of datasets by a minimal neighborhood information.",
            "18": "Scientific reports, vol.",
            "19": "7, no.",
            "20": "1, p. 12140, 2017"
        },
        "EPJSuPePcr": {
            "0": "None This paper seems to be a study.",
            "1": "And an incomplete one.",
            "2": "It goes from related work where they describe the TwoNN method proposed by another paper, to experiment results.",
            "3": "There is no technical section?",
            "4": "It is also written kind of strangely."
        }
    },
    "VpCqrMMGVm": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the interpretability of large language models (LLMs) by focusing on their internal mechanisms for performing mathematical addition tasks.",
            "1": "- It is the first attempt to reveal specific mechanisms related to how LLMs implement reasoning tasks involving simple one-digit integer addition.",
            "2": "- The findings contribute to the development of more transparent and interpretable LLMs, which is a significant step forward in the field of natural language processing.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel approach to understanding the internal workings of LLMs, specifically in the context of mathematical addition.",
            "4": "- The experiments are comprehensive and well-designed, providing robust evidence for the claims made.",
            "5": "- The findings have practical implications for improving the interpretability and transparency of LLMs, which is a critical issue in the field.",
            "6": "- The paper demonstrates the generalizability of the identified key attention heads across different datasets and related tasks, such as subtraction.",
            "7": "Potential reasons for rejection\n   - **Limited Scope of Mathematical Tasks**\n     - The study focuses solely on simple one-digit integer addition, which may not fully represent the complexity of mathematical reasoning tasks that LLMs can perform.",
            "8": "- The paper does not explore other mathematical operations such as multiplication or division, which could provide a more comprehensive understanding of LLMs' mathematical capabilities.",
            "9": "- **Insufficient Analysis of Earlier Layers**\n     - The paper primarily focuses on the middle layers of the LLMs, with limited analysis of the earlier layers' contributions to the addition task.",
            "10": "- There is a lack of detailed investigation into how the earlier layers influence the key attention heads identified in the middle layers.",
            "11": "- **Potential Overemphasis on Attention Heads**\n     - The study emphasizes the role of attention heads in mathematical addition, potentially overlooking other important components of the transformer architecture, such as MLPs and residual connections.",
            "12": "- The knockout experiments focus on attention heads, but the impact of other components on the model's performance is not thoroughly examined.",
            "13": "Suggestions for improvement\n   - **Expand the Scope of Mathematical Tasks**\n     - Include experiments on other mathematical operations such as multiplication, division, and more complex addition and subtraction problems to provide a broader understanding of LLMs' mathematical reasoning capabilities.",
            "14": "- Investigate the performance of LLMs on multi-step mathematical problems to assess their ability to handle more complex reasoning tasks.",
            "15": "- **Analyze Earlier Layers in More Detail**\n     - Conduct a more detailed analysis of the earlier layers of the LLMs to understand their contributions to the addition task and how they influence the key attention heads in the middle layers.",
            "16": "- Explore the interactions between different layers and components of the transformer architecture to provide a more holistic view of the model's internal mechanisms.",
            "17": "- **Examine the Role of Other Components**\n     - Investigate the impact of MLPs, residual connections, and other components of the transformer architecture on the model's performance in mathematical tasks.",
            "18": "- Perform knockout experiments on these components to assess their importance and contribution to the model's overall capabilities.",
            "19": "- **Provide More Detailed Visualizations and Examples**\n     - Include more detailed visualizations of the attention patterns and other relevant metrics to help readers better understand the findings.",
            "20": "- Provide additional examples and case studies to illustrate the practical implications of the identified key attention heads and their generalizability across different tasks and datasets."
        },
        "GQN85sAbm5": {
            "0": "The analysis is thorough and rigorous.",
            "1": "The paper is clearly written, and the presentation is well-organised and presented.",
            "2": "On top of identifying the “key heads” being focused on addition, the paper shows that the same heads are also involved in subtraction.",
            "3": "While this might be intuitive, considering that one of these tasks is the opposite of the other, it is not obvious that an LLM would discover and utilize this duality.",
            "4": "However, it is unclear whether that is because the heads focus only on numbers or because they are utilising the duality of summation and subtraction.",
            "5": "The paper recognises that later heads depend on earlier ones and attempts to analyse these dependencies (although it appears there are none).",
            "6": "The paper identifies attention heads that take part in the processing of summation but does not look into or explain what each of the “key heads” actually does and what is the mechanism through which it contributes to summation.",
            "7": "Therefore, the paper focuses on _localization_ of the heads that partake in summation, rather than _interpreting_ them.",
            "8": "The paper does not look at alternative representations of numbers.",
            "9": "For example, in words (“two” instead of 2), Roman numerals (II instead of 2), and other languages (二 or ٢ instead of 2).",
            "10": "The lack of such analysis leaves the question open whether these heads simply attend to numerical tokens or whether they are involved in higher-order reasoning about numbers and arithmetic.",
            "11": "Related to the above, the paper seems to focus only on single-digit summation.",
            "12": "It is unclear whether the results would translate to the summation of larger numbers (or more than two numbers).",
            "13": "This is important as prior works have shown that the ability of LLMs to do arithmetic quickly decreases with the increase of the number of digits.",
            "14": "It would be interesting to see if your analysis would be able to provide insights into this phenomenon.",
            "15": "I am not sure how to read the attention patterns in Fig.",
            "16": "How can the attention be negative?",
            "17": "In fact, it does not seem that these heads attend to all numbers.",
            "18": "The first head seems to attend to the completion of “or” with “anges” and the full stop.",
            "19": "Both heads seem to attend only to 3 while solving the task would also require attention to 5.",
            "20": "Therefore, it is not clear how these heads participate in performing summation.",
            "21": "The paper looks predominantly at attention heads.",
            "22": "However, it is well known that a lot of the computation and processing happens in the MLPs.",
            "23": "Hence, a full picture of the interoperation of the mechanisms for summation should also include the MLPs"
        },
        "1bdsUgaWIC": {
            "0": "Authors are tackling an important problem by aiming to understand the inner workings of LLMs.",
            "1": "With the increased pace of advancements happening in the field, it is imperative to gain this understanding.",
            "2": "Authors tackle the problem in a clear manner, by coming up with a clean task (involving addition of 2 integers) and testing their hypothesis systematically.",
            "3": "Their findings indicate that a limited number of attention heads suffice for achieving strong performance across a range of addition tasks.",
            "4": "Importantly, the methodology they introduce can prove valuable for conducting sensitivity analyses in other areas of interest and even facilitate model sparsification.",
            "5": "They validate their hypothesis on several LLMs and a few addition tasks.",
            "6": "Additionally, their preliminary investigations reveal that the attention heads vital for addition tasks also exert a substantial influence on subtraction.",
            "7": "While the authors have indeed posed a clear problem and approached it systematically, I find the setup to be somewhat restrictive.",
            "8": "- Although the authors make a great effort to tackle the task of addition, their focus remains solely on the addition of two integers.",
            "9": "It would be intriguing to see whether their findings extend to addition of multiple integers and rational numbers, as well as their applicability to problems involving multiple addition operations.",
            "10": "- The robustness of this study could be significantly enhanced if the authors were to conduct analogous experiments on subtraction, multiplication, and division.",
            "11": "Such investigations would shed light on whether a select group of attention heads can consistently influence performance across all four mathematical operations."
        },
        "WHjm3zElaq": {
            "0": "Strengths:\n\n- The language of the paper is concise and clear.",
            "1": "- The breadth and depth of the paper is excellent - specifically the use of 3 LLMs\n(LLaMA2-7B, Qwen-7B and chatGLM2-6B), 20 question formats and 10K sample\nquestions.",
            "2": "- The rigorous nature of the paper is excellent - the claims re addition are confirmed via\ndetailed experimentation.",
            "3": "- The most significant finding is that a small number of attention heads are consistently\nused by each model to perform one-digit addition across the various question formats.",
            "4": "Weaknesses:\n\n- The paper (seems to) limit itself to one-digit addition and subtraction - reducing its scope\nto a subset of addition and subtraction.",
            "5": "The abstract should explicitly say that the scope\nis one-digit integer addition.",
            "6": "-  The paper (seems to) limit itself to simple one-digit addition and subtraction (without\n“carry over one” or “borrow one” examples - reducing its scope to a subset of addition\nand subtraction.",
            "7": "The abstract should explicitly say that the scope is simple one-digit\ninteger addition.",
            "8": "-  The paper does not explain how the attention heads (&/or MLP layer) actually perform\nthe addition calculation.",
            "9": "This explanation is left for future work.",
            "10": "-  The paper touches on subtraction, showing similarities, but a detailed analysis is left for\nfuture work.",
            "11": "-  A discussion of the differences in how each of the LLMs implement one-digit addition\nwould have been interesting e.g.",
            "12": "do all the models use roughly the same number of attention heads to implement addition?",
            "13": "If no differences were found, then this would be\nan interesting finding in itself.",
            "14": "-  The small scope of this paper limits the reusability of this work."
        },
        "ax1aEHFP6r": {
            "0": "- a timely topic is treated, how models that are used in practice perform mathematical addition and subtraction\n- a large number of figures that show how attention heads are activated on concrete examples help to make the paper readable - The authors didn't include, as related work, some publications that also deal with mathematical reasoning, such as [1]\n- studying only mathematical addition and subtraction seems restrictive.",
            "1": "I do note that the authors state at the end however: \"_A more thorough study on the subtraction task as well as the validation on more computation tasks (e.g., multiplication and division, etc.)",
            "2": "is left for future work._\"\n\n[1] https://arxiv.org/pdf/2305.08809.pdf"
        }
    },
    "ZDGKPbF0VQ": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Advantage-Leftover Lunch RL (A-L OL), a new class of offline policy gradient algorithms for language model training.",
            "1": "- A-L OL is designed to be sample-efficient, stable, and easy to implement, addressing the instability and data-hungriness of traditional RLHF methods.",
            "2": "- The method assumes the entire LM output sequence as a single action, allowing the use of sequence-level classifiers or human-designed scoring functions as rewards.",
            "3": "- A-L OL only trains on positive advantage data points, making it resilient to noise and improving learning efficiency.",
            "4": "Potential reasons for acceptance\n   - The proposed method addresses significant limitations of existing RLHF methods, such as instability and high data requirements.",
            "5": "- A-L OL demonstrates strong performance across multiple language generation tasks, outperforming both online RL and recent offline RL baselines.",
            "6": "- The method's resilience to noisy or suboptimal training data is a notable advantage, making it applicable in a wider range of scenarios.",
            "7": "- The paper provides thorough experimental validation, including comparisons with various baselines and ablation studies.",
            "8": "- The authors release the code, promoting reproducibility and further research.",
            "9": "Potential reasons for rejection\n   - **Limited novelty in the core idea**:\n     - The concept of using offline policy gradients and advantage-based training is not entirely new.",
            "10": "- The paper could benefit from a more detailed comparison with closely related methods to highlight the unique contributions of A-L OL.",
            "11": "- **Complexity of the method**:\n     - While the authors claim that A-L OL is easy to implement, the method involves several components (e.g., value estimate, importance weight, advantage calculation) that may add complexity.",
            "12": "- The paper could provide more practical guidance on implementing A-L OL in real-world scenarios.",
            "13": "- **Evaluation on limited tasks**:\n     - The experiments focus on a few specific language generation tasks.",
            "14": "It would be beneficial to see the method tested on a broader range of tasks to demonstrate its generalizability.",
            "15": "- The paper could include more diverse datasets and tasks to strengthen the claims of robustness and versatility.",
            "16": "- **Potential overfitting to specific benchmarks**:\n     - The method's performance is evaluated on specific benchmarks like HHA and Reddit response generation.",
            "17": "There is a risk that the method is overfitted to these benchmarks.",
            "18": "- Additional experiments on unseen or more challenging benchmarks could provide a more comprehensive evaluation.",
            "19": "Suggestions for improvement\n   - **Expand the evaluation**:\n     - Test A-L OL on a wider variety of language generation tasks and datasets to demonstrate its generalizability and robustness.",
            "20": "- Include more challenging and diverse benchmarks to provide a comprehensive evaluation of the method's performance.",
            "21": "- **Clarify implementation details**:\n     - Provide more practical guidance and detailed steps for implementing A-L OL in real-world scenarios.",
            "22": "- Include code snippets or pseudocode for key components to help readers understand the implementation process.",
            "23": "- **Enhance the comparison with related work**:\n     - Provide a more detailed comparison with closely related methods, highlighting the unique contributions and advantages of A-L OL.",
            "24": "- Discuss the limitations and potential drawbacks of A-L OL in comparison to other methods.",
            "25": "- **Address potential complexity**:\n     - Simplify the explanation of the method's components and their interactions to make it more accessible to a broader audience.",
            "26": "- Consider providing a step-by-step guide or flowchart to illustrate the training process and key steps involved in A-L OL.",
            "27": "- **Include additional ablation studies**:\n     - Conduct more ablation studies to isolate the impact of each component (e.g., value estimate, importance weight, advantage calculation) on the overall performance.",
            "28": "- Analyze the sensitivity of the method to different hyperparameters and provide recommendations for optimal settings."
        },
        "S4SuC0Y8KJ": {
            "0": "The proposed method is clear and easy to implement, with relatively few assumptions.",
            "1": "Therefore, it may have practical merits.",
            "2": "The experiments are relatively throughout and the results are promising.",
            "3": "Human study helps demonstrating the efficacy of the proposed method.",
            "4": "The paper is generally easy to follow.",
            "5": "Unclear method contribution: the proposed method is nearly, if not exactly, a special case of TRPO/PPO method in the bandit setting.",
            "6": "Such a special bandit instantiation has been widely considered in classical RLHF works, such as [1,2].",
            "7": "Advantage-weighted policy optimization is also a well-studied method in offline RL.",
            "8": "For example, Eq.",
            "9": "5 in this paper is very similar to Eq.",
            "10": "4 in AWAC [3], except for the importance weighting that basically comes from TRPO/PPO.",
            "11": "The formulation of considering the entire output sequence as a single action step may suffer from exponentially large action space, which may make policy training harder and less stable.",
            "12": "See for example [4, 5].",
            "13": "As an aside, recent works have already tried to learn a per-token reward function that incorporates arbitrary human-designed scoring function(s), which may better cope with the large action space in NLG problem, see, e.g., [6].",
            "14": "Weighted behavior cloning has been quite extensively used in prior NLP papers, e.g., [6,7,8,9,10,11].",
            "15": "It will make the algorithmic contribution of this paper more clear if the authors can have a paragraph discussing and comparing with such related works, instead of only citing the CRR paper from offline RL.",
            "16": "In Table 2, the comparison with PPO may not be fair, because the reward for PPO is a good-or-bad classifier.",
            "17": "The offline RL methods, on the other hand, are fitted towards the original responses that themselves show high linguistic diversity, which would implicitly guide the offline RL methods, especially A-LoL, towards generating longer and more diverse sequences.",
            "18": "In short, there is no guiding signal for PPO to generate such sequences, while the offline RL methods implicitly have the guidance.",
            "19": "There are several well-established exogenous components in the proposed method, such as (1) importance clipping, (2) discarding negative advantage datapoints, (3) prioritized sampling.",
            "20": "It is unclear how each of those exogenous components contribute to the overall performance.",
            "21": "It is also unclear if the baselines can also benefit from such exogenous components, e.g., (2) and (3).",
            "22": "This again muds the algorithmic contribution of the proposed method and make the experiment results less convincing.",
            "23": "[1] Stiennon, Nisan, et al.",
            "24": "\"Learning to summarize with human feedback.\"",
            "25": "Advances in Neural Information Processing Systems 33 (2020): 3008-3021.",
            "26": "[2] Ouyang, Long, et al.",
            "27": "\"Training language models to follow instructions with human feedback.\"",
            "28": "Advances in Neural Information Processing Systems 35 (2022): 27730-27744.",
            "29": "[3] Peng, Xue Bin, et al.",
            "30": "\"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.\"",
            "31": "arXiv preprint arXiv:1910.00177 (2019).",
            "32": "[4] Guo, Han, et al.",
            "33": "\"Text Generation with Efficient (Soft) $ Q $-Learning.\"",
            "34": "(2021).",
            "35": "[5] Snell, Charlie, et al.",
            "36": "\"Offline rl for natural language generation with implicit language q learning.\"",
            "37": "arXiv preprint arXiv:2206.11871 (2022).",
            "38": "[6] Yang, Shentao, et al.",
            "39": "\"Preference-grounded Token-level Guidance for Language Model Fine-tuning.\"",
            "40": "arXiv preprint arXiv:2306.00398 (2023).",
            "41": "[7] Govardana Sachithanandam Ramachandran, Kazuma Hashimoto, and Caiming Xiong.",
            "42": "2022.",
            "43": "[CASPI] Causal-aware Safe Policy Improvement for Task-oriented Dialogue.",
            "44": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 92–102, Dublin, Ireland.",
            "45": "Association for Computational Linguistics.",
            "46": "[8] Feng, Y., Yang, S., Zhang, S., Zhang, J., Xiong, C., Zhou, M., & Wang, H. (2023).",
            "47": "Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems.",
            "48": "arXiv preprint arXiv:2302.10342.",
            "49": "[9] Norouzi, Mohammad, et al.",
            "50": "\"Reward augmented maximum likelihood for neural structured prediction.\"",
            "51": "Advances In Neural Information Processing Systems 29 (2016).",
            "52": "[10] Sayan Ghosh, Zheng Qi, Snigdha Chaturvedi, and Shashank Srivastava.",
            "53": "How helpful is inverse reinforcement learning for table-to-text generation?",
            "54": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 71–79, 2021.",
            "55": "[11] Marcin Junczys-Dowmunt, Roman Grundkiewicz, Shubha Guha, and Kenneth Heafield.",
            "56": "Approaching neural grammatical error correction as a low-resource machine translation task.",
            "57": "arXiv preprint arXiv:1804.05940, 2018."
        },
        "bIEMVkp07E": {
            "0": "I appreciate the direction of exploring objectives that are not on-policy RL, which is the hot topic these days in fine-tuning LLMs.",
            "1": "I’m glad the discussion of comparison with GOLD exists (Section 2.4), because the motivation and derivation are extremely similar to GOLD (except for a few differences like the per-token action vs. single action distinction, the different treatment of importance weights, etc., as described in Section 2.4).",
            "2": "I think it’s totally fine even if it’s similar to GOLD – there are design differences, a few tricks are used, and more experiments are done.",
            "3": "Related to the above point: for experiments, I especially appreciate the experiments on the helpful-harmless assistant task.",
            "4": "Approximations don’t seem justified mathematically.",
            "5": "Maybe it’s alright given RL+NLP research has too many approximations in general – I’ll need to see what other reviewers think.",
            "6": "- For the “ref free” variant: Is it justified to use 1 as importance weights, in the reference free variant of the algorithm?",
            "7": "I can’t wrap my head around whether that’s an acceptable approximation, or whether that's making the derived Equation (3) or Equation (4) simply incorrect.",
            "8": "- For the “sequence” variant: The approximation of importance rule is a bit strange.",
            "9": "See line 6 of the “variants with alternative importance weight” paragraph on page 4. it’s essentially saying a1 * a2 * … * aT * (b1 + b2 + … + bT) = a1 * b1 + a2 * b2 + … + aT * bT.",
            "10": "But this seems wrong?",
            "11": "Am I understanding this correctly?",
            "12": "Perhaps an explanation of why this is a good approximation will be helpful.",
            "13": "But at the same time, the empirical results aren’t really impacted much, so I’m conflicted on how much I should treat this approximation seriously.",
            "14": "A major issue: did the authors train PPO methods for more training steps (more than 1.3 times the training steps of offline methods)?",
            "15": "If for more training steps, PPO results improve but your results stay stable, then we can’t say PPO is worse.",
            "16": "Phrasing of the main question in paragraph 1 – the main question seems to be the italicized sentence at the end of the first paragraph: “can we perform rewarded learning, similar to PPO, while only using pre-existing data” but the answer is already yes given the literature in the past few years.",
            "17": "- Cringe loss (https://aclanthology.org/2023.acl-long.493.pdf), as well as the older director loss (https://arxiv.org/pdf/2206.07694.pdf) and unlikelihood loss are relevant.",
            "18": "The other algorithms the authors cited are also examples where we can learn from rewards while only using pre-existing data.",
            "19": "I think the authors’ research question can be more specific & take prior work into account.",
            "20": "- In addition, I’m also confused about what “similar to PPO” means: do the authors mean that PPO is a form of “rewarded learning” or do the authors mean “can we perform rewarded learning such that the results on X benchmark is similar to PPO performance?”\n\nNo on-policy RL performance on Reddit generation task.",
            "21": "Is PPO helpful here (given that it’s so popular)?"
        },
        "HvaC6wmQGY": {
            "0": "The single-action assumption made here is very reasonable in the RLHF setting because the transition kernel in natural language generation is deterministic and trivial, such that the standard RLHF is in effect a contextual bandit problem instead of an RL problem.",
            "1": "RL algorithms like PPO are unnecessarily complicated in the standard RLHF setting, so it's nice to see a more stable contextual bandit problem algorithm.",
            "2": "The proposed method is different enough from other alternatives.",
            "3": "In the common HHA benchmark, A-LOL beats other recent preference-based offline RL baselines such as DPO and PRO and other common baselines such as weighted behavior cloning and PPO.",
            "4": "Experiments on reddit generation task also shows the advantage of the proposed method and its flexibility in terms of optimizing for versatile rewards.",
            "5": "A-LoL does not seem to perform better than DPO on the HHA benchmark with the common reward function.",
            "6": "In particular, A-LoL seems to be less \"Helpful\" compared to DPO.",
            "7": "Is there any explanation on that?",
            "8": "The paper seems to be suggesting that there is an issue of reward hacking with the common reward function, is there a concrete example supporting this claim?",
            "9": "In the offline setting where all the data comes from existing offline datasets, the best that we can do seems only to be as good as the best trajectories in the offline datasets.",
            "10": "Is it possible to modify A-LoL such that it can continue to improve itself with online data generated by itself and labeled by the reward model?",
            "11": "minor - The single-action assumption might not always hold especially when the dialogue involves multi-step interaction with the users."
        },
        "esu8qcqbw1": {
            "0": "Strengths:\n- The motivation of the paper and the technical contributions are clear.",
            "1": "- The authors perform a thorough empirical investigation of their proposed technique across several tasks.",
            "2": "The authors conducted various ablation experiments of the proposed idea to show why the algorithm performed well (e.g., R-LOL versus A-LOL).",
            "3": "- The author studies an important question of comparing online and offline policy gradient algorithms.",
            "4": "- The authors also discuss an interesting issue around a subset of RLHF techniques needing human preference data, whereas other techniques do not.",
            "5": "Weaknesses:\n- Given that in language, the transition function is trivial - it is unclear why offline algorithms are more sample-efficient than online algorithms.",
            "6": "Offline algorithms assume access to a lot of quality data, while online algorithms can work with small amounts of data and well-designed reward functions.",
            "7": "- The paper relies on the assumption that each token is not an action but instead a sequence is an action, but it is unclear why this assumption matters.",
            "8": "Most RLHF techniques optimize policies on sequence-level losses, not token-level losses because the reward functions are defined on a sequence.",
            "9": "- For the A-LOL algorithms to work, there is a set of assumptions that are not explicitly mentioned in the paper that could have a big impact on performance.",
            "10": "In particular, if you don't have good data coverage and a good initial policy, then A-LOL will fail, which means that A-LOL and PPO in RLHF have the same assumptions.",
            "11": "- The experiments did not include PPO due to a seed collapsing, but there has been evidence in the literature that this does not happen, which means the authors did not tune this baseline algorithm properly [1].",
            "12": "- The authors claim that their proposed approach is more data-efficient because they filtered out 33% of good responses, but the same procedure can be done for other techniques.",
            "13": "However, a similar procedure was not conducted for the baseline algorithms to show that with less data, their proposed approach performs better.",
            "14": "[1] PAIRWISE PROXIMAL POLICY OPTIMIZATION: HARNESSING RELATIVE FEEDBACK FOR LLM ALIGN- MENT  https://arxiv.org/pdf/2310.00212.pdf"
        }
    },
    "d94x0gWTUX": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant limitation in conventional reward models (RMs) by proposing a tool-augmented preference modeling approach, named Themis.",
            "1": "- The integration of external tools such as calculators and search engines into RMs is a novel approach that enhances the interpretive capacity and scoring reliability of RMs.",
            "2": "- The paper provides a comprehensive dataset (TARA) and makes the code, data, and model checkpoints publicly available, which is valuable for further research in the field.",
            "3": "Potential reasons for acceptance\n   - The proposed approach demonstrates a noteworthy overall improvement of 17.7% across eight tasks in preference ranking, showcasing its effectiveness.",
            "4": "- Themis outperforms Gopher 280B by 7.3% on the TruthfulQA task in zero-shot evaluation, indicating its superior performance in factuality and truthfulness.",
            "5": "- The paper includes extensive experimental evaluations, including human evaluations, which provide robust evidence for the practical utility and superiority of the approach.",
            "6": "- The public availability of the dataset and code facilitates reproducibility and further research advancements.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The paper could benefit from a more detailed explanation of the tool invocation process and how the model decides which tools to use and when.",
            "8": "- The description of the training objectives and the auto-regressive training process could be more explicit to ensure a clear understanding of the approach.",
            "9": "- **Limited comparison with other state-of-the-art methods:**\n     - The paper primarily compares Themis with conventional RMs and Gopher 280B but lacks a comprehensive comparison with other recent advancements in the field.",
            "10": "- Including more baseline models and a broader range of tasks could strengthen the evaluation and highlight the unique contributions of Themis.",
            "11": "- **Potential overfitting to specific tools:**\n     - The performance of Themis might be heavily dependent on the specific tools integrated into the model, raising concerns about its generalizability to other tools or domains.",
            "12": "- The paper should address how the model can adapt to new tools or changes in the external environment without significant retraining.",
            "13": "Suggestions for improvement\n   - **Enhance clarity in methodology:**\n     - Provide a more detailed step-by-step explanation of the tool invocation process, including examples of how the model decides which tools to use and when.",
            "14": "- Clarify the training objectives and the auto-regressive training process with additional diagrams or pseudocode to aid understanding.",
            "15": "- **Expand comparison with other methods:**\n     - Include a broader range of baseline models and tasks in the experimental evaluation to provide a more comprehensive comparison with other state-of-the-art methods.",
            "16": "- Highlight the unique contributions and advantages of Themis over these methods to strengthen the case for its novelty and effectiveness.",
            "17": "- **Address generalizability concerns:**\n     - Discuss potential strategies for adapting Themis to new tools or changes in the external environment, such as transfer learning or modular tool integration.",
            "18": "- Conduct experiments to evaluate the model's performance with different sets of tools or in different domains to demonstrate its robustness and adaptability.",
            "19": "- **Improve dataset and tool diversity:**\n     - Expand the TARA dataset to include a wider variety of tools and tasks, ensuring that the model is tested on a diverse range of scenarios.",
            "20": "- Provide detailed statistics and analysis of the dataset to highlight its comprehensiveness and relevance to the research community."
        },
        "Rr8S0Vp25s": {
            "0": "**Strength 1**: The idea of augmenting reward models with tools is very interesting, novel, and timely.",
            "1": "**Strength 2**: The proposed method provides a nice and logical way for tools to be included in the reward design process.",
            "2": "**Strength 3**: This paper provides some interesting experiments such as application to RLHF and scaling experiments.",
            "3": "**Weakness 1**: One of my main concerns is lack of experiments on standard reward modeling datasets.",
            "4": "There are many datasets not included in the paper such as the Anthropic HH dataset, Stack Overflow, OpenAI WebGPT, and ChatGPT comparisons datasets.",
            "5": "They do conduct analysis on a small portion of the HH dataset, but not on the provided testing set.",
            "6": "In addition, they show worst test accuracy than is reported in some other papers that only use conventional reward modeling [1].",
            "7": "Since the main claim of the paper is that by using tools they can improve the accuracy of reward models, I think their method should be validated on these popular datasets.",
            "8": "[1] Dong, Hanze, et al.",
            "9": "\"Raft: Reward ranked finetuning for generative foundation model alignment.\"",
            "10": "arXiv preprint arXiv:2304.06767 (2023).",
            "11": "**Weakness 2**: Little Hyperparameter study.",
            "12": "THEMIS introduces various hyperparameters, but the sensitivity of model performance to these hyperparameters is not discussed.",
            "13": "**Weakness 3**: This paper does not discuss a significant limitation of this method: the difficulty of creating the dataset.",
            "14": "The dataset creation process consists of various complex steps, involves tool selection and design of heuristics.",
            "15": "This seems to be difficult to scale to large scale preference datasets.",
            "16": "**Weakness 4**: I think that this paper could use a more in depth discussion of related works.",
            "17": "In particular, various works have attempted to use similar tools such as a compiler in the reward design process [2,3,4,5] and [4] use it to guide the reward model training.",
            "18": "Discussing these works could help better frame the contribution of this work.",
            "19": "[2] Le, Hung, et al.",
            "20": "\"Coderl: Mastering code generation through pretrained models and deep reinforcement learning.\"",
            "21": "Advances in Neural Information Processing Systems 35 (2022): 21314-21328.",
            "22": "[3] Shen, Bo, et al.",
            "23": "\"Pangu-coder2: Boosting large language models for code with ranking feedback.\"",
            "24": "arXiv preprint arXiv:2307.14936 (2023).",
            "25": "[4] Bukharin, Alexander, et al.",
            "26": "\"Deep Reinforcement Learning from Hierarchical Weak Preference Feedback.\"",
            "27": "arXiv preprint arXiv:2309.02632 (2023).",
            "28": "[5] Shojaee, Parshin, et al.",
            "29": "\"Execution-based code generation using deep reinforcement learning.\"",
            "30": "arXiv preprint arXiv:2301.13816 (2023)."
        },
        "Drpkvb5ubb": {
            "0": "- Having a level of reasoning and interpretability is great feature to have for reward models\n- The experiments and provided implementation details look comprehensive 1- How to trust tools is an important aspect to consider here.",
            "1": "At least in the examples, it looks like there is a risk of biasing the reward model and generative model to outputs of specific tools being used.",
            "2": "This could be concerning as tools are not necessarily unbiased.",
            "3": "2- It is not entirely clear how GPT-4 is used to generate RM training data.",
            "4": "Note that GPT-4 itself is a system if the proposal is to use GPT-4 to train RM, one can argue why not directly train RM on GPT-4 data or use GPT-4 directly as reward model.",
            "5": "3- When alpha in eq.",
            "6": "2 is set to zero, we converge to typical tool use via simple prompting right?",
            "7": "the RM is still a pretrained model and can be prompted to use tools even without explicit loss terms on tool use.",
            "8": "Is this understanding correct?",
            "9": "4- Model size of 7B is quite small to capture knowledge to compete with the tools used in this paper.",
            "10": "I think a more realistic setup would be to take a larger model and the gap between say Wiki tool and that result could look very different.",
            "11": "5- I think the write-up could improve, especially for Sec 3.1, I had difficulty understanding exactly how GPT-4 was used and how training data was prepared"
        },
        "NpKwNl7kbO": {
            "0": "(1) The paper addresses an important issue in reward modeling by introducing a tool-augmented approach to enhance the effectiveness of RMs.",
            "1": "(2) The proposed methodology of integrating external tools into RMs is innovative and practical, allowing for dynamic decision-making and reasoning processes.",
            "2": "(3) The experimental results demonstrate significant improvements in preference ranking and outperformance of Themis compared to baseline RMs, validating the effectiveness of the approach.",
            "3": "The description of the method is not very clear.",
            "4": "My understanding is that the reward model first generates some explanations based on the inputted question and answer, and then connects a fully connected layer to the final hidden state to produce a scalar reward."
        }
    },
    "tUM39YTRxH": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a longstanding challenge in reinforcement learning (RL) by automating the generation and shaping of dense reward functions using large language models (LLMs).",
            "1": "- The proposed framework, TEXT2REWARD, is data-free and generates interpretable, free-form dense reward codes that cover a wide range of tasks.",
            "2": "- The approach allows for iterative refinement with human feedback, enhancing the adaptability and effectiveness of the generated reward functions.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and innovative approach to reward shaping in RL, leveraging the capabilities of LLMs.",
            "4": "- The experimental results demonstrate the effectiveness of TEXT2REWARD, showing that policies trained with generated reward codes achieve comparable or better performance than expert-written codes.",
            "5": "- The framework's ability to incorporate human feedback for iterative refinement is a significant contribution, addressing the challenge of language ambiguity and improving policy success rates.",
            "6": "- The real-world applicability of the approach is demonstrated through successful deployment on a real robot, showcasing its practical relevance.",
            "7": "Potential reasons for rejection\n   - **Lack of detailed comparison with existing methods:**\n     - The paper could benefit from a more comprehensive comparison with other state-of-the-art reward shaping methods, including inverse RL and preference learning.",
            "8": "- Detailed analysis of the limitations and advantages of TEXT2REWARD compared to these methods would strengthen the paper.",
            "9": "- **Limited exploration of failure cases:**\n     - The paper does not provide an in-depth analysis of the scenarios where TEXT2REWARD fails to generate effective reward functions.",
            "10": "- Understanding the limitations and potential failure modes of the approach is crucial for assessing its robustness and generalizability.",
            "11": "- **Insufficient details on human feedback integration:**\n     - The process of integrating human feedback into the reward generation and refinement process is not thoroughly explained.",
            "12": "- Providing more details on how human feedback is collected, processed, and utilized to improve reward functions would enhance the clarity and reproducibility of the approach.",
            "13": "Suggestions for improvement\n   - **Expand the comparison with existing methods:**\n     - Include a more detailed comparison with other reward shaping techniques, such as inverse RL and preference learning, highlighting the strengths and weaknesses of TEXT2REWARD.",
            "14": "- Provide quantitative and qualitative analyses to support the comparison.",
            "15": "- **Analyze failure cases:**\n     - Conduct a thorough analysis of the scenarios where TEXT2REWARD fails to generate effective reward functions.",
            "16": "- Identify common failure modes and propose potential solutions or improvements to address these issues.",
            "17": "- **Detail the human feedback integration process:**\n     - Provide a step-by-step explanation of how human feedback is collected, processed, and integrated into the reward generation and refinement process.",
            "18": "- Include examples of feedback and how it leads to specific changes in the reward functions.",
            "19": "- **Enhance the real-world applicability section:**\n     - Provide more details on the real-world experiments, including the setup, challenges faced, and how they were addressed.",
            "20": "- Discuss the potential for scaling the approach to more complex real-world tasks and environments."
        },
        "C81YNNAhLY": {
            "0": "The paper addresses the important problem of dense reward specification in reinforcement learning, using a method based on zero-shot and few-shot prompting of a language model, something not shown before except for concurrent work **[1]**.",
            "1": "The authors show that their method is competitive with human expert reward specification in a broad series of experiments.",
            "2": "They moreover demonstrate one such policy to transfer successfully to real world execution.",
            "3": "The authors also include an analysis of the failure modes of reward specification, specifically for the cases in which the generated reward function code leads to a python runtime error.",
            "4": "**[1]** Ma et al., Eureka: Human-Level Reward Design via Coding Large Language Models, https://eureka-research.github.io/ The paper essentially proposes prompting techniques for code generation with pre-trained language models accessible via API, specifically GPT-4.",
            "5": "Despite its interesting conclusion and results, it consists at most of an interesting observational study over the capabilities of GPT-4, as the prompting techniques appear straightforward, fundamentally easy to execute manually by any user of the GPT-4 online api platform in a process of trial and error for reward design.",
            "6": "I would not consider this a problem worthy of rejection per se, as similar “prompting techniques” results have been published before in machine learning venues, such as the famous “chain of thought” prompting technique.",
            "7": "Still, it is my opinion that this limits the significance of the work, as GPT-4 simply fills in the shoes of an expert reinforcement learning reward coder on well known simulation environments (while “chain of thought” prompting was an innovation in how to query a model for more general purpose NLP benchmarks).",
            "8": "More importantly, to my understanding, no truly novel environments are tackled in this paper.",
            "9": "For most of the shown tasks, GPT-4 can reasonably be expected to draw from training data containing countless reward function specifications from those environments, if not reward functions for the desired task itself.",
            "10": "Moreover, the reliance of the paper on GPT-4 makes the results inherently irreproducible, as OpenAI does not have a policy of indefinitely supporting api access to specific snapshot versions of their models (specifically, GPT-4-0314 will be deprecated on June 13th 2024 at the earliest)."
        },
        "DrwOPh39nW": {
            "0": "************************************************************Evaluation Comprehensiveness:************************************************************ The authors utilized 17 tasks in ManiSkill2 and Metaworld, 6 in Mujoco, and also a few real world experiments — this is quite comprehensive compared to the average ICLR RL paper.",
            "1": "****************************Experiments:**************************** \n\n- Real world experiments are always nice!",
            "2": "- Human feedback experiments are also great and demonstrate that on certain tasks the method can improve the policy learning performance — something that isn’t always straightforward even with humans redesigning reward functions to try to improve polichy learning\n\n******************Results:****************** I think results are solid compared to the oracle.",
            "3": "**********Motivation:********** The paper motivation is nice — reducing human effort in scaling policy training by using LLMs for python/numpy reward function design.",
            "4": "************************************************************Contribution over prior work:************************************************************ I’m not too convinced on the major **********technical********** contribution over Yu et al.",
            "5": "2023 (Language to Rewards for Skill Synthesis).",
            "6": "Compared to that paper, the main claimed novelty is dense vs sparse reward and the use of standard python code: “Different from recent work….our free-form dense reward code has a wider coverage of tasks and can use established coding packages.” But that paper also uses very pythonic code, furthermore utilizes sparse reward mainly due to using MPC.",
            "7": "I’m not too convinced that the pythonic → python and MPC → RL are large technical contributions on their own.",
            "8": "This should be clarified more specifically/clearly in the paper if there is another technical contribution over Yu 2023, and if not, then is one of the main reasons for my score.",
            "9": "************************Experiments:************************\n\n- The authors should compare against Yu 2023, especially if claiming their dense free-form reward + use of established coding packages can result in superior performance.",
            "10": "The comparison isn’t exactly 1-1 given the claims, but currently there is no comparison to any baseline to contextualize the performance of the method.",
            "11": "In fact, I think a comparison with Yu 2023 + RL would be fairest, as Yu 2023 likely can use RL instead of MPC without change.",
            "12": "- Open-source LLMs: Utilizing closed-source LLMs has obvious downsides, e.g., reproducibility (API backend can change at any time) and access to academic researchers (cost per token vs able to be used on a standard GPU setup).",
            "13": "It would be beneficial to the community to demonstrate some results with some smaller open source models like LLaMa-2.",
            "14": "**************************Minor Issues:**************************\n\n- 4.1: Appendex → Appendix\n- I think it’d be nice to have a few small examples in the **********main paper********** of generated reward functions (not full things, just a few lines).",
            "15": "This makes the experiment section more readable without needing to jump around to the appendix."
        },
        "pCZXpMBSgf": {
            "0": "This paper studies the pertinent problem of automated reward design using LLMs.",
            "1": "Given that reward design is a fundamental challenge in RL and that LLMs for decision making have largely been limited to high-level planning tasks, this paper offers a fresh perspective and a nice solution to the growing literature of LLMs for problem design and low-level robotic control.",
            "2": "This paper's method is novel and more flexible than a prior work (Yu et al., 2023) in that it does not require extensive manual templating per robotic embodiment or task and is capable of generating free-form reward functions.",
            "3": "It incorporates a compact representation of the environment, background knowledge (e.g., available function APIs), and/or few-shot examples to successfully do so.",
            "4": "Finally, this paper demonstrates interesting use case of the proposed method, such as real-world evaluation as well as learning from human feedback.",
            "5": "The paper is well-written and free of grammatic errors.",
            "6": "The primary weakness of the paper is that most evaluation tasks are from benchmarks that have been released before GPT-4's knowledge cutoff date (September, 2021).",
            "7": "Mujoco and Metaworld tasks have been extensively studied in the reinforcement learning literature; ManiSkill2, though released recently, have many overlapping tasks with ManiSkill, which was released in mid 2021; in particular, most of the tasks, to the best of my knowledge, were in the original ManiSkill benchmark.",
            "8": "Given this, it is not clear whether the reward design capability of T2R can readily transfer to an unseen task in a new simulator.",
            "9": "Relatedly, the \"novel\" behavior on the Mujoco locomtoin tasks have appeared in prior literature; for example, Hopper back flip is shown in Christiano et al., 2017.",
            "10": "It's unclear whether T2R has benefited from that knowledge possibly being in the training set of the backbone LLM.",
            "11": "Most manipulation tasks are of \"pick-and-place\" or opening/closing/pushing nature.",
            "12": "These are also the most common types of manipulation tasks that the RL literature has studied.",
            "13": "It is possible that GPT-4 is generally adept at writing reward functions for those task types.",
            "14": "T2R appears to still work best with few-shot examples.",
            "15": "In many tasks that do not belong to a family of tasks introduced by a benchmark, providing few-shot examples can still be difficult.",
            "16": "For each task, only one reward function is reported.",
            "17": "It is not clear whether T2R is robust to stochasticity in LLM generation."
        },
        "DHH1jLUgfp": {
            "0": "- Interesting application of a new tool!",
            "1": "Using LLMs to generate reward code seems like an easy way to simplify problems we may not already have solutions for, but can describe in language, and is a completely different way around the sparse reward problem\n- Zero-shot results seem pretty strong across all environments\n- Nice results on new tasks that (as far as I know) we don't have expert reward for (e.g.",
            "2": "Hopper flipping) - There is no qualitative analysis/discussion of what the source of the improvement is:\n  - Why does Zero-shot outperform Few-shot on Turn Faucet, Open Cabinet Door, Open Cabinet Drawer?",
            "3": "- Why does Few-shot fail to outperform Oracle even though Oracle is in context (Lift Cube, Turn Faucet, Push Chair, Open Cabinet Drawer)?",
            "4": "- In the cases that few-shot improves on zero-shot, what is the source of this improvement?",
            "5": "- There are a lot of missing details in the experiments:\n  - There is only one example of human feedback (Figure 6) and it is in a schematic diagram for a task that does not have ground truth, I would like to see a few traces of the whole round (generation, feedback, generation, feedback, generation) in order to understand what exactly is happening\n  - For results in Table 1 and Figure 4 on novel behaviors, the standards for human evaluation and who conducted the evaluation (the authors, peers from the same institution, crowdsourced to a platform) are missing\n  - The experiments in Table 1 and Figure 4 are supposedly conducted in a zero-shot setting (caption in Figure 4), yet Figure 6 gives a schematic for ambiguity resolution which would imply a few-shot result for novel behaviors, an experiment which I do not see in the paper and appendices.",
            "6": "- I do not see the choice of $k$ for the number of neighbors that appear in the few-shot context\n  - For generated rewards in Appendix D.1 on Pick Cube, the few-shot vs. oracle code is almost indistinguishable except for 2 constants (cube placed at goal reward, grasped reward).",
            "7": "Given this difference is so small, it seems important to know what the human feedback was: are we just getting lucky?",
            "8": "- As before, given that the few-shot vs. oracle code is so close on Pick Cube (the only example we have to judge), why is it the case that the few-shot generation is underperforming oracle generation in other settings (Lift Cube, Turn Faucet, Open Cabinet Drawer, Push Chair)?",
            "9": "- Is it always the case that the Oracle code for the task is put into the context for Few-shot?",
            "10": "- Section 4.2 is about code errors that occur before any RL happens, and this seems like a necessary filtering step, but I think having an example of the generation/feedback process is much more important in the main body than Table 2.",
            "11": "- How long does the iteration loop take?",
            "12": "Each iteration requires training policies, so it is quite expensive, and it may be nice to think about early evaluation\n- It would be nice to include code examples for the novel behavior tasks to see what is happening.",
            "13": "Given there is no baseline in this case, simply presenting a quantitative evaluation without any analysis is a little sparse..."
        }
    },
    "EE75tyB5Ay": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical issue in the field of natural language processing: the detection of texts generated by ChatGPT, which is essential for preventing misuse such as fake news and plagiarism.",
            "1": "- The study provides a comprehensive investigation into the generalization behaviors of training-based ChatGPT detection methods under various distribution shifts, including prompts, text lengths, topics, and language tasks.",
            "2": "- The introduction of a new dataset, HC-Var, which includes a variety of human and ChatGPT-generated texts, is a significant contribution to the field.",
            "3": "Potential reasons for acceptance\n   - The paper tackles an important and timely problem, given the widespread use and potential misuse of ChatGPT.",
            "4": "- The introduction of the HC-Var dataset is a valuable resource for future research in ChatGPT detection.",
            "5": "- The extensive experiments and theoretical analysis provide deep insights into the factors affecting the generalization of ChatGPT detection models.",
            "6": "- The findings offer practical guidance for improving data collection strategies and developing more robust detection methodologies.",
            "7": "Potential reasons for rejection\n   - **Limited Scope of Language Models:**\n     - The study focuses solely on ChatGPT and does not consider other popular language models like LLaMA2 or GPT-3.",
            "8": "- The paper does not address the detection of texts that are partially generated by ChatGPT and then manipulated by other models.",
            "9": "- **Insufficient Exploration of Mixed Authorship:**\n     - The paper does not investigate scenarios where a text is partially written by a human and partially by ChatGPT.",
            "10": "- There is no discussion on the detection of programming code generated by ChatGPT, which is a significant use case.",
            "11": "- **Dataset Limitations:**\n     - While HC-Var is a valuable contribution, the paper does not provide a detailed comparison with other existing datasets in terms of size, diversity, and representativeness.",
            "12": "- The dataset may not cover all possible prompts and text lengths that users might employ in real-world scenarios.",
            "13": "Suggestions for improvement\n   - **Expand the Scope of Language Models:**\n     - Include experiments and analysis on texts generated by other language models such as LLaMA2, GPT-3, and others.",
            "14": "- Investigate the detection of texts that are a mix of human and multiple language models' contributions.",
            "15": "- **Address Mixed Authorship:**\n     - Conduct studies on the detection of texts that are partially written by humans and partially by ChatGPT.",
            "16": "- Explore the detection of programming code generated by ChatGPT, as it is a significant application area.",
            "17": "- **Enhance Dataset Comparisons:**\n     - Provide a more detailed comparison of HC-Var with other existing datasets in terms of size, diversity, and representativeness.",
            "18": "- Ensure that the dataset includes a wide range of prompts and text lengths to better simulate real-world scenarios.",
            "19": "- **Broaden Theoretical Analysis:**\n     - Extend the theoretical analysis to include more complex scenarios, such as mixed authorship and multi-model text generation.",
            "20": "- Provide more empirical evidence to support the theoretical claims, especially in diverse and unforeseen tasks and topics."
        },
        "mzihAVfCN9": {
            "0": "This work studies an important and timely problem for detecting LLM-generated content.",
            "1": "Experiments have conducted for in-distributed settings as well as OOD settings involving content length shift and topic/domain shift.",
            "2": "The feature attribution analysis is an interesting and novel angle of study in the context of LLM-generated content detection.",
            "3": "The study seeks for detecting content generated by ChatGPT, which is just an interface where the backend model keeps evolving.",
            "4": "Hence, it is hard to say if the experimental results and analysis are reproducible and sustainable.",
            "5": "In my opinion, this type of study should be conducted for a static LLM.",
            "6": "Length shift and domain/topic shift represent limited types of distribution shift that is easily detectable.",
            "7": "The authors could have considered more implicit shift where content is paraphrased with syntax-controlled paraphrasing or style transfer, like those used in recent approaches for data pollution attack / defense.",
            "8": "Typos:\n\n3.1:\ndon't -> do not"
        },
        "KUMpz1d3yx": {
            "0": "* The authors present a novel data set.",
            "1": "* The analysis is detailed and comprehensive.",
            "2": "* They provide insights on the data collection and domain adaption strategy.",
            "3": "* ChatGPT Direction seems to be not well motivated.",
            "4": "It needs a why, not just a  what and a how.",
            "5": "* This work exclusively discusses the train-based methods, which are smaller in scope."
        },
        "SZhaUPcWsw": {
            "0": "- **Comprehensive Investigation**: The paper conducts a thorough analysis of the generalization behaviors of existing methods under distribution shifts caused by various factors like prompts, text lengths, topics, and language tasks.",
            "1": "- **New Dataset**: The authors contribute to the field by collecting a new dataset containing both human and ChatGPT-generated texts, facilitating in-depth studies on the detection methods.",
            "2": "- **Insightful Findings**: The research uncovers insightful findings, providing valuable guidance for the development of future methodologies and data collection strategies for ChatGPT detection.",
            "3": "- The authors used three prompts in Figure 1; but in general, users might use various prompts.",
            "4": "This is not aligned with real user usage,\n- The experiments are limited to CHATGPT, we do not know whether these conclusions still hold in GPT4 or other open-source LLMs.",
            "5": "- Most findinds seem obvious."
        }
    },
    "vwSxJEq8VO": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a longstanding challenge in machine learning: translating natural language descriptions into compilable code.",
            "1": "- Introduces a novel framework that iteratively transforms natural language task descriptions into code using high-level machine learning instructions.",
            "2": "- Fine-tunes the Llama model to rank and select appropriate solutions for given tasks, showcasing a significant step towards automating code generation.",
            "3": "Potential reasons for acceptance\n   - The framework demonstrates a novel approach to bridging the gap between natural language descriptions and executable code.",
            "4": "- The use of fine-tuning Llama and integrating Smart-GPT for refining instructions is innovative and shows promise in improving code generation quality.",
            "5": "- The experimental results validate the effectiveness of the approach, with generated code performing well on Kaggle competitions.",
            "6": "- The paper provides detailed insights into the process and methodology, contributing valuable knowledge to the field of machine learning and code generation.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology**\n     - The paper could benefit from more detailed explanations of the fine-tuning process and the specific modifications made to the Llama model.",
            "8": "- The description of the Smart-GPT technique and its integration into the framework is somewhat vague and could be elaborated further.",
            "9": "- **Limited scope of experimental validation**\n     - The experiments are limited to a few Kaggle competitions, which may not fully represent the diversity of machine learning tasks and datasets.",
            "10": "- The paper does not provide a comprehensive comparison with other state-of-the-art code generation models, which would strengthen the validation of the proposed approach.",
            "11": "- **Potential over-reliance on specific datasets**\n     - The reliance on the Code4ML dataset may limit the generalizability of the approach to other datasets and domains.",
            "12": "- The paper does not discuss potential limitations or challenges in applying the framework to different types of machine learning tasks or datasets.",
            "13": "Suggestions for improvement\n   - **Enhance methodological clarity**\n     - Provide a more detailed explanation of the fine-tuning process for the Llama model, including specific parameters and modifications.",
            "14": "- Elaborate on the Smart-GPT technique, including how it identifies and corrects logical errors in the instructions.",
            "15": "- **Expand experimental validation**\n     - Include a broader range of machine learning tasks and datasets in the experimental validation to demonstrate the generalizability of the approach.",
            "16": "- Provide a comparative analysis with other state-of-the-art code generation models to highlight the advantages and potential limitations of the proposed framework.",
            "17": "- **Address potential limitations**\n     - Discuss the potential challenges and limitations of applying the framework to different types of machine learning tasks and datasets.",
            "18": "- Explore the impact of varying the quality and complexity of natural language task descriptions on the performance of the framework.",
            "19": "- **Improve presentation and readability**\n     - Ensure that all figures and tables are clearly labeled and referenced in the text for better readability.",
            "20": "- Consider providing more examples and case studies to illustrate the practical application and effectiveness of the framework."
        },
        "QmhPVCvjSw": {
            "0": "- I found the decomposition of broad task descriptions into concrete subsection instructions interesting and it seems like a decent idea to explore.",
            "1": "- I don't think this work is anywhere close to publication.",
            "2": "Honestly, it feels more like a project report submitted for publication, and it does not hold the rigor required for publication.",
            "3": "- There are some major issues with the paper, some of which I list below:\n  - Related work: The entire related work section needs to be more thorough and relevant to the domain.",
            "4": "Authors have cited works like the Sparks of AGI, Codex, and Palm papers, but do not mention numerous other works such as CodeT5, StarCoder etc.",
            "5": "- Seemingly random papers were mentioned in the Related work, such as [1].",
            "6": "I could not figure out how this work explored zero-shot learning and how zero-shot learning extends the capabilities of LLMs to generate code in unseen languages!",
            "7": "- The entire field of AutoML has not been mentioned, even though they do something similar to what this work aims to do.",
            "8": "- Approach: The entire paper is NL to ML code using GPT.",
            "9": "Llama is used for prompt generation given natural language instructions, but the training data for it comes from the GPT model.",
            "10": "Why can't the GPT model be used to decompose the description for the test data itself, especially when generating code from detailed instructions is done by the GPT model?",
            "11": "What is the utility of the fine-tuned Llama model?",
            "12": "- Baselines: There are no baselines provided.",
            "13": "How does the GPT model do without any prompt decomposition?",
            "14": "How do other models perform?",
            "15": "- Test set size: Results are presented for 5 competitions.",
            "16": "That is not enough sample size to judge the performance.",
            "17": "- Overall, the paper text does not do a good job of describing the approach well and it needs to be improved.",
            "18": "References:\n\n[1] - Thirunavukarasu, Arun James.",
            "19": "\"Large language models will not replace healthcare professionals: curbing popular fears and hype.\"",
            "20": "Journal of the Royal Society of Medicine (2023): 01410768231173123."
        },
        "H8Dd2vgGKa": {
            "0": "The topic is very interesting.",
            "1": "The authors try to design a framework that can solve ML tasks with large language models.",
            "2": "Several parts look good, for example, first create high-level code instructions, then transform these instructions into Python code.",
            "3": "In Table 1, the authors showed reasonable results in five competitions.",
            "4": "The presentation is bad.",
            "5": "Several parts are unclear or missing.",
            "6": "For example, what are the inputs for prompts in Figure 3 and Figure 4?",
            "7": "What is \"**Kaggle code**\"?",
            "8": "What is your training data from?",
            "9": "In Section 4.1, the authors say that GPT-3 is used to generate high-level instructions.",
            "10": "However, in Figure 1, it looks like OpenAI models are used.",
            "11": "It is important that the authors have a clear description.",
            "12": "There are no detailed performance results: the performance of high-level code instruction creation or the quality of code generation.",
            "13": "It is not enough to have sample results on 5 competitions ( shown in Table 1).",
            "14": "There is no baseline in this submission."
        },
        "EhQvj6lB8M": {
            "0": "* Two-phase approach: The two-step approach of instructions followed by code generation provides flexibility and control lacking in other text-to-code techniques.",
            "1": "* Modular design: The overall pipeline is modular with clear interfaces between the stages.",
            "2": "Limited evaluation: The framework was only evaluated on a small set of unseen Kaggle competitions.",
            "3": "More comprehensive evaluation on diverse ML tasks could strengthen the results.",
            "4": "No comparison to other text-to-code methods: The paper does not provide any quantitative comparison to other natural language to code generation techniques like PaLM-Coder, Code Llama, StarCoder etc.",
            "5": "This could have demonstrated superiority.",
            "6": "Lack of ablation studies: Ablation studies could have isolated the contributions of different components like SmartGPT, inference with Llama, etc.",
            "7": "This would give more insights.",
            "8": "No analysis of generated code quality: Beyond correctness, metrics analyzing code quality like modularity, comments, naming conventions could have provided more insights.",
            "9": "No discussion of limitations: The conclusion does not discuss any limitations of the current approach or challenges that need to be addressed in future work.",
            "10": "Discussing limitations would have provided a balanced perspective.",
            "11": "Overall, while the paper introduces a novel framework, more rigorous evaluation and comparisons to other techniques could have strengthened the results.",
            "12": "Providing more analyses of the generated code quality and limitations of the approach could have added valuable insights as well.",
            "13": "Expanding the evaluation to more diverse tasks remains an area of future work."
        },
        "tq6nxcRybK": {
            "0": "The paper presents a novel framework that addresses the challenge of transforming natural language machine learning task descriptions into executable code, leveraging large language models.",
            "1": "This approach marks a significant contribution to the field of code generation and machine learning.",
            "2": "The paper demonstrates a well-designed framework, incorporating fine-tuning of the Llama model and instruction-based sequential generation.",
            "3": "The experimental evaluations showcase the effectiveness of the approach, with promising results in the machine learning domain.",
            "4": "The paper is well-structured and easy to understand, providing a clear explanation of the proposed framework, its components, and the underlying methodology.",
            "5": "The use of figures and tables effectively illustrates the approach and the results.",
            "6": "The proposed framework has the potential to advance machine learning applications across diverse domains by automating code generation and bridging the gap between task descriptions and executable code.",
            "7": "This work holds promise for improving the efficiency and accessibility of machine learning development.",
            "8": "One potential weakness of the paper is the occasional suboptimal performance of SmartGPT compared to unprocessed Llama instructions.",
            "9": "This inconsistency indicates the need for further refinement and evaluation of the algorithmic approach employed by SmartGPT to ensure consistently high-quality solutions.",
            "10": "Another limitation might be the generalizability of the framework.",
            "11": "While the paper demonstrates promising results in the machine learning domain, it would be valuable to explore its applicability and effectiveness across a wider range of tasks and domains.",
            "12": "Lastly, the paper could benefit from a more comprehensive comparison with existing approaches in code generation and text-to-code conversion.",
            "13": "Providing a deeper analysis of the framework's strengths and weaknesses in relation to other methods would help establish its position in the field and identify areas for further improvement."
        }
    },
    "tmBKIecDE9": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Motif, a novel method that leverages intrinsic motivation from artificial intelligence feedback to train reinforcement learning agents.",
            "1": "- The approach bridges the gap between high-level knowledge from Large Language Models (LLMs) and low-level decision-making in complex environments.",
            "2": "- The method is evaluated on the challenging NetHack game, demonstrating significant improvements over existing approaches.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel and generalizable method for integrating LLMs into reinforcement learning, which could have broad implications for AI research.",
            "4": "- The experimental results show that Motif outperforms existing methods on both dense and sparse reward tasks, highlighting its effectiveness.",
            "5": "- The paper provides a thorough analysis of the behaviors induced by Motif, demonstrating its alignment with human intuition and its ability to generate diverse policies through prompt modifications.",
            "6": "- The method's scalability with LLM size and sensitivity to prompt variations are well-documented, showcasing its robustness and flexibility.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in methodology:**\n     - The description of the dataset generation and annotation process could be more detailed to ensure reproducibility.",
            "8": "- The specifics of the reward model architecture and post-processing steps are not fully elaborated, which may hinder understanding and replication.",
            "9": "- **Limited comparison with other state-of-the-art methods:**\n     - While the paper compares Motif with RND and other baselines, it does not include a comprehensive comparison with the latest advancements in intrinsic motivation and LLM-based reinforcement learning.",
            "10": "- The paper could benefit from a broader evaluation on different environments beyond NetHack to demonstrate the generalizability of the method.",
            "11": "- **Potential over-reliance on LLMs:**\n     - The method's dependence on the quality and size of the LLM raises concerns about its applicability in scenarios where such models are not available or feasible to use.",
            "12": "- The sensitivity to prompt variations, while an interesting aspect, also indicates potential instability in the method's performance, which could be problematic in real-world applications.",
            "13": "Suggestions for improvement\n   - **Enhance methodological clarity:**\n     - Provide a more detailed explanation of the dataset generation, annotation process, and reward model architecture to improve reproducibility.",
            "14": "- Include pseudocode or a flowchart to visually represent the steps involved in implementing Motif.",
            "15": "- **Expand experimental evaluation:**\n     - Compare Motif with a wider range of state-of-the-art methods in intrinsic motivation and LLM-based reinforcement learning to strengthen the validity of the results.",
            "16": "- Test the method on additional environments with varying levels of complexity to demonstrate its generalizability and robustness.",
            "17": "- **Address potential limitations:**\n     - Discuss the implications of the method's reliance on large LLMs and explore potential solutions for scenarios where such models are not available.",
            "18": "- Investigate ways to mitigate the sensitivity to prompt variations, possibly by developing more stable and robust prompting techniques.",
            "19": "- **Provide additional insights:**\n     - Include a more in-depth analysis of the misalignment by composition phenomenon and propose potential strategies to address it.",
            "20": "- Explore the ethical considerations and potential risks associated with using LLMs for intrinsic motivation in reinforcement learning, and suggest guidelines for responsible use."
        },
        "ybgpiwvG1G": {
            "0": "### quality and clarity\n- This paper is well-written and easy to follow.",
            "1": "### significance\n- The empirical results are strong.",
            "2": "It would be notable to solve the difficult, sparse-reward NetHack environments that previous intrinsic-motivation methods cannot solve by leveraging preference-based reward.",
            "3": "- I think the point of this paper is that \"joint optimization of preference-based and extrinsic reward helps resolve the sparse reward problems\".",
            "4": "As the source of feedback, either humans or LLMs are OK.",
            "5": "I think describing this as LLM's contribution might be an overstatement.",
            "6": "- As a preference-based RL method, I guess there are no differences from the original paper [1].",
            "7": "In the LLM literature, [2] leverages GPT-4 to solve game environments, and [3] incorporates LLM-based rewards for RL pretraining.",
            "8": "- Terminology: I'm not sure if a preference-based reward should be treated as an \"intrinsic\" reward.",
            "9": "I think it is extrinsic knowledge (from humans or LLM).",
            "10": "[1] https://arxiv.org/abs/1706.03741\n\n[2] https://arxiv.org/abs/2305.16291\n\n[3] https://arxiv.org/abs/2302.06692"
        },
        "QAyb3tGtSA": {
            "0": "- S1.",
            "1": "First of all, this paper is well-written and well-organized.",
            "2": "- S2.",
            "3": "The idea of using a LLM as a preference annotator for preference-based RL is interesting and promising.",
            "4": "- S3.",
            "5": "This paper provides a loss function (equation 1) to train an intrinsic reward model.",
            "6": "- S4.",
            "7": "This paper shows that training agents with intrinsic rewards is very effective.",
            "8": "- W1.",
            "9": "One of my main questions is whether Motif can be generally applied to other environments.",
            "10": "Even though the NetHack Learning Environment (NLE) is a very challenging environment, it seems that the NLE may be one of environments that a LLM can easily annotate preferences."
        },
        "YmGIe1M2XC": {
            "0": "The paper is clear and well presented.",
            "1": "The idea of using intrinsic rewards generated from an LLM's preferences is both innovative and practically useful, potentially paving the way for more human-aligned agents.",
            "2": "The method scales well with the size of the LLM and is sensitive to prompt modifications, offering flexibility and adaptability.",
            "3": "The paper provides a comprehensive analysis, covering not just the quantitative but also the qualitative behaviors of the agents.",
            "4": "The paper could benefit from a more extensive comparison to other methods, especially those that also attempt to integrate LLMs into decision-making agents.",
            "5": "There is a lack of discussion on the computational cost and efficiency aspects of implementing Motif.",
            "6": "While the paper makes a strong case for Motif, it doesn't delve deeply into the limitations or potential drawbacks of relying on LLMs for intrinsic reward generation."
        },
        "VZz21uJuD5": {
            "0": "- Figure 1 provides a nice clean bird's eye view of the overall approach and helps with readability.",
            "1": "- The evaluation of the agent for not just the game score but also other dimensions provides a helpful qualitative assessment of the proposed approach and baselines through the spider graph in Figure 4.",
            "2": "- The ablation experiments for the approach are quite exhaustive, covering scaling laws, prior v/s zero knowledge, rewordings of the prompts, etc.",
            "3": "- Using a 70-billion LLM to generate a preference dataset from given captions is quite expensive; while I understand this is out of the scope of the paper, perhaps using a large VLM to annotate frames without captions might have been more economical?",
            "4": "- Given that one of the key contributions of the paper is the intrinsic reward function that is learnt from preferences extracted from the LLM, it might be worthwhile having a baseline that gives preferences using a simpler model (say sentiment analysis) and learn the RL policy using this intrinsic reward model."
        }
    },
    "LAEd3kHao9": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenging task of compositional zero-shot learning (CZSL), which is significant for recognizing novel visual concepts without extensive training data.",
            "1": "- The proposed method, PLID, introduces a novel approach by leveraging large language models (LLMs) to generate diverse and informative class descriptions, enhancing the generalization to unseen compositions.",
            "2": "- The integration of visual-language primitive decomposition (VLPD) and stochastic logit mixup (SLM) strategies further distinguishes this work from existing methods.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-motivated and novel approach to CZSL, addressing key limitations of existing methods.",
            "4": "- Experimental results on multiple datasets (MIT-States, UT-Zappos, and C-GQA) demonstrate the superior performance of the proposed method compared to state-of-the-art approaches.",
            "5": "- The proposed method is shown to be parameter-efficient and capable of capturing intra-class diversity and inter-class separability, which are crucial for CZSL.",
            "6": "- The paper provides a comprehensive analysis of the proposed method, including ablation studies and qualitative results, which strengthen the validity of the approach.",
            "7": "Potential reasons for rejection\n   - **Complexity and clarity of the method:**\n     - The proposed method involves multiple components (LID, TFE, VFE, VLPD, SLM), which may make it difficult for readers to fully grasp the overall approach.",
            "8": "- The explanation of certain components, such as the stochastic logit mixup (SLM) and its impact on the final performance, could be more detailed and clearer.",
            "9": "- **Generality and applicability:**\n     - While the method shows strong performance on the selected datasets, it is not clear how well it would generalize to other types of compositional tasks or domains.",
            "10": "- The reliance on large language models (LLMs) for generating class descriptions may limit the applicability of the method in scenarios where such models are not available or feasible to use.",
            "11": "- **Evaluation and comparison:**\n     - The paper primarily compares the proposed method with other CLIP-based approaches.",
            "12": "It would be beneficial to include comparisons with non-CLIP-based CZSL methods to provide a broader perspective on the performance improvements.",
            "13": "- The evaluation focuses on standard metrics, but additional analysis on computational efficiency and scalability could provide a more comprehensive understanding of the method's practicality.",
            "14": "Suggestions for improvement\n   - **Simplify and clarify the method:**\n     - Provide a more concise and clear explanation of each component of the proposed method, possibly with visual aids or diagrams to illustrate the workflow.",
            "15": "- Include a detailed discussion on the role and impact of each component (LID, TFE, VFE, VLPD, SLM) on the overall performance.",
            "16": "- **Expand evaluation and comparison:**\n     - Include comparisons with a wider range of CZSL methods, including non-CLIP-based approaches, to highlight the general advantages of the proposed method.",
            "17": "- Provide additional analysis on the computational efficiency and scalability of the method, especially in terms of training and inference times.",
            "18": "- **Generalization and applicability:**\n     - Discuss the potential limitations and challenges of applying the proposed method to other types of compositional tasks or domains.",
            "19": "- Explore the feasibility of using alternative sources of class descriptions in scenarios where large language models are not available.",
            "20": "- **Detailed analysis of SLM:**\n     - Provide a more in-depth explanation of the stochastic logit mixup (SLM) strategy, including its theoretical basis and practical impact on the performance.",
            "21": "- Include additional experiments to analyze the sensitivity of the method to the choice of Beta distribution parameters and their effect on the final results."
        },
        "A3oky3xA9M": {
            "0": "1.The paper is organized and clearly written.",
            "1": "2.The proposed method seems to be intuitively reasonable.",
            "2": "1.The proposed method relies much on the quality of LLMs, and the transferability of the model is not reflected in the paper.",
            "3": "2.According to the Ablation study, the experiment w/o VLPD does not change much (even the H_cw value decreases).",
            "4": "3.The proposed languageinformed distributions (LID) can effectively avoid the issue of intra-class variety.",
            "5": "However, the authors would better also intepret how to solve the issue of inter-class correlation."
        },
        "Gfs4cWMJm4": {
            "0": "The motivations of suggested primitive decomposition and stochastic logit mixup are okay, which have positive effects on zero-shot compositional visual recognition.",
            "1": "The paper is well presented and the suggested method slightly exceeds the comparison method.",
            "2": "The authors did not mention whether the comparison methods also used N-view augmentation.",
            "3": "If the answer is no, I think the comparison is unfair.",
            "4": "Please indicate which method uses the same augmentation.",
            "5": "The author mentioned in the ablation study that LID significantly improves performance compared to the baseline is confusing.",
            "6": "According to Table 2, TFE and VFE seem to have little effect, while LID appears to be effective, and its effectiveness should come from LLM, which in this paper belongs to incremental engineering.",
            "7": "Figure 2 should be consistent with its caption.",
            "8": "For example, Figure 2 should provide the corresponding parts of LID, which should be more helpful for understanding."
        },
        "6jZPRIVAsx": {
            "0": "Despite the method having several building blocks, the idea and motivation behind each introduced component are well-described in the text: e.g.",
            "1": "LLM is used to create a pool of sentences describing the compositions, which in turn is used to estimate a distribution that can be in turn used to estimate pairwise-margins between compositions.",
            "2": "Fig.",
            "3": "1 also helps the reader understand the starting idea for the model (i.e.",
            "4": "composition ambiguities), and why a distribution over the language space may help deal with such uncertainty.",
            "5": "The experiments show that the proposed approach (PLID) surpasses by a margin all competitors in all settings, especially in the closed-world scenario where unseen compositions are known (Table 1).",
            "6": "In this latter setting, the gap in AUC is remarkable, with 1.5 points improvement on MIT-states, 2.7 on UT-Zappos, and 0.5 on C-GQA.",
            "7": "Sections 2, 3, and 4 give credit to the approaches the method builds on, presenting a detailed overview of the literature and the proposed contributions.",
            "8": "I have two main concerns regarding the experimental analysis and the presentation.",
            "9": "For the former, the proposed method contains several components (i.e.",
            "10": "LID, TFE, VFE, VLPD, SLM).",
            "11": "While all components have motivations justifying their use, each of them has specific design choices whose impact is not fully clear from Section 5.2.",
            "12": "Examples are:\n1.",
            "13": "The gap between considering and not considering distributional-based margins in Eq.",
            "14": "(1) and Eq.",
            "15": "(4) is mild accordingly is mild according to Table 4 (i.e.",
            "16": "gap lower than 0.5 points but for harmonic mean OW).",
            "17": "Fig.",
            "18": "5.a shows that indeed going from 4 to 64 LLM-based sentences improves the overall results but less than 1 point and less in the more challenging OW setting.",
            "19": "Given that querying LLM is costly in this setting (i.e.",
            "20": "the number C of compositions might be in the order of thousands) and can be noisy (as per Appendix A), it is questionable whether LLMs and distributional semantics are crucial for the approach (as suggested by the title).",
            "21": "The ablations are also conducted on the MIT-states dataset which is known to be noisy (Atzmon et al.",
            "22": "2020), thus it should be verified if the findings hold across datasets.",
            "23": "Related to the previous points, TFE and VFE are modules that refine visual/textual embeddings.",
            "24": "The article does not contain ablations on their number of parameters/design choices and the improvement from the added views is mild (i.e.",
            "25": "less than 0.5 points on Fig.",
            "26": "5b).",
            "27": "Ablating variants of these modules, potentially taking out the set of text embeddings/views and focusing on their parameters (e.g.",
            "28": "even via MLPs, etc.)",
            "29": "would strengthen the need for their implementation as cross-modal blocks and also of their specific input choices.",
            "30": "This applies also to the specific implementations of the prediction modules $f_s$ and $f_o$ in Eq.",
            "31": "(2).",
            "32": "The SLM module should provide flexibility to the model regarding which predictions to trust.",
            "33": "However, SLM is not compared to other aggregation strategies (e.g.",
            "34": "average, max, product) and on the single scoring mechanism (compositional vs primitive-based).",
            "35": "Thus, it is hard to assess the need for this module.",
            "36": "Regarding the presentation: from the abstract and introduction it is unclear what is the role of LLMs/how they are used.",
            "37": "It is implied that a language-informed class distribution is used, but not how this is achieved (or kept this information generic in Section 1).",
            "38": "This is not a major weakness per-se, but given that the title focuses on this distribution, it would be helpful to give hints on how this distribution is estimated already at the beginning of the manuscript, clarifying the methodological idea to the reader.",
            "39": "This is a purpose that Fig.",
            "40": "1 serves well, but the text does not stress.",
            "41": "The notation of Section 3 is not straightforward to follow.",
            "42": "The main reason is not the lack of explanations (each term is properly defined) but the number of terms defined that the reader should remember to fully appreciate the method.",
            "43": "While I understand that it is not easy to make the notation simpler given the presence of multiple components, in some cases, the notation could be simplified.",
            "44": "For instance, the name of the text embeddings is detached from their inputs (e.g.",
            "45": "S becomes D, [p:..] becomes q, x and X become v, etc.).",
            "46": "The end of the VLPD part introduces $\\mathbf{h}$ elements and $\\mathbf{H}$, that could be replaced by simply stating that $h^{rc}_y = h_s + h_o$ (even directly on Eq.",
            "47": "(4)).",
            "48": "These are (arguable and probably subjective) examples on how some of the elements could be not defined and/or the notation simplified.",
            "49": "Minors:\n- The qualitative results in Fig.",
            "50": "8 do not provide specific insights on the model as the predictions are only compared with the ground truth.",
            "51": "As in Fig.",
            "52": "7, it would have been more helpful to investigate how predictions are affected by different design choices.",
            "53": "- Fig.",
            "54": "2, caption, \"VEF\" vs \"VFE\"."
        },
        "YvJGnLKC2U": {
            "0": "The overall organization is reasonable, and the writing is good.",
            "1": "The class-wise distribution modeling and its afterward alignment with the image modal is novel.",
            "2": "Sufficient experiments and ablation studies are performed.",
            "3": "The motivation for modeling class distribution is not new, i.e., it has been proposed and used in the vision-language model.",
            "4": "Also, the augmentation to image input seems like a test-time adaptation strategy that is also explored in the community.",
            "5": "These aspects degrade the contribution to the community.",
            "6": "The framework incrementally follows ProDA by introducing D^{(y)} and introduces VLPD by compositing v which is also widely used in CZSL.",
            "7": "The framework seems like a combination of many existing techniques.",
            "8": "This degrades its novelty.",
            "9": "Why A in Eq.",
            "10": "1 is defined like that?",
            "11": "Is the dimension shape of A correct by defining it as A_k,y?",
            "12": "What’s the shape of A_k,y?",
            "13": "Some parameter analysis is shown, however, the results w.r.t.",
            "14": "the value of N = 0 is not shown.",
            "15": "Also, how to balance the tradeoff between different losses.",
            "16": "The final training loss is not shown."
        }
    },
    "LY3ukUANko": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical gap in the performance of gated-convolution language models compared to attention-based models, specifically in the context of associative recall.",
            "1": "- It introduces a new formalization of the associative recall task, termed multi-query associative recall (MQAR), which better reflects real language modeling challenges.",
            "2": "- The study provides both empirical and theoretical insights into the parameter-efficiency and performance differences between attention and gated-convolution models.",
            "3": "Potential reasons for acceptance\n   - The paper tackles a significant problem in the field of efficient language models, providing a thorough analysis of the associative recall gap.",
            "4": "- It offers a novel task (MQAR) and demonstrates its relevance through extensive empirical evaluations.",
            "5": "- The proposed hybrid models show promising results in closing the performance gap while maintaining efficiency, which could have practical implications for future model designs.",
            "6": "- The theoretical contributions, including the formalization of the MQAR task and the analysis of gated-convolution capacity, add depth to the understanding of these architectures.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability:**\n     - The paper is dense with technical details, which might make it challenging for readers to follow the main arguments and contributions.",
            "8": "- Some sections, particularly the theoretical analysis, could benefit from clearer explanations and more intuitive descriptions.",
            "9": "- **Experimental validation:**\n     - While the paper presents extensive empirical results, it would be beneficial to see more diverse datasets beyond the Pile to validate the generalizability of the findings.",
            "10": "- The evaluation of hybrid models could be expanded to include more variations and ablation studies to better understand the impact of different components.",
            "11": "- **Practical implications:**\n     - The paper focuses heavily on theoretical and synthetic tasks, which might limit its immediate practical applicability.",
            "12": "More real-world applications and case studies could strengthen the impact of the work.",
            "13": "- The proposed solutions, while effective, might still be complex to implement in practice.",
            "14": "Simplifying the implementation or providing more detailed guidelines could enhance the usability of the findings.",
            "15": "Suggestions for improvement\n   - **Improve clarity and readability:**\n     - Simplify the presentation of theoretical results and provide more intuitive explanations to make the paper accessible to a broader audience.",
            "16": "- Use diagrams and visual aids to illustrate key concepts and findings, particularly in the theoretical sections.",
            "17": "- **Expand experimental validation:**\n     - Include additional datasets from different domains to demonstrate the generalizability of the proposed MQAR task and hybrid models.",
            "18": "- Conduct more ablation studies to isolate the effects of different components in the hybrid models and provide a deeper understanding of their contributions.",
            "19": "- **Enhance practical implications:**\n     - Provide more real-world examples and case studies to showcase the practical benefits of the proposed solutions.",
            "20": "- Offer detailed implementation guidelines and best practices to help practitioners adopt the proposed hybrid models in their own work.",
            "21": "- **Address potential limitations:**\n     - Discuss the limitations of the proposed approaches, such as potential scalability issues or challenges in training hybrid models, and suggest possible solutions or future research directions.",
            "22": "- Consider the impact of different hyperparameters and model configurations on the performance of the hybrid models and provide recommendations for optimal settings."
        },
        "tffThhYAox": {
            "0": "The studied problem is important and may have a big impact.",
            "1": "The pinpointed failure model (i.e., associative recall) is novel and reasonable.",
            "2": "Both empirical and theoretical studies are conducted to support the argument.",
            "3": "To further demonstrate the impact of the analyses, the authors examine two alternative strategies, which support the intuition of the author.",
            "4": "The proposed attention hybrid method seems to perform well in the experiment.",
            "5": "However, it is not clear how it would perform on a larger scale."
        },
        "luL4VyJwP5": {
            "0": "Using the bigram frequency and the test perplexity of real data was insightful.",
            "1": "There's a lot of typos and confusing writing.",
            "2": "It's hard to properly understand all the theoretical claims of the work.",
            "3": "For starters, Proposition 4.3 should say `u \\in \\{ 0, 1 \\}^{N \\times \\log c}`.",
            "4": "The description of definition 3.1 could be improved; a simple example would be quite helpful.",
            "5": "In appendix, `N` sometimes means the entire sequence length or the number of triplets in MQAR; this confusion is exacerbated by the fact that the meaning changes in the same theorem/proof.",
            "6": "Page 6 states \"[e]ach BaseConv layer uses O(Nd) parameters and ... O(Nd) operations\".",
            "7": "I believe it uses `O(Nd +d^2)` parameters and `O(d N log N + Nd^2)` FLOPs?",
            "8": "This makes me believe that the rest of the theoretical results may have to be carefully revisited by the authors.",
            "9": "Page 26.",
            "10": "Proof of C.19.",
            "11": "It's unclear how `Q[i, :]` can be set to a zero vector when `i \\notin Q` (also boldface 0 suffices to express a vector; no need for a superscript d; also, d is defined as `log(C)` in the same page but `C` is a set.",
            "12": "`d = log(c)` small c since `c := |C|`.)",
            "13": "and similarly for `K` and `V`, because QKV is actually a linear projection of the input.",
            "14": "Linear projection can not implement this non-linear operation of masking some of the activations out.",
            "15": "There's a sentence that reads `... where Q, K, V ... are positional embeddings ...`.",
            "16": "They are different projections of u.",
            "17": "The calculation of the percentage of gap due to AR hits could be better motivated and justified in appendix.",
            "18": "Other minor typos:\n - `u * k = FFT^{-1} (FFT(u) FFT(k))` (should drop the convolution operation in frequency domain).",
            "19": "- Page 5: Other tokens: ... `1,000` => `1250`."
        },
        "LL4kNOsUMV": {
            "0": "The paper provides a comprehensive study of the associative recall capability of neural language models with different neural architectures, and examines its impact on the next token prediction performance of the models on real world data.",
            "1": "The authors empirically demonstrate that boosting the associative recall capability of GCMs can mostly bridge its performance gap with the attention-based model under the scale of 360M parameters.",
            "2": "The authors derive a theoretical scaling bound for data-independent GCMs to solve AR, and validate it with synthetic data.",
            "3": "The novelty of the paper is limited.",
            "4": "The lack of AR ability of State Space Models (which is a special kind of long convolution model with embedded recurrency) has been analyzed in the H3 paper [1] through synthetic data.",
            "5": "The proposed architectural modification of hybridization is a simple replication of the Hybrid-H3.",
            "6": "The scale of the experiments is limited.",
            "7": "The authors only empirically examine their hypothesis for models with the size up to 360M number of parameters.",
            "8": "It is not clear whether their claims still holds empirically given the shrinking trend of the performance gap that can already be observed under the current setting.",
            "9": "The paper does not provide important technical details for reproducibility.",
            "10": "The implementation details of the proposed modification of selectively look-up is missing.",
            "11": "The research problem that the authors are trying to solve has been alleviated with existing [1,2] or emerging solutions [3,4].",
            "12": "The authors do not examine the empirical AR ability of data-dependant convolutions by claiming technical difficulties, but there does exist data-dependant SSMs, such as Liquid-S4 [2], that support causal language modeling.",
            "13": "Not to mention the latest GCM, Monarch Mixer [3], that also supports causality.",
            "14": "On hybridization, a previous work [4] on dynamic input subset selection for attention modules has also been proposed for efficiently combining SSMs with attention.",
            "15": "The authors should consider comparing the proposed architectural modifications with these works to avoid being outdated upon publication.",
            "16": "---\n[1] Hungry Hungry Hippos: Towards Language Modeling with State Space Models (ICLR 2023)\n\n[2] Liquid Structural State-Space Models (ICLR 2023)\n\n[3] Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture (NeurIPS 2023)\n\n[4] Sparse Modular Activation for Efficient Sequence Modeling (NeurIPS 2023)"
        }
    },
    "hESD2NJFg8": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant challenge in node classification on graphs, particularly the need for high-quality labels, by proposing a novel pipeline that leverages Large Language Models (LLMs) for label-free node classification.",
            "1": "- The proposed LLM-GNN pipeline combines the strengths of both Graph Neural Networks (GNNs) and LLMs, aiming to mitigate their respective limitations.",
            "2": "- The novelty lies in the integration of LLMs for annotating a small portion of nodes and using these annotations to train GNNs, along with the development of a difficulty-aware active selection and confidence-based post-filtering strategy.",
            "3": "Potential reasons for acceptance\n   - The paper presents a novel approach to address the challenge of label scarcity in node classification tasks, which is a critical issue in the field.",
            "4": "- The proposed LLM-GNN pipeline demonstrates promising results, achieving high accuracy on large-scale datasets with minimal cost.",
            "5": "- The methodology is well-explained, and the experimental results are comprehensive, validating the effectiveness of the proposed approach.",
            "6": "- The integration of LLMs for generating annotations and the use of confidence scores to refine these annotations is a novel and practical contribution.",
            "7": "Potential reasons for rejection\n   - **Complexity and Scalability:**\n     - The proposed pipeline involves multiple components, including difficulty-aware active selection, confidence-aware annotations, and post-filtering, which may introduce complexity in implementation and scalability.",
            "8": "- The scalability of the approach to extremely large datasets or real-time applications is not thoroughly discussed.",
            "9": "- **Dependence on LLMs:**\n     - The reliance on LLMs for generating annotations may limit the applicability of the approach in scenarios where access to powerful LLMs is restricted or costly.",
            "10": "- The cost-effectiveness of using LLMs, despite the reported low cost in the experiments, may vary depending on the specific LLM used and the scale of the dataset.",
            "11": "- **Evaluation and Comparison:**\n     - The comparison with other label-free node classification methods is limited, and the paper could benefit from a more extensive evaluation against a broader range of baseline methods.",
            "12": "- The impact of different hyperparameters and the sensitivity of the proposed approach to these parameters are not thoroughly explored.",
            "13": "Suggestions for improvement\n   - **Simplify and Clarify Methodology:**\n     - Simplify the explanation of the proposed pipeline and provide a more detailed step-by-step guide to enhance reproducibility.",
            "14": "- Include a flowchart or diagram to visually represent the entire pipeline, highlighting the interactions between different components.",
            "15": "- **Scalability and Real-World Applicability:**\n     - Discuss the scalability of the proposed approach in more detail, particularly for extremely large datasets and real-time applications.",
            "16": "- Provide insights into the computational requirements and potential bottlenecks when scaling the approach.",
            "17": "- **Broaden Evaluation:**\n     - Conduct a more extensive evaluation by comparing the proposed approach with a wider range of baseline methods, including traditional and state-of-the-art node classification techniques.",
            "18": "- Explore the impact of different hyperparameters and provide a sensitivity analysis to understand the robustness of the proposed approach.",
            "19": "- **Cost Analysis:**\n     - Provide a detailed cost analysis, considering different LLMs and varying scales of datasets, to offer a clearer understanding of the cost-effectiveness of the approach.",
            "20": "- Discuss potential strategies to further reduce the cost of using LLMs for annotation, such as optimizing the prompts or using smaller, more efficient LLMs."
        },
        "c2ESuNg2pC": {
            "0": "LLM-GNN presents an innovative approach to node classification on graphs by harnessing the complementary strengths of GNNs and LLMs.",
            "1": "It acknowledges the challenges of obtaining high-quality labels and proposes a label-free solution, which is a significant contribution to the field of machine learning.",
            "2": "The paper demonstrates the cost-effectiveness of LLM-GNN by achieving high accuracy on a large dataset with annotation costs under 1 dollar.",
            "3": "This cost-efficient approach is particularly relevant for real-world applications with resource constraints.",
            "4": "LLM-GNN offers a comprehensive methodology that not only utilizes LLMs for annotations but also considers active node selection, confidence-aware annotations, and post-filtering.",
            "5": "This approach ensures the quality, representativeness, and diversity of annotations, addressing key challenges in label-free node classification.",
            "6": "Since LLMs generate annotations without access to ground truth labels, there is a risk of noisy annotations.",
            "7": "It would be better to investigate the robustness of LLM-GNN to noisy annotations and potential strategies for mitigating their effects.",
            "8": "LLM-GNN's performance is demonstrated on a specific dataset (OGBN-PRODUCTS), and while it achieves impressive results, its generalizability to other datasets or domains is not thoroughly explored in the paper.",
            "9": "The effectiveness of the approach in different scenarios and with various types of graphs should be investigated to assess its broader applicability.",
            "10": "There could be better with a detired comparison on the economic perspective."
        },
        "7SQadl2a8U": {
            "0": "The proposed method is among the first trials of combining LLMs with GNNs to solve a novel problem, i.e., label-free node classification.",
            "1": "The proposed method is clearly described and the paper is easy to follow in general.",
            "2": "The authors compare with various heuristic baselines and conduct analyses to demonstrate the efficacy of the proposed method.",
            "3": "Though I acknowledge that the proposed method is a valid solution, the technical contribution of the paper is somewhat limited, especially considering that the three major components are largely based on heuristic observations, and the rest are based on existing LLMs and GNNs.",
            "4": "It would make the paper stronger if some theoretical analyses could be provided for the proposed components.",
            "5": "The authors should more explicitly mention that their proposed method only works for text-attributed graphs rather than any general graph, e.g., in the abstract and introduction.",
            "6": "Otherwise, the paper may have overclaiming issues.",
            "7": "In generating the initial node labels using LLMs, it seems that only the feature information is utilized and no structure is considered.",
            "8": "Since it is well-known in the graph machine learning literature that both features and structures greatly affect the node labels, there exists a large room for improvement.",
            "9": "There are some missing related works regarding zero-shot node classification such as [1-2], which should be added.",
            "10": "I also wonder how different LLMs affect the model (the reported results are all based on GPT-3.5-turbo).",
            "11": "[1] Zero-shot Node Classification with Decomposed Graph Prototype Network, KDD’21  \n[2] Dual Bidirectional Graph Convolutional Networks for Zero-shot Node Classification, KDD’22"
        },
        "qpFCDvj1cs": {
            "0": "- The idea of label-free annotation using LLMs on text attributed graphs is an interesting research direction introduced by the paper.",
            "1": "- The paper has experimented with different datasets and incorporated various existing techniques to come up with a cost-effective model.",
            "2": "- This paper appropriately balanced traditional graph active selection criteria with annotation quality by incorporating difficulty-aware active selection with post filtering to obtain training nodes from LLM.",
            "3": "- Difficulty aware (DA) selection: According to the paper, LLMs annotation quality degrades when they have to annotate nodes which are away from the centers.",
            "4": "It implies that the LLMs annotation quality would suffer in case of the diverse nodes (away from center).",
            "5": "However, GNNs accuracy will only improve if the nodes are diverse.",
            "6": "Hence, difficulty aware selection i.e.",
            "7": "use of c-density might not always help and, in fact, it may hinder in some cases.",
            "8": "This is also evident from the results shown in Table 2 : Active_Selection_Methods and the corresponding  DA-Active_Selection_Methods show similar performance on average across different techniques (i.e., selection methods).",
            "9": "Moreover, for at least 50% of the cases, the DA-method (row 2) performs poorly compared to the corresponding active learning method (row 1).",
            "10": "- Though the accuracy from the LLM-GNN model is good (and of course, the model is efficient), it couldn't outperform LLM as a predictor (Table 3).",
            "11": "- The methods are heuristics and do not have theoretical evidence."
        },
        "6f2Z4aNkfX": {
            "0": "- The proposed method is well-motivated.",
            "1": "- Each component supports motivation reasonably.",
            "2": "- Well written paper, it is easy to follow.",
            "3": "- The performance gain is incremental, especially for Difficulty-aware active node selection (DA).",
            "4": "- Explanations about experiments are not enough, and some parts are unclear.",
            "5": "Please refer to questions for details."
        }
    },
    "YKK1jXEWja": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Prospector, a novel LLM agent that enhances decision-making tasks by incorporating Self-Asking and Trajectory Ranking.",
            "1": "- The approach addresses the limitations of current few-shot in-context learning (ICL) methods by optimizing action trajectories based on environmental rewards.",
            "2": "- The proposed method demonstrates significant improvements over existing methods like ReAct and Reflexion on benchmarks such as ALFWorld and WebShop.",
            "3": "Potential reasons for acceptance\n   - The introduction of Self-Asking and Trajectory Ranking provides a novel and effective way to improve LLM agents' decision-making capabilities.",
            "4": "- The empirical results show substantial performance gains over state-of-the-art methods on challenging benchmarks.",
            "5": "- The paper provides a comprehensive evaluation, including detailed experiments and comparisons with existing methods.",
            "6": "- The approach is versatile and can be applied to various LLMs and decision-making tasks without requiring model parameter updates.",
            "7": "Potential reasons for rejection\n   - **Complexity and Efficiency:**\n     - The proposed method, while effective, introduces additional complexity compared to simpler methods like ReAct.",
            "8": "- The Trajectory Ranking process, which involves generating and evaluating multiple trajectories, may be computationally expensive and less efficient.",
            "9": "- **Generality and Applicability:**\n     - The paper focuses on specific benchmarks (ALFWorld and WebShop), and it is unclear how well the method generalizes to other decision-making tasks or environments.",
            "10": "- The reliance on reward prediction models may limit the applicability of the method in scenarios where accurate reward signals are difficult to obtain or define.",
            "11": "- **Evaluation and Comparison:**\n     - The paper primarily compares Prospector with ReAct and Reflexion, but additional comparisons with other state-of-the-art methods could strengthen the evaluation.",
            "12": "- The impact of different LLMs and reward prediction models on the performance of Prospector could be explored in more detail.",
            "13": "Suggestions for improvement\n   - **Efficiency Optimization:**\n     - Investigate ways to reduce the computational overhead of the Trajectory Ranking process, such as by optimizing the number of generated trajectories or improving the efficiency of reward prediction.",
            "14": "- **Generality and Robustness:**\n     - Evaluate the method on a broader range of decision-making tasks and environments to demonstrate its generality and robustness.",
            "15": "- Explore the use of alternative reward prediction models and assess their impact on the performance and applicability of Prospector.",
            "16": "- **Additional Comparisons:**\n     - Include comparisons with a wider range of state-of-the-art methods to provide a more comprehensive evaluation of Prospector's performance.",
            "17": "- Analyze the performance of Prospector with different LLMs and reward prediction models to understand their contributions and limitations.",
            "18": "- **Ablation Studies:**\n     - Conduct ablation studies to isolate the contributions of Self-Asking and Trajectory Ranking components, providing insights into their individual and combined effects on performance.",
            "19": "- **User Study:**\n     - Consider conducting a user study to evaluate the practical benefits and usability of Prospector in real-world decision-making scenarios."
        },
        "RqgyEZ01AU": {
            "0": "- The proposed method is both simple and intuitive, using LLMs for both planning and critiquing of possible trajectories.",
            "1": "The paper is well-written and clear.",
            "2": "- The experiments seem thorough, with comparisons against state-of-the-art methods in the same task domains, and ablations of each component of the proposed Prospector method (removing the trajectory ranking, evaluating the accuracy of the different LLM critics, comparing few-shot and finetuned LLM critics).",
            "3": "In particular, studying the choice of either a fine-tuned or ICL-based critic is interesting and seems novel.",
            "4": "While the method is straightforward and intuitive with impressive experimental results, my main concern is that the two main components of the methods seem to lack novelty in themselves.",
            "5": "This can maybe be clarified with further experimentation: \n\n-  It’s not clear how much of the overall performance improvement is just due to giving the LLM multiple attempts at a single question with the trajectory ranking process.",
            "6": "Further experiments disentangling this would be helpful: for example, if we used the same LLM critics and trajectory ranking process with the ReAct prompt, would it perform on par with Prospector (these experiments seem to be present for ALFWorld but not WebShop)?",
            "7": "Would majority voting at every step, which also allows multiple trajectory attempts but without an explicit LLM critic, be less useful than using the LLM-based critic as in Prospector?",
            "8": "- The AskAct process is does not seem like a novel contribution in and of itself, as it was proposed in Measuring and Narrowing the Compositionality Gap in Language Models (Press et al., 2022).",
            "9": "While the authors note that that Self-Ask work was developed for QA tasks specifically, applying the same general technique of prompting the LLM to ask itself a limited set of questions to reason is a limited contribution.",
            "10": "In particular, it seems like AskAct was not applied to the ALFWorld benchmark for the Prospector agent, and only tested in WebShop as a single fixed question that asks \"which observed object is most proper to select\" (shown in Figure 2 and Table 13 and 16).",
            "11": "Further experiments eliciting different types of self-asked questions across all the tasks would strengthen this contribution."
        },
        "2SqlrxhfRb": {
            "0": "The paper addresses a gap in current LLM-based decision-making methods by integrating feedback from the environment and incorporating stochasticity in trajectory generation.",
            "1": "The proposed method shows empirical success, outperforming existing state-of-the-art methods on standard benchmarks.",
            "2": "Prospector offers an approach that avoids costly fine-tuning, making it more generalizable and efficient.",
            "3": "Both the critic and the generator are LLMs.",
            "4": "This could amplify any existing issues inherent to LLMs.",
            "5": "Limited discussion on the limitations of the reward prediction models used for Trajectory Ranking.",
            "6": "The paper could benefit from a more comprehensive analysis comparing the computational overhead introduced by the Self-Asking and Trajectory Ranking components."
        },
        "qBMxjidHW4": {
            "0": "The paper has a few strong points, such as:\n1.",
            "1": "A comprehensive evaluation across different language models used as critics.",
            "2": "On different parameters of the experiments, a proper experimentation schedule was used, such as few-shot reward prediction accuracy.",
            "3": "The success rate on the evaluated benchmarks show marked improvement over previous work, however, I am not familiar with the benchmarks in the field enough to know if this is sufficient.",
            "4": "The positive impact of the paper is beset by several downsides.",
            "5": "Here are these in the order of importance:\n1.",
            "6": "I am not certain about the magnitude of the impact of the method introduced in this paper.",
            "7": "The method of self-asking itself does not seem significant enough in and of itself without the trajectory ranking, and is quite similar to many different previous methods such as thinking step by step.",
            "8": "Trajectory ranking is definitely the more interesting of the two components, but I am not sure it is a novel and significant enough contribution to merit a place in this venue.",
            "9": "Following up on this, the work is beset by the fact that the new methods are only evaluated in two benchmarks only.",
            "10": "While they perform well on the benchmarks, the question of how easy they will be to scale to a variety of other tasks remain unanswered from the paper itself.",
            "11": "While there is a comprehensive study run on LLM critic and which language model is best for that task, it does not extend to the LLM actor itself.",
            "12": "Rather, only two models of incredibly large sizes are used, which keeps the evaluation quite one-sided.",
            "13": "Overall, this paper shows promise in a few direction, but does not make a noteworthy contribution in any of the directions in my opinion.",
            "14": "However, given my limited experience in such works, I am happy to reconsider my take at the word of the area chair."
        },
        "WkvztltWTx": {
            "0": "The paper is well-written and easy to follow\n2.",
            "1": "Extensive ablations and analysis presented is nicely done -- it shows how the two components of the prospector framework work and improve the performance of the baseline react models.",
            "2": "Limited novelty: While it is good to see how two simple ideas when put together in the prospector framework can lead to good task performance in interactive decision making scenarios, the two ideas themselves are very close to existing work.",
            "3": "Consequently, the novelty seems a bit limited, IMO.",
            "4": "Broader baselines: I liked the authors ablations and comparison with React and its variants given the closeness of the approach (prospector) to react.",
            "5": "These were helpful in understanding how prospector's individual components improve performance.",
            "6": "However, it would have been also useful to see how prospector's performance compares to other llm planning approaches e.g., the ones that combine llms + tree search/classical planning approaches such as https://arxiv.org/pdf/2307.08962.pdf to see how far does prospector push the performance.",
            "7": "Lastly, given that prospector does some training for critics using example trajectories, I am wondering how the performance of prospector would compare to finetuned LLM planner/policy e.g., with LIMA (https://arxiv.org/abs/2305.11206) which can be used to finetune LLM with limited data.",
            "8": "Without these, right now, it is unclear whether prospector should be the goto planning approach for interactive decision making problems or is it really just a better version of react?"
        }
    },
    "qiOqgphnVL": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the emerging field of Diffusion Language Models (DLMs) for text generation, which is a novel and promising area in Natural Language Processing (NLP).",
            "1": "- It introduces the Democratized Diffusion Language Model (DDLM), a re-implementation of the Diffusion LM trained with the CDCD framework, and makes it publicly available for further research.",
            "2": "- The study explores the concept of adaptive early exiting in DLMs, which can significantly enhance the efficiency of text generation without compromising quality.",
            "3": "Potential reasons for acceptance\n   - The paper provides a comprehensive comparison of different DLMs, including SSD, Plaid, and the newly introduced DDLM, offering valuable insights into their performance and characteristics.",
            "4": "- The introduction of adaptive early exiting criteria for DLMs is a significant contribution that can lead to more efficient text generation processes.",
            "5": "- The empirical evaluation and detailed analysis of the proposed methods and models are thorough and well-documented, providing strong evidence for the claims made.",
            "6": "- The release of the DDLM model and its training framework promotes open research and can facilitate further advancements in the field of DLMs.",
            "7": "Potential reasons for rejection\n   - **Reproducibility Concerns:**\n     - The DDLM model is a re-implementation of the CDCD framework, and the lack of source code for the original CDCD model may raise concerns about the accuracy and fidelity of the reproduction.",
            "8": "- The paper acknowledges discrepancies in optimal hyperparameters for training the model, which could affect the validity of the results and comparisons.",
            "9": "- **Limited Scope of Models:**\n     - The study focuses on a limited set of DLMs (DDLM, SSD, Plaid) and omits other potentially relevant models like GENIE or DiffuSeq, which could provide a more comprehensive understanding of the field.",
            "10": "- The exclusion of these models may limit the generalizability of the findings and the applicability of the proposed early exiting criteria.",
            "11": "- **Evaluation Metrics:**\n     - The paper primarily uses AR-NLL, MAUVE, distinct N-grams, and Zipf’s coefficient for evaluation, which may not fully capture all aspects of text generation quality.",
            "12": "- Additional metrics or qualitative analysis could provide a more holistic assessment of the generated text and the effectiveness of the early exiting criteria.",
            "13": "Suggestions for improvement\n   - **Enhance Reproducibility:**\n     - Provide more detailed documentation and code for the DDLM model and its training process to facilitate reproducibility and validation by other researchers.",
            "14": "- Include a more thorough comparison with the original CDCD model, if possible, to strengthen the credibility of the re-implementation.",
            "15": "- **Expand Model Comparison:**\n     - Incorporate additional DLMs, such as GENIE and DiffuSeq, into the study to provide a more comprehensive comparison and validate the generalizability of the findings.",
            "16": "- Explore the performance of the proposed early exiting criteria on a broader range of models to assess their applicability and robustness.",
            "17": "- **Diversify Evaluation Metrics:**\n     - Introduce additional evaluation metrics, such as human evaluation or task-specific performance, to provide a more nuanced assessment of the generated text quality.",
            "18": "- Consider including qualitative analysis or case studies to illustrate the practical impact of the early exiting criteria on text generation.",
            "19": "- **Clarify Methodological Details:**\n     - Provide more detailed explanations and justifications for the chosen hyperparameters and training settings to enhance the transparency and rigor of the study.",
            "20": "- Include a discussion on the potential limitations and trade-offs of the proposed early exiting criteria, as well as suggestions for future research directions to address these challenges."
        },
        "CWwKPtMqAp": {
            "0": "The paper studies a new field for text generation using diffusion models.",
            "1": "Given the inherent complexities and resource-intensive nature of running diffusion models continuously during generation, the research investigates the feasibility of early exiting by monitoring token switches across various pre-training checkpoints.",
            "2": "The methodology of evaluating Cos between the score function and L2 norm the sample embeddings, and subsequently observing score angle changes, provides a novel insights to assess diffusion models.",
            "3": "The paper focuses on the concept of early stopping in diffusion models, which is an idea that has been previously explored, as noted in \"Accelerating Diffusion Models via Early Stop of the Diffusion Process\" as an example.",
            "4": "The contribution to extend to text generation needs to be assessed.",
            "5": "The technique of early stopping is a recognized practice during the inference stage of diffusion models.",
            "6": "While the current paper's examination of token switches across different pre-training checkpoints offers a fresh angle, the approach's broader implications and significance in comparison to established methodologies could be further elucidated.",
            "7": "From Table 1 main results, we can see the choice of steps also provides very marginal impact to the final performance.",
            "8": "It might be beneficial for the research to delve deeper into how this method stands out from or builds upon existing techniques in the field of diffusion models."
        },
        "Mk69bc3ec4": {
            "0": "This paper reimplemented the CDCD framework.",
            "1": "If the code and checkpoint can be open-sourced, it can provide support for the research of DLMs.",
            "2": "This paper makes sufficient experiments and analysis on the existing DLMs, and obtains the early stopping strategy of DLMs by observing the AR-NLL curve.",
            "3": "The innovation of the paper is insufficient.",
            "4": "The main contribution is to reproduce the CDCD structure and analyze the existing DLMs, without proposing new models or methods.",
            "5": "The length of the trained model is limited to 64, and it is not clear whether there will be different conclusions for longer lengths.",
            "6": "The length of 64 is still a bit far from actual application.",
            "7": "We still care about the performance of pre-trained models on downstream tasks, and the paper did not select some downstream tasks for evaluation.",
            "8": "Writing issues:\n\n    (1) The main contribution of the paper, such as the analysis of DLMs, is not given in the title.",
            "9": "The writing style is a bit messy.",
            "10": "(2) It is recommended to add the model parameter quantity to the comparison in Table 1."
        },
        "Kyv735YEON": {
            "0": "- Re-implementation benifts the community.",
            "1": "- The step-by-step analysis could help us understand the generation process of diffusion models.",
            "2": "- It is good to see analysis of sampling between different diffusion models, however, no further explanation about the deep reason to cause these differences.",
            "3": "- The advantage of DDLM is to early exit and speedup the generation process.",
            "4": "However, compared with some faster ODE solvers (e.g.",
            "5": "DPM-solver[1]), early exit of DDLM maybe not superior than them.",
            "6": "- Early exit leads to the downgrade of generation diversity."
        }
    },
    "ndR8Ytrzhh": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a significant issue in the field of multi-step reasoning with large language models (LLMs), specifically the high computational cost associated with the self-consistency (SC) decoding strategy.",
            "1": "- The proposed Early-Stopping Self-Consistency (ESC) method introduces a novel approach to reduce the sampling cost without sacrificing performance, which is a valuable contribution to the field.",
            "2": "- The paper's novelty lies in its dynamic control scheme for ESC, which allows for a performance-cost balance tailored to different tasks and models.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-defined problem and offers a clear, innovative solution that is both simple and scalable.",
            "4": "- Extensive experiments on a variety of reasoning tasks and language models demonstrate the effectiveness of ESC in reducing sampling costs while maintaining performance.",
            "5": "- The theoretical analysis provided supports the empirical findings, adding robustness to the proposed method.",
            "6": "- The control scheme for ESC is a practical addition that enhances the applicability of the method in real-world scenarios.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in some sections**\n     - The explanation of the theoretical analysis, particularly the derivation of the z-statistic and its implications, could be clearer and more detailed.",
            "8": "- The description of the control scheme for ESC might benefit from additional examples or a more step-by-step breakdown to improve comprehensibility.",
            "9": "- **Limited discussion on potential limitations**\n     - The paper does not thoroughly discuss potential limitations or edge cases where ESC might not perform as well, such as tasks with highly variable answer distributions.",
            "10": "- There is a lack of discussion on how ESC might be affected by different types of reasoning tasks beyond the ones tested.",
            "11": "- **Insufficient comparison with other adaptive methods**\n     - While the paper compares ESC with standard SC and CoT, it does not provide a detailed comparison with other adaptive stopping criteria or cost-saving methods in the literature.",
            "12": "- The paper could benefit from a more comprehensive review of related work to position ESC within the broader context of existing methods.",
            "13": "Suggestions for improvement\n   - **Enhance clarity and detail in theoretical analysis**\n     - Provide a more detailed and step-by-step explanation of the theoretical analysis, particularly the derivation of the z-statistic and its implications for the performance of ESC.",
            "14": "- Include additional examples or visual aids to help readers better understand the theoretical concepts.",
            "15": "- **Discuss potential limitations and edge cases**\n     - Include a section discussing potential limitations of ESC, such as tasks with highly variable answer distributions or scenarios where the method might not perform as well.",
            "16": "- Explore how ESC might be adapted or improved to handle these edge cases.",
            "17": "- **Expand comparison with other adaptive methods**\n     - Provide a more detailed comparison with other adaptive stopping criteria or cost-saving methods in the literature, highlighting the unique advantages and potential drawbacks of ESC.",
            "18": "- Include a more comprehensive review of related work to better position ESC within the broader context of existing methods.",
            "19": "- **Improve the description of the control scheme**\n     - Offer a more detailed and step-by-step breakdown of the control scheme for ESC, possibly with additional examples to illustrate its application.",
            "20": "- Clarify how the control scheme can be adjusted for different tasks and models, providing practical guidelines for users."
        },
        "KuKtlTzhnY": {
            "0": "(1) The method is simple and effective.",
            "1": "(2) It is backed by a solid theoretical foundation.",
            "2": "(3) Extensive experiments have been conducted to confirm its effectiveness and reliability.",
            "3": "(1) A related paper with a similar idea, called \"Let’s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs\" (https://arxiv.org/pdf/2305.11860.pdf), was not referenced.",
            "4": "(2) In Table 1, there appear to be inaccuracies in some of the results highlighted in green.",
            "5": "For instance, in the row labeled \"Lˆ-SC (GPT4)\" and the column labeled \"SQA,\" the value \"(-0.27)\" should actually be \"(+0.87)\" because the correct difference is 0.78 (81.42 - 80.55 = 0.78).",
            "6": "Similar issues can be found in the \"SQA\" column.",
            "7": "Additionally, it's puzzling that in the \"SQA\" dataset, Lˆ-SC outperforms SC, even though SC has a larger sample size.",
            "8": "This phenomenon requires further explanation."
        },
        "2E5rjpfmQJ": {
            "0": "Originality: The introduction of ESC offers a fresh perspective in the realm of efficient machine learning algorithms.",
            "1": "Quality: The experimental setup, including testing on six benchmarks, demonstrates the thoroughness of the research.",
            "2": "Clarity: The paper, for the most part, is well-written and concepts are explained clearly.",
            "3": "Comparison with State-of-the-art: It would be helpful to see direct comparisons with current state-of-the-art methods in terms of efficiency and performance.",
            "4": "Generalizability: The paper could discuss potential limitations or scenarios where ESC might not be the optimal solution."
        },
        "A8jimmXNql": {
            "0": "LLMs are a popular topic currently and their execution is costly, either in monetary terms or computationally.",
            "1": "Therefore, it is a good approach to reduce the number of calls necessary, as is proposed in the paper.",
            "2": "It is also a positive thing that existing proven techniques and statistical approaches are re-visited and used in these settings, such as early stopping or using answer entropy as a cut-off criterion.",
            "3": "The experimental evaluation confirms the suitability of the approach over the more exhaustive standard SC technique.",
            "4": "Experiments are extensive and consider many facets of the proposed approach.",
            "5": "The contribution is not particularly strong.",
            "6": "Early stopping or using the confidence respectively the variation in multiple answers in an ensemble of answers is a well known technique.",
            "7": "While we have (maybe, I'm not sure) not seen this in LLM sampling, it is not a particularly strong contribution in the context of an ICLR paper.",
            "8": "I'm also not sure we actually need the notion of the window in the method or if other statistical measurements of the confidence resp.",
            "9": "variability  could be used to determine the cut-off point.",
            "10": "Unfortunately, this has not been discussed."
        },
        "C5gU1B0zgi": {
            "0": "ESC is a simple yet effective adaptation of the original self-consistency to reduce the sampling cost.",
            "1": "The ablation studies and theoretical analysis show that ESC is generally applicable to different benchmarks, and stays effective with different setups.",
            "2": "The novelty of this work is unclear.",
            "3": "[1] already proposed an adaptation of self-consistency to reduce the sampling cost, but this work did not cite and discuss this prior work.",
            "4": "Without a thorough discussion and direct comparison, it is unclear whether ESC is more effective.",
            "5": "In Table 1, when comparing ESC and L-SC, the performance difference is generally small.",
            "6": "The reason can be that the improvement of SC saturates when the sample size increases, thus reducing the sampling size also does not drastically degrade the performance for SC.",
            "7": "It is helpful to show this comparison for smaller sampling sizes, e.g., those in Table 2, and see if the performance improvement achieved by ESC can be more significant.",
            "8": "There are some issues in Table 1.",
            "9": "For example, the SQA results of L-SC are generally much higher than SC, which look problematic.",
            "10": "Also, it is confusing to list L in the table without additional notes, as L represents the sample size, while all other rows represent the task accuracies.",
            "11": "[1] Aggarwal et al., Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs, EMNLP 2023."
        }
    },
    "sY5N0zY5Od": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces DSPy, a novel programming model that abstracts language model (LM) pipelines as text transformation graphs.",
            "1": "- DSPy aims to replace hard-coded prompt templates with a more systematic approach, using declarative modules that can learn and optimize their behavior.",
            "2": "- The model includes a compiler that optimizes DSPy pipelines to maximize a given metric, demonstrating significant improvements over traditional few-shot prompting and expert-created demonstrations.",
            "3": "Potential reasons for acceptance\n   - The introduction of DSPy represents a significant advancement in the development and optimization of LM pipelines, addressing the limitations of hard-coded prompt templates.",
            "4": "- The paper provides empirical evidence through case studies that DSPy can outperform existing methods, including few-shot prompting and expert-created demonstrations.",
            "5": "- DSPy’s modular and parameterized approach allows for greater flexibility and adaptability across different tasks and LMs, making it a valuable tool for the ML community.",
            "6": "- The open-source availability of DSPy encourages further research and development, promoting transparency and reproducibility.",
            "7": "Potential reasons for rejection\n   - **Complexity and Learning Curve**\n     - The introduction of new abstractions (signatures, modules, teleprompters) may present a steep learning curve for practitioners unfamiliar with these concepts.",
            "8": "- The paper may not provide sufficient guidance or examples for users to effectively implement and optimize their own DSPy programs.",
            "9": "- **Evaluation and Benchmarking**\n     - The evaluation is limited to two case studies (GSM8K and HotPotQA), which may not fully demonstrate the generalizability and robustness of DSPy across a wider range of tasks.",
            "10": "- The comparison with existing methods may not be comprehensive enough, lacking direct head-to-head comparisons with state-of-the-art approaches on the same datasets.",
            "11": "- **Scalability and Efficiency**\n     - The paper does not thoroughly address the computational costs and scalability of DSPy, particularly when dealing with large-scale datasets and more complex pipelines.",
            "12": "- The optimization process, including bootstrapping and ensembling, may be computationally expensive and time-consuming, potentially limiting its practical applicability.",
            "13": "Suggestions for improvement\n   - **Provide Detailed Tutorials and Examples**\n     - Include more detailed tutorials and examples in the paper or supplementary materials to help users understand and implement DSPy programs effectively.",
            "14": "- Offer step-by-step guides for common tasks and use cases, demonstrating the full potential of DSPy’s modular and parameterized approach.",
            "15": "- **Expand Evaluation and Benchmarking**\n     - Conduct additional experiments on a broader range of tasks and datasets to demonstrate the generalizability and robustness of DSPy.",
            "16": "- Include direct comparisons with state-of-the-art methods on the same datasets to provide a clearer picture of DSPy’s performance relative to existing approaches.",
            "17": "- **Address Scalability and Efficiency**\n     - Provide a more thorough analysis of the computational costs and scalability of DSPy, including potential optimizations to reduce the time and resources required for compilation and optimization.",
            "18": "- Explore ways to streamline the bootstrapping and ensembling processes, making them more efficient and accessible for practical use.",
            "19": "- **Clarify and Simplify Concepts**\n     - Simplify the presentation of key concepts (signatures, modules, teleprompters) to make them more accessible to a broader audience.",
            "20": "- Use clear and concise language, supported by visual aids and diagrams, to illustrate the relationships and interactions between different components of DSPy."
        },
        "qIhxcYk7zs": {
            "0": "Novel Approach.",
            "1": "This paper introduces a novel approach to systematically build LM pipelines and compile modules into a set of prompts (or fine-tunes) optimized for specific tasks.",
            "2": "The approach is promising, showcasing its ability to reduce the human effort required to develop prompt pipelines.",
            "3": "Well-Written.",
            "4": "The paper is clearly written and presents the core concepts of DSPy in a straightforward manner.",
            "5": "Lack of details: DSPy stands out from other frameworks, like LangChain, due to its capacity for automatic prompt generation and optimization.",
            "6": "However, the introduction of the DSPy Compiler lacks sufficient specificity.",
            "7": "It would be beneficial to provide more comprehensive details on how DSPy generates candidate values for module parameters (instructions, field description, and example input/output) based on signatures."
        },
        "uzjRMISkL7": {
            "0": "* The paper introduces a novel programming paradigm for defining and optimizing parameterized LLM-based text transformations.",
            "1": "* Though I have not used DSPy myself, my impression is that there is a focus on expressiveness and usability in the programming model, which enhances its usefulness for the ICLR community.",
            "2": "One piece of evidence for this expressiveness is the ability showcased to write CoT, reflection, ReAct, and other prompting strategies concisely within the DSPy model.",
            "3": "Another is the composibility e.g.",
            "4": "of different programs with different optimization (aka \"compilation\") approaches.",
            "5": "* DSPy reduces the reliance of creating LLM-based text transformations on manual prompting.",
            "6": "This can be seen as a strength, reducing the need for expertise in prompt writing.",
            "7": "(It can also on some occasions be a weakness if it increases the amount of examples required e.g.",
            "8": "from none.)",
            "9": "* My impression is that the value-add of DSPy for any of the listed strategies is somewhat small.",
            "10": "E.g.",
            "11": "implementing any of CoT, Reflection, or ReAct without DSPy does not require much code.",
            "12": "The same is true, I think, for the optimization approaches / compilers.",
            "13": "I expect this value-add grows when working with many such strategies at once, and additionally that DSPy provides organizational value both as the programs and compilers grow in complexity, and as the set of people using them grows.",
            "14": "Being able to compose different strategies and compiler techniques is also one of the key beneficial properties of the system.",
            "15": "* The evaluations performed do not provide a measure of compute usage or time (both for compilation as well as inference of the compiled programs), which makes comparisons across programs and compilers less meaningful.",
            "16": "* There are no examples of compiled programs provided in the paper or appendices.",
            "17": "I think an analysis of compiled programs would benefit the paper meaningfully.",
            "18": "In particular, some unanswered questions about the compiled programs include: how do they differ from the types of prompt programs that people write by hand or using other frameworks?",
            "19": "how do programs compiled for small LMs differ from those compiled for large LMs?",
            "20": "are there any obvious patterns in the compiled programs?",
            "21": "how about obvious shortcomings or irregularities, where additional hand-optimization would be easy?",
            "22": "any evidence that the optimization techniques overfit to the validation set?"
        },
        "Q00ZqwA9Bd": {
            "0": "DSPy is a major improvement over manually composing complex LM pipelines by hand.",
            "1": "The paper also demonstrates the possibility of automatically optimizing parts of the pipeline once written in the DSPy framework.",
            "2": "Finally, the experimental results are very impressive, particularly given the simplicity of user experience.",
            "3": "There is a major missing related work [1], which takes a similar approach of expressing LM pipelines as programs, and also comes with built-in optimizations.",
            "4": "I would be happy to increase my score if the paper is revised to include a discussion comparing the two approaches.",
            "5": "A more minor concern is that the optimization techniques demonstrated here are relatively limited in scope.",
            "6": "From a conceptual standpoint, I would have liked to see more than ensembling or bootstrapping few shot examples.",
            "7": "[1] Luca Beurer-Kellner, Marc Fischer, and Martin Vechev.",
            "8": "2023.",
            "9": "Prompting Is Programming: A Query Language for Large Language Models.",
            "10": "Proc.",
            "11": "ACM Program.",
            "12": "Lang.",
            "13": "7, PLDI, Article 186 (June 2023), 24 pages.",
            "14": "https://doi.org/10.1145/3591300\n\n====\n\nThe rebuttal has addressed my concerns and I have increased my score from a 6 to 8."
        }
    },
    "wHlYlC2TCM": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper presents a novel approach to open-vocabulary detection by integrating detection-oriented image-text pretraining.",
            "1": "- It introduces a shifted-window learning approach to enhance the robustness and translation-invariance of the backbone representation.",
            "2": "- The proposed method achieves state-of-the-art performance on the LVIS open-vocabulary detection benchmark and competitive results on the COCO benchmark.",
            "3": "Potential reasons for acceptance\n   - The method demonstrates significant improvements over existing state-of-the-art approaches, with a notable increase in mask AP and box AP metrics.",
            "4": "- The approach is simple yet effective, leveraging standard contrastive loss without the need for pseudo-labeling or weak supervision.",
            "5": "- The paper provides comprehensive experimental results, including ablation studies and visualizations, to support the effectiveness of the proposed method.",
            "6": "- The proposed shifted-window learning technique addresses a common issue in vision transformers, enhancing the model's performance.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability:**\n     - Some sections of the paper are dense and may be difficult for readers to follow, particularly those not familiar with the specific technical details.",
            "8": "- The explanation of the shifted-window learning approach could be more detailed to ensure a clear understanding of its implementation and benefits.",
            "9": "- **Novelty and originality:**\n     - While the method shows significant improvements, it builds heavily on existing techniques such as contrastive learning and vision transformers.",
            "10": "The novelty may be perceived as incremental rather than groundbreaking.",
            "11": "- **Evaluation and comparison:**\n     - The paper primarily focuses on the LVIS and COCO benchmarks.",
            "12": "Additional evaluations on other datasets or real-world applications could strengthen the generalizability claims.",
            "13": "- The comparison with other state-of-the-art methods could be more extensive, including a broader range of baselines and variations of the proposed method.",
            "14": "- **Reproducibility:**\n     - Although the authors plan to release the code, the current paper lacks detailed implementation specifics that would facilitate immediate reproducibility by other researchers.",
            "15": "Suggestions for improvement\n   - **Enhance clarity and readability:**\n     - Simplify and clarify the explanations of key concepts, particularly the shifted-window learning approach and the detection-oriented pretraining methodology.",
            "16": "- Include more diagrams or visual aids to illustrate complex processes and architectural components.",
            "17": "- **Expand evaluation:**\n     - Conduct additional experiments on diverse datasets to demonstrate the robustness and generalizability of the proposed method.",
            "18": "- Include more detailed comparisons with a wider range of baseline methods and state-of-the-art approaches.",
            "19": "- **Provide implementation details:**\n     - Include more comprehensive implementation details in the paper or supplementary materials to facilitate reproducibility.",
            "20": "- Provide specific hyperparameters, training schedules, and data preprocessing steps used in the experiments.",
            "21": "- **Address potential biases:**\n     - Discuss potential biases in the pretraining data and the impact on the model's performance and fairness.",
            "22": "- Include fairness and bias mitigation strategies in the model development and evaluation process."
        },
        "FEXRgDsRQk": {
            "0": "- The paper is well-written and easy to follow.",
            "1": "- The motivation of the method is clear and the proposed strategy is straightforward yet effective.",
            "2": "My primary concern pertains to the novelty of the proposed method.",
            "3": "Firstly, the detection-oriented image-text pertaining has been explored extensively since RegionCLIP[1].",
            "4": "Subsequently, there have been endeavors to employ randomly selected proposals for augmentation [2] and align multiple regions [3].",
            "5": "Given the foundation laid by the aforementioned works, the contrastive pretraining methodology introduced in this article, employing a detector architecture, may appear to lack a sufficient level of novelty.",
            "6": "Secondly,  I think the shifted-window learning technique is not enough to support the second technical contribution.",
            "7": "Overall, I agree with the practical values of this paper, but more scientific values from a paper in ICLR are expected.",
            "8": "[1] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao.",
            "9": "Regionclip: Region-based languageimage pretraining.",
            "10": "In CVPR, 2022.",
            "11": "[2] Wei, Fangyun, et al.",
            "12": "\"Aligning pretraining for detection via object-level contrastive learning.\"",
            "13": "Advances in Neural Information Processing Systems 34 (2021): 22682-22694.",
            "14": "[3] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy.",
            "15": "Aligning bag of regions for open-vocabulary object detection.",
            "16": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.",
            "17": "15254–15264, 2023a."
        },
        "W6Z427OuKh": {
            "0": "+ The paper is easy to follow.",
            "1": "+ The method demonstrates good performance on rare categories.",
            "2": "- The proposed contrastive pre-training method is not novel.",
            "3": "Incorporate region information in contrastive learning has been explored in many works [1][2][3].",
            "4": "- Using a private data on pre-training is not fair and not convincing.",
            "5": "- The baseline method is already better than previous works.",
            "6": "It makes the effectiveness of proposed components questionable.",
            "7": "- Figure 2 is misleading.",
            "8": "The similarity map using the backbone features in the middle should be comparing with CLIP based method, not your own.",
            "9": "- In Table 6 (c), stride 4 clearly performs better.",
            "10": "The paper doesn't explain why it chooses a suboptimal one.",
            "11": "- Since the method also involves external box annotations, it should compare with previous methods on pure zero-shot OVD benchmark.",
            "12": "[1] RegionCLIP: Region-based Language-Image Pretraining.",
            "13": "CVPR 2022\n\n[2] Aligning Pretraining for Detection via Object-Level Contrastive Learning.",
            "14": "NIPS 2021\n\n[3] Point-Level Region Contrast for Object Detection Pre-Training.",
            "15": "CVPR 2022"
        },
        "Q6RO939rgb": {
            "0": "- VL pretraining for detection seems a valuable direction of research and\nto my knowledge, this work is one of the first attempts along this direction.",
            "1": "- The overall frame is kept simple without using extra annotation or region proposals, \nwhich may inspire the following researches for extensions or applications for new tasks.",
            "2": "- Clear SOTA performance in the large-scale LVIS OVD benchmark.",
            "3": "- Justification of pretraining with random boxes: I am concerned that the random boxes may produce noisy supervisory signals.",
            "4": "For example, multi-object captions like \"a bird and a dog\"  may be assigned both for bird regions and dog regions.",
            "5": "I think that some discussions around here are needed.",
            "6": "Did the Authors test other choices here, for example, using regular grids?",
            "7": "- Basic comparisons with CLIP: DITO looks like a VL pretraining method, in essence, rather than an OVD method,\nbecause a significant part of the proposed modifications are in the pretraining phase.",
            "8": "It would be natural to compare it with CLIP in the tasks that it was designed for, \nfor example, text-sentence matching or zero-shot classification.",
            "9": "Or is it impossible due to architectural changes?"
        },
        "bxjnijrmoD": {
            "0": "Overall, the performance of the proposed framework is very good, and the motivation is reasonable.",
            "1": "* I also believe that aligning semantics at the region-level is very important and meaningful.",
            "2": "Now many methods are exploring how to achieve region-level pre-training.",
            "3": "The core point of open-vocabulary lies in region-language alignment, thereby generalizing to novel classes.",
            "4": "* Due to the framework being based on detection pre-training, it eliminates the need for distillation or pseudo-labeling, which is a strength of this work.",
            "5": "* The comparison experiments are very sufficient, and the proposed method achieves state-of-the-art results on various benchmarks.",
            "6": "I have several points that I find confusing in this article: \n\n* 1.",
            "7": "The entire framework relies on two backbones during inference.",
            "8": "The article mentions (there is a tendency to lose the pretrained image-text knowledge, Inspired by previous work (Kim et al., 2023a), we use a separate frozen ViT backbone as an open-vocabulary region classiﬁer at inference time to compute the VLM score z).",
            "9": "This indicates that the entire framework does not maintain semantic information during detection pre-training, which seems strange to me and makes the framework appear bulky.",
            "10": "If the image-text semantics could be preserved during detection pre-training, this would make the work more reasonable.",
            "11": "* 2.",
            "12": "This method uses the aligned dataset, while previous methods follow the clip pretrain setting, making it difficult to compare this method with previous methods.",
            "13": "It gives me the feeling that the good results are due to the large amount of data pre-training, rather than Detection-Oriented Pretraining.",
            "14": "* 3.",
            "15": "The design of shifted-window learning is strange.",
            "16": "It is more of a general network design rather than a solution for open vocabulary detection.",
            "17": "Furthermore, shifted-window and open vocabulary seem disconnected."
        }
    },
    "vLJg4wgBPu": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper presents a novel approach to leveraging GPT-3's capabilities to execute iterative algorithms through specific prompting techniques.",
            "1": "- The concept of Iteration by Regimenting Self-Attention (IRSA) is introduced, which could have significant implications for the use of LLMs in education and software engineering.",
            "2": "- The work challenges the current understanding of LLMs' limitations and suggests that prompt design plays a more critical role than previously recognized.",
            "3": "Potential reasons for acceptance\n   - The paper introduces a novel and potentially impactful method (IRSA) for enhancing the capabilities of LLMs.",
            "4": "- The experimental results demonstrate significant improvements in the performance of GPT-3 on various algorithmic tasks, suggesting practical applications in education and software engineering.",
            "5": "- The work provides a detailed analysis and comparison with existing methods, highlighting the advantages of IRSA.",
            "6": "- The paper addresses a critical issue in the evaluation of LLMs, suggesting that current evaluations may underestimate their abilities.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability:**\n     - The paper is dense and may be difficult for readers to follow, especially those not familiar with the technical details of LLMs and prompting techniques.",
            "8": "- The explanations of the IRSA method and its implementation could be more clearly articulated, with more intuitive examples and visual aids.",
            "9": "- **Experimental validation:**\n     - The experiments are primarily conducted on synthetic datasets and specific algorithmic tasks, which may not fully represent real-world applications.",
            "10": "- The paper could benefit from additional experiments on more diverse and complex tasks to validate the generalizability of the IRSA method.",
            "11": "- **Comparative analysis:**\n     - While the paper compares IRSA with existing methods, the comparison could be more comprehensive, including a wider range of baselines and state-of-the-art models.",
            "12": "- The impact of different prompt designs on the performance of IRSA could be explored in more detail to provide a deeper understanding of its effectiveness.",
            "13": "Suggestions for improvement\n   - **Improve clarity and readability:**\n     - Simplify the explanations of the IRSA method and its implementation, using more intuitive examples and visual aids to help readers understand the concepts.",
            "14": "- Provide a clearer structure and flow to the paper, with well-defined sections and subsections that guide the reader through the content.",
            "15": "- **Expand experimental validation:**\n     - Conduct additional experiments on more diverse and complex tasks, including real-world applications, to validate the generalizability of the IRSA method.",
            "16": "- Include more detailed analysis and discussion of the experimental results, highlighting the strengths and limitations of IRSA in different contexts.",
            "17": "- **Enhance comparative analysis:**\n     - Compare IRSA with a wider range of baselines and state-of-the-art models to provide a more comprehensive evaluation of its performance.",
            "18": "- Explore the impact of different prompt designs on the performance of IRSA in more detail, providing insights into the factors that contribute to its effectiveness.",
            "19": "- **Address potential limitations:**\n     - Discuss potential limitations and challenges of the IRSA method, such as its scalability and applicability to different types of tasks and models.",
            "20": "- Provide suggestions for future research and improvements to address these limitations and further enhance the capabilities of LLMs."
        },
        "rP7bGyBAP8": {
            "0": "Importance of contribution: The proposed solution can achieve outperformance than state-of-the-art approaches.",
            "1": "Meanwhile, it also highlights the significance of prompting engineering as GPT-3 applied IRSA can generate more accurate results than GPT-4 without IRSA.",
            "2": "Soundness: The author(s) explain the approach in detail, and conduct evaluation via comparative analysis regarding different questions.",
            "3": "Quality of presentation: The paper is well-organized, and the language is technical yet understandable for readers with domain knowledge.",
            "4": "Comparison with related works: The author(s) introduce extant studies on large language model prompting.",
            "5": "- The methodology can be elaborated for better clarity.",
            "6": "- The overall structure of this paper can be adjusted.",
            "7": "- The research gaps can be further highlighted and discussed."
        },
        "AQPBJQORMX": {
            "0": "This is an interesting work.",
            "1": "The proposed approach seems to work given the evaluated programs.",
            "2": "The proposed work may point out an interesting direction for using LLMs to execute programs, but its current form and results are premature and it’s not ready to be published.",
            "3": "The presentation of IRSA has been majorly illustrated by examples.",
            "4": "While such examples are useful, there still lacks a formulation of IRSA.",
            "5": "The IRSA prompting for these examples look ad-hoc, and it is not clear how IRSA can be automatically applied to execute general programs, without significant manual efforts.",
            "6": "The example and its execution path play the critical role in IRSA.",
            "7": "Isn’t the availability of an execution path too strong assumption for enabling GPT to execute a program?",
            "8": "There are many things not discussed, including how to select the example and how to achieve the execution path.",
            "9": "Is there one example or multiple examples used in IRSA?",
            "10": "It is not clear why IRSA is not used together with GPT4, which casts doubts on the applicability of IRSA approach.",
            "11": "The evaluation is neither comprehensive nor systematic.",
            "12": "The example programs in the evaluation look simplistic."
        },
        "YPbYKwHyxx": {
            "0": "## potentially significant\n\nThis paper outlines several good prompting strategies, which are useful if you want to have the LLM to reason programmatically, with a rigid syntactic structure in its execution trace.",
            "1": "The paper explains each strategy, irsa, skip attention, fragmented prompting with examples, and show the proposed prompts can achieve better results paired with a weaker model (gpt3.5) than a naive prompt with a more advanced model (gpt4)\n\nThe analogy of GPT as a turing machine is good too.",
            "2": "## poor quality and clarity\n\nIt is unclear if the proposed method can be reliably replicated to other domains, given that it is only evaluated on a handful of problems.",
            "3": "I believe this work can be made substantially better if an automated method could be derived turning an existing complex program into a prompt, and evaluated on a larger set of problems, rather than the simplistic 100 python arithmetic problems.",
            "4": "## less than ideal novelty\nIt is also unclear how the proposed method is significantly different from Nye (2021)'s work on scratchpad, as both leverages trace information extensively.",
            "5": "It would be good to have a related work section to spell out the exact differences.",
            "6": "The paper note that the proposed technique may be beneficial in the education domain.",
            "7": "However, wouldn't having a LLM to simply mark up an existing execution trade of actually running the program be a better (and more correct) alternative?"
        },
        "xbmG2Tmjx1": {
            "0": "+ The paper explores an important question of broad interest to the community.",
            "1": "The exact capabilities and limits of LLMs remain unclear.",
            "2": "This work shows that highly structured prompts can be used to better control LLM output on tasks requiring precise state control (memory) and iterative execution (loops).",
            "3": "+ The proposed prompts (IRSA) are intuitively clear.",
            "4": "They seem novel, to my knowledge.",
            "5": "+ The experiments demonstrate that the IRSA prompts do indeed help the LLM correctly simulate algorithms requiring loops over the distribution of inputs considered.",
            "6": "There is a good amount of detail included in the main paper and appendices.",
            "7": "- The use of a trace in the prompt raises a few questions, which are not addressed.",
            "8": "A classic sorting algorithm can correctly sort very long lists using a relatively short specification of the algorithm.",
            "9": "Can IRSA do the same (sort long input lists with a short trace)?",
            "10": "The experiments seem restricted to short inputs length 5 in sorting, for example).",
            "11": "The scaling, generalization and robustness of the IRSA prompt to different inputs aren't well explored in the paper.",
            "12": "- The paper could better highlight its algorithmic and empirical contributions relative to a rapidly growing body of literature on how to improve a LLM's instruction-following abilities.",
            "13": "At the moment, I'm not sure if the experiments conclusively demonstrate that IRSA improves the instruction-following (via algorithm execution) abilities of a LLM.",
            "14": "- The terminology used can sometimes be a bit loose.",
            "15": "For example, \"this strategy hardens the attention\", \"skipping unnecessary attention saves computation\", etc.",
            "16": "More formal descriptions of these important ideas would increase the technical rigor of the paper.",
            "17": "Alternatively, the paper could simplify the description to emphasize the empirical aspects (i.e., prompt engineering), which are also valuable."
        }
    },
    "x6u2BQ7xcq": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces Tag2Text, a novel vision-language pre-training (VLP) framework that incorporates image tagging to guide the learning of visual-linguistic features.",
            "1": "- The approach leverages large-scale annotation-free image tags parsed from image-paired text, providing a more diverse set of tag categories beyond objects.",
            "2": "- Tag2Text demonstrates superior zero-shot performance and enhances the performance of vision-language models on both generation-based and alignment-based tasks.",
            "3": "Potential reasons for acceptance\n   - The introduction of image tagging into vision-language models is a significant and novel contribution that addresses limitations of previous detector-based and detector-free models.",
            "4": "- The framework shows state-of-the-art results across a wide range of downstream benchmarks, demonstrating its efficacy and robustness.",
            "5": "- The ability to utilize large-scale annotation-free image tags and provide diverse tag categories is a valuable advancement in the field.",
            "6": "- The paper provides comprehensive experimental results and comparisons with existing models, showcasing the superiority of Tag2Text.",
            "7": "Potential reasons for rejection\n   - **Clarity and readability of the paper**\n     - The paper is dense with technical details, which may make it difficult for readers to follow the key contributions and innovations.",
            "8": "- Some sections, such as the explanation of the text semantic parser and tag category system construction, could benefit from clearer and more concise descriptions.",
            "9": "- **Evaluation and comparison with existing models**\n     - While the paper provides extensive comparisons, it would be beneficial to include more detailed ablation studies to isolate the impact of different components of the Tag2Text framework.",
            "10": "- The paper could provide more insights into the limitations and potential failure cases of the proposed approach.",
            "11": "- **Generalization and scalability**\n     - The paper primarily focuses on specific datasets and benchmarks.",
            "12": "It would be valuable to discuss the generalization of the approach to other datasets and real-world applications.",
            "13": "- The scalability of the framework to handle even larger datasets and more complex tasks could be further explored.",
            "14": "Suggestions for improvement\n   - **Improve clarity and readability**\n     - Simplify and clarify the descriptions of the text semantic parser and tag category system construction to make them more accessible to a broader audience.",
            "15": "- Provide a high-level overview of the key contributions and innovations at the beginning of the paper to help readers grasp the main ideas quickly.",
            "16": "- **Enhance evaluation and comparison**\n     - Include more detailed ablation studies to isolate the impact of different components of the Tag2Text framework, such as the image-tag recognition decoder and the image-tag-text generation decoder.",
            "17": "- Provide more insights into the limitations and potential failure cases of the proposed approach, along with possible solutions or future work directions.",
            "18": "- **Discuss generalization and scalability**\n     - Discuss the generalization of the approach to other datasets and real-world applications, including potential challenges and solutions.",
            "19": "- Explore the scalability of the framework to handle even larger datasets and more complex tasks, and provide insights into how the approach can be adapted or extended for such scenarios."
        },
        "RETplhNAvl": {
            "0": "I really like the paper.",
            "1": "It is clearly motivated, well written, technically well executed, has clear novelty, and has good experimental results.",
            "2": "But I want to highlight one thing that is surprisingly hard to find: it seems useful.",
            "3": "The reasons: 1) Current V&L models are trained with long captions, which means small tag-like queries are not well supported.",
            "4": "There are indeed some object detection methods a-la CLIP (owl-vit and co) but they offer different functionalities and have different requirements.",
            "5": "So this is a good addition to the V&L toolbox.",
            "6": "2) it offers a way of controlling caption generation through the use of input tags.",
            "7": "Minor suggestions (up to the authors and no reply needed):\nFig.",
            "8": "1 might get a bit confusing as both the \"prior work strategy\" and the current strategy are included in the same flow graph.",
            "9": "Table 2 shows the last 3 methods seem apart from the rest but it's unclear why they are separated.",
            "10": "The method is based on some relatively standard techniques that are however well executed and put together.",
            "11": "While not much of a minus, but maybe a reason for an accept vs strong accept."
        },
        "V7u8zGLppk": {
            "0": "The idea of the utilization of image tags parsing from large-scale image-text pairs is interesting, efficient and effective from improving the performance of vision-language models.",
            "1": "The framework of Tag2Text employs a multi-task pretraining approach, including Tagging, Generation, and Alignment.",
            "2": "These tasks are reasonable and share the same visual features obtained from the image encoder, guaranteeing the efficiency of the framework.",
            "3": "A large number of experimental results over image tagging, image captioning, and image-text retrieval tasks prove the effective of the proposed methods.",
            "4": "The detail of text semantic parser is not clear.",
            "5": "Although it is based on existing work of [Wu et al.",
            "6": "2019], it should make clear how to obtain the corresponding tags in the paper.",
            "7": "In Fig.",
            "8": "2, it is difficult to understand which are users’ input desired tags, since they share the same forms of the recognized image tags.",
            "9": "In the Image-Tag-Text Generation paragraph, the introduction of this task is not clear.",
            "10": "According to Fig.",
            "11": "4(c), it seems the text embedding should not be used as input, which is conflicting with the introduction in the corresponding paragraph.",
            "12": "In the experiment on controllability analysis, it is not clear how the threshold of tagging head to control the tagging guidance.",
            "13": "There are some typos in the manuscript."
        },
        "ekIi9di4rb": {
            "0": "- The idea is straightforward and easy to grasp.",
            "1": "- Given the high cost of human labeling and the limited diversity in generating captions for Image Captioning models, such as BLIP, finding new methods for generating comprehensive captions is both challenging and valuable.",
            "2": "- This work offers a potential benefit in the context of person re-identification (REID).",
            "3": "- The motivation behind this approach is also quite appealing.",
            "4": "- Notably, the Tag method diverges from the commonly used Faster R-CNN-based object detectors and demonstrates significantly improved speed.",
            "5": "- Data and Pre-training Settings: The author of the paper used a 4M setting, which includes training data from COCO (Common Objects in Context) and Visual Genome.",
            "6": "NoCaps data is sourced from OpenImages and COCO.",
            "7": "Importantly, this work does not incorporate any out-of-distribution data.",
            "8": "The success of this approach on COCO-related tasks is attributed to the similarity between the pre-training data and COCO style, and it's noted that many works beyond BLIP face similar challenges.",
            "9": "- TagEval Task: TagEval is mentioned as a task, but it is not considered popular, and its persuasiveness is limited.",
            "10": "Models trained on tags are noted to perform well in this case.",
            "11": "- Tag Introduction in Pre-training: The introduction of tags in pre-training is not a novel idea.",
            "12": "There are existing works, like OSCAR, that have explored the concept of using tags to improve vision-language pre-training."
        },
        "MtYflFGaJd": {
            "0": "The paper is well written and easy to follow.",
            "1": "The method is simple and is shown to work well.",
            "2": "The idea of using image tags to aid VL pre-training makes a lot of sense.",
            "3": "The results are not always SOTA but are very good.",
            "4": "Results on multiple tasks/datasets are provided.",
            "5": "I believe the results are sufficient to show that the main idea behind the paper works as well as expected.",
            "6": "I think the main problem with the paper is that all of its components have been proposed before so the paper looks more like a re-implementation of known ideas with more recent architectures and pipelines which is of course expected to work better.",
            "7": "Specifically the main idea of using tags to aid VL pre-training appears in many works including OSCAR or more recently in DiHT (Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training) while other losses used like I2T are very commonly used in most works in VL pre-training.",
            "8": "But the method for sure can serve as strong baseline.",
            "9": "Somewhat less important concern: as most experiments are on COCO/Flick which are datasets very close to the ones used for training I am wondering whether the authors could carry out an experiment on out-of-domain data ."
        },
        "VlsCVYBflf": {
            "0": "Including image tagging task to vision language pretraining seems to be promising, which particularly benefit image captioning.",
            "1": "The proposed design that combines multi-tasks for vision language pretraining is interesting.",
            "2": "The experimental results are promising.",
            "3": "The paper is well-written in general and easy to comprehend.",
            "4": "This paper discusses VL pretraining methods that are either based on 1) object detection or 2) image tagging (the proposed method) and argues that image tagging is faster and introduces significantly fewer parameters.",
            "5": "However, this may not be a compelling motivation for choosing image tagging, as object detectors are fixed (with no additional learnable parameters) and only need to be executed once before training, incurring marginal computational cost compared to the training phase.",
            "6": "It would be beneficial for this paper to incorporate specific mathematical formulations to provide a more comprehensive description and discussion of the fundamental problems that require resolution.",
            "7": "There is room for improvement in the typesetting.",
            "8": "Table 1 appears to be disorganized, making it unclear which numbers to focus on and compare.",
            "9": "It is advisable to separate results for different tasks into distinct tables.",
            "10": "The \"SOTA\" comparison for image tagging (multi-label classification) seems to omit a substantial portion of recent works.",
            "11": "As a result, the reviewer maintains a skeptical stance concerning the associated claims and conclusions.",
            "12": "Metrics such as Precision, Recall, and F1 score, which are commonly used for image tagging and multi-label classification, are notably absent from the results.",
            "13": "The construction of the tag category system appears to involve human annotation in the process (Section 3.2), which contradicts the earlier claim of being an \"automatic\" approach (Section 1).",
            "14": "Conducting an ablation study on the choice of vocabulary set (tag set) size to be mined could offer valuable insights into the proposed method."
        }
    },
    "KIPJKST4gw": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical question in the field of Large Language Models (LLMs): at which training stage does the introduction of code data enhance the reasoning capabilities of LLMs.",
            "1": "- The study systematically explores the impact of code data at different stages of training (pre-training, instruction-tuning, and both) and evaluates the reasoning capabilities of LLMs across various tasks and domains.",
            "2": "- The findings provide valuable insights into the optimal use of code data in training LLMs, which can significantly influence future research and development in the field.",
            "3": "Potential reasons for acceptance\n   - The research question is highly relevant and timely, given the increasing importance of LLMs in various applications.",
            "4": "- The paper presents a comprehensive experimental setup, including multiple reasoning tasks across different domains, ensuring a thorough evaluation of the models.",
            "5": "- The findings offer clear and actionable insights into the benefits of introducing code data at different training stages, which can guide future LLM training strategies.",
            "6": "- The authors provide open-source resources, including model implementations and trained model parameters, which contribute to the broader research community.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in experimental details:**\n     - The paper could benefit from more detailed descriptions of the datasets used, including their specific characteristics and how they were processed.",
            "8": "- The methodology for evaluating the models' reasoning capabilities could be explained in greater detail, particularly the criteria for selecting the six reasoning tasks.",
            "9": "- **Limited discussion on potential limitations:**\n     - The paper does not thoroughly discuss the potential limitations of the study, such as the impact of model size and computational resources on the results.",
            "10": "- There is a lack of discussion on the generalizability of the findings to other LLM architectures and training setups.",
            "11": "- **Insufficient comparison with existing work:**\n     - The paper could provide a more in-depth comparison with related work, particularly studies that have explored the impact of code data on LLMs.",
            "12": "- The discussion on how the findings align or contrast with previous research could be expanded to provide a clearer context for the contributions of this study.",
            "13": "Suggestions for improvement\n   - **Enhance clarity in experimental details:**\n     - Provide more detailed descriptions of the datasets used, including their specific characteristics, sources, and any preprocessing steps.",
            "14": "- Elaborate on the methodology for evaluating the models' reasoning capabilities, including the criteria for selecting the six reasoning tasks and the specific metrics used for evaluation.",
            "15": "- **Discuss potential limitations:**\n     - Include a section discussing the potential limitations of the study, such as the impact of model size, computational resources, and the generalizability of the findings to other LLM architectures.",
            "16": "- Address any assumptions made during the experiments and their potential impact on the results.",
            "17": "- **Expand comparison with existing work:**\n     - Provide a more comprehensive comparison with related work, highlighting how this study builds on or differs from previous research on the impact of code data on LLMs.",
            "18": "- Discuss how the findings align or contrast with existing studies, providing a clearer context for the contributions of this research.",
            "19": "- **Improve presentation of results:**\n     - Use more visual aids, such as graphs and tables, to present the experimental results more clearly and concisely.",
            "20": "- Include more detailed statistical analyses to support the significance of the findings, such as confidence intervals and effect sizes.",
            "21": "- **Provide practical recommendations:**\n     - Offer practical recommendations for researchers and practitioners on how to implement the findings in their own LLM training setups.",
            "22": "- Discuss potential future research directions based on the insights gained from this study, such as exploring the impact of code data on different LLM architectures or training strategies."
        },
        "uvtMuxvWvx": {
            "0": "- This paper is well-motivated.",
            "1": "The impact of code data in LLMs is a hot research question.",
            "2": "This paper answers this issue from the reasoning capability aspect.",
            "3": "- The experiments are comprehensive, and the insights are remarkable.",
            "4": "The reasoning capability of LLMs is evaluated via six tasks in five domains.",
            "5": "The authors provide critical analyses and significant insights on training LLMs and the reasoning capability of LLMs.",
            "6": "- The idea of dynamic mixed strategy is easy to follow yet effective.",
            "7": "It helps LLMs learn reasoning skills progressively during training.",
            "8": "- The authors provide comprehensive open-source resources, demonstrating the reproducibility of the models.",
            "9": "These resources are valuable for the LLM community.",
            "10": "- Missing discussion on the applications.",
            "11": "Although the authors conduct experiments and provide insights on training LLMs and improving their reasoning capability, this paper does not discuss how to apply the insights to enhance the LLM products in different domains.",
            "12": "- Unclear construction of training corpus.",
            "13": "The author should provide more details about data collection, data cleaning, and training data construction.",
            "14": "The authors use fuzzy data deduplication, but they have not explained the tools of fuzzy.",
            "15": "They should open-source the data for reproducibility.",
            "16": "Besides, the detailed model architecture is missing.",
            "17": "- Table 7 is confusing.",
            "18": "The results in Table 7 show the code data will lead to a performance drop on four out of five datasets.",
            "19": "It indicates that the code data may not help to improve the reasoning capability of LLMs.",
            "20": "The authors should provide valid reasons.",
            "21": "- The related work is limited.",
            "22": "Recently, there have been various papers discussing the reasoning capability of LLMs.",
            "23": "Therefore, the authors should survey more related papers and compare with them.",
            "24": "- Fix the grammar errors and improve the presentation.",
            "25": "On page 8, “The experiment found that…’’ -> “The experiment showed that’’.",
            "26": "- Missing future work.",
            "27": "The authors should provide the potential future work on LLMs based on the experimental results and insights provided in this paper.",
            "28": "[1] Roziere B, Gehring J, Gloeckle F, et al.",
            "29": "Code llama: Open foundation models for code[J].",
            "30": "arXiv preprint arXiv:2308.12950, 2023."
        },
        "dXTv0waMHt": {
            "0": "Valuable research question.",
            "1": "The paper raises a meaningful research question: at which training stage introducing code data can really help the reasoning capabilities of LLM?",
            "2": "This question is of critical significance for understanding the training and application of LLM.",
            "3": "Comprehensive experimental design.",
            "4": "This paper provides a comprehensive and fair evaluation of the reasoning capabilities of LLMs on six reasoning tasks covering five domains.",
            "5": "This broad experimental scope ensures the generalizability and reliability of the conclusions.",
            "6": "Additionally, the authors compare models with different sizes to verify the generalization of the conclusion.",
            "7": "In-depth analyses and insights.",
            "8": "The paper not only provides experimental results but also performs in-depth analysis, providing insights into mixing code and text data to enhance the general reasoning capabilities and code reasoning capabilities of LLM.",
            "9": "Specifically, in the pre-training stage, mixed code data helps LLM improve general reasoning capabilities, and in the SFT stage, mixed code data helps LLM improve specific code reasoning capabilities.",
            "10": "Experimental details are insufficient.",
            "11": "The paper may not provide enough details on experimental settings and parameter selection in some parts (such as data mixing strategies, decoding strategies, etc.).",
            "12": "This might challenge researchers attempting to replicate or extend this work.",
            "13": "Recently, various code foundation models, such as CodeLlama [1], have been opened.",
            "14": "However, the authors do not conduct any discussions or experiments on them.",
            "15": "In my opinion, the reasoning capability of code foundation models is also an essential part of the interest scope of this work.",
            "16": "Quality of code data.",
            "17": "The quality of code data can have a significant impact on the reasoning capabilities of LLM.",
            "18": "How the author ensures the high quality of code data is not discussed in depth in the article.",
            "19": "The related work part is weak.",
            "20": "Missing important papers, such as [1,2,3].",
            "21": "[1] Roziere B, Gehring J, Gloeckle F, et al.",
            "22": "Code llama: Open foundation models for code[J].",
            "23": "arXiv preprint arXiv:2308.12950, 2023.",
            "24": "[2] Yang A, Xiao B, Wang B, et al.",
            "25": "Baichuan 2: Open large-scale language models[J].",
            "26": "arXiv preprint arXiv:2309.10305, 2023.",
            "27": "[3] Li P, Sun T, Tang Q, et al.",
            "28": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors[J].",
            "29": "arXiv preprint arXiv:2305.05711, 2023."
        },
        "87M2Eq4k3D": {
            "0": "- **Clear Hypothesis:** The paper asks a clear question and provides a clear setup for testing various hypothesis about that question.",
            "1": "- **Clear message:** The results show a clear message about the research question, albiet only on smaller-sized models.",
            "2": "- **Clarity of the writing:** The writing was mostly clear and easy to follow - **Datasets:** I was not familiar with some of the datasets used in this work and after looking into some of them, I could not get a sense of how general and challenging they are.",
            "3": "The majority of the computation cost for this project seems to be on the training stage, so I believe reporting results on a few more datasets (maybe only for Tables 2 and 3) can strengthen the main arguments of the paper.",
            "4": "That could include datasets from other reasoning domains (e.g., math might be an important one that does not appear in the results) or from the same domains but on datasets that are more established.",
            "5": "- **Mixture experiment:** The experiment on exploring ways to mix code and text data is interesting, but given that adding code at the instruction-tuning stage was already shown to be not that effective, I wonder why it was tested for the instruction-tuning stage.",
            "6": "I understand the high computation cost of pre-training, but it seems to me that given the previous set of results, this experiment makes sense mostly at the pre-training stage where we have seen that code data can be effective.",
            "7": "- **Language Inconsistency:** - For the results in Section 3.3.5, the authors conclude that training with code data *has little negative impact* on the performance of other tasks.",
            "8": "But the numbers in Table 7 don't seem to show little negative impact.",
            "9": "The large impact on the DuReader dataset has been already pointed out by the authors.",
            "10": "Moreover, on CMNLI the performance decreases from 45.07 to 43.49 which is almost equal in magnitude to some of the gains reported in Table 2.",
            "11": "I believe the language should be more consistent on the amount of improvement/decrement that can be considered a significant amount for the datasets and the experimental setup of the paper.",
            "12": "- **Minor suggestions:** 1- In Table 3, there are multiple equal numbers but only one of them is in bold face.",
            "13": "2- In Table 4, I suggest reversing the rows and columns to make it consistent with the other tables."
        },
        "ndP5xZp2gJ": {
            "0": "* The problem studied is interesting: the relationship between code data and reasoning and the paper aims to somehow tackle this issue.",
            "1": "* The paper is well-written and the results are well-presented.",
            "2": "* From my understanding (and correct me If I'm wrong), the code model is trained on more overall tokens than the NL model.",
            "3": "I would expect a study like that to control for the number of pre-training tokens while changing their nature i.e., text vs. code.",
            "4": "If the code model is trained on as many natural text tokens as the baseline model in addition to having code in the pre-training data, then the code model should be expected to perform better because it was trained on more data.",
            "5": "No surprise there.",
            "6": "* It's hard to say whether the results reported have statistical significance.",
            "7": "For example, in Table 2, the code model is only 0.13 points better than the NL model on ScienceQA.",
            "8": "Are these results significant?",
            "9": "And are these enough to conclude that code reasoning?",
            "10": "I would expect the authors to run a statistical significance test to support their results.",
            "11": "* The proposed dynamic mixing strategy produces very marginal improvements (except over logical reasoning) and has a negative effect on the performance over three tasks.",
            "12": "The paper does not thoroughly investigate why this is the case.",
            "13": "Also, the design of the mixing strategy seems rather arbitrary.",
            "14": "* The evaluation does not cover mathematical reasoning, although it's one type of reasoning where we should expect great improvements since code data is roughly similar to math data."
        }
    },
    "XTJ0YVBM10": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the novel problem of task-oriented asking (TOA) in large language models (LLMs), which is crucial for improving real-life collaboration on reasoning tasks.",
            "1": "- It introduces a new framework and methodology, Fact-Level Masking (FLM), to create self-supervised TOA datasets, which is a significant contribution to the field.",
            "2": "- The paper provides a comprehensive evaluation of current zero-shot models on the newly created FLM-HotpotQA dataset, highlighting the limitations and potential areas for improvement in TOA.",
            "3": "Potential reasons for acceptance\n   - The introduction of the TOA framework and FLM methodology is innovative and addresses a gap in the current research on LLMs.",
            "4": "- The creation of the FLM-HotpotQA dataset and the associated evaluation pipeline provides a valuable resource for future research in TOA.",
            "5": "- The experimental results are thorough and demonstrate the current limitations of zero-shot models in TOA, providing a clear direction for future research.",
            "6": "- The paper is well-structured and clearly written, making it accessible to a broad audience in the field of natural language processing.",
            "7": "Potential reasons for rejection\n   - **Limited scope of evaluation:**\n     - The evaluation is primarily focused on the HotpotQA dataset, which may not generalize to other types of reasoning tasks or domains.",
            "8": "- The experiments are limited to a few zero-shot models, and it would be beneficial to see a broader range of models and configurations tested.",
            "9": "- **Lack of detailed analysis on failure modes:**\n     - While the paper identifies some failure modes, it does not provide an in-depth analysis of why certain models fail and how these failures could be mitigated.",
            "10": "- There is a need for more detailed qualitative analysis of the types of questions generated by the models and their impact on the primary task.",
            "11": "- **Insufficient exploration of alternative approaches:**\n     - The paper focuses on the FLM methodology but does not explore other potential methods for generating TOA datasets or improving TOA performance.",
            "12": "- There is a lack of comparison with other existing methods or benchmarks in the field of question generation and task-oriented dialogue.",
            "13": "Suggestions for improvement\n   - **Expand the scope of evaluation:**\n     - Include additional datasets from different domains and reasoning tasks to test the generalizability of the TOA framework and FLM methodology.",
            "14": "- Evaluate a wider range of models, including fine-tuned models and those specifically designed for question generation or dialogue tasks.",
            "15": "- **Provide a more detailed analysis of failure modes:**\n     - Conduct a thorough qualitative analysis of the questions generated by the models, identifying specific patterns or common issues that lead to failures.",
            "16": "- Investigate the impact of different types of questions on the primary task and provide insights into how to improve question generation.",
            "17": "- **Explore alternative approaches:**\n     - Consider other methods for creating TOA datasets, such as using human annotations or leveraging existing question-answering datasets in novel ways.",
            "18": "- Compare the FLM methodology with other state-of-the-art techniques in question generation and task-oriented dialogue to highlight its strengths and weaknesses.",
            "19": "- **Enhance the experimental setup:**\n     - Include ablation studies to understand the impact of different components of the FLM methodology and the TOA framework.",
            "20": "- Provide more detailed results and analysis for each model tested, including error analysis and case studies to illustrate key findings."
        },
        "MiB8m7PqbC": {
            "0": "It is interesting to formulate the follow-up question generation task as a problem of task-oriented asking (TOA).",
            "1": "For the TOA task, this paper presents a fact-level masking method to generate TOA dataset, which can be used for training and evaluation.",
            "2": "Multiple zero-shot large language models are evaluated on the proposed FLM dataset and show that they struggle to ask a proper question for the given task.",
            "3": "A significant point of contention revolves around the definition of the term \"task-oriented.\"",
            "4": "Utilizing a question from HotpotQA as the primary task, denoted as $t$, may not be the most reasonable approach.",
            "5": "The task itself could encompass a repetitive and abstract procedure, and task-oriented questions primarily serve the purpose of populating missing values.",
            "6": "One noteworthy contribution of this paper pertains to the introduction of the TOA dataset.",
            "7": "However, the exposition of this dataset remains somewhat unclear.",
            "8": "Key details, such as the dataset's fields and the methodology employed for its measurements, are either absent or challenging to comprehend.",
            "9": "In section 4.2, the pipeline directly employs established models like GPT4 and other large language models as M2, with Flan-T5-base serving as $\\Phi$.",
            "10": "Given that all these models are readily available and widely known, it raises the question of what unique contribution this paper brings to the field.",
            "11": "Merely establishing a dataset and making what could be construed as a minor modification to an existing one may not be considered particularly groundbreaking for a conference like ICLR.",
            "12": "Furthermore, solely assessing this dataset using existing models might fall short of the standards expected for this conference."
        },
        "ywzIWsNQbb": {
            "0": "The task of TOA is an interesting testbed, especially for LLMs.",
            "1": "Although similar to slot-filling for dialogue systems, the proposed TOA task is more flexible and is potentially better suited for LLMs.",
            "2": "The introduced FLM-HotpotQA dataset is useful for evaluating future LLM performance in question generation.",
            "3": "Extensive ablation studies on the primary QA model and the oracle model provide insights for understanding the task.",
            "4": "Key implementation details are missing in the paper, which harms the understanding of the paper.",
            "5": "It is unclear how exactly different Oracle models select facts based on the questions, and what the prompt for the QA model is.",
            "6": "Although the idea of TOA is interesting, the implementation of the task in this paper does not fully reflect its motivation, which is \"(TOA) models’ ability to ask for missing information\".",
            "7": "For example, the involvement of the oracle model in this task makes the actual evaluation of the TOA model tricky.",
            "8": "According to Figure 7, the tested oracle models have a low MFRR even with human-generated questions.",
            "9": "The low performance of the oracle model is a serious bottleneck in the evaluation, which makes the model potentially impossible to achieve a high performance on the task.",
            "10": "The lack of Oracle models' implementation details along with the low performance of the Oracle models harm the soundness of this paper.",
            "11": "The results in Figure 6 are doubtful.",
            "12": "With supporting facts, the task for the primary model is degenerated to the original HotpotQA.",
            "13": "Seeing FLAN-T5-Base on par with GPT-3.5-Turbo and FLAN-T5-Large outperforming GPT-4 on HotpotQA is surprising.",
            "14": "The lack of primary models' implementation details along with the surprising results in Figure 6 also harm the soundness of this paper.",
            "15": "The related work section could be updated.",
            "16": "The area of Asking Clarification Questions (ACQ) is active in dialogue systems research.",
            "17": "Please refer to [1].",
            "18": "[1] Rahmani, Hossein A., et al.",
            "19": "\"A Survey on Asking Clarification Questions Datasets in Conversational Systems.\"",
            "20": "arXiv preprint arXiv:2305.15933 (2023)."
        },
        "t4hev3A975": {
            "0": "This work addresses an (understudied) problem of asking follow-up/clarification questions, and contributes a novel framework for addressing the problem, within the realm of QA.",
            "1": "Overall, this paper is also quite well-written, and very clearly lays out its method.",
            "2": "The paper very comprehensively articulates its scope and limitations, making note of any caveats where they may arise, and also makes the right comparisons and baselines to try and address any limitations in the evaluation (e.g.",
            "3": "comparing against the repeater baseline to address limitations laid out in bullets on page 5).",
            "4": "It seems like the authors have worked hard to address any spurious correlation or potential biases that may affect the evaluation.",
            "5": "Consequently, the results are quite convincing.",
            "6": "The paper also provided very comprehensive ablation studies in section 5, and provided concrete examples of failure cases.",
            "7": "The paper explores its TOA method with lots of different models, including open-source models.",
            "8": "The current setup in the paper seems somewhat dataset-specific, and the evaluation is also currently only focused on a single task & dataset.",
            "9": "In the introduction, the paper frames TOA as a more generic technique for all tasks (e.g.",
            "10": "intro makes reference to legal tasks and states “we propose task-oriented asking (TOA) as a benchmark for language model question generation”, without narrowing the scope to just QA tasks with a single missing supporting fact.)",
            "11": "Thus, either the claims in the introduction need to be tempered, or the current evaluation scheme should be broadened to give a sense of how well TOA generalizes to other tasks.",
            "12": "More specifically, the answer model is currently restricted to picking between a limited set of facts (one of them being the masked supporting fact necessary to answer the original question, and the others being distractor facts), which likely overestimates performance compared to an answering \n    1.",
            "13": "While understandably the authors were trying to simulate an oracle answer model, note that this does not necessarily tell us how well the question-asking model is, and does necessarily simulate a “perfect answerer”.",
            "14": "In particular, the task for the question-asking model shifts from “ask a question seeking, specifically, the missing information” to “ask a question that privileges the masked supporting fact as an answer over any of the provided distractor facts”.",
            "15": "In the latter case, we don’t even need to guarantee that the question being asked is comprehensible, or would organically be answered with the supporting fact, but simply that the supporting fact seems like a marginally better response than any of the distractors.",
            "16": "For example, it could be the case that the input itself carries information about what information is missing and the question was unnecessary, or the question isn’t actually asking for the missing information / only asks for part of the missing information but the masked fact is still the most pertinent answer.",
            "17": "While comparing against the Repeater baseline takes care of some of these concerns, this still does not take away from the fact that there are factors that aren’t explored due to the answer setup.",
            "18": "For example, how comprehensible & easy to answer are questions?",
            "19": "Would they naturally lead to an answer that contains the correct fact, supposing when didn’t have a constrained set of possible answers?",
            "20": "Answering these questions are important if we’d want to generalize beyond the setup here, as generally we do not have access to a set of possible answers.",
            "21": "Indeed, one of the key challenging considerations of asking questions is that the model needs to ask not just a question that recovers the missing information — but recovers the minimal unit of information that is necessary to perform the end-task.",
            "22": "Otherwise, we can imagine a question like “tell me everything you know” being maximally useful under the recovery metric for all inputs.",
            "23": "The current task setup is unable to measure whether the questions \n\n3.",
            "24": "Related to the above, the paper claims that TOA is able to “generate plausible-sounding questions”.",
            "25": "It would be great to get empirical concrete evidence of this — perhaps through some evaluation of the wellformedness of the resulting questions.",
            "26": "There were some places in the description of the evaluation framework that were unclear / missing important details (see questions).",
            "27": "Can you report error bars for Figure 4?"
        }
    },
    "lwtaEhDx9x": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses a critical and often overlooked issue in the evaluation of Large Language Models (LLMs) on tabular data: data contamination and memorization.",
            "1": "- It introduces novel techniques to assess the degree of contamination and memorization in LLMs, which is crucial for ensuring valid performance evaluations on downstream tasks.",
            "2": "- The release of an open-source tool for performing these tests adds practical value and facilitates future research in this area.",
            "3": "Potential reasons for acceptance\n   - The paper tackles an important problem that has significant implications for the reliability of LLM evaluations on tabular data.",
            "4": "- It provides a comprehensive set of tests to detect contamination and memorization, offering a principled distinction between learning and memorization.",
            "5": "- The empirical results are thorough and demonstrate the efficacy of the proposed tests on multiple datasets.",
            "6": "- The open-source tool released by the authors can be a valuable resource for the research community.",
            "7": "Potential reasons for rejection\n   - **Lack of access to training data:**\n     - The authors do not have access to the training data of the LLMs (GPT-3.5 and GPT-4), which limits the ability to draw definitive conclusions about the extent of memorization.",
            "8": "- The paper relies on conjectures about the training data, which may weaken the strength of the findings.",
            "9": "- **Assumptions in tests:**\n     - Some of the proposed tests rely on assumptions, such as the existence of a canonical CSV file or the presence of highly unique feature values, which may not always hold true.",
            "10": "- The power of the tests is difficult to estimate, and there is a possibility that the LLM has memorized the data but the tests fail to detect it due to limitations in prompting.",
            "11": "- **Generality of findings:**\n     - The findings are tied to the capabilities of current LLMs (GPT-3.5 and GPT-4), and it is unclear how well the results generalize to other models or future versions of LLMs.",
            "12": "- The paper focuses on tabular data, and it is not clear if the proposed techniques can be easily adapted to other data modalities.",
            "13": "Suggestions for improvement\n   - **Access to training data:**\n     - If possible, obtain access to the training data of the LLMs to provide more concrete evidence of memorization and contamination.",
            "14": "- Alternatively, collaborate with organizations that have access to the training data to validate the findings.",
            "15": "- **Refine assumptions and tests:**\n     - Clarify and justify the assumptions made in the proposed tests, and explore ways to relax these assumptions to make the tests more broadly applicable.",
            "16": "- Investigate additional prompting techniques to improve the sensitivity of the tests in detecting memorization.",
            "17": "- **Expand scope:**\n     - Extend the analysis to other data modalities, such as free-form text or images, to assess the generality of the proposed techniques.",
            "18": "- Compare the performance of the proposed tests on a wider range of LLMs, including smaller models and models from different organizations.",
            "19": "- **Detailed analysis of zero-knowledge samples:**\n     - Provide a more detailed analysis of the zero-knowledge samples, including a comparison with other sampling techniques and a discussion of the implications for learning and memorization.",
            "20": "- Explore the impact of different sampling temperatures on the diversity and accuracy of the generated samples."
        },
        "WzTKlCpFu8": {
            "0": "- They propose several methods to test whether the language models were trained with those datasets.",
            "1": "- The idea of comparing the distribution generated by the model and the distribution in the datasets (Sec 3.3) is novel and interesting.",
            "2": "- The claim that they show that some language models are pretrained with some tabular datasets is somewhat convincing and interesting.",
            "3": "Main concerns:\n\n1.",
            "4": "This work does not provide strong evidence supporting the validity of their proposed approaches.",
            "5": "I think one main takeaway of this paper is that some models are pretrained with some datasets, so their performance is not indicative of.",
            "6": "But this takeaway is based on the validity of their proposed approaches.",
            "7": "I think the authors need to address this more.",
            "8": "I can’t understand the purpose of having these many different testing approaches, probably because the structure of this paper is hard to follow.",
            "9": "The authors propose many approaches, some of them are interesting, but they do not provide a holistic interpretation of the results from these many approaches.",
            "10": "The descriptions of the testing approaches are vague and not rigorous.",
            "11": "Writing down the testing approaches with simple math equations could help.",
            "12": "For example, in page 6, I can’t understand what it means by “we can perform a t-test between the similarity of model completions with actual vs. random rows.",
            "13": "Knowledge, learning, memorization should be defined more specifically.",
            "14": "The authors (claims to) show data contamination exists in some datasets.",
            "15": "However, I am not sure whether those datasets are commonly used to benchmark the language model.",
            "16": "Thus I am not sure whether the findings are important (if they are valid).",
            "17": "More specific (writing) issues:\n\n\n2.",
            "18": "The second paragraph in Sec 3.2: Here 4 possible causes are provided, but I don’t see how they are discussed in the following experimental designs.",
            "19": "The last sentence in page 5: “Empirically, we find … a very intuitive test …”.",
            "20": "I don’t understand how your empirical results support this.",
            "21": "Page 8: “It might be that this learning task is relatively simple, that our memorization test are not sensitive enough”.",
            "22": "I can’t understand why it is the case.",
            "23": "Grammar:\n\nThere are many grammar errors.",
            "24": "I suggest that the authors do some proofreading."
        },
        "qZSAjmfZao": {
            "0": "The paper presents several novel methods to evaluate memorization of tabular data in LLMs, and evaluation results on a series of datasets correlate well with the publication time and availability of the data, confirming the effectiveness of the proposed methods in identifying memorization.",
            "1": "The different evaluation methods also complement each other, elucidating the different aspects of memorization of tabular data.",
            "2": "The paper is overall well-written and very easy to read, the visualizations present the main findings nicely.",
            "3": "The contamination and memorization of training data by LLMs is a critical issue.",
            "4": "The findings provoke essential discussions on the evaluation of LLMs on tabular data, which is likely to become more relevant given the rising usage of LLMs in diverse tasks.",
            "5": "The introduced tools and code potentially provide easy and accessible ways to evaluate memorization of tabular data, reusable in future research.",
            "6": "Some important details in the experiment design may be missing or incomplete: \n\n* Evaluation metric for knowledge, learning, and memorization is unclear.",
            "7": "In Table 1, the evaluation results are categorized into three categories (✓,X, and ?",
            "8": "), but the metric for the categorization is not given.",
            "9": "It is probably a better idea to show the raw values (e.g., accuracy) than using categories to give the reader a direct comprehension of the degree of memorization on each dataset.",
            "10": "Notations such as \"✓\" could be misleading as it may be confused as perfect memorization.",
            "11": "The appendix gives raw accuracies for Row Completion Test, Feature Completion Test, and First Token Test, why raw accuracies for Feature Names, Feature Values, and Header Test are not provided as well?",
            "12": "* The differentiation between learning and memorization is not clear: the authors use feature distributions to examine learning, but memorization can also result in a high similarity of the generated data's feature distributions to the original data.",
            "13": "Learning is defined as the model's ability to perform tasks in the current paper, but task performance is heavily affected by memorization and may fail to reflect true learning.",
            "14": "Even with considerable discussion, the paper does not seem to arrive at a conclusion about how learning can be clearly assessed.",
            "15": "* Evalulation of memorization needs to take the nature of data fields into consideration.",
            "16": "Some data fields in the tabular dataset are considerably harder to memorize verbatim or to predict exactly (such as measurement values) than other simpler fields (categorical values such as sex, occupation, nation).",
            "17": "For numerical values, it may be more reasonable to measure the relative distance from the predicted value to the true value than using exact match (perhaps in a similar vein as the \"first token test\" in the paper but more principled).",
            "18": "Under the current evaluation protocol, it is likely that datasets containing more easy fields are more likely to be judged as memorized.",
            "19": "To compare the degree of memorization across datasets, it seems necessary to perform some kind of \"normalization\" before measuring memorization, for example, selecting a fixed number of categorical and numerical fields from each dataset.",
            "20": "Results in Table 3 could suffer from this limitation as well.",
            "21": "* Evaluation of memorization needs to be evaluated separately for the training and test split.",
            "22": "It may be possible that the training sets are memorized more than the test set due to more exposure on the internet.",
            "23": "Memorizing the test set definitely compromises evaluation, but memorizing the training set may not always compromise evaluation.",
            "24": "* Connection between memorization and downstream performance is not reliably established.",
            "25": "The main observation from Section 5 is that for datasets with a high degree of memorization, LLM performs better than decision tree and logistic regression, while for datasets with a low degree of memorization the reverse is true.",
            "26": "Such observation alone may not be sufficient to conclude that memorization compromises evaluation, because there is no evidence that LLM cannot perform better than decision tree and logistic regression under no memorization.",
            "27": "It would be much better to solicit new test sets for the tasks to use in evaluation, which can be used to show exactly how much performance gap is caused by memorization.",
            "28": "In case finding new examples is difficult, perhaps one can modify the values of the fields known to be irrelevant to the label in existing examples, and that may break the reliance on memorization in LLMs.",
            "29": "Some main conclusions of the paper are compromised because of the above limitations:\n\n* \"We emphasize the importance of verifying data contamination before applying LLMs\": the implication of data contamination is not reliably demonstrated in Section 5.",
            "30": "Also, from the current discussion, it is not very clear how to interpret the test results on knowledge, learning, and memorization together.",
            "31": "For example, if knowledge and learning show positive results and memorization show negative results, should we conclude that there is data contamination or not?",
            "32": "And could the performance on downstream tasks be trusted in this situation?",
            "33": "It can be argued that knowledge and learning will not directly compromise evaluation on downstream tasks, so there may not be as much need to evaluate them compared to memorization.",
            "34": "I would suggest allocating more space in the paper for extended experiments and discussions on memorization, which is the ultimate reason why people are concerned about data contamination.",
            "35": "* \"... and propose practical methods to do so\": the proposed method verifies memorization of data, but does not give a definite metric to judge when memorization is severe enough to compromise evaluation.",
            "36": "* \"We offer a principled distinction between learning and memorization in LLMs\": the distinction is not given clearly enough.",
            "37": "One can tell whether there is memorization from the proposed test, but it is not clear how to tell whether learning exists (especially when memorization is present)."
        },
        "MAjYHtyiHz": {
            "0": "* I believe this work (a) raises an important overlooked question, (b) addresses it, (c) by proposing an original technique.",
            "1": "I particularly like the zero-shot prompting technique that allows sampling from a dataset w/o leaking information in the prompt.",
            "2": "* The paper disentangles a few levels of training data contamination and comprehensively tests for those.",
            "3": "* The paper showcases the potential impact of the contamination on the downstream comparisons, hence proving a strong motivation to the work.",
            "4": "* The text and the story are clear.",
            "5": "* The code is made public.",
            "6": "* The paper only studies ChatGPT-3.5 and 4.",
            "7": "Those are very likely to be strongly correlated in terms of the data used, which harms the representativeness of the study.",
            "8": "* As there is no ground-truth knowledge on whether a particular dataset was seen at training, it is impossible to strictly verify the findings.",
            "9": "Including an LM trained on a known dataset would allow us to verify the used methods.",
            "10": "* Another related issue: the work is mostly relevant when we consider closed-data models w/ a black-box API access.",
            "11": "This scenario reflects a dominant situation at the moment, but it is not given that this will not/should not change.",
            "12": "Minor:\n* Table 1 is mentioned on page 3, yet only appears on page 6.",
            "13": "Is there a way to bring it closer?",
            "14": "* Would it make sense to consider swapping sections 5 and 6?",
            "15": "I feel the S6 is more connected to the S3-4 than S5."
        },
        "2QzbpIB3Le": {
            "0": "* LLMs are pervasive currently, and it's important to understand and control their behavior.",
            "1": "The authors emphasize the importance of verifying data contamination before applying LLM.",
            "2": "* Their setup based on tabular data is an elegant way to test “knowledge”, “learning”, and “memorization” of an LLM.",
            "3": "* Moreover, they assume only blackbox API access, without assuming access to the probability distributioin over tokens or the ability to re-train the model.",
            "4": "* Release of an open-source tool that can perform various tests for memorization.",
            "5": "* My main point of criticism is that the paper feels a bit like a collection of remarkable examples and the analysis largely confirms known concerns/behavior of LLMs.",
            "6": "* \"we also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim\": It's not clear to me what this statement means.",
            "7": "See also Question 1 below.",
            "8": "* Just echoing the authors: \"A limitation of our work is that we do not have access to the training data of GPT-3.5 and GPT4.\"",
            "9": "I.e., the interpretation of results often remains speculative.",
            "10": "* Figure 3: Why are some results with gpt-3.5 and some with gpt-4?",
            "11": "* Typo: \"two publicly available dataset that are highly memorized\"\n* Typo: \"UCI repository athttps://\""
        }
    },
    "iTrd5xyHLP": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper introduces LLMatic, a novel Neural Architecture Search (NAS) algorithm that leverages the code-generating capabilities of Large Language Models (LLMs) and the robustness of Quality-Diversity (QD) optimization.",
            "1": "- The approach is significant as it combines the strengths of LLMs in generating diverse neural network architectures with the efficiency of QD algorithms in exploring a wide search space.",
            "2": "- The novelty lies in the procedural approach of using LLMs for NAS, which is a departure from traditional methods like reinforcement learning and evolutionary computation.",
            "3": "Potential reasons for acceptance\n   - The integration of LLMs with QD optimization is a novel approach that could inspire further research in the field of NAS.",
            "4": "- The empirical results on CIFAR-10 and NAS-bench-201 benchmarks demonstrate the effectiveness of LLMatic in finding competitive neural network architectures with fewer evaluations.",
            "5": "- The extensive ablation studies provide a thorough analysis of the contributions of different components of LLMatic, adding credibility to the proposed method.",
            "6": "- The open-sourced code allows for reproducibility and further exploration by the research community.",
            "7": "Potential reasons for rejection\n   - **Limited Benchmarking:**\n     - The experiments are limited to CIFAR-10 and NAS-bench-201, which may not fully demonstrate the generalizability of LLMatic to other datasets and tasks.",
            "8": "- The comparison with other state-of-the-art NAS methods is not exhaustive, potentially limiting the perceived impact of the proposed method.",
            "9": "- **Computational Constraints:**\n     - The use of a 6.1B parameter LLM (CodeGen) may not be accessible to all researchers due to computational resource constraints.",
            "10": "- The paper does not explore the scalability of LLMatic with larger LLMs or more extensive search spaces, which could be a limitation for practical applications.",
            "11": "- **Clarity and Detail:**\n     - The description of the procedural approach and the integration of LLMs with QD optimization could be more detailed to enhance understanding.",
            "12": "- The pseudocode provided in the supplementary material could be more comprehensive to facilitate replication of the experiments.",
            "13": "Suggestions for improvement\n   - **Expand Benchmarking:**\n     - Include additional datasets and tasks, such as other computer vision benchmarks (e.g., ImageNet) and natural language processing tasks, to demonstrate the generalizability of LLMatic.",
            "14": "- Compare LLMatic with a broader range of state-of-the-art NAS methods, including recent advancements in reinforcement learning and evolutionary computation.",
            "15": "- **Address Computational Constraints:**\n     - Explore the scalability of LLMatic with larger LLMs and more extensive search spaces to provide insights into its practical applicability.",
            "16": "- Investigate the performance of LLMatic with smaller LLMs to make the approach more accessible to researchers with limited computational resources.",
            "17": "- **Enhance Clarity and Detail:**\n     - Provide a more detailed explanation of the procedural approach and the integration of LLMs with QD optimization to improve the clarity of the methodology.",
            "18": "- Include more comprehensive pseudocode and additional implementation details in the supplementary material to facilitate replication and further exploration by other researchers.",
            "19": "- **Future Work Directions:**\n     - Suggest potential applications of LLMatic in other domains, such as open-ended learning and reinforcement learning benchmarks, to highlight the versatility of the proposed method.",
            "20": "- Discuss potential improvements and extensions to LLMatic, such as incorporating transfer learning or multi-objective optimization, to inspire future research."
        },
        "4XwBmmlHEd": {
            "0": "After reading the manuscript, I summarize the below strengths.",
            "1": "- The topic of the paper is new and interesting.",
            "2": "Leveraging generative models may produce impacts into the NAS field.",
            "3": "- The designed algorithm makes sense that should produce some high-performing DNNs.",
            "4": "Meanwhile, I have following concerns.",
            "5": "- The presentation of the paper is not satisfactory.",
            "6": "Especially the pseudocode, which lacks proper mathematical annotations.",
            "7": "Meanwhile, there exist a lot of typos, e.g., 'ta simple' on page 4.",
            "8": "The legends in Figures are invisible, etc.",
            "9": "- Again the presentation, the key components in the algorithm such as mutation operators, crossover operators and temperature mutation lacks proper description and explanations.",
            "10": "It makes the algorithm unclear.",
            "11": "- This paper looks more like an investigation paper to explore how to use LLM on generating new DNNs, yet lacks sufficiently novel algorithm to guide LLMs generating DNNs of higher fidelity.",
            "12": "The rank on ImageNet is significantly lower than CIFARs, which increases my concern regarding the effectiveness of the proposed algorithms on general tasks."
        },
        "vFMe6YyWJY": {
            "0": "- I like the idea of carrying out NAS directly on the code-level instead of an abstract representation such as a DAG, as it makes the search flexible.",
            "1": "This also removes the need for a compiler from the abstract model description to the code which can save time and hence lower the barrier to actually use NAS.",
            "2": "Joint NAS and HPO may also be expressed in a very natural way in this paradigm.",
            "3": "- Using quality diversity in this paradigm makes sense to me, not just for diverse hardware requirements, but also for good ensembling.",
            "4": "- The ablation experiment nicely illustrates the contribution of each component.",
            "5": "- Could you run additional experiments on Nas-Bench-101?",
            "6": "As the search space is constrained in a different way than 201, I would be interested to see whether your approach can remain within the search space at all times.",
            "7": "- Could you provide optimization trajectories for Nas-Bench-201 along with a baseline such as random search?",
            "8": "I am curious how your approach compares with more traditional approaches in terms of any-time performance.",
            "9": "- Overall the visual quality of the paper may be improved.",
            "10": "In Figures 2 and 3 the white space around each Figure should reduced.",
            "11": "Figure 4 should be changed to a jpg or contain a reduced number of points because it causes rendering issues.",
            "12": "Also please increase the fontsize to be at least footnotesize for all figures.",
            "13": "- I find Algorithm 1 to be hard to read.",
            "14": "As you don't define each function you use in the pseudo code, you could consider writing short sentences instead."
        },
        "z1rc3MMmXc": {
            "0": "- The use of large models' knowledge for network structure generation provides an example of applying large models in practice.",
            "1": "- While utilizing the code generation capability of large models for network structure generation is a good application, previous works such as GENIUS have already proposed very similar methods and demonstrated prompts.",
            "2": "Therefore, the novelty and contribution of this paper appear insufficient.",
            "3": "- The experimental description is not detailed, and the analysis is not sufficiently deep.",
            "4": "For example, Table 1 does not report the search cost of this paper compared to related works.",
            "5": "- There are relatively few experimental comparisons, and the comparison with related works is not comprehensive enough.",
            "6": "- After reading this paper, I did not get much insight.",
            "7": "Importantly, the assistance of prior knowledge from LLMs in network structure design remains unclear."
        },
        "nxh59jW2zv": {
            "0": "- The application of code-generating LLMs effectively for NAS is very novel.",
            "1": "Defining architecture generation as a language/code modelling task is a new way of formulating the neural architecture search problem.",
            "2": "- The presentation of the paper is mostly clear except in some parts (refer to suggestions and questions in the weakness and questions section)\n- Evaluation on the NB201 dataset and CIFAR10 dataset fairly exhaustive and well ablated.",
            "3": "- The authors release their code and additional details on the prompts used - Evaluation is limited: Currently NAS focuses a lot of transformer-based spaces [1], mobilenet-spaces [2] which are more practical and realistic compared to cell-based spaces.",
            "4": "A lot of thee approaches release a surrogate predictor or the supernet itself, to save training times ie the architecture training part in algorithm 1.",
            "5": "I recommend the authors evaluate the method on these search spaces too.",
            "6": "This is also in my opinion very important to study how llmatic scales across larger architecture definitions (code) and datasets eg: ImageNet.",
            "7": "- Comparison to black box approaches and generative approaches: Since llmatic requires training a lot of architectures (or querying a benchmark multiple times), its search time is more comparable to black-box approaches instead of approaches like lambda-darts.",
            "8": "It would be great to add other black-box methods (in addition to random search) to Table 1.",
            "9": "Furthermore since the work very much falls in the line of generative NAS a comparison with DiffusionNAG [3] would also be great.",
            "10": "- Clarity: In the current version of the paper in figure 4, I couldn't see the green points referring to llmatic architectures, am I missing something?",
            "11": "- Minor : Page 4 2nd paragraph \"In the first generation , ta simple neural network with one convolutional and one fully connected\nlayer initiates the evolution\" -> \"In the first generation , take simple neural network with one convolutional and one fully connected\nlayer initiates the evolution\"\n\n[1] Chen, M., Peng, H., Fu, J. and Ling, H., 2021.",
            "12": "Autoformer: Searching transformers for visual recognition.",
            "13": "In Proceedings of the IEEE/CVF international conference on computer vision (pp.",
            "14": "12270-12280).",
            "15": "[2] Cai, H., Gan, C., Wang, T., Zhang, Z. and Han, S., 2019.",
            "16": "Once-for-all: Train one network and specialize it for efficient deployment.",
            "17": "arXiv preprint arXiv:1908.09791.",
            "18": "[3] An, S., Lee, H., Jo, J., Lee, S. and Hwang, S.J., 2023.",
            "19": "DiffusionNAG: Task-guided Neural Architecture Generation with Diffusion Models.",
            "20": "arXiv preprint arXiv:2305.16943."
        },
        "fbIQrEiv6s": {
            "0": "- the idea of employing LLMs for NAS is novel and aligns with the emerging trend of leveraging these models in diverse fields, considering that LLMs have been applied in domains with structured data formats such as drug design and materials science, - LLMatic's performance heavily depends on the LLM's prior exposure to relevant coding patterns and neural network architectures during its training.",
            "1": "This dependence could be a potential weakness, as the model might not generalize well to novel or highly specialized architectural search spaces.",
            "2": "- The paper focuses on CIFAR-10 and NAS-bench-201 for performance evaluation.",
            "3": "While these are standard benchmarks, the scope of testing could be broadened to include more diverse and challenging datasets.",
            "4": "This would provide a more comprehensive understanding of LLMatic's capabilities and potential limitations, especially in real-world scenarios or more complex tasks.",
            "5": "- The paper could benefit from ablation studies to understand the contribution of each component of LLMatic to its overall performance.",
            "6": "- The writing should improve."
        }
    },
    "gCjeBKuDlc": {
        "llm_review": {
            "0": "Significance and novelty\n   - The paper addresses the challenging problem of visual commonsense reasoning (VCR) by leveraging the complementary strengths of pre-trained vision-and-language models (VLMs) and large language models (LLMs).",
            "1": "- It introduces a novel framework, ViCor, which categorizes VCR into visual commonsense understanding (VCU) and visual commonsense inference (VCI), and proposes a collaborative approach where LLMs guide VLMs to focus on relevant visual elements.",
            "2": "- The proposed method outperforms existing methods that do not require in-domain supervised fine-tuning on two benchmark datasets, demonstrating its effectiveness and generalizability.",
            "3": "Potential reasons for acceptance\n   - The paper presents a well-motivated and novel approach to VCR, addressing the limitations of existing methods that rely heavily on supervised training and fine-tuning.",
            "4": "- The proposed ViCor framework effectively combines the strengths of VLMs and LLMs, leading to improved performance on VCR tasks without the need for in-domain fine-tuning.",
            "5": "- The experimental results on two benchmark datasets (VCR and A-OKVQA) show that the proposed method achieves state-of-the-art performance among methods without supervised in-domain fine-tuning.",
            "6": "- The paper provides a thorough analysis of the problem, including a detailed categorization of VCR tasks and an ablation study to validate the effectiveness of the proposed approach.",
            "7": "Potential reasons for rejection\n   - **Lack of clarity in problem categorization and task definitions**\n     - The distinction between VCU and VCI may not be clearly defined or justified, leading to potential confusion about the scope and applicability of the proposed method.",
            "8": "- The paper could benefit from more concrete examples and clearer explanations of how VCU and VCI tasks differ and why this categorization is necessary.",
            "9": "- **Insufficient evaluation and comparison with state-of-the-art methods**\n     - While the paper compares the proposed method with some existing methods, it may not provide a comprehensive evaluation against the latest state-of-the-art methods, especially those that use supervised fine-tuning.",
            "10": "- The evaluation on the VCR dataset is limited to a subset of the validation set, which may not provide a complete picture of the method's performance.",
            "11": "- **Limited discussion on potential limitations and future work**\n     - The paper briefly mentions some limitations, such as the reliance on text as the communication medium between LLMs and VLMs, but does not provide a detailed discussion on how these limitations could be addressed in future work.",
            "12": "- The potential impact of the loss of visual details caused by captions is not thoroughly explored, and alternative approaches to mitigate this issue are not discussed in detail.",
            "13": "Suggestions for improvement\n   - **Clarify the problem categorization and task definitions**\n     - Provide more concrete examples and clearer explanations of the differences between VCU and VCI tasks, and justify why this categorization is necessary for the proposed method.",
            "14": "- Include a detailed discussion on how the proposed method handles different types of VCR tasks and how it can be adapted to other related tasks.",
            "15": "- **Enhance the evaluation and comparison with state-of-the-art methods**\n     - Conduct a more comprehensive evaluation against the latest state-of-the-art methods, including those that use supervised fine-tuning, to provide a more complete picture of the method's performance.",
            "16": "- Expand the evaluation on the VCR dataset to include the full validation set, and consider additional benchmark datasets to further validate the generalizability of the proposed method.",
            "17": "- **Discuss potential limitations and future work in more detail**\n     - Provide a more detailed discussion on the limitations of the proposed method, including the reliance on text as the communication medium and the potential loss of visual details caused by captions.",
            "18": "- Explore alternative approaches to mitigate these limitations, such as using visual embeddings or other communication mediums between LLMs and VLMs.",
            "19": "- Outline potential directions for future work, including fine-tuning approaches and the integration of additional visual tools to further enhance the method's performance."
        },
        "gEKSftVrAy": {
            "0": "- The studied problem - visual common sense reasoning, is useful and practical for evaluating large models' reasoning capabilities.",
            "1": "- The finding that the captions cannot be used for answering questions is interesting to know.",
            "2": "- The re-definition of visual common sense reasoning is not convincing at all.",
            "3": "There is a large overlap between the two sub-problems of visual common sense understanding and visual common sense inference.",
            "4": "Moreover, reasoning and understanding are also very close.",
            "5": "- Even with this new definition, the authors still perform their experiments on the VCR dataset, which is extremely confusing.",
            "6": "- There is no explicit definition of VCI.",
            "7": "- In fact, the definition of VCU is no different from that of image text retrieval.",
            "8": "- Fig.4, I think, should be changed with a table rather than drawing a table."
        },
        "q1IfDKSKiV": {
            "0": "This paper provides important insights about the comparative advantages of VLMs and LLMs, and introduces an effective framework where they can collaborate.",
            "1": "Concretely, VLMs are better at recognizing literal visual content.",
            "2": "Their contrastive pretraining has equipped them with strong image-text alignment capabilities.",
            "3": "However, VLMs lack commonsense or world knowledge.",
            "4": "Thus VLMs could be benefit from LLMs providing texts including meaningful visual clues to compute alignment scores.",
            "5": "On the other hand, LLMs posses a wealth of  commonsense and world knowledge, and are better at expanding or decomposing problems.",
            "6": "But LLMs do not have direct access to visual information, thus would require a VLM to act at their commands.",
            "7": "Studies in this paper have attempted to revealed that:\n- VLM as the decision model is suboptimal due to the lack of overall reasoning ability, unless the task is as simple as recognizing the literal visual content.",
            "8": "- LLM as the decision model works only when it partners with a VLM and queries the VLM with **visual-clue**-rich texts rewritten from the original textual question.",
            "9": "- The collaborative paradigm between VLM and LLM mitigates mistakes originated from a) easily overlooked visual clues in the surroundings, b) lack of explicit mentions of relevant visual factors in the question, c) misdirecting objects in the foreground.",
            "10": "- Judging from Table1, the number of examples studied is very small.",
            "11": "It is unclear if the evidence derived from the results is robust.",
            "12": "- No multiple runs across decoding configs (e.g.",
            "13": "temperature, selection of in-context examples).",
            "14": "This limits the robustness and generality of the findings.",
            "15": "- Judging from Table1's LLM section: I'm having a hard time drawing conclusive insights from these results.",
            "16": "LLM+Caption wins in two columns, LLM+Caption+VQA wins in two columns, while LLM+Caption+LLMclue wins in 4 columns.",
            "17": "None of the settings is consistently stronger.",
            "18": "Nor do the results suggest a consistent way to choose settings based on the problem category.",
            "19": "I believe that demonstrating how LLM and VLM can wisely collaborate in reasoning tasks is a direction worth pursuing.",
            "20": "Therefore, I don't question the motivation of this paper.",
            "21": "However, this paper only produced preliminary results on small validation sets and with a single set of decoding configurations.",
            "22": "So, the results might lack generality and comprehensiveness.",
            "23": "More comprehensive experiments across larger datasets and multiple seeds/decoding configurations would significantly strengthen the arguments the authors have sought to put forth."
        },
        "AncuRU2D59": {
            "0": "The proposed method is well-motivated.",
            "1": "Current pre-trained VLMs do not extract visual context based on the input questions.",
            "2": "The proposed method can address this problem.",
            "3": "Existing methods, such as BLIP2, instructBLIP, and mini-GPT4, align the visual context with the LLM inputs embedding instead of input words and achieve better performance.",
            "4": "It is not clear why the proposed method uses words(caption or VQA result) to transfer information from the VLM to LLM.",
            "5": "The paper lacks implementation details of the proposed model and compared methods.",
            "6": "BLIP2 has a different setting to obtain the answer to the original one."
        },
        "Gmzr5WF5sU": {
            "0": "This paper provides strong empirical results when combining proprietary language models such as GPT with vision-language models for solving VCR problems.",
            "1": "I wouldn't want to dismiss the paper as \"combination of multiple proprietary blackboxes\" (although it probably is that) -- the paper does demonstrate that there are novel ways to leverage these tools for solving challenging problems in vision.",
            "2": "The paper is well written and well explained to someone who is already familiar with the advances in this domain.",
            "3": "See Weakness 4 for the flip side.",
            "4": "Experiments could be more exhaustive -- for instance, why not expand the experiments into more VCR datasets such as OKVQA (Marino et al.",
            "5": "), VisualComet (Park et al.",
            "6": "), V2C (Fang et al)?",
            "7": "The pipeline doesn't seem to be specific to VCR and could be used for any VQA dataset (eg.",
            "8": "VQAv2, GQA, CLEVR, etc.)",
            "9": "-- it's not clear whether the proposed method also improve performance on these datasets.",
            "10": "In practice, questions to a real-time system could be of any type (those about commonsense or those about simple perception) -- so it would be important to improve performance on both.",
            "11": "In Table 1, it is unclear how each dataset is divided into two parts VCU and VCI for evaluation.",
            "12": "The paper is well written and well explained to someone who is already familiar with the advances in this domain, but this assumption could have limiting effects on who learns from the paper -- one of the advantages of publishing NLP papers in ICLR is a wider reach to the broad ML community (and a large part of this community does not work on NLP or LLMs)."
        }
    }
}