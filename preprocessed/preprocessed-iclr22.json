{
    "pMQwKL1yctf": {
        "1T7HoNYk9ae": {
            "0": "I really like the main modelling contribution of this paper.",
            "1": "It is this reviewer's personal opinion that to do long-form text generation, it is not enough to generate token-by-token, but that some high-level planning is required, and the Brownian bridge process model (Time Control; TC) the authors propose is definitely a good candidate to model the latent drift of discourse (indeed, papers like [1] already used random walk-style models to explain properties of word vectors).",
            "2": "There are some prior works on using structured probabilistic models, such as switching latent dynamical systems, for text generation [2], which should also be cited.",
            "3": "The motivation of the model present is clear, and the description of how the model is trained is generally clear enough to reimplement.",
            "4": "It wasn't immediately clear that training the model on triples only is enough to guarantee general Brownian bridge dynamics for the entire text trajectory, I feel a note should be added to clarify this.",
            "5": "My other quibble here is with how the model is presented: although the general probabilistic model is written down in Equation 1, the likelihood function (i.e.",
            "6": "the functional form of p(z_t | x_0, x_t, x_T)) is not explicitly written down anywhere, which leads to confusing things like the variance of the process \\sigma^2 being used in Equation 3 without prior introduction.",
            "7": "I feel like explicitly writing down the likelihood would make the equations in the paper flow much better.",
            "8": "I feel the major weakness of this paper is with the experimental sections.",
            "9": "For various reasons, I have objections to each of the experiments, which I will go through below:\n\n- The first experiment attempts to show that TC is a better model of local discourse coherence.",
            "10": "The authors take two sentences from a document k steps apart, embeds them and them attempts to predict the sentence ordering from the embeddings.",
            "11": "They say that for k=1 all models considered perform at chance level on all datasets, and only show results for k=5 and k=10.",
            "12": "However, models trained using the k=1 objective (such as ALBERT [3] and StructBERT[4]) seem to be able to perform the task better than chance, so theoretically this should be possible.",
            "13": "Therefore, I think the baselines should at least include an ALBERT model to show the performance upper bound on this problem.",
            "14": "Further, k=5 (or even 10) starts meaning the sentences start becoming very far apart (10 dialogue turns is more than enough to complete some of the simple dialogue agent tasks!",
            "15": "), so it's questionable whether the model is really modelling 'local' dynamics at this point.",
            "16": "- The second experiment looks at text infilling on the ROCStories dataset, and use BLEU and BLEURT to automatically evaluate their models (although the BLEURT results do not appear to be anywhere in the paper).",
            "17": "The reported BLEU results are really low, to the extent that it's unclear whether an improvement from 2 to 5 BLEU is really meaningful.",
            "18": "Part of the issue is that BLEU measures precision, which penalises text generation where there are a variety of possible outputs; for this reason, [2] report ROUGE results on ROCStories, which are much better.",
            "19": "The missing BLEURT results would help contextualise model performance here.",
            "20": "The human evaluation shows the model performs about as well as the ILM baseline from [5], which is ok I guess?",
            "21": "In addition, the table ordering is incredibly confusing.",
            "22": "Table 6, which shows the human evaluation for experiment 2, appears much later in the text, after tables for the later experiments.",
            "23": "It took me a long time to find it.",
            "24": "Can you group the tables a bit better, in thematic order?",
            "25": "- The third experiment attempts to measure 'global text dynamics' by measuring length mismatch per section on Wikisections.",
            "26": "It's unclear what notion of 'global text dynamics' the authors are referring to - there are many theories on discourse coherence of long text, and none of them easily map onto a simple measure of section length.",
            "27": "If the authors simply mean whether the model has learnt a notion of document structure, I think it would be better to be more explicit about this: showing that fine-tuned GPT2 can't even replicate the structure of a homogenous document corpus is an interesting negative result.",
            "28": "- The fourth experiment forces models to generate beyond the expected document length by suppressing generation of the EOD token.",
            "29": "I'm really not a fan of this experiment, because I don't even expect TC to perform well on it.",
            "30": "Do the authors just keep on conditioning the decoder on z_T, and force the model to generate from this?",
            "31": "At this point, the model is just a standard autoregressive model, so the modelling contribution should have no effect.",
            "32": "Alternatively, do the authors resample z_{t+1} each time the model finishes generating a sentence?",
            "33": "In which case, how do the authors preserve the Brownian bridge dynamics, conditioning on hitting a target state z_T?",
            "34": "There are a few methodological issues with this experiment.",
            "35": "A better experiment to run would be to simply ask the human annotators to score texts freely generated from GPT2 and TC for coherence, as a measure of how well TC can generate coherent text.",
            "36": "Overall, while the experimental section is weak, I really believe the core idea of directed Brownian dynamics for planning is a cool one, and deserves to be shared more widely.",
            "37": "This is why I recommend acceptance.",
            "38": "References: \n- [1]: RAND-WALK: A latent variable model approach to word embeddings, Sanjeev Arora et al.",
            "39": "2015\n- [2]: Generating Narrative Text in a Switching Dynamical System, Noah Weber et al.",
            "40": "2020\n- [3]: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, Zhenzhong Lan et al.",
            "41": "2021\n- [4]: StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding, Wei Wang et al.",
            "42": "2021\n- [5]: Enabling Language Models to Fill in the Blanks, Chris Donahue et al.",
            "43": "2020\n\n\n==================\n\nPost author response:\n\n> Nonetheless, we think these observations fit well with the intuition our work proposes: Neighboring sentences are close to each other and act like Brownian motion where ordering is difficult to infer, and goal-orientedness / discourse structure emerges on longer stretches of sentences in a document.",
            "44": "I like this framing - currently it's implicit in the paper, but maybe it can be made more explicit that we expect the larger k results to be better, and that this verifies the Brownian bridge approach towards modelling text dynamics.",
            "45": "> Nonetheless, the end arbiter of this task is a human (how coherent do the generations sound to a human?)",
            "46": "and we care about at least matching ILM, a method developed specifically for text-infilling.",
            "47": "So, it’s promising that our method performs better and/or competitively with ILM on human-based metrics (BLEURT and Human evaluations in Table 6).",
            "48": "I think it should be made explicit then that ILM is in effect an upper bound for model performance, as it is a model trained specifically to do the task, and that matching the performance of ILM is actually a strong result for the TC model.",
            "49": "> So, to directly answer the reviewer’s question: we do not condition the decoder on z_T and do not resample new latents during generation.",
            "50": "The model is thus primed to generate much longer text than it was typically exposed to?",
            "51": "Thank you for the clarification.",
            "52": "> We in fact do already ask human annotators to score the generation (rf.",
            "53": "Table 7).",
            "54": "In this setup, we remove the middle section of the generated output as the text is extremely long.",
            "55": "See Figures 3-6 for examples of the full forced long text generation results.",
            "56": "I believe the stronger (and more realistic) human evaluation is to not just evaluate the tail coherence on forced long text generation, but  instead directly sample from the model naturalistically and evaluate that output using human annotators.",
            "57": "If TC better captures global coherence, this should be visible even in this setting.",
            "58": "Overall, I would like to thank the authors for their response.",
            "59": "Many of my concerns have been addressed, and I am happy to increase my score."
        },
        "Ct0RJ731kF3": {
            "0": "This paper has an interesting approach and tackles an important problem of streamlining sequence generation from autoregressive models.",
            "1": "The experiments show the value of learning a manifold over the latents that have a linear relationship with some stochastic perturbation.",
            "2": "They provide evidence that learning in such a manner is promising in order to maintain coherence over long text generation.",
            "3": "However, the setting is fairly limited because this approach requires two contextual endpoints, the start and finish.",
            "4": "This is especially underwhelming given that the introduction states that this approach aims to perform \\emph{controllable goal-oriented} generation.",
            "5": "In my view, the setting described and experimented with doesn't reflect this goal.",
            "6": "For example, there are limited experiments with regard to controllable generation, or goal-oriented generation tasks.",
            "7": "Secondly, the assumption that autoregressive generation follows a Brownian motion is strong and I would like to see some empirical evidence or theoretical argument supporting this.",
            "8": "One simple experiment could be to actually try to fit a Brownian motion model to a bunch of sequences generated from GPT-2, and show that this fitted model is not suitable for naturally occurring text.",
            "9": "Experiment wise, my biggest concern is the VAE baseline.",
            "10": "The point of this baseline is to show that for the same setup of Brownian bridge process, contrastive learning is better than the VAE objective, but I feel that the VAE implementation as described in the appendix does not make the comparison fair.",
            "11": "Due to lack of details in the paper, I am assuming that the priors p(z0) and p(zT) are standard gaussians.",
            "12": "If this is not true, then a clarification would ease this concern of mine.",
            "13": "But assuming this is true, the loss basically tries to match the encoder distributions q(z0) and q(zT) obtained by f_{\\theta}(x0) and f_{\\theta}(xT) to the standard Gaussian.",
            "14": "What this means is that there is a pressure to make the 0 and T embeddings similar which is not at all what we want from this bridge process model.",
            "15": "A more careful instantiation of prior for VAE, or even learning a time sensitive prior would be a better implementation of the VAE baseline.",
            "16": "Table 1 is another concern.",
            "17": "This experiment basically trains a linear classifier over the encodings to identify if they are in-or-out of order.",
            "18": "The proposed approach is naturally suited for this metric/classifier because the encodings at different times are more or less linearly related with some stochasticity.",
            "19": "However, this is not true for the other baselines, so I am not sure what is the takeaway message from this experiment.",
            "20": "Also, more exposition on the Brownian motion baseline would be helpful.",
            "21": "The current description is not enough to get an idea about what exactly was done for generation and other experiments with this baseline.",
            "22": "On a related point, I don't get why BM for Table 2 would be the same as the brownian bridge.",
            "23": "Isn't it the case that Brownian motion baseline doesn't get to see \\emph{both} the endpoints?",
            "24": "If I am mistaken about this, then more exposition is required here because I checked both the paper and the appendix carefully for this.",
            "25": "Table 5 shows mixed results.",
            "26": "More discussion and analysis here would be helpful.",
            "27": "For clarification: please make explicit whether the triplets have a notion of distance or not i.e.",
            "28": "it is sensitive to different value of t depending on which sentence in the middle was sampled.",
            "29": "From the context, I am assuming this is the case but clarification would be helpful.",
            "30": "Also, notation in equation 2's denominator is confusing.",
            "31": "Are you summing over all the negative x_{t'}?"
        },
        "mILODfUxog": {
            "0": "This paper proposes a generation from a language model not only from an initial\nstate, but also using a goal state.",
            "1": "Instead of Brownian motion, the authors\nemploy a draw from Brownian bridge by designating initial and end states,\ncalled Time Control.",
            "2": "Experimental results show the proposed generation from Brownian bridge is more\nnatural and coherent for text-infilling task, and also preserves text \nstructures both by automatic evaluation and human evaluation.",
            "3": "Using Brownian bridge is a very simple and effective idea for text generation.",
            "4": "My only concern is the range of its applicability: while it is far more natural\nthan a simple random walk, Time Control only allows designating the first and\nlast states for generation.",
            "5": "However, in the actual situation, it is not always\nthe case for the first (and sometimes, last) sentence should have a designated\nstate.",
            "6": "First few sentences might constitute just an ice-break, and the actual \ncontent might start after that.",
            "7": "More generally, it is more desirable that we can condition the generation at\narbitrary time.",
            "8": "In fact, I think that this can be done by a conditional draw\nfrom a Gaussian process.",
            "9": "Since Brownian motion corresponds to using an \nexponential kernel of GP, sentence generation from conditional GP would be \nthe way for the future extention of this work.",
            "10": "Anyway, this work will surely pave the way for such principled generations.",
            "11": "Minor\n\n- Some tables are located within the main text.",
            "12": "Tables and Figures should be\nplaced top or bottom of the paper for readability: please use \\begin{figure}[t]\nfor something like that.",
            "13": "- Numerical results in Tables can be rendered in a smaller font (i.e.",
            "14": "\\small).",
            "15": "Also I recommend to condense line spacing for Tables for readability, using \n\\usepackage{setspace} and begin{spacing}{0.9} ... end{spacing}, for example."
        },
        "7_d4nPUism2": {
            "0": "**Pros**\n\n- The paper is well structured and easy to follow, the idea of modeling sentences to a Brownian bridge latent space is neat and generic enough to (1) allow for noise given its stochasticity (2) doesn't require explicit domain knowledge for planning.",
            "1": "- Well Structured Experiments sections with 4 RQs and results that confirm each of the hypotheses\n\n- Reproducibility and Transparency in reporting of experiments in terms of available source code, dataset information, details about human evaluation, generation examples.",
            "2": "**Areas of Enhancement & Questions to authors**\n\n- The information about each of the ablations (ID, BM) could be explained better.",
            "3": "namely the section \"ablations\".",
            "4": "- There's a clear Inconsistency in the best TC method between different latent dimensions (8,6,32), in most of the experiments there's at least one of the 3 that is performing drastically worse than the other baselines, while there's overall no clear winner.",
            "5": "I wonder if you have thoughts about this.",
            "6": "- Table 5 the VAE(32) method performs the best overall in \"Wiki section\" although the TC (16) method has been highlighted as the best.",
            "7": "Is there a reason behind this?",
            "8": "- During the training of the decoder how do you make sure that the decoder uses the information given by the latent plan?",
            "9": "- Overall the paper would have benefited from an intrinsic visualization of the latent space, to make sure for example that there's no  Information collapse of the embeddings when dealing with long sentences.",
            "10": "This could be done by visualizing the planning trajectory difference between coherent and incoherent text."
        }
    },
    "G89-1yZLFHk": {
        "YdBRQRFk2NG": {
            "0": "Strengths\n1)\tThe authors provide an interesting and solid idea based on CLIP.",
            "1": "2)\tThe presentation is very good and easy to follow.",
            "2": "The authors clearly present their ideas and describe the technical details.",
            "3": "3)\tThe ablation study and visualization analysis of the experimental results are sufficient.",
            "4": "The visualization of OTTER’s matching illustrates its effectiveness in handling many-to-many relationships.",
            "5": "Weakness\nWhile the ablation study and visualization analysis are done, the key evaluations are missing.",
            "6": "The details can be found as follows.",
            "7": "The experiment results in Tables 1 & 2 cannot plausibly prove OTTER is more effective than CLIP.",
            "8": "The larger dataset may relate to more many-to-many relationships when training the model.",
            "9": "For example, in the visualization in Figure 2, the maximum many-to-many relationship of a sample is 3.",
            "10": "However, the dataset with 400M may contain too many many-to-many relationships like 7 or 8 (maybe more, the data scale is about 100 times the used datasets in this paper).",
            "11": "In this case, OTTER may be degraded due to the multiple learning targets.",
            "12": "In contrast, focusing on a solid target and ignoring the rest of potential targets, CLIP may still infer the generalized information provided by many-to-many relationships due to the wide range of data collection.",
            "13": "Therefore, it is necessary to compare OTTER and CLIP on the same-scaled datasets.",
            "14": "The authors provide OTTER using hard labels (InfoNCE) as a baseline, but ZSL methods are sensitive to hyper-parameters and training sets.",
            "15": "I doubt that InfoNCE can represent the best performance of CLIP trained on CC (3M) and WIT(5M).",
            "16": "Regarding that CLIP does not release the 400M dataset (mentioned in this paper by the authors), the authors may not be able to train OTTER on the 400M dataset.",
            "17": "The provided experiments are more like baselines for self-evaluation other than state-of-the-art performance.",
            "18": "Without the experimental results using the same training settings, I do not think the authors are able to prove the superiority of OTTER over CLIP fully."
        },
        "tW9Yiv5oyL0": {
            "0": "---\nStrengths:\n\n1.",
            "1": "The work has a clear hypothesis and is well-motivated.",
            "2": "Many-to-many relationships / soft correspondences are generally important for vision-and-language learning since there is never really one unique and perfect correspondence between an image and text.",
            "3": "Additionally, improving data efficiency of pre-training vision-and-language models can be beneficial to the community as many recent works rely on enormous amounts of image-text data.",
            "4": "Others would likely upon this work in a similar direction.",
            "5": "2.",
            "6": "The proposed method addresses the issue of many-to-many relationships in a technically valid way, and doesn't increase the complexity of the general approach significantly\n\n3.",
            "7": "The authors present experiments with a good set of baselines (LS, KD, InfoNCE) as alternative methods and ablation studies on each component of their approach\n\n4.",
            "8": "The experiments show that the proposed approach consistently outperforms alternative approaches (baselines)\n\n\n---\nWeaknesses:\n\n\n1.",
            "9": "Although the proposed approach doesn't increase the complexity significantly, it does introduce a number of new hyperparameters – many of which the models seem highly sensitive to (Table 3).",
            "10": "Intuitively, different datasets might have more or less similarities between samples, noise level, etc.",
            "11": "- therefore the hyperparameters that work well on CC & WIT, which are of relatively high quality, might not be very transferable to more noisy datasets (like the one used in CLIP, for example).",
            "12": "The ablation studies indicate that, e.g., batch size has a very significant impact on the performance, and intuitively the best choice should depend on the particular dataset since, for a given batch size, the average number of similar samples might vary greatly among different datasets.",
            "13": "2.",
            "14": "The comparison with ZSL methods trained on ImageNet1K (Table 2) is misleading.",
            "15": "These ZSL methods are trained on ImageNet1K and evaluated on ImageNet21K because of a relatively clear separation between seen and unseen (novel) classes.",
            "16": "However, the proposed approach is trained on the CC dataset, which likely has a substantial overlap of concepts with the test set (ImageNet21K).",
            "17": "Therefore, the comparison of the prior works and the proposed approach and claims of improving state-of-the-art are somewhat misleading because training on CC means that the model has effectively seen images + names (likely in captions) those classes that are supposed to be \"unseen\".",
            "18": "Even though all works can be said to do zero-shot, the prior methods trained on ImageNet1K do zero-shot with respect to novel classes/concepts, while the proposed method & CLIP model do something more of zero-shot with respect to a dataset (but having possibly seen at least some of the concepts/classes already).",
            "19": "Thus, making a side-by-side comparison without discussing this difference can be misleading.",
            "20": "3.",
            "21": "The problem of noise in datasets is discussed in the context of more than one possible matching for a given text/image.",
            "22": "But what about the image-text pairs not being \"accurate\" - e.g.",
            "23": "text not relevant, or not descriptive enough?",
            "24": "---\nOther:\n\n- Question: Why do both $M^V$ and $M^T$ need to be computed (for the OTTER variant)?",
            "25": "Shouldn't they be the same (at least for a larger number of Sinkhorn-Knopp iterations)?"
        },
        "dg31U4v1kqG": {
            "0": "Strengths:\n+ The paper is very-well written and easy to follow.",
            "1": "In particular, the approach section is clearly written and presented in a good pedagogical manner.",
            "2": "+ Overall, IMO the proposed approach solves the missing match problem in language-vision contrastive pretraining in a sensible way.",
            "3": "+ The results seem to suggest that the proposed approach is overall a good fix to existing language-vision pretraining methods, as it generally provides a gain (in certain cases bigger than others) across different target datasets and choice of encoder architectures.",
            "4": "+ The authors have made good efforts to ablate different components of their approach.",
            "5": "+ The authors have promised to release their code for training and evaluation.",
            "6": "Specific things I like in the paper:\n+ I like Section 3.5 as it helps clearly distinguish the proposed method to related methods with term swapping.",
            "7": "+ I like the visualization in Figure 3, which gives me a sense of what kind of missing matchings are mined.",
            "8": "Weaknesses:\n- One issue I have for the approach is that, it's not clear to me why self-distillation is necessarily beneficial?",
            "9": "Since the authors use the original encoder (or their momentum-updated counterparts) as teachers, what extra information does the distillation bring in?",
            "10": "At a conceptual level, the only extra source of information is the introduction of text-text and visual-visual similarities in Eq.",
            "11": "7.",
            "12": "But from the results (i.e., 'running SK for 0 iterations' vs the full OTTER method), it seems to suggest that's not the only reason for the improved accuracy.",
            "13": "It would be good if the authors could have a summary on where they think all the gains come from.",
            "14": "- The authors have motivated their approach by giving the example in Figure 1 to show that, there are many missed matches between images and texts.",
            "15": "However, there lacks a quantitative study showing how prevalent this problem is.",
            "16": "Although one can argue from the improved results, it would be more convincing to have a ballpark of how many missed matches there are in a batch.",
            "17": "- The proposed method has many parameters (alpha gamma_v gamma_t, eta, lambda), there lacks a detailed sensitivity analysis for them (Table 3 only does coarse ablation that mostly turn on or off the term).",
            "18": "- Page 5, \"we train on the two publicly available datasets, Conceptual Captions 3M (CC) (Sharma et al., 2018), and Wikipedia-base Image-Text Dataset (WIT)\", it would be helpful to train on a larger and more noisier dataset like Conceptual Captions 12M to see how the proposed method scales (my impression is that the benefit of the proposed method should be more significant due to higher noise level there).",
            "19": "- In Table 1, for three image/text encoder pairs \"FBNetV3-A/DeCLUTR-Sci-base\", \"FBNetV3-C/DeCLUTR-Sci-base\" and \"ResNet50/Sentence-BERT-base\", OTTER only has very marginal gain over KD, especially on IN10k, any insights on why this is case?",
            "20": "- In Table 2, for OTTER, why not use ResNet50 as the image encoder to enable a more fair comparison?",
            "21": "Again, for the \"FBNetV3-C/DeCLUTR-Sci-base\" setting, the gain is quite marginal between OTTER and KD.",
            "22": "Questions/confusions:\n- Page 3, \"Directly minimizing Wasserstein loss between image/text embeddings in our case will lead to collapsed representations\", any insight why this might be happening?",
            "23": "- Page 3, \"tau is a (trainable) temperature parameter\", is there evidence showing the benefit of learning this parameter, instead handpicking one using validation?",
            "24": "Also, what value does tau converge to?",
            "25": "- Table 3, it would be good to have results for setting lambda to 0 (no entropy regularization).",
            "26": "- Page 6, \"with a total batch size of 512 (64 per GPU) for 10 epochs\", how long does it take to train for 10 epochs?"
        },
        "fKZI4Iou8R7": {
            "0": "Strengths:\n\n1.",
            "1": "The authors showcase a fundamental problem of InfoNCE by providing interesting examples in Figure 1 and Figure 3.",
            "2": "This is a valuable insight for the community, and may encourage future work to make more realistic assumptions about image-caption datasets.",
            "3": "2.",
            "4": "The proposed method is simple and elegantly formulated.",
            "5": "It is versatile and can be applied to many vision-language tasks besides zero-shot recognition.",
            "6": "There are hyperparameters alpha and lambda that control how much we trust the image-caption pairs, and how much we trust the similarity estimations made by the teacher model.",
            "7": "3.",
            "8": "The paper is well-written and easy to follow.",
            "9": "4.",
            "10": "The experiments show consistent improvements in various training and evaluation settings, on difficult and realistic classification tasks.",
            "11": "Weaknesses:\n\n1.",
            "12": "Although the improvements shown in Table 1 are consistent, they are not significant.",
            "13": "Hence, using OTTER may not be worth the extra computational cost.",
            "14": "2.",
            "15": "The comparison to other ZSL methods in Table 2 is unfair, since the baselines are trained on a smaller dataset and with different pretrained image and text encoders.",
            "16": "3.",
            "17": "Although in Figures 1 and 3, the authors provide interesting evidence for the problem they identified, it is not clear how prevalent and important this problem is, especially considering the limited impact of the proposed method on the performance.",
            "18": "Is it possible that only a handful of examples exist like those shown in the paper?",
            "19": "Are there ways to quantify how common this problem is?",
            "20": "4.",
            "21": "Although zero-shot recognition is a relatively new problem, learning cross-modality embeddings using image-caption datasets has been studied for a long time, with applications ranging from text-to-image retrieval to visual grounding.",
            "22": "It is not clear whether the problem of image-caption alignment noise has been identified and studied before, or the authors are the first to identify this.",
            "23": "It is also not clear how effective this method would be for such vision-language tasks besides zero-shot recognition.",
            "24": "5.",
            "25": "In Table 1, it is strange that CLIP's performance using 400M images is close to the InfoNCE baselines trained on 3M images, on the GOI dataset.",
            "26": "Is that only due to using pretrained image and text encoders?",
            "27": "If so, it is important to elaborate the pretraining process, and compare the resources used for pretraining to CLIP's resources.",
            "28": "It would also be interesting to show the performance without pretrained encoders, as a lower bound."
        },
        "ggvj3HwWaak": {
            "0": "Pros: \n+ The motivation of this paper is very clear.",
            "1": "+ Overall, the paper is well written.",
            "2": "In particular, the INTRODUCTION section has a nice flow.",
            "3": "Cons: \n\n +For the optimal transmission algorithm, in the METHODS part, the author should emphasize the difference from the existing algorithm.",
            "4": "+More visualizations should be added to analyze the effectiveness of the proposed method (E.g Figure 2).",
            "5": "+For IN10K FH@K, the proposed OTTER does not achieve satisfactory performance.",
            "6": "The author should analyze the reason for this result.",
            "7": "Minor comments: \n\n- In the 4th line in 4.2 ABLATION STUDIES, it should be \"define\" instead of \" defined \"."
        }
    },
    "uPv9Y3gmAI5": {
        "-onpufoQbH": {
            "0": "Strengths: \n1.",
            "1": "Solid experiments demonstrating the proposed method outperforms other existing approaches.",
            "2": "2.",
            "3": "Novel weighting scheme for SVD for low-rank weight compression (though should double check this more thoroughly).",
            "4": "Some comments: \n\n1.",
            "5": "I would suggest using the phrase \"weighted SVD\" early on in the introduction (e.g., exactly when you introduce your new method).",
            "6": "2.",
            "7": "It might be good to address the existing literature on compressing trained neural networks which also goes beyond simply trying to minimize the Frobenius norm of the difference between the weights.",
            "8": "For example, the paper https://arxiv.org/abs/1505.06798 also goes beyond simply considering the reconstruction objective on the weights, and includes the nonlinearity as well in the reconstruction objective.",
            "9": "Your work obviously is different enough to stand on its own, but it might be good to make a mention of this work and others (e.g.",
            "10": "do more of a lit search / related work on low rank compression).",
            "11": "3.",
            "12": "It might be good to find the best hyperparameters for each setting to truly do a fair comparison (avoiding hyperparameter tuning isn't really a fair comparison, IMO -- but this depends on how much compute you have available).",
            "13": "4.",
            "14": "You forgot to bold the best performer in line 3 of Table 2 (in this case, the original compact model)."
        },
        "NUvoltr_vqw": {
            "0": "\n-) Written English needs improvement.",
            "1": "-) The idea is interesting and the authors explain the main intuition adequately clear.",
            "2": "The authors explain the advantages and limitations properly.",
            "3": "-) How easy/fast is to generate the fine-tuned model?",
            "4": "In other words, what is the overall \ncomputational cost to create a fine-tuned model and apply FWSVD versus generic training \nand SVD?",
            "5": "-) Unfortunately it is not clear to me whether the proposed method comes with any guarantees.",
            "6": "I understand that minimizing the Euclidean norm of the approximation of W does not necessarily \nimply lower classification (or other tasks) error, however I am not sure the proposed method does \neither.",
            "7": "Is that so?",
            "8": "For example, will the new method always be better than SVD?",
            "9": "One needs to \naccount for the cost of pre-processing costs too.",
            "10": "-) In addition to my comment above, Figure 6 seems to complicate the discussion.",
            "11": "The legend reads \nas \"Unlike SVD, the performance drop of our FWSVD perfectly imitates the ideal trending shown in Figure 1.",
            "12": "\", \nhowever I can see that on the left subfigure \"Rank group 8\"  leads to a higher performance drop than \"Rank group 7\" which \nis exactly what the authors wanted to avoid in the first place.",
            "13": "Furthermore, on the right subfigure, FWSVD \nactually introduces higher reconstruction error, which does not even reduce monotonically.",
            "14": "I understand that \nthe goal is to achieve higher accuracy in the classification task and not the matrix reconstruction part, \nbut this seems to complicate the optimization part of the model, e.g., how do the authors make sure \nthey do not overfit or spend more time than necessary computing the low-rank approximation?",
            "15": "-) The above experiment is explained better in Section 5.5.1 but the following sentence is confusing \n\"The results suggest that FWSVD’s objective (Equation 6) aligns with the task objective better by sacrificing the reconstruction error.\"",
            "16": "-) Section 5.2.2 is rather unclear; can you please provide more details, perhaps in the appendix?",
            "17": "Without \nproper explanation in this step, the experiments can not be judged fairly.",
            "18": "-) Please make the y-axis in Figure 6 run in logarithmic scale.",
            "19": "-) While generally providing better results than SVD for the experiments shown in this paper, the proposed \nmethod does not offer dramatic improvements in accuracy, nor it is guaranteed to do so.",
            "20": "It would be very \nhelpful if the authors could provide additional experiments about other benefits of the method, if applicable, \nin the appendix (e.g., is the new method faster than previous approaches based on SVD etc.)",
            "21": "Minor comments:\n\n-) Equation (1) and related text is confusing.",
            "22": "The \"\\approx\" symbol should be \"=\" since matrix W is of rank 'k'.",
            "23": "I suggest the authors to simplify discussion by using 'r' instead of 'k' and simply mention that matrix W is of rank k\\geq r.\n\n-) \"we propose a simplification by making the same row of the W matrix to share the same importance\".",
            "24": "Did you mean to say that each weight located in row 'i' will use the average weight of its row?"
        },
        "nwG4sbZhdv": {
            "0": "The paper is well-written and easy to follow.",
            "1": "Strengths:\n+ the approach does not require re-training (doing pre-training again)\n+ experimental results on several tasks show only a small drop in model quality\n+ the approach can further reduce the size of already-reduced-size models (e.g.",
            "2": "TinyBERT) \n\nWeaknesses:\n- the reduction in the number of parameters is modest compared to methods that re-train the network in a compact representation, like ALBERT\n- Results in Table 2 look incomplete: why is only one task reported for each of the models?",
            "3": "Why different task is used for each different model?",
            "4": "It would be better to pick e.g.",
            "5": "three tasks and use them for all four models.",
            "6": "- the work does not provide much insight into the problem that motivates the approach (e.g.",
            "7": "why \"group 10\" would affect \"important parameters\" more than \"group 9\")\n\nQuestions:\n- how common is the behavior in Fig 1?",
            "8": "It would be interesting to see evidence of it on at least one more dataset/task.",
            "9": "It might also be useful to use log scale for the axis, currently it is not possible to see if e.g.",
            "10": "removing group 9 leads to lower/similar/higher performance drop vs group 8 (i.e., is group 10 special, if yes, why?).",
            "11": "- what is the average/median magnitude of singular values in group 9 vs in group 10?",
            "12": "- have you tried minimizing eq.",
            "13": "(5) using SGD?",
            "14": "Do you expect substantial gap between (5) and the simplified version in eq.",
            "15": "(6)?",
            "16": "---\nPost-rebuttal:\nThank you for the clarifications and additional experiments, I've updated my score to 6."
        }
    },
    "ffS_Y258dZs": {
        "-McbUfD4UvY": {
            "0": "This is an ambitious paper that could offer some promise.",
            "1": "Learning to generalize compositionally is an intriguing idea.",
            "2": "However, I think the work needs substantially more development along a number of axes: \n\nSCS vs. purely continuous representations\n---------------\n\nThis may partly be an issue of communication (see below), but it's not entirely clear to me why the authors don't consider fully continuous representations in addition to OHE or SCS ones.",
            "3": "The argunments of prior work referenced below (e.g.",
            "4": "Chalmers, 1990) show that continuous representations can be effective for compositional generalization, as do many of the empirical studies the authors exhibit in RL, as well as the successes of large language models, etc.",
            "5": "All modern approaches rely on continuous representations, and often achieve some degree of effective compositional generalization.",
            "6": "It certainly seems important to run a continuous-representation comparison condition to justify the value of the SCS encoding.",
            "7": "Indeed, the authors (accurately) criticize OHE for not being able to extend without changing the shape of the vector representations.",
            "8": "However, their SCS representation is subject to limitations as well, so far as I can see: it cannot adapt to more latent dimensions than are pre-allocated, and I suspect that it also would fail to extrapolate to dimensions with a larger $d(i)$ than the architecture saw in training (although this is speculation, and perhaps something that could/should be evaluated experimentally in a revised version of this work).",
            "9": "Furthermore, constructing an SCS seems to require a) that the semantics of the task be simple enough to be stated as discrete values along a small number of latent dimensions, and b) that we be able to construct these representations from the inputs (which might generally be e.g.",
            "10": "images).",
            "11": "Indeed, continuous representations are used in basically all modern applications for just this reason---we don't know how to construct an appropriate discrete representation for the space, and it's not clear if one even necessarily exists.",
            "12": "Thus, in order for this method to be applicable beyond toy settings, it would be useful for the paper to explain how this might work.",
            "13": "Ideally, the authors would *demonstrate* such success on existing benchmarks for compositional generalization (see below).",
            "14": "Clarity\n------------\n\nNeither the paradigm nor the results are stated in sufficient detail or clarity to understand how to build upon the work.",
            "15": "For example: \n\n* The rule-based speaker the authors use to evaluate uses compositional language, but what exact form does this take?",
            "16": "This should be clearly stated, since it is central to understanding the main experimental evaluations.",
            "17": "* The only description of how the agent's take communication actions is that the output is \"a distribution over 29 actions, which corresponds to an action space combining both the language output and the decision output.\"",
            "18": "How are these combined?",
            "19": "These kind of details can make a substantial difference in RL, and should certainly be reported.",
            "20": "Furthermore, there are many other clarity issues, e.g.",
            "21": ":\n\n* The paper spends a great deal of time discussing issues like \"object-centric\" representations vs. \"stimulus-centric\" ones, but I do not see how such notions could apply to the SCS representation as suggested by the paper.",
            "22": "Or are the stimuli represented differently as inputs to the agents?",
            "23": "Even this detail of the experimental evaluation is not clearly stated.",
            "24": "* What the paper calls a \"One-Hot Encoding\" OHE seems to really be a multi-hot encoding (that is, it is one-hot along each dimension, but a single stimulus has multiple  1s within it).",
            "25": "However, this is not clearly explained, and it's only really communicated if the reader carefully examines figure 1.",
            "26": "A point like this should be clarified in the text.",
            "27": "Evaluation\n------------\n\nThe results of this approach are barely-above chance performance (29% at most, where chance is 25%).",
            "28": "While this may be interesting in some sense, it is not a particularly compelling endorsement of the SCS representation.",
            "29": "In addition, even this experimental evaluation is limited.",
            "30": "First, the results suggest poor tuning of the  hyperparameters: the DNC performs worse than an LSTM memory, but a DNC *contains* an LSTM in the controller, and so should almost certainly perform better (even if just by learning to ignore the memory part).",
            "31": "Of course, these are challenging architectures to optimize, but this means that the claims of the paper are suspect: perhaps the performance is low for reasons of poor training, rather than inherent difficult of the task.",
            "32": "Furthermore, the authors perform effectively no ablation studies, or further evaluation of which aspects of the approach matter.",
            "33": "All of these would be necessary to make a valuable contribution to the literature.",
            "34": "Finally, it would be much more interesting if the authors could demonstrate the value of their approach on existing compositional generalization challenges such as SCAN or gSCAN (which they cite), where baselines do exist.",
            "35": "Could SCS offer benefits in these more complex settings?",
            "36": "This would force the authors to grapple with some of the challenges outlined above about using SCS in environments with slightly more complexity (but still much simpler and more amenable to structured representations than the stimuli used for reference games in cognitive psychology).",
            "37": "Literature\n-----------\n\nAs the authors note, compositional generalization has been a major topic of research for some time.",
            "38": "The work needs to be better situated within this broader literature, to clarify its contributions.",
            "39": "For example, the author's points about discrete vs. continuous representations overlap with long-standing replies to Fodor & Pylyshyn's claims about compositionality.",
            "40": "For example, see Chalmers (1990) or Smolensky (1987), as well as much of Smolensky's subsequent research (e.g.",
            "41": "Smolensky, 1990; McCoy et al., 2018).",
            "42": "This paper would convey its contributions much more clearly if it were situated within this broader literature.",
            "43": "I don't think it's as essential to engage with with, but the authors may also be interested in Santoro et al.",
            "44": "(2021), an opinion piece which discusses a number of issues related to several aspects of this paper, including discussions of discrete vs. continuous symbol representations, and arguments that communication provides a unique path to instilling symbols.",
            "45": "References\n-------------\n\nChalmers, D. (1990, July).",
            "46": "Why Fodor and Pylyshyn were wrong: The simplest refutation.",
            "47": "In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, Cambridge, Mass (pp.",
            "48": "340-347).",
            "49": "Santoro, A., et al.",
            "50": "(2021).",
            "51": "Symbolic behaviour in artificial intelligence.",
            "52": "arXiv preprint arXiv:2102.03406.",
            "53": "Smolensky, P. (1987).",
            "54": "The constituent structure of connectionist mental states: A reply to Fodor and Pylyshyn.",
            "55": "Southern Journal of Philosophy, 26, 137-161.",
            "56": "Smolensky, P. (1990).",
            "57": "Tensor product variable binding and the representation of symbolic structures in connectionist systems.",
            "58": "Artificial intelligence, 46(1-2), 159-216.",
            "59": "McCoy, R. T., Linzen, T., Dunbar, E., & Smolensky, P. (2018).",
            "60": "RNNs implicitly implement tensor product representations.",
            "61": "arXiv preprint arXiv:1812.08718."
        },
        "6yYdglgLmal": {
            "0": "**SCS**\n\nThe authors propose a new method to represent symbolic stimuli as continuous representations as an alternative to one-hot encodings.",
            "1": "The SCS representations seem like a flexible way to define differently structured semantic spaces in synthetic settings (one might need to change the dimensions of the input if they were to use one-hot encodings).",
            "2": "The paper then does a good job of showing how using one-hot encodings compared to SCS change the problem definition leading to a difference in performance on the same task.",
            "3": "Despite this, I fail to see how the SCS representations might be useful outside the scope of the experiments shown in the paper.",
            "4": "I also encourage the authors to simplify the experiment described in section 3.1 to make it more clear.",
            "5": "The applicability and the novelty of the SCS representation seem limited.",
            "6": "I would like the authors to have a more extended discussion of how SCS can be used outside of their work.",
            "7": "**BENCHMARK & BASELINES**\n\nThe paper does an excellent job of introducing the meta-referential games and the different setup settings they use, and the corresponding reason behind them.",
            "8": "While the authors say that they use vocabulary permutation, it is not clear what the size of the vocabulary is.",
            "9": "Similarly, I could not find the value of the dimension of the symbolic space $N_{dim}$ that was used to generate the stimuli.",
            "10": "In a similar vein, I would have appreciated more details about the model architecture and the training regimes used (possibly in the appendix).",
            "11": "I found the result that the LSTM-based model successfully reconstructs the stimuli but fails on the main game to be interesting.",
            "12": "But without additional details about the failure or examples of failure cases and a discussion about what these results imply outside the tasks they were tested for, I was not convinced by their utility.",
            "13": "I would also urge the authors to have a speculative discussion on what successful inductive biases might look like.",
            "14": "**Other Comments:**\n\nPage 2: Further more → Furthermore \n\nPage 3: On-hot-encoded → One-hot-encoded\n\nPage 4: Relational Frame Theory (RTF) →Relational Frame Theory (RFT)\n\nPage 6: Simplify → Simplifies\n\nPage 6: observes and have → observes and has\n\nPage 9: zsc → ZSC\n\nSome lines in the introduction and the related work sections are incredibly similar.",
            "15": "Eg: \n\nThe following line is exactly the same in the Introduction and Related Works section.",
            "16": "Introduction and in Related Woks: \"Chaabouni et al.",
            "17": "(2020) showed that, when a specific kind of compositionality is found in the emerging languages (the kind that scores high on the positional disentanglement (posdis) metric for compositionality that they proposed), then it is a sufficient condition for systematicity to emerge.\"",
            "18": "I would advise the authors to make the introduction and related works sections more succinct."
        },
        "6iHow_EeoC": {
            "0": "I think the posed problem, using meta-RL for continual referential games with different semantic structures, is an interesting problem.",
            "1": "However, the paper is difficult to follow, and details are missing from models and experiments.",
            "2": "1- The paper is difficult to follow; especially the introduction where the authors give a lot of related work without a clear introduction of the problem and why it is important.",
            "3": "I think it would help if the introduction is kept simple and references are moved to related work.",
            "4": "- Upon first reading the abstract, I thought one of the main contributions of the work is a new benchmark for compositional learning.",
            "5": "But, it is only mentioned on the last paragraph of the introduction without much explanation on what it entails.",
            "6": "I think there needs to be more emphasis on what the benchmark is; how it is different from existing compositional benchmark, what it is measuring, how it is structured etc.",
            "7": "- There are several concepts that the paper uses but are never defined such as \"positional disentanglement metric\" (where the speaker uses a language with this characteristic) or \"systematic generalization\" (that the paper is targeting).",
            "8": "- I think more concrete examples are needed to make the paper more readable such as a real stimulus example (for Figure 1), sample conversations between two agents to understand compositional learning, sample tasks and meta-tasks, etc.",
            "9": "- Sentences are in general long and repetitive in some cases such as the last sentence of abstract or \"differently semantically structured symbolic spaces\".",
            "10": "2- I think more discussion on why OHE shouldn't be used and SCS should be preferred is needed.",
            "11": "Stimuli are already discrete (Figure 1) and should come from a bounded vocabulary, nonetheless a very large one, why would using OHE be prohibitive for meta-learning?",
            "12": "Considering that humans communicate with a finite vocabulary and can generate unlimited compositional structures, why can't we use a similar approach to represent stimuli?",
            "13": "I think a more concrete example that shows a realistic continual task learning problem with discrete stimuli where the boundary of the stimuli can't be fixed and needs to be significantly increased over time is needed.",
            "14": "- Your initial experimental results in Figure-2 also suggest that OHE clearly gives higher performance and SCS might be introducing unnecessary noise to the process.",
            "15": "- It is also not clear to me whether the performance of SCS in Figure-2 can be improved or not.",
            "16": "For example, can we use millions of updates to achieve results on par with OHE?",
            "17": "Instead of sampling d(i) randomly, if you equally divide the interval [-1,1] into two sections and take N_{dim}=2, what would be the performance?",
            "18": "3- For the experiments in Section 3.1., there is no setup.",
            "19": "I think a more detailed setup that includes what is the baseline RL agent, how it is trained in this 2-player game, what is its architecture, how many conversations took place, etc.",
            "20": "4- The setup for meta-referential games using SCS is not clear to me.",
            "21": "If at any point in the future, we introduce more stimuli which increases the boundary, causing problems with OHE, this means that we are dividing the interval [-1, +1] into more and more sections.",
            "22": "Wouldn't this cause intersections with previous SCS representations and lead to ambiguity?",
            "23": "Are you suggesting that, over time the agents would forget some of the earlier tasks that they learned to accommodate for new tasks?",
            "24": "- I think some ablations are needed here, including increasing the boundary of the stimuli gradually for new tasks to see learning progress (and forgetting), increasing number of latent dimensions N_{dim}.",
            "25": "5- In Figure 4 (a), there is no improvement other than LSTM+Rec.",
            "26": "This is strange to me as using only ~1K updates gives the same performance as using 8K updates.",
            "27": "Could you explain the reason why?",
            "28": "6- Could you explain what you mean by \"supportive training-purpose stimuli\" in Section 5.4?",
            "29": "Do you mean these are supportive for the test task adaptation?",
            "30": "7- Some grammatical mistakes:\n- Apostrophes are missing in some places, \"Neural networks' induction biases\", \"Both players' goals\".",
            "31": "- \"Further more\" --> \"Furthermore\"\n- \"On the otherhand\" --> \"On the other hand\"\n-\"shceme\" --> \"scheme\"\n- \"we hope will spur\" --> \"we hope it will spur\""
        },
        "BJz8rBDXnp": {
            "0": "The authors introduce a symbolic continuous stimulus representation that has the particularity of enabling the representation of stimuli sampled from differently semantically structured symbolic spaces while maintaining the same representation shape.",
            "1": "The paper is a bit hard to read, I am not sure I fully understand this paper.",
            "2": "Could the authors provide more details of why the proposed representation is helpful for compositional learning?",
            "3": "Is the proposed method adaptable to solve other tasks that do not belong to preferential games?",
            "4": "The experiments are not sufficient to support the claimed contributions.",
            "5": "The authors did not compare the proposed SCS representation with other representations, for example, which baselines use one-hot-encodings?",
            "6": "The text in Figure 1, Figure 2, and Figure 3 are too small."
        }
    },
    "wgR0BQfG5vi": {
        "BImYFN7X3wH": {
            "0": "Positive points:\n\n+ the paper proposes a simple way of adapting the smoothing parameter alpha.",
            "1": "And it works well in the experiments.",
            "2": "+ Instead of a simple distribution that is often used for smoothing, the paper proposes to use the distribution of the same model but from an earlier training epoch.",
            "3": "+ experimental results show improved accuracy and improved calibration compared to baselines\n\nNegative points:\n\n- it is not clear to me how important it is in the proposed approach that function g in  Eq 5 is the metric that is eventually used in evaluation (e.g., BLEU), rather than the training-objective itself, e.g., likelihood/cross entropy during training.",
            "4": "Given that alpha is tuned dynamically based on function g, it seems that this could be a unique/unfair advantage of the proposed approach over the other baselines, which are only trained based on likelihood/cross-entropy, but have no access to the evolution metric (e.g., BLEU), which is different.",
            "5": "For this reason, it could also be interesting to have experiments where the evaluation metric and training loss are the same function, e.g., log likelihood.",
            "6": "Also, in Table 1, where different evaluation metrics are shown (BLEU, METEOR,...), is the 'Ours\"-model always trained with function g = BLEU, or is function g chosen to be identical to the evaluation metric, e.g., when evaluated w.r.t.",
            "7": "METEOR, then g-METEOR is used during training?",
            "8": "- also, it might be good to add the original paper that proposed knowledge distillation:  Bucilua, Caruana and Niculescu-Mizil: Model compression.",
            "9": "KDD 2006."
        },
        "8XCwfWOb1OY": {
            "0": "Strengths:\n1.",
            "1": "This paper proposes the adaptive \\alpha computed by the entropic level of model probability distribution per sample, which leads to updating the model parameters to lower the predictive score on the ground-truth target, as opposed to the effect of the cross-entropy with hard targets.",
            "2": "The hyperparameter search on \\alpha in label smoothing is removed.",
            "3": "2.",
            "4": "This paper set an assumption that the teacher network makes a less confident prediction than that of the student and extends gradient analysis in the perspective of regularization effect in the proposed adaptive label smoothing.",
            "5": "Weakness:\n1.",
            "6": "This paper demonstrates why self-knowledge distillation as a prior distribution is a form of regularization with theoretical analysis on the gradients.",
            "7": "However, there is no theoretical result about the effectiveness of assigning the self-knowledge distillation to label smoothing.",
            "8": "2.",
            "9": "The authors claim that “There are a number of benefits of adopting the adaptive smoothing parameter”.",
            "10": "However, they only show that the hyperparameter search on \\alpha is removed and the adaptive smoothing parameter can be connected to the gradient rescaling effect on self-distillation.",
            "11": "More details should be reported to show the benefits of adopting the adaptive smoothing parameter.",
            "12": "3.",
            "13": "In the ablation study, the authors only consider the fixed \\alpha as the base.",
            "14": "However, \\alpha could also be changed in the training process.",
            "15": "Therefore, the dynamic \\alpha by hyperparameter searching should also be added as a base.",
            "16": "[1]  Learning Better Structured Representations Using Low-rank Adaptive Label Smoothing, ICLR 2020.",
            "17": "[2]  Adaptive label smoothing,  arXiv 2020."
        },
        "NbZ2fprb0dW": {
            "0": "\n1.",
            "1": "Strengths\n\nThe paper proposes a simple extension for label smoothing that enables adaptive and instance-level smoothing.",
            "2": "Experiments show good results w.r.t.",
            "3": "model performance measures and calibration error metrics, even when compared with recent self-distillation methods such as TF-KD and SKD-PRT.",
            "4": "Ablation study and gradient analysis provide further insights into the proposed method.",
            "5": "It is also nice that the smoothing hyper-parameter tuning is not required anymore.",
            "6": "2.",
            "7": "Weaknesses\n\nThere are a few instance-level label smoothing methods that are not discussed (nor compared) in the paper: Li et al.",
            "8": "\"Regularization via structural label smoothing\" (AISTATS2020).",
            "9": "Zhang and Sabuncu \"Self-distillation as instance-specific label smoothing\" (NeurIPS2020).",
            "10": "Although already discussed in the final section, it would be interesting to see results for datasets from other domains, such as images.",
            "11": "Since most label smoothing methods were proposed to work well with image datasets, it would make the paper much better to show how it will perform under under other tasks/datasets, such as image classification."
        },
        "xbeGi5PZ6M": {
            "0": "Strengths \n- Overall, the paper is well written, and the proposed method is well motivated.",
            "1": "- Experiments seem to demonstrate the effectiveness of the proposed method.",
            "2": "- Thorough empirical analysis is done on several benchmark datasets.",
            "3": "- A careful ablation study is also conducted to further demonstrate the effectiveness of the proposed method.",
            "4": "Weakness \n- The main weakness of the paper in my opinion is the lack of novelty.",
            "5": "The proposed method in my opinion is very similar to the previous works like [1], which proposed to use predictions from previous time stamps for self-distillation, and [2], which also proposed a method for adaptive label smoothing based on predictions from previous time stamps.",
            "6": "While the authors of the paper addressed the key difference between the proposed method and [1], I do still feel that the proposed method lack novelty, despite good improvements in performance.",
            "7": "On top of [1] and [2], [3] and [4] are also potentially relevant prio works to potentially discuss in literature review and benchmark against.",
            "8": "[1] Kyungyul Kim, ByeongMoon Ji, Doyoung Yoon, and Sangheum Hwang.",
            "9": "Self-knowledge distillation with progressive refinement of targets, 2021.",
            "10": "[2] Zhang, Zhilu, and Mert R. Sabuncu.",
            "11": "\"Self-distillation as instance-specific label smoothing.\"",
            "12": "arXiv preprint arXiv:2006.05065 (2020).",
            "13": "[3] Li, Xingjian, et al.",
            "14": "\"One Generation Knowledge Distillation by Utilizing Peer Samples.\"",
            "15": "(2019).",
            "16": "[4] Yun, Sukmin, et al.",
            "17": "\"Regularizing Predictions via Class-wise Self-knowledge Distillation.\"",
            "18": "(2019)."
        }
    },
    "q23I9kJE3gA": {
        "Fo6KR4cyrSu": {
            "0": "Strength:\n- The introduction of the order as the latent variables is reasonably intuitive and well-motivated.",
            "1": "- The proposed approach of jointly modeling the cardinality with topological sort-based TSAMPLE data augmentation seems interesting.",
            "2": "- The authors promise the availability of corresponding code and provide a detailed description of hyperparameters which would be essential for reproducibility if open-sourced.",
            "3": "Weakness:\n\n- Missing state-of-the-art comparisons.",
            "4": "Relevant baselines have not been used for this task, such as non-seq2seq or classification-based models.",
            "5": "- It is not clear how this approach would generalize in a zero-shot setting where the partial order graph is not known beforehand.",
            "6": "- An ablation study is required to understand how the model is performing for different set lengths in Section 4.3 (Role of cardinality).",
            "7": "It is expected that the model might be performing well for shorter set lengths, which could help understand the impact of the proposed approach.",
            "8": "- Some model-generated examples would provide more insight.",
            "9": "- Human evaluation is not provided.",
            "10": "- Pictorial depiction of the models either in the main section or in the appendices would increase the understanding and interpretability of the proposed approach.",
            "11": "Questions:\n- On page 7, could the authors specify the type of sampling (greedy/beam/etc.)",
            "12": "used to report results in the rest of the main text (in table 2 etc.)?",
            "13": "- Have the authors experimented with pre-trained weights (decoders)?",
            "14": "- Section 5 Conclusion -> Could the authors provide more information as to how they could include cardinality in dialog generation and how would it help?",
            "15": "Suggestions/Comments:\n- Section 1 can be improved significantly by providing more concrete real-life examples or discussing Appendix B (Table 3).",
            "16": "- Please take care of using citep compared to citet (natbib style) appropriately.",
            "17": "Eg on Page 6.",
            "18": "- Please provide a more detailed description of |sYt| in Section 3.2\n- Section 4.2 Model networkx -> network\n- performance SET SEARCH -> performance of SET SEARCH\n- Figure 5 Right -> It would help to also include (T) in the legend to indicate TSAMPLE\n- Section 4.3 Role of cardinality: results 2 -> Table 2\n- It would help to differentiate between the definition of `n’ in Section 4.1 and Section 4.2 baselines and in the main text.",
            "19": "- Some missing references:\nhttps://aclanthology.org/P19-1139.pdf\nhttps://aclanthology.org/2021.findings-acl.121.pdf"
        },
        "j2XRWMWyG3n": {
            "0": "I appreciate the detail that the authors have given to describing the imposition of order on labels using PMI, and find it interesting that the pairwise order threshold beta does not seem to impact the results.",
            "1": "I would like the authors to expand on the storage complexity (|Y|^2 in the worse case) and how it could impact cases where this model may be applied to dense multilabel data.",
            "2": "I would like to see a comparison between TSAMPLE and other seq2seq set generation baselines and non-seq2seq methods (e.g.",
            "3": "a multi-label classification and/or pairwise scoring approach e.g.",
            "4": "[2, 3]) to assess whether there is a large gap and thus whether there is a fundamental difference in applicability of a seq2seq approach to set generation based on the task and label space.",
            "5": "While the authors claim that their method is particularly useful for tasks such as OpenEnt, the relatively small label vocabulary (2.5K) makes ranking/scoring and classification (extreme or otherwise) based approaches reasonable to apply.",
            "6": "The unique label space is even smaller in GO-EMO and REUTERS.",
            "7": "Did the authors compare their method with naive usage of the seq2seq model (BART-base) without set constraints at training time?",
            "8": "This would represent the most naive baseline that may still perform relatively well, given the inclusion of a simple post-processing step that removes repeated predictions in a sequence.",
            "9": "This post-processing step seems necessary regardless, as the proposed method here does not explicitly constrain the generated sequence to contain unique elements.",
            "10": "I would like to see a comparison with previous work that constrains seq2seq models directly to generate sets without data augmentation, such as [1] which incorporates a cardinality penalty and changes the decoding strategy for a seq2seq model to generate sets.",
            "11": "This accommodates the same base model architecture, and has been applied to a set prediction task (Ingredient prediction) at a scale that is comparable to the larger of datasets in this paper (e.g.",
            "12": "OpenEnt).",
            "13": "Small edit: Are the max/min labels in Table 1 reversed?",
            "14": "References:\n[1] Salvador, Amaia, et al.",
            "15": "\"Inverse cooking: Recipe generation from food images.\"",
            "16": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
            "17": "2019.",
            "18": "[2] Dai, Hongliang et al.",
            "19": "“Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model.” ACL/IJCNLP (2021).",
            "20": "[3] Wu, Ledell, et al.",
            "21": "\"Scalable Zero-shot Entity Linking with Dense Entity Retrieval.\"",
            "22": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "23": "2020."
        },
        "at67xDIvZk2": {
            "0": "Strengths:\n- The paper proposes a simple, interesting idea and deals with a timely topic.",
            "1": "- The experiments are described thoroughly and seem well-executed.",
            "2": "Weaknesses:\n- My main concern is that seq2seq is unnecessary for tackling the kinds of problems the authors consider, namely, where all target sets are subsets of a finite set of discrete elements.",
            "3": "Here it seems more natural to simply make a binary prediction for each set element.",
            "4": "Note that such an approach is compatible with first predicting cardinality and also with using pretrained components (like BART), at least on the encoder side.",
            "5": "It's possible that seq2seq would outperform such an approach, but the authors do not provide evidence of this.",
            "6": "- More minor, since it doesn't affect the rest of the paper much, but Lemma A.1 and its proof seem incorrect.",
            "7": "Update after author response: Thanks for your response and your comments.",
            "8": "I'm increasing my score in response to clarifications and the improved results over baseline multi-label classifiers."
        },
        "E9MCU3twgzK": {
            "0": "Strengths:\n- The motivation that using order and cardinality for set generation task is clearly expressed.",
            "1": "- Using topology sort to generated sorted label is an attracting idea.",
            "2": "Weaknesses:\n- The F1 score of GO-EMO in the original paper (Demszky et al.",
            "3": "2020) is 46 (using BERT-base).",
            "4": "However, in Table 3, the best score is only 30.",
            "5": "Can you explain the difference here?",
            "6": "- Stronger baselines are missing in Table 2.",
            "7": "Although sorting the labels is important, the effectiveness of using mutual information for ordering is unknown.",
            "8": "It would be better to compare with other sorting methods.",
            "9": "- When using partial order to construct the graph, it is possible that there will be circles.",
            "10": "It’s not clear how this case is handled.",
            "11": "- As the cardinality is predicted, can we use it to control the label generation progress?",
            "12": "E.g., use it to decide when to stop generation instead of <EOS>.",
            "13": "- Typos:\n      o  The recent successes of pretraining-finetuning paradigm has => The recent successes of pretraining-finetuning paradigm have\n      o  From the results 2 => From the results in Table 2"
        }
    },
    "mQDpmgFKu1P": {
        "wpB5YVFNhK": {
            "0": "The quadratic memory and time complexity of transformers have resulted in recent efforts to replace the softmax self-attention with efficient variants.",
            "1": "This paper presents one such effort, which is very interesting, and could pave the way for a new generation of transformers.",
            "2": "The approach is simple and relatively straightforward, and the writing is overall clear.",
            "3": "I have two main concerns about the paper that prevent me from recommending acceptance, both boil down to the same point: I am not convinced that the experiments support the authors interpretation.",
            "4": "First, the paper focuses on the main idea of using LMU, but the proposed model contains many different components, that are orthogonal to it.",
            "5": "For instance, why is the $L_i$ projection and the following non-linearity needed?",
            "6": "Would applying it to the transformer lead to similar gains?",
            "7": "The authors say _Our modified self-attention acts on this matrix to combine temporal information.",
            "8": "As a result, self-attention does not act directly on the input sequence, but rather on a compressed version of the input._ But the same compression can be applied on standard SA as well.",
            "9": "Further, the FFN layer after the input was found to be useful by the authors.",
            "10": "Did they also add it to their baselines?",
            "11": "In general, some ablations would make the paper much stronger and shed more light on the proposed model (on top of what is mentioned above, I would also like to see the effect of the values of q and q').",
            "12": "Second, the experimental setup is underspecified: the paper's main selling point is the strong performance compared to the transformer, but important details are missing with respect to both the models and the baselines.",
            "13": "How many layers does each model have?",
            "14": "What is the hidden size?",
            "15": "Importantly, the last experiment (Fig.",
            "16": "5), which appears in the paper title, is even less specified: was the same transformer model trained on 130GB of data?",
            "17": "Where did this data come from?",
            "18": "As a side note, it is also unclear that such small model could leverage 130GB of data, and thus this experiment might not be ideal to test the sample efficiency of the proposed model.",
            "19": "I would recommend providing more details about the experimental setup, and justifying the experimental choices."
        },
        "byM2oVZvEeg": {
            "0": "Strengths\n* Improving the data efficiency in language models is an important problem that so far studies have shown that can be achieved by scaling the size of the model.",
            "1": "Having additional ways to improve data efficiency by changing the model design is definitely of interest.",
            "2": "* Using the Legendre Memory Unit to substitute self-attention in transformers is interesting and has several potential merits: it can reduce the complexity and does not increase the size of the layer.",
            "3": "Although it turns out that additional components need to be introduced for good performance.",
            "4": "Weaknesses\n* Something that stands out is the lack of discussion and comparison to related works that employ recurrent formulations of attention.",
            "5": "Kernel-based variants of self-attention have a recurrent formulation and lead to linear complexity (see [1,2,3,4]).",
            "6": "* The rationale behind the architectural choices for the self-attention component is not well explained or empirically verified.",
            "7": "I understand that LMU might have limited capacity but this is not specifically discussed and it is unclear how each component contributes to the end performance.",
            "8": "* Even though this paper proposes a new efficient transformer, the evaluation does not focus on computational efficiency aspects and comes across as incomplete.",
            "9": "Quantifying the latency and scaling with respect to sequence length would be needed for convincing the reader about its usefulness.",
            "10": "*  The evaluation focuses on comparing with an empirical law learned on a different experimental configuration and there is a concern about how comparable are the results to the ones obtained in this study and the validity of the conclusions.",
            "11": "I don't think it is good practice to use these power laws universally, especially for this purpose.",
            "12": "Another limitation is that it is unclear whether the improvement would hold when the size of the model increases; the evaluation is dealing with scaling laws after all.",
            "13": "I would suggest the following:\n  * To make the comparison more fair, I would suggest to train transformer models with varying size and derive a power law based on the exact same experimental configuration used for LMU models.",
            "14": "*  The claims regarding the 10x better data efficiency are not well supported and I would suggest the authors to compare with transformer models on standard LM benchmarks and potentially to some downstream tasks such as MT to make a stronger case.",
            "15": "Questions: \n* How can we be sure that the specific power law derived from a different experimental configuration in Kaplan et al.",
            "16": "(2020) corresponds to the empirical law that can be derived for transformers in this particular evaluation setting?",
            "17": "It would be great if the authors discuss this or provide some supporting evidence about its correctness.",
            "18": "* From the evaluation, it seems that comparing to other efficient transformers was not the main focus of the work but people might wonder how well it works compared to alternatives.",
            "19": "What recommendation the authors would give for those interested in using it?",
            "20": "*  What is the contribution of each specific design choice, such as the FFN/global attention and implicit self-attention, to the end performance?",
            "21": "An ablation analysis would be most appropriate for quantifying this.",
            "22": "* Could the authors provide more details about the evaluation settings and number of parameters of each model for the per-token loss comparison?",
            "23": "Is the per-token loss computed on the test set or training set?",
            "24": "Why is the sequence index capped at 10^3?",
            "25": "[1] https://arxiv.org/pdf/2006.16236.pdf\n\n[2] https://arxiv.org/pdf/2009.14794.pdf\n\n[3] https://openreview.net/pdf?id=QtTKTdVrFBB\n\n[4] https://openreview.net/pdf?id=ot2ORiBqTa1"
        },
        "JGSt2_zC2m_": {
            "0": "Strengths:\n1.",
            "1": "Novel and interesting idea, well motivated architecture.",
            "2": "2.",
            "3": "Empirical evaluation over a wide range of model sizes in terms of # parameters.",
            "4": "Major limitations:\n1.",
            "5": "The paper presents an incomplete comparison that plays to the strength of the LMU architecture by comparing only parameter-matched models.",
            "6": "Given that a significant fraction of the LMU compute is hidden in non-trainable parameters, a fair comparison should also compare models trained with an equivalent amount of training computation; ideally in terms of total training compute used.",
            "7": "2.",
            "8": "Limited empirical evaluation on 1 language modeling task: also evaluating on tasks other than language modeling (say downstream tasks like the SuperGLUE benchmark, or Machine Translation) would significantly strengthen the claims made in the paper.",
            "9": "Questions or comments:\n1.",
            "10": "The idea of disentangling input dimensions from order to reduce kernel computation costs sounds very similar to depthwise separable convolutions [1].",
            "11": "2.",
            "12": "Is the x-axis in Figure 5 mislabeled?",
            "13": "From the caption and the text I would expect it to represent number of training steps or number of tokens trained on.",
            "14": "References:\n[1] Xception: Deep Learning With Depthwise Separable Convolutions, Chollet et al."
        },
        "bXzwmGeH9u2": {
            "0": "Strengths:\n- The output of the LMU can compress past history at each time step.",
            "1": "The use of LMU reduces the self attention complexity from n^2 to q'xq where q is the order of the q Legendre polynomials.",
            "2": "This provides advantage to tasks of larger sequence lengths.",
            "3": "Weaknesses:\n- Like discussed in the paper, LMU's implicit self-attention is good at prediction with limited context, while the traditional self-attention can capture long-range dependencies.",
            "4": "Also, LMU lacks a mechanism to provide pairwise information, which can be captured by self-attention.",
            "5": "This can be illustrated using entailment tasks or translation tasks.",
            "6": "- Empirical results are very limited.",
            "7": "Cross-entropy scores are limited because language modeling or a masked language modeling does not reflect how good the model can be at tasks that requires pairwise information.",
            "8": "Better to compare with Transformer on down stream tasks with finetuning as well.",
            "9": "- Lacks a comparison with stronger baselines of efficient transformers.",
            "10": "Better to compare agains Linformer (low-rank), Synthezer (Random generation), and Primer (learning the primitives).",
            "11": "- Lacks ablation study on the selection of window size and order in the LMU.",
            "12": "q is only effective when sequence length is larger than q.",
            "13": "For  tasks of smaller sequence length, they won't benefit from the LMU method.",
            "14": "[1] Linformer: Self-Attention with Linear Complexity\n\n[2] Synthesizer: Rethinking Self-Attention in Transformer Models\n\n[3] Primer: Searching for Efficient Transformers for Language Modeling"
        }
    },
    "luO6l9cP6b6": {
        "v0MV94nFjfz": {
            "0": "Strength:\n1. the paper is mostly well-written and easy to follow.",
            "1": "2. the experiments are well-designed to support the claims\n\nWeakness:\n2.",
            "2": "I'm not convinced by the results about BERT in S7.3.",
            "3": "The confounder factor -- pre-training, is not considered.",
            "4": "For instance, for the pre-trained BERT, the word frequencies information is already captured in the parameters (along with many other features that also hold for scrambled text).",
            "5": "Such that when fine-tuned on scrambled data (word frequency preserved), it's not surprising that pre-trained BERT can clearly perform better.",
            "6": "And the reason why LSTM is more robust is that the model size is very small such that it can relearn all weights.",
            "7": "We can observe the same trend for 1-layer-BERT on classification tasks, the gaps are generally smaller than other model variants.",
            "8": "Some claims in the paper can be further substantiated if the author can experiment with a BERT that is pre-trained on scrambled English.",
            "9": "3.",
            "10": "I also have some doubts about S6.2, in which the author claim that the bad performance is because sequence labeling tasks are more likely to rely on word identities.",
            "11": "However, as explored in Hewitt & Liang, those labeling supervisions (although in POS tagging) can be easily learned by fine-tuning.",
            "12": "Can authors find a better explanation for the poor performance on labeling tasks?",
            "13": "1.",
            "14": "The method proposed in this paper is widely used in many other studies, e.g., in adversarial attacking.",
            "15": "But it is still somewhat novel since it is applied to new domains.",
            "16": "Also, the observations made are marginally novel or significant, there are not many new results compared to prior observations.",
            "17": "The title is about pre-trained models, but only BERT is discussed throughout the paper."
        },
        "XlpgD1isw2U": {
            "0": "Overall, I think the paper is well-written and debates interesting points in transferability.",
            "1": "However, I want to see some measures of intrinsic evaluation related to the scrambled datasets\nas well (e.g., perplexity).",
            "2": "My concern is that these scrambling techniques might push your\ndataset outside of real word adversarial attacks.",
            "3": "I would suggest comparing them all together\nto support validity of the scrambling methods.",
            "4": "Furthermore, could you every tried replacing\nword using ontology or a collection of semantically related words?",
            "5": "Please add similar\nexperiment to show that scrambling with synonyms and antonym have different effects (or\npositive vs negative replacements)."
        },
        "i0K0dUeqqZr": {
            "0": "The paper is well-written and does a reasonable job evaluating the robustness of architectures under an artificial form of vocabulary shift.",
            "1": "However, the authors fail to convince me why studying this distributional shift would bring new and relevant insights to the table.",
            "2": "I think for two reasons: \n\n(a) The shift is not interesting in the sense that it is related to cross-domain or cross-lingual or cross-time shifts, or any other shifts that we observe in the wild.",
            "3": "(b) The shift does not seem to add value over other synthetic shifts that have already been studied, e.g., word-level shuffling, character-level shuffling, etc.",
            "4": "You could have studied many other scrambling functions, so why this one?",
            "5": "That said, I liked §7.3, which nicely baselines the scrambled model performance.",
            "6": "§7.5 is also nice, except that it’s hard to see why any network would learn to reassociate words in the context of learning a particular task.",
            "7": "(Learning to reassociate words is not necessary to solve most tasks.",
            "8": "Maybe in machine translation?)",
            "9": "Writing: The paper is overall well-written, but I found the use of the word ‘scrambling’ for a left-to-right in-order replacement of words, misleading.",
            "10": "In related work, scrambling refers to scrambling of word order.",
            "11": "I would also have liked to see a stronger motivation for focusing on frequency.",
            "12": "Other weaknesses: \n\n(i) BERT is old now.",
            "13": "We have already seen a lot of studies of BERT (in the so-called ‘Bertology’-literature), but isn’t it time to move on?",
            "14": "I’m a little worried about community-wide overfitting of our intuitions about language models.",
            "15": "There’s the popular alternatives (RoBERTa, GPT-2, t5, etc.",
            "16": "), but also a lot of faster, fairer, more interpretable alternatives.",
            "17": "(ii) The two sequential labelling tasks are very similar.",
            "18": "POS is a give-away of NER.",
            "19": "How about finegrained sentiment, coreference resolution, semantic tagging, grammatical error detection, etc.?",
            "20": "(iii) As shown in related work, the GLUE tasks are relatively easy (e.g., solvable without word order information).",
            "21": "Maybe also consider harder sequence classification problems?",
            "22": "(iv) Using only a small subset of GLUE tasks looks like cherry-picking.",
            "23": "(v) It is unclear how sensitive the results for each architecture are to hyper-parameter changes."
        },
        "tABD54QKiT": {
            "0": "\nThe overall presentation is regular.",
            "1": "I think authors fail to properly state all the possible explanations to support the existence of transfer learning with scrambled or random input, \nIf transfer learning were the explanation of why scrambled input keeps showing transfer learning capabilities an experiment training with the original data while tested on scrambled or vice versa is needed.",
            "2": "Why didn't the authors try that configuration?",
            "3": "The analysis section can be largely improved.",
            "4": "Three out of four experiments show inconclusive results.",
            "5": "If results cannot be interpreted, authors should find other experiments to support (or refute) their claims.",
            "6": "Despite the interesting results and large number of experiments, the inconclusive results make unclear the scientific contribution of this research piece.",
            "7": "Some details:\n\n_Paper's title is somehow misleading \"Cross-Domain Knowledge Transfer for Pretrained Models\".",
            "8": "Authors use just BERT as pretrained model and a LSTM with GloVe embeddings as part of the experimentation.",
            "9": "_ The introduction and the Conclusions are inconsistent:\n\"we evaluate LSTMs using GloVe embeddings, BERT, and baseline models\" vs \"we take an English pretrained BERT off-the-shelf and fine-tune it with a scrambled English dataset\"\n\n\n_table 4 is shown on page 6, but referenced on page 8.",
            "10": "Please bring the table closer to its reference.",
            "11": "_ \"Our leading hypothesis here is that the LSTMs may actually relearn all weights without taking advantage of pretraining\"."
        }
    },
    "rhDaUTtfsqs": {
        "8aU5q7OulPy": {
            "0": "Strengths\n- Method is simple and easy to understand\n- Topic of efficient pretraining is important\n\nWeaknesses (reasons to reject)\n- In terms of new ideas, the paper is not particularly ambitious or novel.",
            "1": "I am not an expert, but given the cited related work, the results of this paper are not surprising or thought-provoking to me.",
            "2": "Curriculum learning has been shown to improve training speed in a variety of scenarios, so I am not at all surprised that if one tries hard, they could make it work with GPT pretraining.",
            "3": "Sorry if this is not actionable.",
            "4": "- The paper claims that the new lesson learned is that stability is improved with curriculum learning.",
            "5": "If a convincing result for this was shown, I have missed it.",
            "6": "I think part of it is that it is still unclear to me what the particular operationalization of \"stability\" is that is used in the paper.",
            "7": "The fact it is really hard to match the colors to lines in Figure 3 doesn't help, either.",
            "8": "For instance, where is the training instability in Figure 2?",
            "9": "Did the GPT-2 authors originally struggle a lot to do \"stable\" training?",
            "10": "If there is prior work on operationalizing stability, then it should probably be explained further in the paper, given that stability is the novel contribution.",
            "11": "- To make the above bullet point even worse, the use of the word \"regularization\" further convolutes what the main learning is.",
            "12": "My understanding of regularization is that it prevents overfitting.",
            "13": "Where's the overfitting in these charts?",
            "14": "Weaknesses (not reasons to reject)\n- The paper would be more attractive in my opinion if they really made it more about curriculum learning than it currently is.",
            "15": "What I mean by this is that the current paper seems more like \"BERT trains fast and with more stability if you gradually increase the sequence length of examples.\"",
            "16": "It would be a lot more interesting, for example, if it were shown that the author's proposed method worked for other operationalizations of curriculum learning in addition to sequence length (e.g., age of acquisition, word frequency, etc).",
            "17": "- Related to the above point, there is very little motivation for why sequence length is a correct operationalization.",
            "18": "- In fact, I do not think it is even shown that curriculum learning is the reason for the improvement.",
            "19": "For example, the author's experiments do not distinguish curriculum learning from the simple hypothesis \"putting similar examples into batches\" improves training, which is consistent with smaller gradient variances.",
            "20": "One way to show that curriculum learning is actually at play here would be to do an \"anti-curriculum\" that starts with long sequence and ends with short sequences.",
            "21": "- It seems to be that it will be hard for curriculum learning to gain traction for pretraining.",
            "22": "The cost of pretraining is high, and so introducing additional complexity will probably be done with caution, and if curriculum learning requires additional hyperparameters, people may hestitate to use it in case they do not choose correct curriclum learning hyperparameters.",
            "23": "Minor comments\n- Cite the transformer paper in sentence 1\n- It is probably not best practice to cite the GPT-3 training time if you aren't doing GPT-3.",
            "24": "Just cite the GPT-2 training time instead.",
            "25": "- The top margin seems too small."
        },
        "l0Oqxs-08Kb": {
            "0": "Strengths:\n+ The paper is generally well written and easy to follow.",
            "1": "+ The authors provide suficient technical details to reproduce results.",
            "2": "+ The experimental results show the utility of curriculum learning.",
            "3": "Weaknesses:\n- The paper applies and existing curriculum learning approach on GPT-2.",
            "4": "Hence, the paper lacks novelty.",
            "5": "The study could have been performed by other researchers without too much effort.",
            "6": "While the work might be suitable perhaps as a workshop, the level of novelty for ICLR is definitely not met.",
            "7": "- It is not clear that truncating text data leads to easier examples.",
            "8": "The truncated part might contain relevant features and by removing them, it would be impossible for the model (and for a human) to make a prediction.",
            "9": "Therefore, proposed method might also be regarded as anti-curriculum approach.",
            "10": "Is there any linguistic motivation that could support truncating the examples?",
            "11": "- The curriculum baselines chosen in Sec.",
            "12": "5.4 are very weak.",
            "13": "There are plenty of curriculum learning methods that could have been considered as baselines.",
            "14": "Minor language corrections:\n\"which helps improves\" => \"which helps to improve\" or \"which improves\";"
        },
        "uFaDiIgwTeZ": {
            "0": "Strengths:\n- simple but effective curriculum method\n- large savings in time and improvements in quality\n\nWeakness:\n- Because of the surprising method simplicity, more analysis would be interesting to add that could shad light on the nuances of interplay between gradients and the CL, and why it helps.",
            "1": "Some suggestions:\n  * Does CL lead to fewer instance of gradient clipping compared to the baseline?",
            "2": "* \"The largest variance on certain dimensions\" is mentioned as a problem in the intro and in the last sentences of sec.",
            "3": "5.3, but no experiment measures it w/ and w/o CL.",
            "4": "* Gradient similarity between neighbouring batches w/ and w/o CL\n  * A simulated experiment: while doing CL, insert a sequence of a few baseline (full-length) batches, -- will you see the \"instability spikes\" like in baseline learning curves?",
            "5": "- Given that Platanios et al reported good results both with length- and word rarity-based curricula, it would be nice to run a few experiments with the word-rarity difficulty definition.",
            "6": "- Regarding tuning the T hyperparameter: since the validation set exhibits fluctuation for some T, I wonder if it's simply because, for larger T, the curriculum hasn't yet ramped up to the actual lengths of validation data?",
            "7": "Esp.",
            "8": "since the fluctuation seem to fade away closer to the 10K cutoff.",
            "9": "If that is true, could you simply set T to a function of some length statistic of the validation set?",
            "10": "Clarification questions/remarks:\n- When trimming the batches to the current length, what happens to the shorter sentences?",
            "11": "If they stay in the batch, than, effectively it's sampling from all lengths below the current seqlen_t?",
            "12": "- The two-stage curriculum has actually two spikes (@20K in Fig.",
            "13": "3(f) and later @30K), what is the reason for the 2nd spike if the transition to full-length has already happened?",
            "14": "- Figure 4: if CL60K is preferred here, does it mean it overtakes the validation curves for CL100K and CL80K?",
            "15": "When does that happen?",
            "16": "- Some of the prior work is mistakenly described as using fine-tuning (e.g.",
            "17": "most references in the paragraph starting with \"In the natural language processing area,...\"), while they are actually using one-stage training, without fine-tuning, as is standard in machine translation.",
            "18": "Minor remarks:\n- \"extremely huge\" -> \"extremely large\" or simply \"huge\"\n- \"the gradient variance norm\" -> \"the norm of this variance\" (to be more specific)\n- \"Inspired of the highly organized curriculum\": what does the definite article refer to?",
            "19": "probably, can omit it\n- \"human and animal,\" -> \"humans and animals,\"\n- could you add a citation to the sentence ending with \", and model divergence.\"",
            "20": "in sec.",
            "21": "2?",
            "22": "- something is wrong with grammar in the sentence \"To quantitatively measure the token...\", sec 5.1\n- Table 2: I'd suggest using more intuitive '+' and '-' instead of arrows-up and arrows-down\n- Tables 1 and 2 are hard to parse because they contain different types of information: consider splitting into 2 half-tables horizontally in the middle (i.e.",
            "23": "on table for target ppl and the rest in the other)\n- a coma missing after \"Because of 1)\"\n- please use more specific wording instead of \"we find that this is not ideal\" (both in sec.",
            "24": "5 and A.2).",
            "25": "What do you mean - not ideal for 117M or in general?",
            "26": "What is \"ideal\" here?",
            "27": "- Figure 2.",
            "28": "\"the first 60K\" -> \"the first T=60K\"\n- please explain the term \"token reduction\" in sec 5.4"
        },
        "k2xc4yD6LPA": {
            "0": "Training massive transformer models in a stable manner has emerged as a challenge for practitioners trying to take advantage of the benefits of model scaling.",
            "1": "As such, providing new tools to stabilize the training dynamics is of significant interest to the community.",
            "2": "The proposed paper indeed demonstrates significant improvements in training stability and speed.",
            "3": "This being said, there are a number of issues that the paper does not properly address:\n\n1- How general are the results of the paper?",
            "4": "Do we observe similar instability issues for other causal transformers beyond GPT-2?",
            "5": "If so, is the proposed curriculum learning approach effective there?",
            "6": "2- The experimental design is not clear: The paper compares only two different hyper-parameter choices.",
            "7": "These two hyper parameter choices differ in every aspect (learning rate, batch size, #steps).",
            "8": "As such, it is unclear where the instability issue is coming from.",
            "9": "Moreover, from the paper, it is unclear to me if these instability issues represent fundamental training limitations that require an involved solution as presented in the paper.",
            "10": "It might be the case that better tuning of the hyper-parameters, alongside with more careful clipping of the gradients and activations is just enough to stabilize training."
        }
    },
    "SvFQBlffMB": {
        "XAjD0MEHWMJ": {
            "0": "The authors propose a self-distillation approach that is derived from a series of observation made regarding how LS and KD work, linking in the two together in the process.",
            "1": "Pro:\n+ The approach and the mathematical proofs provided are sound\n+ The two step training process is fast and matches or outperform the vanilla KD baseline.",
            "2": "+ The performance is tested on both vision and language data showing that the approach generalizes well.",
            "3": "Cons:\n- There is a relatively large body of work on self-distillation however there is no comparison against them, neither on explaining how they differ nor empirically.",
            "4": "A non-exhaustive list:\n[A] Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation, Zhang et al, ICCV 2019\n[B] Self-Knowledge Distillation with Progressive Refinement of Targets, Kim et al, ICCV 2021\n[C] MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation, Liu et al, ECCV 2020\n[D] Regularizing Class-Wise Predictions via Self-Knowledge Distillation, Yun et al, CVPR 2020\n- How does this method work when combined with other distillation losses (such as feature matching)?",
            "5": "KD generally can boost their performance, is the same true here?",
            "6": "- Little comparison with state-of-the-art on knowledge distillation and while surpassing the KD baseline is important in this context its unclear if applying such method in practice will suffice.",
            "7": "As such the authors should offer a comparison with the latest work on KD too.",
            "8": "- How well does this scale to larger datasets, such as ImageNet?",
            "9": "Minor:\nTable 2, some values are wrongly in bold (ex: KD outperforms P-KD on CIFAR-10)"
        },
        "9eCnjLOnm2": {
            "0": "Strength:\n1.",
            "1": "The paper proposes a new formulation of an objective function for instance-specific label smoothing regularization to unify label smoothing (LS) regularization and knowledge distillation (KD), which is quite interesting.",
            "2": "2.",
            "3": "It is good to find a closed-form solution for the inner level of the bi-level optimization problem of the proposed objective function.",
            "4": "3.",
            "5": "Experiments have been done with data from different domains (images and texts) and have confirmed the effectiveness of the proposed objective function.",
            "6": "Weakness:\nPresentation of the experimental results can be improved:\nIn Tables 4.2 and 4.4, it would be interesting to also show the standard deviation of the test accuracy/GLUE scores.",
            "7": "The results of the propose Pseudo-KD model does not seem particularly attractive in these two tables.",
            "8": "Minor presentation issues:\nPage 5: \"in the second phase, we fix the\" => \"in the second stage, we fix\"\nPage 6: It would be good to add references for the KD-Shuffle and KD-29 models."
        },
        "tSovZ-c8X2w": {
            "0": "Pros:\n1.",
            "1": "The paper is well-written and well-organized.",
            "2": "Codes are provided.",
            "3": "2.",
            "4": "The paper proposes an adaptive label smoothing method, which is novel to some extent.",
            "5": "3.",
            "6": "The proposed bi-level framework and its implementation look reasonable.",
            "7": "Cons:\n\n1.",
            "8": "The motivation of the paper is my main concern.",
            "9": "The method is called Pseudo Knowledge Distillation and the authors claim the method bridges the gap between label smoothing and knowledge distillation, however it seems to have no connection to Knowledge Distillation.",
            "10": "The generated smoothed labels don't consider the relevance of classes which is defined as the dark knowledge in [1].",
            "11": "The motivation of adding a KL divergence term into Eq.7 is not elaborated clearly.",
            "12": "2.",
            "13": "The effectiveness of the proposed method is not convincing.",
            "14": "The improvement of Pseudo-KD in practice is not significant.",
            "15": "3.",
            "16": "Some baselines are missing, such as Deep Mutual Learning [2] and Knowledge Refinery [3].",
            "17": "[1] Geoffrey E. Hinton, Oriol Vinyals, Jeffrey Dean.",
            "18": "Distilling the Knowledge in a Neural Network\n[2] Zhang, Ying, Tao Xiang, Timothy M. Hospedales, and Huchuan Lu.",
            "19": "Deep mutual learning.",
            "20": "CVPR2018\n[3] Qianggang Ding, Sifan Wu, Tao Dai, Hao Sun, Jiadong Guo, Zhang-Hua Fu, Shutao Xia.",
            "21": "Knowledge Refinery: Learning from Decoupled Label.",
            "22": "AAAI 2021"
        },
        "PrmIWB_aa6W": {
            "0": "This paper presents an online self-distillation method.",
            "1": "The paper is well-written and I enjoy reading it.",
            "2": "However, the proposed link between label smoothing and knowledge distillation, in my opinion, is still based on previous research findings (e.g., Yuan et al., 2020).",
            "3": "When compared to previous research, it would be interesting to see what new insights can be gained from the newly proposed loss function (7) (other than adding the smoothing kl term to the original ce loss).",
            "4": "Or, to put it another way, I believe it is nearly equivalent if we use the formulation proposed in previous work (Yuan et al., 2020).",
            "5": "However, in section 3.2, this paper presents a closed-form solution to the two-stage optimization problem.",
            "6": "In the evaluation part, it would be great if authors could compare Pseudo-KD to the recent efforts on self-distillation (instance-specific label smoothing), for example:\n   Zhang, Zhilu, and Mert R. Sabuncu.",
            "7": "\"Self-distillation as instance-specific label smoothing.\"",
            "8": "Yuan, Li, et al.",
            "9": "\"Revisiting knowledge distillation via label smoothing regularization.\""
        }
    },
    "GthNKCqdDg": {
        "xGSeWcSWbqQ": {
            "0": "The reformulation of the proposed idea (selector to switch generation between a base PLM and a task-specific PLM) as a reinforcement learning problem is interesting.",
            "1": "However, I have some major concerns on the technical clarify/rigor and experiment presentations that hopefully authors could clarify and help me better understand your contributions.",
            "2": "---\n\nOn the technical side:\n- during optimization, the authors seem to be computing the reward at each time step (equation (9)) of the generation instead of on the whole-sequence level.",
            "3": "However, as far as I know (I may be wrong), when RL methods are applied to NLG, the reward is typically computed on the whole-sequence instead of on the sub-sequence at each time of the generation.",
            "4": "If the reward is computed on the partial generated sequence, how is it actually implemented?",
            "5": "Why not compute the loss on the entire sequence which I think makes more sense for the BLEU and ROUGE metrics because both are operated on the whole-sequence level?",
            "6": "- What are the instantiations of $Q$, $A$, and $V$?",
            "7": "From my understanding, $Q$ and $V$ are actually not computed and $A$ equals the metric score such as BLEU score.",
            "8": "- The selector module performs hard sampling, i.e., outputting either 1 or 0, in equations (5), (6), and (7).",
            "9": "However, during _actual_ optimization, the selector performs soft sampling, i.e., outputting a probability between [0,1].",
            "10": "Why is there a mismatch?",
            "11": "If the softmax is required as an approximation to the hard sampling, why do the authors need it?",
            "12": "From my understanding, if the authors choose to use RL, then the loss function does not necessarily need to be differentiable, so approximating $\\pi_s$ as softmax may not be necessary?",
            "13": "Which implementation (hard or soft sampling) did the authors use in the experiments?",
            "14": "- What is the \"penultimate representations\" in the paper?",
            "15": "Is it the last layer hidden states of all previously generated tokens, the last layer hidden state of the last generated token, or all layer hidden states of the last generated token?",
            "16": "---\nOn the experiment side:\n- Overall I think more discussion and analyses are needed.",
            "17": "Without these additional results, it is difficult to get insights into why and how the method works.",
            "18": "The authors provide some discussions but I think it is insufficient; these claims should ideally be supported by empirical results.",
            "19": "More below.",
            "20": "- My main concern is Table 2, which does not seem to make sense to me.",
            "21": "If I understand correctly, for the ROUGE metric, the higher number the better, and it cannot be a negative number.",
            "22": "In Table 2, PLM, which is a baseline, achieves far better results than any other methods under comparison by a huge margin, including the authors own proposed method.",
            "23": "Moreover, one ROUGE result is negative.",
            "24": "What happened?",
            "25": "- There is another baseline that I think the authors should compare but did not.",
            "26": "How about setting $A$ in equation (9) to 1.0 and just run MLE using equation (9) as the loss?",
            "27": "To me, $A$, which results from the authors' RL formulation of the optimization problem, is in practice simply a scaling factor to the usual MLE loss if they did not involve RL.",
            "28": "This comparison would suggest how useful is RL training compared to MLE.",
            "29": "- There are different BLEU metrics relying on unigram, bi-gram, etc, and similarly for ROUGE metric.",
            "30": "How does the different choices of these variants of the metrics as the reward function impact results?",
            "31": "- What does the selector do and when does it switch?",
            "32": "Some more insights would be really useful.",
            "33": "For example, the authors can provide some results on which word in the generation is generated by the frozen PLM $\\pi_{\\rm LM}$ and which word is generated by the fine-tuned policy $\\pi_a$."
        },
        "5s28ObMu1Hv": {
            "0": "Strengths:\n- The method is novel and brings knowledge of reinforcement learning to the challenges of NLP.",
            "1": "- Paper is overall well written and easy to follow\n\nWeaknesses:\n- Gains with the method are limited, especially in summarization.",
            "2": "Breath of results is also limited as the authors consider only two tasks\n- I would have appreciated seeing human evaluations.",
            "3": "It is not obvious from the few samples in the appendix that the method leads to qualitatively superior generations.",
            "4": "- Beyond the numerical gains, it is not clear why the method leads to better generations.",
            "5": "For instance, it is not clear how the token predictions at each step differ between the baseline and the introduced method?",
            "6": "Questions:\n- The paper seems to be built on the hypothesis that fine-tuning PLMs in low data regimes leads to overfitting.",
            "7": "Yet, it is not obvious from the numbers for PLM in line Table 1 and 2.",
            "8": "Thoughts?",
            "9": "- Another core hypothesis seems to be the intuition that there are “task-relevant parts in sequence generation”.",
            "10": "I couldn’t find any definition of what “task relevant parts” are.",
            "11": "How does it differ from learning the format of the task?",
            "12": "- Why use the fine tuned model as an initialization for the model with adapters (non-stg).",
            "13": "It seems related but I don’t understand this sentence: “this fine-tuning phase can be skipped when the advanced networks were used in the PLM such as External Encoders or Adapters which can cover the large domain shift”.",
            "14": "- Table 2: are the numbers for non-stg, ne and stg rouge points or % difference with the PLM baseline?",
            "15": "- Could you elaborate on the robustness of the method?",
            "16": "For instance, what is the standard deviation between multiple runs?",
            "17": "Additional Feedback:\n- Typo - Equation (1) - What is \\theta_g?",
            "18": "I think you mean \\theta_a."
        },
        "dFTLrdJIQZZ": {
            "0": "Overall the paper is well written, barring some unnecessary complexity in the description of the technique (details below), and the paper does a good job iterating through details of the method and considering some alternatives.",
            "1": "However I find the results unconvincing, primarily in their improvement over the base method.",
            "2": "There are also some questions I have with the proposed method and how it is supposed to solve the overfitting issue like the paper claims.",
            "3": "As summary, I would say the paper is clear and the proposal is not something I am familiar with in the literature, but the improvement seems marginal.",
            "4": "**Detailed Comments:**\n\n* **<Introduction>** You describe large-scale training data as needing high quality annotations, but much of the larger models are trained on unsupervised text.",
            "5": "I think this should be clarified.",
            "6": "* **<Section 2.2>** Can you explain the footnote?",
            "7": "* **<Section 2.2>** How exactly is the approach motivated by auxiliary training?",
            "8": "* **<Sections 2/3>** Equations 6 and 7 are clear and convey the key aspect of the proposed method well (absent the RL loss), however the majority of the other questions seem more dense and wordy than necessary, and are hard to follow at times (what is G in equation 1?).",
            "9": "I think the sections introducing the method could benefit from more clear and concise mathematical formation (i.e.",
            "10": "equation 9 can be re-written with less repetition).",
            "11": "* **<Section 3>** The motivation for STG is to prevent overfitting., I understand why wanting to rely on the pretrained logits mostly and only sometimes use the finetuned ones would help ameliorate overfitting, what I don't understand is exactly how this instantiation helps.",
            "12": "Why does the RL not learn to always use the finetuned logits downstream, since surely that would help on the finetuning loss?",
            "13": "* **<Section 4>** Prompt engineering is brought up, how well does that compare to the other methods discussed in terms of finetune performance?",
            "14": "* **<Section 4>** Why not try NE(sum)?",
            "15": "* **<Section 5>** Why does the hierarchical policy reduce the action space?",
            "16": "* **<Table 5>** STI should be replaced with STG."
        },
        "pzGqhE3Ecz8": {
            "0": "Overall speaking, this paper is somehow not qualified enough.",
            "1": "Below are the main points:\n1.",
            "2": "The algorithm is the RL-based method, the difference is that the author introduces a policy selection policy to decide the generation policy.",
            "3": "This is straightforward and not new to the NLP generation community.",
            "4": "The novelty and technical contribution are truly limited.",
            "5": "If this is not the case, the results are also not surprising to see the improvements.",
            "6": "Therefore, it is hard to see the shining points.",
            "7": "2.",
            "8": "Even the method is not interesting, the experimental comparisons are also not enough.",
            "9": "The authors do not give a comprehensive study of the results and the comparison with other papers or baselines.",
            "10": "This is again, not qualified.",
            "11": "3.",
            "12": "The selective policy is introduced in a self-learning way, which is automatically learned by the same hidden states of generation tokens.",
            "13": "Therefore, a study of the policy selection policy is required.",
            "14": "However, it is missed, also for other parts in the model.",
            "15": "The authors are highly encouraged to make a major modification of the paper."
        }
    },
    "gKWxifgJVP": {
        "ZwfEsJzN7re": {
            "0": "Strengths:\nThe paper is well-written with clear motivations and structure.",
            "1": "The concept of fact units is interesting and novel which are easily constructed via dependency trees.",
            "2": "Compared with commonsense and entity-relation knowledge, the fact units are more informative to each specific question.",
            "3": "By associating nodes across different fact units based on coreferences and mentions, a supergraph is built that connects all related information and conducts graph reasoning for answer predictions.",
            "4": "The proposed model is then experimentally verified on three logic-driven datasets which demonstrates some performance gain.",
            "5": "Weaknesses:\n1.",
            "6": "Some of the details for model description are missing and confusing, e.g., (1) How the representation is initialized for a supernode corresponding to a fact triplet and how does the supernode propagate information to the sub-nodes?",
            "7": "(2) Equation 2 and 3 use the same graph encoder $F_{G}$.",
            "8": "Are they the same?",
            "9": "Since they both use $m$ nodes, does it mean the supergraph also operates on each sub fact node?",
            "10": "If so, does the global node $V_g$ connect to all the sub fact nodes?",
            "11": "It is then not clear how each fact block (grey squares in Figure 4) functions as a whole.",
            "12": "The notations in equation 2 and 3 are also inconsistent with equation 5.",
            "13": "What does L, S, F and m represent?",
            "14": "(3) The loss for answer prediction in Eq 6 is not clearly described: do you use an aggregated embedding over all nodes for prediction?",
            "15": "(4) How exactly is the interaction module processed?",
            "16": "2.",
            "17": "According to the description, the fact units are constructed using the dependency parser.",
            "18": "Hence, the model relies on whether the parser accurately discovers the crucial information.",
            "19": "I am also wondering if the extracted facts could bring too much noise to the question.",
            "20": "As shown in Figure 6, when more than 12 facts are constructed, the performance becomes worse.",
            "21": "3.",
            "22": "More discussions on comparing with symbolic logic reasoner model LReasoner are needed.",
            "23": "Indeed, the proposed graph model seems to only implicitly convey knowledge across facts in terms of local reasoning.",
            "24": "On the other hand, the symbolic logic rules are able to express global patterns.",
            "25": "Hence, to my understanding, it may not be suitable to use the term \"global reasoning\" in this work.",
            "26": "4.",
            "27": "For experiments, Table 1 shows the FOCAL Reasoner (DeBERTa) brings much performance gain, compared to FOCAL Reasoner (RoBERTa) (the performance seems comparable with DAGN and LReasoner using RoBERTa).",
            "28": "Can you explain why?",
            "29": "Is it possible to run DAGN (DeBERTa)?"
        },
        "kkXt0oJFGW1": {
            "0": "As I served as one of the reviewers for this paper before, I am also sharing a summary or quote of reviews from the last round here as the paper did not go through major changes since the last submission.",
            "1": "Content-wise, $LReasoner_{RoBERTa}$ , $LReasoner_{DeBERTa}$ , as well as $FocalReasoner_{DeBERTa}$ were added as new result in Table 1.",
            "2": "### Strength\n\n- (From the previous review) All reviewers noted that the main strength of the work to be its good empirical results on the 3 benchmarks considered.",
            "3": "- This strength is corroborated with the additional results of competitive baselines and  $FocalReasoner_{DeBERTa}$.",
            "4": "- The paper is well-written overall.",
            "5": "I appreciate the authors making an effort to organize and clarify the complex steps that this paper takes.",
            "6": "### Weakness\n-  (From the previous review) The unanimous criticism was that while the results are impressive, given the complex nature of the proposed system, it was difficult to understand which aspects of the proposed techniques are truly effective.",
            "7": "- While the coreference, and entity linking are the major resource in creating the supergraph, I don't think the challenges nor machinery in getting this information is well described.",
            "8": "(It might be in the text, but I am not sure how the authors get dependency parse trees as well.)",
            "9": "It would be nice to have  more description on the challenges of identifying the global relations (same entity, coreference)\n- As described in the *summary* section, I think the main contribution of this paper is in getting fine-grained facts and connecting them with global information.",
            "10": "I am not entirely convinced whether this is a very novel approach.",
            "11": "What is the major scientific contribution here?",
            "12": "- The authors attempt to decompose some parts of the method in Tables 5 and 6.",
            "13": "Related to the earlier weakness on the complex nature of this work, I am not sure what role the *logical fact regularization loss* serves in this paper.",
            "14": "Is this one of the contributions of this paper?",
            "15": "Or is it simply a tool to get a better results?",
            "16": "Without it, the performance 66.8 --> 64.2 which is lower than competing models.",
            "17": "Some of the comments that other reviewers made from the last round of review (which resonated with me):\n- \" **Significance**: This is where I am most conflicted about the paper.",
            "18": "The results are difficult to interpret.",
            "19": "Certainly, there are gains from the paper's technique and the ablations show that the different components of the model all contribute to the performance.",
            "20": "But when fairly important elements of the model like coreference edges contribute only around 2% accuracy, it's hard to know how much to trust the narrative here about reasoning.\"",
            "21": "- \"More generally, in a multiple-choice task, it's hard to know that the model is really behaving as advertised rather than just adding capacity on top of the baselines.",
            "22": "It's not easy to form an apples-to-apples comparison in terms of number of parameters.",
            "23": "The case study in Section 5.3 shows that something is happening in the graph component, but whether this is *causally associated* with the ability to get the right answer as opposed to a byproduct is hard to determine.\"",
            "24": "### Questions/Suggestions\n\n- I am not sure what each edges really do: (*default-in, default-out, reverse-in, reverse-out, self*).",
            "25": "It would be nice if authors can describe the roles of these edges.",
            "26": "- Figure 5's *global edge* and what this paper claims as *global (typical entity or sentence relation)* is confusing.",
            "27": "Maybe try to use distinguishable terms?",
            "28": "- Why don't you update 4.3 (1) sentence to match newly updated best result table 1?",
            "29": "- \"FOCAL REASONER also outperforms the prior best system LReasoner, reaching 77:05% on the EASY subset, and 44:64% on the HARD subset.\""
        },
        "JTAlexPz0XG": {
            "0": "## Strengths\n\n* The model seems fairly intuitive.",
            "1": "The idea of representing the (entity + relational) structure of a text directly and propagating information through that structure has been prevalent in the IE / RC space for a while but it seems that many have had difficulty getting it to work well.",
            "2": "This paper shows improvements which are big enough to indicate that there's probably something to it (though I have a few concerns about this, described in the weaknesses).",
            "3": "It seems to me that the key to the success of the model is probably just in the distribution of data that it's applied on.",
            "4": "Such approaches may not have worked for earlier datasets (such as SQuAD) simply because they were not suited to teach or test the kind of information aggregating abilities that this model is designed to do.",
            "5": "* The ablations are nice.",
            "6": "## Weaknesses\n\n* I take issue with some of the terminology — use of the terms \"fact\" and \"logical reasoning\" are, in my view, a bit misleading as descriptions of what a model like this one is doing.",
            "7": "The tuples being extracted from the text are more like \"propositions\" than \"facts\" and the operations in the GCN don't seem to have much in common with \"logical reasoning\" (which normally involves the application of rules from a formal system).",
            "8": "I understand there seems to be a bit of precedent for the terms here, as the datasets being used were designed to capture aspects of logical reasoning, but I think it's less correct to reuse them when describing model components such as \"Logical Fact Regularization.\"",
            "9": "Something like \"propositional structure regularization\" might be more appropriate.",
            "10": "* I'm worried that this model's improvements will end up being specific to the distribution in \"logic\"-focused datasets like ReClor and LogiQA, because of these datasets' focus on aggregating and combining information about a small set of entities in a paragraph.",
            "11": "While I understand that this capability is the focus of this work, I think it's important to know if the proposed architecture is too specialized: does it maintain performance over its RoBERTa or DeBERTa baseline if applied to more naturalistic, extractive reading comprehension datasets like Natural Questions, QuoRef, or HotpotQA?",
            "12": "If not, is it because of the distribution of text, extractive format, or other factors?",
            "13": "It's fine if the the model doesn't end up improving these cases — I think investigating these issues would greatly improve the paper either way.",
            "14": "* I would suggest more caution when it comes to interpretability.",
            "15": "Attention weights cannot be relied on to provide explanations of model behavior.",
            "16": "To make the interpretability case study in Section 5.3 more believable, I would suggest modifying the attended element and seeing if the answer changes: for example, change \"playing football can improve students' academic performance\" to \"football players are held to higher academic standards than non-athletes\" and see if the attention pattern or the answer changes.",
            "17": "If it doesn't, then the attention pattern — while still indicative of the information relevant to answer the question - might not be indicative of the kind of logical reasoning we suspect.",
            "18": "* I'm not sure if the findings are statistically robust.",
            "19": "All of the test sets here are very small — less than 1000 items.",
            "20": "The numbers seem good but I'm not sure what the uncertainty looks like.",
            "21": "Please provide confidence intervals for the Focal Reasoner and at least some of the stronger baselines.",
            "22": "* Are the baselines well-tuned?",
            "23": "Time and time again, modeling innovations are proposed which ultimately get beaten by a better-tuned baseline down the line.",
            "24": "How many experiments did you run for each model variant?",
            "25": "Are you reporting the best of several runs?",
            "26": "How did the performance of each model vary over runs?",
            "27": "Can you say that the distribution of the new model's performance is appreciably better than the baseline models?",
            "28": "By how much, and with what confidence?",
            "29": "## Minor comments\n\n* In equations 4 and 5, wouldn't it be better to use $v_\\text{predicate}$ instead of $v_\\text{relation}$?",
            "30": "It's a bit confusing to me to use the word \"relation\" here since earlier the word is used to refer to the types of edges between vertices in the graph."
        },
        "swfjZpjnLnH": {
            "0": "Strengths:\n1.",
            "1": "The approach is interesting boasts great results on two benchmarks compared to previous approaches.",
            "2": "2.",
            "3": "The authors also perform useful analysis on the model which is interesting and would be useful to the community.",
            "4": "3.",
            "5": "The approach is, to some extent, described well.",
            "6": "4.",
            "7": "The authors also plan to open source their code which would be beneficial to the community.",
            "8": "Concerns, questions, and suggestions:\n1.",
            "9": "In the related works section, the authors fail to describe and differentiate themselves from the two baselines that they compare against (DAGN and LReasoner).",
            "10": "2.",
            "11": "In the related work section, the authors describe a slew of other methods that the model is not compared against.",
            "12": "This makes it hard to place their approach in comparison with prior work thus making it hard to understand the novelty of the approach.",
            "13": "3.",
            "14": "Perhaps, the authors could add a section describing related works that are used for the tasks described.",
            "15": "(The authors claim that the baselines are described in the appendix but that is not the case)"
        }
    },
    "Q8OjAGkxwP5": {
        "NLDANw4h8d": {
            "0": "Strengths:\n\nThe topic investigated in this work is interesting and would be useful for further works since pre-trained models have been shown effective at learning with small amounts of annotations and AL is also a method with similar spirit.",
            "1": "This paper provides a good and solid empirical analysis on this topic with various different models, datasets and AL methods.",
            "2": "There are several detailed ablation analyses which I find interesting, such as the influence of question format and adversarial filtering on the datasets.",
            "3": "Weakness:\n\nOne of my major concerns is on the point of training instability.",
            "4": "I feel that this might not be the problem of AL, but at least some part of the problem is that we need a more robust way to train the model?",
            "5": "Surely the random baseline is also treated in similar ways, so the comparisons are fair.",
            "6": "Especially, fine-tuning might be sensitive to hyper-parameters, and it seems that there are no specific tunings?",
            "7": "Surely I understand the cost of doing this, but I think it would be nice to select some of the datasets to carry out some hyper-parameter tuning.",
            "8": "As discussed in Section 3.4.4, annotation batch size could be important.",
            "9": "I think there should be more experiments on this, for example, with larger sizes rather than smaller.",
            "10": "One specific question that I have but might not be fully answered in this work is that: why are the examples selected by AL more likely to cause the training instability?",
            "11": "Some experiment results show that outliers might not tell the full story.",
            "12": "Then why does this happen, is it related with instance diversity or even simpler label diversity (for example, maybe in some iterations, AL only chooses instances with the same label)?",
            "13": "I'm quite interested to see more analysis on this.",
            "14": "Other comments:\n\nFor the hyper-parameter issue above, what are the differences of multiple runs in the multi-training (in Section 3.3), only changing seed or changing hyper-parameters?",
            "15": "If only seed, I suggest doing similar experiments with some degree of tuning of hyper-parameters.",
            "16": "It would be nice to have results where all the datasets are available as in standard supervised settings as upper-bounds.",
            "17": "It would be interesting to explore tasks beyond simple classification."
        },
        "pZicUZMjsyw": {
            "0": "Strengths: The experimental scale of this work is undoubtedly large and comprehensive, involving many data sets, different types of active learning algorithms.",
            "1": "It is constructive for the researchers who tried to use active learning methods for reducing the labeling cost in NLP tasks.",
            "2": "Weaknesses: \n1) compare with [1], the research questions and the corresponding conclusions of this work are not enough novel and attractive.",
            "3": "Actually, current studies have involved: i) marginal benefits of deep active learning methods; ii) employing active learning with pre-trained LMs like Bert on regular NLP tasks; iii) employing active learning on challenging tasks like VQA.",
            "4": "In fact, these studies have covered almost all existing research topics that worth studying and revealed the problems existing in deep active learning.",
            "5": "For now, it is more meaningful to address these issues than to continue to discuss whether deep active learning has similar problems in some subdivided tasks.",
            "6": "2) In the experiments, the authors only adopt 500 quota for each AL experiment, it's too small.",
            "7": "For example, in Figure 2 (CODAH), the performance of AL methods are far from convergence, the author should adopt more quota or at least provide the performance trained on full dataset, like [2].",
            "8": "--------------------------------------- Minor Issues -------------------------------------------\n\n3) Many writing problems could be found in this paper.",
            "9": "For instance, in page 2, in contribution 2, the sentence is incompleted.",
            "10": "In section 2.2, should add ';' or '.'",
            "11": "at the end of each item when describing the AL algorithms.",
            "12": "4) It is inaesthetic to plot standard error bar, like Figure 1, can use plt.fill_between instead of plt.errorbar.",
            "13": "5) Should pay more attention to design the layout of tables, e.g., in Table 6, it should be Method | Dataset | AL methods.",
            "14": "References:\n[1] Karamcheti S, Krishna R, Fei-Fei L, et al.",
            "15": "Mind your outliers!",
            "16": "investigating the negative impact of outliers on active learning for visual question answering[J].",
            "17": "arXiv preprint arXiv:2107.02331, 2021.",
            "18": "[2] Margatina K, Barrault L, Aletras N. Bayesian Active Learning with Pretrained Language Models[J].",
            "19": "arXiv preprint arXiv:2104.08320, 2021."
        },
        "cZ50f-Pgla-": {
            "0": "Strengths:\n \nThe authors connect active learning example selection to more severe training instability than random selection.",
            "1": "Further, they show downside-variability (“greater instability on the losses than on the wins”) suggesting specific sets of examples are responsible for systemic failures in active learning, but not necessarily that other selections would greatly outperform the random baseline.",
            "2": "Both of these observations are of interest to the NLP active learning community, adding depth to the characterization of the problem.",
            "3": "The wide range of datasets and active learning techniques they use (including BALD which prior works shows is very competitive) lends credence to the conclusions.",
            "4": "It especially corroborates that certain tasks are innately more challenging for active learning than others.",
            "5": "Their thorough ablation experiments and other analysis yield some interesting findings the authors could emphasize more.",
            "6": "For instance, the greater instability of larger Transformers to active learning bodes poorly for practitioners leveraging ever increasing model sizes for low-resource datasets.",
            "7": "Additionally, the authors conclude that pre-training is not a significant factor in the efficacy of active learning, but their numerical results suggest active learning methods (Entropy and Coreset) narrow the gap with the random baseline significantly from BERT-Base to RoBERTa-Base!",
            "8": "The margin of change seems even larger than some results which are discussed in the paper as significant.",
            "9": "More discussion could bolster this finding.",
            "10": "Weaknesses:\n \nThe primary weakness of the paper is the lack of convincing justification that the authors have discovered a phenomenon distinct from “collective outliers” (Karamcheti et al., 2021) -- or if it is distinct, how exactly is it distinct?",
            "11": "1.",
            "12": "For context, Karamcheti et al.",
            "13": "(2021) introduce an \"oracle\" active learning method that prunes uninformative (outlier) examples from the unlabelled, selectable pool U, but it requires gold labels to filter these.",
            "14": "This work tries a single pruning rate and finds it significantly degrades, rather than improves results: for both active learning and random selection.",
            "15": "However, they only mention a single pruning rate of 50%, which could easily over-prune (all challenging as well as outlier examples) for the NLP datasets used here.",
            "16": "Especially because of the surprising magnitude by which this pruning degrades absolute performance, it is unfortunately necessary to try more pruning rates for a fair comparison.",
            "17": "2.",
            "18": "Even if pruning is ineffective (at multiple pruning rates), it’s never really explained why, despite this being a core contribution of the paper.",
            "19": "A qualitative analysis was required in Karamcheti et al., (2021) to explain “collective outliers”.",
            "20": "A similar analysis here could greatly demystify why these sets of examples cause instability, and whether they are indeed “informative”.",
            "21": "There are a couple decisions related to methodology that may need some further examination.",
            "22": "1.",
            "23": "The first is the total unlabelled pool size.",
            "24": "It appears to vary by orders of magnitude according to Table 10 in the Appendix.",
            "25": "While the batch selection L and total selection Q variables are constants, fixed at 25 and 500 respectively, the pool of examples to choose from varies wildly.",
            "26": "This is very likely a confounding factor in the efficacy of active learning and pruning techniques.",
            "27": "It deserves some discussion or control.",
            "28": "2.",
            "29": "The second is the choice of L=25 examples chosen at each acquisition, and also as the seed set.",
            "30": "I am concerned this is quite low and could yield exaggerated instability, especially for large Transformer models.",
            "31": "If I read them correctly, Ein-Dor et al.",
            "32": "(2020) use at least L=50 for simple classification tasks, and Karamcheti et al., (2021) use L>= 400 and larger seed sets.",
            "33": "The authors investigate L=12 in the ablation, but in a real setting, it seems unlikely practitioners will label <50 examples before re-training.",
            "34": "Particularly because the reported accuracy margins are so slim, either of these variables could modify the empirical conclusions.",
            "35": "A statistical test on the Average performances would also improve confidence in the conclusions.",
            "36": "Questions and minor suggestions for the authors:\n\n1.",
            "37": "Why is Entropy missing from Table 2?",
            "38": "And BALD-MC from Tables 3 and 4?",
            "39": "And why is BALD missing again from Table 6?",
            "40": "Computational expense is used to justify the omission of BALD in one instance, but the others appear inconsistent?",
            "41": "2.",
            "42": "It’s very hard to compare the absolute differences Tables 1 and 2 for ourselves.",
            "43": "Perhaps there is some visual representation that could help demonstrate the comparisons you make in the text?",
            "44": "3.",
            "45": "Margins are very small for the Average differences across all datasets as well -- have you considered confidence intervals on those as well?",
            "46": "4.",
            "47": "Minor: Only half the datasets are shown in Tables 3 and 4, but it’s unclear how/why those were chosen?"
        },
        "PRwSOfCZOTS": {
            "0": "Strength:\n- Experiments on many data sets.",
            "1": "- The idea of using convex hull of AUC is interesting, but it would it nice to confirm it experimentally.",
            "2": "Weakness:\n- Q seems to be too small, which might amplify the instability issue.",
            "3": "- Little novelty \n\nWhile their experiments, and findings are interesting, it leaves many questions that could (at least in principle) be answered by more experiments.",
            "4": "Their main finding seems to be that larger transformer models lead to more instability for training with the instances selected by AL, Table 4 and Table 5.",
            "5": "However, Table 4 is only an approximation to results that could be achieved with training with different initializations.",
            "6": "Though computationally expensive it would be very interesting to see whether the results from the approximation can be confirmed.",
            "7": "Several questions:\n- What exactly is meant by the Entropy method?",
            "8": "My understanding that is that Entropy is the same as Least Confidence, since high entropy <-> low confidence.",
            "9": "- Why is Entropy missing in Table 2?",
            "10": "- Why is Q set to 500?",
            "11": "This seems to be too small.",
            "12": "It would be interesting to learning curve graphs with larger Q."
        },
        "gaXylHaxXWD": {
            "0": "Strengths:\n - I could learn various empirical observations of how active learning works in these large pre-trained models on various NLP tasks.",
            "1": "I hope to see this paper at the conference and others can learn from it as well.",
            "2": "Weakness:\n - It would be nice to see some macro analysis of data points showing the overall tendency of each setup.",
            "3": "- I hope to see more insights regarding how these two different AL methods (density-based and entropy-based) would work in a different way."
        }
    },
    "9zcjXdavnX": {
        "OneLxjUeZy8": {
            "0": "Sampling from unnormalized distributions is notoriously difficult.",
            "1": "Traditional rejection sampling cannot be applied in many settings (including the settings addressed in this work) and because of this, MCMC techniques are often favored.",
            "2": "Unfortunately, diagnostics for tuning MCMC methods often do not exist and therefore they are often tuned in qualitative ways.",
            "3": "QRS gives more to the user in this way.",
            "4": "If the user has a fixed computation target, the user can get some idea of the quality of their samples within some budget.",
            "5": "The same cannot be said for MCMC methods.",
            "6": "This is a desirable property in a method intended to be used by people who may not be MCMC whizzes.",
            "7": "As well, the method is very simple and near trivial to implement which will greatly add to the impact of this method.",
            "8": "Given a computation budget or a divergence target, the single parameter appears simple to tune and could even be done automatically (have you thought of this?).",
            "9": "Another nice property of the approach is that it can be easily applied on top of any new advances in learning the proposal distributions.",
            "10": "If development is made in this area, QRS could easily be dropped-in on top to further improve results.",
            "11": "Weaknesses:\n\nThe main weakness in this work is in the empirical evaluation.",
            "12": "The method is mainly compared with sampling only from the original proposal distribution.",
            "13": "In this comparison, the method (clearly) appears favorable.",
            "14": "As well, there is a comparison with Independent Metropolis-Hastings (IMH) in the appendix where there appears to be some benefit but it is not as drastic.",
            "15": "I believe a comparison with localized MCMC methods should also be provided.",
            "16": "There has been recent development in this space and it would be useful to understand how well this approach works in the context of these recent developments.",
            "17": "The same evaluations could be used as in Figure (b) in the appendix.",
            "18": "You could seed the chains with your proposal distribution and then apply some MCMC algorithm for a certain number of steps.",
            "19": "You could compare with any number of MCMC samplers such as random-walk metropolis, Gibbs sampling, locally-balanced proposals, gibbs-with-gradients, discontinuous HMC, discontinuous SGLD, etc (I am not asking you to compare with all of these approaches, just one or two maybe).",
            "20": "The x% acceptance rate QRS requires 1/x samples per accept.",
            "21": "Since each sample requires L model evaluations (where L is sequence length) this requires L/x model evaluations in total.",
            "22": "Metropolis requires 2 evals, gibbs-with-gradients requires 4 (2 evals + 2 gradients), so a runtime-fair comparison could easily be obtained with many of these local MCMC approaches.",
            "23": "I want to reiterate that I enjoyed this work and would like to see it accepted, but I feel it is lacking context in its current presentation.",
            "24": "The authors claim a major upside of QRS is due to a number of deficiencies in these methods, so I think a comparison with the most recent methods in this area is warranted.",
            "25": "I am happy to facilitate a discussion about this and if you feel this request is unreasonable, I am happy to discuss.",
            "26": "Besides this, I am also somewhat concerned with the importance sampling estimators of TVD/KL/accept-rate.",
            "27": "Importance sampling maybe unbiased but it can have high variance.",
            "28": "Could you also provide some information on the variance of the estimates used in your experiments?",
            "29": "---------Post discussion period------------\nI thank the authors for responding to my comments.",
            "30": "I appreciate the new experiments on localized MCMC.",
            "31": "It appears that the proposed approach gives improved performance for a similar compute budget.",
            "32": "While I would love to have seen a GWG comparison, I understand the overhead that it would require and am fine that it was not included.",
            "33": "Unlike the other reviewers, I do not see the \"lack of novelty\" as an issue of this work.",
            "34": "It should be favored when simpler ideas, applied in novel ways or to new problems, lead to good results on important problems.",
            "35": "Based on the authors response, I will raise my score slightly in favor of acceptance."
        },
        "ymiDt5M8KMO": {
            "0": "## Weaknesses\n\n1 - The authors propose a relaxation of rejection sampling which is using an arbitrary parameter $\\beta$ instead of the true upper bound of the ratio $\\frac{p}{q}$ when the latter cannot be computed.",
            "1": "The reviewer fails to understand why the authors did not directly use Importance sampling in the first place.",
            "2": "2- In algorithm 1, the reviewer fails to see a difference between QRS and RS, and will change their opinion if the authors can point out a value of `u` for which QRS and RS will behave differently.",
            "3": "3 - Uninteresting Section 2.2:\n    - Equation 1 needs a parenthesis to avoid confusion\n    - Equation 3 is pretty much obvious from the definition of TVD (Lemma 2.19 Aldous and Fill https://www.stat.berkeley.edu/users/aldous/RWG/book.html)\n    - Equation 4 is obvious since using the apropriate upper bound gives you rejection sampling which is a perfect sampling algorithm.",
            "4": "4 - In the abstract the authors require the proposal distribution to upper bound the target everywhere which is not true as the authors themselves clarify in the text.",
            "5": "5 - While Equation 9 and 10 are great in that they can be used to compute TVD and KL between the true and QRS distributions, there are multiple issues which are neither stated as assumptions nor addressed appropriately, namely:\n    - They're not unbiased estimators since $Z$ is not known and needs to be estimated, this point is not explicitly stated.",
            "6": "- It is assumed that the normalizing constant of $q$ is known, which is not always the case.",
            "7": "- They rely on importance sampling, which begs question 1."
        },
        "ohGu5shAh3F": {
            "0": "The proposed method is very simple and easy to use in practice.",
            "1": "However I have the below concerns.",
            "2": "1.",
            "3": "The main concern I have is how efficient the proposed method is given a usable sampling quality.",
            "4": "From the results in the experiment section, to reach a reasonable TVD (e.g.",
            "5": "< 0.001), the acceptance rate has to be very low which means QRS needs many steps to produce one sample.",
            "6": "Besides, the hyperparameter beta and the proposal distribution need to be carefully chosen.",
            "7": "2.",
            "8": "Given the above concern, I wonder how the proposed method compared to MCMC methods.",
            "9": "Given a similar number of steps, the MCMC method might also be able to mix and produce usable samples.",
            "10": "This comparison is important and it is currently missing.",
            "11": "For example, the authors could consider comparing with [Oops I Took A Gradient: Scalable Sampling for Discrete Distributions, ICML 2021] where they also tested on EBM, and the independent Metropolis-Hastings as mentioned in the paper.",
            "12": "3.",
            "13": "Algorithm 2 with automatically tuned beta is interesting since beta is an important hyperparameter and may require a lot of tuning to make QRS work.",
            "14": "It will be better to show some experimental results of Algorithm 2.",
            "15": "Right now it is not clear to me what the distribution it samples from.",
            "16": "Since beta changes along with the training, the target distribution also changes."
        }
    },
    "0RDcd5Axok": {
        "-ZIWcdyn-tP": {
            "0": "Pros:\n\n- The common framework for different approaches is useful overall.",
            "1": "In particular, I liked the section clarifying the connection between Prefix Tuning and Adapters.",
            "2": "Although there is nothing groundbreaking here, I believe these frameworks can be useful and help researchers.",
            "3": "- The authors show how the framework can be used to derive new approaches.",
            "4": "They also show that these approaches can be more effective than existing ones.",
            "5": "- The experimental setup is well-described overall, with hyperparameters being provided\n- This is a hard analysis to perform, with many possible things to change.",
            "6": "I feel like the experiment set chosen is convincing overall, with the caveat mentioned below.",
            "7": "Cons:\n\n- Significance of results:\n\nThere are no standard deviations for any results in the paper, which makes it hard to assess significance of many results, esp.",
            "8": "since some of the performance gaps are small.",
            "9": "For instance, in table 6, authors have a 0.4 BLEU discrepancy in their replication of full fine-tuning performance but draw conclusions on MAM being best based on a 0.2 BLEU gap.",
            "10": "The gap is bigger when comparing MAM to methods not introduced by the authors, but still.",
            "11": "The story makes sense but I am not 100% confident in the robustness of the results.",
            "12": "Using two decimal numbers for tasks (e.g: XSum) also gives a false impression of precision.",
            "13": "I understand fine-tuning on some of these tasks (MT / XSUM) can be resource-intensive but having standard deviations in even a subset of the experiments would be useful.",
            "14": "- Interpretation of Figure 4 in Section 5.2 and effectiveness of MAM Adapters for encoder models\n\nThe authors highlight that while existing methods perform well on MNLI/SST, they are underwhelming on en-ro / XSum.",
            "15": "They conclude that this means existing parameter-efficient transfer learning (PETL) approaches are not great for higher-resource / more challenging tasks.",
            "16": "However, I would point out that this data can also support a different conclusion: existing PETL approaches work well for encoder-only models but are not great for encoder-decoder models.",
            "17": "Including the T5 datapoint is also not very relevant since in that case Superglue is treated as a single task (w/ one adapter).",
            "18": "Answering “For which tasks / architectures do PETL methods perform well” is an interesting question in itself and I feel like this section expedites this.",
            "19": "It is likely that both the architecture and #datapoints in the task matter.",
            "20": "The author’s choice at the end of section 4.2 is to focus on XSum / en-ro MT.",
            "21": "It would be great to highlight that not only are those higher-resource tasks, but they are also generative ones, and thus all the conclusions of this paper might not apply widely to encoder models.",
            "22": "Indeed, the results of MAM adapters in table 2 are quite mixed.",
            "23": "Right now the paper is claiming more generality than deserved.",
            "24": "Two possible fixes here: (1) Make claims less general and specific to enc-dec models, (2) Conduct more experiments on encoder-only models \n\n- Writing:\n  - The writing is subpar right now.",
            "25": "Example: section 4.4: “its counterpart at attention” -> “its attention counterpart”,  “FFN can better utilize modification at larger capacities” -> modifications or “modification of the FFN is better at larger capacities”, etc.",
            "26": "- Minor: The citation style is often wrong for the sentence.",
            "27": "Use \\citet more often.",
            "28": "- Minor: Replace “For classifications” with “for classification tasks” in the Appendix (twice)\n  - Typos: Section 5 name Discussions -> Discussion ;  Appendix A3 Learning -> Learning.",
            "29": "- Minor:\n  - For Figure 2, I feel like the full fine-tuning number should come from the original baseline, not the replication.",
            "30": "- The protocol in 4.5 for choosing the scaling parameter for Scaled PA is a bit surprising.",
            "31": "I would just suggest that it should be an hyperparameter, the current description makes it seem like it is based on hyperparam tuning for LoRA.",
            "32": "- It seems to me that 4.5 is about composition function *and* \\delta h functional form since LoRA changes both.",
            "33": "~~~~~~~~\n\nRevised score upwards after response, see below"
        },
        "XbEUCnnma0e": {
            "0": "Strengths:\n- A new perspective of parameter-efficient methods within a unified framework.",
            "1": "- A number of controlled studies which give insights into which components are meaningful and in which cases.",
            "2": "- The released code base can help boost future research in this area.",
            "3": "- Clear and well-written.",
            "4": "Weaknesses:\n- No major weaknesses identified.",
            "5": "Questions:\n- It would be interesting to see the variance observed when applying parameter-efficient methods.",
            "6": "Previous work found large variance in downstream performance across different seeds when performing full fine-tuning [1,2,3].",
            "7": "This is something you could add, for example, to Table 2.",
            "8": "Typos:\n- In Figure 2, the performance of Adapter is 21.00, not 20.46\n- The \"v\" subscript is missing in Eq.",
            "9": "5\n\n[1] Dodge et al.",
            "10": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping.",
            "11": "2021\n[2] Mosbach et al.",
            "12": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines.",
            "13": "ICLR 2021\n[3] Bugliarello et al.",
            "14": "Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs.",
            "15": "TACL 2021"
        },
        "PBPjBdrsEve": {
            "0": "Strengths:\n\n1.",
            "1": "The analysis and conclusion on Adapters, Prefix Tuning, and LoRA are very penetrating.",
            "2": "Specifically, the paper derives an equivalent form of preﬁx tuning to establish its connection with adapters in a unified view.",
            "3": "2.",
            "4": "The paper proposes a uniﬁed framework for parameter-efﬁcent tuning that includes several state-of-the-art methods as instantiations.",
            "5": "3.",
            "6": "The paper is well organized and well-motivated with theoretical analysis and proof.",
            "7": "4.",
            "8": "Experiment setting and analysis are comprehensive; the proposed method Scaled PA consistently shows its advantages against the other baselines."
        }
    },
    "figzpGMrdD": {
        "zhqKTyU59e": {
            "0": "## Strenghts:\n\nThe analysis of PLM is carried out over different continual learning techniques, NLP tasks, PLM variants and finally by layer-wise probing analysis.",
            "1": "Understanding the strengths and weaknesses of each PLM is very desirable research progress.",
            "2": "The authors also provide new research questions that arise from the analysis and point to interesting unsolved research challenges.",
            "3": "Evaluation starts off with the expected lower and upper bound of performance, and then moves on to disentangle FWT and BWT (backward, forward performance) when using various CL techniques.",
            "4": "The chosen data sets are not well behaved, i.e.",
            "5": "experience imbalances, which makes the results more realistic (less artificial).",
            "6": "Overall, this study provides a necessary step towards exploring future continual learning methodology and explores many important factors on eight pages.",
            "7": "From the batch-learning adaptation literature on PLMs one may expect baselines such as various adapter block versions or 'anti forgetting hacks', but it is understandable that the authors did not test these, since adapters would likely introduce complexity per increment and quickly become impractical.",
            "8": "As the authors mention, CL specific future extensions to adapters are conceivable, but a work of their own.",
            "9": "### Minor weaknesses (easily fixable suggestions for improvement -- 1 content page left for improvements):\n\n- Some plots seem to be a very small, and may be enlarged to use the 9th page.",
            "10": "- Fig 2 plots should share a larger model legend (on the figure top or bottom), so the bars can become wider and easier to distinguish\n- Fig 2 color could be made more distinguishable, especially since the plots are narrow.",
            "11": "- Tab 1 could underline the best non joint performance — makes it easier to glance\n- sec 4.",
            "12": "That Transformer layers, except the classification layer, do not adapt much during fine-tuning, is a known result (hence assumptions 1 and 2 in the paper), which should be cited — see BERTOLOGY Primer by Anna Rogers for references.",
            "13": "Here intermediate layers are shown to forget as well, so this is a nice new finding, that can be contrasted.",
            "14": "- Fig 3: I assume the figure color-scale is probe performance?",
            "15": "Also, the buffer size, e.g.",
            "16": "er-200 should be explained with an example.",
            "17": "Is it 200 er samples, 200 samples/ class?",
            "18": "In section 5.1 it should be (re-)mentioned what the buffer size means.",
            "19": "### Spelling errors (not all listed): using a TTS app/function makes it easy to find these\n\n- sec 2.3.",
            "20": "\"eg\".",
            "21": "- sec 3.2:\n    - \"could be find\" ... can be found\n    - tendency to forgetting ... to forget/ towards forgetting\n    - reach the highest .. reaches\n- 4\n    - interferes ... with the PLMs ability to retrain _ representations\n- 4.1\n    - Joint ... Joint multi-task training\n    - from sketch ... from scratch\n- 4.2.",
            "22": "- methods making sense ... make sense\n    - The sentence \"(2) The classification layer clf is typically the most fragile of BERT, where continual learning learning methods making sense.\"",
            "23": "is not understandable.",
            "24": "## Questions to the authors:\nNone, regarding clarity."
        },
        "v17qRS6HB4h": {
            "0": "Strengths:\nOverall the study is very thorough covering both the correct range of options for each axis studied and a set of relevant cross-axis multiple variable experiments.",
            "1": "The organization is good (but not perfect, see below), its strengths are that the different options considered along each axis are clearly laid out ahead of time, with the exception of the continual learning strategies.",
            "2": "The layer-wise analysis in particular is interesting and tells a coherent story, despite the challenges of displayed complicated 3D data.",
            "3": "Overall, it seems that recently many studies compare quantitatively against multiple PLMs, which ultimately appear similar due to only slightly different performance numbers.",
            "4": "This study's most successful contribution in my opinion is an exploration of the qualitative differences among PLMs in the continual learning setting.",
            "5": "Weaknesses:\n- It would be really great to see how the insights after analysis can be used to improve performance.",
            "6": "It's probably not absolutely required given the focus on probeing, but it would go a long way towards validating the insights.",
            "7": "- Adjusting the numbers to achieve the 5/4/3/2 cuteness gets slightly in the way of understanding, unfortunately.",
            "8": "The main reason for this is that the \"veins of CL methods\" has a different number over the course of the paper, which makes it hard to identify when a given list of N things is a list of the \"veins of CL methods\".",
            "9": "Specifically, in the abstract and intro this number is 4, in section 2.3 this number is 3, in section 3.1 this number is 6, in Table 1 this number is 5 (two different sets of 5), in Figure 1 this number is 6, and in Figure 2 this number is 4.",
            "10": "For a paper that has so much going on and so many different lists of different sizes, keeping these consistent would make it much easier for the reader to understand at any point what exactly this given list of N items is referring to.",
            "11": "Along the same lines, it would help to be consistent with the language around each set of N things.",
            "12": "For example, the \"veins of CL methods\" are called at least \"veins\", \"schemes,\" and \"approaches\" at different points.",
            "13": "- Section 3.2, Table 1, and Figure 1 are relatively weak in my opinion.",
            "14": "What am I supposed to conclude?",
            "15": "I can look at the table and see the different results, but so what?",
            "16": "What should I be drawing my eye to in the table (bold would help)?",
            "17": "The Figure here is too small to even attempt to parse.",
            "18": "- It would be very helpful to have a sentence that gives intuition about what's being measured with the accuracy metric.",
            "19": "The definition is there, but it took me a second to realize that the intuitive idea is that it's measuring the accuracy of past tasks after the model has moved on to learning new tasks."
        },
        "fKlyAPB6drG": {
            "0": "Although the authors have conducted quite a lot of experiments, the phenomena shown in experiment results is hardly surprising to me.",
            "1": "It is not surprising that the pre-trained language models would have forgetting issues when fine-tuned on downstream tasks.",
            "2": "It is also not surprising that rehearsal-based methods perform the best for pre-trained models.",
            "3": "Moreover, the paper draws a conclusion that BERT is the most robust one and is a good option if a continual learning process is going to be conducted.",
            "4": "Based on this, the authors provide a few analyses on BERT’s ‘secret’ for continual learning.",
            "5": "However, compared with other pre-trained models, I don’t see that BERT is significantly better than others given the figures and tables.",
            "6": "I feel from the figures and tables, BERT and other models look similar.",
            "7": "The authors didn’t give a comprehensive explanation on how they read such information or a concrete quantitative comparison to support this claim."
        },
        "Bj8K6YPwt9J": {
            "0": "I think a comparative study paper should suffice at least two conditions to be considered for a publication at a venue like ICLR.",
            "1": "First, it should present a novel view on the problem, and second, it should draw a novel conclusion out of the experiments.",
            "2": "Although the paper could be a good survey for readers who want to learn about continual learning, I think its viewpoint is not new and its conclusion is not surprising.",
            "3": "While it is helpful to know that rehearsal works better than regularization in most datasets, this is not entirely a surprising result.",
            "4": "I think it is a common belief that rehearsal-based is more robust against catastrophic forgetting, while regularization-method is more space-efficient in that it doesn't have to store examples.",
            "5": "The fact that the last layer suffers from catastrophic forgetting is also not a surprising result, given that the lower layers are known to encode linguistic features and the upper layers encode task-specific features."
        }
    },
    "41e9o6cQPj": {
        "D_2GFBoFtOe": {
            "0": "Strengths:\n1.",
            "1": "The idea of multi-layer fusion of two modality through additional interaction token and node is interesting.",
            "2": "2.",
            "3": "The model achieves good results on three multi-choice question answering datasets.",
            "4": "3.",
            "5": "The ablation study shows some interesting finds such as the importance weight sharing in Mint layers.",
            "6": "Weakness:\n1.",
            "7": "The technical contribution is somewhat small: except the multi-layer fusion part, others are very similar to QA-GNN including the GNN model.",
            "8": "2.",
            "9": "As the main contribution is about the modality interaction layer, more analysis should be made such as the effects of number of fusion layers $M$, why only perform interaction through one single embedding (interaction token and node) instead of directly applying cross attention through all the tokens and nodes?",
            "10": "There are also some other ways to perform embedding interaction such as the one in ERNIE [1] Figure 2 (b).",
            "11": "To answer these questions, both intuitive analysis and empirical results are needed.",
            "12": "Additional Reviews:\n1.",
            "13": "Since this paper only focuses on question answering tasks, I suggest adding \"for question answering\" into the title similar to previous works QA-GNN and MHGRN to be more accurate.",
            "14": "2.",
            "15": "There are also some related but missing references such as GLM[1], JAKET[2], CoLAKE[3].",
            "16": "Although they focus on language model pre-training, the idea of combining graph reasoning and language modeling is related.",
            "17": "[1] Zhang Z, Han X, Liu Z, et al.",
            "18": "ERNIE: Enhanced language representation with informative entities[J].",
            "19": "arXiv preprint arXiv:1905.07129, 2019.",
            "20": "[2] Shen T, Mao Y, He P, et al.",
            "21": "Exploiting structured knowledge in text via graph-guided representation learning[J].",
            "22": "arXiv preprint arXiv:2004.14224, 2020.",
            "23": "[3] Yu D, Zhu C, Yang Y, et al.",
            "24": "Jaket: Joint pre-training of knowledge graph and language understanding[J].",
            "25": "arXiv preprint arXiv:2010.00796, 2020.",
            "26": "[4] Sun T, Shao Y, Qiu X, et al.",
            "27": "Colake: Contextualized language and knowledge embedding[J].",
            "28": "arXiv preprint arXiv:2010.00309, 2020."
        },
        "haPRgiteuFE": {
            "0": "Strength:\n- A novel architecture to enable deeper interaction between LM and GCN.",
            "1": "- Clear explanation of the proposed model.",
            "2": "- Extensive ablation studies to demonstrate the importance of modality interaction layer \n- selection, parameter sharing, graph connectivity, and parameter initialization.",
            "3": "-  Consistent performance improvement over the baseline models on all of the three QA benchmarks.",
            "4": "Weakness:\n- paper title is misleading, not directly related to LM\n- no citation and description for the baseline method T5+KB (Table 4)\n- As shown in Table 7, the proposed method is very sensitive to many factors.",
            "5": "The performance drops significantly when we change any of them.",
            "6": "- The data preprocessing and training steps are complex.",
            "7": "Questions:\n- According to the parameters presented in Table 8, the knowledge from LM and GNN are only fused at the last 5 layers (parameter M) when 24-layer LMs are used, and at the last 3 layers when 12-layer LMs are used.",
            "8": "It may be useful to show how the performance changes when using different M.\n- According to [2], the CommonsenseQA IH-dev set contains 1,221 questions in total.",
            "9": "May I know many questions are in each data split shown in Table 5?",
            "10": "- Is there any case that no entities are found during KG retrieval (Sec.",
            "11": "3.1)?",
            "12": "If there are any, how do you handle these cases?",
            "13": "What’s the percentage of such cases.",
            "14": "Reference:\n[1] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec.",
            "15": "QA- GNN: Reasoning with language models and knowledge graphs for question answering.",
            "16": "ArXiv, abs/2104.06378, 2021.",
            "17": "[2] Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren.",
            "18": "Kagnet: Knowledge-aware graph networks for commonsense reasoning.",
            "19": "In Empirical Methods in Natural Language Processing (EMNLP), 2019."
        },
        "Q7zsr3-rl5e": {
            "0": "Overall I like this paper since the method is simple and the results are good.",
            "1": "I have only one problem: the initialization of LMs.",
            "2": "The authors say that they initialize different LMs for different datasets to show that the method is agnostic to them.",
            "3": "I am not convinced by this argument.",
            "4": "A better way to do this would be to experiment with different LMs on one dataset to show that the method is agnostic to it and then use the best one for the other dataset(s) (Note: I understand that this would not apply to the biomedical dataset).",
            "5": "Other than this, I enjoyed reading the paper and liked the analysis and the detailed ablations.",
            "6": "I am willing to increase my rating if the above concern is resolved.",
            "7": "EDIT: changed my rating after the author response."
        },
        "pj_9cVJxAc": {
            "0": "Strengths of the paper: \n\n1.",
            "1": "The paper presents a novel way of combining information from text and a KB in a bidirectional way.",
            "2": "2.",
            "3": "The results presented in the paper show strong gains against baseline methods on 3 different datasets.",
            "4": "3.",
            "5": "Ablation studies show that the model achieves good performance on more complex questions.",
            "6": "Weaknesses, suggested improvements and requested clarifications\n\n1.",
            "7": "Regarding preposition phrases as a proxy for complexity: since the hypothesis is that the more the number of prepositional phrases in a question, the harder it is to answer.",
            "8": "But from Table 5., the trend of performance seems to increase with the increased prepositional phrases (with 84.7 being the max for 4 PPs).",
            "9": "It would be good if the authors could provide some discussion around this observation.",
            "10": "Additionally, it would be good if the authors could report the number of examples (either raw number or as a fraction of the total dev set) for each of the categories: having that would help draw better conclusions.",
            "11": "2.",
            "12": "The paper mentions that the entity extraction was done following Yasunaga et al.",
            "13": "Was relevance scoring was done for the entities ?",
            "14": "3.",
            "15": "Alongside with qualitative analysis, some quantitative analysis would be good to show what the model learns.",
            "16": "Specifically, having the BFS analysis of the attention weights as a function of different GreaseLM layers (as done by Yasunaga et al.)",
            "17": "can help demonstrate how having the bidirectional context information flow helps improve reasoning.",
            "18": "Additionally, showing this for negations and / or examples which GreaseLM gets correct but QA-GNN does not (and vice-versa) can shed some light on what the model improves on (and what are the limitations).",
            "19": "4.",
            "20": "Having an ablation on the number of GreaseLM layers would also be quite useful to answer if performance improves with more GreaseLM layers, are there diminishing returns or do we need just a few GreaseLM layers, beyond which it is detrimental to the model's performance.",
            "21": "5.",
            "22": "The Graph connectivity ablation states that connecting the e_int node to all entities (instead of just the input text entities) hurts performance.",
            "23": "It would be good if the authors could provide some intuition / insight as to why that might be the case.",
            "24": "Presentation Suggestions:\n\n1.",
            "25": "Page 5, line 1: should \\tilde{e}^{(l-1)} be \\tilde{e}^{l} instead ?",
            "26": "2.",
            "27": "Page 5, Equations (6, 7, 8): should e^{l}_{s} and e^{l}_{j} be e^{(l-1)}_{s} and e^{(l-1)}_{j} respectively ?",
            "28": "Note to the authors:\n\nThere is a contemporaneous work submitted to ICLR 2022 [GNN is a Counter?",
            "29": "Revisiting GNN for Question Answering](https://openreview.net/forum?id=hzmQ4wOnSb), whose hypothesis seems to be that by only using embeddings for node types and relation types, the models are able to attain good performance (86.67 acc on OpenBookQA) without needing any cross-modal information.",
            "30": "While I understand this is contemporaneous work, but since the work is so relevant to this paper and seems to directly contradict the premise of this paper, it might be good to have a short discussion on this (just a suggestion)."
        }
    },
    "8f95ajHrIFc": {
        "uhcvlAeWnEf": {
            "0": "The idea to use a baseline to stabilize training in DM is rather simple but quite efficient as demonstrated by the experiments, and I think this is an overall useful, if somewhat incremental contribution.",
            "1": "That being said is not quite clear to me that there isn't a simpler, more straightforward way to reduce variance in DM (see my full review for details).",
            "2": "Overall I think that the paper would be interesting to the community and I recommend acceptance, however I set my score as \"borderline\" until the concerns I raised in my review are addressed by the authors.",
            "3": "Strengths:\n- Fine-tuning pre-trained language models with distributional preferences is an important research topic\n- Simple, yet clever use of baselines.",
            "4": "- Convincing experimental results\n\nWeaknesses:\n- Some of the results of the paper (esp.",
            "5": "section 3.2 on interpreting REINFORCE w/ KL control) are not particularly insightful as they don't really inform any modeling decision (as opposed to the \"parametric reward\" idea which suggests using variance reduction techniques from RL to apply to DM).",
            "6": "What's more, I think that saying \"KL-control developed in the RM paradigm can also be construed as belonging to DM\" is slightly misleading because DM's core advantage is not so much in that it minimizes a KL but presumably that it minimizes the KL *in a specific direction*.",
            "7": "- Missing baseline for DM?",
            "8": "It seems to me like optimizing the DM objective by actually sampling from the target distribution $p(x)$ (by eg.",
            "9": "sampling from a large unlabeled dataset to get samples from $a$ and reweighting/subsampling according to $P(x)$) would be a more natural approach than first sampling from the model, and then reweighting with the likelihood ratio.",
            "10": "The reason I think this is worth using as a baseline is that the variance issue the paper tries to address arises precisely because the DM objective is estimated by sampling from the model rather than sampling from the data.",
            "11": "If we need RL variance reduction techniques solely because we are formulating a straightforward log-likelihood/cross-entropy objective as an RL objective, this begets the question: why formulate DM as RL in the first place when it doesn't need to be?",
            "12": "I hope the authors can clarify this point.",
            "13": "- Results are reported exclusively as curves.",
            "14": "I think that at least the core results should be reported as a table with a single number (with standard deviation) for each method (and each metric).",
            "15": "It is also not entirely clear to me if the curves are training curves, or whether they represent quantities computed on a test/validation set."
        },
        "39WASszd0OT": {
            "0": "Strengths:\n\n- presents novel baselines for DPG, which improve optimization performance.",
            "1": "Weaknesses:\n\n- The connection between importance sampling based distillation and REINFORCE was introduced in in the DPG paper, and from this perspective, adding a baseline is a straightforward exercise, and lower in novelty.",
            "2": "- While baselines improve the DPG objective, constraint satisfaction, as shown by the second subfigure in figure 3, is NOT improved, contrary to claims in the abstract and main body of the paper.",
            "3": "- With lower objective and equal constraint satisfaction, text generated by $\\pi$ may nevertheless be better.",
            "4": "Are the improvements evident?",
            "5": "No test results, quantitative or qualitative, are given on an application task to demonstrate this (the Khalifa 2021 paper, which utilizes the same technique less baselines, shows improved constraint satisfaction by using DPG-based DM, which is the point of utilizing DPG since EBMs are more difficult to sample)\n- More generally, it is not clear to me that the DPG approach (baseline or not) is highly effective.",
            "6": "The constraints in the EBM are NOT satisfied in either case, it would be great to investigate this in detail and figure out why.",
            "7": "- Related, the importance sampling step in DPG assumes that q(x)>0 where p(x)>0 to make the equality in (5) true, although this in the current context feels like a minor limitation.",
            "8": "- Based on Algorithm 1, it seems that the DPG importance weights are not normalized, which is the most common way to reduce variance (at the expense of bias) in IS.",
            "9": "Is this the case?"
        },
        "-uW_A2HJsw8": {
            "0": "* Strength\n    * This paper proposes an interesting idea and interpretation to connect RM and DM paradigm\n    * This paper proposes a variance reduction method for DPG which demonstrates its improvement on performance, stability and sample efficiency\n* Weakness\n    * From my understanding, the baseline mostly comes from the observation in 3.3, which has limited technical novelty.",
            "1": "It will be great if authors can justify more on the technical novelty.",
            "2": "* It will be cool to show some results on BLEU improvement on machine translation, or user studies on language generation tasks, to demonstrate its practical impact, as the quantitative metrics right now are still kind of artificial."
        },
        "ovcEGb4wFBL": {
            "0": "While I think it's nice to analyze the connection between RM and DM, the math provided in the paper is simple, and the main contribution is just to add a baseline to the algorithm of Khalifa et al.",
            "1": "I encourage the authors to try out ideas they mentioned as future work to have a substantial contribution for a conference publication.",
            "2": "Typos: In Sec 5.2 Tasks (f), \"$\\phi_2(x)=1$\" should be \"$\\bar \\mu_2=1$\".",
            "3": "In Sec 5.5 Gradient Variance, $\\pi$ is missing in \"$G_\\theta(x)=A(x)\\nabla_\\theta \\log_{\\theta}(x)$\""
        }
    },
    "TXqemS7XEH": {
        "TzmfrPl9P6": {
            "0": "Strengths:\n\n1.",
            "1": "The proposal training strategy is novel and interesting, and has the potential to become the standard practice for training transformers.",
            "2": "I personally will certainly try it for my next large language model training.",
            "3": "2.",
            "4": "The proposed strategy is simple and straightforward to implement.",
            "5": "3.",
            "6": "Experiments are conducted on real systems and on real datasets.",
            "7": "Weakness or areas of improvements.",
            "8": "In general, this paper will be greatly improved with a better experiment design.",
            "9": "1.",
            "10": "It's unclear whether the M6-1B model with longer training time in Table 2 was started from P2R strategy or from the Real Run.",
            "11": "Actually it's better to list the results from both to study how P2R affects the long term convergence of the model.",
            "12": "I'm interested in see what happens to Figure 2 after 7k mins of training time until convergence.",
            "13": "Will M6-1B (P2R) perform worse than M6-1B (Real) at convergence?",
            "14": "2.",
            "15": "It' unclear to me what speed-accuracy was achieved by the proposed P2R strategy from the experiments section.",
            "16": "There are a lot of unanswered questions such as: what's the budget set in table 2?",
            "17": "How are the downstream task performance (few-shot) changed with the compute budget?",
            "18": "How much quality loss is introduced by the 5x saving in the compute cost during the pseudo phase?",
            "19": "I would suggest the authors conduct more thorough studies to help readers understand the pros and cons of P2R.",
            "20": "These additional experiments can be done with little additional training cost as P2R also applied to smaller models as long as the stacking layers are identical in structure.",
            "21": "For example,  I would expect that  a larger model (like Transformer-Big) trained with P2R will have less training time but worse performance than training from scratch (like Transformer-Big Real) , but still outperforms the base model (like Transformer-Base) in model quality with compatible training cost.",
            "22": "Something like the table below:\n\nModel &  Training time &.",
            "23": "pplx & one-shot downstream performance.",
            "24": "Transformer-Base (Real) &.",
            "25": "3  & 2.5 &  40\\\\\n\nTransformer-Big (P2R) &.",
            "26": "3 &  2.2 & 45 \\\\\n\nTransformer-Big (Real) &.",
            "27": "10  & 2.1 & 50 \\\\\n\n3.",
            "28": "In Table 2, M6-1B's Gigaword performance is worse than M6 with smaller scale, which is unexpected.",
            "29": "I disagreed with the author's conclusion that the worse performance is not related to the training strategy.",
            "30": "4.",
            "31": "I think the significance of the proposed strategy to large language models is actually limited.",
            "32": "First, it's not going to save the serving cost.",
            "33": "Second, the amount of  time saving from the P2R approach might be small compared to the overall convergence time, especially for the 10T scale.",
            "34": "It will be more interesting to see how this approach works with regular size transformer in various applications."
        },
        "0Xwo6-HxA__": {
            "0": "The paper tackles an important problem of improving the efficiency (time and resource) of training massive models, as these models rapidly scale in size to achieve greater performance.",
            "1": "P2R seems to be a promising approach that aligns with the intuition that the structure of the model (and data) for effective learning could be dynamic throughout the training process.",
            "2": "I believe this intuition is the basis of successful schemes such as pre-training+finetuning, lottery tickets hypothesis, etc.",
            "3": "Two major weakness of the paper are:\n1.",
            "4": "Does not demonstrate P2R for M6-10T, the 10T parameter model, since no downstream task results are presented for this model in the evaluation section.",
            "5": "Similarly, the claim of 10 days pre-training remains unsupported by the draft.",
            "6": "2.",
            "7": "The parameter sharing approach seems arbitrary and un-insightful.",
            "8": "Why have all the layers share 1 set of parameters, which not 2 or 4, or something fewer than number of layers?"
        },
        "BM-CWmNh_Ox": {
            "0": "The proposed approach makes sense.",
            "1": "However, I have the following major concerns:\n\n1.",
            "2": "The 10-trillion parameter model (M6-10T P2R) might be under-trained.",
            "3": "It is hard to tell if training a relatively smaller model (e.g, 100 billion parameter) on the same number of GPU hours can give better numbers.",
            "4": "I believe a larger model can outperform smaller models only if it's trained enough.",
            "5": "The authors advocate for green AI, and train a 10-trillion parameter model.",
            "6": "Thus, the main point should be the 10-trillion parameter model uses fewer GPU hours to get the same quality.",
            "7": "I do not see such evidences.",
            "8": "* In the \"Convergence Analysis\" paragraph and Figure 4, the authors showed that M6-T converges faster than M6-T in terms of the same number of seen examples.",
            "9": "But I think the more important question is, does M6-T achieve better numbers than M6-T given the same GPU hours yet.",
            "10": "* In Table 2, how long does \"training for long\" mean?",
            "11": "Additionally, if there is only \"limited budget\", the important question is should we train M6-1B (P2R), or a smaller model for longer.",
            "12": "2.",
            "13": "The pretraining dataset is very small compared to the scale of the network.",
            "14": "Practically speaking, training a multi-billion parameter model requires more data than BookCorpus and English Wikipedia, let alone a multi-trillion parameter model.",
            "15": "The experimental results would be more convincing if the authors used larger datasets.",
            "16": "3.",
            "17": "As the authors mentioned, we do not have results of finetuning the 10-trillion parameter model on downstream tasks.",
            "18": "We don't know how impactful the proposed method can be.",
            "19": "4.",
            "20": "The sharing-delinking paradigm is very similar to the sharing and unsharing method proposed in a previous ICLR submission: https://openreview.net/forum?id=jz7tDvX6XYR .",
            "21": "The authors should discuss this work, even if it's a resubmission by the same authors, since the claims and experiments are different."
        },
        "8owfgJA9naR": {
            "0": "**Pros:**\n1.",
            "1": "The paper focuses on training extremely large models with limited computational resources, which is of great empirical importance.",
            "2": "2.",
            "3": "The proposed P2R method is easy to implement and can be adapted to different models (as long as there is a decent number of repeated layers, whose weights can be shared).",
            "4": "3.",
            "5": "The efficacy of P2R is empirically verified.",
            "6": "**Cons:**\n1.",
            "7": "*Insufficient related work discussion on fast training methods*.",
            "8": "There is a large volume of works on fast training of deep learning models, and the notion of \"weight sharing\" was also explored previously.",
            "9": "For instance, \"Trellis networks for sequence modeling\", \"Speeding up Deep Model Training by Sharing Weights and Then Unsharing\" show that sharing weights leads to faster convergence.",
            "10": "Other works that train a smaller model first and then grow the model into a large one can also sometimes be recast as sharing and delinking (for example, \"Net2Net: Accelerating Learning via Knowledge Transfer\"; \"Multi-level Residual Networks from Dynamical Systems View\"; \"Efficient Training of BERT by Progressively Stacking\").",
            "11": "2.",
            "12": "*Need a more comprehensive empirical evaluation*.",
            "13": "Given that the main contribution of this paper is to propose a novel empirical training method, it needs more thorough evaluations.",
            "14": "In particular, I think it would be good to have experiments covering: a) various models.",
            "15": "Only M6 models are trained in this paper.",
            "16": "The scale of M6-10T is promising, but it would be good to include other models which also have repeated layers.",
            "17": "b) various downstream tasks.",
            "18": "Evaluations only on WikiText-103 and Gigaword are somewhat limited.",
            "19": "c) various domains.",
            "20": "There are models commonly used in other domains (e.g., image classification) that have repeated layers.",
            "21": "Does the proposed method work for them?",
            "22": "3.",
            "23": "*Clarifications needed for Section 4.3*.",
            "24": "In the training efficiency, the M6-10T with P2R is trained for around 10 days and another M6-10T is trained without P2R for around 3 days for comparison.",
            "25": "Why is P2R training time longer than without P2R?",
            "26": "Figure 4(a) plots the Log Perplexity v.s.",
            "27": "clock time and the conclusion is \"P2R can outperform the one trained from scratch by a large margin\".",
            "28": "I feel this claim is not fully supported as the M6-10T does not converge yet.",
            "29": "Give both more time, will they converge to a similar performance?",
            "30": "Figure 4(b) compares M6-10T P2R v.s.",
            "31": "M6-T on sample-basis.",
            "32": "Why the baseline is changed here and how does M6-10T P2R compare to M6-10 Real on a sample basis?",
            "33": "Further, both do not seem to converge given 10M samples.",
            "34": "How will the curve look like when giving more training samples (e.g., 20M)?",
            "35": "It also confuses me a little as the M6-10T P2R curves in 4(a) and 4(b) do not look the same - I'm confused because I think they should only differ by some scaling in the x-axis (changing from training time to samples trained)?",
            "36": "**Questions:**\n\n1.",
            "37": "For Table 1, the speed difference between Pseudo and Real is huge.",
            "38": "What leads to the acceleration?",
            "39": "Is that primarily coming from the GPUs' communication overhead?",
            "40": "It seems that both Pseudo and Real are trained on 48 GPU devices according to the description, then they should have similar communication overhead?",
            "41": "2.",
            "42": "For Figure 4, when does the \"delink\" happen?",
            "43": "How frequently does P2R check for switching, and is that time included in Figure 4(a)?",
            "44": "3.",
            "45": "After delink, the \"Real Giant\" will require the same amount of computational resources as training the model in the standard way?",
            "46": "Is the computational saving then only appears in the \"Pseudo Giant\" stage?",
            "47": "If so, what is the typical training time spent in the \"Pseudo\" and \"Real\" stages, respectively?",
            "48": "**Minor Comments:**\n\n1.",
            "49": "In the second paragraph of Section 3.2.1, it says \"the amount of gradients becomes 2/L of the original one\".",
            "50": "Why not 1/L?",
            "51": "2.",
            "52": "In the description of Table 1, $l$ should be $L$.",
            "53": "3.",
            "54": "It's probably better to use a larger fontsize for the numbers and legends in Figure 2 and 4."
        },
        "AKVMp0-eZ6j": {
            "0": "Strengths:\n* Overall, the paper is well-written and the contributions of authors are clearly explained.",
            "1": "* To my knowledge, this is the first work that leverages both expert sparsity and parameter offloading to train a model of unprecedented scale, which is an important engineering achievement if it was indeed executed succesfully.",
            "2": "Weaknesses:\n* The overall claim of pretaining a 10-trillion parameter model in 10 days is a bit misleading: first, the stopping criterion for pretraining is unclear, that is, it's not entirely clear whether the model has converged after just 15k steps and other models of this scale took significantly more steps for pretraining with even larger batches.",
            "3": "Second, there is no downstream evaluation for M6-10T or comparison with other models of comparable scale: it might have been possible to reimplement some of the prior work as baselines.",
            "4": "* Although pretraining with parameter sharing may indeed be more memory-efficient, the (seemingly) inherent disadvantage of P2R is that in the second stage of training, that efficiency is lost due to the unlinking.",
            "5": "As a result, the users of this methodology will still be forced to train the model of full size at some point, and the gains of P2R are not fully quantified.",
            "6": "For instance, why not just pretrain the model with parameter sharing for the entire duration of the experiment, as in the original ALBERT paper?",
            "7": "Moreover, even if the wall-time convergence of P2R is much faster, how does Pseudo pretraining compare with Real in terms of iteration convergence?",
            "8": "* The pretraining objectives, such as text denoising and image-based text denoising, are not described in detail.",
            "9": "Authors should provide the code of the objectives, describe it in the paper or at least give the exact reference to prior work, because there exist several different objectives which fall into this category.",
            "10": "For instance, it is unclear how the resulting model is applied to the language modeling and summarization tasks, because in some definitions of denoising pretraining, the model actually \"sees\" the entire input sequence and thus does not learn to generate it in an autoregressive manner.",
            "11": "* In Section 3.2.3, authors note that in order to determine the moment of pseudo-to-real switching, they need to repeatedly attempt to swiitch at different training steps and measure the difference in loss between continuing to train with shared parameters and training with unlinked ones.",
            "12": "This strategy seems to highly increase the computation load of the entire procedure, unless in practice the intervals between attempts and the evaluation period are very conservative: the exact values for the main experiment as well as the number of \"unsuccesful\" attempts are not given in the paper, and I believe that this issue needs to be discussed in more detail.",
            "13": "* Looking at the results in Table 1, it was quite surprising to observe a difference in throughputs of Pseudo and Real which is this significant.",
            "14": "Although the parameters are shared across layers, the computational requirements of both forward and backward passes should be approximately the same.",
            "15": "While there might be some additional inefficiencies connected with the use of pipeline parallelism, the work does not discuss or analyze this difference in performance; in the original ALBERT paper, authors have found training with shared parameters to be only 1.7 times faster than without sharing.",
            "16": "I believe that the comparison should be without offloading, as it is an orthogonal factor that needs to be evaluated separately.",
            "17": "* The granular offloading mechanism also needs to be compared both with ZeRO-Offload [1] and L2L [2] in terms of its efficiency.",
            "18": "As of now, it is not quite clear whether the bottleneck outlined by authors is indeed a problem: in [1], the authors attempt to address the communication bottleneck by overlapping it with computation and asynchronous optimization.",
            "19": "* Lastly, using idle GPU memory for storing extra layers means less training examples processed in a microbatch for pipelining or gradient accumulation, which may also reduce the efficiency of distributed training.",
            "20": "Currently, there is no discussion of this tradeoff, and this question might be important in practice, since currently the most popular approach is to train models of this scale on extremely large batches.",
            "21": "Questions and typos:\n* In Section 3.2.1, you say that \"the amount of gradient [when sharing parameters] becomes $2/L$ of the original one\"; if there are L times fewer parameters, shouldn't that be just $1/L$, the same factor which you arrived at for weights and optimizer states?",
            "22": "* For the M6-10T model, what datasets and pretraining objectives did you use?",
            "23": "Is it the same data and tasks as in Section 3.3.1?",
            "24": "* Small nitpick: the methods offered as a part of the DeepSpeed framework are grouped under the name of ZeRO, not ZERO.",
            "25": "[1] ZeRO-Offload: Democratizing Billion-Scale Model Training.",
            "26": "Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He\n[2] Training Large Neural Networks with Constant Memory using a New Execution Algorithm.",
            "27": "Bharadwaj Pudipeddi, Maral Mesmakhosroshahi, Jinwen Xi, Sujeeth Bharadwaj"
        }
    },
    "shpkpVXzo3h": {
        "O2d1TdTc3d": {
            "0": "This paper shows very impressive empirical results and has an open-sourced codebase which makes it reproducible.",
            "1": "Memory saving and time-saving are especially impressive.",
            "2": "These empirical results themselves are very valuable to the community given there are not many open-sourced codebases, to begin with.",
            "3": "The methodology of dynamic (tree) quantization and stable embedding are reasonable but not surprising to the quantization research community.",
            "4": "The authors modified the dynamic tree quantization, which also appears to be reasonably motivated.",
            "5": "Appendix E seems to support the argument despite there being few places where strong implicit assumptions were made:  mean of gradient zero over time, and constant variance.",
            "6": "These should be thoroughly discussed in the appendix as well.",
            "7": "Quesion:\n1.",
            "8": "Why image tasks do not have as much saving as NLP tasks?",
            "9": "In table 1, Image related tasks's savings are marginal versus NLP related tasks.",
            "10": "Is there a way to make this work for Image, speech task as well?",
            "11": "There are a few minor points that do not affect my score:\n- Several broken reference links throughout the text with ?",
            "12": "?",
            "13": "- Many attempts have been made on quantizing the optimizer states from 32-bit to 8-bit using non-linear quantization, but just not as successful maybe.",
            "14": "E.g.",
            "15": "<Sun et al.",
            "16": "2019, HFP8> <Pappalardo 2021, Brevitas><Li et al.",
            "17": "2020 End-to-end Quantized Training via Log-Barrier Extensions>  can do such jobs as well."
        },
        "fuLheAd9jKJ": {
            "0": "**Pros:**\n- The authors conducted a wide range of experiments on diverse NLP and computer vision tasks performing consistent memory footprint saving without performance degradation.",
            "1": "- The proposed method is stand-alone and potentially can be applied in parallel with other compression techniques such as weight, activation quantization, and pruning.",
            "2": "**Cons:**\n- The proposed method is beneficial more to neural networks with a high amount of the parameters proportionally to the activations thus for convolutional neural networks the memory saving ratio is much smaller rather for transformer-based NNs.",
            "3": "- There are several broken references that should be fixed"
        },
        "AujXwsKEuE": {
            "0": "```Updated Score```: See my comments why: https://openreview.net/forum?id=shpkpVXzo3h&noteId=PWzRo82xR07 and hopefully authors address the additional comments in the final version.",
            "1": "Paper proposes block-wise quantization (dynamic) to reduce the states (momentum and second moment) in diagonal first order optimizers and successfully implements a 8-bit Adam implementation that works as well as its f32 variant while being memory efficient.",
            "2": "Paper further identifies embedding layers as a source of instability and proposes layer norm to improve stability,  and leaves optimizer states in f32 for this layer.",
            "3": "Strengths:\n+ Efficient 8bit implementation of Adam on GPUs\n+ Careful work to reduce quantization errors, and improvement to existing algorithms (for example quantile estimation) \n+ Blockwise quantization is very neat way to deal with outliers\n\nWeakness/Improvements:\n+ Lack of comparison to previously established SOTA for low memory optimization (See comments on AdaFactor)\n+ Results are at similarish (lower) batch sizes.",
            "4": "It would have been interesting to see the method is applicable for large batch training.",
            "5": "One could conjecture that heavy tailed nature noise (https://arxiv.org/abs/1912.03194) and its associated effects on the diagonal preconditioner might make quantization harder.",
            "6": "+ Ignores AdaGrad line of work\n\n==AdaFactor comparisons== \n\nAdaFactor has the option to reduce the memory used by momentum states completely by replacing it with the cheaper adaptive gradient clipping, and has been used for large model training (see https://arxiv.org/pdf/2006.16668.pdf for hyper-parameters.",
            "7": "(See beta1=0))\n[1] https://github.com/tensorflow/lingvo/blob/master/lingvo/core/optimizer.py#L1044 \n\nThe paper includes the f32 variant, but do not include the bfloat16 variant [1] comparison\n[2] https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/adafactor.py See parameter encoding.",
            "8": "== AdaGrad and its variants == \nPaper currently misses citation of the entire AdaGrad-line of work, and comparision/implementation, I hope the authors can address these easily.",
            "9": "[1] https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf \n[2] Memory efficient adaptive optimization https://proceedings.neurips.cc/paper/2019/file/8f1fa0193ca2b5d2fa0695827d8270e9-Paper.pdf\n[3] https://openreview.net/pdf?id=SklKcRNYDH (ICLR 2020)\n\nDoes the proposed block quantization work well for AdaGrad variants as well?",
            "10": "Reason to ask is, after reading the paper, it's not entirely clear if the proposed approach works for AdaGrad, SM3 and Extreme Tensoring as it accumulates statistics over the entire horizon and has a very different range of values (distribution of values) than Adam's second-moment statistics.",
            "11": "It would be valuable to the community if authors could address this in the paper by demonstrating improvements or showing negative results and challenges to this end.",
            "12": "For example, in this work authors leave compressing states for embedding layers (with layernorm) as future work.",
            "13": "== Some related citations for block wise quantization ==\nFor weights of a network; similar idea of block-wise quantization (but for binarization) has been employed in \"Improving Bi-Real Net with block-wise quantization and multiple-steps binarization on activation\", Duy H. Le; Tuan V. Pham"
        }
    },
    "057dxuWpfx": {
        "KAkBp7l0XjZ": {
            "0": "## Summary\n\nI really liked this paper.",
            "1": "There are a bunch of typos /repeated words, such as 'allow allow', but I felt the structure and the paper was good, the pacing worked well for me, the explanations were clear.",
            "2": "I did have a slight hiccup over the use of both PPO and Gumbel, which I eventually realized is because the sender-receiver network as a unit has to operate in an environment which itself gives a stochastic reward, so a small diagram illustrating the relationship between the sender agent, the receiver agent, and the environment/reward might faciltitate this point slightly.",
            "3": "I felt a little odd that the paper both introduces PPO as a way to handle stochastic reward, I mean, relative to other emergent communication papers, and then draws conclusions based only on using PPO.",
            "4": "But PPO is a common approach to stochastically rewarding environments, and the conclusions, experimental, and theoretical work, seemed solid to me.",
            "5": "I felt the analogy between ECRP and PPO was slightly stretched to me, but I could more or less see the connection betweeen the two.",
            "6": "I think the point I felt least confortable with was comparing positively rewarded episodes in the PPO buffer with the individual 1/beta samples in ECRP.",
            "7": "I am unfamiliar with PPO, and would have appreciated some clarification why we are only considering positively-rewarded episodes in this comparison.",
            "8": "Does PPO only store positively rewarded episodes in the buffer?",
            "9": "Or are considering that only the positively-rewarded episodes will modify the weights, in a particular direction?",
            "10": "So, some clarification on your exact thinking on this point, and exactly how PPO works, on this point, would be appreciated please.",
            "11": "## Good points\n\n- use of PPO in emergent communication is new\n- empirical exploration of effect of reward shaping on the entropy of the emergent language is new\n- theoretical analysis of why the reward shaping affects the entropy of the emergent language is new\n\n## Bad points\n\n- a few typos/repeated words (e.g.",
            "12": "'allow allow')\n- could do with a diagram showing the relationship between the sender model, the receiver model, and the task/reward (and in particular how Gumbel and PPO fit into this)\n\n## Questions for author\n\nNone.",
            "13": "The work seemed rather complete to me.",
            "14": "## Notes\n\nNotes I made whilst reading the paper:\n\n### Abstract\n\nby the end of the abstract I'm excited to see what the paper is going to show.",
            "15": "### Introduction\n\nseems rather specialized (experience buffer size), but if it does provide both empirical evidence and theory, then, ok :)\n\n### 2.1 taxonomy\n\nconcept and analysis of 'trivially optimal language' is interesting\n\ntable 1: nit: can we make it so we have both columns to be 'yes', rather than one aiming for 'no' and one for 'yes'.",
            "16": "- maybe a way of phrasing trivial in the other way around.",
            "17": "('non-trivial'?",
            "18": ":) )"
        },
        "PTdnPdzdnu2": {
            "0": "My main issue with this paper is that the core contributions may not be particularly useful to people working in emergent communication for different environments and tasks.",
            "1": "The general claim that \"shaped rewards bias emergent language\" seems well known in the literature.",
            "2": "Many papers have been using shaped rewards in some form or another to try to bias the language towards certain results—usually the target is trying to increase the systematicity and compositionality of the language, e.g.",
            "3": "with auxiliary losses ([Chaabouni et al., 2019](https://arxiv.org/abs/1905.12561); [Luna et al., 2020](https://arxiv.org/abs/2004.03868); [Mihai and Hare, 2021](https://arxiv.org/abs/2106.02067), among many others).",
            "4": "This work does not pretend that the shaped rewards are not biasing the language—indeed, **the goal is to bias the language**, and the central question is *what shaped rewards lead to the kinds of qualitative language behavior we desire in emergent communication*.",
            "5": "The authors suggest that shaped rewards obscure our ability to study how emergent languages arise from \"basic principles.\"",
            "6": "It's not precisely clear to me what these \"basic principles\" are (every study likely has some sort of contentious design choice somewhere), but there also seems to be some consensus that languages developed with minimal guidance/shaping are successful, but are generally uninteresting and unhelpful (e.g.",
            "7": "[Kottur et al., 2017](https://arxiv.org/abs/1706.08502)), both for people interested in computational simulations of linguistic evolution and for those ultimately interested in connecting emergent languages to real ones.",
            "8": "The authors show that, for a simple multi-agent navigation environment, adding shaped rewards (i.e.",
            "9": "auxiliary signals) to the environment bias the emergent language.",
            "10": "Granted, since this environment is more slightly more complex than some previous work (insofar as it involves communication over multiple timesteps), so there may be some takeaways for researchers looking at emergent communication in multi-step settings.",
            "11": "Such shaped rewards look different than the kinds of regularizers and auxiliary losses used in non-multi-step games, and perhaps the PPO experience buffer observation will be useful for people who use such experience buffers in other studies.",
            "12": "Despite this, however, my main concern is that the analyses in this paper seem to have limited utility and applicability outside of the specific navigation environment explored.",
            "13": "It would have been good to see a wider variety of environments/tasks explored, including those that have already been explored in the literature.",
            "14": "Otherwise it's unclear what other researchers will get from this paper, unless they're using the exact same navigation task used in this paper (assuming that the insight that \"shaped rewards bias emergent language\" is already known to them, which I believe is fairly well-established).",
            "15": "I believe a more useful version of this paper would show how consistent kinds of shaped rewards (e.g.",
            "16": "denser rewards) consistently bias the language across a variety of tasks, thereby substantiating more general claims and considerations that may be useful to people studying multi-agent communication regardless of the environment.",
            "17": "# Strengths\n\n- Some insights into how auxiliary rewards and training parameters bias emergent communication, esp.",
            "18": "in RL, may be useful considerations for future researchers.",
            "19": "It does paint a worrying picture that something as simple as PPO experience buffer size has dramatic effects on the entropies of the learned languages, which calls into question how stable conclusions drawn from studies of RL communication can be.",
            "20": "Though it would be nice to see other environments where these hypotheses also hold, especially environments that have previously been used in multi-agent communication (of which there are now several RL environments have that been explored).",
            "21": "# Weaknesses\n\n- Limited evaluation—no other environments explored.",
            "22": "- The investigation of how shaped rewards change the entropy/semantics of the language is interesting, but seems limited to this environment (especially, for example, with world radii).",
            "23": "I don't think it's particularly surprising that shaping rewards change the dynamics of the language, particularly with respect to their sensitivity to features that are specific to the environment (e.g.",
            "24": "world radii).",
            "25": "In general we must watch out for confounding factors when modifying our environment across all studies in emergent communication.",
            "26": "It's not clear to me how the confounding factors in this environment are teaching us something new about how to approach emergent communication studies across all environments.",
            "27": "- I feel like the term \"basic principles\", \"first principles\" is thrown around a lot and it's unclear to me what precisely this means.",
            "28": "Some more time could be spent explaining what the authors mean, e.g.",
            "29": "end of section 2.2 p1, explanation of Bullard et al.",
            "30": "2021 in intro p1,.",
            "31": "In fact I expect many design choices made in previous studies on emergent communication argue that their particular reward/choice of game/etc are already \"first principles\".",
            "32": "- The sea star plots are confusing to me, and more time should be spent explaining them.",
            "33": "Why are there only 3-6 arms per plot if there are 64 bottleneck units?",
            "34": "Does this mean the teacher does not use the other units to communicate, or do the other units not have interpretable effects on the receiver's trajectory?",
            "35": "Does the receiver **always** take an action in a deterministic direction upon seeing?",
            "36": "Surely there's some spread/variance—in which case that should be evident in the graph somewhere.",
            "37": "Or rather, are all 6 sea star plots responses to different units in a single language?",
            "38": "But then the caption \"Each sea star corresponds to an independent language\" wouldn't be correct...\n- Writing seems incomplete in some places, with obvious omissions.",
            "39": "Figures should have captions.",
            "40": "Intro: \"there is a risk that the design choice simply mirrors\" repeated.",
            "41": "- \"We remove any models which do not [achieve a 100% success rate] from consideration.\"",
            "42": "Without any additional details, this seems highly suspect; ideally you'd find a setting where the agents consistently reach 100% success rate (else the reproducibility statement claim that \"the training process is stable\" is false); at a minimum, you should describe what proportion of agents fail to converge, and whether the languages in this discarded portion differ qualitatively from the included languages in a way that might bias the results examined here.",
            "43": "- I think some of the specifications in Table 1 are incorrect.",
            "44": "E.g.",
            "45": "Lazaridou 2018, task is considered \"trivial\" but Lazaridou et al.",
            "46": "also explore a real pixel reference game where the task is to communicate over real images.",
            "47": "Authors argue this task is trivial since one can learn the underlying features (of shape and color), but extracting these features is very much a non-trivial task.",
            "48": "In the same way, then, the Havrylov and Titov paper can also be considered trivial, in that one should just be able to learn the underlying MSCOCO objects that the dataset is sampled from and those objects.",
            "49": "- The organization of this paper seems off.",
            "50": "I feel like section 4 (the CRP) should be introduced around section 5.4, since it is completely ignored until then.",
            "51": "The relation between section 5.4 and the rest of the paper could also be made more clear—this investigation seems completely tangential to the question of how shaped rewards bias the environment, and the experiment is not mentioned in abstract.",
            "52": "- Figure 4: the connection between \"lower entropy at higher world radii\" needs to be better connected with the claim that \"agent pair often fails to learn a fully successful language at the highest world radii\".",
            "53": "What does \"fully successful language\" mean?",
            "54": "Don't agents get 100% task accuracy on this task---then why isn't the language \"successful\"?",
            "55": "- Following the prev.",
            "56": "question, in terms of actually exploring the emergent language, the only metric really investigated here is entropy.",
            "57": "In any experiment I could likely find some metric for the resulting language that changes wildly if I modify training hyperparam or environment reward.",
            "58": "Authors should therefore more clearly justify why entropy is the only relevant part of the story here.",
            "59": "Why is it good or bad that shaped rewards may lead to a language with higher entropy than is expected from \"basic principles\"?",
            "60": "There are a lot of other metrics, e.g.",
            "61": "related to compositionality ([Brighton and Kirby, 2006](https://pubmed.ncbi.nlm.nih.gov/16539767/)), or positive signaling/listening; ([Lowe et al., 2021](https://arxiv.org/abs/1903.05168)), that we might care about, and would be good to measure here.",
            "62": "Why should we be so concerned/worried about changes to our environment affecting the entropy of the resulting languages?",
            "63": "# Questions\n\n- What does it mean to \"emerge from basic principles?\"",
            "64": "- Are the experiments in Section 5 done in the edgeward environment, centerward environment, or both?",
            "65": "Are there qualitative differences in the behavior of both settings, and if not, what is the motivation for exploring both settings?",
            "66": "- If i'm understanding this correctly, It's interesting that the agent messages encode horizontal commands (Figure 3c) even though horizontal information is given in the biased reward.",
            "67": "I would expect that agents learning to encode vertical actions would help fill in the deficiencies of the environment reward.",
            "68": "Though presumably the vertical actions are harder to learn.",
            "69": "- How often does the sender send messages to the receiver - at every timestep, or just once at the beginning of each episode?",
            "70": "- Why is the PPO experience buffer experiment an \"alternative explanation\" for the differing entropies observed in the paper?",
            "71": "Couldn't experience buffer size and reward shaping affect entropy (or other language metrics) in orthogonal ways?",
            "72": "# Minor\n\n- 5.4 \"model presented in the next section\"...what next section?"
        },
        "pzX2ws-zrs3": {
            "0": "The question of how inductive biases can affect the solutions of emergent communication algorithms seems interesting and potentially impactful.",
            "1": "However, I think the current manuscript has too many fundamental issues that prevent it from making progress on this problem.",
            "2": "1.",
            "3": "The contributions are unclear.",
            "4": "The current study is conducted in a very informal manner---without a detailed problem description or an explanation of the empirical methodology---which makes it difficult to identify the main claims, understand the connection between the intended claims and the empirical conclusions, and ultimately leads me to question the correctness and significance of the whole study.",
            "5": "2.",
            "6": "It is unclear whether the presented data contains a positive result that would make a significant contribution to research community.",
            "7": "The experiment section presents data that seem mostly the same between baseline and comparator.",
            "8": "3.",
            "9": "The manuscript does not contain enough details to reproduce its results.",
            "10": "Several plots are presented in figures 3, 4, and 5 without a detailed explanation of how they were generated or what their implications may be.",
            "11": "It feels like readers are supposed to trust they are correct without question.",
            "12": "#### Questions\n* What are the specific research questions this work intends to address?",
            "13": "* How is the presented algorithm used in the experiments?",
            "14": "* What purpose does the Chinese Restaurant Process serve in this work?",
            "15": "* How exactly are rewards shaped in the experiments?",
            "16": "* How are the experiments supportive of the general claims about reward shaping and prior information that you wish to make?"
        },
        "2r7fw3hZNzR": {
            "0": "Strengths\n\nThe key contribution of the paper is the findings on the impacts of shaped rewards to the the emergent language:\n1. shaped rewards can explicitly bias the semantics of the learned language;\n2. shaped rewards change the entropy of the learned language;\n3. shaped rewards masks the potential effects of other environmental variables of interest.",
            "1": "Weaknesses\n\nThe findings are only confirmed in a specific setting, a simple sender-receiver navigation game with two specific bias rewards, Euclidean distance and horizontal distance.",
            "2": "I find the use of Chinese restaurant processes as an analogy not necessary and confusing.",
            "3": "In some places, the analogy breaks down as stated \"The restaurant analogy breaks down here as we would have to say that in each iteration, β customers simultaneously\nand independently make a decision ...\""
        },
        "2R1N5cf19Tn": {
            "0": "Strengths:\nThe paper is well written and motivated, the experiments in the paper are performed with rigor.",
            "1": "All the concepts are explained in the paper making the work easily understandable.",
            "2": "Weakness:\nThe argument for using CRP as a way to explain the RL behavior is not convincing for me, since in the former case the action is picking a table, and every action of picking a table that has already been picked leads to a \"reward\" but in the case of RL, the action doesn't directly correspond to rewards.",
            "3": "And the action and state space for both are different.",
            "4": "The analysis and the results obtained with the agent’s entropy and behavior differences can be said for RL problems in general and not just to emergent language learning.",
            "5": "I am not familiar if such analysis has been performed on RL in general.",
            "6": "Thus similar analysis on the effects could be carried out in RL in general.",
            "7": "The related work section also doesn't include literature on analyzing reward shaping [1] .",
            "8": "The paper has some minor grammatical mistakes that can be fixed with revisions.",
            "9": "The description of the model is confusing in the appendix, the author should use the standard method deep learning community describes a network.",
            "10": "[1] M. Grzes and D. Kudenko, \"Theoretical and Empirical Analysis of Reward Shaping in Reinforcement Learning,\" 2009 International Conference on Machine Learning and Applications, 2009, pp.",
            "11": "337-344, doi: 10.1109/ICMLA.2009.33."
        }
    },
    "sX3XaHwotOg": {
        "IbxMg1dDjlB": {
            "0": "Strengths:\n1) The paper is very well written and easy to follow.",
            "1": "The paper is well-placed w.r.t recent developments in pre-training by presenting similarities and differences with related work.",
            "2": "2) The idea of using a committee of generators to create the replaced token for training the discriminator, so as to provide a diverse signal for learning different complexity levels of RTD is natural and logical (this also can be linked with curriculum learning).",
            "3": "The neat trick utilized for modelling AMOS is to create multiple generators from the same single monolithic generator by means of individual MLM heads at different layer depths which : (i) reuses computation and is efficient, and (ii) exploits underlying representations learned by lower layers to make the deeper layer MLMs stronger.",
            "4": "3) The empirical results of the AMOS pretraining approach are strong.",
            "5": "Despite several discussions in the NLP community on using GLUE as an evaluation benchmark, a 1 absolute point improvement using a 12-layer base model is significant over the previously strong baseline of COCO-LM (which additionally uses the InfoNCE loss).",
            "6": "4) The ablation studies on MNLI and SQuAD help justify the modeling decisions made in the paper w.r.t the learnable weights, the use of stop-gradient between layers, not using the adversarial discriminator loss to train the learnable weights, etc.",
            "7": "Weaknesses:\n1) The ablation studies on MNLI and SQuAD are provided without standard deviation error bars.",
            "8": "The improvements of AMOS over the ablated components is rather small, and thus it is important to present the error bar estimates to ensure the statistical significance of the results.",
            "9": "2) For Table 3, the experiments are performed using independent 4,6 and 8 layer generators to establish the point that the pretraining signals provided are diverse.",
            "10": "A more appropriate experiment would have been to perform the pre-training of AMOS once, and then use the 4,6 and 8 -layer MLMs (having common transformer blocks) and then see whether this diversity in probing performance exists.",
            "11": "The objective is to show that the 4,6 and 8 layer generator with the shared MLM have diverse probing performance on tasks.",
            "12": "3) While extensive, the experiments are limited to pretraining a 12-layer transformer model.",
            "13": "Since the paper presents arguments such as the 12-layer generator being very strong, it is natural to question whether the improvements from AMOS will translate to larger architectures (24 layers) using the same mixture of multiple generators approach.",
            "14": "4) I think the paper should include a societal impact section and discuss the extensive compute resources that have been consumed for the pretraining runs and the ablation studies.",
            "15": "Furthermore, the reproducibility statement is missing details of the compute infrastructure used : Number of GPUs, types of GPU/TPUs, etc.",
            "16": "Questions:\n1) How was the \\lambda in Equation-(4) chosen for the experiments?",
            "17": "Was some form of cross-validation performed?",
            "18": "2) Instead of random masking, do you have any high level thoughts/intuitions on whether the AMOS approach will also provide empirical improvements with targetted masking approaches (For example: REALM that masks specific POS entities for retrieval-augmented MLM)?",
            "19": "3) While the different MLMs in AMOS have different number of layers, they all have the same structural transformer block, and thus are from the same \"family\" of generators.",
            "20": "Since the goal is to increase the diversity and complexity of signal from the generator for improving the strength of the discriminator, it will be interesting to experiment with a different \"family\" of generators (based on LSTM, word-CNN, etc.).",
            "21": "Any thoughts/comments on this?",
            "22": "4) The paper presents the AMOS multi-generator guided pretraining approach for a general k number of generators, but all the experiments are performed using k=3.",
            "23": "While I understand that each pretraining run is computationally expensive, I was intrested to know if the authors had any intuitions/initial results from experiments by changing the number of MLM heads k?"
        },
        "TPhSWWrz48Y": {
            "0": "Strengths:\n\nThe paper is well written and easy to understand.",
            "1": "Since it builds on the well known ELECTRA architecture, it is easy to identify the main contribution, which is adding more inductive learning curriculum to ELECTRA training.",
            "2": "The authors achieved this by using multiple auxiliary MLMs and combining their outputs to generate the input to the discriminative text encoder.",
            "3": "To improve training efficiency and further streamline the architecture, all the auxiliary MLMs are derived from the same base MLM with outputs of multiple layers used as its own sub-MLM realization.",
            "4": "Finally, the mixture parameter is adversarially learned using the negative of the discriminator gradient.",
            "5": "The ablation study is exhaustive providing empirical evidence of superiority of the proposed architecture.",
            "6": "Weaknesses:\nThere is a typo “date-centric” --> \"data-centric\"\nThe paper applied the MLM to only 3 layers (4/6/8) of the 8-layer generator.",
            "7": "Do you have an intuition on the effect of increasing MLM heads on the discriminator performance on the downstream tasks?",
            "8": "It was shown that using just one MLM head irrespective of the layer is worse, but is there an optimal number of of MLM heads for an N-layer generator network?"
        },
        "_JqL9yzIjJu": {
            "0": "The paper is clear written and relatively easy to follow.",
            "1": "The paper idea is majorly from 3 aspects: curriculum learning, adversarial training via Gumble-softmax, layer wise understanding of pretrained models.",
            "2": "These ideas are reasonable.",
            "3": "The results show on GLUE dev and SQUAD 2.0 also looks significant.",
            "4": "There are two parts of the work I think needs further clarification.",
            "5": "First, the author states that the model is based on mix of training signal generator, however, the work is actually using different layer mixture of signals.",
            "6": "To make the statement solid, I think another experiment of independent generators with same layers as generators should be conducted.",
            "7": "Also, the meaning of vector v in weight calculation is not clearly discussed.",
            "8": "Second, the adversarial training does not include re-sample the masked tokens, which may be a strong signal in adverbial training.",
            "9": "Besides this, as this is a pretraining work.",
            "10": "I would expect to see the performance on GLUE test set instead of just dev set which should make the results more convincing.",
            "11": "Also, the training time/speed/performance comparison with ELECTRA may be better shown in the work."
        },
        "w4K1PCPnuW": {
            "0": "Strengths:\n\n1.",
            "1": "A clever approach to mitigate generator-discriminator dynamics in end-to-end training of Electra-like models by using a computationally efficient mixture of generators.",
            "2": "2.",
            "3": "Strong empirical results on many downstream NLU tasks outperforming strong baselines.",
            "4": "3.",
            "5": "Fairly thorough ablations for different model components.",
            "6": "Weaknesses:\n\n1.",
            "7": "It is unclear to me why the discriminator cannot just optimize gammas to be high for lower layers, thereby making it easier to optimize its own objective.",
            "8": "Wouldn’t it, therefore, be better to have the gammas affect the MLM loss as well to help the lower layers learn better.",
            "9": "2.",
            "10": "Following up on the previous point 1) Fig 3 (a) is counter-intuitive as to why layer 8 has the highest coefficient followed by 4 and then 6.",
            "11": "I would have expected the discriminator to learn weights in the order 4 > 6 > 8 since the only gradient for these parameters comes from the discriminator loss and not the generator MLM loss (b) how robust is the mixture weights to restarts with different seeds and what if we trained from scratch with fixed mixture weights using what was obtained at the end of 120k steps in Fig 3 (a)?",
            "12": "3.",
            "13": "Although computationally expensive, it may be useful to establish a performance “upper bound” by training M different generators."
        },
        "m7qMZyP2Tq": {
            "0": "  Questions:\n\n1- Figure 1: the generator with more layers (8) gains lower training loss during pretraining, meaning it can better recover the masked token, which means during sampling it should recover the original token.",
            "1": "In other words, the replaced-token sequence should have less replaced tokens and it should be an easier task for the discriminator!",
            "2": "2- Are the MLM heads shared across different layers of generator?",
            "3": "3- Table 2: the ablation on stop-gradient is only evaluated on MNLI and Squad which should degrade in performance.",
            "4": "Is this ablation studied on other GLUE tasks as well?",
            "5": "4- the author proposed using gumble-softmax to enable gradient backpropagation from discriminator.",
            "6": "In ELECTRA paper, reinforcement learning is used to leverage this.",
            "7": "Is the proposed approach can be used using RL too?",
            "8": "what is the performance on downstream task?",
            "9": "5- The author mentioned that the discriminator loss is used to train the mixture weights of MLM output to combine a  more difficult signal for discriminator, but it will not update the MLM embeddings.",
            "10": "This is confusing, because generator is trained jointly with discriminator, and MLM embeddings are trained via two gradients, one from discriminator, and the other from MLM pretraining of generator!",
            "11": "Moreover, in Figure 2, it is shown that discriminator gradient are backpropagated through generator too.",
            "12": "In table 2, it is shown that using adv.",
            "13": "MLM hurts MNLI matched performance by 0.3 points, whereas other tasks are untouched.",
            "14": "what is the performance of this setting on other GLUE tasks?",
            "15": "6- Table 1: why the number of parameters in Base++ setting is larger than Base one?",
            "16": "7- Table 2: all the ablation results are evaluated on MNLI and squad tasks.",
            "17": "however, I think all GLUE tasks should be considered in this study to understand the actual contribution of different components of the proposed AMOS.",
            "18": "8- Table 2: in layer switch and random layer configuration, are all MLM layers are pretrained equally?",
            "19": "9- Table 2: w. separate MLM gen. configuration shows only 0.1 performance decrease on MNLI-matched compared to -stop grad setup.",
            "20": "what is the performance on this setting on other GLUE tasks?",
            "21": "10- Figure 3(b): the discriminator accuracy with 4-layer generator have a smooth increase with higher performance than AMOS, which does not use any adversarial training, which seems an intuitive learning curriculum too.",
            "22": "what is the explanation on this?"
        },
        "GjB4sVpTx_3": {
            "0": "There are several strengths from the paper that make me believe it is a good paper to be published in ICLR 2022:\n1.",
            "1": "The paper made a significant contribution to idea of using adversarial training as part of the self-supervision signal for language learning.",
            "2": "2.",
            "3": "The paper made a impactful finding for practicing adversarial training, that mixture of signals at different depth of of the generator can stabilize ELECTRA-style models trained adversarially using Gumble-Softmax relaxation.",
            "4": "3.",
            "5": "The experiments in the paper demonstrated the superiority of adding adversarial training to a self-supervision framework, in that significant improvements can be obtained for similar-sized networks.",
            "6": "4.",
            "7": "Good set of ablation studies to show that each component of the model is necessary, especially because the entire model already has many moving parts in addition to adversarial training.",
            "8": "With the strengths being said, I hope to also point out that the paper's application of adversarial training is one attempt in many possibilities, and in many cases it is not clear where the improvements come from.",
            "9": "For example:\n1.",
            "10": "The paper pointed out that ELECTRA framework [1] explored the idea of using REINFORCE [2] as the the way of adding adversarial training signals to the model but observed degenerated results.",
            "11": "Compared to this, the paper made 2 changes to the model: 1) using Gumble-Softmax instead of REINFORCE, and 2) using mixture-of-signals instead of straightforward gradient back-propagation.",
            "12": "It is hard to know here which one of these actually made the adversarial setup useful.",
            "13": "Is it possible to run an ablation study using the combination of REINFORCE and mixture-of-signals to verify whether Gumble-Softmax relaxation is the reason for it to work?",
            "14": "2.",
            "15": "There are many past papers that apply the discriminator to some internal representations of the generator instead of on the softmax outputs of the discrete text signal (for example, [3][4][5]).",
            "16": "In the practices of GAN for text, these are proven to be more stable than both Gumble-Softmax relaxation and REINFORCE.",
            "17": "What are the reasons for the paper to choose Gumble-Softmax relaxation instead?",
            "18": "Please discuss.",
            "19": "References:\n\n[1] Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning, ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLR 2020\n\n[2] Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu, SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient, AAAI 2017\n\n[3] Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, Lawrence Carin, Adversarial Feature Matching for Text Generation, ICML 2017\n\n[4] Jake Zhao (Junbo), Yoon Kim, Kelly Zhang, Alexander M. Rush, Yann LeCun, Adversarially Regularized Autoencoders, ICML 2018\n\n[5] Sandeep Subramanian, Sai Rajeswar Mudumba, Alessandro Sordoni, Adam Trischler, Aaron C. Courville, Chris Pal, Towards Text Generation with Adversarially Learned Neural Outlines, NeurIPS 2018"
        }
    },
    "LGTmlJ10Kes": {
        "Y4k2H7mVw0": {
            "0": "Strengths:\n+ The authors conducted a braod set of experiments.",
            "1": "+ The paper is easy to follow.",
            "2": "Weaknesses:\n- The main contribution of this work is to add changing weights to easy, medium, and hard training subsets, which an incremental development in my opinion.",
            "3": "- At the beginning of section 2.3, the authors should acknowledge curriculum methods that do not maipulate data samples, such the \"model-level\" curriculum approaches surveyed in [A].",
            "4": "- The approach divides the training data into 3 parts {easy, medium, hard}, based on a difficulty score.",
            "5": "Ablation results with a different number of parts and other difficulty scores should be presented.",
            "6": "I think it is particulary interesting to see what happens if th examples are randomly partitioned, i.e.",
            "7": "does the difficulty play an important role?",
            "8": "- The introduction leaves the impression that weights for data shards are somehow predicted by the model at each iteration.",
            "9": "However, in section 2.4, the reader learns that \"curriculum discovery\" is based on hyperparameter tuning.",
            "10": "The claims from the introduction should be toned down.",
            "11": "- The performance gains shown in Figure 4 are rather small (wintin the standard deviations) compared to competing curriculum methods.",
            "12": "The same applies to the supplementary.",
            "13": "Formatting and language mistakes:\n- \"Without a curriculum learning will be an intractable\" => \"Without a curriculum, learning will be an intractable\";\n- \"Nie et al.",
            "14": "(2020), which also studies\" => \" Nie et al.",
            "15": "(2020), who also study\";\n- many citations are wrongly formatted, e.g.",
            "16": "\"logistic functions Richards (1959)\" should be \"logistic functions (Richards, 1959)\".",
            "17": "References:\n[A] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe.",
            "18": "Curriculum learning: A survey.",
            "19": "arXiv preprint arXiv:2101.10382, 2021."
        },
        "s4pED1kpR9w": {
            "0": "In general, the approach appears to be rather constrained (only 3 difficulty levels; limited to sigmoids; reliance in part on multi-annotator labels), may be onerous to implement (pre-training a baseline; fitting the sigmoids; making manual decisions on entropy vs loss) and was validated on rather small data (balanced datasets with <10k samples) to be useful in practice.",
            "1": "Strengths:\n1.",
            "2": "Addresses a hard unsolved problem.",
            "3": "2.",
            "4": "Modeling simplicity of the curriculum functional class (simple sigmoids that would cover most of the human-hypothesized heuristics; however, it may not be what we need - see below).",
            "5": "Weaknesses:\n1.",
            "6": "I find the approach conceptually different to classical curriculum learning, since it reweighs instances in the loss rather than schedules them in a particular order (except for the zero-weight case, equivalent to example skipping, where both ways converge); as far as could tell, SGD still samples data uniformly at any time.",
            "7": "Besides, since the loss function changes, the baseline and the weighted tasks optimize different objectives and it's not straight-forward to compare them.",
            "8": "2.",
            "9": "Regarding sigmoids, I'm not convinced that practical cases of ML curricula can be covered by such simple monotonic schedules; rather, the functional class should be capable of dynamically changing the scheduling weight depending on the learning stage to mitigate forgetting and revisiting learned difficulty levels (and this encompasses monotonic functions too).",
            "10": "In fact, such bandit-based curricula increasing/decreasing importance dynamically have been already proposed (Graves et al, 2017, https://arxiv.org/abs/1704.03003) and even evaluated for NLP tasks beyond classification (Kreutzer et al, 2021, https://arxiv.org/abs/2110.06997).",
            "11": "The draft could be completed with a discussion of why the approach of Graves et al.",
            "12": "is not considered or, better, a comparison experiment could be added.",
            "13": "Note that the bandit approach is free from limitations such as number of difficulty levels.",
            "14": "3.",
            "15": "The requirement to pre-train the baseline prior to curriculum-enabled training defeats one of the main purposes of curriculum learning -- saving resources in the large data regime and relieving developer from manual training design.",
            "16": "Again, dynamic curricula that are trained on the fly would be a more practical approach, as they don't require pre-training.",
            "17": "4.",
            "18": "The entropy-based curriculum relies heavy on assumed human-perceived difficulty, which may not be the same for the network's point of view.",
            "19": "The fact, that Zhang et al.",
            "20": "(2018) and Kocmi & Bojar 2017 both found that reverse curriculum works similarly well, witnesses that human intuitions of what is difficult for the network may be wrong.",
            "21": "5.",
            "22": "It looks like data balancing increases the gap to the no-curriculum approach.",
            "23": "This again changes the task (in addition to different objectives) and raises the question of practical relevance for tasks where such balancing is not desired or not possible (seq2seq).",
            "24": "Minor:\n- i found Fig.",
            "25": "1 redundant, it is just illustrating what is sigmoid function\n- fuzzy wording in a few places (\"good metric\", \"fairly distributed\", \"significant variance\")\n- change \\citet to \\citep in some places, Richards (1959), Shannon (2001)\n- sec 3.2: \"configurations is\" -> \"configuration is\"\n- \"in Nie et al (2020), which\" -> \"by Nie et al (2020), who\""
        },
        "XdCNRJ7VKoL": {
            "0": "#### Strengths\n- Proposed curriculum method encompasses other well known curricula by parameterizing the data partition and weighing schemes.",
            "1": "- Data partitioning is effective, on the three datasets the authors evaluate upon, as is evident from improvements over no curriculum approach.",
            "2": "- Specialized curricula obtained using curriculum discovery improves over no curriculum and other state of the art approaches.",
            "3": "- Sample weight patterns shows the relative importance of samples on different datasets, which is interesting.",
            "4": "- One quite interesting advantage of this approach is that curriculum discovered for smaller, balanced datasets work well on larger datasets.",
            "5": "#### Weaknesses\n- The proposed method partitions a dataset into three classes $\\textit{easy}$, $\\textit{medium}$ and $\\textit{hard}$.",
            "6": "Will this partitioning scheme work for different types of datasets, where there might be multiple levels of difficulty, multiple sub tasks?",
            "7": "How will this method scale?",
            "8": "- Using human agreement as a measure of entropy might not be applicable to datasets where there is less ambiguity between samples.",
            "9": "That will cause majority of samples to be in a particular partition.",
            "10": "For those datasets using entropy as a measure to partition samples might not be optimal.",
            "11": "-  In Section 2.3, the authors describe how the proposed framework encompasses other well known CL approaches.",
            "12": "It will be interesting to add more analysis on how often the TPE algorithm(Section 2.4) discovers curriculums where the parameters fall into pruning or sub-sampling strategy space.",
            "13": "- How does this approach scale to larger datasets in vision like Imagenet, CIFAR etc, that other SOTA methods like SuperLoss evaluate on?",
            "14": "Authors evaluate on three datasets, not all of which are well known to the community.",
            "15": "I will encourage the authors to also evaluate this approach on more popular datasets.",
            "16": "- How significant are the improvements over other baselines?",
            "17": "Showing average over full and balanced datasets(Fig 4) hides some important issues.",
            "18": "It seems from Figure 10, that DP method outperforms the proposed CL method on full datasets.",
            "19": "We should pay more interest to this number as we would want any approach to work well on full dataset, rather than a subset(balanced) dataset.",
            "20": "- Hard examples are down weighted more aggressively in this approach.",
            "21": "It can be seen that the proposed approach often lags behind other approaches like No-CL, MentorNet, DP when it comes to hard examples.",
            "22": "This issue is more prominent when it comes to full datasets.",
            "23": "Authors should provide some additional analysis on these hard samples from these datasets for more clarity.",
            "24": "What % of these datasets are partitioned into $\\textit{hard}$ class?"
        }
    },
    "6PvWo1kEvlT": {
        "ZWjPZYz9M9": {
            "0": "-Strengths\n\nThe authors make good points, pointing out that the prior attempt to interpret a MLM (like BERT) as an MRF is incorrect and proposing to define the proposal distribution by masked conditionals for the MH sampler.",
            "1": "A series of empirical experiments on effects of temperature, nucleus sampling and block MH sampling are conducted.",
            "2": "Both conditional generation (NMT) and unconditional generation are covered.",
            "3": "-Weaknesses\n\n1.",
            "4": "Missing important relevant references.",
            "5": "EBMs (a.k.a.",
            "6": "un-normalized models, random fields) have been successfully developed for language modeling in recent years.",
            "7": "The sampling methods proposed in this manuscript for the energy-based language model has been studied in [1-5], including Gibbs sampling, MH sampling, block MH sampling, albeit the specific energy functions are different.",
            "8": "A recent work in [6] also defines an energy-based language model from MLMs.",
            "9": "Connecting and comparing to these previous works are needed.",
            "10": "[1] B. Wang, Z. Ou, and Z. Tan, “Trans-dimensional random fields for language modeling,” ACL, 2015.",
            "11": "[2] B. Wang, Z. Ou, and Z. Tan, “Learning trans-dimensional random fields with applications to language modeling,” IEEE transactions on pattern analysis and machine intelligence, 2018.",
            "12": "[3] B. Wang and Z. Ou, “Language modeling with neural trans-dimensional random fields,” IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2017.",
            "13": "[4] B. Wang and Z. Ou, “Learning neural trans-dimensional random field language models with noise-contrastive estimation,” IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.",
            "14": "[5] B. Wang and Z. Ou, “Improved training of neural trans-dimensional random field language models with dynamic noise-contrastive estimation,” IEEE Spoken Language Technology Workshop (SLT), 2018.",
            "15": "[6] Clark, Kevin, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.",
            "16": "\"Pre-training transformers as energy-based cloze models.\"",
            "17": "EMNLP 2020.",
            "18": "2.",
            "19": "The theoretical contribution seems weak, given the above prior works.",
            "20": "This manuscript presents extensive empirical results, but such contribution may not be substantial enough to motivate acceptance.",
            "21": "3.",
            "22": "When the tasks are text generation (conditional or unconditional), it is expected to compare the proposed method with GPT based generation, in both quality and computational cost.",
            "23": "Otherwise, it is not clear how useful the proposed method is.",
            "24": "The proposed samplers seem to be computational very expensive, and the authors do not provide any discussions on how to reducing the cost.",
            "25": "Minors: Many notations in Algorithm 1 are not defined in the main text or not consistent when compared to notations in the main text, such as:\nf_mlm, \\sigma, f_E"
        },
        "aHpBfTquv7R": {
            "0": "strengths：\n1.",
            "1": "The author introduces energy networks to rectify the incorrect assumption which is existed in previous work [1].",
            "2": "2.",
            "3": "This paper proposed two novel energy parametrizations and introduces Metropolis-Hasting sampling to text generation based on MLM models.",
            "4": "3.",
            "5": "All methods are empirically verified in a variety of experiment settings.",
            "6": "weaknesses：\n1.",
            "7": "This paper lacks the introduces of previous works, such as undirected generation approaches and energy networks, which makes the paper is hard to understand without reading the references.",
            "8": "2.",
            "9": "There are many experiments between proposed method with the degenerate Gibbs sampling, but do not compared with other generation models like GPT, BART, etc.",
            "10": "The potentiality of  proposed methods can not be estimated.",
            "11": "3.",
            "12": "The computational complexity of MH sampler is very high as described in this paper, which may not valuable for real applications.",
            "13": "questions:\n1.",
            "14": "Is the methods proposed in this paper applied in inference stage?",
            "15": "Which means the model cannot applied in other datasets except the dataset which is used for training the pre-trained model.",
            "16": "typos:\nIn section 4,  \"We empirically study the proposed Metropolis Hastings scheme .... and the task of uncondtional generation.\"",
            "17": "-> unconditional \n\nReference:\n[1] Wang and K. Cho.",
            "18": "BERT has a mouth, and it must speak: BERT as a Markov random field language model.",
            "19": "arXiv preprint arXiv:1902.04094, 2019."
        },
        "SivELbInxMk": {
            "0": "The strengths of the paper is its technical novelty and rigor as well as its thorough experimental studies.",
            "1": "The weakness of this paper is its lack of clarity on a few specifics.",
            "2": "The proposed method could approximate energy-based sequence models with a good proposer (i.e., MLM), and the paper clearly discusses its general theoretical advantages over a Gibbs sampling alternative.",
            "3": "The extensive experiments on machine translation and unconditional generation well support the claims.",
            "4": "However, I would appreciate more details on the specific target distributions used in the experiments: e.g., I am not familiar with MASK-PREDICT and it is not obvious to me how the general technical remarks apply to this specific case."
        },
        "ha-1X-7lPuo": {
            "0": "*Pros*\n- The paper tackles an interesting problem of sampling from energy functions with local conditionals.",
            "1": "- The theoretical contributions of the paper are quite sound.",
            "2": "The rationale behind (wang and cho'19) faulty assumption and the use of MH as an alternative.",
            "3": "- The experiments show interesting trials with different variations and honest explanations of the obtained scores.",
            "4": "Overall sampling from MLM is a new task compared to auto-regressive LMs it is natural that the performance obtained is going to be slightly inferior.",
            "5": "*Areas of enhancement*\n\n- The readability of Section 2 (Energy) could be enhanced, the connections between E_local and E_raw could be elaborated since both are quite similar computationally.",
            "6": "The current structure gives the impression that they are completely different formulations.",
            "7": "- Section 6 (Experiments) could be enhanced by providing a global narrative and coherence between the experiments, It is easy to get lost and compare different tables.",
            "8": "For example, it took me some time to notice that the results from Table 4 are superior to those of table 3 and 2 and therefore Block MH sampling is useful.",
            "9": "- For Open-ended text generation experiments the corpus level diversity using (SELF-BLEU) should be reported overall, some generation examples could be displayed in the appendix to judge the overall quality (see questions below).",
            "10": "*Questions / recommendations to authors*\n\n- What are the theoretical/conceptual differences between E_norm and E_raw?",
            "11": "I see they have been used both in previous work and empirically they perform differently, are there any conceptual differences that motivated you to use both formulations?",
            "12": "If it was merely experimental it is fine just to mention it clearly in the text.",
            "13": "- Samples from MH are known to have high auto-correlation (i.e.",
            "14": "i.i.d), this is due to the nature of the local proposal.",
            "15": "For applications like open-ended generation (unlike NMT) where the user expects large corpus diversity, this is a crucial feature to have.",
            "16": "Although authors report *Novel* transition rates this could be misleading as it only shows the novel transition while the space of effective X could still be limited.",
            "17": "I recommend reporting SELF-BLEU in the open-ended generation experiments to indicate corpus-level diversity.",
            "18": "For more details see: https://arxiv.org/pdf/1811.02549.pdf\n\n- section 7: \"Although the autoregressive approach is superior (30.18 de-en ...\"  it would be great adding those results in table 5 to allow easier comparison even when they are superior to non-autoregressive models (which is understandable).",
            "19": "- Generation examples: For transparency, one would like to see examples generated from the MH algorithm, perhaps dump a subset of all generations of each method in a table in the appendix.",
            "20": "One would like to asses the auto-correlation effect of MH on the diversity of the generated samples.",
            "21": "- The locally normalized scoring formulation is referred to as  $E_local$ $E_norm$, \"norm\" and \"local\" interchangeably this confuses the reader.",
            "22": "- Equation of $E_{local}$ in the bottom of page 3 has two symbols t and i. it is not clear where the $i$ comes from is this a typo?",
            "23": "- Page 3: \"the following free conditionals are inconsistent\" in fact this example is not clear, I had to go back and forth many times with the appendix to get it right.",
            "24": "It would be great to take the time to explain this example in a self-contained way.",
            "25": "- Figure 2 is not clear that is the green curve"
        }
    },
    "HCRVf71PMF": {
        "2VIxSZ4FXGu": {
            "0": "\nStrengths\n- Few-shot setting and lifelong learning are important settings.",
            "1": "- I can't give any more strengths at this time because I had a hard time understanding the paper.",
            "2": "Weaknesses (reasons to reject)\n- The main contribution of saying that LFPT5 \"can be seen as a vital step towards general language intelligence\" is surely an overclaim, right?",
            "3": "If I ask 100 researchers, how many of them will see this paper as a vital step towards general language intelligence?",
            "4": "- I did not understand why the proposed method is restricted to the few-shot setting.",
            "5": "Do gains go away in the full data setting?",
            "6": "- The experiments are OK but not probably not good enough to support the general claims about lifelong learning.",
            "7": "E.g., it would be great to show more than 3 tasks.",
            "8": "- I had a hard time understanding the paper, see below (and I will remove this comment from reasons to reject once it is clarified):\n\t- The key part that I did not understand is why prompt tuning isn't already a sufficient life-long learning method.",
            "9": "My understanding of prompt tuning is that no model weights are modified, only the prompt is optimized, and optimized prompts are stored for each task.",
            "10": "Then given a new task, a new prompt can be trained and stored.",
            "11": "This works because the size of prompts is many orders of magnitude smaller than the size of the model.",
            "12": "Maybe another way of asking this question is, how is the following different from regular prompt tuning: \"while adapting to a new task type, LFPT5 includes and tunes additional prompt embeddings for the new task\"?",
            "13": "- Another part I do not understand is why pseudo samples need to be generated.",
            "14": "Why can't you just use the prior examples?",
            "15": "- I am missing why this idea of retraining on previous domains each time is a good one.",
            "16": "Let's say we're up to 1000 tasks.",
            "17": "How much weight does the new task get in this mixture?",
            "18": "Won't this take a long time?",
            "19": "Weaknesses (not reasons to reject)\n- The introduction could make it more clear why the second two limitations of LLL are indeed limitations.",
            "20": "Why NER in particular, instead of other tasks that haven't been explored thoroughly?",
            "21": "The negative transfer point needs some more evidence as well, as I believe the decaNLP challenge showed positive transfer, right?",
            "22": "Minor comments\n- Is the finetuned BERT-Large a few-shot finetuned or with the full dataset?",
            "23": "- PT is not defined at first use in the abstract\n- All the acronyms are pretty confusing\n- Typo: review socre 5"
        },
        "_GDdjlacobG": {
            "0": "The paper has several strengths that make it a promising submission:\n\n1.",
            "1": "The idea of using prompt-tuning for life-long language learning is a promising direction and the methods proposed in the paper constitute a novel and effective way in bridging the gap between these 2 methodologies.",
            "2": "The discussion on few-shot learning also clearly demonstrated the difference of the proposed methods and a major collection of prior work using prompt-tuning.",
            "3": "2.",
            "4": "The methods proposed in the paper are effective in utilizing the assumption of language modeling, such that tasks are formed as prompt-label pair sequences, and autoregressive sequence generation is used to anchor the model to previously learnt knowledge.",
            "5": "3.",
            "6": "The experimental setup is representative of actual uses of life-long language learning, with 3 different tasks included.",
            "7": "Comparisons to fine-tuning, prompt tuning, and their combinations with elastic weight consolidation (EWC) and memory-aware synapses (MAS) are solid choices to demonstrate the effectiveness of the proposed methods.",
            "8": "In most of these comparisons, the proposed methods achieved better results.",
            "9": "There are a few weaknesses that I hope the paper can address to improve the scoring:\n\n1.",
            "10": "In the paragraph below equation 4, the loss form is an addition of 3 values: 1) the task loss L^{task}; 2) the language modeling loss L^{lm}; 3) label consistency loss L^{KL}.",
            "11": "Both 1) and 2) are reasonable, but 3) is new in the paper and it is unclear why it is absolutely necessary.",
            "12": "In particular, the paper should include an ablation study on a range of \\lambda_{KL}, not just a comparison between models with and without L^{KL}.",
            "13": "2.",
            "14": "Some statistics that are useful for understanding the propose methodology are missing in the experimental section: 1) it is probably beneficial to include some numbers on the state-of-the-art results on methods for the original tasks and datasets.",
            "15": "They may not be fair statistics in a life-long scenario, but they are helpful for the readers to understand where the methods stand in a larger picture of machine learning methods.",
            "16": "2) For the domain order experiments in section 5, it could be useful to also include accuracy for each of the datasets separately.",
            "17": "This will provide the readers with additional knowledge on the relationship between task ordering and individual task performance.",
            "18": "Finally, some suggestions to the paper, but these will not affect the scoring:\n\n1.",
            "19": "Remove the quote in page 1.",
            "20": "2.",
            "21": "The paper should include a brief introduction to the 2 regularization-based life-long learning methods that it compares with.",
            "22": "These are elastic weight consolidation (EWC) and memory-aware synapses (MAS).",
            "23": "3.",
            "24": "Try to reduce the use of abbreviations in tables.",
            "25": "For example, by typing fine-tuning and prompt-tuning instead of FT and PT, it could make the table much easier to read without the need to refer to the paper text."
        },
        "fZXAtwibeLE": {
            "0": "\n=================================\nStrengths:\n  - Combining lifelong and few-shot learning is a new setting.",
            "1": "- Experiments contain 3 different tasks and each has datasets from different domains.",
            "2": "- Experiments are well-designed: many baselines are implemented to compare proposed method with traditional lifelong learning methods.",
            "3": "- Paper is well-written.",
            "4": "=================================\nWeaknesses:\n  - The actual method is simple combination of existing ideas.",
            "5": "- The number of tasks and domains is minimal setting.",
            "6": "To really become a benchmark to measure the progress of LFLL, more tasks/datasets will be needed.",
            "7": "=================================\nAdditional Suggestions/Questions:\n  - Question Answering and NLI are also very important NLP problems and many datasets exists from different domains.",
            "8": "Perhaps the authors can consider adding them to setup a more comprehensive benchmark.",
            "9": "- Perhaps adding more pre-trained LMs such as GPT-2 and different sizes of T-5.",
            "10": "The community will be interested how the model type / scale affect the LFLL capability.",
            "11": "- How will the pseudo data generation amount affect the learning / forgetting performance?",
            "12": "Since the data is generated by T-5, couldn't we generate as much as we want?",
            "13": "Maybe adding another ablation on this would be a good idea."
        },
        "KgLblNRI8H": {
            "0": "Strengths:\n\n- The paper attempts to solve an important problem of LFLL in the context of the pre-trained Transformers.",
            "1": "With the rise of larger models, it is important to consider the scalability of these models for lifelong learning.",
            "2": "Given that this paper builds upon the recent developments in parameter-efficient transfer learning, the number of parameters in the proposed prompt tuning-based method would not grow dramatically with the number of tasks.",
            "3": "- Empirical results showcase that the proposed approach is superior to existing methods like EWC and MAS when evaluated on the sequence of 2-4 domains/tasks.",
            "4": "Weaknesses:\n\n- For the LFLL problem, this paper proposes to use an existing prompt tuning-based transfer learning method where the underlying model is frozen and a few additional task/domain-specific parameters are learned.",
            "5": "Furthermore, it repurposes existing lifelong learning methods like deep generative replay [1], distillation-based methods [2], dynamically expandable network [3] as a unified framework when applied to prompt tuning.",
            "6": "Thus, the contributions of this paper are marginally novel/ significant.",
            "7": "- Another major limitation of this paper is that it completely ignores comparison with other parameter-efficient transfer learning methods like adapters [4].",
            "8": "Concretely, AdapterFusion [3] work proposes to learn a task-specific composition of adapters from previous tasks.",
            "9": "How does the prompt tuning-based method compare with adapter fusion when applied to the LFLL problem?",
            "10": "Is there a consensus that prompt tuning-based methods are better than adapters for few-shot learning?",
            "11": "- By definition, the lifelong learning paradigm deals with a large number of tasks in sequence.",
            "12": "However, the paper evaluates the proposed approach on the sequence of 2-4 tasks.",
            "13": "So it is unclear whether the gains persist as we increase the number of tasks to 10-15 (towards a more realistic evaluation setup).",
            "14": "As we increase the number of tasks, does it enable positive forward/ backward transfer or increase the chance of negative transfer?",
            "15": "- Overall, the paper is written in good style.",
            "16": "However, there are key experimental details that are missing.",
            "17": "This paper defines prompts as a series of tunable tokens but then it is unclear how many tokens are defined per task prompt.",
            "18": "This is important because as we scale the number of tasks, the input sequence length scales linearly, and with that self-attention scales quadratically.",
            "19": "This issue limits the practical applicability of the prompt tuning-based method for lifelong learning.",
            "20": "- What does it mean to use validation sets for hyper-parameter selection?",
            "21": "Ideally, during lifelong learning, we do not know the sequence of tasks apriori.",
            "22": "It is unfair to freeze a particular task order, then search for optimal hyper-parameters, and then report results on multiple runs with the same task order.",
            "23": "At least, report the results on unseen task orders?",
            "24": "Or search on a subset of disjoint tasks [5].",
            "25": "The paper should consider updating their results with a fair hyper-parameter selection strategy.",
            "26": "Additional clarification question(s):\n- The paper trains a single prompt for both task solving and data generation.",
            "27": "It is unclear whether this is an optimal strategy when it comes to generating pseudo-samples?",
            "28": "Did the paper try learning a separate prompt for task solving and data generation?",
            "29": "Does it help in addressing the quality of generated pseudo-samples?",
            "30": "Furthermore, how effective is the current pseudo-sample generation strategy?",
            "31": "How many samples are discarded to get valid samples?",
            "32": "- There are a few hyper-parameters that are set without any justification: 16-shot learning?",
            "33": "How does the method perform with varying the number of training examples?",
            "34": "How does the performance change with varying the number of pseudo-samples?",
            "35": "Does it hurt if we have more pseudo-samples (generating them is feasible and cheaper)?",
            "36": "- In terms of the multi-task experiments (MT-PT), do we have separate prompts or a single prompt being trained with all data?",
            "37": "If it is a single prompt then is it fair to compare it with LFPT5 with more capacity?",
            "38": "If there are multiple prompts then how does one ensure that there are dedicated prompts for each task?",
            "39": "[1] Sun, Fan-Keng, Cheng-Hao Ho, and Hung-Yi Lee.",
            "40": "\"LAMOL: LAnguage MOdeling for Lifelong Language Learning.\"",
            "41": "International Conference on Learning Representations.",
            "42": "2020.",
            "43": "[2] Li, Zhizhong, and Derek Hoiem.",
            "44": "\"Learning without forgetting.\"",
            "45": "IEEE transactions on pattern analysis and machine intelligence 40.12 (2017): 2935-2947.",
            "46": "[3] Pfeiffer, Jonas, et al.",
            "47": "\"AdapterFusion: Non-Destructive Task Composition for Transfer Learning.\"",
            "48": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume.",
            "49": "2021.",
            "50": "[4] Houlsby, Neil, et al.",
            "51": "\"Parameter-efficient transfer learning for NLP.\"",
            "52": "International Conference on Machine Learning.",
            "53": "PMLR, 2019.",
            "54": "[5] Chaudhry, Arslan, et al.",
            "55": "\"Efficient Lifelong Learning with A-GEM.\"",
            "56": "International Conference on Learning Representations.",
            "57": "2018."
        }
    },
    "Ek7PSN7Y77z": {
        "7DQt6IC0PVW": {
            "0": "**Strengths**\n\n1.",
            "1": "The disentanglement of exploration and exploitation makes sense.",
            "2": "The phase-alternating pipeline is nicely designed.",
            "3": "2.",
            "4": "The paper is clearly written, it is relatively easy to understand how the model look like (although the intuition of each component isn't too clear).",
            "5": "3.",
            "6": "The set of ablation experiments in Section 4.2 are well designed.",
            "7": "**Questions and concerns**\n\n1.",
            "8": "What's the reason of choosing this subset of 12 games?",
            "9": "While the list seems to cover a wide range of game difficulties, but why not using the entire Jericho suite?",
            "10": "2.",
            "11": "The authors cited the INV-DY agent (Yao et al., 2021) in their Section 3.1.3, and actually, if I understand correctly, the entire Section 3.1.3 is describing Yao et al.",
            "12": "'s model, without any new contribution.",
            "13": "Why do not the authors compare their agent with INV-DY in result tables?",
            "14": "3.",
            "15": "In Section 3.1.PHASE 1, the authors describe two criteria that switch the agent to exploration phase.",
            "16": "Can the authors elaborate on the second criterion, what does it mean if the number of steps in an episode equal to the longest of the $k$ sampled trajectories?",
            "17": "If an agent moves back and forth between two locations, which may result a super long steps, but this behavior is not necessarily desired.",
            "18": "4.",
            "19": "In Section 3.1.2, Sampling trajectories, the authors describe the way they use to sample trajectories.",
            "20": "However, to my understanding, the loss shown in Eqn.",
            "21": "5 is a game-step-wise loss.",
            "22": "Does the authors also sample game steps from the sampled trajectories (if so, how?",
            "23": "), or they compute this loss on all game steps within the sampled trajectories?",
            "24": "5.",
            "25": "In the paragraph under Eqn.",
            "26": "2, the authors mentioned that \"Note that the action distribution over actions $a$ induced by $\\pi_{inv-dy}$ is conditioned only on the current observation $o$\".",
            "27": "However, according to Eqn.",
            "28": "6, it is also conditioned on $o'$, which is the next observation, i.e., $o_{t+1}$.",
            "29": "**References**\n1.",
            "30": "Keep CALM and explore: Language models for action generation in text-based games.",
            "31": "Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan.",
            "32": "EMNLP 2020.",
            "33": "2.",
            "34": "Reading and acting while blindfolded: The need for semantics in text game agents.",
            "35": "Shunyu Yao, Karthik Narasimhan, and Matthew Hausknecht.",
            "36": "NAACL 2021.",
            "37": "---------------------------- Nov 29, 2021\nWe had a good discussion among reviewers, let me give the authors some update.",
            "38": "**1.",
            "39": "Increased my score to 6.",
            "40": "** This is because the authors have somewhat addressed my comments, I'm relatively satisfied.",
            "41": "There are a few concerns remaining, as listed below:\n\na) on modelling novelty: the novel components are only a) the sampling strategy in exploitation phase and b) the two-phase pipeline.",
            "42": "It was a bit weird to (almost) \"copy and paste\" a subsection from a prior work into the main body of this submission, which may confuse readers by giving a false message about the contribution.",
            "43": "However, if other co-reviewers are fine with it, I'm fine too.",
            "44": "b) After a few paper updates, the main results (in Table 1) is only marginally higher than prior work.",
            "45": "The authors can add more discussion addressing this in their cameraready.",
            "46": "**2.",
            "47": "We recommend the authors to remove the Dragon row from the result tables** (or rerun when the Jericho team fixes the bug):\n\nAs Reviewer PsKh find out, the proposed agent's scores exceed the max score on that game.",
            "48": "I happen to know some core Jericho contributors, and we tested the [Dragon game](https://ifdb.org/viewgame?id=sjiyffz8n5patu8l).",
            "49": "Usually, when reaching the goal, this will pop up:\n\n```\nDragon's Treasure Store\n\nThe Dragon's secret hoard is open before you.",
            "50": "By the flickering light of your little candle, you can make out a heaps of treasure stacked untidily around the floor.",
            "51": "You can see piles of gold and heaps of jewels, many rising higher than the top of your head.",
            "52": "The Dragon has told you it has no use for the treasure and it is now yours.",
            "53": "You are rich beyond your wildest dreams!",
            "54": "*** You have won ***\n\nIn that game you scored 25 out of a possible 25, in 101 turns.",
            "55": "Would you like to RESTART, RESTORE a saved game, UNDO your last move, give the FULL score for that game or QUIT\n```\n\nIn that game, the scoring function works like this:\n```\n1 for buying the box\n1 for finding the screwdriver\n2 for finding the candle\n1 for finding the matches\n1 for for opening the castle door\n1 for building the hand-glider in the right place\n2 for getting the sword/booklet\n1 for escaping from the tower using the hang-glider\n2 for killing the Troll to get the horn\n5 for talking to the Troll to get the horn\n2 for killing the dragon\n5 for charming the dragon instead of killing him\n5 for finding the treasure\n\n= 25 points maximum total (e.g., multiple ways to get the horn)\n\n(minus 2 points for each RESCUE or 'dead' recovery)\n```\nGiven this -2 points for each RESCUE action, an agent can get negative total points.",
            "56": "Because the original game did some *short* to *unsigned char* converting, this caused underflow (-128 vs. 128).",
            "57": "This may because the author of the Dragon game (in 2003) didn't expect machines to play his game, because most humans will give up playing before reaching this underflow point :)\n\nSo the weird numbers are not the authors' problem.",
            "58": "As I mentioned, they can either remove that row, or to rerun whenever Jericho fixes that."
        },
        "xMrrbzgABl_": {
            "0": "The main contribution is an exploration strategy with an in-episode switch from an exploitation policy to one aimed at exploration.",
            "1": "This approach to combining exploration and exploitation is different from much of the existing literature, where typically a single policy is used throughout the episode, and often throughout training, that merges two reward signals.",
            "2": "Since the switching policy in this paper is the element that looks most hardcoded, and therefore potentially brittle, it would be valuable to investigate a bit more whether a more flexible solution is also possible here.",
            "3": "While different, Agent57 (whose predecessor NGU is cited) might offer inspiration here: it also uses multiple policies, and manages the switching with a learned (bandit) mechanism.",
            "4": "A significant difference is that there the switching only happens between episodes, but a similar switching mechanism might be considered here within episodes nonetheless.",
            "5": "The in-episode switch is there to ensure that exploration happens at the edge of the known region of state space, where it is needed and meaningful.",
            "6": "That is a very sensible thing for the agent to do, but there are other exploration strategies that effectively also do that, such as random network distillation (Burda et al., 2018) and inverse dynamics (Pathak et al., 2017), which the authors use to train their exploration policy.",
            "7": "While the exploration region is less explicitly located at the edge of the known state space region in those algorithms than in this paper, the prediction errors that they rely on for intrinsic reward generation are more likely to occur at that edge.",
            "8": "One question I have for the authors here is whether the inverse dynamics reward signal itself can be used to indicate when to switch from explore to exploit.",
            "9": "In that case, the two-policy solution can be simplified again to a single policy that merges the two behaviours.",
            "10": "I did not see this ablation in the paper, but I believe it would be a good thing to include.",
            "11": "Conversely, it would be valuable to see the performance of the strategy proposed here on other exploration benchmarks, such as the hard exploration games from the Atari suite (Bellemare et al., 2016).",
            "12": "While I appreciate that text adventure games are in some ways different from their video counterparts, since they have a different observation space (language, not pixels) and action set (again language, not moves), they are still both RL environments, and general agents should be able to play both.",
            "13": "Furthermore a game like Montezuma’s Revenge has a bottleneck aspect similar to the one that many text based games have, as well as the need for exploration on the frontier of the known region of state space.",
            "14": "All in all it seems that the proposed strategy here could work on a wider range of environments than addressed in the paper.",
            "15": "If that is not the case, it is still a valuable contribution, but if it is, it would be good to know.",
            "16": "A last comment: the agent proposed in the paper has another unusual feature in that its exploitation policy is trained only by self-imitation.",
            "17": "While it is important to find the edge of the explored region of state space, and the self-imitation training regime can help with this, the XTX strategy can also be implemented with an exploitation policy that is trained in a more traditional way, with one of the many RL approaches available.",
            "18": "Can the authors comment on why they chose the self-imitation approach instead?"
        },
        "w95EgrMj1TL": {
            "0": "This paper is well motivated and most parts are well written, but the main method section is written to be difficult to follow.",
            "1": "The results demonstrate empirical gains in the Jericho environment.",
            "2": "However, the baselines consist only of simple algorithms without an exploration strategy.",
            "3": "The detailed comments and questions are as follows:\n\n1.",
            "4": "In the experiment, the performance is compared with DRRN and MPRC-DQN, which lack exploration strategy.",
            "5": "XTX seems to be an exploration method very similar to Go-Explore.",
            "6": "Moreover, in the paper, Go-Explore and PC-PG are mentioned as the most closely related approaches, but they are excluded from the baseline algorithms.",
            "7": "It would be better to demonstrate the results of them together.",
            "8": "2.",
            "9": "(Page 5, section 3.1.2, sampling trajectories) It is hard to follow the explanation.",
            "10": "Can it be understood as a kind of weighted behavior cloning?",
            "11": "Moreover, I understand the motivation of biased sampling towards high scores, but don’t understand the motivation for the length.",
            "12": "I think that a shorter trajectory length is not necessarily better.",
            "13": "Can you give an intuitive explanation?",
            "14": "3.",
            "15": "In the paper, policy $\\pi_\\text{il}$ is modeled as GPT-2, and policy $\\pi_\\text{inv-dy}$ is modeled as DRRN.",
            "16": "Is there any reason why each policy is modeled differently?",
            "17": "Especially, the policy $\\pi_\\text{il}$ is renormalized over the valid action set, is there any reason or advantage to learn the policy with GPT-2?",
            "18": "4.",
            "19": "In the experiments, the results demonstrate XTX underperforms DRRN on ENCHANTER.",
            "20": "Is there any intuitive explanation for this result?",
            "21": "It would be better if a discussion about what characteristics in the ENCHANTER made the XTX not work would be added."
        },
        "UITF31OhBG0": {
            "0": "Pros:\n\nThe paper is generally well-written and easy to follow.",
            "1": "The novelty of XTX is clearly elaborated.",
            "2": "The method surpasses the existing method with a large margin on text-based games.",
            "3": "The ablation studies show the individual components introduced by XTX can bring improvements.",
            "4": "Cons:\n\nOne weakness of the paper is these experiments did not clarify why the novel part of XTX (i.e.",
            "5": "exploration with novelty bonus on the frontier) is helpful over random actions.",
            "6": "The paper hypothesizes that novelty bonuses can encourage the agent to select promising actions in large action spaces.",
            "7": "However, the ablation study (Figure 2) casts doubts on this hypothesis.",
            "8": "XTX brings significant improvements over Go-explore in Zork1 but not other games.",
            "9": "The difference doesn't seem to be correlated to the size of action spaces.",
            "10": "Questions:\n\nI don't fully get why the method is motivated to solve the problems with large action space.",
            "11": "How can an agent receive a novelty bonus if it did not enter that novel state by trying random actions?",
            "12": "Do the authors assume the generalization of the neural network plays a key role here?",
            "13": "Other Suggestions:\n\nThe author might want to try other hard-exploration tasks.",
            "14": "For example, minigrid or maze can be tested, if not Atari games like Montezuma Revenge.",
            "15": "Since these are environments where existing exploration methods are developed, we can have a better understanding of how exactly XTX compares to other exploration algorithms, rather than the existing text-base game agent without directed exploration."
        }
    },
    "_LNdXw0BSx": {
        "2gWdWN4Kcp9": {
            "0": "Strengths and Weakness:\n1.",
            "1": "The proposed approach for tackling entity consistency and coherency through the use of memory structures that augment language modeling is novel and interesting.",
            "2": "Further, the dynamic nature of the proposed entity memory network allows multiple attributes to be associated with an entity.",
            "3": "2.",
            "4": "One of the contributions of this work is the proposed evaluation metric for entity coherence and consistency.",
            "5": "The metric for entity coherence based on calculating C_i as the difference in the indices of first and last mentions does not truly capture coherence.",
            "6": "Further, the example provided in the paper Fig 1b seems to be an issue with local coherence instead of long-range entity coherence.",
            "7": "3.",
            "8": "With regards to entity consistency, the authors propose quantifying it through the use of attributes associated with each entity is interesting.",
            "9": "However, this approach does not take into consideration multiple entities in a sentence and attribution association in those cases.",
            "10": "4.",
            "11": "Evaluation is done on the standard datasets of writing prompts and wikiplots.",
            "12": "However, it would have been better if the authors had used more baselines to compare their proposed approaches instead of a simple LM for both datasets.",
            "13": "From the automated metric perspective, the proposed addition of entity network seems to be performing worse than a vanilla LM on the wikiplots datasets on the metrics as shown in Table 1.",
            "14": "This trend of vanilla LM outperforming the proposed approach is evident in the proposed automated metrics in Table 2.",
            "15": "5.",
            "16": "The proposed automated metrics when compared to the human evaluation show a low to moderate correlation to human judgments.",
            "17": "The correlation for consistency is pretty low which raises questions about the proposed metric and its effectiveness to capture entity consistency.",
            "18": "Questions:\n1.",
            "19": "With regards to entity consistency, how were the attributes assigned if there are multiple entities mentioned in the sentence with attributes?",
            "20": "2.",
            "21": "Why does the attribute consistency on the wikiplots dataset using a simple vanilla LM outperform the proposed entity memory network augmented LM?",
            "22": "3. why is the scale for coherence different from the other metrics?",
            "23": "4.",
            "24": "How many human annotators evaluated the generated outputs?",
            "25": "What is the IAA?"
        },
        "l64HQEfAdu3": {
            "0": "Pros:\nThe motivation of the work which is relevant to a well-known issue in existing long text generative models has been written clearly and figures are very helpful in this regard.",
            "1": "Author try to contribute in two main problems first proposing automatic metrics to measure and analyze the coherence of generated texts by existing LMs and second proposing a model to improve the coherence of generated texts by augmenting an entity memory.",
            "2": "Different experiments and ablation studies show the effects of each component in the model from different perspectives.",
            "3": "Cons:\nThere are some not clear sections that need to be explained more clearly.",
            "4": "Here are a set of questions and comments that addressing them can be beneficial in better understanding the paper.",
            "5": "Main question is regarding different forms of the same entity and how they are handled in the model.",
            "6": "Assume we have one entity but in two different forms such as mom and mother throughout the plots, according to what has been explained in the paper model considers them as two separate entities.",
            "7": "If this is true, what is the impact of language different forms of words on the model's performance?",
            "8": "For automatic evaluation of coherence of texts the C metric only considers the first and last happening location of the entities in the text, while this is very abstract level and can not be a good and fair representative of the coherence in the text level.",
            "9": "There are some recent works that try to assess the coherence of the generated narratives by using the plots and manipulating them (Ghazarian et al.",
            "10": "2021).",
            "11": "In order to show the positive impact of proposed evaluation metric it is recommended to compare with more recent works.",
            "12": "In the method section, since the main idea of the work is about entity memory, presenting some instances for plots/entities/attributes  can help author to better follow the steps and the proposed model.",
            "13": "According to table 1, if the model can have access to the whole context then there is not that much difference between the proposed model and the vanilla LMs?",
            "14": "Therefore if we use LMs that can process long texts such as longformer then we won't have benefits coming from the entity coherence proposed by this model?",
            "15": "For human evaluation, how do you ensure the quality of the annotations are good?",
            "16": "what are the agreements between human judgments?",
            "17": "how many annotations have been collected for each narrative?"
        },
        "qaA9gSQvpd6": {
            "0": "Strengths:\n1.",
            "1": "The authors address important problems of LMs on entity coherence and consistency when generating long stories.",
            "2": "2.",
            "3": "The work proposes new automatic metrics for analyzing narrative generation where some show high correlation with human evaluation.",
            "4": "3.",
            "5": "The work proposes an improved pre-trained LMs for narrative generation by adding an entity memory which shows competitive results compared with the Vanilla LM.",
            "6": "4.",
            "7": "Extensive results discussion, evaluation, and ablation analysis are presented to clearly show the performance of some of the proposed techniques.",
            "8": "Weaknesses:\n1.",
            "9": "The codes are not made publicly available for reproducibility.",
            "10": "2.",
            "11": "The proposed entity memory-based pre-trained LM is only compared with Vanilla LM.",
            "12": "It would be more interesting to show how the proposed model performs when compared with other state-of-the-art LMs such as BERT and the main prior related works of Clark et al.",
            "13": "(2018) and Ji et al.",
            "14": "(2017) that have been discussed in the introduction section.",
            "15": "3.",
            "16": "There is a lack of experimental support of why the proposed entity consistency metric is  needed since it gives lower correlation with human ratings in Table 4.",
            "17": "It would be better to add more discussion on why it can still be considered to be adapted.",
            "18": "4.",
            "19": "Minor weakness:\nFigure 1(a) is not easy to understand.",
            "20": "The <sep> tokens used do not clearly show where the entity starts and ends.",
            "21": "It is suggested that you prefer using <sep> and </sep> pair, where the former shows the beginning of the entity and the latter shows the end."
        },
        "F0Ek0N0PRUi": {
            "0": "Strengths: \n(1) The work is to address a challenging and practical problem in generating long narratives.",
            "1": "(2) The authors proposed two intuitive metrics to measure the entity coherence and consistency based on their mentions and context, which are beneficial to the following research.",
            "2": "(3) The proposed entity memory augmented model is reasonable and improved entity coherence/consistency in some of the experiments.",
            "3": "The authors also provided a detailed analysis of the models in various settings.",
            "4": "Weakness:\n(1) The two proposed metrics about entity coherence and consistency are intuitive but not rigorous.",
            "5": "The entity coherence is based on the occurrence of the entity mentions across different sections.",
            "6": "However, the coreference is accurate especially when the context is wide, and even an entity is mentioned across a wide context, there are a lot of other factors to consider to quantify the coherence, e.g., the intermediate mentions used in-between the first and the last mention.",
            "7": "Similarly, for consistency, the authors considered all verbs and adjectives as the attributes of the entity.",
            "8": "What's the reason?",
            "9": "and how can it measure the inconsistency in the example of Figure1 (b)?",
            "10": "(2) The model design is reasonable, but it depends on several key hyperparameters, e.g., the size of the entity memory or the context memory, the length of each narrative section.",
            "11": "They may have much impact on the final performance and on different datasets, they should have different choices.",
            "12": "Also, it seems the entity set is given to initialize the memory, then what if the model generates new entities?",
            "13": "(3) The results didn't consistently demonstrate the improvement of the proposed approach, e.g., in Table 2, on the WikiPlots dataset, the VanillaLM is better than other models for most of the metrics; and on the WritingPrompts dataset, the VanillaLM seems to generate more mentions in average.",
            "14": "(4) The analysis section is very difficult to follow.",
            "15": "Some analysis is just based on wordy descriptions without details or examples.",
            "16": "For example, in the automatic evaluation section, it's not clear how the uncertainty is computed, how to interpret the Figures, especially Figure 4, what does the control in section 5.2 mean?",
            "17": "and what are the detailed criteria for human rating (1-4)?"
        }
    },
    "5LXw_QplBiF": {
        "Nwc7HUgrQVm": {
            "0": "# Strength\n\n- It is a very interesting work on differentiable non-deterministic PDA in terms of the theoretical view point.",
            "1": "Although the gains are not large and computationally demanding, the proposed enhancements over prior work are very interesting in that they allow its application to the real World data, i.e., PTB.",
            "2": "# Weakness\n\n- The gains are not large, but meaningful improvements are observed in the synthetic data set and the gains in SG scores would be good enough, I believe.",
            "3": "- I feel the bound for stack could be treated as one of the contribution of this work.",
            "4": "I'd suggest moving the description of section 5.1 into section 3 to emphasize the nice contribution.",
            "5": "- I'd like to see further analysis to indicate whether the proposed model could learn hierarchical structures as indicated in the title under PTB.",
            "6": "Probably manual assessments would be good enough to show some evidence."
        },
        "qX86kWdll7": {
            "0": "The proposed method solves two defects of NS-RNN.",
            "1": "The ideas are technically sound and empirically proved.",
            "2": "However, I have several concerns:\n1.",
            "3": "The RNS-RNN is based on a weighted PDA, which has a finite set of states.",
            "4": "This setting limits the capacity and expressiveness of the proposed model, especially when modern deep learning methods usually use very large hidden states to achieve high expressiveness.",
            "5": "2.",
            "6": "The RNS-RNN achieves a marginally better result on PTB language modeling task while comparing to NS-RNN, but still falling far behind any strong baselines.",
            "7": "3.",
            "8": "The formal language tasks are toyish.",
            "9": "It would be interesting to test the proposed method on less toyish formal language tasks, for example, logical inference (Bowman et al., 2015) and ListOps (Nangia and Bowman, 2018).",
            "10": "Missing reference:\n- Shen, Yikang, et al.",
            "11": "\"Ordered Memory.\"",
            "12": "Advances in Neural Information Processing Systems 32 (2019): 5037-5048."
        },
        "5Z3YxvMGRge": {
            "0": "=quick pros=\n- proposed changes are simple, clear, and appear to improve the model on some synthetic tasks (even significantly for one of them).",
            "1": "=quick cons=\n- the model does not seem very good on natural language tasks.",
            "2": "However, if my understanding of stack-rnn research is correct, this is to be expected.",
            "3": "=full review=\n\nNote: I have reviewed this paper before, and appreciate that the authors have added experiments and metrics as requested.",
            "4": "I am adding some presentation comments, some of which are new, and some of which I think are important (even if they are minor), please take a look through them!",
            "5": "Note: I recognise you responded to some of these questions in the previous review round, but I would like them answered in the paper too!",
            "6": ":)\n\n==some comments==\n\n1.",
            "7": "I find the first change ((1) in the summary) interesting, if simple, and the second ((2) in the summary) a little odd: intuitively, because the RNN outputs a next-move distribution for *every* current top-token and current-state pair, why would it need to know what the current one *actually* is?",
            "8": "I am glad that you evaluated each change independently as well as together, and so I can be convinced that the change is necessary (e.g.",
            "9": "through the results on unmarked reversal), but still I do not fully understand it.",
            "10": "I would appreciate a greater discussion of *why* this change is good here - why does it help despite my intuition?",
            "11": "(The explanation of section 3.2 is not working for me: following my intuition here, I still don't see the importance of the state-read.)",
            "12": "2.",
            "13": "It is not clear what the SG score adds to the discussion, or what I should conclude from it, if at all.",
            "14": "It would help to add a definition of the SG score to the paper (as done for cross entropy), or at least some greater discussion if this is not possible, and provide some intuition on what it means.",
            "15": "3 (important!).",
            "16": "I appreciate that the authors have been very explicit in all of their constructions and all details of their experiments, I find this very valuable.",
            "17": "However, I would also like to note that as someone not familiar with the NS-RNN, I do struggle with the formulas.",
            "18": "In particular in equation 2 I would like some more explanation on the [i->t] inputs - it seems there are jumps over several time steps??",
            "19": "This is a very confusing point that could do with some plain-english explanation.",
            "20": "(3b.",
            "21": "Maybe personal preference: Another thing to note in this direction is that the last paragraph of the introduction describes the RNS-RNN according to how it differs from the NS-RNN, but for anyone who is not already familiar with the NS-RNN, it is not useful.",
            "22": "If possible, I think adding another paragraph that more deeply explains---in non-technical terms (i.e., no formulas)---what exactly an NS-RNN is doing would be very helpful.",
            "23": "(Approximately, though will need rephrasing: defining and maintaining a distribution over all possible configurations of a non-deterministic PDA, by computing at each step: for each state and stack-top combination, a distribution over all possible next-state and stack-actions.",
            "24": "And from this, each configuration's probability is the sum of the products of each sequence of transitions and state-actions that get to that configuration). )",
            "25": "4.",
            "26": "In figure 3, it would be nice to also see this plot for NS, JM, Gref, and NS+U!",
            "27": "Maybe in the appendix at least?",
            "28": "5.",
            "29": "Section 4, paragraph after Hardest CFL: \"...hardest CFL requiring the most.\"",
            "30": "I don't understand what this means - what does it mean for one non-deterministic language to require \"more\" non-determinism than another?",
            "31": "==some questions==\n1.",
            "32": "In unmarked reversal, it is interesting to note that each of the changes alone seems to barely improve on the NS-RNN, but that together they improve the model significantly.",
            "33": "Could you add a discussion, sharing any intuition on why this is?",
            "34": "(Section 3.2 is not enough - at that point I still do not feel I understand why the second change is needed, as noted above in my review).",
            "35": "2.",
            "36": "In figure 3, there is a white line in the middle of the image for both models, suggesting that it is not learning the correct action at the point where the reversing 'begins'.",
            "37": "Could you add some discussion explaining this?",
            "38": "3.",
            "39": "Can you explain the intuition behind adding the EOS tokens to the end of each sample (as opposed to the experiments in the original NS-RNN paper)?",
            "40": "I understand that it makes the model define a proper distribution (as opposed to weights) over the sequences, but I don't understand why this is important for these evaluations.",
            "41": "(Is predicting the EOS hard for any of these tasks, or something like that?",
            "42": "I see that you comment that it makes things harder for the baseline NS, do you have ideas on why?)",
            "43": "==typos, presentation, minor comments==\n1.",
            "44": "Section 2.3 (superposition) is hard to understand, an analogy (similar to cake for stratification) could help.",
            "45": "If one is not available, at least a concrete example, e.g., what happens if I have a stack of depth 5 and push/pop/noop with probabilities 0.3/0.5/0.2 ?",
            "46": "2.",
            "47": "Section 2.4, second paragraph, the description of the possible operations: the paragraph says that it is showing Op(Gamma), but it seems you are showing Q \\times Op(Gamma).",
            "48": "Consider removing the \"r\"'s from each operation, or introducing the list as Q\\times Op(Gamma).",
            "49": "3.",
            "50": "Formal languages results: Can you explain the higher difference in cross entropy, for all models and languages, on the shorter (but in train range) sequences?"
        },
        "83zw0h6nxhH": {
            "0": "Strengths\n- The discussion on previous stack RNNs was a strong segment of the paper as the authors connect the various approaches using common notation.",
            "1": "- The modifications to the NS-RNN are well motivated by appealing to toy problems or an explanation of what happens numerically during training.",
            "2": "- Transitioning the discussion from formal languages to natural languages was a good motivation for introducing the memory-limited technique.",
            "3": "Good use of SG score metric instead of just perplexity.",
            "4": "Weaknesses\n- It would have been good to explain what Hardest CFL (Greibach, 1973) is in the paper.",
            "5": "**[addressed by author response and paper update]**\n- I might be missing something, but what is the intuition behind the choice of $Q$ or $\\Gamma$ for the models in Table 1?",
            "6": "**[addressed by author response and paper update]**"
        }
    },
    "US2rTP5nm_": {
        "6BcIANkl6i3": {
            "0": "This paper is clearly written, has a working original idea, and provides a great new option for entity linking that has empirical promise.",
            "1": "It's in good shape, and I don't have a lot of criticisms.",
            "2": "Just a comments and questions below:\n\nPage 1 - \"End-to-end models alleviate the problem of error propagation, but the search is only approximate and the dilemma, albeit to a lesser degree, remains\" - I think this sentence need some additional justification.",
            "3": "Sure, search is approximate.",
            "4": "That on its own doesn't mean that the dilemma remains.",
            "5": "How would you establish that the dilemma remains?",
            "6": "Page 3 - \"In our experiments we simply set ψtopic(x) = x1 ∈V (i.e., the first token in the document).\"",
            "7": "That seems odd.",
            "8": "The first token may not be very useful.",
            "9": "It could be \"The\".",
            "10": "Could you use the encoded first phrase?",
            "11": "Page 3 - \"We write ⊕to denote the text concatenation (we insert a special symbol to represent the concatenation)\" I don't understand the parenthetical.",
            "12": "Can you elaborate?",
            "13": "The bad threshold suggest that the model is perhaps not calibrated in some key way.",
            "14": "Could an explicit calibration step on the validation set help fix these errors?",
            "15": "More broadly, the fixed threshold seems like the weakest part of the system.",
            "16": "What ideas do you have about making it more flexible or adaptable?",
            "17": "Did you try other approaches to thresholding and found them to not work?",
            "18": "These findings would be good to include in the paper."
        },
        "7UuS46-EtBn": {
            "0": "Strength:\n* The novel entity linking paradigm proposed by this paper is insightful and the performance gain is significant.",
            "1": "* The proposed paradigm addresses a key issue in prior entity linking works.",
            "2": "In EntQA, mention detection happens after entity detection, so that mention detection can utilize entity information.",
            "3": "* The proposed QA paradigm allows EntQA to take advantage of existing question answering datasets.",
            "4": "Pretraining on open domain question answering datasets may improve the generalizability of EntQA.",
            "5": "So not surprisingly, EntQA performs better on out-of-domain datasets.",
            "6": "* The proposed method has several practical advantages.",
            "7": "It costs less GPU hours and memory space in comparison with prior SOTA according to the paper.",
            "8": "However, providing the exact number will be much better.",
            "9": "Weakness: \n* When discussing the difference between entity linking paradigms, this paper doesn’t compare with GENRE [1], although it is listed as a baseline.",
            "10": "As the main contribution of this paper is the novel paradigm, I would like to see a section instead of a brief paragraph in introduction to discuss the theoretical and empirical difference between different paradigms.",
            "11": "However, considering that Table 1 already shows the proposed paradigm performs better than other paradigms empirically, the theoretical analysis can be left for future work.",
            "12": "* The term ‘unknown entities’ is confusing, because it can also refer to entities not covered by KB.",
            "13": "I suggest the authors clarify the definition.",
            "14": "I think it is better to say ‘predict mentions when not knowing the corresponding entities’ than ‘predict mentions of unknown entities’.",
            "15": "Besides, I wonder how mentions of entities not covered by KB will influence different paradigms, although it may be a minor issue.",
            "16": "* The effectiveness of EntQA is only shown by overall performance.",
            "17": "It would be better to have experiments to compare the mention detection results of different methods.",
            "18": "[1] De Cao, N., Izacard, G., Riedel, S., & Petroni, F. (2020, September).",
            "19": "Autoregressive Entity Retrieval.",
            "20": "In International Conference on Learning Representations."
        },
        "6h2rzWTwkHW": {
            "0": "This paper addresses a problem of entity linking.",
            "1": "The authors propose a reformulation of the problem as an inverted open-domain Question-Answers (QA) The authors used dense retriever in the first phase of their algorithm (retrieving top K entities) and then in the second phase authors trained reader mouse for ranking and extracting entity mentions from the output of the retriever.",
            "2": "The main advantages of the proposed method (besides outperforming previous strategies) is the data efficiency gained by eliminating dependency on hardcoded mention-candidate dictionary.",
            "3": "Instead it finds entities from text passages and then finds lined mentions in test passages.",
            "4": "Authors work comes with strong backing from trying they model on GERBIL benchmark platform (trying it on various datasets) I especially appreciated the authors explaining the reason why their method did not do as well on OKE15/16 datasets compared to how it performed on other datasets.",
            "5": "Regarding the reproducibility of the results, the authors did not share their source code so it might take some effort to reproduce what they have done.",
            "6": "On the other hand, they provided very detailed description of their training process including very helpful Model details section with lot's of relevant details parameters used with training and inference.",
            "7": "Authors provided very insightful discussion of their results.",
            "8": "In addition to what's been provided I would be interested to learn how quick is the inference and how the time to run inference is impacted by the choice of K."
        }
    },
    "ek9a0qIafW": {
        "ls2airr7Xoj": {
            "0": "Strengths:\n - The paper is generally well-written, with excellent motivation and empirical setup/analysis\n - The overall strategy of differentiable prompt optimized to maintain fluency is reasonable and novel.",
            "1": "- The ablation experiments and optimized prompt analysis are insightful.",
            "2": "Weaknesses:\n - The notation/description of section 4 is not immediately intuitive.",
            "3": "I had to read it a couple of times before I could fully follow the method.",
            "4": "The authors could add more content to figure 1, which may resolve this issue.",
            "5": "- The choice in deciding how many template tokens are used is unclear.",
            "6": "An additional ablation experiment trying different number of tokens to optimize could be illuminating (even for just 1 dataset).",
            "7": "- The ablation study would be better if specific numbers were provided.",
            "8": "- I know GPT3 access is hard to get, but I wish experiments with k=8 prompts could be compared against\n\nPost rebuttal: I think another pass for clarity over the paper would be good for the final version, but otherwise I'm happy with the paper updates and I'm happy to see the ablation numbers aren't too sensitive to token count.",
            "9": "I have updated my score to reflect this."
        },
        "zMCCAwyozPJ": {
            "0": "Strengths:\n\n1) The paper proposes an interesting framework for few-shot fine-tuning without any prompt engineering.",
            "1": "This approach can be easily used with other pre-trained models, which makes it a more generalizable framework.",
            "2": "2) The problem space is well defined (and also well contrasted with previous work), and the paper is clearly written and easy to follow.",
            "3": "Weaknesses: \n\n1) Even though the results in Table 3 for relation extraction and event extraction datasets suggest that the proposed methods perform significantly better than LM-BFF, the results on the 10 popular datasets seems to be very close to LM-BFF.",
            "4": "From the results, only two datasets (SST-2 and MR) seem to have significant improvement over LM-BFF.",
            "5": "It would be great to further discuss these results in detail."
        },
        "jiUwp09W1Ep": {
            "0": "Strengths of the paper:\n- Clearly written and easy to follow.",
            "1": "- A simple, scalable, and effective approach of prompt tuning has been proposed.",
            "2": "- Importance of differentiable label has been well demonstrated.",
            "3": "Weaknesses of the paper:\n- Missing information: Because the proposed Fluency Constraint Object is based on MLM, GPT-2-medium (in section 5.5 and figure 2-b) cannot use this objective.",
            "4": "Comments:\n- In section 4.5, the authors said that their method requires no external parameters.",
            "5": "But this claim could be controversial since adding new tokens (for prompts and labels) on the pre-trained language models' token embedding seems more efficient than pre-training language models that already have unused or special tokens in their token embedding.",
            "6": "I know many PLMs already have unused tokens in practice, but I would like to treat this claim theoretically.",
            "7": "- For GLUE tasks, I'd like to know the performance of DART with full training set.",
            "8": "- Comparing with Prefix-tuning and WARP in experiments would be great.",
            "9": "But this is not mandatory.",
            "10": "- In figure 3 and 4, training iteration or epoch should be presented."
        },
        "YyFIWwaDP-2": {
            "0": "Strengths\n- Very clearly written, the key ideas were well explained and simple.",
            "1": "- The results seemed convincing.",
            "2": "- As far as I can tell, all related works have been covered.",
            "3": "Weakness\n- I am bit unconvinced about claiming that this work requires no external architecture in Table 1.",
            "4": "There are new parameters that are getting trained/fine-tuned before we can do inference.",
            "5": "The main difference I see between this and another approach like p-tuning is that the most of the existing architecture is getting reused vs in p-tuning there is explicitly an external LSTM.",
            "6": "- It would've been nice to see what happens if we constrain the additional tokens to be a part of the same vocabulary.",
            "7": "This is more or less the approach taken in AutoPrompt, except with the current training methodology.",
            "8": "Questions\n- I don't think I quite understood Sec 4.3, based on this section, it looks like there is an explicit embedding of output classes into an embedding?",
            "9": "How are these learnt without fine-tuning?",
            "10": "If they are learnt from scratch, then how is this different from regular fine-tuning?",
            "11": "- While the work claims that it reduces the need to fine-tune, I am seeing this work as a point on the spectrum between effort-to-tune vs zero-shot.",
            "12": "Engineering prompts is truly zero-shot, P-tuning is somewhere in middle that it requires an external LSTM, whereas this approach requires retraining the big model (modulo all parameters except T1, T2.. held constant).",
            "13": "Is this fair to say?",
            "14": "How computationally easy is it to actually fine-tune these unused embeddings vs learning an external model\n\n- How do we decide the number of unused tokens T1, T2..  to use?",
            "15": "- How much does this work translate to a completely novel task beyond classification?",
            "16": "e.g.",
            "17": "let's say constituency parsing or dependency parsing task, for which it is possible to engineer a novel prompt."
        }
    },
    "PtuQ8bk9xF5": {
        "Tq6L2NPDwhu": {
            "0": "**Strengths**\n\n- The method achieves the state-of-the-art performance on the ALFRED benchmark, which is quite challenging.",
            "1": "- The set of ablation experiments are informative.",
            "2": "**Weaknesses**\n\n- The paper is heavily engineered for the ALFRED benchmark.",
            "3": "I am not sure if any of these modules can be used for generic embodied tasks.",
            "4": "- The paper makes a big deal about the \"affordance-aware\" representation in the introduction and other places in the text, but it is actually a simple heuristic that the agent backs up a few steps when it is next to an object.",
            "5": "According to the text \"the agent will back up 3 steps if the target object is a fridge, 2 steps if it is a safe, a cabinet or a drawer, and 1 step for everything else\".",
            "6": "These types of heuristics are not generalizable or scalable.",
            "7": "The affordance-aware representation should be learned instead.",
            "8": "- There is a strong assumption that \"we know the changes (∆x, ∆y, ∆r) of the agent after each action\".",
            "9": "These types of assumptions are valid only in simulation.",
            "10": "Several previous works have made similar assumptions.",
            "11": "However, those approaches are suitable only for simulation.",
            "12": "In reality, the estimates for agent pose are quite noisy.",
            "13": "I would like to see how robust the proposed approach is against noise in the agent pose.",
            "14": "- It is not clear if the exploration steps and the steps used for capturing panoramic views are included in the evaluations."
        },
        "usPN-RzIGB5": {
            "0": "strengths:\n- results are much better than state-of-the-art and the improved margins are significant.",
            "1": "- the proposed technical module of learning an affordance-aware semantic representation is valid, reasonable, and somewhat novel.",
            "2": "- the system designs are really solid in mixing up many well-designed heuristics and learned modules into a big system that produces very good results.",
            "3": "weaknesses:\n- my biggest concern is that the paper presentation is not selling the framework well as an easily-readable research paper, for example, a) there are no qualitative figures showing the performance of the work, only tables with numbers are presented.",
            "4": "It is quite important to show qualitative results and your qualitative comparisons to baselines, additionally with some analysis figures clearly proving the effectiveness of each proposed module, for readers to be better convinced where your superior performance comes from.",
            "5": "b) the writing for Sec.",
            "6": "4 can be improved.",
            "7": "The current writing is quite messy mixing up the two contributions together.",
            "8": "Also, many important details (e.g.",
            "9": "the architecture to learn the affordance maps and the aggregation steps) are put in supplementary.",
            "10": "c) for the experiments, the authors spent many more words on Sec 5.1 and 5.2 than Sec 5.3 (the main results).",
            "11": "The main results subsection is very short and does not contain analysis over the tables or show qualitative figures to analyze the results.",
            "12": "- another of my major concerns is whether or not using the language instructions during the exploration stage is a reasonable and realistic setting.",
            "13": "My opinion is that we should not assume we know the task (goal) and the instructions for subgoals beforehand.",
            "14": "and during the exploration.",
            "15": "Otherwise, we need different exploration steps given different tasks.",
            "16": "Or, at least, you need to report your exploration steps as part of the task execution steps, since you need different exploration steps given any new test task.",
            "17": "Please clarify if I'm wrong with this.",
            "18": "But, the confusion regarding this point prevents me from recognizing the claimed technical contribution on leveraging multimodal inputs during exploration.",
            "19": "- though there are some novel designs for the affordance learning part, there are previous works (Qi et al, 2019, and Nagarajan & Grauman, 2020) that have proposed the essential ideas, and I don't see too much difference other than adding some heuristics-derived waypoint definitions.",
            "20": "Please clarify if I missed or misunderstood the main differences from previous works.",
            "21": "- though I recognize that the presented system achieves amazing results and significantly refreshes the state-of-the-art, it's unclear how much performance gains come from combining many state-of-the-art architectures, say transformers or bert, and how much comes from the claimed two novel technical design points.",
            "22": "Can you compare to baseline methods with the same architectures or report the parameter amounts for different models, to give us a sense of this?",
            "23": "- one minor issue is to use \\citep instead of \\cite in latex for a more valid citation format of papers"
        },
        "099LIraOPpL": {
            "0": "Strengths:\n- This submission is well motivated.",
            "1": "Training robotic assistants to follow human instructions to complete an interactive task is an important and challenging problem.",
            "2": "- The proposed method achieves good performance, improving 40% over prior published approaches to achieve 23.48% success rate.",
            "3": "Weaknesses:\n- The writing clarity and readability can be improved significantly.",
            "4": "The method seems extremely complicated and is very difficult to understand.",
            "5": "There are way too many forward and backward references in Section 4.",
            "6": "Even after reading over the methods section twice and going over the details in the Appendix, I am unable to understand many details in the method.",
            "7": "For example:\n    - In sec 4, what is meaning of module operating at the \"subgoal level\"?",
            "8": "In contrast to what in the E.T.",
            "9": "paper?",
            "10": "Why is the explored area considered an extra modality?",
            "11": "- In sec 4.1, how does the agent predict the task described in the language instructions?",
            "12": "How does the agent know the subgoals in each task?",
            "13": "How does the agent switch between exploration and execution phases?",
            "14": "What is the difference between planned and prediction actions?",
            "15": "- In Figure 1, it is unclear where and how the semantic map is used.",
            "16": "It is unclear where the explored area is coming from (my understanding is that the environment does not provide the explored area).",
            "17": "The caption refers to \"affordance-aware semantic representation\" which is not shown in the figure.",
            "18": "What is the meaning of the \"actual\" task?",
            "19": "- In Sec 4.2, what are the delta rule-based functions?",
            "20": "- In Sec 4.3, how does the agent know the which low-level instruction corresponds to the current navigation and subsequent object interaction subgoals?",
            "21": "- The method seems to use of lot of arbitrary choices, rules and hacks.",
            "22": "For example, \"we define a single-step explored region as a binary map where a 5×3 rectangle grid (as a hard-coded field of view)\", \"We manually inject 4 RotateRight after every 2 MoveAhead predicted by our module to acquire 360◦ views of the scenes\", \"we further inject two LookUp or two LookDown actions alternately after every exploration action\", \"The agent then goes through 6 horizons {60◦, 45◦, ..., 0◦, −15◦} and obtains the mask prediction (with confidence > 0.8)\".",
            "23": "All these choices seem to be specific to the Alfred benchmark.",
            "24": "I can not imagine a robot operating in the real-world in this manner.",
            "25": "My worry is that the authors have exploited unrealistic approximations in simulation environments (such as discrete action and state space, no noise in motion and pose sensors and use of high-level interaction actions), and over-optimized the design choices specific to this benchmark, which are likely to not result in better performance in more realistic settings.",
            "26": "- The technical contributions of this paper are unclear:\n   - The authors claim in the introduction \"we propose the first multimodal exploration module that takes language instruction as guidance and keeps track of visited regions.\"",
            "27": "I believe the Active Neural SLAM model, which AMSLAM is based on, also keeps track of visited regions to learn exploration.",
            "28": "Based on authors' definition, even Active Neural SLAM is multimodal as it used pose, explored area and visual observations as input.",
            "29": "Is the addition of language instructions as input the only change to the model?",
            "30": "- The authors claim they introduce \"Affordance-aware semantic representation that estimates object positions, heights, and relative spatial relationships\".",
            "31": "I believe the semantic representations in Chaplot et al.",
            "32": "2020b and Blukis et al.",
            "33": "2021 also estimate object positions, heights and relative spatial relationships.",
            "34": "It is unclear what makes the proposed representation \"affordance-aware\"."
        }
    },
    "yuv0mwPOlz3": {
        "m1H1fmwFja2": {
            "0": "Reasons to accept:\n\n- The study has a very ambitious goal.",
            "1": "In the framework that the problem is defined, the authors, in my opinion, do a good job of defining sub-problems and validating the claims.",
            "2": "- The paper is framed and written well.",
            "3": "The study inherently has many parts, consequently this makes following up the content a bit difficult.",
            "4": "But it appears that the authors have recognized this challenge and structured the paper to minimize this issue for the reader, e.g., summarizing the findings at the beginning of each section.",
            "5": "- The paper is very detailed.",
            "6": "Namely, the experiments are thorough, the discussions are extensive, and the references are comprehensive.",
            "7": "Reasons to reject:\n\n- My first concern is about the topic itself.",
            "8": "As a person who has experience in text classification.",
            "9": "Has done research in Active Learning and also in Domain Adaptation.",
            "10": "I am not clear in what real world situations one might need to do active learning with multiple-source domain adaptation.",
            "11": "The authors cite some papers as the studies in this area.",
            "12": "Some I already knew, and some I specifically checked.",
            "13": "But to my knowledge none of them actually did research in this exact topic.",
            "14": "Please note that the problems of label shift or dataset shift are different from what the authors are proposing here.",
            "15": "At the beginning of the introduction section the authors provide a very short description.",
            "16": "But I am still not clear about this.",
            "17": "On the other hand, if this problem applies to only some very specific settings, then why not specifically write the paper for those settings.",
            "18": "Because, currently the paper claims that the proposed problem is a general and common challenge.\\\nAnother issue that further adds to the confusion is the data split used in the experiments.",
            "19": "The authors assume each target domain already has several thousand labeled documents.",
            "20": "So I can again ask, under what text classification scenarios there are several thousand labeled documents available but there is no unlabeled document available in that domain, so that we may be forced to use unlabeled documents from several other sources.",
            "21": "- My second concern is regarding some of the methods used in the experiments.",
            "22": "I understand that preparing such a paper is difficult, but still to me it was not initially easy to relate the KNN methods and active learning.",
            "23": "Or H-divergence methods and active learning.",
            "24": "Of course, KNN relates to representativeness metric and H-divergence relates to data sampling.",
            "25": "But the way that the authors frame these methods implies that these are commonly used in active learning methods.",
            "26": "I disagree with the authors, the connection is not initially obvious at all.",
            "27": "Another method used in the paper, which is still unclear to me, is to rank data points in an ascending order of uncertainty.",
            "28": "It is still not clear to me why one should select the points with most confident label predictions.",
            "29": "this contradicts the active learning purpose.",
            "30": "- My third concern is regarding the experimental setting: 1) The correct way of comparing active learning methods is to compare the learning curves.",
            "31": "The authors only compare the final F1 measure.",
            "32": "2) None of the methods used in the paper are actual domain adaptation models.",
            "33": "The authors simply aggregate the source and target data points to create a training set.",
            "34": "This is not Domain Adaptation.",
            "35": "3) In H-divergence methods, authors use a decision tree-based classifier as discriminator.",
            "36": "On the other hand the vectors are distributed representations, and the features are not independent.",
            "37": "Tree-based classifiers assume feature independence.",
            "38": "Other less severe issues:\n\n- Several questions that authors pose to empirically answer remained open and without answers, e.g., the questions in Section 6.2.",
            "39": "There is nothing wrong with reporting failed expectations and failed methods.",
            "40": "These are still informative.",
            "41": "But when the number of these go up, it indicates that some of the choices were initially incorrect.",
            "42": "This further confirms my concern about the lack of relation between some of the methods used in the paper and active learning.",
            "43": "- When the authors describe the method BALD, they cite a paper and state that the paper used dropout to quantify uncertainty.",
            "44": "The paper that actually used dropout for this purpose is a different one [1].",
            "45": "- The two tasks used to evaluate the AL methods are very similar in nature.",
            "46": "- Not sure why the authors have used bold faced fonts so frequently.",
            "47": "They also have used red font a few times, which is not necessary.",
            "48": "[1] Gal and Ghahramani.",
            "49": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning.",
            "50": "ICML 2016."
        },
        "p2c4AQ7fNVP": {
            "0": "Overall, I like this work since it serves as a guide for practitioners faced with similar settings, and gives good guidelines on what works in these challenging domain shift settings.",
            "1": "The experiments are  extensive and cover many of the domain shift settings in NLP.",
            "2": "However, I think some of the analysis could be improved.",
            "3": "Weaknesses: \n- I like the analysis on measuring how different methods rank examples.",
            "4": "However, I would highly recommend performing some kind of normalization - e.g.",
            "5": "to compare method a1 from family A and method b1 from family B, kendall’s tau score should be normalized w.r.t the score between methods from the same family.",
            "6": "- I would like to challenge some of the claims made in Section 6.2 of the paper.",
            "7": "First, while it does seem like a diversity of examples helps, the boost is *very minimal* and it is certainly not true in general.",
            "8": "Indeed, one can construct a domain distribution where getting data from several domains could hurt performance, if the mapping function (or the underlying P(y | x)) changes between domains.",
            "9": "Based on these weaknesses, I think the paper is currently borderline.",
            "10": "However, if these are addressed i’m happy to adjust my scores."
        },
        "3BFF41AI5jc": {
            "0": "Unlike most of previous work on active learning (AL) focusing on a single domain, this paper targets the multi-domain AL setting, a more realistic setting.",
            "1": "The paper presents an extremely thorough study of how different active learning methods (18 acquisition functions from 4 different families of methods) and rigorous experiments & analysis (correlation analysis among rankings of examples, searching for the best combination of source domains) provides insights to several key questions regarding multi-domain AL.",
            "2": "Overall, the paper is very well written and I quite enjoyed reading it.",
            "3": "However, with it being a great experimental report, my main concern is regarding the limited technical novelty of the work besides the two proposed simple variants (DAL-E derived from DAL-T, and RCA-smoothed derived from RCA, and all other are existing AL methods).",
            "4": "Plus, part of the analysis on AL is also covered by previous work (e.g., Lowell et al., 2019).",
            "5": "Also in terms of providing a empirical study to multi-domain AL, it appears [1] also studies a similar problems and offers similar analysis.",
            "6": "How much does the proposed work connect and differ from it?",
            "7": "This is not discussed in the paper.",
            "8": "[1] Multi-Domain Active Learning: A Comparative Study.",
            "9": "He et al.",
            "10": "https://arxiv.org/pdf/2106.13516v1.pdf (June 2021)"
        },
        "5vj6M5jcn0T": {
            "0": "This paper makes comparison with techniques used in active learning (AL), domain shift detection (DS), and multi-domain sampling to combine data from multiple sources.",
            "1": "The experiments are conducted on datasets from questions answering and sentiment analysis.",
            "2": "The paper is well organized and easy to follow.",
            "3": "However, the contribution of this paper is not clear.",
            "4": "Specifically, I would expect authors provide more detailed recommendation for AL, DS, and multi-domain sampling in terms of sampling techniques, and population of different sources for certain application.",
            "5": "My another is concern is that the motivation of the experimental design is not clear.",
            "6": "Why authors consider questions answering and sentiment analysis as the applications?",
            "7": "It requires more analysis about experimental results, such as Figure 1 and tables in Section D."
        }
    },
    "o9DnX55PEAo": {
        "xOwQ6qJCfDJ": {
            "0": "I am curious how it performs with a single CBOW or a single CMOW, not a hybrid one.",
            "1": "Did you try other sizes of matrix embeddings of size?",
            "2": "How does it perform?",
            "3": "Should the CBOW and CMOW embeddings have the same dimension size?",
            "4": "What are their individual functions for CBOW and CMOW?",
            "5": "Are they complementary or just redundant?",
            "6": "I know CMOW is from another paper, but as a reader, we might want to read a self-contained paper."
        },
        "worx0ExiHWu": {
            "0": "The main contribution of this work mainly lies in the bidirectional CMOW and a two-stage knowledge distillation method.",
            "1": "However, the technical contribution in bidirectional CMOW is quite limited, the related extension has been widely used in existing works, like bidirectional LSTM.",
            "2": "Furthermore, the two-stage knowledge distillation method was originally proposed in TinyBERT, a well-known work on BERT distillation, which is directly used in this work.",
            "3": "The proposed method has poor performances on several GLUE tasks, e.g., CoLA and MRPC, and many related baselines are not included, e.g.",
            "4": "TinyBERT, BERT-PKD, MiniLM.",
            "5": "The proposed method even does not have advantages on #parameters and inference time with comparison to 4-layer TinyBERT."
        },
        "T7m0J4s_zGl": {
            "0": "Strengths:\n1.",
            "1": "The proposed idea is novel: the CMOW model can only be used in sentence-level tasks.",
            "2": "The authors extend CMOW using the bidirectional variant to extract token-level representations, so now it can be used for the masked language modeling training objective.",
            "3": "2.",
            "4": "DiffCat is designed specifically for two-sequence tasks.",
            "5": "It improves the performance by 32% compared to naive joint encoding.",
            "6": "3.",
            "7": "The proposed method can achieve comparable results to ELMo, while having a much higher encoding speed.",
            "8": "Weaknesses:\n1.",
            "9": "The evaluation of the proposed method is focused on the GLUE task, which is a collection of classification tasks.",
            "10": "I encourage the authors to test the model on other tasks as well.",
            "11": "For example, DistilBERT can also perform well on question answering datasets like SQuAD.",
            "12": "It's interesting to see whether this shallow model can perform appropriately on more complex tasks.",
            "13": "2.",
            "14": "How can we trade-off between this fast inference speed and good performance?",
            "15": "It seems that this model is very fast at inference speed, but the performance was compromised.",
            "16": "It would be better if we are allowed to find a balance between speed and performance according to our demand.",
            "17": "For example, TinyBERT provides a 4-layer version and a 6-layer version, so we can find a balance between the inference time and performance.",
            "18": "In this paper, it seems that the author only provides 20x20 matrix embeddings for the CMOW model."
        }
    },
    "5sP_PUUS78v": {
        "XN1TskBXRdf": {
            "0": "Strengths: This paper is well-written and technically sound.",
            "1": "Protecting text data is important given there is a rich body of work on attacks for such datasets; however, the protection technique that can provide provable DP guarantees is somewhat understudied.",
            "2": "This is a timely and interesting topic and has many applications, for example, the smart compose.",
            "3": "The approach is clearly stated, and the empirical evaluation is sufficient.",
            "4": "In terms of novelty, SeqPATE has several non-trivial changes compared to the classical PATE.",
            "5": "Weaknesses: I have a few minor comments.",
            "6": "1.",
            "7": "When \\eps is large, DPSGD outperforms SeqPATE.",
            "8": "In practice, the choice of \\eps depends on the data, so could you offer any instructions on choosing the algorithm when \\eps is not very small (like between 5-10, which is also common in practice)?",
            "9": "2.",
            "10": "I'm confused about how you define P-n and why it can be considered as a privacy metric?",
            "11": "3.",
            "12": "In Section 4.3, the sentence `` Eq.",
            "13": "1 shows the top-k selection at i-th step ...'' is confusing.",
            "14": "Eq.",
            "15": "1 only shows the renormalization of the probabilities."
        },
        "APdZbRppfBm": {
            "0": "## Strengths:\n1.",
            "1": "Training a DP language model is a less explored problem, and the authors provide a technically sound solution along with a well-setup empirical evaluation.",
            "2": "2.",
            "3": "The paper is overall well written and the method is easy to follow.",
            "4": "3.",
            "5": "The authors propose a new evaluation metric P-N to indicate how many generated n-grams can be found in the private set, which is an interesting and useful indicator in addition to PPL on the private set.",
            "6": "4.",
            "7": "The authors also provide an interesting discussion on user-level DP and n-grams-level DP for language models.",
            "8": "## Weaknesses:\n1.",
            "9": "My main concern of this paper is whether SeqPATE can achieve satisfactory utility preservation given small epsilon (say epsilon=2).",
            "10": "From Table 1, the B-4 score of SeqPATE is different from that of the upper bound by 2 orders of magnitude, which is a bit concerning.",
            "11": "Moreover, the PPL gap between the upper bound and SeqPATE (13.67 - 3.88) is much larger than the gap between the lower bound and SeqPATE (19.39 - 13.67), which suggests that SeqPATE may barely learn useful information on the private data domain.",
            "12": "2.",
            "13": "The advantage of SeqPATE vanished given a large epsilon compared with DP-SGD, which makes the algorithm less scalable.",
            "14": "Although the authors explain it is due to the partitioning of the training data, could this issue be resolved if reducing the number of teachers?",
            "15": "3.",
            "16": "The authors discuss user-level DP and n-grams-level DP.",
            "17": "It is better to provide some quantitative evaluation on SeqPATE on these settings.",
            "18": "4.",
            "19": "Some experimental setups are a bit unclear.",
            "20": "it is better to include more experimental details in the main paper, e.g., how the hyper-parameters are chosen (say #/ teachers) and the computational overhead of SeqPATE and DP-SGD.",
            "21": "## Additional questions:\n1.",
            "22": "I see the paper is adopting a strict DP standard (delta=1e-9).",
            "23": "Could raising the delta to 1e-6 or 1e-5 can help improve the utility of SeqPATE?",
            "24": "2.",
            "25": "Why Pub-GPT + \\tilde{D}^{pub} can significantly improve (the largest PPL improvement) the PPL of Pub-GPT, considering both methods use the public records only.",
            "26": "This result is a bit weird and counterintuitive.",
            "27": "I am willing to raise my scores if the problems above can be well addressed."
        },
        "W71Y7i2Fmjw": {
            "0": "This paper provides results in the very interesting direction of private language models.",
            "1": "I believe that, given the recent progress in non-private language models, the time is ripe to further investigate this area privately, which has been sorely under-explored.",
            "2": "As a result, I commend the authors for taking steps in this direction.",
            "3": "First, I will evaluate the algorithmic contributions.",
            "4": "As I enumerated above, there are a few modifications to the basic PATE setting.",
            "5": "Of these, I believe using the student model to reduce the support size is an interesting idea.",
            "6": "Using a pre-trained GPT-2 model to extend the training data is also kind of nice, though it is not 100% clear why this is necessary in the experimental settings considered.",
            "7": "In the example provided, I understand: \"Cats sit\" is too short a sentence to perform meaningful training from.",
            "8": "However, the datasets used seem to have much larger utterances (indeed, it says in Appendix C that sentences less than 8 tokens long were filtered out).",
            "9": "The other two modifications seem relatively straightforward: it is of course natural to consider using multiple prefixes of the same pseudo sentence, rather than performing predictions sequentially.",
            "10": "And performing PATE on the output distribution rather than the output prediction is also natural (I would be surprised if it has not been done before).",
            "11": "I believe it may also be inspired directly from knowledge distillation (which is uncited and uncredited).",
            "12": "Overall, the algorithmic modifications are nice, but nothing offering significantly new insights into the nature of natural language generation.",
            "13": "Next, I will comment on the experimental results.",
            "14": "The utility metrics provided in Table 1 seem to indicate that the proposed method performs somewhat better than the previous methods which are private with respect to the sensitive dataset.",
            "15": "However, the utility is still very far from the non-private utility.",
            "16": "Perhaps this is to be expected, given the general cost of guaranteeing DP, but other recent results seem to show how effective pre-trained models are in terms of maintaining utility even with DP -- see, for example, Tramer-Boneh (https://arxiv.org/abs/2011.11660), which demonstrates the efficacy of transfer learning to image classification (resulting in relatively comparably small drops to the accuracy versus no transfer learning), as well as two recent arXiv papers on the same topic (https://arxiv.org/abs/2110.06500 and https://arxiv.org/abs/2110.05679) which achieve relatively small loss in utility compared to the non-private pre-trained model.",
            "17": "Of course the latter two works appeared only after the deadline and thus were impossible to compare with, so perhaps one should disregard them, but the story of both papers seem to be at odds, which is puzzling to me.",
            "18": "Regardless, the [TB20] paper still indicates the significant power of transfer learning for private learning).",
            "19": "Additionally, I noticed that experiments were only performed on two relatively large datasets.",
            "20": "This begs the question of what happens when the datasets are relatively small.",
            "21": "Lack of accuracy on smaller datasets may be why the authors did not evaluate on more common GEM benchmarks.",
            "22": "Comparison with prior methods seem a little mixed.",
            "23": "While SeqPATE indeed outperforms previous methods in various regimes, it is somewhat inconclusive at eps = 5, which is closer to the regime usually considered in DP ML papers.",
            "24": "As a final note, my impression is that this is an incredibly costly infrastructure to run.",
            "25": "I believe this uses k = 2000 teachers, each of which is GPT-2 Small, resulting in an overall model which is comparable in number of parameters to the full GPT-3.",
            "26": "This sounds impractical to run in most settings, and this matter is not discussed at all in the paper.",
            "27": "Overall, the experimental results are modest, but given this discussion, I am not sure if they mark a qualitatively impressive shift in our understanding of the problem (which, as I mentioned before, is seemingly quite under-explored).",
            "28": "My final major comment is on the writing.",
            "29": "The quality of the language is rather poor, with issues in almost every sentence, to the effect that the final paper feels very rushed.",
            "30": "I tried to be charitable to the authors and fill in the gaps intelligently, but there were certain sentences which were incomprehensible in their current state.",
            "31": "Beyond this, other (comparatively minor) writing issues include insufficient discussion of related work, incorrect citations, and unusual choice of content in the body.",
            "32": "I list some of these issues below.",
            "33": "Overall, presentation issues in the submitted version are to the level where I would not recommend anyone to read this until it is better polished.",
            "34": "Further points and comments:\n- The citation for differential privacy is Dwork, McSherry, Nissim, and Smith, TCC 2006.",
            "35": "- Bassily, Smith, Thakurta, FOCS 2014 should be cited as another reference for DPSGD.",
            "36": "- As mentioned above, what happens if one just uses D^pub instead of ~D^pub?",
            "37": "Why is this insufficient for the datasets used in the experiments?",
            "38": "This question is true for many of the approaches studied, not just SeqPATE.",
            "39": "- The term knowledge distillation (KD) is used extensively in the paper.",
            "40": "As I am sure the authors are aware, KD refers to a specific technique for training from a teacher.",
            "41": "Is the KD used in the paper the same as KD as used in the literature?",
            "42": "If so, then it should be cited and discussed (or otherwise, contrasted with).",
            "43": "If it is different, then the term KD should be changed.",
            "44": "- It may be helpful for the presentation to keep going with the running example demonstrated in Figure 1, and how the procedure would work with this concrete instance.",
            "45": "- Although I am not too concerned with what appears in the body of the paper versus the appendix, some choices that I found unusual include: basics of DP and Equation (1), which may be immediate or already known to most readers.",
            "46": "- It is not clear what bold and underlined numbers in Table 1 are meant to represent.",
            "47": "- I am curious what happens for the non SeqPATE methods if one uses D^pub instead of ~D^pub.",
            "48": "It feels like you're cheating if you're just using the model to generate more training data, and it may or may not have an effect.",
            "49": "- The choice of P-3 or P-4 is not clear as a privacy metric.",
            "50": "It seems to say something more about the nature of the dataset.",
            "51": "See in particular how these numbers differ significantly between AirDialog and Europarl_v6.",
            "52": "The canary approach used in the Carlini et al Secret Sharer paper seems more meaningful to me.",
            "53": "- Broken reference in Appendix D\n- It appears that footnote 7 was never filled out appropriately\n- It seems like more teachers result in monotonically better utility.",
            "54": "What is the limit of this phenomenon?"
        },
        "AUcOnQJwD8I": {
            "0": "Strengths: I think the paper does a good job of adapting PATE to the text generation framework and making it work reasonably well.",
            "1": "Given that PATE is a very general framework that works for any learning algorithm and not just deep learning, it is useful to have adaptations of it to sequence/text generation settings.",
            "2": "Weaknesses: \n\nI am not fully convinced by the authors claim that their adaptation of PATE beats DPSGD.",
            "3": "(1) The authors cite Kerrigan et al (2020) to claim that DPSGD doesn't work for text-generation using large models like GPT2 due to high dimensionality of the parameter space.",
            "4": "This is debunked conclusively in the two recent papers by Li et al 2021 (https://arxiv.org/abs/2110.05679) and Yu et al 2021 (https://arxiv.org/abs/2110.06500).",
            "5": "Li et al (2021) show that by careful choice of hyperparameters, training pretrained GPT2 on private datasets using DPSGD achieves performance comparable to non-private training.",
            "6": "We also note that these are very recent and possibly concurrent with this work.",
            "7": "But nevertheless, the authors should not be coming to an opposite conclusion as these papers.",
            "8": "(2) I am also not convinced by the experiments in Table 1.",
            "9": "The authors compare their approach to the baseline DPSGD+$\\tilde{D}^{pub}$.",
            "10": "This means that, from what I understand, a model is trained from scratch on private data $D^{priv}$ using DPSGD and then fine-tuned on $\\tilde{D}^{pub}$.",
            "11": "I think that the correct baseline to compete against is Pub-GPT + $\\tilde{D}^{pub}$ + DPSGD on $D^{priv}$ in that order.",
            "12": "That is we are fine-tuning the pretrained GPT2 on $\\tilde{D}^{pub}$ and then fine-tuning on $D^{priv}$ using DPSGD.",
            "13": "I will be more convinced that their framework works if they compare against this benchmark.",
            "14": "(3) Also to claim that, their method is beating DPSGD, they have to demonstrate they did a systematic search of the hyperparameter space for DPSGD.",
            "15": "But I didn't find any evidence of this in the paper.",
            "16": "The importance of choosing right hyperparameters for DPSGD is demonstrated in the work of Li et al (2021).",
            "17": "Other remarks: Is the grey line in Figure 2, Pub-GPT + $\\tilde{D}^{pub}$?",
            "18": "Is it a typo?"
        }
    },
    "WxuE_JWxjkW": {
        "vzr_Rz5DiYW": {
            "0": "#### Strength\n- This paper provides novel and interesting insights to the language emergence analysis by introducing the concepts of expressivity, context complexity and unpredictability.",
            "1": "These concepts provide a deeper understanding of the behaviors of emergent languages from (referential) language games.",
            "2": "- The hypotheses are clearly stated, reasonable, and well supported by the experiment results.",
            "3": "- Besides the main results, the paper also proposes a simple update on training loss, contrastive loss, which improved the model behavior.",
            "4": "#### Weakness\n- The introduced concepts (context complexity and unpredictability) are finally simplified to be only dependent on the candidate set size $|C|$.",
            "5": "Also, experiments and analysis design are focused on $|C|$.",
            "6": "The scope might be narrow (but also specific, so not necessarily a weakness).",
            "7": "- Although the hypotheses are clearly stated and supported, they are not very surprising based on intuition.",
            "8": "- Regarding the proposed training loss, to my understanding, it simply replaces the original candidate set with the batch.",
            "9": "In this way, the contribution does not look significant.",
            "10": "#### Questions & Suggestions\n- The definition of key concepts, context complexity and unpredictability, do not seem fully accurate.",
            "11": "In text, the context complexity is defined as the expectation of the probability that \"$C(x_t)$ includes an object from $N_k(x_t)$\".",
            "12": "However, equation 2 means the expectation of the probability that \"a random object from $C(x_t)$ is in $N_k(x_t)$\".",
            "13": "Nonetheless, the simplified form $1-(\\frac{|X|-|N_k(x_t)}{|X|})^{|C|}$ is according with the text definition.",
            "14": "Maybe equation 2 needs to be modified.",
            "15": "For unpredictability, the text definition is the expectation of \"the probability that $C^{e+1}(x_t)$ contains an object that is not from $C^e(x_t)$\", which seems wrong because this is clearly not monotonically decreasing on |C|.",
            "16": "Based on the simplified form, I guess it should be the expectation of \"the proportion of $C^{e+1}(x_t)$ that are not from $C^e(x_t)$\"."
        },
        "YCrkOqCZIDU": {
            "0": "The authors attempt to tackle the important issue of how functional pressures affect the properties of resulting emergent communication protocols.",
            "1": "In particular, the are interested in quantifying the 'expressivity' of the resulting communication protocol, and give an operational definition of expressivity in terms of learnability by a randomly initialised listener.",
            "2": "The authors note that mutual information is a poor measure for expressivity, and I like their proposed metric.",
            "3": "It reminds me a little bit of various definitions of probing in the NLP literature, and perhaps a connection can be drawn here.",
            "4": "The resulting partial order in language transferability is described in text on page 8, but I think it would be better to draw the poset as a diagram, to make it easier to see what is comparable to what.",
            "5": "In addition, I think the target games should be the x-axis of Figure 4, and the colours should be used for the source games.",
            "6": "This way, we would be able to read off expressivity dominance immediately from the plot, as which curve was higher overall than the other.",
            "7": "Further, it's unclear whether the generalisation performance is measured using a held-out set of language games, or whether the same games are used to train and evaluate the meta-suite of models.",
            "8": "The authors investigate the effect of two factors on language expressivity: scenario complexity and scenario unpredictability.",
            "9": "Scenario complexity is defined as how many similar items each batch of data contains, on the assumption that items close in perceptual space are more difficult to distinguish.",
            "10": "Scenario unpredictability is defined as the probability that the next batch of data contains an item not contained in the current batch.",
            "11": "I feel like complexity as the authors have defined is a very sensible metric.",
            "12": "However, I'm not convinced by the unpredictability measure.",
            "13": "It only considers bigram transitions, so considering a game with 4 inputs a, b, c, d and cycling the inputs endlessly as (a, b), (c, d) would be maximally unpredictable under this metric, when clearly it shouldn't be.",
            "14": "I feel like the right notion of unpredictable should consider some kind of online learning scenario, such as prequential prediction.",
            "15": "In addition, the human experiments used to motivate unpredictability rely on the fact that humans can remember past contexts when playing the game.",
            "16": "In comparison, the models considered in the paper are memoryless, and the only effect of data presentation order is implicit in the optimisation dynamics.",
            "17": "I understand that the intuition is that unpredictable environments force the speaker to be maximally informative in its utterances, but it's not clear that the proposed metric is the right way to capture this intuition.",
            "18": "Another contingent issue is that, at least as presented, complexity and informativeness are dependent variables, and the true controlled variable is batch size (indeed, the x axis of all the plots is batch size, and the formulae used to define complexity and unpredictability are not used anywhere else in the paper).",
            "19": "This is because data points for a batch are sampled i.i.d.",
            "20": "This means that the two axes of variation are not independent, which leads to conclusions which can't tease apart the effect of either measured variable, and reduces the overall impact of the paper as the empirical results suffer from confounding.",
            "21": "I believe that alternative data sampling schemes would help tease apart the effect of both variables.",
            "22": "One final thing is that the 'contrastive' loss presented in equation 1 is contrasted with a 'referential' loss which is never explicitly given in the paper.",
            "23": "To make the exposition self-contained, it would at least help to see how the 'referential' loss is computed, so that the reader can make a better comparison.",
            "24": "(As an aside: Wittgenstein did use the phrase 'language game', but his concern was not acquisition, but rather how meaning is modulated by context.",
            "25": "The first sentence of the introduction therefore misrepresents his statements, and I feel should be removed.)",
            "26": "=============\n\nPost author response:\n\n> Memory of human participants also decay, thus the context is always limited in a finite number of time steps.",
            "27": "Defining unpredictability on a longer history simply changes the power of Equation 3 in our setting, but doesn’t influence the growth relationship between unpredictability and context size.",
            "28": "Therefore, it won’t influence our conclusion if we extend Equation 3 to a longer history.",
            "29": "This is only assuming a uniform bigram/trigram/n-gram distribution.",
            "30": "Often in the real world, sequential data has long-term structure which isn't well captured by any n-gram transition model (which is why RNN/Transformer language models so dramatically outperform count-based ones), and so it's not obvious that using more sophisticated transition schemes would result in similar findings.",
            "31": "To be honest, I find the unpredictability measure more opaque after author response.",
            "32": "Section 5.1 varies complexity while keeping unpredictability constant using something called a 'fixed batch', but it's not very well explained what this is, and why it makes minimises unpredictability.",
            "33": "Again, I wonder why predictability of the future given the past is important for these communicating agents, considering they are memoryless across episodes.",
            "34": "I brought up this point in my original review, and it is not addressed in the author response.",
            "35": "> We would be happy to include more useful definition of unpredictability if there’s other helpful definition you have in mind.",
            "36": "We’ll also be happy to see more exploration on how unpredictability of context implicitly influence the training dynamics of agents.",
            "37": "I gave a definition: online learnability using prequential coding (see, e.g.",
            "38": "The Description Length of Deep Learning Models, Blier and Ollivier 2018 for a description).",
            "39": "There is a large literature relating unpredictability and online learning, and I believe that many ideas in that field will be applicable here.",
            "40": "> We would be happy to include a new data sampling scheme, if there’s some other useful method you have in mind.",
            "41": "Any kind of non-independent sampling would decouple complexity from unpredictability, such as stratified sampling of contexts according to similarity given a target.",
            "42": "I believe that there is much scope to precisely control complexity and unpredictability using such schemes, which I believe would make a future revision of this paper much more solid.",
            "43": "After seeing the author resposne, I don't feel that many of the issues I raised in my review have been substantially addressed, and I have not changed my score.",
            "44": "Most notably, the issue of the definition of unpredictability and how it affects the communication are still unclear, and I believe this needs to be clarified substantially in future revisions."
        },
        "39jyNt_e48Q": {
            "0": "In my opinion, the biggest strength of the paper is the empirical evaluation conducted to support the claims.",
            "1": "The experimental design and setup look sound.",
            "2": "The definitions and different notions introduced in the paper seem natural and reasonable.",
            "3": "At the same time, given the notions of complexity and unpredictability, a number of conclusions seem very natural based on intuition.",
            "4": "The paper is very well written and the arguments are backed up with reasonable experiments.",
            "5": "The proposed metric seems to work well in the given setting.",
            "6": "Isn't the metric is dependent on the transfer learning abilities of the model to a certain extent?",
            "7": "Will the partial ordering be consistent across different types of models?",
            "8": "The idea of applying contrastive loss is not particularly novel.",
            "9": "It seems like to address the issue of space efficiency, they simply confined the previous method to batches rather than the entire set."
        },
        "u2MpAIfYFU": {
            "0": "I like the overall idea of the paper on exploring the relationships between expressivity, complexity and unpredictability in emergent languages.",
            "1": "**Strengths:**\n\n- The paper properly motivates with examples, the study of expressivity, complexity and unpredictability in the framework of deep learning based language games as a way to gain further insights into natural languages which I find really interesting.",
            "2": "- The paper introduces a novel measure of expressivity for emergent languages that is defined through its generalization performance across different tasks.",
            "3": "- The paper studies the usage of contrastive loss in referential games and this loss helps alleviate the collapse of message types in language games which is interesting and novel.",
            "4": "- The experiments are extensive which is great to see.",
            "5": "- Further discussions in the appendix is also nice.",
            "6": "Particularly, those around mutual information.",
            "7": "**Weaknesses:**\n\nI have included actions that can be taken to strengthen this paper.",
            "8": "If these actions are addressed, I am willing to increase my score.",
            "9": "- **Section 3.1/4.1:**\n    - **Complexity/unpredictability:** I don't fully understand what equations 2, 3 mean, both are of the form $f(x_t) = E_{x_t}[...]$.",
            "10": "So, $x_t$ is both an argument to the function and the expectation.",
            "11": "How is the expectation over $x_t$?",
            "12": "- **Action:** Clarify equations 2 and 3.",
            "13": "- **Complexity/unpredictability:** I am a little confused at how the closed form solutions for equations 2 and 3 were arrived at.",
            "14": "Equation 2 for example, might be correct only if it is sampled with replacement, not without.",
            "15": "Both are of the following form: the probability that a set A contains an object from set B where A, B are sampled from X without replacement.",
            "16": "The solution to this is 1 - (|X| - |B|  \\choose |A|) / (|X| \\choose |A|).",
            "17": "- **Action:** Correct or clarify solutions to equations 2 and 3.",
            "18": "- **Section 3.2:**\n    - The paper does not fully motivate why Definition 3.1 is a good quantifiable metric for studying expressivity in emergent languages.",
            "19": "How does generalization that is studied in emergent languages like [2] relate to this particular definition of expressivity?",
            "20": "- **Action:** Compare expressivity with the notion of generalization studied in past works.",
            "21": "- **Section 4.1:**\n    - Predictions in section 4.1 depends on the expressions being accurate in Section 3.1.",
            "22": "It is also not clear to me how the definitions of unpredictability and complexity leads to predictions over expressivity.",
            "23": "- **Action:** Clarify how we can expect these predictions to be true from the definitions provided.",
            "24": "- **Section 5.3:**\n    - Looking at Figure 4 and the corresponding Table 4 (in the appendix), it does not seem that referY is statistically worse than referX for Y > X as claimed in this section.",
            "25": "- **Action:** Including these comparisons with a statistical significance test would be needed to draw these conclusions.",
            "26": "**Comments:**\n\n- **Section 2.2:**\n    - What are source and target games?",
            "27": "They are defined in a later section and pointing the readers to section 4.2 would make it easier to follow.",
            "28": "- Section 4.2 is a little difficult to follow.",
            "29": "This section defines ReferX and ReferX/f which is used in the rest of the paper.",
            "30": "The definition for ReferX/f is particular is not very clear.",
            "31": "The following statement is also not very clear: \"since language games with fixed context are not good simulation of human communication , we keep only the ones having varying batches from $G_s$ in $G_t$.\"",
            "32": "- **Action:** Include further discussions into the definitions for ReferX, ReferX/f, $G_s$ and $G_t$.",
            "33": "[1] Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015).",
            "34": "Compression and communication in the cultural evolution of linguistic structure.",
            "35": "*Cognition*, *141*, 87-102.",
            "36": "[2] Chaabouni, R., Kharitonov, E., Bouchacourt, D., Dupoux, E., & Baroni, M. (2020).",
            "37": "Compositionality and generalization in emergent languages.",
            "38": "*arXiv preprint arXiv:2004.09124*."
        }
    },
    "Vx8l4vwv94": {
        "O27v9ETPQXs": {
            "0": "- The proposed TDE method is just a simple mix of the ideas of Doc2VecC proposed by (Chen 2017) and STE proposed by (Shi et al., 2017).",
            "1": "- Although the paper clearly explains the motivation and the related work section has a good collection of references on conventional word embeddings and document embeddings, most references (including two baselines) are publications of 4 or 5 years ago and completely misses recent major progresses of contextual sentence/document embeddings since ELMO and BERT.",
            "2": "- What's the advantage of the proposed TDE over the recent contextual embedding methods for representation of sentences and documents?",
            "3": "-  Both Doc2VecC (Chen 2017) and STE (Shi et al., 2017) conducted extensive experiments on the quality of the embeddings and performance on the downstream application of document classification.",
            "4": "But this paper only presents very limited experiments and is lack of clear interpretation of experimental results.",
            "5": "For example, there are only 2 base lines (STE and Doc2VecC) studied in the experiments, but why STE is still missing Table 1, and Doc2VecC is missing in Table 5?",
            "6": "What's the key observations and insights from Table 4?",
            "7": "- It seems there is lack of basic check and proof reading\n    - Typos: for example, \"Tow popular complementary\" in Page 1 and \"flowing objectives\" in Page 1\n    - error on major equation: for example: I guess “d/1-q” should be \"x_d/1-q\" in equation (1) \n    - Missing definition: for example, \"PV\" in Page 1 and other pages (I guess \"PV\" means \"paragraph vector\")\n    - Poor Notations and writings, hard to read and follow, for example, section 3.1"
        },
        "kRJdtJqLdb": {
            "0": "strengths\n1) This paper is well written and easy to read.",
            "1": "2) TDE follows the previous work and aims to learn three different level embeddings.",
            "2": "3) This paper shows visualization of the interaction between words and latent topics in the embedding space.",
            "3": "weaknesses\n1) Approaches are straightforward and lack originality.",
            "4": "This approach is simple combination of  Doc2Vec and STE.",
            "5": "2) Baselines are old and weak in now.",
            "6": "3) The experimental results reported are validated on a single dataset,\nand no human evaluation and error analysis."
        },
        "Rj1rLCTQi7I": {
            "0": "The proposed Skip-gram model contains two major parts that are different from the standard Skip-Gram model:\n\n* One is to assume each word has a different representation under each individual topic.",
            "1": "In such a way, the topic assignment of a word can be factored in the computation of the skip-gram probability.",
            "2": "In other words, the probability of a context word will depend on not only the centre word but also their topic assignments.",
            "3": "However, this idea is from the Skip-gram topical embeddings by Shi et al, 2017.",
            "4": "* The other is the use of a document vector as the global context, which originates from Chen 2017.",
            "5": "The proposed model simply injects this context vector into the Skip-gram topical word embedding.",
            "6": "If one compares eq (4) in Shi et al, 2017 and eq (3) of this paper, the only difference is the term $\\frac{1}{T} U_{\\tilde{x}_d}$, constructed similarly to Eq (2) of Chen 2017.",
            "7": "This simple modification makes the learning algorithm naturally follow the algorithm design discussed in Section 3.2 of Shi et al, 2017.",
            "8": "There is nothing wrong with combining the idea of two models together.",
            "9": "However, it makes one have doubt about the innovation of this work unless the proposed model is very effective in terms of performance, which is not really the case.",
            "10": "In regards to the experiments, the proposed model is compared mainly compared with the models in Shi et al, 2017.",
            "11": "The experimental methodology follows closely that used by Shi et al, 2017, i.e.",
            "12": "document classification, topic coherence, word embedding.",
            "13": "The classification performance is only marginally better than STE-same, much worse than STE-diff.",
            "14": "Meanwhile, the paper claims that the proposed model is capable of dealing with polysemy, which however is only insufficiently demonstrated by Figures 3 and 4.",
            "15": "The writing of this paper could be further improved, particularly those notations.",
            "16": "Besides, I have the following questions\n* Should the two project matrices be in $\\mathcal{R}^{k \\times s}$?",
            "17": "If that is the case, most likely, then it column is not a word embedding.",
            "18": "I understand if you follow the notation of Chen 2017, then the column should be word embeddings, However, the equations tell us that U has a topic dimension.",
            "19": "Let me know if I was wrong.",
            "20": "* By Eq (4), do the authors mean that the skip-gram probability conditions on both $w_t$ and $\\tilde{x}$?",
            "21": "And presumably, the first probability of the left term in Eq (5) is computed based on Eq (3).",
            "22": "* Eq (1): $\\tilde{x}$ is scalar.",
            "23": "The sum in Eq (2) will be scalar, won't it?",
            "24": "According to Chen 2017,  Eq(1) will generate a word masking vector for  a document.",
            "25": "The masking vector is then used to generate the document representation via averaging.",
            "26": "Right?"
        },
        "_ggUiv4vhXA": {
            "0": "Strengths:\n+ introduced a single framework that jointly learns topically-aware word embeddings and document vectors where topic-embeddings are learning using global content and each word embedding can be part of different underlying topics of the document.",
            "1": "On top, document vectors are computed by averaging topic word embeddings.",
            "2": "+ clear problem statement \n+ paper is well written  \n+ sound qualitative evaluation of word embedding visualization \n\nWeaknesses:\n- missing empirical comparison to related works in compositional models [1, 2, 3] of jointly learning topic and word embeddings \n- experimental setup is unclear for document classification task: How is classification task performed?",
            "3": "First LM-based objective then using the document representation into classification?",
            "4": "- related works section is weak, missing several related works [1, 2, 3]\n- quantitative evaluation is limited (missing key baselines and requires additional data sets in comparative analysis) \n- Document classification is performed on a single (large) data set.",
            "5": "Would be interesting to validate the methodology using at least 2-3 datasets and in sparse data settings.",
            "6": "- Document classification should be further evaluated on standard data sets for example 20newsgroups, Reuters, TMN, etc.",
            "7": "- Empirical evaluation is incomplete for document and word embedding evaluation: missing comparison with several strong and most related baselines such as [1, 2, 3] etc.",
            "8": "that jointly learns topic and word embeddings\n- please include a case study and/or comparison/differences to related works [1, 2, 3] in terms of:\n\t- jointly learning topic and word embeddings in a single framework \n\t- captures both the syntactic and semantic linguistic structure \n\t- combines both local and global contextual information from word, topic and document\n\t- incorporates both: topic and word embedding spaces in language modeling based training objective (loss function)\n\nQuestions:\n1.",
            "9": "Since the Document vector is computed by averaging its word vectors, therefor the averaging scheme may not consider the syntactic structure e.g.",
            "10": "word ordering.",
            "11": "However, the methods such as [1, 2] employs LSTM to compute document vectors and globsl context considering linguistic structures.",
            "12": "2.",
            "13": "In equation 3, does the global context \\~{x} include w_{i+1} ?",
            "14": "Unclear.",
            "15": "3.",
            "16": "In qualitative evaluation of topics, why are the topics of bigram words only?",
            "17": "Please also include qualitative topics analysis of unigram words?",
            "18": "Missing references:\n1.",
            "19": "Gupta et al., 2019. textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR.",
            "20": "ICLR 2019.",
            "21": "2.",
            "22": "Lau et al., 2017.",
            "23": "Topically driven neural language model.",
            "24": "ACL 2017.",
            "25": "3.",
            "26": "Dieng et al., 2016.",
            "27": "TopicRNN: A recurrent neural network with long-range semantic dependency."
        }
    },
    "0bXmbOt1oq": {
        "jusANKgQMQi": {
            "0": "Good points:\n- pushes emergent communication in a new direction: that of oral communications\n\nBad points:\n- some key missed references, i.e.",
            "1": "Foerster et al 2016 section 6.4 uses a continuous channel, and finds that adding noise to the channel is necessary in order to effectively 'discretize' the outputs.",
            "2": "I also feel that Lazaridou et al 2018 is a better reference than Chaabouni et al 2020, since Lazaridou et al 2018 was more or less the first work to use a referential task in the recent wave of emergent communication literature.",
            "3": "- a lot of the space in the paper appears to me to be used to describe relatively vanilla concepts, such as the architecture of the sender and receiver RNN models, what a replay buffer is, and some of the earlier experiments are I feel more like 'debugging'/smoke-test experiments, that show the architecture learns\n- I wasn't sure what I was supposed to learn from the grounded experiments.",
            "4": "I felt that it was unsurprising that grounding one agent in English would cause the other agent to learn a language that learns English\n- the text to speech synthesize is pretrained, which goes I feel against the spirit of emergent communication.",
            "5": "I understand that motivation for this was provided, and that motivation makes sense to me, but listing this for completion\n- the meaning/object spaces being used are very small (1 dimension, of size 16; or two dimensions, each of size 4; for example)\n- code is not provided with the submission\n- I didn't come away with any sense of 'oh that was interesting/surprising; I learned something'\n\n## Notes\n\nNotes I made as I read through the paper:\n\n### Abstract\n\nI like the idea of using an accoustic channel :) It's something I've been intending to write a paper on sometime, just didn't get around to it yet :)\n\nIt seems clear to me that spoken language preceded written language, so starting with accoustic language, for emergent communication, seems like a great idea to me :)\n\nI'm a little surprised/unclear why we map from symbols to accoustic.",
            "6": "why not just keep everything continuous?",
            "7": "noisy communication channel being needed for compositionality and/or generalization has been shown in a few previous works.",
            "8": "### 1.",
            "9": "Introduction\n\nMising reference \"Learning to Communicate with Deep Multi-Agent Reinforcement Learning\", Foerster et al 2016, which in section 6.4, uses a continuous channel, and discovers that by adding noise, the channel effectively becomes discretized.",
            "10": "Citing Chaabouni et al 2020 for 'referential signaling game' seems I feel not the original reference.",
            "11": "\"Emergence of linguistic communication from referential games with symbolic and pixel input\", Lazaridou et al, 2018 was one of the first works to bring the idea of a referential game into emergent communication domain.",
            "12": "Prior to that, the Lewis signaling game, Lewis, 1969, is, as far as I know, the original reference.",
            "13": "Why use deep Q-learning?",
            "14": "Not that it's not a good choice, but curious why you choose this, when the vast majority of emergent communication works use simple REINFORCE (or Gumbel).",
            "15": "### 2.",
            "16": "Environment\n\nUsing a pre-trained text-to-speech synthesizer seems to go against the goals of 'emergent' communication?",
            "17": "The explanation for why this is (later in the section) does make sense to me though.",
            "18": "Baby steps :)\n\n### 3.",
            "19": "Learning to speak and hear using RL\n\nA large amount of space (~1.5 pages) is being spent describing what appear to be quite vanilla sender and receiver models.",
            "20": "I'm not suggesting that the models should be made 'fancier', but that that space could perhaps be filled with additional experiments, or analysis, etc.",
            "21": "### 4.",
            "22": "Experiments\n\nI feel discussing eSpeak vs Festival gives a sort of 'engineer-y' feel to the paper; and could be omitted.",
            "23": "#### 4.2 Unconstrainted communication of single concepts\n\nI realize at this point that somehow I missed what are the actual input data.",
            "24": "Ok, skimming right back to page 3, it looks like there is only a single meaning dimension, of size N?",
            "25": "This could represent eg 'color'.",
            "26": "From 4.2, it looks like N is 16, though it's not stated clearly.",
            "27": "This is I feel an extremely small meaning/concept/object space.",
            "28": "On the whole, 4.2 seems like a superfluous debugging section, which doesn't really provide any information/signal, and could be omitted.",
            "29": "#### 4.3 Unconstrained communication generalizing to multiple concepts\n\nI was briefly excited by the mention of Kirby's ILM, but I feel that the citation of Kirby's ILM is superfluous here: the experiment is simply changing from a 1-dimensional meaning space of size 16, to a 2-dimensional meaning size, where each dimensino is of size 4.",
            "30": "Again, this experiment seems to me to be quite vanilla.",
            "31": "I feel that in addition to providing a table of utterances to eye-ball, it could be slightly more rigorous to use objective measures of compositionality such as topological similarity.",
            "32": "I feel that the effect of channel noise in Table 2 appears quite weak to me.",
            "33": "In addition, it could be argued that the noise is simply providing training time regularization, which we see as a drop in performance of the training codes, rather than leading to a qualitative shift in the emergent communication, as Foerster et al 2016 showed, in their section 6.4.",
            "34": "#### 4.4 Grounding emergent communication\n\nI feel that 'grounded emergent communication' is a contradiction in terms: either the language is grounded, or it emerges.",
            "35": "If there is any grounding provided, then it is a grounded task, not an emergent task, I feel?",
            "36": "I'm not really sure of the purpose of this set of experiments.",
            "37": "The various agents are being grounded in english in various ways, and end up outputing some English-like language.",
            "38": "I'm not really sure what I can learn from this?",
            "39": "I feel that using English-language grounding in a paper positioning itself as emergent communication is somewhat risky.",
            "40": "With sufficient motivation and analysis, I don't have any fundamental objection to doing this though.",
            "41": "Preference to come away with some sense of 'that was interesting; I learned something'.",
            "42": "This might mean making the experiments challenging enough so that they do *not* actually work, and then digging into why, and what we can learn from that, perhaps?"
        },
        "zcZ8axtzxfq": {
            "0": "This paper describes some experiments to simulate language acquisition in which a speaker talks to a listener over a continuous voice channel.",
            "1": "The speaker uses a GRU conditioned by one or more concepts to generate a sequence of sounds selected from an inventory of 160 English phonemes released by eSpeak.",
            "2": "The listener applies a convolutional net to the spectrogram of received sounds, then decodes sequential frames to generate a distribution over phonemes from which an argmax generates the decoded concept.",
            "3": "Learning uses a simple 1-step DQN.",
            "4": "The channel between speaker and listener has additive noise and time/pitch shifts applied.",
            "5": "The experiments include the unconstrained transmission of 1 and 2 concepts and the same but constrained so that either the speaker knows English words or the listener knows how to recognise English words.",
            "6": "Claims made include the ability to use composition and generalisation when moving from 1 to 2 concept transmission.",
            "7": "There are many things that I do not understand about this paper.",
            "8": "Firstly, when the speaker is generating a sequence of M phonemes, each phone is regarded as an action but the optimisation treats this an M-armed bandit problem and simply averages the Q values over all actions.",
            "9": "Why is this not treated as an M-step trajectory and solved using the standard DQN optimisation?",
            "10": "Secondly, it appears that all phoneme sequences are exactly M long, rather than a maximum of M long (see eg Table 1).",
            "11": "Is this true?",
            "12": "Wouldn't it be better to allow variable length and perhaps use the reward to encourage brevity.",
            "13": "Thirdly, how are the input concepts coded?",
            "14": "The examples suggest that each concept is coded as a 1-hot vector.",
            "15": "For multiple concepts, the natural assumption would be that the concepts are simply concatenated.",
            "16": "If this is the case, how does the listener work since it is only capable of forming a distribution over 1 set of concepts.",
            "17": "How does the listener distinguish multiple component concepts?",
            "18": "The claim that the results in Table 1 demonstrate the ability to learn a compositional language protocol are unconvincing.",
            "19": "Quoting 0.25 as the chance mean reward ignores the fact that the agents could simply recognise one of the concepts whenever the combination was unseen.",
            "20": "Adding noise will often improve the robustness of a classifier, why is the generalisation implied by Table 2 any different?",
            "21": "What is the statistical significance of the numbers in this table?",
            "22": "On page 9, it is claimed that using DTW for the 2 word listener case is impractical.",
            "23": "Why is this?",
            "24": "If the Listener only has to consider a vocabulary of 50 words, there are only 2500 two word combinations which should be tractable for a DTW style recogniser with pruning."
        },
        "k6abGf5DQz": {
            "0": "## Strengths\n- The environment is novel and provides a good basis for studying certain traits of continuous-channel referential games.",
            "1": "- Excellent clarity and presentation of ideas\n\n## Weaknesses\n- The experiments provided do not analyze the unique characteristics of the environment introduced; instead, the experiments are similar to the typical gamut for a discrete symbol-based referential game.",
            "2": "- The analysis of compositionality given is insufficient.",
            "3": "It appears that a single example was analyzed qualitatively.",
            "4": "In the presence of definitive results, this might be acceptable, but in its absence, as in this paper, there needs to be quantitative analysis across multiple runs to demonstrate the robustness of the phenomenon.",
            "5": "## Recommendation\nI recommend rejecting the paper.",
            "6": "While I appreciate the environment introduced and believe it to be promising, the experiments presented fail to highlight the novelty of the environment.",
            "7": "It is not the case that I find the experimental results inadequate, rather, the experiments run in the first place are generic and do not illustrate the points of interest with a continuous-channel referential game.",
            "8": "It is possible that I could be convinced otherwise as I truly appreciate the environment, but I would need to see an argument as to how the paper properly supports its introduction of a new environment.",
            "9": "## Justification\nAs my primary critique concerns the experiments, I will address each individually mentioning any deficiencies and potential improvements.",
            "10": "- unconstrained, single-concept:\n    This is adquate to demonstrate that a minimal environment works (as stated in the paper).",
            "11": "- unconstrained, multi-concept:\n    This needs a direct comparison to a traditional discrete-channel referential game.",
            "12": "Thus, the experiment could answer questions such as:\n    - Does compositionality emerge at the same rate in continuous- and discrete-channel games?",
            "13": "- What is it about an audio channel model that makes compositionality easier or more difficult to learn?",
            "14": "- Does the interaction between various phonemes restrict the means by which compositional codes can be communicated (e.g., /aio/ and /aeo/ are harder to distinguish than /abo/ and /ato/)?",
            "15": "- multi-concept and noise:\n    The noise you add to the channel has a number of different components; there should be an ablation study to illustrate the effects of these components.",
            "16": "If the experiment does not peer inside the structure of the noise and treats it as a gray box instead, it is not much different than a discrete-channel environment where random edits are made to the message.",
            "17": "- grounded language learning:\n    In both of these experiments, there is analysis provided on what aspects of the audio-based communication channel make the problem harder, easier, or just different from the same experiment with a discrete channel.",
            "18": "For example, experiments could investigate if using words which are phonologically similar (e.g., \"boat\" and \"moat\") is harder to distinguish than dissimilar words.",
            "19": "Or an experiment could address if using the phonotactics from a natural language results is more effective than using a randomly selected phonemes due to necessity for natural languages to have acoustically distinct words.",
            "20": "Specific criteria:\n- Correctness: 4\n    - The claims are supported, but I do not think the claims go far enough (e.g., \"noise has an effect on generalization\" is claimed when instead what that effect is needs to be characterized).",
            "21": "- Technical Novelty and Significance: 3\n- Empirical Novelty and Significance: 1\n\n## Questions\n_What_ is unique about the environment has been presented well, so my primary question is _how_ is it unique?",
            "22": "In other words, I see that the structure of the environment is unique, but what are the consequences which this structural difference begets?",
            "23": "## Additional Comments\n- `s1`: Introduction is paced well\n- `s2p2`: what is T?",
            "24": "Is it just every tick of the audio signal?",
            "25": "- `s3.2p2`: What is the specific reasoning for using a bidirectional GRU?",
            "26": "Using a model capable of streaming would make sense from the point of view of inductive biases.",
            "27": "- `s3.2 p2`: Due to the argmax, is it the case that the \"confidence\" of the predictions is not used in optimization?",
            "28": "This seems like it would artificially make optimization harder.",
            "29": "- `Table 2`: Are these values computed over a single or multiple runs (in which case include stddev/confidence intervals).",
            "30": "- `s4.3 Findings p4`: \"the channel ... influences generalisation\" -- Without further explication, this potentially interesting point does not gain any traction."
        },
        "H0sINJzJodV": {
            "0": "The paper explores a two-agent communication problem where the Sender agent sends a continuous waveform to the Listener who is tasked to reconstruct the original concept vector.",
            "1": "There are two settings of concepts used, single concepts (or a flat vector) and double concepts (consists of a combination of two separate meanings).",
            "2": "However, there are many important details missing in the main text.",
            "3": "It is not clear how big is the alphabet size that is used by the Speaker to construct the waveform.",
            "4": "It is an important parameter as it affects the compositional generalization of the agents (Chaabouni et al.",
            "5": "2020, Kottur et al.",
            "6": "2017, Resnick et al.",
            "7": "2020, Li and Bowling 2019).",
            "8": "Regardless of the alphabet size used, the problem setting used is quite 'toyish'.",
            "9": "Even in the double concepts game the total possible combination of all concepts is 16.",
            "10": "In most of the previous literature in this field, even the minimal setup uses 3 concepts of 5 types each (Chaabouni et la.",
            "11": "2020) and then results are validated using a more realistic problem setting either using real images as input or increasing the input space.",
            "12": "The question here is would this method scale to larger problem settings.",
            "13": "The idea of using signalling to make one agent learn about the underlying concept and active listening to help the listener solve the given downstream task is already explored in Eccles et al 2019.",
            "14": "Unfortunately, there has been no comparisons made with this work that seems to be the most relevant to the proposed method.",
            "15": "Moreover, many relevant are not cited in this work (for example the above works).",
            "16": "I would encourage the authors to do a thorough literature survey of the recent work done in the space of compositionality and generalization in emergent languages.",
            "17": "The hypothesis around compositionality is only evaluated qualitatively.",
            "18": "There have been some good progress done to compute domain independent quantitative compositionality metrics that are well established and grounded (Chaabouni et al.",
            "19": "2020, Lazaridou et al.",
            "20": "2018, Resnick et al.",
            "21": "2020, Andreas 2019).",
            "22": "In addition, the notion of compositionality captured in the paper is more aligned to combinatorial generalization.",
            "23": "Good-enough compositional data augmentation.",
            "24": "Andreas 2019  \nEntropy minimization in emergent languages.",
            "25": "Kharitonov et al.",
            "26": "2020  \nCompositionality and generalization in emergent languages.",
            "27": "Chaabouni et al.",
            "28": "2020  \nEase-of teaching and language structure from emergent communication.",
            "29": "Li and Bowling 2019  \nCapacity, bandwidth, and compositionality in emergent language learning.",
            "30": "Resnick et al.",
            "31": "2020  \nNatural language does not emerge ‘naturally’ in multi-agent dialog.",
            "32": "Kottur et al.",
            "33": "2017  \nEmergence of linguistic communication from referential games with symbolic and pixel input.",
            "34": "Lazaridou et al.",
            "35": "2018"
        }
    },
    "qaxhBG1UUaS": {
        "X6wet93oBR": {
            "0": "The paper proposes a nice way to handle large scale natural language action space for RL.",
            "1": "An action in this model is a system action plus response (a sequence of tokens) rather than pre-specified variables in discrete latent models for dialogue (LaRL, etc.",
            "2": "), so the proposed model can be easily incorporated into a transformer-based language model (e.g.",
            "3": "GPT-2).",
            "4": "My biggest concern is the q-value estimation.",
            "5": "Estimating the Q value for an offline dataset is hard due to the problem of overestimation, which is already pointed out by the authors in the paper.",
            "6": "However, the authors didn't describe in detail how they mitigate this problem.",
            "7": "The only sentence in the paper about this is \"However, we avoid this OOD problem by xxx ... revised by generated system response and evaluated reward using offline automatic evaluation\".",
            "8": "More details are needed, and ablation study about this is needed.",
            "9": "Otherwise, I don't know if the proposed method can generalize to other natural language action space applications where the ground-truth reward function is unknown for offline automatic evaluation.",
            "10": "My second concern is the exploration perspective of self-generation.",
            "11": "The paper mentioned that {a_k}^N is a set of N response candidates generated from the current policy \\pi.",
            "12": "What number is N set to?",
            "13": "What is the impact of N to the final performance?",
            "14": "Moreover, is{a_k}^N sampled by beam search or vanilla sampling?",
            "15": "I'd assume that we want to sample {a_k}^N that is diverse enough, rather than similar system responses with only one or two different words.",
            "16": "More discussion about the generation is needed."
        },
        "Ey06tojuBuq": {
            "0": "Strengths:\nThis paper introduces a critic value function on the top of the pretrained GPT-2 task-oriented dialogue agent, to guide the response generation.",
            "1": "Weakness:\n1.",
            "2": "The novelty may be limited by only adding a critic value function on the top of the existing work.",
            "3": "2.",
            "4": "Is it possible to add human evaluation for at least of the datasets?",
            "5": "(fixed in the rebuttal)"
        },
        "0D3R6eAVOWo": {
            "0": "The proposed approach is very simple and should be easy to implement.",
            "1": "The experimental results seem promising.",
            "2": "However, I do have a couple of concerns.",
            "3": "Firstly, I think that some important details are missing in the paper.",
            "4": "For example, what is the action space of the agent?",
            "5": "Do you treat the conjunction of a dialogue act and a system response as a single action?",
            "6": "If so, how exactly are the candidate actions generated?",
            "7": "Is some kind of beam search employed?",
            "8": "I think some actual examples of actions (and rewards) would help the reader understand the proposed method more clearly.",
            "9": "The paper does not really describe how the rewards are computed, either.",
            "10": "In particular, I am wondering how the reward for the newly selected action is computed.",
            "11": "Is it given to the agent by an external program?",
            "12": "Then, it seems to me that the whole training procedure is more like (a somewhat restricted version of) actor-critic-based reinforcement learning than offline reinforcement learning (in which the agent cannot interact with the environment).",
            "13": "If that is the case, what is the novelty of the proposed method?",
            "14": "The authors claim in Table 3 that their proposed approach gives much better results than UBAR, but the original paper of UBAR (Yang et al., 2021) reports much better results (e.g., Inform score of 95.4).",
            "15": "Why is there such a big difference?",
            "16": "Algorithm 1 states that the policy is updated by behavior cloning until \"convergence\".",
            "17": "I am wondering if it causes any overfitting problem.",
            "18": "Is overfitting the reason why the whole training process is stopped at the fourth iteration (Table 2)?",
            "19": "Minor comments:\n\np. 1: outperforms the state-of-the-art -> outperforms the state of the art;\np. 1: not trained to for -> not trained for?",
            "20": "p. 2: fine-turning the GPT-2 -> fine-turning GPT-2?",
            "21": "fine-turning the GPT-2 model?",
            "22": "p. 2: generates strategically -> generates a strategically;\np. 2: Pr(O_{t+1} ... ) -> should not be italic?",
            "23": "p. 3: by training action-value -> by training the action-value;\np. 3: of critic network -> of the critic network;\np. 4: for $i$-th -> for the $i$-th;\np. 4: in task-oriented -> in the task-oriented?",
            "24": "p. 4: generated system response -> generated system responses?",
            "25": "p. 4: from (Zhao et al., 2019) -> from Zhao et al.",
            "26": "(2019);\np. 4: prohibitory -> prohibitively?",
            "27": "p. 4: over response candidates -> over the response candidates?",
            "28": "p. 4: updated policy by above -> the updated policy by the above?",
            "29": "p. 5: on MultiWOZ domain -> on the MultiWOZ domain;\np. 5: ConvLab framework -> the ConvLab framework;\np. 5: HuggingFace Transforms library -> the HuggingFace …;\np. 7: prices -> priced?",
            "30": "p. 8: user goal -> the user goal?",
            "31": "p. 8: with following -> with the following;\np. 8: the all -> all the;\np. 8: whereas original -> whereas the original?",
            "32": "p. 9: straightforward -> straightforwardly?",
            "33": "p. 8: large scale -> large-scale?"
        },
        "NKM-5v8he7h": {
            "0": "This paper focuses on improving the dialogue policy together with the responses by utilizing a pre-trained language model and offline RL.",
            "1": "The claimed contributions include:\n1) The proposed method is free from the common issue of diverging from human language, because it learns from the sentences sampled from the pre-trained LM.",
            "2": "2) The proposed method outperforms other SOTA models in offline and interactive online settings, MultiWOZ and ConvLab respectively.",
            "3": "The proposed method is reasonable and moderately novel.",
            "4": "The experimental results are promising for both settings.",
            "5": "However, there are unclear parts to be addressed or clarified.",
            "6": "- Because the policy learning procedure utilizes the additionally generated dialogue acts and corresponding responses, it is easy to think that naively fine-tuning the GPT-2 model on the additional generated data may also improve the dialogue model performance in terms of its policy and responses (similar to a data augmentation method).",
            "7": "Did the authors try this as another compared baseline?",
            "8": "This method should be included in the experiments in order to justify the proposed RL approach is necessary.",
            "9": "- The paper mentioned that the standard RL methods easily fail and generate responses diverging from human language, even when fine-tuning a pre-trained LM.",
            "10": "Hence, it will be better to additionally include the results of other standard RL algorithms for better justifying this claim.",
            "11": "(Current experiments only include the results of models that are free from this issue.)",
            "12": "- In the experiments in MultiWOZ, this paper only evaluates the response generation results.",
            "13": "However, evaluating dialogue policy is also important to justify the learned policy is suitable.",
            "14": "It is unclear why the authors only show the response generation results.",
            "15": "- The experiments contain two setups, one is offline response evaluation via MultiWOZ, and another is interactive simulation via ConvLab.",
            "16": "Both settings show the better performance of the proposed method.",
            "17": "The paper can be better if adding the real-user interactions, because the performance may be different between the simulation environment and the real-user interactions reported by prior results (DSTC in ConvLab).",
            "18": "Conducting real-human interactions can better justify the effectiveness of the proposed RL method in practical scenarios.",
            "19": "In sum, the proposed method is relatively novel and the idea is reasonable.",
            "20": "The performance seems promising in both settings.",
            "21": "However, the paper does not include detailed descriptions about the proposed method, making readers not easy to understand.",
            "22": "Also, some additional experiments need to be added in order to better justify its claims."
        }
    },
    "49A1Y6tRhaq": {
        "3wZWFxBX_pz": {
            "0": "# Summary\n\nThe authors propose to use corpora generated from _emergent communication_ as a fine-tuning signal for NLP tasks (language modeling and image captioning in particular).",
            "1": "They show that, especially in small-data regimes, pre-training on an emergent language can yield significant performance boosts in both tasks.",
            "2": "(In particular, pre-training on emergent language performs on average better than synthetic hierarchical data, but not quite as well as a different natural language, and all of these pre-training methods do better than training from scratch.)",
            "3": "This is the newest of a small but growing body of literature that seeks to connect emergent communication with genuine NLP tasks.",
            "4": "The results are intriguing and promising and should be of interest both to the emergent communication community as well as to the broader community working on low-resource NLP.",
            "5": "Strengths:\n* Novel method of using emergent language for pre-training (as opposed to transferring an entire artificial agent)\n* Some good ablations to identify what contributes to successful transfer\n* A new evaluation metric (emergent --> NL translation performance) that best correlates with fine-tuning performance\n\nWeaknesses:\n* Some parameter choices and the design of some ablations are not completely justified\n* Some additional related works could be included\n\n\n# Minor comments / questions\n\n* \"However, this metric is too rigid in its definition of compositionality, ignoring aspects like argument structure, context or morphology which play a key role in determining the combination of word semantics (Goldberg, 2015).\"",
            "6": "Steinert-Threlkeld (2020) \"Towards the Emergence of Non-trivial Compositionality\" makes similar points and could be cited here as well.",
            "7": "* Why are the lines for \"from scratch\" flat in Figure 2?",
            "8": "I would have thought this was training an LM on progressively larger portions of the relevant data (being used for fine-tuning the others), in which case I'd also expect a downward trend in perplexity.",
            "9": "Are these rather the ppl resulting from training an LM on the full dataset?",
            "10": "It should be stated more clearly in the text what this means.",
            "11": "* Why does the EC pre-training use |V| = 4035, as opposed to 50 that's used in the other tasks and the fine-tuning corpora?",
            "12": "Do the authors expect better results with a larger vocab size?",
            "13": "* Ablation: the model vs corpus transfer comparison seems unfair to me.",
            "14": "In particular, you are comparing a small GRU LM to a larger transformer LM, where the latter is, as you mention, a much more powerful model.",
            "15": "While it is also true that corpus transfer _enables_ this divergence in model types, to really test whether transferring the whole model versus using the corpus works or not, I would think you would want to compare fine-tuning the sender GRU on the LM data vs. starting from scratch _with a GRU of the same type_ and pre-training on the EC corpus before fine-tuning.",
            "16": "Did you do an experiment of that type?",
            "17": "* Related work: I would also include a discussion of this Lazaridou et al paper where they compare ways of combining EC with non-EC learning signals (e.g.",
            "18": "image caption training): https://aclanthology.org/2020.acl-main.685.pdf\n\n* Ethics statement: I appreciate this statement and agree with the possible positive impacts.",
            "19": "Do the authors see any potential negative impacts?",
            "20": "If not, that should also be explicitly stated.",
            "21": "# Typographic comments\n\n* p 1, \"the input out of detractors\" --> \"the input out of distractors\"\n \n* p 2: \"transferable benefits for downstream natural language tasks\" the single hyphens surrounding the subsequent list should be em dashes (three hyphens in TeX)\n\n* p 3: \"uses another GRU layer to decode the message m into a hidden vector hl\" I would use \"encode\" instead of \"decode\" here, since text-->representation is usually what an encoder does\n\n* p 3: \"The most straightforward metric is the accuracy of playing the referential game p(guess = Ii).\"",
            "22": "Given the way p(guess=Ii) is used above, I think this should be more like E[argmax(p(guess=Ii)) = i].",
            "23": "Or if they are measuring the probability assigned to the true image and not just accuracy, the name shoudl be changed from accuracy.",
            "24": "* p 4: \"es should set a upper bound\" --> \"es should set an upper bound\"\n\n* p 5: \" ec perform better than or \" --> \" ec performs better than or \""
        },
        "bM1QjyIArTa": {
            "0": "Strengths:\n- I think this research question is important and interesting, and that the study of what EC agents output and how they might relate to natural language is important for both EC and NLP\n- The setting was made very clear.",
            "1": "Figure 1 is excellent.",
            "2": "- The claims, particularly those about language modeling, are evaluated on a variety of settings\n\nWeaknesses:\n- Some of the claims don't seem entirely justifiable.",
            "3": "It's true that pretraining on EC is better than pre-training on ES or parens at 2M tokens, but it's not entirely clear to me that this is an interesting setting because _all_ models perform poorly.",
            "4": "It's more interesting to me that ES is similar to parens\n- It's not clear to me how specific to this particular experimental setup the proposed evaluation technique is.",
            "5": "It seems to correlate better with LM performance in this setting, but how about others?",
            "6": "Using a particular method for evaluation requires some degree of confidence that it holds generally, and I'm not sure that the results here convince me of that."
        },
        "OBWsOt8Lyc5": {
            "0": "# Strengths\n\n- This is a great idea, and one of those ideas that I (and others) will wish they had come up with.",
            "1": "The idea of pretraining on emergent corpora neatly brings together new ideas about transfer learning and modern studies of emergent communication.",
            "2": "- Significant experiments in both grounded (captioning) and ungrounded (LM) settings convincingly demonstrate the efficiency of EC pretraining.",
            "3": "The ablation studies are also very useful, but also open up several questions (see Weaknesses).",
            "4": "- There is a very rich space of future experiments to try in this area.",
            "5": "I imagine it will inspire plenty of follow-up work, exploring how different aspects of the EC corpus do or do not result in better transfer.",
            "6": "This may also become a typical evaluation w.r.t.",
            "7": "how useful/systematic a language corpus is—how useful is it for downstream transfer?",
            "8": "- The natural language translation metric is extremely clever, and is a natural extension of similar ideas for \"translating\" emergent sentences (Andreas et al., 2018).",
            "9": "# Weaknesses\n\n- Poor comparison to parameter transfer approaches\n    - Perhaps the biggest issue I have with the paper is the lack of comparison to parameter transfer approaches (Li et al., 2020).",
            "10": "The GRU transfer line in Table 2 is the right idea, but is woefully insufficient: it makes no sense to compare a 1-layer GRU perplexity to transformer complexity, and there's no reason that you can't generate the EC corpus with transformers and therefore more cleanly compare corpus transfer to parameter transfer.",
            "11": "I would be really interested in these results.",
            "12": "To be clear, I don't think it's necessary that corpus transfer has to do \"better\" than parameter transfer for the paper to be useful—there are some potential advantages of corpus transfer: (1) learning from EC corpora generated by heterogenous models, (2) cheaper to generate EC corpora from a cheaper model.",
            "13": "But I still really want to see this comparison.",
            "14": "- I don't think this is a fatal weakness, and perhaps it illustrates follow up experiments that this work inspires, which is to vary the architecture/capability of the EC agents and seeing what kinds of agents produce EC corpora that are more useful for transfer, and how those EC corpora differ using the various tools we have for analyzing emergent languages (e.g.",
            "15": "topographic similarity).",
            "16": "- In fact, I think this experiment is nonsensical and misleading enough (every other line in the table uses a Transformer), that, without rerunning modified experiments, I would encourage authors to just remove this number entirely.",
            "17": "- Limited evaluation of non-EC pretraining for image captioning.",
            "18": "- The experimental evaluation is overall quite comprehensive, except in the image captioning studies.",
            "19": "Authors say \"We note that such a transfer scheme cannot work for other synthetic corpora such as music, code, or paren-zipf as they are not grounded on perceptual stimuli.\"",
            "20": "While these other corpora indeed are not grounded in perceptual stimuli, you could still imagine pretraining on the ungrounded data, then finetuning while introducing grounding, or even arbitrarily associating different strings from a corpus with images.",
            "21": "I would really like to see this comparison, as part of the strengths of Sec 3.1 are in the careful comparison of EC pretraining w/ other choices of pretraining corpora.",
            "22": "- Results on permuted EC corpora could be explained further\n    - One outstanding question-issue I have with this paper is the results on permuted EC corpora.",
            "23": "As I understand it, this is for the ungrounded language modeling task for ro and he languages.",
            "24": "The permuted EC corpus does not seem substantially worse than EC pretraining (e.g.",
            "25": "195 vs 211 perplexity is not a huge difference compared with 211 vs 266), so I'm somewhat suspicious of the author's claim that \"speaker develops a coding scheme with sentence structures beyond bag of words.\"",
            "26": "A significance test would be helpful to measure the effect here.",
            "27": "But regardless, it's clear that training on permuted EC results in impressive gains.",
            "28": "Then what benefit does the EC corpus actually provide?",
            "29": "The permutation experiments show that it's not syntax/sequential structure.",
            "30": "And it's not learning a grounding between objects in images and emergent language tokens, since these numbers are for the ungrounded LM task.",
            "31": "Maybe it's just token colocation statistics then?",
            "32": "- As an aside, I would really love to see the same experiments made with the image captioning task as well (e.g.",
            "33": "even if the language ignores sequential structure, the bag of words annotation of relevant objects/features in the grounded input that the permuted EC corpus provides may be just as good as the original EC corpus)\n- Connections to related work.",
            "34": "- There have been a few attempts ta combining emergent communication and supervision, e.g.",
            "35": "via multitask training ([Lowe et al., 2021](https://arxiv.org/abs/2002.01093), also [Lazaridou et al., 2020](https://arxiv.org/abs/2005.07064)) which are missing from related work.",
            "36": "The connection could be made more clear.",
            "37": "Why might we expect corpus transfer to do better or worse than multitask training?",
            "38": "Perhaps these methods could be combined, e.g.",
            "39": "multitask training on EC corpus and real corpus?",
            "40": "# Questions/Minor\n\n- How does changing the vocabulary size and bandwidth of the EC pretraining corpus change its utility for pretraining?",
            "41": "There is surely a point at which EC corpora are too simple (e.g., imagine just a single token and a very small vocab).",
            "42": "Would be interesting to quantify the benefits gained not just by varying corpus size, but by varying emergent language complexity.",
            "43": "Figure 3 partially answers this question (in that untrained EC corpora are not helpful for transfer), but I'd be interested, for example, in seeing a plot of (V, T) and transfer efficacy.",
            "44": "- One of the central issues in studies of emergent communication is that agents often develop non-compositional, unintuitive communication protocols.",
            "45": "The EC setup described in 2.1 doesn't seem to have any of the guards against degenerate languages that more modern studies of EC have investigated, e.g.",
            "46": "different ImageNet categories (Lazaridou et al., 2017) or SimCLR views ([Dessi et al., 2021](https://arxiv.org/abs/2106.04258)), or even other ways of regularizing a language (e.g.",
            "47": "[Luna et al., 2020](https://arxiv.org/abs/2004.03868)) Yet the EC languages still seem useful for transfer.",
            "48": "So questions are, (1) do we still see degeneracy in the languages used in the EC corpora, and (2) if so, does trying to reduce degeneracy/improve systematicity of the language result in better transfer performance?",
            "49": "- Why are there no standard deviations for EC pretrain/from scratch in Table 2?",
            "50": "- Table 3 - how many data points actually go into the computation of correlations here?",
            "51": "Is it 5 x 4 = 20?",
            "52": "I would really love to see the full scatterplots rather than just correlation, e.g.",
            "53": "in the appendix, and to actually see how much variance we see in the different EC corpora, as measured by the metrics.",
            "54": "- The details of the training steps expt in Figure 3 are a little underexplained, and I tried my best to reconstruct, though I invite the authors to clarify if I've misunderstood anything.",
            "55": "You take checkpoints of the EC corpora generated from EC agents at 0, 200, 400, ... 1k training steps, and at each step measure the various EC langauge metrics, and also do the pretrain/finetune experiment with transformers, starting with the EC corpora, to get plot d - perplexity.",
            "56": "Is that correct?"
        },
        "Rl8WUe07Ms_": {
            "0": "The paper's very clearly written, well-organized, and much of the discussion was grounded in a broad range of references to previous work.",
            "1": "My concerns with this paper are in regards the experimental design, the strength of the conclusions, and how the authors are choosing to interpret them.",
            "2": "Perhaps the best way to discuss this paper is through the authors' goals, to understand: unsolved: \"(i) can emergent languages be used outside the game?",
            "3": "(ii) if so, would these metrics predict the usefulness of emergent languages for downstream tasks?",
            "4": "We tackle these two questions in Sections 3 and 4 respectively.\"",
            "5": "## can emergent languages be used outside the game?",
            "6": "To question (1), it is of course important to note that this has already been shown in Li et al.",
            "7": "2020 in the context of pre-training for MT.",
            "8": "There is plenty of opportunity to broaden the scope of their findings, but as it pertains to this question, it no longer needs to be asked and should be presented truthfully in this way.",
            "9": "This language persists throughout, and a reader not aware of Li's work would easily go through most of the paper believing that this paradigm is being attempted here for the first time.",
            "10": "For instance, even at the end of the paper \"We present a new perspective for studying emergent communication by linking the corpora of emergent and natural languages in two ways.\"",
            "11": "One of these seems like an established perspective, and what sets this work apart is primarily the choice of tasks: language modeling and image captioning.",
            "12": "Returning to the question, can emergent languages be used outside the game for language modeling or image captioning?",
            "13": "First, it is also important to note that this EC setup is only one possibly game, and the properties of the EC are hugely shaped by the properties of the game.",
            "14": "So again, for accuracy, it's important to clearly define the scope of the work as pertaining to this one particular scenario.",
            "15": "For language modeling, the authors show that indeed the EC pre-training is useful for the downstream language modeling task, but the results are not very convincing that this would ever be useful in a practical sense.",
            "16": "Pre-training on EC data is rarely the best option, often outperformed by simple synthetic language data, and is never the best option as more data is observed.",
            "17": "And since pre-training on Spanish is still the most effective strategy even in low-resource languages, and Spanish is more easily acquirable than EC data, in what situation would one ever opt for using this strategy?",
            "18": "Back to the research question, yes, EC languages *can* be used outside the game, but it's not apparent that it would ever be best choice nor the easiest.",
            "19": "The case is similar for the captioning task, except the margins (even those between the best choice -- natural language -- and the worst choice baselines) is exceedingly small.",
            "20": "I think it throws into question whether MS-COCO was ever a good testbed for this research question, as (as the authors note) the captions are very regular and structured.",
            "21": "It seems like a reasonable attempt, but it's difficult to find a take-away.",
            "22": "When it comes to understanding the properties of the emergent languages, how these properties are translating into useful biases for downstream tasks, it is also left quite open.",
            "23": "The ablation experiments compare against random scenarios, and I personally wasn't able to gain much insight into where the performance comes from.",
            "24": "Personally I would have liked more qualitative analysis into the real correlations and differences of the EC and natural languages as it pertains to the downstream tasks, but even barring that I imagine there are also some purely quantitative metrics which could have been more targetted, and in doing so, be more informative.",
            "25": "## if so, would these metrics predict the usefulness of emergent languages for downstream tasks?",
            "26": "Another thread of this work is criticism of some existing EC metrics, like topographical similarity, which may be failing to correspond well to the types of structure and compositionality found in NLs.",
            "27": "The authors propose a new method of evaluating ECs by training (briefly) a translation model to translate to natural language captions from EC language generated from the same images.",
            "28": "The metric corresponds better to downstream task accuracy than topographic similarity.",
            "29": "The authors argue that \"intuitively, a higher translation score means the emergent and natural sentences are closer in structure and semantics, similar to how French-English translation might be easier than Chinese-English\".",
            "30": "But isn't the purpose of a such a linguistically-focused EC metric to evaluate how well EC reflects the types of structures found in natural (big-L) Language?",
            "31": "i.e., not any specific language.",
            "32": "It seems that in closely tracking the language used in the caption data, it is only natural that this relates to higher downstream task accuracy, but isn't that at the cost of low similarity to another natural language?",
            "33": "Criticisms of existing metrics are likely warranted, but I fail to see how this metric really improves upon it.",
            "34": "An EC learns a Chinese-like structure from a caption dataset, the translation model performs poorly at mapping it to English (vs a German-like EC source language) and now we have a plausible, NL-like EC with low scores under the metric.",
            "35": "I think ideally a good EC metric should score all human languages highly, or one should answer the question of which of two human languages contains more \"human-like\" language structure.",
            "36": "A metric that targets specific aspects of human languages cuts around this issue, but I think in this case the authors are burdened with providing some answer to this question.",
            "37": "So overall the story of the paper feels like a bit of circuitous reasoning: EC languages might be more helpful to downstream tasks than natural languages if they somehow capture a greater structural similarity to the target (presumably low-resource) language than available natural languages.",
            "38": "But the closer the language to existing natural language, the less useful it is, since it doesn't provide a unique advantage over simply using that natural language.",
            "39": "And in the space of all human languages, only the one closest in structure to the particular fine-tuning task achieves high marks.",
            "40": "So I'm curious how the authors reconcile this?",
            "41": "## Conclusion\n\nOverall the work is tackling important problems with an interesting method, but in its current state I believe it's lacking clear insights into the problem, does not provide a practical usefulness, and doesn't dig sufficiently deep in understanding why models behave the way they do.",
            "42": "It also feels almost like two disjoint papers -- a paper could explore pre-training on ECs but with more focus given on understanding the properties of the ECs, and broadening it to more games to ensure lessons are more generally applicable.",
            "43": "Another paper could target metrics like topographical similarity, but in this case it is important to discuss the seemingly obvious failure cases of the proposed measure.",
            "44": "I think this is summed up well in the conclusion:\n\"Our methodology and results open up possibilities of establishing a synergy between the research of language emergence and natural language processing, where we could potentially better understand and improve emergent communication through the lens of natural language, and in turn improve upon low-resource natural language tasks with the help of emergent languages\"\n\nWhile the phrasing is again speculative/potential, I find it hard to view the paper experiments as strongly supporting either of these cases: I don't think we've learned much about the structure of ECs, even the limited examples used here in the paper, and I don't know of situations where EC would reasonably be used for improving low-resource monolingual NLP over NLs.",
            "45": "That's not to say the paper doesn't have a lot going for it, but each experiment felt like it only scratched the surface of the problem, and more thorough research feels necessary.",
            "46": "Typos / Grammar:\n((Figure 3(a)))\n\"Our research in based on\""
        }
    },
    "rpxJc9j04U": {
        "ssJ8jrDMNRq": {
            "0": "This paper is an interesting application of a data augmentation or self-supervised learning type of approach for tactic based theorem proving.",
            "1": "The idea itself is not completely new as the authors readily explain in the paragraph MACHINE LEARNING WITH PROOF ARTIFACTS on page 2.",
            "2": "The paper appraisal therefore rests on the clarity of presentation, how convincing the experiments are, and how reproducible.",
            "3": "Overall the paper presentation is okay, although the clarity could be improved.",
            "4": "One issue is that the main contribution is  mostly condensed into section 3.2 which is less than one page.",
            "5": "In general the writing does not make the mechanisms by which proof artifacts may be extracted from Lean clear enough.",
            "6": "It would be nice to include (possibly as an appendix) a focussed primer on Lean which allows to give more depth in the main paper while still being relatively self contained.",
            "7": "The experments are fairly convincing, although it is not entirely surprising that this approach works, and to repeat, the basic idea of extracting additional training data in this way is not entirely new.",
            "8": "TacticZero by Wu et al 2021 is missing from the related work.",
            "9": "This is relevant because, by training end to end, that work effectively generates arbitrary amounts of training data through interaction the the HOL4 ITP system (intermediate theorems which are proven give some reward in that work).",
            "10": "Typos:\n- synthesis -> synthesise\n- DeepHOLZero -> DeepHOL\n- wrong bold number in Figure 3"
        },
        "jR35uP3NY_i": {
            "0": "The main idea of using proof terms for self-supervised training is reasonable.",
            "1": "The paper is well-written and the main approach is easy to understand.",
            "2": "The experiment details are thoroughly demonstrated and the results look solid.",
            "3": "The main drawback of this paper is the lack of technique novelty in terms of matching learning.",
            "4": "Question: when extracting subterms from each proof term, do you parse a proof term according to the steps of human-written proofs, or do you parse the proof term according to its own grammar?"
        },
        "40qJYzDMoA-": {
            "0": "Below is my (mildly edited) previous review for another conference that the authors have seen more than half a year ago.",
            "1": "There are no significant changes to the major issues of the paper and hence practically no changes to my review.",
            "2": "The major issues with this work remain to be: \n\n1.",
            "3": "Poor data hygiene.",
            "4": "Training on an unpublished WebMath corpus that may contain unknown amount of information about the testing data.",
            "5": "This practically invalidates most of the experimental results.",
            "6": "Training on (html-ized/isomorphic version of) the testing data is as much taboo in ML as circular proofs (proving T by T) in math.",
            "7": "Unless there is full disclosure, there is no way to assess the results and compare them to other results obtained by established ML/ATP training/evaluation methods.",
            "8": "If nothing else, the authors had more than half a year to publish their training dataset and let the readers and reviewers analyze it.",
            "9": "2.",
            "10": "No ATP evaluation of the system when trained without WebMath.",
            "11": "Given (1) this should have been done long ago but is still missing.",
            "12": "3.",
            "13": "To re-iterate on (1) and the futility of not addressing the issues head-on, note that the checks for contamination (p. 14) added in this version of the paper are again poorly designed.",
            "14": "A quick search of the html code of mathlib on the web shows pages such as https://leanprover-community.github.io/mathlib_docs/order/zorn.html where the rintro keyword is wrapped in the \">\" \"<\" tokens (likely to do syntax highlighting in html).",
            "15": "Old review:\n\n%%%%%%%%%%%%%%%%%%%\n\nStrengths:\n\n- The largest contribution is that this nontrivial engineering work has\nbeen done for the first time for Lean, a relatively young but quickly\ndeveloping proof assistant.",
            "16": "Making a working system such as this is\nnever easy, due to many technical issues.",
            "17": "The system works and already\nassists Lean users.",
            "18": "- The results seem encouraging, reaching between 32% and 48% depending\non what is used for pretraining.",
            "19": "- The most honest evaluation on the temporarily held-out set gives 37%,\ncompared to 22% done by a rather naive search procedure.",
            "20": "This is a\nreasonable improvement.",
            "21": "- A number of training tasks are experimented with in the\nLean setting.",
            "22": "Neither the tasks nor the results seem extremely novel\nor surprising compared with systems like TacticToe and its successors,\nbut this is certainly solid and useful work done for Lean.",
            "23": "Weaknesses:\n\n- Similar to the previous work on Metamath, it is hard to understand\nwhat are the possible leaks between the pretraining \"WebMath\" corpus\nand the testing set.",
            "24": "This seems quite negligent when training large\nmodels with almost a billion of parameters with a large capacity for\nmemorization.",
            "25": "The evaluation should really include the success rate\nwithout the WebMath pretraining.",
            "26": "- To expand on this, many ITP corpora are published on the web and in\npublic repos in many different forms, sometimes after various\nsyntactic translations (to latex, etc.",
            "27": "), which are however often\neasy to recover by neural architectures\n(https://doi.org/10.1007/978-3-319-96812-4_22).",
            "28": "It is completely\nunclear to me how big would be the effect of GPT pretraining on the\ntest dataset translated in various ways.",
            "29": "See e.g.",
            "30": "the work of\nGauthier for relatively simple statistical methods that quite\nreliably transfer the proof knowledge between syntactically\ndifferent corpora (https://doi.org/10.1016/j.jsc.2018.04.005 ,\nhttps://doi.org/10.1007/978-3-662-48899-7_26 ).",
            "31": "- It is unclear what resources go into the test evaluation that\nremotely uses gptf.",
            "32": "This makes an honest comparison with non-remote\nmethods using standard hardware difficult.",
            "33": "This should be at least discussed.",
            "34": "- I do not understand the argument that RL-based data synthesis is\nmore expensive.",
            "35": "The RL-style proof data synthesis done in TacticToe,\nhttp://arxiv.org/abs/1805.07563 or\nhttps://doi.org/10.4230/LIPIcs.ITP.2019.34 is most likely orders of\nmagnitude cheaper than just the pretraining done on WebMath here.",
            "36": "- While the contribution is solid and data augmentation methods are certainly useful in DL, the introductory claims about data scarcity being a newly encountered difficult problem in ML-for-TP are uninformed.",
            "37": "In particular, all of the larger ITP corpora (Isabelle, Mizar, HOl, Coq) are capable of easily exporting\nmillions of problems and proofs to start with and this has been done\nmany times since long ago.",
            "38": "Already the 2003 version of MPTP and\nthe AI/TP experiments based on it\n(https://doi.org/10.1007/s10817-004-6245-1) allowed and announced a\nstraightforward generation of 630000 related proof tasks from\nMizar.",
            "39": "Further millions/billions/zillions of proving and training\ntasks can be created easily by chasing the large derivation graphs\nof the ITP libraries.",
            "40": "This has been to various extent used in works\nsuch as [1,2,3], where the benefit of learning from additional proof\ntasks was also demonstrated.",
            "41": "Equally easy and cheap is chasing the\ngraphs of large ATP proofs and generating problems from them,\nrunning ATPs to generate terabytes of further data, etc.",
            "42": "The theorem\nproving domain is really the antithesis of data scarcity, has been\nsuch for long time, and it is one of its great advantages over NLP\ndomains.",
            "43": "- A number of related experiments have been done with argument,\nwitness, conjecture and proof (step) synthesis recently.",
            "44": "See e.g.",
            "45": "[4-7].",
            "46": "[1] Cezary Kaliszyk, Josef Urban:\nLearning-assisted theorem proving with millions of lemmas.",
            "47": "J. Symb.",
            "48": "Comput.",
            "49": "69: 109-128 (2015)\n\n[2] Kaliszyk, C., Urban, J.",
            "50": "& Vyskocil, J. Lemmatization for Stronger Reasoning in Large Theories in\nFroCoS 2015 9322 (Springer, 2015), 341–356.",
            "51": "[3] Bartosz Piotrowski, Josef Urban:\nStateful Premise Selection by Recurrent Neural Networks.",
            "52": "LPAR 2020: 409-422\n\n[4] Thibault Gauthier:\nDeep Reinforcement Learning for Synthesizing Functions in Higher-Order Logic.",
            "53": "LPAR 2020: 230-248\n\n[5] Thibault Gauthier:\nDeep Reinforcement Learning in HOL4.",
            "54": "CoRR abs/1910.11797 (2019)\n\n[6] Bartosz Piotrowski, Josef Urban:\nGuiding Inferences in Connection Tableau by Recurrent Neural Networks.",
            "55": "CICM 2020: 309-314\n\n[7] Josef Urban, Jan Jakubuv:\nFirst Neural Conjecturing Datasets and Experiments.",
            "56": "CICM 2020: 315-323\n\nUPDATE:\n\nI do not agree with the idea that the potential test set contamination would be a significant advantage due to its autoformalization potential:\n- The Wang et all 2018 paper I mentioned and their related 2020 paper show that RNNs and Transformers are very good at such translations.",
            "57": "I don't agree this would be surprising today for GPT.",
            "58": "- This is really an opposite of a \"significant advantage\".",
            "59": "The test set evaluation is thus made incomparable to any other honest evaluation done on Lean in the future.",
            "60": "- The WebMath dataset has not been published and is impossible to check by readers and researchers.",
            "61": "I would expect at least its publication as a partial response to such concerns.",
            "62": "- Not doing the test set ATP evaluation without the WebMath pretraining is a serious omission.",
            "63": "I also do not see any improvement in explaining the hardware resources used for the evaluation.",
            "64": "This again makes the numbers here hard to compare with for other researchers and methods.",
            "65": "There is also hardly any improvement of the overclaims (noted also by other reviews) about the technical novelty, claims about comparable slowness of RL setups, etc.",
            "66": "%%%%%%%%%%%%%%%%%%%"
        },
        "UU707mY2xv4": {
            "0": "* This work makes the following contributions.",
            "1": "1.",
            "2": "The PACT methodology: The paper proposes a methodology for extracting auxiliary tasks that can be trained jointly along with the main task (tactic prediction task).",
            "3": "The research shows how low-level proof artifact data may be used to significantly boost performance on high-level theorem proving by co-training auxiliary tasks.",
            "4": "The auxiliary tasks themselves will be useful for designing similar tasks for other theorem provers.",
            "5": "More importantly the PACT methodology is more general; the idea of incorporating diverse auxiliary tasks as a language modeling task by introducing a distinct token for each task is novel.",
            "6": "In my opinion, this idea has a potential to be applied to domains other than theorem proving.",
            "7": "2.",
            "8": "The paper contributes LEANSTEP dataset and the Learning environment.",
            "9": "This is the first such dataset for the Lean Theorem prover.",
            "10": "It is nontrivial to gather the data as it involves hooking into the Lean's compilation process.",
            "11": "The source for generating the data is a big contribution to the theorem prover and machine learning community.",
            "12": "* The paper is well motivated and easy to understand and follow.",
            "13": "The methodology is explained clearly and experiments are executed with a considerable amount of detail.",
            "14": "I feel that the conclusions are in general well supported by the results.",
            "15": "* It is hypothesized that PACT acts a regularizer while imparting useful knowledge to the model due to mutual information across tasks.",
            "16": "An ablation study is carried out to rule out the possibility that the benefits from PACT come from simply regularizing the model.",
            "17": "* It is interesting to know that WebMath pre-training is still helpful even in the presence of PACT.",
            "18": "There are several such insights in the experiments section that will be helpful to the community.",
            "19": "**Weakness/Suggestions/Questions**\n\n* Although The paper is well written overall, I think section 3.2 Proof Artifact Training can be improved by adding an example explaining the Lean terminology proof term, proof type, tactic, tactic state, etc.",
            "20": "It is a bit difficult to understand the task definitions without first understanding the various constituents of the proof.",
            "21": "* Description of refl and tidy-bfs baselines appears much late in the paper.",
            "22": "It would be nice if these baselines are described before Fig.",
            "23": "2 is referred.",
            "24": "* It is explained in the paper that the runtime environment ensures that the proofs are never circular.",
            "25": "Is any care taken to handle this in training data?",
            "26": "* I am wondering why the authors chose Lean for this this work (as opposed to say MetaMath).",
            "27": "Is it just because of the popularity of Lean or the richness of Lean tactics.",
            "28": "Can PACT be applied to simpler theorem provers like MetaMath where there are no tactics?",
            "29": "I suppose it would be hard to define auxiliary tasks for simpler systems.",
            "30": "Would like to know author's view on this.",
            "31": "* page 3: s/\"It has also been previous observed\"/\"It has also been previously observed\""
        }
    },
    "gEZrGCozdqR": {
        "EVEmcRlRkAc": {
            "0": "While the method is a simple and straightforward scaling up of concepts and ideas from prior works (e.g.",
            "1": "Zhong et al, Adapting ...; Mishra et al, cross-task generalization ...), the empirical results are thorough and impressive (outperforming GPT-3 with a slightly smaller model).",
            "2": "The analyses also helps us understand when this method would work and inform us about future research directions.",
            "3": "Below are my concrete questions and comments: \n\n**Additional Tasks Results (3.4)**\n\nIn Appendix A.1, the paper mainly draws conclusions based on comparisons between GPT-3 and FLAN, which I do not think are fair: GPT-3 and FLAN differ in model size and pre-training data distribution.",
            "4": "Instead, I think Base LM vs. FLAN might be a better comparison between “Off the shelf LM” and “instruction-tuned” model (though it won’t change the conclusion).",
            "5": "It is also worth pointing out in the main paper that for most of the additional tasks, even though it does not lead to higher accuracy, the performance of FLAN is still at least comparable (e.g.",
            "6": "<1% worse and difference generally negligible) to Base-LM 0-shot.",
            "7": "The only outlier seems to be ReCorD where the performance drops significantly after instruction-tuning, and this probably deserves some discussion.",
            "8": "Also I might have missed it - for the Base LM 137B zero-shot result, is it on the average template, or the best template?",
            "9": "**Number of Task Clusters (section 4.1)**\n\nFor Figure 5 can you add the untuned model to the curve with the x-axis=0 (0-task cluster)?",
            "10": "This can help us understand how much even 1 cluster (e.g.",
            "11": "summarization) may help.",
            "12": "**Explanation for scaling (section 4.2)** \n\nIt is an insightful empirical result that instruction tuning only works when model size reaches 68B.",
            "13": "However, I am not entirely sure about the potential explanation of “model capacity”.",
            "14": "There might be two potential explanations to this phenomena: 1) “model capacity”: as the paper has mentioned, smaller pre-trained models do not have enough model capacity and underfit the instruction tuning data, and 2) “better OOD generalization”: better quality pre-trained models have higher OOD generalization ability (and OOD accuracy) and they are less likely to “overfit” to in-distribution data.",
            "15": "I personally find the second explanation more convincing.",
            "16": "For example, Sahn et al.",
            "17": "(https://arxiv.org/abs/2110.08207) finds that even models with only 11B parameters can generalize to unseen tasks, using T5 (MLM) and a larger set of prompts.",
            "18": "The use of MLM objectives (might) improve the pre-training quality, while more prompts reduce the “overfitting to in-domain data” issue.",
            "19": "I appreciate the fact that the author explicitly states the model capacity hypothesis more as a conjecture rather than a solid explanation.",
            "20": "It’d be great if the authors can support the explanation further with more empirical evidence.",
            "21": "On the other hand, however, since the results from Sanh et al came out only 2 weeks ago,  I would not change the score based on the response to this question.",
            "22": "**In-context Few-shot vs.",
            "23": "Fine-tuned Few-shot (Section 4.3)**\n\nCan the authors compare “fine-tuning/prefix-tuning an instruction tuned model with 16 examples” (appendix C but with only 16 examples) with “in-context prompting” (in 4.3 of the main paper), similar to Chen et al.",
            "24": "(https://arxiv.org/abs/2110.07814 )?",
            "25": "This would further inform us how we should use the few-shot learning examples for larger language models: put it in-context, or fine-tune?",
            "26": "Again, since the comparison of Chen et al.",
            "27": "came out only 2 weeks ago and the paper limit is 9 pages, I would not change the score based on the response to this question.",
            "28": "**Others**\n\nResults of Appendix C are interesting and potentially impactful - this might imply that instruction-tuned models will become the new “base” model for the pretraining-finetuning paradigm.",
            "29": "Is it possible to briefly mention it in the main paper as well (and redirect the readers to the appendix to see the full results)?",
            "30": "It might be too late to change the name, but “Finetuned LAnguage Net” (FLAN) is uninformative, since it does not capture any unique aspect of this method.",
            "31": "What does “LAnguage” mean here, \"natural language instruction\" or \"language model\"?",
            "32": "If it is the former, then directly including the word “instruction” might be better; and hopefully it’s not the latter, since even fine-tuned BERT on SST-2 counts as a fine-tuned language model ...\n\n**Typo**\nIntro: \nInstruction tuning is “a” simple method that, as ….",
            "33": "Conclusion:\nMoreover, our work supercedes recent work such “as”"
        },
        "G8KXj5zrhxo": {
            "0": "----\nDetailed comments:\n----\n\n- \"For each dataset, we manually compose ten unique templates\":  Why not have templates per task cluster instead of per dataset?",
            "1": "it is likely a relatively minor effect given the results from Appendix B but it seems like it could slightly prevent overfitting\n\n- The ablation in 4.1 was great (number of clusters).",
            "2": "Nit: I would have tried to move the (datasets per cluster/templates per dataset) ablation to the main body as well and shortened Section 3\n\n- The 4.2 (scaling laws) ablation is perhaps the most interesting of all.",
            "3": "- In figure 6A, why was performance not increasing for untuned models w.r.t model size?",
            "4": "This seems to contradict findings from Brown et al where larger models did better on essentially all tasks.",
            "5": "Were there perhaps some poor datasets that happened to be in the held-in split (since the held-out tasks don't seem to have the same trend)?",
            "6": "----\nAppendix:\n----\n\nI liked the section B ablations (as implied above).",
            "7": "That more templates per dataset didn't help is particularly interesting and suggests some questions.",
            "8": "You hypothesize that more templates doesn't help because \"models at such scale do not easily overfit to a finetuning single task\" - but my intuition is for an opposite explanation -- that the models at such scale easily memorize a small number of templates!",
            "9": "One may even wonder if the instruction nature of the templates is helping at all.",
            "10": "From what I can tell, Appendix C on prompt tuning (which is very interesting) is maybe the primary evidence the instructions are important.",
            "11": "I think more could be done here, some ideas, probably there are better ways to test:\n- Have templates that leave out \"instructions\":  I would guess it wouldn't affect held-in task performance much, but would affect held-out tasks.",
            "12": "- Consider HellaSwag/PiQA/etc, where FLAN underperformed few-shot and even zero-shot.",
            "13": "One might hypothesize that if using a (subotimal) template that is less natural for language modeling, that zero-shot performance would suffer, but that FLAN performance wouldn't\n- One might hypothesize that the \"turn the task around\" templates help more than the other more straightforward templates that don't swap information between the prompt and response.",
            "14": "- Easy but probably not great thing to try:  held-out tasks with wrong/useless templates\n\nA final thought:  It's not obvious that using as many training examples per dataset as possible is optimal, given that the model could overfit to dataset-specific spurious correlations.",
            "15": "This could be another area to investigate\n\n----\nMisc: \n----\n\n- UnifiedQA seems potentially worth citing as prior work"
        },
        "g-U7KRV0UCv": {
            "0": "Pros:\n1.",
            "1": "The problem addressed has high practical value: it tries to make large pre-trained language model more accessible to a range of NLP tasks.",
            "2": "The \"instruction tuning\" idea will significantly reduce the cost for task-specific fine tuning, labeled data and prompt engineering compared to other approaches.",
            "3": "2.",
            "4": "The method is simple and easy to implement.",
            "5": "Authors carefully design the experiment to minimize the leakage between the fine-tuning and inference data.",
            "6": "Given that, it still shows superior performance on different types of NLP tasks.",
            "7": "The result on specific task can be further improved when adapting with \"prompt tuning\" on labeled data, which shows that the instruction-tuning process does not drop much task-specific knowledge from the original pretrained model.",
            "8": "3.",
            "9": "The analysis presented in the main paper and the appendix is thorough enough.",
            "10": "Authors also discussed about the limitation of model when downstream tasks are more similar to language modeling tasks.",
            "11": "Cons:\nThere are still a few questions that can be addressed to make the analysis comprehensive.",
            "12": "1.",
            "13": "Have authors try to use the FLAN prompts on GPT3 or BaseLM and how does the performance look like?",
            "14": "2.",
            "15": "Since instruction tuning will adjust all the parameters in the original pre-trained language model, there is a question what about what is the potential impact of this tuning process?",
            "16": "Will it drops any knowledge of any tasks, which will be a disadvantage when the task's labeled data is available?",
            "17": "In the Analysis C in the appendix, it will be good to have results for tasks other than classification such as summarization or question answering; and also to have a baseline where the BaseLM model is fine-tuned directly with the task labeled data (without prompt/soft-prompt)."
        },
        "-YgUOpXUim": {
            "0": "Overall well-written with compelling results, this paper describes a new language model (FLAN) and shows how it improves upon the zero-shot task performance of previous language models such as GPT-3.",
            "1": "While the paper is lacking some additional analysis, I am hesitant to recommend extremely compute-intensive ablations due the large size of the model (137B parameters).",
            "2": "Strengths:\n - Considers a reasonably wide set of 62 datasets; although the inherent arbitrariness in dataset clustering was listed as a limitation, the clusters look quite reasonable to me, and the removal of overlapping datasets (e.g., \"Reading Comprehension w/ Commonsense\") seems appropriate.",
            "3": "- Results are better than a strong Base LM baseline, as well as existing state-of-the-art models (GPT-3)\n - Overall the approach is intuitive and conceptually compelling\n - Highly relevant to ongoing work on language modeling, prompt tuning, and zero-shot learning\n\nWeaknesses:\n - From these experiments, it is unclear whether models are actually \"learning to follow instructions\" or just learning a very large space of tasks from the fine-tuning procedure.",
            "4": "In other words, even though prompt variance is reported at inference time, the models could potentially perform just as well with nonsense or missing prompts during fine-tuning.",
            "5": "As far as I can tell, no experiments that rule out this possibility exist.",
            "6": "- Although qualitatively useful, the analysis in 4.1 does not conclusively show that the number of instruction tuning clusters aids performance, or that this trend is likely to continue with more clusters.",
            "7": "Most of the gain could be acquired by tasks which are most difficult, or most similar to the heldout task, and this analysis cannot disprove such an interpretation.",
            "8": "A proper analysis would consider more heldout tasks and permutations of training data, but presumably this is prohibitively expensive.",
            "9": "- The paper is missing important details about hardware usage and training time\n - Some possible issues which might be resolved by the additional questions below\n\nAdditional Questions:\n - \"For each dataset, we manually compose ten unique templates that use natural language instructions to describe the task for that dataset.\"",
            "10": "Do you have unique prompts for each dataset or only for each dataset cluster?",
            "11": "Based on a cursory look at the supplementary material, I would assume the latter.",
            "12": "- I didn't fully understand the justification for the OPTIONS token.",
            "13": "Are the fine-tuned models successfully putting (almost) all of their probability mass on the corresponding options?",
            "14": "How is the Base LM evaluated (if it's not fine-tuned, presumably it doesn't learn how to handle these options)?",
            "15": "- Figure 6A: why does the untuned model see worse performance with more parameters?",
            "16": "Nits:\n - Figure 1 (Bottom) is possibly misleading, since AFAICT zero-shot FLAN underperforms few-shot GPT-3 on the majority of tasks\n - Not clear what \"turning the task around\" means for some tasks, or why this is a useful type of prompt diversity"
        }
    },
    "rF5UoZFrsF4": {
        "1E-QONO6Og": {
            "0": "**Strengths:**\n\n1.",
            "1": "Overall, the formulation of the problem and various design choices (global positional encoding, focus map, pointer head for the grounding task) make sense.",
            "2": "The authors also seem to have carefully designed their experiments.",
            "3": "For instance, ensuring that comparisons are fair (same number of parameters, etc) for the object detection task.",
            "4": "2.",
            "5": "Given the nature of the problem statement (with multiple tasks, inputs and outputs), the authors have done a good job in explaining each of them properly.",
            "6": "Overall, I thought the paper was easy to read and understand.",
            "7": "3.",
            "8": "The authors provide several reasonable insights that might be relevant to the user interface modeling community — using object detection as part of multi-task learning instead of standalone pre-training task, design choices to create a single unified architecture for all the tasks, multi-task learning outperforming single-task learning etc.",
            "9": "**Weaknesses**\n\n1.",
            "10": "I understand that a single model is helpful for multiple UI tasks, but I wonder if this approach is scalable beyond the 5 tasks and 5 modalities mentioned.",
            "11": "The authors comment that the model architecture is designed to remain stable for a growing set of tasks, but this seems to be under the assumption that the input and output modalities would remain constant.",
            "12": "A discussion on how to add new tasks to the same framework might help, or a discussion on why the current framework is enough.",
            "13": "2.",
            "14": "One of the important motivations of multi-modal multi-task learning mentioned was to achieve better or on-par performance with a single model (and supposedly fewer computations) which is crucial for devices with limited computing resources.",
            "15": "But a direct head to head comparison for the computational cost of a multi-task model and individual models is not provided.",
            "16": "Are there any overheads/disadvantages because of multi-task learning (Like a larger model size, inference time for individual tasks etc)?",
            "17": "The model sizes are varied across experiments to achieve on par performance which makes the comparison of the computational cost not so obvious.",
            "18": "Such a comparison would highlight the advantages of the multi-task model and would be helpful for the relevant audience.",
            "19": "3.",
            "20": "I also believe that the downstream tasks are also somewhat similar (language command grounding, tappability, UI object detection, UI summarization, widget captioning).",
            "21": "It is therefore not surprising that multi-task learning should help these tasks.",
            "22": "4.",
            "23": "There is no comparison provided with any baseline for 2/5 tasks (Language grounding, Tappability)\n5.",
            "24": "The novelty in the paper is somewhat lacking.",
            "25": "The techniques used in the paper (multi-branch transformer, pointing mechanism, cross-modal attention, global positional encodings, etc) have been shown to work in the past for image-text tasks [1, 2].",
            "26": "The paper is taking all the lessons from past works and applying it to a new domain.",
            "27": "**Clarifications:**\n\n1.",
            "28": "The training procedure mentioned in section 5.2.2 talks about joint training but the procedure followed for training for individual tasks or a subset of tasks is not described in detail.",
            "29": "Were the same hyperparameters used for all configurations?",
            "30": "**Updates after rebuttal period** \nThe authors addressed some of the concerns -- showing inference time, model size and a discussion about training details and hyperparameters in the appendix.",
            "31": "However, I am not convinced that the paper presents new insights that are relevant for the broader ICLR community.",
            "32": "Concerns around novelty and the multi-task setup was also raised by another reviewer (e7Hg).",
            "33": "Responses to reviewer e7Hg also aren't convincing.",
            "34": "If the tasks are not similar, and the learning objectives are not aligned, then the motivation for multi-task learning is solely for reducing memory footprint and computational cost.",
            "35": "I believe that this contribution isn't enough for me to recommend acceptance.",
            "36": "Thus, I am going to stick to my original rating.",
            "37": "[1] R. Hu, A. Singh, T. Darrell, M. Rohrbach, *Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA*.",
            "38": "in CVPR, 2020\n\n[2] J. Lu, V. Goswami, M. Rohrbach, D. Parikh, S. Lee, 12-in-1: Multi-Task Vision and Language Representation Learning.",
            "39": "in CVPR, 2020"
        },
        "7_iPaRPN95h": {
            "0": "In summary, their main works can be summarized as followed:\n(1) The authors formulate multi-modal multi-task learning for graphical user interfaces and design a VUT model.",
            "1": "(2) The authors create a so-called Language Grounding dataset for language grounding task.",
            "2": "This topic is interesting and it is pioneer to incorporate muti-task learning in graphical user interfaces.",
            "3": "The manuscript is easy to follow.",
            "4": "And the technical contribution is significant.",
            "5": "The experimental results conducted on 5 distinct UI tasks demonstrate its superior performance over some state-of-the-art methods.",
            "6": "The reviewer suggests borderline.",
            "7": "The weakness of this paper is as follows.",
            "8": "(1)\tTables 5,6,7 and 8 compare the multi-task learning with the single task counterpart.",
            "9": "However, their results show that the multi-task learning cannot significantly facilitate the studied tasks.",
            "10": "For example, in the Table 5, the setting of “Screen Summarization alone” performs better than the setting “all 5 tasks”.",
            "11": "This is contrary to the original intention of this paper.",
            "12": "More explanations should be given.",
            "13": "(2)\tThere are some typos in this manuscript, e.g., “We experiment with VUT on 5 distinct UI task”."
        },
        "hNn9VfKmF9r": {
            "0": "Strengths\n+ Formulated a multi-modal multi-task learning for graphical user interfaces\n+ A new two-tower Transformer architecture is designed\n\nWeakness\n- By jointly trained on 5 tasks, the model is on par or slight improves over models trained on individual data sets.",
            "1": "This does not match the observation on other image-language data joint training where significant improvement is achieved.",
            "2": "- The 5 tasks are closely related, and it is hard to see how the model can generalize to a different task, such as generating/editing new UI layout\n- The main architecture of the model is new, while all the building blocks are not.",
            "3": "The technical contribution is somewhat lacking."
        }
    },
    "1bEaEzGwfhP": {
        "xjw9FFpgYpq": {
            "0": "The outset of the paper is valid: Conditioning on the complete (or longer) revision history can potentially model the editing process of documents more accurately than models that just take the latest history into account.",
            "1": "However, I'm not completely convinced by the evaluation.",
            "2": "I think that it is hard to justify an approach with perplexity improvements only.",
            "3": "Perplexity can be a valuable tool to get a better understanding of the system, but ultimately showing the usefulness in a downstream task can make a much stronger case.",
            "4": "The two downstream tasks in the paper that do not use perplexity (Tab.3 / Sec.",
            "5": "6.2) are (a) predicting code commit messages / Wikipedia edit comments and (b) semantic intent classification.",
            "6": "If I understand correctly, both of these tasks actually do not require the *prediction* of the document edits.",
            "7": "Tab.",
            "8": "3 does not compare with related work on these tasks.",
            "9": "BLEU and F1 scores do improve with increasing order which is a good sign, but simple baselines/ablations would strengthen the results.",
            "10": "For example, task (a) may simply benefit from having access to previous commit messages, not from a better modeling of the edits.",
            "11": "Similarly, for (b) the model could learn that two subsequent \"Vandalism\" revisions are unlikely without using edits from previous revisions.",
            "12": "Minor comments:\n- The paper sometimes claims that previous models are just able to represent single *edits* (abstract, Sec.",
            "13": "4).",
            "14": "This is, of course, not true: they are able to represent multiple edits with a document, just in a single *edit step*.",
            "15": "This subtle difference is quite important so the wording should be more precise.",
            "16": "- Eq.",
            "17": "2: This might be true in theory, but it is highly impractical since it seems unlikely that the sum over all document revision histories can be approximated to a reasonable degree, let alone enumerated.",
            "18": "- Eq.",
            "19": "2: The notation under the sigma is wrong.",
            "20": "It should start with \\tilde{X}\\in\\{...\\}\n- Sec.",
            "21": "2: \"x_0 represents [...] generally the null string\".",
            "22": "Is this true for both WikiRevisions and CodeRevisions tasks?",
            "23": "Doesn't this significantly increase the length of the edit representations for subsequent revisions?",
            "24": "x_0 is not empty in Fig.",
            "25": "1."
        },
        "EF-pXRnr5Lk": {
            "0": "Strengths:\n - Modeling the task of sequence editing as a multistep, iterative process.",
            "1": "Almost all previous methods model sequence editing as a single-step process.",
            "2": "The proposed method is technically sound.",
            "3": "- Edit processes are modeled on a larger scale e.g.",
            "4": "document-level edits.",
            "5": "- Modeling multi-order edits up to the order of 3, does result in improved performance (but only in comparison to the same model with an edit order of 1)  \n\n - New proposals for datasets and metrics related to iterative sequence editing: Edit Perplexity, Generation Perplexity, and Operation Perplexity.",
            "6": "(However, the model's design explicitly favors these metrics.)",
            "7": "Limitations / Concerns\n \n- Comparisons with single-step edit models: There are prior works on iterative sequence editing that utilize single-step edit models and find them useful.",
            "8": "E.g.",
            "9": "see Table 4 in [1].",
            "10": "How effective is the proposed model in comparison to these methods?",
            "11": "- The paper starts with the motivation that \"Revising and editing are a central part of the human creative workflow, with most original content being developed not in a single iteration, but in many iterations with each more refined than the last\".",
            "12": "I agree with the authors on this part.",
            "13": "However, the datasets considered in this paper may not be well aligned with this motivation.",
            "14": "E.g.",
            "15": "in many cases, Wikipedia edits and code edits may not be just stylistic edits required for improving sentence formation.",
            "16": "A good portion of edits might also introduce new information as well.",
            "17": "- Grammatical Error Correction is a very well-known task in the domain of sequence editing and aligned with the motivation of iteratively refining and editing a given sentence.",
            "18": "The experimental observations could have been more convincing if the authors could show that their method is more effective than prior single-step edit methods for such tasks.",
            "19": "- The proposed evaluation metrics in Section 5.2, are very much tied to the model design (Eqn 6).",
            "20": "Thus the proposed metrics may naturally favor the model over other models which were not trained using similar objectives.",
            "21": "Hence, I feel that the proposed model requires a more rigorous evaluation.",
            "22": "- The proposed method is only compared with a single baseline.",
            "23": "In the related work, authors do acknowledge prior work on sequence editing.",
            "24": "However, a comparison with those methods is currently not provided.",
            "25": "- Details about the posterior regularization being used in Section 6.2 are not fully clear.",
            "26": "- It would be interesting to know how important is the order of edit modeling, in Table 2 and Table 3?",
            "27": "Does performance gains diminish beyond order 3?",
            "28": "References:\n\n[1] Parallel Iterative Edit Models for Local Sequence Transduction (https://aclanthology.org/D19-1435.pdf)"
        },
        "bbwimoj7UY0": {
            "0": "Strengths:\n1.",
            "1": "The idea is simple and intuitive\n2.",
            "2": "The paper is well organized and easy to follow\n\nWeaknesses:\n1.",
            "3": "My main concern is the motivation of this work.",
            "4": "It is not clear why interactively editing is better than single-step editing.",
            "5": "The authors need to elaborate on the source of the gain in more detail.",
            "6": "2.",
            "7": "Similar idea (interactively editing) is already presented in work[1].",
            "8": "The authors do not take this method as a baseline, or at least they should thoroughly discuss the differences with this work.",
            "9": "3.",
            "10": "In section 6, the paper only compares their model to a limited baseline, making the experiment less convincing.",
            "11": "The paper needs to add some baseline, especially in the dataset CODEREVISIONS.",
            "12": "References: [1] Shaohan, Yu, Furu, Ming (2018).",
            "13": "Text Morphing."
        },
        "gmZXIdAjE2R": {
            "0": "Strong points of the paper:\n1.",
            "1": "Previous approaches only consider predicting editing based on the current text (therefore just one revision history).",
            "2": "This paper considers more revision to predict and it show more revisions (up to three) are beneficial.",
            "3": "2.",
            "4": "The paper prepares two corpus with full revision history, which might be beneficial for the community on future research.",
            "5": "3.",
            "6": "The paper propose a modified semi-autoregressive Transformer model to predict both the editing operator and generator for insertion and replacement operations.",
            "7": "The model is reasonable.",
            "8": "4.",
            "9": "The paper shows improvement on the metrics on WikiRevisions dataset.",
            "10": "Weak points of the paper:\n1.",
            "11": "One of the metrics proposed in the paper is slightly strange.",
            "12": "Why to measure the perplexity of operation?",
            "13": "Why not measuring the accuracy or F1 score of the per-token (excluding the no-edit operation).",
            "14": "2.",
            "15": "There are some details missing about the method in Section 3.2.",
            "16": "What is the network for autoregressive operation prediction $P(e_j | e_1^{j-1})$?",
            "17": "What is the decoder for insertion and replacement?",
            "18": "Is it Transformer decoder?",
            "19": "3.",
            "20": "Examples in Table 4 actually show that this EditPro model generates rather bad text.",
            "21": "Those text are fluent but semantically incoherent.",
            "22": "4. continue from the above point, Human evaluation of the editing quality is missing.",
            "23": "Do you conduct human evaluation of the generated text?",
            "24": "5.",
            "25": "It seems a rather simple baseline is missing.",
            "26": "If you directly use Transformer encoder-decoder to predict both the operation and the generation text, how is EditPro compare to this baseline?",
            "27": "It could be a good paper if the authors could fix some major concerns above.",
            "28": "Some minor issue: \nIt seems the perplexity equations are incorrect, missing negative sign.",
            "29": "There are also additional related work on editing based method for text generation.",
            "30": "[1] uses simulated annealing to search for edit operations, and [2] uses Metropolis-Hastings sampling for editing.",
            "31": "Do those apply to the edit generation problem here?",
            "32": "[1]: Unsupervised Text Generation by Learning from Search.",
            "33": "2020.",
            "34": "[2]: CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling.",
            "35": "2019"
        }
    },
    "xNO7OEIcJc6": {
        "cyFIEPxg9A_": {
            "0": "The question of what similarity CLIP should assign to text in images\nvs.  pictured objects is potentially interesting.",
            "1": "There doesn't seem\nto be a \"right\" answer, as both transcribing the red text and naming\nthe underlying pictured object are arguably justified.",
            "2": "The connection\nto Rosinski's work is interesting, and understanding how CLIP deals\nwith the ambiguity of this situation is perhaps interesting from the\nperspective of adversarial examples.",
            "3": "I had a few fundamental concerns with this work.",
            "4": "First, I didn't\nentirely understand the motivation.",
            "5": "Goh et al.",
            "6": "2021 demonstrated that\nCLIP's predictions might flip if a word was written in the image, but\nI hesitate to call this a \"misclassification\" as the authors say:\ngiven CLIP's objective (matching images/captions), it stands to reason\nthat, e.g., CLIP would assign a high score to \"white dog with the word\nelectronic written on it\", \"a white dog\", and \"electronic\" to the\nimage in the top left of Figure 1.",
            "7": "None of these, to me, seem like\noutright misclassifications.",
            "8": "The authors motivate their consideration\nby citing Rosinski's work, in which he showed children pictures of\nvarious objects with correct and incorrect labels, where the children\nwere specifically tasked with labeling the image (and not the\ntext).",
            "9": "But, CLIP has no access to such asymmetric direction, so to\ncall transcribing the text incorrect, to me, is misleading.",
            "10": "The authors' most important argument is that CLIP doesn't behave the\nsame as a human in this setting.",
            "11": "The evidence: label-switched\n\"misclassifications\" (where the model predicts the written word rather\nthan the object depicted) don't depend on the semantic distance to the\ndistractor word (as measured by word2vec vs. the true object class)\nnor the spelling (as measured by Jaro-Winkler), as they do with\nhumans.",
            "12": "The alternate hypothesis would be that, like in humans, it\nwould be more difficult to name the correct pictured object (say,\n\"dog\") if the distractor were semantically related (e.g., \"cat\") or\nclose in spelling (e.g., \"bog\").",
            "13": "But, my concern regarding the ambiguity of the setup remains: I don't\nthink it's fair to simply call the text transcription \"incorrect\".",
            "14": "And\nso, without accounting for that, I don't think I buy the arguments the\nauthors are making with respect to comparison to human\nexperiments.",
            "15": "The equivalent human test, IMO, is ill-designed: handing\nchildren images with text on them and asking them to write something\ndoesn't seem fair, and, for me, neither does this setup."
        },
        "smFhWaC2zvZ": {
            "0": "[Overall] This is a very interesting piece of work that analyzes whether the state-of-the-art artificial vision and language model; CLIP, resembles human cognition or not.",
            "1": "[Formatting]\n- Figures and Tables should be aligned to the top of the page.",
            "2": "[Writing]\n- I though that CLIP is trained by inputting image and words separately, but according to the authors statement in Section 5: \"how CLIP acquired the ability to read superimposed words\", it seems that it models language information from the image (superimposed text).",
            "3": "Is this true?",
            "4": "If so, I doubt it, and accordingly, results and discussions based on it may be results of some undiscussed reasons.",
            "5": "Please also introduce in detail the task settings so that this kind of confusion will not occur."
        },
        "yDeig1-Y_gn": {
            "0": "Overall this paper is well-written.",
            "1": "The motivation and background are clearly stated.",
            "2": "To evaluate the effect of word-picture interference, the authors applied multiple analyses, including assessing the model behaviors under different conditions (section 4.2, 4,5), connecting model behavior to label similarity (section 4.3, 4.4), and comparing the similarity of visual model representations (section 4.6).",
            "3": "The methods are described in details and should be easy for readers to reproduce their analyses.",
            "4": "The authors are trying to figure out how the visual representations of word form and object images interfere with each other and bias the classification behavior.",
            "5": "This is an intriguing question driven by observations and theories from cognitive science.",
            "6": "Comments:\n1.",
            "7": "In general, I agree with the authors' claims.",
            "8": "However, I feel there exists some interesting findings in the current results remain further explanation or investigation.",
            "9": "First, during pre-training, the CLIP model already matches the representation of \"*object in visual form*\" and \"*word in textual form*\" in the same metric space.",
            "10": "And the authors further find that the image classification (section 4.2, 4.5) is strongly biased by the superimposed word (e.g., examples in Figure A1), even in the B/S or S/B condition (e.g., recognize tulip image labeled 'electronic' to semantic class 'keyboard', or recognize tulip image labeled 'laptop' to semantic class 'electronic').",
            "11": "Does it imply that the representational space of \"*words in visual form*\" should have similar structure (in the sense of pair-wise similarity) as the representational space of \"*word in textual form*\"?",
            "12": "However, all the similarity analyses based on representations (section 4.3, 4.4, 4.6) showed non-distinguishable results across conditions, which lead to the authors' statement that \"the CLIP image encoder has different representations of word forms from visual image representations\".",
            "13": "But it seems that at least both \"word forms\" and \"object image\" representations from CLIP image encoder captures semantic similarity in the text-based word space.",
            "14": "What's authors interpretation on this?",
            "15": "2.",
            "16": "I am a bit surprised about the good performance of CLIP recognizing spelling word into the same category word.",
            "17": "Were all the results based on pre-trained CLIP without fine-tuning?",
            "18": "Was CLIP trained on some images with visual word forms to learn typographic features?",
            "19": "3.",
            "20": "Were all 'misclassified' word-embedded images classified as the superimposed label?",
            "21": "If not, it might be better to divide the 'misclassification' condition into two cases: 'biased' (predict the superimposed label) and 'random' (predict a label not original or superimposed), just to make results in section 4.2 more informative.",
            "22": "4.",
            "23": "The left box in each panel of Figure 4 is a bit ambiguous to me.",
            "24": "What is 'label-switched' for 'original images'?",
            "25": "5.",
            "26": "What would be the model behavior and similarity results if the image is interfered with a nonsense pseudoword?",
            "27": "6.",
            "28": "Typo: 'fisrt' should be 'first' on page 5.",
            "29": "7.",
            "30": "What does the \"semantic compositionality\" in the paper title refer to?",
            "31": "I feel this term is weakly connected to the main text."
        },
        "XlDHiAWhSAc": {
            "0": "Strengths:\n- The resulting dataset provides a useful tool to evaluate the bias of image recognition models that are jointly trained with language.",
            "1": "This can be used by future researchers to evaluate whether their models acquire human-like biases.",
            "2": "- The authors present different analyses to disentangle the effects of picture-word interference in CLIP.",
            "3": "Weaknesses:\n- The writing can be improved.",
            "4": "I found hard to follow some of the results sections.",
            "5": "Figure 1 could also be improved to depict an example of the task itself (Fig A1 is already better), which is unclear solely by the figure and potentially confuses the reader.",
            "6": "Similarly, Figure 2 could be improved by showing more outputs, those corresponding to the target behaviour and those corresponding to the interference behaviour.",
            "7": "- The RSA analysis shows that CNNs are less affected by superimposed words than CLIP.",
            "8": "However, as also mentioned by the authors in Section 5, CLIP was trained on a different dataset.",
            "9": "It would have been instructive to train CLIP on ImageNet and investigate whether the language-biased modelling is still different from CNNs.",
            "10": "It is, in fact, unclear to me whether CNNs might observe similar patterns when trained on large, noisy data."
        }
    },
    "lnEaqbTJIRz": {
        "S5GhA_Db7Fv": {
            "0": "I thought that this paper was very thought-provoking, and I appreciated the attempts to better understand what is going on with pre-trained language models, why they work well, and what might we be able to improve from theses insights.",
            "1": "Strengths:\n- Thorough theoretical analysis that reveals the connection between (practically-necessary) small learning rates and inability to use dependencies across text chunks\n- Useful framing and discussion of the \"in-context bias\", where models are more likely to learn dependencies within text chunks seen during pre-training.",
            "2": "- reasonable initial experimental results demonstrating some ways to help models better use cross-text-chunk dependencies (put them into a contiguous text chunk), providing some hope that these results could make models better.",
            "3": "Weaknesses:\n- I felt like the empirical validation could have been stronger.",
            "4": "The only two tasks examined are sentence similarity tasks (which seem a bit more like a sanity check), and NaturalQuestions.",
            "5": "It's particularly surprising to me that this works so well on NQ, and I wish the authors had dug a bit deeper into this, but I also recognize that page limits exist.",
            "6": "- Where else do you think the in-context bias could be useful?",
            "7": "It wasn't intuitive for me that it'd be useful for NQ.",
            "8": "- Do you have any initial experiments on the \"self-improving\" aspect of this technique?",
            "9": "This is mentioned several times, but there are no initial results or anything suggesting that it might be a promising direction to pursue."
        },
        "O_F_Z7yCpXv": {
            "0": "Strengths:\n * Clearly written paper\n * The theoretical contributions will have a high impact on training transformer-based models\n * The theoretical analysis is supplemented by experimental analysis\n\n Weaknesses:\n  * No major weaknesses\n\n  \nSome minor aspects:\n * \"correct on less than 50 questions out of 20, 000 in the evaluation set\" – Separating thousands by commas (American system) is ok.",
            "1": "Separating them by space is also ok.",
            "2": "Doing both is not.",
            "3": "* \"correct on roughly than 250 questions in the evaluation set\" – \"than\" shouldn't be there\n * Equation 4 would have been easier to read if the right-hand side was expanded to S1 and S2's `{w_2^j}` notation from the previous line."
        },
        "921tlMcxv4l": {
            "0": "Strengths: 1.",
            "1": "Some experimental results are interesting.",
            "2": "Simply adding related sentences in the pre training input context helps end performance.",
            "3": "Weaknesses: 1.",
            "4": "I think the presentation of the paper needs to be improved.",
            "5": "For the theoretical analysis, it was not clear to me what is the contribution of the current analysis compared to the Levine 2020 paper.",
            "6": "I felt similar conclusions can be drawn from the results of that paper as well.",
            "7": "It will be good to rewrite highlighting the contributions.",
            "8": "I also noticed a lot of repeated text in section 2 from Levine 2020 paper; will be good to modify.",
            "9": "2.",
            "10": "The empirical part of the paper shows improved performance of adding similar sentences to the context of LM training.",
            "11": "I felt this was quite separate from the theoretical analysis.",
            "12": "It does not follow from the theoretical results that adding similar sentences will be a good thing.",
            "13": "As a result, having these in the same paper looked incoherent to me.",
            "14": "3.",
            "15": "The experimental results are weak.",
            "16": "The gains are not super high.",
            "17": "I will be more convinced if evaluation is done on a wider range of tasks."
        },
        "sUPa1qI4yF": {
            "0": "Strength:\n1.",
            "1": "The authors analyze the in-context bias of the self-attention model, which could inspire some research works on designing training examples.",
            "2": "2.",
            "3": "The authors propose to include related texts retrieved by the kNN method in a single training sample, which is proved effective in solving sentence similarity tasks.",
            "4": "3.",
            "5": "A theoretical analysis of the in-context bias.",
            "6": "Weakness:\n1.",
            "7": "The introduction of the motivation (the concept of in-context bias) is not easy to understand at the very beginning.",
            "8": "The paper said: “the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples.”  Acutally it seems quite natural for me and I did not realize it is a problem until I saw more explanations in section 1.1.",
            "9": "2.",
            "10": "The theory is a bit complicated and not easy to follow.",
            "11": "3.",
            "12": "The experiments are limited.",
            "13": "The authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks.",
            "14": "However, there are many other tasks that involve sentence pairs.",
            "15": "For example, sentence inference tasks such as MNLI and RTE are common tasks in NLP field.",
            "16": "The authors should conduct experiments on more types of sentence pair tasks."
        },
        "VluDZlu8XBI": {
            "0": "The main results of this paper are of broad interest.",
            "1": "Questions about how the nature of the input to LMs affects learning are important and poorly understood.",
            "2": "The claim that the within- and between-example is crucial is novel and thought provoking.",
            "3": "The arguments are highly technical and require a lot of expertise, (including familiarity in particular with Levine, 2020, on which the proof builds substantially) to understand.",
            "4": "This is not necessarily a strength or a weakness---there is great value to developing techniques for making this kind of formal argument.",
            "5": "However, I find it difficult to follow even the thread of the argument at an intuitive level, which is a shame because it's interesting to me.",
            "6": "I suspect I'm not the only potential reader who will have this problem.",
            "7": "The paper could reach a much wider audience (which I think would be an improvement) if it provided more intuitive paraphrases of the main results in Section 2.3.",
            "8": "There are a few unaddressed issues with the authors' main suggestion: that we can improve LMs’ abilities to make cross-example connections by constructing examples out of interleaved texts.",
            "9": "Assuming a fixed example length, this means portions of text from the same source that otherwise would have been in the same example in the standard chunking scheme would now be separated.",
            "10": "However, it seems that, in general, dependencies within a text are more important than dependencies between random semantically related sentences from different texts.",
            "11": "Another problem is that a language model trained on this kind of interleaved text would presumably also generate less coherent texts.",
            "12": "I also have some doubts about the empirical results.",
            "13": "The results about kNN-task adaptive pretraining seem to have a confound: The method for retrieving data for this intermediate training step involves finding sentences with high similarity, but the task on which this method is shown to lead to improvements is itself a semantic similarity task.",
            "14": "The claim is that the method should be helpful for many kinds of tasks, but my intuition is that this \"coincidental\" alignment of the data collection and the evaluation task makes the result less generalizable.",
            "15": "The empirical results about zero-shot QA don't help much because the task is clearly too difficult.",
            "16": "The improvement on the models trained with the new method is from F1 of ~0.001 to ~0.01.",
            "17": "Do larger LMs succeed at this task?",
            "18": "If so, then it seems that models tested in this paper are just too small or ineffective for the task.",
            "19": "Is there an easier task where this method leads to strong performance but ordinary pretraining does not?"
        }
    },
    "EVqFdCB5PfV": {
        "SoCwBSVmL5S": {
            "0": "Strengths:\n\n1.",
            "1": "This paper proposes a machine reading comprehension model, which is computationally efficient and also can handle long-document in multiple granularities.",
            "2": "2.",
            "3": "The computational efficiency of recent QA models has been a problem in building a real-time QA system.",
            "4": "This paper tries to solve the computational efficiency of recent MRC models.",
            "5": "Their idea is somewhat similar to the idea used in phrase-indexed QA in open-domain QA.",
            "6": "Thus, the topic of this paper is well-aligned with the recent approaches in the QA community.",
            "7": "Weakness:\n\n1.",
            "8": "The authors converted some QA datasets (ShARC and HybridQA) to long-document QA forms and evaluated DocHopper on these datasets.",
            "9": "However, the authors did not show evidence that their modification on ShARC and HybridQA datasets is valid.",
            "10": "2.",
            "11": "The runtime efficiency of DocHopper is one of the main contributions that the authors claim in their paper.",
            "12": "However, this paper does not provide enough information on measuring the runtime.",
            "13": "For example, does the runtimes of the baselines and DocHopper measured in a fair environment?",
            "14": "Please provide more detailed settings used in measuring runtime efficiency.",
            "15": "3.",
            "16": "IRRR is a multi-hop QA model in the full-wiki setting, and HotpotQA-long is in the distractor setting.",
            "17": "Therefore, comparing IRRR and DocHopper is not a fair comparison.",
            "18": "Although the authors provide the performance of HGN (in distractor setting), the performance of DocHopper underperforms.",
            "19": "However, this is not a weak point since one of the research problems of this paper is alleviating the runtime problem while maintaining the MRC performance.",
            "20": "However, since it is not clear how the authors conduct experiments to compute the runtime of each model, the HotpotQA-long results become insufficient evidence."
        },
        "UT3L6gN8qZi": {
            "0": "1.",
            "1": "If it is claimed as \"hierarchical\", I would expect something like a \"general and narrow down\" strategy.",
            "2": "Unfortunately, what the model actually does is just simply flatten every paragraph (see the definition of content embedding C and Eqs.",
            "3": "4, 5, and 6).",
            "4": "The only \"hierarchical\" stuff is a simple add-on embedding that is a simple weighted sum of all sentences.",
            "5": "Also, the \"iterative\" comes from the problem definition itself while having nothing to do with the method.",
            "6": "In this sense, the proposed attention model is neither \"iterative\" nor \"hierarchical\".",
            "7": "I do think it makes novel technical contributions.",
            "8": "2.",
            "9": "The process when attending to a paragraph vector (i.e., unpack the paragraph by reusing dot-product attention, then pack it again with the extra learnable attention) seems tricky and cumbersome but the intuition behind is not clearly illustrated.",
            "10": "3.",
            "11": "The experiments on the expanded datasets are unfair to other baselines.",
            "12": "As their deeply contextual QA interaction either falls in token-level or is subject to the ability of the backbones (e.g., BERT’s 512 length limit).",
            "13": "I would recommend authors provide results on the original dataset.",
            "14": "4.",
            "15": "The performance of the proposed model lags far behind MATE on HybridQA dataset.",
            "16": "It is curious that how MATE would be performed if applied to QASPER dataset.",
            "17": "Minor comment:\n\nStrictly, the softmax term in Eq.3 (and argmax term q_t c_m) should be s^i_j q_t^T, as q_t and s^i_j are both 1*d matrices according to the notation at the bottom of page 3."
        },
        "5C-Ckh7i7GD": {
            "0": "The proposed approach is interesting and more efficient compared with cross encoding approaches at inference time.",
            "1": "Most prior results in the literature suggest that separate encoding approaches work inferior to cross encoders, thus results in this paper are encouraging.",
            "2": "Despite encouraging results, there are some shortcomings in the execution.",
            "3": "- One critical result from last year's research was that Transformers can capture multihop relationships in long documents without the need for directly modeling multi-hop behavior and hierarchical structures (see BigBird, ETC, Longformer papers).",
            "4": "This paper somewhat goes against that direction and suggests that explicitly encoding multi-hop information can improve results.",
            "5": "However, it remains difficult to fully accept this claim as some of the key comparisons are not fully controlled.",
            "6": "More details below.",
            "7": "- As mentioned in the paper, the ETC baseline is hampered by the 4096 context size limit.",
            "8": "For the baseline, the authors encode the context paragraph by paragraph to sidestep the issue.",
            "9": "However, this makes this baseline significantly weaker.",
            "10": "Why not just evaluate the proposed model on the original HotpotQA data so that results are directly comparable?",
            "11": "In addition Longformer can handle sequences of up to 16K tokens.",
            "12": "I would have liked to see a Longformer based encoder as retriever + a BERT based reader as another more fair baseline on all the datasets.",
            "13": "- While motivation for longer documents is important, most of the datasets used are not the original versions.",
            "14": "E.g., Hotpot and ShARC are modified to make them longer and for Qasper only a subset of extractive answers are used.",
            "15": "This makes direct comparisons with SOTA not possible.",
            "16": "- I would have liked to see a comparison of number of parameters of the proposed model compared with baselines.",
            "17": "For example, the LED baseline for QASPER is a single stage model where as the proposed approach uses two models, one for extracting evidence + a BERT-based reader for extracting the final answer.",
            "18": "In this regard, the comparison is not entirely fair.",
            "19": "- The exact method of constructing the paragraph embeddings by a weighted average of sentences based on query relevance is a bit under motivated.",
            "20": "It would have been nice to see an ablation on importance of this design (e.g., compared with simple or learned weighted average pooling).",
            "21": "- One of the stated benefits of the proposed approach is that it is more efficient by reducing the need for cross encoding the query and context at inference time.",
            "22": "However, I would have liked to see some discussion about the added storage cost compared with the cross encoding methods.",
            "23": "Pre-computing and storing vectors of all sentences and paragraphs in a document can be incur significant storage costs.",
            "24": "- It would have been nice to see some error analysis, case study, or some discussion looking at what types of information the proposed model is able to capture compared with the baselines.",
            "25": "Questions for authors:\n- What is finally used as text of q_null?",
            "26": "- Which ETC model size is used?",
            "27": "Which model size is used for the BERT-based reader?",
            "28": "Terminology:\n- I'm not sure \"hierarchical attention\" is standard terminology for the model architecture of ETC.",
            "29": "It might be confused by multi-layer models that are designed so that lower layers encode smaller pieces of information (e.g, words) and subsequent layers ingest these fine-grained representations and produce coarser level representations (Yang et al 2016).",
            "30": "I would either use another terminology or add an explicit definition to make it clear that by \"hierarchical\" you mean that the model provides representation of individual blocks of text within the full sequence, as ETC does not have an explicit hierarchical structure."
        },
        "7BV8cWdLJuZ": {
            "0": "### Strengths\nI think overall the proposed approache is technically sound well motivated by the challenges in complex reasoning over a long context.",
            "1": "They also test their approaches on diverse datasets including conversational QA and show the applicability to diverse QA data.",
            "2": "Their proposed method largely outperforms its base model, ETC, showing the effectiveness of newly introduced modules.",
            "3": "Also, removing the necessity of jointly encoding query-document and reusing the encoded document embedding is useful to improve the run time efficiency.",
            "4": "### Weaknesses\nI have two main concerns listed below.",
            "5": "1.",
            "6": "**Limited novelties and technical contributions**: although the method seems to be technically sound, the main component is based on ETC, and from my understanding, their technical contribution mainly lies in iterative attention to update the query representations in embedding space.",
            "7": "Updating query representations in embedding spaces instead of actually updating query text for multi-step retrieval&reasoning have been studied in prior work such as Feldman and El-Yaniv (2019) or Das et al.",
            "8": "(2019), but those papers are not cited, and I would like to see comparison or discussions on the differences between those studies.",
            "9": "2.",
            "10": "**Comparisons with the prior work**: Although they claim the proposed approach achieves state-of-the-art results on three of the benchmarks, the experimental settings are often slightly different from the original settings (e.g., only evaluate on the subset of the questions, use more passages than the default settings such as HotpotQA-Long).",
            "11": "Claiming state-of-the-art results on those variant settings may not be fair, especially when the baselines to be compared are not specifically designed for the new setting.",
            "12": "In addition, I sometimes have difficulties following the paper due to some missing details in the method section.",
            "13": "For example, I think there should be more details on how \"attention is supervised\" in the loss function paragraph in Section 3.4, instead of talking about them in the implementation details.",
            "14": "On the other hand, how to embed queries for each task is discussed in detail in Section 3.2 before talking about DocHopper's technical contribution (Section 3.4.",
            "15": "Iterative attention).",
            "16": "This makes it hard for me to understand what is the key technical contribution of DocHopper, and which parts are generally appreciable to different tasks, and which parts should be customized for each task.",
            "17": "### Missing reference \n- Feldman and El-Yaniv.",
            "18": "2020.",
            "19": "Multi-Hop Paragraph Retrieval for Open-Domain Question Answering.",
            "20": "In Proc.",
            "21": "ACL.",
            "22": "- Das et al.",
            "23": "2019.",
            "24": "Multi-step retriever-reader interaction for scalable open-domain question answering.",
            "25": "In Proc.",
            "26": "ICLR."
        }
    },
    "PlFtf_pnkZu": {
        "q-RsjLgnDtB": {
            "0": "Strengths: \nThe authors delivered these findings via extensive experiments on different language pairs, data scales, model sizes, and translation task settings (bilingual, multilingual, zero-shot, etc.",
            "1": "), so these findings have good reliability and generalization.",
            "2": "Some of the findings are inspiring to the follow-up research on this topic.",
            "3": "Weaknesses: \nSome experiments require more careful design to make findings more reliable.",
            "4": "Such as, in Figure 2(d), the Test Bleu of EndDec, CasualLM-Deep, and Prefix-TopOnly-Deep decrease as the parameters increase in the interval of #Params 0.19-0.43E8.",
            "5": "This phenomenon is not in line with the general trend, also the intuition.",
            "6": "I suspect that the experimental settings could be improved.",
            "7": "Othervise, the authors need to give a reasonable explanation.",
            "8": "There is another question I would like to promote a discussion.",
            "9": "That is how to measure the transferring capabilities.",
            "10": "The conclusions drawn from Figure 6 is that LMs do not really facilitate the transfer to low-resource languages, because LM-based models do not outperform EncDec model in all #params scales.",
            "11": "However, we cannot see the improvement each model gain through transferring over the baseline.",
            "12": "These relative improvements over the baseline (not the absolute scores) may be more appropriate to measure the transferring capabilities."
        },
        "ZuM4hIEX36": {
            "0": "**Strengths**\n\n1.",
            "1": "The empirical evaluations of this paper are comprehensive and solid, the results and findings are interesting.",
            "2": "2.",
            "3": "The paper is well-organized and clearly written.",
            "4": "**Weaknesses**\n\n1.",
            "5": "Despite good empirical efforts, the motivation of this paper seems somewhat unclear.",
            "6": "Given that the encoder-decoder paradigm dominantly governs machine translation (also from the experiment part of this paper that LMs underperform EncDec most of the time.",
            "7": "), for what reason should we need to consider a shift to a unified language model for such a seq2seq task?",
            "8": "If the zero-shot transfer is the case, why not directly fix the off-target issue for EncDec and preserve the good of translation performance, instead of changing the paradigm?",
            "9": "I feel like the authors didn't convey an incentive for this.",
            "10": "2.",
            "11": "The paper conducted extensive experiments to show how scaling affects LMs for MT, however, few suggestions based on the findings are given for future development of MT.",
            "12": "3.",
            "13": "The setting of CausalLM seems a bit weird that a unidirectional encoding behavior makes obviously no sense.",
            "14": "**Questions**\n1.",
            "15": "Section 5 seems to mainly examine p and L_{\\inf} In equation (7), whereas \\alpha remains undetermined.",
            "16": "How was the value of \\alpha determined for diagrams in Figure 3?",
            "17": "Get fitted from Figure 2 I guess?"
        },
        "Y_8uTqRWR_U": {
            "0": "This is an OK paper.",
            "1": "The writing and presentation are clear, the experiments are relatively thorough, the related works are discussed comprehensively, the results are convincing, the claims are well supported by the results.",
            "2": "However, after a careful reading of the paper, I still could hardly find enough insights from the paper.",
            "3": "***How could the findings benefit the community?",
            "4": "*** The design principle of EncDec-based NMT is the sub-optimal performance/efficiency of LM-based NMT, and this paper just says: \"yes, the intuition is true\", which is not exciting at all.",
            "5": "One insight is the zero-shot performance of multilingual NMT, however, I do not think there are many differences between 4.80 BLEU scores and 7.95 BLEU scores, where are too low to make sense.",
            "6": "It would be nice if the authors could discuss the applicability of the findings in the author response and the future version.",
            "7": "And better yet, directly utilizing the findings to enhance existing NMT systems.",
            "8": "Furthermore, the paper topic is too narrow, it would be much better to extend to other language generations tasks, like dialogue and QA (not a weakness but a suggestion)."
        },
        "x-gj0d9f6o5": {
            "0": "This is a paper on investigating the gaps of information in a relatively new area of research in machine translation.",
            "1": "The approach, experiment setup, and evaluation methods are reasonable and provide confident insight into the questions that the authors ask.",
            "2": "Here are a number of questions for the authors:\n\n- Your TopOnly setting is supposed to mimic how the encoder-decoder model uses the topmost-layer source encodings for translation.",
            "3": "This setting indeed shows improvements and achieves lower perplexity on almost all language pairs.",
            "4": "One can interpret that two fundamentally different architectures are still bound by similar features.",
            "5": "Did you investigate why the topmost-layer is the most informative way (at least in the tried approaches) of transferring information from the source to the target generation?",
            "6": "Following on that, looks like that the difference between the _Deep_ and _TopOnlyDeep_ variations is very small.",
            "7": "What is your understanding on the difference of information captured by deeper _and_ multiple layers versus deeper and topmost layer?",
            "8": "Did you compare _Deep_ and _TopOnly_ setups?",
            "9": "This question is mostly about the PrefixLM model.",
            "10": "-  You mention that the _TgtOnly_ setting is supposed to enrich the CasualLM model with additional information.",
            "11": "However it\nrequires and occupies part of modeling capacity.",
            "12": "Can you explain what you mean by occupying the modeling capacity?",
            "13": "- You conclude in one of the sections that \"When there are fewer parameters, the model with inductive biases favoring translation will achieve better quality\" and follow it with four observations two of which I want to discuss here.",
            "14": "It is not entirely clear to me why \"deeper LMs (_Deep_) rather than wider\" and \"training LMs without source-side language modeling loss (_TgtOnly_)\" are both considered as high inductive biases.",
            "15": "For the former, how is deeper models have more inductive biases than wider models?",
            "16": "For the later, wouldn't the other way around be correct?",
            "17": "- You observe that the PrefixLM model performs surprisingly well on zero-shot directions.",
            "18": "Do you have any intuitions or have you looked into finding _why_ that's the case?",
            "19": "- According to your experiments, it looks like CausalLM (unidirectional) model is not performing well, or at least as good as PrefixLM or EncoderDecoder models, in almost any settings.",
            "20": "This is intuitive because the model has access to less information at each step.",
            "21": "I'm wondering if you _can_ think of a scenario where such a model will be more effective.",
            "22": "- Since this is an examination work on multiple translation models and configurations, it would have been nice to provide a deeper analysis on the linguistic aspects of the experiments as well: Language pairs with different characteristics (morphologically rich languages for instance) work differently with different models.",
            "23": "How different variations of the same model perform on different classes of languages is a significant question to investigate."
        }
    },
    "ivQruZvXxtz": {
        "73qUbbCmCrA": {
            "0": "Pros:\n1.",
            "1": "The experimental results are strong and very promising.",
            "2": "On all settings considered in this paper, the proposed method significantly outperforms all methods.",
            "3": "If this can generalize to other MTL problems, the method has a good potential for the MTL community.",
            "4": "2.",
            "5": "The paper provides plenty of experimental evidence to demonstrate the effectiveness of the method.",
            "6": "These results also reveal its behavior in the multilingual setting that can be valuable for future research (although I do find some of them to be a bit hard to understand, please see below).",
            "7": "Cons:\n1.",
            "8": "The novelty of the method is somewhat limited.",
            "9": "The proposed modification is just a simple change to the original reptile.",
            "10": "In addition, the resulting SR algorithm is actually very similar to a trivial MTL algorithm such that its inner loop is essentially a naive MTL algorithm without within-batch mixing, while the outer loop can be seen as a form of regularization.",
            "11": "So indeed the method is a combination of naive MTL with regularization, as suggested in Eq (1).",
            "12": "This, however, seems to deviate from the original design principle of Reptile.",
            "13": "2.",
            "14": "The intuition behind the method and the source of improvement is not clear to me.",
            "15": "This is particularly important given my first point that the method is quite similar to existing methods at the first glance yet shows superiority.",
            "16": "There are several points that need better clarification: (1) why are you considering Reptile specifically in the first place?",
            "17": "what is the intuition of applying a few-shot learning method here?",
            "18": "(2) the intuition behind SR is 'to consider gradient alignment across tasks as well'.",
            "19": "While it does sound natural to do so, I find it hard to understand the overall framework with this modification.",
            "20": "The original reptile (as well as MAML) aims to find a good initialization for all tasks and therefore considers the bi-level optimization setup which contains task-specific inner loop and task-universal outer loop.",
            "21": "Here, this small modification of SR actually fundamentally changes this intuition and set task-universal goals for both inner and outer loops.",
            "22": "This does not make sense to me and I wish to get more intuition on this point.",
            "23": "(3) The source of improvement is said to be 'our method can effectively filter out language specific knowledge when solving the downstream tasks, which prevents negative transfer and helps retain linguistic knowledge acquired from the pretraining.'",
            "24": "But why is that case?",
            "25": "The results have shown that the resulting model is closer to the pretrained mBERT.",
            "26": "But there is nothing specifically designed for SR to do so.",
            "27": "The outer loop can be a potential cause yet regular reptile also has this step (while being much less similar in terms of L2 distance).",
            "28": "I think it will be helpful if more analysis can be included to show the source of improvement.",
            "29": "Minor issues/questions/suggestions:\n1.",
            "30": "It seems to me that adding regularization towards the original mBERT is helpful for the performance.",
            "31": "Can we add it for other baselines considered (or perhaps other related techniques introduced in [1,2,3])?",
            "32": "Would that improve their performance?",
            "33": "2.",
            "34": "How about using within-batch mixing for the proposed method?",
            "35": "How would that compare against the current version?",
            "36": "[1] Noise Stability Regularization for Improving BERT Fine-tuning.",
            "37": "Hua et al., 2021.",
            "38": "[2] Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models.",
            "39": "Leet et al., ICLR 2020.",
            "40": "[3] SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization.",
            "41": "Jiang et al., ACL 2020."
        },
        "0ztepnZgbTX": {
            "0": "Strength: \n- This paper tackles a problem which is less understood nor solved.",
            "1": "The experiments results demonstrate the empirical significance of the work.",
            "2": "- The method is simple and the authors provide insights into the problem and the methods, i.e.",
            "3": "the adaptation of Reptile.",
            "4": "- The experiments design are reasonable, specifically the zero-shot crosslingual transfer experiments are interesting.",
            "5": "- The overall writing is clear and the content flows well.",
            "6": "Weakness:\n- One major drawback of the paper is that it mixes two problems, i.e.",
            "7": "catastrophic forgetting (of knowledge in pretrained model) vs. negative transfer between tasks (languages) during finetuning.",
            "8": "It seems the authors focus on solving the second problem, but they also hypothesize that solving the second problem would improve the first problem as a by-product.",
            "9": "For example, the paper made several claims such as \"As a result of such negative transfer, we see from Fig.",
            "10": "5a and Fig.",
            "11": "5b that the baselines suffer from catastrophic forgetting.\".",
            "12": "However, such hypothesis (that the former caused the latter) was never verified.",
            "13": "- Another limitation is the incompleteness in experimentation.",
            "14": "For example, the authors use BERT base, which is a relative small model and draw conclusions such as \"It implies that the baselines suffer from negative transfer while ours is relatively less vulnerable.\".",
            "15": "Several work has shown that negative transfer is related to model capacity, e.g.",
            "16": "capacity bottleneck has been well studied in MTL literature and specifically in multilingual translation [1].",
            "17": "Therefore, I'd like to see experiments with larger models to verify that this method is still as effective in mitigating negative transfer as was observed in small models.",
            "18": "- Related to the above point, it's unclear whether some of the worse performance from baselines are due to specific experiment setup which puts those approaches at disadvantage.",
            "19": "For example, the authors shows that \"We see that all the baselines butours highly degrade the performance on high resource languages\" but the experiment was conducted in a manner where such observation may be expected, i.e.",
            "20": "the authors adjust the sampling distribution to pt∝(Nt)^1/5, which upsamples low resource while downssamples high resource, that is, it intrinsically hurts high resource languages' performance.",
            "21": "The technical details in several places are not clear or lack of justification:\n1.",
            "22": "In Eq.",
            "23": "1, the MTL loss is defined as the sum of single task losses.",
            "24": "This definition is quite limited since minimizing this loss may not be the objective of multi-task learning.",
            "25": "A more general MTL objective has been studied as a Pareto front of task losses [2].",
            "26": "2.",
            "27": "This work points out the importance of optimization setting which leads to different learning trajectories.",
            "28": "However, the experiments were done with a specific set of hyperparameter, e.g.",
            "29": "the authors chose 8 tasks (languages) in inner loop and sampled them with a specific distribution, the batch sizes were chosen differently for different tasks (e.g.",
            "30": "QA vs. NLI).",
            "31": "However, the author did not provide empirical justification that how senstive is the method to the choice of those hyperparameters.",
            "32": "3.",
            "33": "In Fig 5.c, Why RecAdam has the largest L2 distance given that it has explicit objective to regularize this metric?",
            "34": "4.",
            "35": "The clustering preprocessing in zero-shot crosslingual transfer experiments are very unclear.",
            "36": "Why do you need to do the clustering to form tasks instead of using language as tasks as is in other experiments?",
            "37": "5.",
            "38": "Fig 7.c does not exactly capture the training overhead from the proposed method.",
            "39": "Could you provide a wall clock based comparison (which is more precise) as you mentioned in the text?",
            "40": "6.",
            "41": "Related work misses relevant work on the optimization methods of multilingual training, e.g.",
            "42": "[3] and [4].",
            "43": "[1] Lepikhin, Dmitry, et al.",
            "44": "\"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.\"",
            "45": "International Conference on Learning Representations.",
            "46": "2020.",
            "47": "[2] Lin, Xi, et al.",
            "48": "\"Pareto multi-task learning.\"",
            "49": "Advances in neural information processing systems 32 (2019): 12060-12070.",
            "50": "[3] Wang, Xinyi, Yulia Tsvetkov, and Graham Neubig.",
            "51": "\"Balancing Training for Multilingual Neural Machine Translation.\"",
            "52": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
            "53": "2020.",
            "54": "[4] Li, Xian, and Hongyu Gong.",
            "55": "\"Robust Optimization for Multilingual Translation with Imbalanced Data.\"",
            "56": "arXiv preprint arXiv:2104.07639 (2021)."
        },
        "gJbZJ5QCR47": {
            "0": "Strength\n\n1.The paper is well-organized and clearly presented.",
            "1": "2.The proposed Sequential Reptile is very simple and extensive experiments show that Sequential Reptile can significantly outperform baseline methods.",
            "2": "Weaknesses\n\n1.Negative transfer and catastrophic forgetting are the common issues in the transfer learning field, which are not specific for multilingual learning.",
            "3": "Besides, the proposed Sequential Reptile method is also general (simply modify the task sampling strategy to a sequential way during the inner optimization), which also has not special designs for the multilingual setting.",
            "4": "I am curious why the authors choose this setting instead of more general transfer learning settings to verify the effectiveness of the proposed method.",
            "5": "2.The authors claim that finetune a model with MTL objective will gradually enforce the model to memorize task-specific knowledge.",
            "6": "Did the \nauthors compare with some transfer learning methods like multi-task adversarial training to learn task-invariant representations?",
            "7": "3.The experiments only focus on two tasks, i.e., QA and NER.",
            "8": "It would be better to verify on more multilingual tasks, e.g., XGLUE and Xtreme.",
            "9": "4.Are the improvements statistically significant?"
        },
        "r7cg8EuMOHf": {
            "0": "Strengths:\n- The hypothesis is very interesting: is the negative transfer actually caused by the misalignments of task gradients?",
            "1": "The paper provides some empirical evidences showing that such correlation (may or may not in causality relationship though) did exist, through a large number of synthetic and multilingual experiments.",
            "2": "- The proposed sequential reptile algorithm is simple and effective.",
            "3": "- The experiments are pretty extensive and show many interesting insights, which also verifies the effectivenesses of the proposed method.",
            "4": "I think the following insights can be important:\n\na).",
            "5": "Both reptile and sequential reptile show better generalizability across different tasks of the final optimized model.",
            "6": "b).",
            "7": "By implicitly adding the task gradient alignment, the resulted finetuned MTL model show less forgetting of the pretrained knowledge, and higher cosine similarities across tasks.",
            "8": "c) The task gradient alignment does have correlation with the MTL task model performance, though it's still not sure if they are in causality relationship.",
            "9": "Weaknesses:\n- The paper's findings would be much more generalizable if the experiments are also performed in the MTL setting of a single language, i.e., in a standard MTL setting.",
            "10": "- There are still a few issues in the experiments:\n\na) In the synthetic experiment, instead of only showing the loss landscape of a single initialization point, it would be more convincing if such patterns hold for random initialization.",
            "11": "b) In Table 1&2, the average performance is calculated as the mean performance across different languages.",
            "12": "Such average doesn't consider the differences of the number of train/eval examples for different languages.",
            "13": "Typically, we observed most MTL methods hurt for high-resource languages while improve on low-resource language.",
            "14": "Simply averaging across different languages lead to a discrepancy between MTL model training and results reporting, as the loss is optimized as the sum of the example losses in training time.",
            "15": "Therefore, simply averaging the performance across languages can be a misleading performance report as the actual empirical effectiveness per example may differ.",
            "16": "But I guess this issue exists in most MTL papers.",
            "17": "c) In Table 5 & 6, it would be good to include the single-task learning (STL) performance on zero-shot cross-lingual experiments.",
            "18": "d) More experiments on some hyper-parameters could lead to better understanding of the algorithms, especially on inner and outer learning rates and inner gradient steps."
        }
    },
    "yfe1VMYAXa4": {
        "JqjUOYwn1DV": {
            "0": "Good paper but more efforts are needed."
        },
        "M11kX3Tw5Ae": {
            "0": "**Strengths**\n1.",
            "1": "The research direction is important and well motivated -- a lot of valuable information is captured in gene ontologies and ought to be valuable to improve protein representations and performance on downstream task.",
            "2": "2.",
            "3": "The paper reads very nicely and easy to follow through, except for a few subsections to clarify / re-write (in particular section 2.4, see below) and would benefit for another read through to correct typos.",
            "4": "3.",
            "5": "The method is applied and tested across a diverse set of experiments.",
            "6": "**Weaknesses**\n1.",
            "7": "Methodological contributions are limited to leveraging existing multi-relational data embedding methods (Bordes et al.)",
            "8": "with no adaptation to the specificities of the GO modality.",
            "9": "The negative sampling is very close to random sampling (with constraints based on entity group type).",
            "10": "2.",
            "11": "This translates into marginal benefits across all experimental setups when comparing with the ProtBert baseline (acknowledged by authors in section 3.4).",
            "12": "Some of the claims in section 3.2 do not seem backed up by the results.",
            "13": "For example, in the PFP experiments, you cannot simultaneously claim that your method outperforms baselines in BPO and that it performs comparably with other methods in other settings (since baselines outperforms OntoProtein in the latter settings by a larger margin that Ontoprotein outperforms them in the BPO setting).",
            "14": "**Clarifying questions**\n1.",
            "15": "Section 3.1: “Specifically, we notice that the frequency of leaf GO terms involved in gene annotations and those more specific concepts relative to their parent nodes are less to non-leaf GO terms” -- could you please clarify what you meant here?",
            "16": "2.",
            "17": "Section 2.4.",
            "18": "Equation 1: you seem to have a typo in equation 1 since it includes r’ but negative triplets involve the same r (r=r’?)",
            "19": "since you only negatively sample the leading/tail entities h’ and t’.",
            "20": "3.",
            "21": "Section 2.4.",
            "22": "Equation 2: why is d not an explicit function of t?",
            "23": "Shouldn’t it be d(h+r, t) following Bordes et al?",
            "24": "4.",
            "25": "Section 2.4.",
            "26": "Equation 3 is confusing.",
            "27": "The ‘|’ operator is not standard for sets / not defined anywhere (did you mean union?).",
            "28": "Additionally, you state that the negative sampling is achieved by sampling h’ and t’ at random from the same family as h and t resp.",
            "29": "-- which I understand means for example that if h is E_{MFO} then also h’ is in E_{MFO} -- but that is not what is expressed mathematically by equation (3).",
            "30": "Could you please clarify?",
            "31": "5.",
            "32": "Section 2.4: please clarify which embeddings you use exactly here?",
            "33": "Are you using aggregate embeddings H_{GO} and H_{Protein} and none of the token level embeddings?",
            "34": "6.",
            "35": "Section 2.4: Do you do any negative sampling for T_{protein--GO} triplets?",
            "36": "Are you limiting to negatively sampling tail (GO) entities?",
            "37": "7.",
            "38": "Section 2.5: How do you form input batches to jointly train this objective?",
            "39": "8.",
            "40": "Figure 4: How is it different from what one would obtain with ProtBert?",
            "41": "**Minor points**\n1.",
            "42": "Introduction: \"different from knowledge-enhanced approaches in NLP\" → please cite\n2.",
            "43": "Introduction: “text deceptions” (towards the top of page 2) -- did you mean \"test descriptions\"?",
            "44": "3.",
            "45": "Would suggest that you give 1 or 2 concrete examples to illustrate the different objects described in 2.2 and appendix A.1.",
            "46": "Table 7 is also a bit confusing (e.g.",
            "47": "difference between GO term Vs GO statement?)",
            "48": "4.",
            "49": "The write up of section 2 could be improved -- there was a lot of overlap between section 2.1., 2.2 and 2.3 which was making some information redundant / making things more difficult to read\n5.",
            "50": "Section 2: would suggest to include an overall architecture diagram to show how an example input triplet is processed (eg., what goes to which encoder type?).",
            "51": "6.",
            "52": "Section 3: bolding of results is not consistent / misleading.",
            "53": "Always bold the best result in each column.",
            "54": "7.",
            "55": "Section 3: would be helpful to have MSA transformer everywhere to put things in perspective (bearing in mind it does have access to MSA data that you do not explicitly leverage)."
        },
        "eJ3dPzEze_m": {
            "0": "Reasons to accept: \n* Interdisciplinary study on extending the language model and KG embedding into protein embedding to empower bio appications\n* Insightful strategy and observation on KG negative sampling \n* Extensive experiments on multiple tasks and ablation study (attention analysis) are conducted.",
            "1": "New useful resources of enhanced protein datasets are created and available publicly with strong reproducibility.",
            "2": "Reasons to reject: \n* The information fusion of text token and protein sequence tokens may face modality challenges in one KG embedding model.",
            "3": "* Some missing references and potential baselines on the PPI task and other works that jointly trained on protein and gene ontology (see more in detailed comments)\n* Lack of quality assessment of curated dataset ProteinKG25.",
            "4": "Detailed comments: \n\n* One of the major concerns of this paper is that some existing works with joint training on protein with other domains.",
            "5": "Examples are [1,2,3] with PPI predictions as one task for evaluation (can be added to PPI baselines).",
            "6": "As for PPI baselines, since the sequences have been added as input, the current evaluation only considered GNN-PPI variations, however, there is a large collection of existing work relying on protein sequences (potentially with the help of PSI-BLAST and multiple sequence alignment) and follow-up works of PIPR.",
            "7": "It is necessary to show that the purely sequence-based model cannot compete with the proposed OntoProtein with an additional facet of GO knowledge.",
            "8": "* In OntoProtein, the GO entity embeddings are based on text description while proteins are encoded with their amino acid sequence tokens through a shared model in MPM.",
            "9": "The features from different modalities are jointly optimized together through KE loss with TransE.",
            "10": "Two features are computed in the same embedding space with a simple translational distance which does not seem to make much sense, especially there are two types of triples (Protein-GO and GO-GO) trained together.",
            "11": "This requires more justification on what the embeddings aim to learn.",
            "12": "* It seems that the performance of OntoProtein does not significantly outperform other baselines in most cases on the TAPE benchmark.",
            "13": "What is the benefit of OntoProtein compared with other models (especially ProtBERT)?",
            "14": "*  It is interesting to see how different aspects can be leveraged into better protein presentations.",
            "15": "Since the paper claims that the protein representation learning benefits from gene ontology, and GO can be divided into BP/MF/CO components, it would be interesting to see how individual subsets in GO can result in different improvements.",
            "16": "Also, it is said that gene ontology (three subsets) and PPI are not independent of each other (some protein functions are annotated due to their known binding activities with other proteins).",
            "17": "It is suggested to see whether such correlations can be identified in the current training framework or the OntoProtein can help solve the incompleteness.",
            "18": "* There is no quality assessment of the newly curated dataset ProteinKG25.",
            "19": "The understanding is that the authors successfully made one collection aligned with external resources of annotations to generate the protein-centric KG.",
            "20": "But it is unclear about the quality (accuracy and completeness).",
            "21": "It is also suggested to make a comparison of other benchmarks such as HetioNet[4] and DRKG[5] in terms of the detailed information or KG schema.",
            "22": "* No sensitivity study on the hyperparameter \\alpha in pre-training objective which balances MLM and KE losses.",
            "23": "Minor suggestions and corrections:\n* Figure 1 is somewhat confusing about the nodes and edges, especially the left subfigure and the number annotations.",
            "24": "It would be beneficial to layout one snapshot of created Protein\n* The acronym “MLM” (masked language model) was used first in the introduction without full name (Page 2 line 10).",
            "25": "References: \n* [1] Onto2Vec: Smaili, F. Z., Gao, X., & Hoehndorf, R. (2018).",
            "26": "Onto2vec: joint vector-based representation of biological entities and their ontology-based annotations.",
            "27": "Bioinformatics, 34(13), i52-i60.",
            "28": "* [2] OPA2Vec: Smaili, F. Z., Gao, X., & Hoehndorf, R. (2019).",
            "29": "Opa2vec: combining formal and informal content of biomedical ontologies to improve similarity-based prediction.",
            "30": "Bioinformatics, 35(12), 2133-2140.",
            "31": "* [3] Bio-JOIE: Hao, J., Ju, C. J. T., Chen, M., Sun, Y., Zaniolo, C., & Wang, W. (2020, September).",
            "32": "Bio-JOIE: Joint representation learning of biological knowledge bases.",
            "33": "In Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics (pp.",
            "34": "1-10).",
            "35": "* [4] HetioNet: Himmelstein, D. S., & Baranzini, S. E. (2015).",
            "36": "Heterogeneous network edge prediction: a data integration approach to prioritize disease-associated genes.",
            "37": "PLoS computational biology, 11(7), e1004259.",
            "38": "* [5] DRKG: Ioannidis, Vassilis N., Song, Xiang, Manchanda, Saurav, Li, Mufei, Pan, Xiaoqin, Zheng, Da, Ning, Xia, Zeng, Xiangxiang & Karypis, George.",
            "39": "(2020) Drug Repurposing Knowledge Graph for Covid-19.",
            "40": "https://github.com/gnn4dr/DRKG/"
        }
    },
    "fILj7WpI-g": {
        "BNEcks2-bA": {
            "0": "#### Strengths\n\n- Major contribution is ability to scale the output to arbitrary length using an output query array.",
            "1": "This enables supporting different types of output and scales well to different tasks.",
            "2": "- Byte level performance of Perceiver IO is impressive compared to BERT baseline.",
            "3": "Byte level embeddings can be fed to Perceiver IO without any tokenization and this setting gets comparable performance with BERT sentence piece tokenization.",
            "4": "- Shows good results with multitask learning, using both single and task specific tokens.",
            "5": "- Authors do a thorough evaluation of this architecture on a wide variety of tasks, and achieves comparable results on multiple tasks in language, vision, multimodal, RL domains.",
            "6": "#### Weaknesses\n\n- Novelty over Perceiver : Although I find this work quite impressive as it scales linearly with output embedding sizes as well as arbitrary types of outputs, the overall architecture seems incremental compared to Perceiver.",
            "7": "One important change is there is no cross attention in the intermediate layers.",
            "8": "I would love to see more detailed analysis why that is removed in this architecture and empirical evidence to support this change.",
            "9": "- How does choosing values for N,D affect the performance on various tasks.",
            "10": "If we decouple input representation, output representation and base architecture, how does model performance scale with depth, N, D values.",
            "11": "Ablations on these hyper-parameters are required for more clarity.",
            "12": "- Given Perceiver can handle multiple modalities, I would be interested to see how it performs on multimodal tasks like VQA, Image/Text Retrieval, Hateful Memes etc and comparison to multimodal models like CLIP[1], ALIGN[2], SimVL[3].",
            "13": "- [1] Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry et al.",
            "14": "\"Learning transferable visual models from natural language supervision.\"",
            "15": "arXiv preprint arXiv:2103.00020 (2021).",
            "16": "- [2] Jia, Chao, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and Tom Duerig.",
            "17": "\"Scaling up visual and vision-language representation learning with noisy text supervision.\"",
            "18": "arXiv preprint arXiv:2102.05918 (2021).",
            "19": "- [3] Wang, Zirui, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.",
            "20": "\"SimVLM: Simple visual language model pretraining with weak supervision.\"",
            "21": "arXiv preprint arXiv:2108.10904 (2021)."
        },
        "RAq_R3k8iF": {
            "0": "The main strength of Perceiver IO is its high flexibility to various input domains.",
            "1": "While keeping the main architecture, we can focus on engineering the input query preprocessing with domain-specific knowledge.",
            "2": "Perceiver IO not only shows flexibility but also shows high performance.",
            "3": "The results are comparable to or sometimes outperform previous domain-specific methods.",
            "4": "For example\n\n- Table 1 shows that Perceiver IO works well even without tokenization, while BERT + UTF-8 bytes show a significant performance drop (81.0 -> 71.5).",
            "5": "It may show that Perceiver IO can perform well even without minimal domain knowledge (tokenization).",
            "6": "- Table 2 shows that the input query engineering can improve the target task (e.g., single-task query to 8 multi-task queries).",
            "7": "- Table 5 shows that the small input preprocessing engineering can improve the performance a lot.",
            "8": "For example, Perceiver IO shows 82.1 top-1 accuracies for conv preprocessing while Perceiver IO with Fourier features shows 79.0.",
            "9": "I have minor concerns about ImageNet experiments.",
            "10": "- Although this paper is not aiming to state-of-the-art ImageNet performances, I think Perceiver IO is not yet comparable to state-of-the-art models on ImageNet image classification (as described in the last paragraph of Section 1).",
            "11": "Perceiver IO still needs tremendous FLOPs compared to vision-specific models (Table 17).",
            "12": "For example, ResNet shows 78.6 top-1 accuracy with 4.1 GFLOPs, while Perceiver IO Fourier shows 79.0 top-1 accuracy with 407 GFLOPs (99 times larger FLOPs).",
            "13": "Stronger Perceiver IO (conv) shows 82.1 top-1 accuracy with 369 GFLOPs, where CaiT-M48 448 shows 86.5 top-1 accuracy and 329.6 GFLOPs.",
            "14": "- \"Perceiver IO (pretrained)\" and \"Perceiver IO\" in 2D Fourier features are different models.",
            "15": "For example, \"Perceiver IO (pretrained)\" does not use weight sharing, and uses larger channel sizes.",
            "16": "As a result, Perceiver IO (pretrained) has 4.38 times larger parameters compared to Perceiver IO (but half GFLOPs).",
            "17": "The current text can make a reader mislead that \"Perceiver IO (pretrained)\" is the same model as \"Perceiver IO\", but the only difference is the JFT pretraining, which is not true.",
            "18": "> Unlike in the other ImageNet experiments, we do not share weights in the latent self-attention process modules, but use a 16-layer latent network with no weight sharing in depth.",
            "19": "Unlike the other ImageNet experiments, the process-module MLPs use a hidden layer with 4× the number of channels (rather than 1× as on other ImageNet experiments)\n\nTo avoid confusion, I suggest replacing Table 5 with Table 17.",
            "20": "Also, I would like to suggest to clarify that \"Perceiver IO (pretrained)\" and \"Perceiver IO\" are not the same model.",
            "21": "## Questions\n\n- Perceiver IO shows better performances consistently compared to Perceiver IO baseline on GLUE (Table 13), Audio Set (Table 7), and ImageNet (Table 17).",
            "22": "To my knowledge, there is no specific reason to the proposed output array cross-attention works better than the conventional classification head.",
            "23": "I wonder what is the opinion by the authors what is the source of the improvements.",
            "24": "- In Table 5, I found that using 2D + maxpool preprocessing harms Perceiver performances (78.6 -> 77.4), but it helps Perceiver IO a lot (79.0 -> 82.1).",
            "25": "As far as the reviewer understood, the main difference between Perceiver and Perceiver IO is the last decoding layer, not the input processing.",
            "26": "I wonder what is the opinion of the authors why Perceiver and Perceiver IO show contradictory results on ImageNet Fourier features and conv preprocessing.",
            "27": "- I wonder why there is no Perceiver IO (conv) + pretraining result.",
            "28": "Do the authors have any plan to add Perceiver IO (conv + pretrained)?",
            "29": "(this is not a mandatory experiment for the rebuttal)\n\n## Minor comments\n\n- Many details are missing in the main text.",
            "30": "To understand the method, the readers should read through the heavy appendix and the previous Perceiver paper.",
            "31": "I fully understand that this is because of the page limitation, but this paper is somewhat hard to understand at first glance without any prior knowledge.",
            "32": "- In my opinion, it would be very helpful to readers if Table 8 and 9 are in the main text.",
            "33": "- pg 3.",
            "34": "Typo -- unnecessary space *( \"inducing points\")*\n- pg 29.",
            "35": "Missing hyperlink (Tab.",
            "36": "?",
            "37": "?)",
            "38": "- In my opinion, it will be very interesting if there is one unified Perceiver model pretrained on various domain data.",
            "39": "For example, one can train a Perceiver model trained by language and vision modalities, where each modality has different input and output arrays."
        },
        "WGNpfTgIIY4": {
            "0": "Pros:\n\n1.",
            "1": "This paper introduced a generic architecture for coping with different tasks with various input and output lengths.",
            "2": "It is different from Transformer architecture and Perceiver architecture in that it uses a latent process to compress the larger number of input tokens into a hidden space, and then decode the output from this hidden space.",
            "3": "This way it can decouple the inputs and outputs which has the potential to significantly reduce the cost brought by a large number of input and output tokens.",
            "4": "2.",
            "5": "The authors presented a thorough study of Perceiver IO across different application scenarios, such as language, vision, multi-modality.",
            "6": "Extensive experiments demonstrate that the proposed method can achieve comparable or even better performance than the baselines.",
            "7": "To facilitate the adaptation, the authors suggested a number of good ways to convert the inputs/outputs into certain formats.",
            "8": "Cons:\n\n1.",
            "9": "The authors should list the sizes of inputs, latent, and output arrays for each of the downstream tasks to give the audience a better sense of the complexity.",
            "10": "For some of the tasks, such as ImageNet classification, the authors should also report the FLOPs in comparisons with prior arts, e.g., ViTs.",
            "11": "2.",
            "12": "The main merit of Perceiver IO compared with Perceiver is that it further introduces a cross-attention-based output decoder on top of the latent arrays.",
            "13": "Though Perceiver was originally proposed for classification tasks, it can be also used to cope with structured outputs.",
            "14": "For some of the tasks except for ImageNet classification listed in this paper, I am curious whether Perceiver can be applied and how it performs compared with Perceiver IO.",
            "15": "3.",
            "16": "Perceiver IO is proposed as a generic architecture for various tasks by modeling them as a read-process-write process.",
            "17": "One question is that whether we should reformulate all these tasks as the way shown in this paper.",
            "18": "This paper does show some encouraging results on various tasks compared with the baselines.",
            "19": "However, they are only compared with baseline methods and still underperforms established methods in specific domains.",
            "20": "As we know, different tasks can still benefit a lot from the specific domain knowledge (e.g., 2D spatial for image recognition).",
            "21": "Actually, we can also think of the Transformer encoder as a generic architecture in numerous domains.",
            "22": "Then, do we need to build up a higher-level generic architecture upon the Transformer encoder?",
            "23": "4.",
            "24": "A good part of unifying architectures for various tasks is that we can train the same set of parameters using different tasks.",
            "25": "In this paper, the authors demonstrated the effectiveness of Perceiver IO across different settings, I am wondering whether such a generic architecture can be used for multi-task training so that it can leverage the training data from different tasks."
        },
        "HkGOlWnfwux": {
            "0": "#### Strength\n- The idea of perceiver IO is novel and solid -- a general architecture capable of handling general-purpose inputs and outputs across different tasks and modalities.",
            "1": "This is very promising to simplify the construction of highly tuned task-specific neural pipelines and improve the multimodal and multi-task problems.",
            "2": "- Each component in perceiver IO is necessary and well defined for the proposed tasks.",
            "3": "2D byte array input enables the task agnostic input for different modalities.",
            "4": "Perceiver network that scales linearly with the size of input and output, and non-auto regressive decoding make the decoding to raw output possible.",
            "5": "- The proposed architecture is tested on massive experiments including language understanding tasks, optical flow, video audio class autoencoding, image classification, and starcraft II and achieves superior performance.",
            "6": "Each task is supported with a detailed ablation study to shed light on future research.",
            "7": "#### Weakness\n- In Table 1, the WNLI task is excluded from the GLUE benchmark, I wonder what is the reason this task is removed?",
            "8": "- The non-autoregressive decoding enables fast decoding for large outputs.",
            "9": "However, for tasks that require more information on prior decoding context (machine translation or image generation tasks), does the proposed model can still perform well on those tasks?",
            "10": "- Although perceiver IO removes the task-specific pre-processing (tokenization, patch embedding, etc.",
            "11": "), the model still requires huge engineering efforts to adapt the model for different tasks.",
            "12": "For example, the model hyper-parameters are quite different for different tasks.",
            "13": "- One huge benefit of perceiver IO is to train different tasks together and explore the transfer between different tasks/modalities, which is not explored in this paper."
        }
    }
}