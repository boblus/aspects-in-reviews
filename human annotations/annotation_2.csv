review,question,yes,no,explanation
* The authors have included ablation tests to demonstrate the robustness of their approach.,Does the review address Result?,FALSE,TRUE,"This sentence addresses ablation tests, not results."
3) The authors compared the robustness of the NVIB-regularized model with that of a regular transformer and found the former more robust to noisy input.,Does the review address Comparison?,TRUE,FALSE,This sentence discusses a comparison of robustness.
Proposes a novel approach by combining ideas from active learning and human-AI collaboration.,Does the review address Methodology?,TRUE,FALSE,"This sentence presents a new approach, related to methodology."
"Buf if that's the case, then there's a controllable parameter that implicitly controls an entropy constraint and it's no longer clear to me that low-entropy is emerging.",Does the review address Methodology?,TRUE,FALSE,"This sentence talks about a controllable parameter and entropy, related to methodology."
Provide additional feedback with the aim to improve the paper.,Does the review address Result?,FALSE,TRUE,This sentence does not mention specific results.
3) The authors compared the robustness of the NVIB-regularized model with that of a regular transformer and found the former more robust to noisy input.,Does the review address Result?,FALSE,TRUE,"This sentence focuses on comparison, not results."
"To pick just one recent example [1], the results reported (such as F1) appear to be way higher than anything in Table 2 (and that is also true for the baselines reported in [1]).",Does the review address Result?,TRUE,FALSE,"This sentence mentions results, such as F1 scores."
This combined with the baseline I proposed above could lead to a general-purpose pattern-based classifier.,Does the review address Comparison?,TRUE,FALSE,"This sentence suggests a combination with a baseline, indicating comparison."
I am also somewhat confused by the second set of experiments.,Does the review address Experiment?,TRUE,FALSE,This sentence refers to a specific set of experiments.
Comprehensive empirical analysis on multiple datasets and CIR tasks demonstrates the effectiveness over baselines.,Does the review address Analysis?,TRUE,FALSE,"This sentence discusses empirical analysis and effectiveness, related to analysis."
* The approach is straightforward and seems to improve over the pattern-verbalizer approach.,Does the review address Result?,TRUE,FALSE,"This sentence mentions improvement, which corresponds to results."
1) The specific design choices made in adapting the original NVIB to self-attention could benefit from more justification and discussion.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence calls for justification and discussion.
Among the biggest issues I consider the following:   * Weak baselines: Given the chosen dataset contains much more information than the textual content I would want to see the results of this work compared to the state of the art reported in the literature that looks at the same dataset.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the dataset and comparison to state of the art, related to Data/Task."
"In addition, the proposed approach could include more labels, e.g., from Wikipedia, which may or may not contain the labels of the downstream tasks, and create a larger dataset.",Does the review address Data/Task?,TRUE,FALSE,This sentence talks about including more labels and creating a larger dataset.
* The randomized and mismatched ablations are not very well designed.,Does the review address Ablation?,TRUE,FALSE,This sentence discusses the design of ablation tests.
3) The ablation study in table 5 seemed more like good baselines which is good to have as it shows GBT is more effective when applied to only hard examples.,Does the review address Ablation?,TRUE,FALSE,This sentence describes an ablation study.
Thus the model may require more steps to adapt.,Does the review address Methodology?,TRUE,FALSE,"This sentence suggests steps needed for model adaptation, related to methodology."
I would expect however to compare these results with the other methods on the same set of samples.,Does the review address Result?,FALSE,TRUE,"This sentence suggests comparison, not results."
"Given the closeness with this proposed work of using paraphrases (both negative and positive), some of baselines are necessary for comparison with GBT, especially counterfactual data-augmentation techniques as GBT uses (negative paraphrases in DA i.e., IBH0) Feng et.al., A Survey of Data Augmentation Approaches for NLP  Li  et.al., Data Augmentation Approaches in Natural Language Processing: A Survey 2) Some other, even more simpler baselines could be lower learning rate and training for more number of epochs.",Does the review address Comparison?,TRUE,FALSE,This sentence talks about necessary baselines for comparison.
The proposed method shows consistent performance improvement on the three benchmark datasets for referring expression comprehension.,Does the review address Presentation?,FALSE,TRUE,This sentence does not address presentation aspects.
Claim verification is an important topic with practical significance.,Does the review address Significance?,TRUE,FALSE,This sentence highlights the importance and significance of claim verification.
The approach could inspire more work on human-machine collaboration for efficient system evaluation.,Does the review address Evaluation?,TRUE,FALSE,This sentence suggests the approach could inspire more work on evaluation.
A human study on how well the verification process leveraging only the top-1 search result is required.,Does the review address Result?,FALSE,TRUE,This sentence calls for a human study but does not address results.
After reading the paper a few times I am a bit confused about how the experimental setup supports the claims & conclusions.,Does the review address Experiment?,TRUE,FALSE,This sentence expresses confusion about the experimental setup.
2) The authors presented interesting results from intrinsic evaluation of the attention distributions of such a model pretrained on Wikitext-2 and from evaluating the quality of the representation in linguistic probing and text classification.,Does the review address Evaluation?,TRUE,FALSE,This sentence talks about results from intrinsic evaluation.
Comprehensive empirical analysis on multiple datasets and CIR tasks demonstrates the effectiveness over baselines.,Does the review address Data/Task?,TRUE,FALSE,This sentence mentions empirical analysis on multiple datasets.
Please provide a justification for this.,Does the review address Result?,FALSE,TRUE,"This sentence asks for justification, not results."
"A lot of emphasis in the paper has been on generating explanations; however, only 30 examples have been evaluated which may not provide enough evidence to support the claims of the paper.",Does the review address Evaluation?,TRUE,FALSE,This sentence discusses the evaluation of examples.
1) There are potentially numerous baselines as data augmentation for hard examples has several work.,Does the review address Comparison?,TRUE,FALSE,"This sentence mentions baselines, indicating a comparison."
"It is understandable that the authors were able to only evaluate 1,000 samples to reduce the cost.",Does the review address Evaluation?,TRUE,FALSE,"This sentence discusses the evaluation of 1,000 samples."
1) The specific design choices made in adapting the original NVIB to self-attention could benefit from more justification and discussion.,Does the review address Methodology?,TRUE,FALSE,"This sentence calls for justification and discussion of design choices, related to methodology."
"As reported in the ProgramFC paper, it achieves 60.63 on HoVER 3-hop (as compared to 54.80 of FOLK) and similarly for the FEVEROUS dataset.",Does the review address Result?,TRUE,FALSE,This sentence mentions specific results achieved by ProgramFC.
I would expect however to compare these results with the other methods on the same set of samples.,Does the review address Methodology?,FALSE,TRUE,"This sentence suggests comparison, not methodology."
5) The text in line 293-295 makes the above point a little bit more unclear.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence refers to specific lines that need clarification.
* The results are neither surprising nor exciting.,Does the review address Result?,TRUE,FALSE,This sentence directly addresses the results.
"To pick just one recent example [1], the results reported (such as F1) appear to be way higher than anything in Table 2 (and that is also true for the baselines reported in [1]).",Does the review address Comparison?,TRUE,FALSE,This sentence discusses the comparison of results and baselines.
The authors use the same setting found during hyper-parameter tuning.,Does the review address Methodology?,TRUE,FALSE,"This sentence mentions the setting used, related to methodology."
"For Chain-of-Thought and Self-Ask baselines, do you provide the retrieved knowledge to verify the sub-claims as done with FOLK?",Does the review address Comparison?,TRUE,FALSE,"This sentence asks about baselines, indicating a comparison."
Discrepancies in the presented results of ProgramFC approach.,Does the review address Methodology?,TRUE,FALSE,"This sentence mentions discrepancies in results, related to methodology."
I don’t see a justification for doing this.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,This sentence asks for justification.
I think the paper does an interesting analysis and makes an interesting point about the problem being studied.,Does the review address Analysis?,TRUE,FALSE,This sentence praises the analysis and points made in the paper.
The experiments show that it leads to some performance improvements.,Does the review address Experiment?,TRUE,FALSE,This sentence talks about experimental results.
A better ablation study could be giving just positive paraphrases (IBH1) and just negative paraphrases (IBH0) 4) Some of the important technical details are unclear.,Does the review address Ablation?,TRUE,FALSE,This sentence suggests a better ablation study.
The proposed method shows consistent performance improvement on the three benchmark datasets for referring expression comprehension.,Does the review address Data/Task?,TRUE,FALSE,"This sentence mentions benchmark datasets, related to Data/Task."
Proposes a novel approach by combining ideas from active learning and human-AI collaboration.,Does the review address Novelty?,TRUE,FALSE,This sentence describes a novel approach.
Comprehensive empirical analysis on multiple datasets and CIR tasks demonstrates the effectiveness over baselines.,Does the review address Comparison?,TRUE,FALSE,"This sentence discusses empirical analysis and baselines, indicating a comparison."
"Is the approach well motivated, including being well-placed in the literature?",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,This sentence asks about the motivation and placement in literature.
Please provide a justification for this.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,This sentence requests justification.
* Conducting an ablation study is a strong aspect of the experimental work as it offers more nuanced insights into what components of the architecture contribute how much  to the overall performance.,Does the review address Presentation?,FALSE,TRUE,"This sentence praises the ablation study, but does not address presentation aspects."
"* I believe the authors should have included one more baseline, i.e., the model trained on the 20NG dataset.",Does the review address Comparison?,TRUE,FALSE,"This sentence suggests including an additional baseline, indicating comparison."
The weaknesses are minor compared to the contributions.,Does the review address Contribution?,TRUE,FALSE,This sentence discusses weaknesses and contributions.
* The approach is straightforward and seems to improve over the pattern-verbalizer approach.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes the approach, related to methodology."
2) The authors presented interesting results from intrinsic evaluation of the attention distributions of such a model pretrained on Wikitext-2 and from evaluating the quality of the representation in linguistic probing and text classification.,Does the review address Result?,TRUE,FALSE,This sentence discusses results from intrinsic evaluation.
Analyzing the relation between correct claim verification decision and the correct generated explanation can be added in the analysis section.,Does the review address Analysis?,TRUE,FALSE,This sentence suggests adding analysis related to claim verification.
Addresses the important challenge of designing labor-efficient evaluation methods for conversational systems.,Does the review address Evaluation?,TRUE,FALSE,This sentence addresses evaluation methods.
I am a bit confused by how the experimental setup supports the claims and their consequences.,Does the review address Experiment?,TRUE,FALSE,This sentence expresses confusion about the experimental setup.
"* There are no statistical significance tests (and terms such as ""outperform"" should therefore not be used)      [1] Donabauer ""Exploring Fake News Detection with Heterogeneous Social Media Context Graphs"".",Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the lack of statistical significance tests, related to presentation."
Analyzing the relation between correct claim verification decision and the correct generated explanation can be added in the analysis section.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence suggests adding analysis and explanation.
* Conducting an ablation study is a strong aspect of the experimental work as it offers more nuanced insights into what components of the architecture contribute how much  to the overall performance.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the components contributing to performance, related to methodology."
"However, the ablated versions have a more difficult task to solve.",Does the review address Data/Task?,TRUE,FALSE,"This sentence mentions the difficulty of the ablated tasks, related to Data/Task."
"Given the closeness with this proposed work of using paraphrases (both negative and positive), some of baselines are necessary for comparison with GBT, especially counterfactual data-augmentation techniques as GBT uses (negative paraphrases in DA i.e., IBH0) Feng et.al., A Survey of Data Augmentation Approaches for NLP  Li  et.al., Data Augmentation Approaches in Natural Language Processing: A Survey 2) Some other, even more simpler baselines could be lower learning rate and training for more number of epochs.",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses baselines and data augmentation, related to Data/Task."
"In no, then verifying each sub-claim (decomposed by existing methods) leveraging the retrieved knowledge should be a fair baseline in comparison to the proposed approach that uses the retrieved knowledge.",Does the review address Comparison?,TRUE,FALSE,This sentence suggests a comparison of methods.
* The results with InstructGPT (text-davinci-003) cannot be compared with the other results.,Does the review address Comparison?,TRUE,FALSE,This sentence states that results cannot be compared.
What is needed is a comparison against what the current state of the art is as reported in the literature (ideally reproduced to conduct significance tests where appropriate).,Does the review address Result?,FALSE,TRUE,"This sentence suggests comparison, not results."
"A lot of emphasis in the paper has been on generating explanations; however, only 30 examples have been evaluated which may not provide enough evidence to support the claims of the paper.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence discusses the number of evaluated examples, related to explanation."
* The work is conducted on a well-known benchmark dataset and is therefore easily contextualisable with other work in the field.,Does the review address Data/Task?,TRUE,FALSE,This sentence mentions the benchmark dataset.
"As reported in the ProgramFC paper, it achieves 60.63 on HoVER 3-hop (as compared to 54.80 of FOLK) and similarly for the FEVEROUS dataset.",Does the review address Data/Task?,TRUE,FALSE,This sentence mentions specific datasets.
Addresses the important challenge of designing labor-efficient evaluation methods for conversational systems.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses evaluation methods, related to methodology."
* There are many missing details (and no supplementary material such as code) making it impossible to replicate the work.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence mentions missing details, related to explanation."
"- Although SGEPG is model-agnostic, this paper conduct experiments using a specific model, VLTVG, to validate the proposed method.",Does the review address Methodology?,TRUE,FALSE,This sentence discusses experiments with a specific model.
3 make it unclear whether the temperature parameter is implicitly controlling entropy.,Does the review address Result?,FALSE,TRUE,"This sentence discusses a parameter, not results."
Reducing human annotation effort would have significant practical impact.,Does the review address Significance?,TRUE,FALSE,This sentence discusses the practical impact of reducing annotation effort.
"Instead, directly instructing the LLMs with a prompt as simple as “Generate questions verifying the sub-claims of the above claim” does very well in directly generating sub-questions that can be used for verifying the claims.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses instructing LLMs with prompts, related to methodology."
"For example, which datasets and how was the paraphraser trained to generate candidate sentences for selecting IBH0 and IBH1.",Does the review address Data/Task?,TRUE,FALSE,This sentence asks about datasets and training.
The additional experiments using the other VLM would be helpful to emphasize the effectiveness of the scene graph-enhanced pseudo-labeling approach.,Does the review address Experiment?,TRUE,FALSE,This sentence suggests additional experiments.
* Conducting an ablation study is a strong aspect of the experimental work as it offers more nuanced insights into what components of the architecture contribute how much  to the overall performance.,Does the review address Result?,FALSE,TRUE,"This sentence discusses ablation study, not results."
"The claim is that agents that try to solve a prediction task subject to a communication bottleneck will exchange low entropy messages, even if these messages are not explicitly encouraged to have low entropy.",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses a prediction task and communication bottleneck, related to Data/Task."
I would be interested to see how the ablated versions would perform if the authors tuned the hyper-parameters on 20NG for each of these settings.,Does the review address Result?,FALSE,TRUE,"This sentence discusses interest in performance, not results."
The problem is timely and the solution is novel.,Does the review address Novelty?,TRUE,FALSE,This sentence describes the solution as novel.
I would be interested to see how the ablated versions would perform if the authors tuned the hyper-parameters on 20NG for each of these settings.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses tuning hyper-parameters, related to methodology."
"The authors fine-tune an MLM using patterns, which can be seen as instructions, and synthetic data.",Does the review address Data/Task?,TRUE,FALSE,This sentence discusses fine-tuning an MLM with patterns and data.
I would expect however to compare these results with the other methods on the same set of samples.,Does the review address Comparison?,TRUE,FALSE,This sentence suggests comparing results with other methods.
"In effect, the model has learned to use these patterns and is therefore able to use this knowledge for classification.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the model's learning process, related to methodology."
"On the other hand, InstructGPT [1] was much better than GPT-3 and the main reason was that it was trained to follow instructions (prompts).",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses training with prompts, related to methodology."
"This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.",Does the review address Result?,TRUE,FALSE,This sentence discusses the correctness and rigor of results.
"In line 268-277, more details would be needed as to how and where the 50K examples were selected from.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence calls for more details, related to explanation."
The negative and positive paraphrase augmentation seems like a good mix of balanced data augmentation (though we don’t know how much just adding IBH1 and just adding IBH0 independently contribute to improvement).,Does the review address Data/Task?,TRUE,FALSE,This sentence discusses data augmentation.
"If yes, then how would you justify the better performance of your approach since both these approaches also result in accurate questions for verifying the subclaims.",Does the review address Comparison?,TRUE,FALSE,"This sentence asks for justification of performance, indicating a comparison."
The experiments show that it leads to some performance improvements.,Does the review address Result?,TRUE,FALSE,"This sentence discusses performance improvements, related to results."
"2) The propose technique seems like a good choice for the PI task and some of the analyses (Figure 2 for boosting interval and ratio, table 5 for why to use GBT for hard examples) highlight interesting takeaways about GBT.",Does the review address Data/Task?,TRUE,FALSE,"This sentence highlights interesting takeaways, related to Data/Task."
The discussion seems to suggest that setting higher temperature in GS creates pressure for lower-entropy messages.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence suggests a pressure mechanism for lower-entropy messages, related to explanation."
* The authors have included ablation tests to demonstrate the robustness of their approach.,Does the review address Ablation?,TRUE,FALSE,"This sentence discusses ablation tests, related to ablation."
2) The authors presented interesting results from intrinsic evaluation of the attention distributions of such a model pretrained on Wikitext-2 and from evaluating the quality of the representation in linguistic probing and text classification.,Does the review address Methodology?,TRUE,FALSE,"This sentence presents interesting results from intrinsic evaluation, related to methodology."
Zero-shot and few-shot learning with GPT-3 using prompting had still a large gap with supervised SotA models.,Does the review address Methodology?,TRUE,FALSE,"This sentence mentions zero-shot and few-shot learning gaps, related to methodology."
What is needed is a comparison against what the current state of the art is as reported in the literature (ideally reproduced to conduct significance tests where appropriate).,Does the review address Comparison?,TRUE,FALSE,"This sentence suggests a comparison to the state of the art, related to comparison."
The negative and positive paraphrase augmentation seems like a good mix of balanced data augmentation (though we don’t know how much just adding IBH1 and just adding IBH0 independently contribute to improvement).,Does the review address Result?,FALSE,TRUE,"This sentence discusses data augmentation, though it doesn't address results."
The motivation and design choice of the proposed method is simple and intuitive.,Does the review address Presentation?,FALSE,TRUE,"This sentence mentions motivation and design choice, related to presentation."
"In addition, the proposed approach could include more labels, e.g., from Wikipedia, which may or may not contain the labels of the downstream tasks, and create a larger dataset.",Does the review address Methodology?,TRUE,FALSE,"This sentence suggests including more labels, related to methodology."
The proposed approach is very similar to the basic idea of InstructGPT.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes the proposed approach, related to methodology."
Reducing human annotation effort would have significant practical impact.,Does the review address Data/Task?,FALSE,FALSE,
Among the biggest issues I consider the following:   * Weak baselines: Given the chosen dataset contains much more information than the textual content I would want to see the results of this work compared to the state of the art reported in the literature that looks at the same dataset.,Does the review address Comparison?,TRUE,FALSE,"This sentence mentions practical impact, related to Data/Task."
"On the other hand, directly generating the decomposed sub-claims has been shown to work really well in prior works.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses baselines and dataset information, related to comparison."
1) The GBT technique shows improvement (as per the result tables 3 and 4) and could be applied generally to other tasks as well.,Does the review address Result?,TRUE,FALSE,"  This sentence shows improvement in the technique, related to results."
"2) The propose technique seems like a good choice for the PI task and some of the analyses (Figure 2 for boosting interval and ratio, table 5 for why to use GBT for hard examples) highlight interesting takeaways about GBT.",Does the review address Methodology?,TRUE,FALSE,"This sentence mentions generating sub-claims, related to methodology. "
Comprehensive empirical analysis on multiple datasets and CIR tasks demonstrates the effectiveness over baselines.,Does the review address Methodology?,TRUE,FALSE,"This sentence mentions empirical analysis, related to methodology."
* I am not entirely convinced by the choice of the authors to use only 4 labels from the 20NG dataset.,Does the review address Data/Task?,TRUE,FALSE,"This sentence critiques the choice of labels, related to Data/Task."
A better ablation study could be giving just positive paraphrases (IBH1) and just negative paraphrases (IBH0) 4) Some of the important technical details are unclear.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence suggests a better ablation study, related to explanation."
* The paper is well-written and easy to understand.,Does the review address Presentation?,TRUE,FALSE,"This sentence praises the paper's clarity, related to presentation."
"Is the approach well motivated, including being well-placed in the literature?",Does the review address Related Work?,TRUE,FALSE,"This sentence asks about motivation and placement in the literature, related to related work."
"2) The propose technique seems like a good choice for the PI task and some of the analyses (Figure 2 for boosting interval and ratio, table 5 for why to use GBT for hard examples) highlight interesting takeaways about GBT.",Does the review address Analysis?,TRUE,FALSE,"This sentence highlights interesting takeaways about GBT, related to analysis."
"* I believe the authors should have included one more baseline, i.e., the model trained on the 20NG dataset.",Does the review address Data/Task?,TRUE,FALSE,"This sentence suggests including an additional baseline, related to Data/Task."
"Furthermore, the paper is well written and provides a good background for the problem statement.",Does the review address Presentation?,TRUE,FALSE,"This sentence praises the paper's writing and background, related to presentation."
The method consistently approximates full human evaluation with minimal labor.,Does the review address Methodology?,TRUE,FALSE,"This sentence mentions approximating human evaluation, related to methodology."
The entropy of the messages was only analyzed for the runs where agents have successfully learned to communicate in order to solve the task.,Does the review address Analysis?,TRUE,FALSE,"This sentence discusses entropy analysis, related to analysis."
The method consistently approximates full human evaluation with minimal labor.,Does the review address Evaluation?,TRUE,FALSE,"This sentence mentions approximating human evaluation, related to evaluation."
The entropy of the messages was only analyzed for the runs where agents have successfully learned to communicate in order to solve the task.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses entropy analysis, related to Data/Task."
* The results with InstructGPT (text-davinci-003) cannot be compared with the other results.,Does the review address Result?,TRUE,FALSE,"This sentence states that results cannot be compared, related to results."
"The paper is well motivated, though I am not an expert in the area.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence mentions the paper's motivation, related to justification."
"The authors fine-tune an MLM using patterns, which can be seen as instructions, and synthetic data.",Does the review address Methodology?,TRUE,FALSE,This sentence discusses fine-tuning an MLM with patterns and data.
After reading the paper a few times I am a bit confused about how the experimental setup supports the claims & conclusions.,Does the review address Presentation?,TRUE,FALSE,"The reviewer mentions confusion after reading the paper multiple times, which suggests issues with clarity, organization, or structure of how the experimental setup and conclusions are presented. This aligns with the Presentation aspect."
I think the paper does an interesting analysis and makes an interesting point about the problem being studied.,Does the review address Contribution?,TRUE,FALSE,This sentence praises the analysis and points made in the paper.
The approach could inspire more work on human-machine collaboration for efficient system evaluation.,Does the review address Methodology?,TRUE,FALSE,"This sentence suggests the approach could inspire more work on evaluation, related to methodology."
1) The GBT technique shows improvement (as per the result tables 3 and 4) and could be applied generally to other tasks as well.,Does the review address Methodology?,TRUE,FALSE,"This sentence shows improvement in the technique, related to methodology."
It would be difficult for readers to understand and evaluate – “we manually observed the generated examples and find the results acceptable.”  6) A very minute point – it may be interesting to compare with openLLM methods like LLaMa (after some instruction tuning for PI  task).,Does the review address Comparison?,TRUE,FALSE,This sentence mentions evaluation and comparison with openLLM methods.
"If yes, then how would you justify the better performance of your approach since both these approaches also result in accurate questions for verifying the subclaims.",Does the review address Result?,TRUE,FALSE,"This sentence asks for justification of performance, related to results."
"- Although SGEPG is model-agnostic, this paper conduct experiments using a specific model, VLTVG, to validate the proposed method.",Does the review address Experiment?,TRUE,FALSE,This sentence discusses experiments with a specific model.
"Make it clear that these points are here to help, and not necessarily part of your decision assessment.",Does the review address Presentation?,TRUE,FALSE,"This sentence suggests clarity in presentation, related to presentation."
The additional experiments using the other VLM would be helpful to emphasize the effectiveness of the scene graph-enhanced pseudo-labeling approach.,Does the review address Presentation?,FALSE,TRUE,"This sentence suggests additional experiments, related to presentation. So I will say No"
One limitation is the need for a differentiable evaluation metric to estimate sample hardness (relevance scores addressed via ChatGPT).,Does the review address Evaluation?,TRUE,FALSE,"This sentence mentions the need for an evaluation metric, related to evaluation."
There are many more benchmark datasets for text classification (including fake news detection) that could be included to provide more confidence in the findings.,Does the review address Result?,FALSE,TRUE,"This sentence suggests including more datasets, related to results. It is more abouut improving the study robustness and generalizability, so it doesn’t directly address the result aspect"
3) The authors compared the robustness of the NVIB-regularized model with that of a regular transformer and found the former more robust to noisy input.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses comparison, related to methodology."
"On the other hand, directly generating the decomposed sub-claims has been shown to work really well in prior works.",Does the review address Related Work?,TRUE,FALSE,"This sentence discusses generating sub-claims, related to related work."
3) The ablation study in table 5 seemed more like good baselines which is good to have as it shows GBT is more effective when applied to only hard examples.,Does the review address Comparison?,TRUE,FALSE,"This sentence discusses the ablation study, related to comparison."
"For example, which datasets and how was the paraphraser trained to generate candidate sentences for selecting IBH0 and IBH1.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence asks about datasets and training, related to explanation."
"* Only a single dataset is used to explore the problem (well, it is two different parts but in the end it is one fairly specific dataset).",Does the review address Data/Task?,TRUE,FALSE,"This sentence mentions the dataset, related to Data/Task."
"The proposed approach is intuitive, easy to implement, and effective.",Does the review address Methodology?,TRUE,FALSE,"This sentence describes the approach, related to methodology."
This would even strengthen the claims of GBT if improvements are significant.,Does the review address Result?,TRUE,FALSE,"This sentence discusses improvements, related to results."
"Is the approach well motivated, including being well-placed in the literature?",Does the review address Methodology?,TRUE,FALSE,This sentence asks about motivation and placement in the literature.
The entropy of the messages was only analyzed for the runs where agents have successfully learned to communicate in order to solve the task.,Does the review address Result?,FALSE,TRUE,"This sentence mentions entropy analysis, but not results."
The randomized version has to learn new embeddings from scratch and the mismatched version has to learn a new meaning for each label.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses learning new embeddings, related to methodology."
- This paper is well-written and easy to follow.,Does the review address Presentation?,TRUE,FALSE,"This sentence praises the paper's clarity, related to presentation."
1) There are potentially numerous baselines as data augmentation for hard examples has several work.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses baselines and data augmentation, related to Data/Task."
"A lot of emphasis in the paper has been on generating explanations; however, only 30 examples have been evaluated which may not provide enough evidence to support the claims of the paper.",Does the review address Result?,FALSE,TRUE,"This sentence discusses the number of evaluated examples, but not results."
There are many more benchmark datasets for text classification (including fake news detection) that could be included to provide more confidence in the findings.,Does the review address Data/Task?,TRUE,FALSE,"This sentence suggests including more datasets, related to Data/Task."
"Also, why not 5 or maybe more search results are used as the knowledge because the first result may not always contain sufficient information to verify the sub-claim.",Does the review address Result?,FALSE,TRUE,"This sentence suggests using more search results, but not results."
* The idea to construct synthetic data using only the labels is interesting.,Does the review address Data/Task?,TRUE,FALSE,"This sentence mentions synthetic data construction, related to Data/Task."
I am also somewhat confused by the second set of experiments.,Does the review address Related Work?,TRUE,FALSE,"This sentence expresses confusion about experiments, related to related work."
"- Although SGEPG is model-agnostic, this paper conduct experiments using a specific model, VLTVG, to validate the proposed method.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence discusses experiments with a specific model, related to justification."
"* There are no statistical significance tests (and terms such as ""outperform"" should therefore not be used)      [1] Donabauer ""Exploring Fake News Detection with Heterogeneous Social Media Context Graphs"".",Does the review address Result?,FALSE,TRUE,"This sentence critiques the lack of statistical significance tests, but not results."
It would be difficult for readers to understand and evaluate – “we manually observed the generated examples and find the results acceptable.”  6) A very minute point – it may be interesting to compare with openLLM methods like LLaMa (after some instruction tuning for PI  task).,Does the review address Evaluation?,TRUE,FALSE,"This sentence discusses evaluation difficulties and comparison, related to evaluation."
These could possibly be answered with ablation studies.,Does the review address Ablation?,TRUE,FALSE,"This sentence suggests ablation studies, related to ablation."
1) The authors propose adaptations of NVIB to self-attention and to use it for learning increasingly abstract representations at higher layers of a transformer encoder.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses adaptations of NVIB, related to methodology."
1) The specific design choices made in adapting the original NVIB to self-attention could benefit from more justification and discussion.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence discusses design choices, related to justification."
* Conducting an ablation study is a strong aspect of the experimental work as it offers more nuanced insights into what components of the architecture contribute how much  to the overall performance.,Does the review address Ablation?,TRUE,FALSE,"This sentence discusses ablation studies, related to ablation."
The authors could do more error analysis and discuss scenarios where the approach may not work as well.,Does the review address Methodology?,TRUE,FALSE,"This sentence suggests error analysis, related to methodology."
The authors could do more error analysis and discuss scenarios where the approach may not work as well.,Does the review address Analysis?,TRUE,FALSE,"This sentence suggests error analysis, related to analysis."
3) The ablation study in table 5 seemed more like good baselines which is good to have as it shows GBT is more effective when applied to only hard examples.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the ablation study, related to methodology."
"These type of questions might already exist in the proposed dataset, it would provide more insights to dive deeper into those.",Does the review address Data/Task?,TRUE,FALSE,This discusses the dataset and its contents
"2.Through a significant number of ablation experiments, this paper extensively researched the model's hyperparameter configurations and training strategies, offering highly instructive guidance for related work.",Does the review address Experiment?,TRUE,FALSE,This refers to ablation experiments and their role.
The key contribution of the paper appears to be the formulation of the Concept-QA model based on query information and answers from GPT + CLIP.,Does the review address Methodology?,TRUE,FALSE,This describes the model's approach and methodology.
- Ablation study shown in table 5 provides good insights for choices of LoRA params and the benefit of curriculum in staged training.,Does the review address Ablation?,TRUE,FALSE,It mentions the results of an ablation study.
The main concern to me about this paper is its limited novelty and scientific merit.,Does the review address Novelty?,TRUE,FALSE,Critiques the paper's originality.
"Having an ablation on the number of GreaseLM layers would also be quite useful to answer if performance improves with more GreaseLM layers, are there diminishing returns or do we need just a few GreaseLM layers, beyond which it is detrimental to the model's performance.",Does the review address Ablation?,TRUE,FALSE,it suggests further ablation studies on layers.
"In experiments, mainly accuracy is shown, but since the major motivation to reduce gradient variance, why not show some comparison of gradient variance of MLM and MLM-FE?",Does the review address Result?,TRUE,FALSE,This questions the absence of gradient variance results.
This looks like an order of magnitude difference in dataset empirical evaluation to me.,Does the review address Comparison?,TRUE,FALSE,it evaluates differences in dataset results.
"This is in sharp contrast to computer vision where techniques like rotation, modification of hue, saturation as well as umpteen other techniques exist.",Does the review address Comparison?,TRUE,FALSE,Contrasts methods with computer vision techniques.
* The proposed RandomMask is effective but simple.,Does the review address Methodology?,TRUE,FALSE,Evaluates the RandomMask method.
"The authors empirically evaluate their N-Bref’s accuracy on a number of problems from the open source LeetCode problem set and generate 25,000 pairs of high-level source and low-level source which are broken into training (60%), validation (20%), and testing (20%).",Does the review address Data/Task?,TRUE,FALSE,Describes the dataset and its usage.
*  The evaluation focuses on comparing with an empirical law learned on a different experimental configuration and there is a concern about how comparable are the results to the ones obtained in this study and the validity of the conclusions.,Does the review address Result?,TRUE,FALSE,Highlights concerns about result comparison.
"As the main contribution of this paper is the increased efficiency of the proposed approach, it must be clear how efficiency is measured.",Does the review address Methodology?,TRUE,FALSE,Mentions the need for clarity in measuring efficiency.
"**Areas of Enhancement & Questions to authors**  - The information about each of the ablations (ID, BM) could be explained better.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,Critiques the clarity of ablation explanations.
"The paper then proposes two practical solutions for better inference: 1) tuning dropout rate, softmax temperature, and the power mean parameter; and 2) deterministic inference with tuned softmax temperature.",Does the review address Methodology?,TRUE,FALSE,Describes specific proposed techniques.
"Specifically, for Table 1, the inference time of each algorithm should be reported (retrieval time included).",Does the review address Significance?,FALSE,TRUE,"This sentence mentions reporting inference time, but not the significance of the work."
"- The paper is well written: it gives an appropriate context, presents the main theoretical results, and verifies _some_ of the claims experimentally.",Does the review address Result?,TRUE,FALSE,Mentions experimental verification of claims.
"Without comparison with SOTA's performance, I will try my best to reject this paper.",Does the review address Result?,TRUE,FALSE,Critiques the lack of SOTA comparisons.
"- Because the policy learning procedure utilizes the additionally generated dialogue acts and corresponding responses, it is easy to think that naively fine-tuning the GPT-2 model on the additional generated data may also improve the dialogue model performance in terms of its policy and responses (similar to a data augmentation method).",Does the review address Data/Task?,TRUE,FALSE,"The sentence discusses data augmentation and how it influences model performance, indicating its relevance to the task."
"For example, Figure 4 validates Assumption 4.1 (log-partition function is roughly quadratic in theta), and Table 1 shows many real tasks are approximately “natural”.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The sentence mentions validation through examples like Figure 4 and Table 1, providing justification for assumptions."
It wasn't intuitive for me that it'd be useful for NQ.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The sentence expresses a lack of intuition regarding the utility of a concept, highlighting the need for better clarity."
**Weaknesses** * Unclear if there are real practical applications to the insights from this paper.,Does the review address Methodology?,FALSE,TRUE,"This sentence mentions practical applications, but not the methodology."
"It's also not explained how Theorem 2 justifies the main conclusion: that ""a prompt engineer aided by enough time and memory can force an LLM to output an arbitrary sequence of ℓ tokens.""",Does the review address Theory?,TRUE,FALSE,The sentence questions the theoretical justification behind a key conclusion in the paper.
"), yet I could not find any actual training experiments, that is training a large LLM from scratch, in the paper.",Does the review address Methodology?,TRUE,FALSE,"The statement highlights the absence of training experiments, which are critical to the methodology."
2.The authors may add subjective evaluations to the ablation experiments to better demonstrate that the LoRA fine-tuning strategy mitigates catastrophic forgetting issues.,Does the review address Methodology?,TRUE,FALSE,The sentence suggests an improvement to methodology by including subjective evaluations.
Why are all publications not used in training the baselines?,Does the review address Data/Task?,TRUE,FALSE,The sentence raises concerns about the completeness of data used for training the baselines.
"Why are the backbone models (RoBERTa and BERT, respectively) different in Table 1 and Table 2?",Does the review address Methodology?,TRUE,FALSE,The statement questions methodological consistency between models in the paper.
The paper also presents a manual evaluation of the inferred time series from a news corpus which is nice to see.,Does the review address Evaluation?,TRUE,FALSE,"The sentence discusses manual evaluation, focusing on the assessment of results."
The authors curated a large-scale dataset for first-stage pretraining and second-stage instruction tuning.,Does the review address Presentation?,FALSE,TRUE,"This sentence discusses dataset curation, not the presentation of the paper."
Hyper-parameter balancing strategies in Section 3.1 is not a solid theoretical analysis.,Does the review address Methodology?,TRUE,FALSE,The sentence critiques the theoretical basis of hyper-parameter strategies in the methodology.
"The paper proposes a new joint learning algorithm that works for two tasks, NER and RE.",Does the review address Methodology?,FALSE,TRUE,The statement doesn’t mention methodology
"Since one of the main contributions of this paper is the analysis of the BERT pretraining process, more experimental analysis on the optimizer should also be included.",Does the review address Methodology?,TRUE,FALSE,The sentence suggests improvements to the experimental analysis in the methodology.
"These objectives certainly improve over the original T5 base _and_ larges models that are used as initializations, and especially outperform the base model in the low-data regime.",Does the review address Result?,FALSE,TRUE,"The sentence discusses results, emphasizing the improvements made over the original models."
Pros:  - A new framework for understanding why learning how to predict the next word helps the downstream task.,Does the review address Data/Task?,TRUE,FALSE,The sentence highlights a contribution related to understanding tasks in the paper.
2017 (https://arxiv.org/abs/1606.05804) perform relation extraction on unseen entities.,Does the review address Related Work?,TRUE,FALSE,The sentence references related work about relation extraction tasks.
- It would have been interesting to see a comparison of POS tagging performance with UD-style tags versus ORCHID tags.,Does the review address Comparison?,TRUE,FALSE,The sentence suggests including a comparison between different tagging systems.
"However, it is unclear how well they perform to the CASP state-of-the art (see also Rives et al, 2020).",Does the review address Result?,TRUE,FALSE,The sentence questions the paper's results relative to state-of-the-art benchmarks.
"Last sentence of intro: ""without scarifying accuracy"" seems like an inaccurate description of the results presented in this paper.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,The statement critiques the accuracy of the description provided in the introduction.
"In general, it may help to reorder some of these results, add forward references to the proofs, and indicate how different results depend on one another.",Does the review address Result?,TRUE,FALSE,This sentence addresses results by suggesting reordering and providing clarity on dependencies between them.
I am not sure on this---my intuition is based on your Lemma D.2 and the fact that for a $p_{\cdot\mid s}$ with full support a non-precise reverse version of [Pinsker's inequality](https://en.wikipedia.org/wiki/Pinsker%27s_inequality#Inverse_problem) holds.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence mentions intuition and references a lemma and inequality, indicating a justification or reasoning process."
"The first use of the proposed definitions to make a non-trivial claim is in Section 5 with Postulate 1, but this claim is not justified.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence critiques the justification of a claim related to proposed definitions, indicating a need for better explanation."
"Also, some additional experiments need to be added in order to better justify its claims.",Does the review address Experiment?,TRUE,FALSE,This sentence explicitly mentions the need for more experiments to strengthen claims.
I therefore believe that these results show merit.,Does the review address Result?,TRUE,FALSE,This sentence explicitly evaluates the merit of the results presented.
"- Table 5 the VAE(32) method performs the best overall in ""Wiki section"" although the TC (16) method has been highlighted as the best.",Does the review address Result?,TRUE,FALSE,"This sentence mentions the performance of specific methods, which directly relates to results."
"The main contribution are the following innovations: a special attention mechanism called block-diagonal conditional attention, a set of modules for adaptation of a pretrained model, and an uncertainty-based multi-task data sampling method.",Does the review address Novelty?,TRUE,FALSE,"This sentence lists innovations introduced in the study, which relate to the originality of the contributions."
It would be really useful to have an ablation study to disentangle which piece of the training or method is contributing to what and how in the performance measure.,Does the review address Result?,FALSE,TRUE,"This sentence mentions an ablation study, but does not explicitly discuss results.."
What would happen to the network if compression is removed?,Does the review address Experiment?,TRUE,FALSE,This is a hypothetical question that directly concerns potential changes in an experimental setup.
The phrasal RNN (pRNN) architecture is achieved by generating subnetworks of phrases.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes the method used, focusing on the architecture of the model."
In particular we don't know if the sacrifice of short sequence time would benefit a lot in long sequences for existing methods.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses trade-offs in methods, specifically relating to sequence handling."
"Additionally, showing this for negations and / or examples which GreaseLM gets correct but QA-GNN does not (and vice-versa) can shed some light on what the model improves on (and what are the limitations).",Does the review address Comparison?,TRUE,FALSE,"This sentence discusses comparisons between models (GreaseLM and QA-GNN), highlighting their respective strengths and weaknesses."
"For experiments, Table 1 shows the FOCAL Reasoner (DeBERTa) brings much performance gain, compared to FOCAL Reasoner (RoBERTa) (the performance seems comparable with DAGN and LReasoner using RoBERTa).",Does the review address Experiment?,TRUE,FALSE,This sentence refers to experimental results and comparisons between models.
"The authors provide several reasonable insights that might be relevant to the user interface modeling community — using object detection as part of multi-task learning instead of standalone pre-training task, design choices to create a single unified architecture for all the tasks, multi-task learning outperforming single-task learning etc.",Does the review address Data/Task?,TRUE,FALSE,"This sentence mentions insights related to tasks and design choices, focusing on multi-task learning."
"What is the impact of a good OCR for training and testing (prediction of new, unseen documents)?",Does the review address Presentation?,FALSE,TRUE,"This sentence asks about the impact of OCR, but does not explicitly discuss presentation.."
"Strengths: * Novel method of using emergent language for pre-training (as opposed to transferring an entire artificial agent) * Some good ablations to identify what contributes to successful transfer * A new evaluation metric (emergent --> NL translation performance) that best correlates with fine-tuning performance  Weaknesses: * Some parameter choices and the design of some ablations are not completely justified * Some additional related works could be included   # Minor comments / questions  * ""However, this metric is too rigid in its definition of compositionality, ignoring aspects like argument structure, context or morphology which play a key role in determining the combination of word semantics (Goldberg, 2015).""",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses strengths and weaknesses of methods, including metrics and parameter choices."
"- When building the relation between the word and the frame, is there any emphasis on verb, some particular word, or self-learned attention?",Does the review address Methodology?,TRUE,FALSE,This question focuses on the methodological emphasis in the relationship-building process.
"It's particularly surprising to me that this works so well on NQ, and I wish the authors had dug a bit deeper into this, but I also recognize that page limits exist.",Does the review address Analysis?,TRUE,FALSE,This sentence expresses surprise at the results and suggests further analysis.
Comparison with GECA: I can read from the paper that the performance is on par with GECA.,Does the review address Result?,TRUE,FALSE,"This sentence mentions performance comparisons, directly relating to results."
"They train low resource NMTs using 4 different tokenization strategies, to show that their proposed tokenization method leads to the best NMT results by several metrics.",Does the review address Methodology?,TRUE,FALSE,This sentence describes a methodological experiment involving tokenization strategies.
It would be really useful to have an ablation study to disentangle which piece of the training or method is contributing to what and how in the performance measure.,Does the review address Presentation?,FALSE,TRUE,"This sentence mentions ablation study, not the presentation.."
"PE seems to be important for wMAN, and the authors provides few sentences analysis about this, but I don't think I fully understand this part.",Does the review address Significance?,FALSE,TRUE,"This sentence discusses PE and understanding, but not significance.."
The graph indicates that for MNLI and QNLI 60% seems like a better choice.,Does the review address Methodology?,TRUE,FALSE,This sentence suggests a methodological choice based on graph analysis.
The analysis shows that it outperforms only all the other baselines only on 5/9 games and matches on 3.,Does the review address Analysis?,TRUE,FALSE,"This sentence summarizes performance analysis, focusing on baseline comparisons."
The margin of change seems even larger than some results which are discussed in the paper as significant.,Does the review address Result?,TRUE,FALSE,This sentence comments on the significance of result margins.
"With the proposed CaptionNet dataset, it is now possible to have a fairer comparison between VL models and CE models.",Does the review address Data/Task?,TRUE,FALSE,This sentence mentions a dataset that improves task evaluation fairness.
The authors have done further experiments and show that there are still gains on these tasks when model sized is increased significantly.,Does the review address Result?,TRUE,FALSE,This sentence refers to experimental results showing gains with increased model size.
"Informally , a task is defined as natural if, just by using the next word distributions as features, the downstream task can be solved with a small loss.",Does the review address Data/Task?,TRUE,FALSE,"This sentence defines a task, connecting it to specific data features."
It appears that a single example was analyzed qualitatively.,Does the review address Analysis?,TRUE,FALSE,This sentence mentions qualitative analysis of an example.
"The experiments cover loss functions, label noise, caption length, caption quality, VL vs CE etc.",Does the review address Experiment?,TRUE,FALSE,This sentence describes the scope of experiments conducted.
"However, the presented theory suffers from various core issues.",Does the review address Theory?,TRUE,FALSE,"The sentence critiques the theory presented in the work, directly addressing theoretical issues."
"Decompilation can mean many things, but the general idea as I understand it, is to take a representation of a software program from one level (e.g., program binary) and then “lift it” to a level that is higher in abstraction (e.g., from binary to assembly, from assembly to C, from C to a lambda calculus, etc.).",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The sentence provides a detailed explanation of what decompilation means, which involves description and interpretation."
I believe such an experiment will definitely make the submission stronger.,Does the review address Experiment?,TRUE,FALSE,"The sentence suggests an experiment to strengthen the submission, which relates to experimental design."
"**Baselines are too weak, leading to a misunderstanding of the effectiveness of the proposed method.",Does the review address Comparison?,TRUE,FALSE,"The sentence critiques the strength of baselines in comparison, highlighting issues with evaluation."
"One might hypothesize that if using a (subotimal) template that is less natural for language modeling, that zero-shot performance would suffer, but that FLAN performance wouldn't - One might hypothesize that the ""turn the task around"" templates help more than the other more straightforward templates that don't swap information between the prompt and response.",Does the review address Result?,TRUE,FALSE,"The sentence speculates on results derived from different templates, which concerns result analysis."
It is hard to know here which one of these actually made the adversarial setup useful.,Does the review address Methodology?,TRUE,FALSE,"The sentence reflects on the adversarial setup's usefulness, touching on the methodology."
"* Fundamentally, beyond simple invariants (array bounds, nullness checks) it is not clear why program invariants would generalize well across different programs.",Does the review address Methodology?,TRUE,FALSE,"The sentence raises questions about the generalizability of program invariants, which is part of the methodology."
"In fact, empirical evidence suggest that LMs do memorize n-grams from their training data somewhat, but not full examples (see [McCoy et al.",Does the review address Methodology?,TRUE,FALSE,"The sentence references empirical evidence and its limitations, which are methodological concerns."
"In the presence of definitive results, this might be acceptable, but in its absence, as in this paper, there needs to be quantitative analysis across multiple runs to demonstrate the robustness of the phenomenon.",Does the review address Analysis?,TRUE,FALSE,"The sentence critiques the lack of quantitative analysis to demonstrate robustness, which relates to analysis."
"Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015 _ Also, the inclusion of the result from those approaches in tables 3 and 4 could be interesting.",Does the review address Result?,TRUE,FALSE,"The sentence suggests including additional results from existing approaches, which concerns the presentation of results."
This method should be included in the experiments in order to justify the proposed RL approach is necessary.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The sentence highlights the need for justification of the proposed RL approach, which relates to intuition and validation."
Another major concern is the use of two separate RNNs which gives the proposed model more parameters than the baselines.,Does the review address Comparison?,TRUE,FALSE,"The sentence raises concerns about model parameters compared to baselines, which involves comparison."
**Summary** This work relates a pre-training performance with a downstream performance for tasks that _can_ be reformulated as next word prediction tasks.,Does the review address Presentation?,FALSE,TRUE,This sentence summarizes the work but does not mention presentation.
"Therefore, the obtained task performance is far from state-of-the-art.",Does the review address Result?,TRUE,FALSE,"The sentence critiques the results as being below state-of-the-art, which addresses result analysis."
"Because if too many parameters are introduced, the performance improvement may come from overfitting of too many parameters.",Does the review address Result?,TRUE,FALSE,"The sentence raises concerns about overfitting due to parameter increases, which relates to result evaluation."
Complexity-based prompting for multi-step reasoning.,Does the review address Related Work?,TRUE,FALSE,"The sentence references a prompting method as related work, connecting it to prior research."
The training procedure mentioned in section 5.2.2 talks about joint training but the procedure followed for training for individual tasks or a subset of tasks is not described in detail.,Does the review address Methodology?,TRUE,FALSE,"The sentence critiques the lack of detail in the training procedure description, which involves methodology."
Pros:  - the paper is well written and very clear - the proposed model has two main advantages: (1) it is very fast to train due to the use of pre-trained BERT representations and (2) it does not depends on any external NLP tool (such as dependency parser)  Cons:   - I think the main source of improvement comes from the BERT representations used as input.,Does the review address Presentation?,TRUE,FALSE,"The sentence evaluates the paper's writing quality and the model’s advantages, focusing on presentation."
Move to E2E system can be motivated a bit more (allows end-user feedback to be passed through all modules easily and don't have to worry about how a change in one module affects all other modules explicitly etc) 3.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The sentence suggests improved motivation for the proposed system, which relates to intuition and validation."
"In order to train reading comprehension models to perform relation extraction, they create a large dataset of 30m “querified” (converted to natural language) relations by asking mechanical turk annotators to write natural language queries for relations from a schema.",Does the review address Methodology?,TRUE,FALSE,"The sentence describes a dataset creation process, which relates to the methodology."
I see this work more as an analysis on language-specific parameters for a particular LS-model rather than a novel architecture.,Does the review address Analysis?,TRUE,FALSE,"The sentence interprets the work as an analysis of language-specific parameters, which relates to analysis."
This paper is well-written and the idea is interesting.,Does the review address Presentation?,TRUE,FALSE,"The sentence evaluates the paper’s clarity and appeal, which focuses on presentation."
The annotations in baseline [1] are much shorter compared to the annotations by the authors.,Does the review address Comparison?,TRUE,FALSE,"The sentence critiques baseline annotations compared to those by the authors, which involves comparison."
"- Additionally, in my opinion, the authors are misrepresenting prior work when saying in line 163 that the ""MTL approach has not yet been successful in NLP"".",Does the review address Presentation?,TRUE,FALSE,"The sentence critiques the authors’ representation of prior work, addressing presentation issues."
"The model is based on a pre-trained BERT model, which provides the word vectors of the input word sequence.",Does the review address Methodology?,TRUE,FALSE,"The sentence describes the use of BERT word vectors in the model, focusing on methodology."
"In particular, you are comparing a small GRU LM to a larger transformer LM, where the latter is, as you mention, a much more powerful model.",Does the review address Methodology?,TRUE,FALSE,"The sentence compares a GRU LM to a transformer LM, discussing methodological differences."
"Please report the total training and total inference time, and make a comparison with standard Transformer model.",Does the review address Significance?,TRUE,FALSE,"The sentence asks for performance metrics and significance comparison, addressing significance."
"For example, what is the experiment environment and training receipts.",Does the review address Experiment?,TRUE,FALSE,"The sentence raises questions about the experimental setup and training environment, focusing on experiments."
"The methods have been tested on two benchmarks focusing on the issue, SCAN and morphological analysis.",Does the review address Presentation?,TRUE,FALSE,"The sentence mentions the benchmarks used to test the methods, which relates to presentation."
"The motivation is very clear, MCTS is generally action agnostic and using language to provide additional semantic information to it can prove to be very effective.",Does the review address Methodology?,TRUE,FALSE,"The sentence highlights the use of language to improve semantic information, which focuses on methodology."
"(2) In this paper, similarity and alignment structures are proposed, but no ablation experiments have been carried out to prove the effectiveness of the improved model.",Does the review address Methodology?,TRUE,FALSE,This sentence discusses methods proposed and the lack of ablation experiments.
"In particular, using a carefully selected subset of ""prompt"" words, the authors observe that learning a linear predictor over the next word distributions of these words achieves performance close to a pre-trained GPT-2 model.",Does the review address Result?,TRUE,FALSE,This sentence explains observed performance improvements.
"Without the ablation studies on these two aspects, we cannot determine whether the performance improvement truly comes from the author's contribution or just longer CoT annotations.",Does the review address Result?,TRUE,FALSE,This sentence questions the validity of performance improvement claims.
"It is naturally expected that vision-language models can benefit from stronger unimodal encoders, for which the best strategies are not very well explored by the community.",Does the review address Methodology?,TRUE,FALSE,This sentence mentions the methodology involving encoders.
I would appreciate the explanation and further evidence to address these concerns.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence calls for further clarification and evidence.
"- P24, Figure 2: What are the x and y axis, and what does each dot mean in this figure?",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence requests clarification regarding a figure in the paper.
The Ablation Study of the paper also provides useful insights about the impact of different pre-training schemas on large-scale information retrieval tasks.,Does the review address Presentation?,TRUE,FALSE,This sentence discusses findings from the ablation study.
"Intuitively, this parameter arises from translating the ""natural"" task assumption, which only guarantees transfer on average to the downstream task.",Does the review address Data/Task?,TRUE,FALSE,This sentence relates to assumptions about the task data.
"- The proposed architecture is tested on massive experiments including language understanding tasks, optical flow, video audio class autoencoding, image classification, and starcraft II and achieves superior performance.",Does the review address Methodology?,TRUE,FALSE,This sentence highlights experiments conducted to test the architecture.
"- `s4.3 Findings p4`: ""the channel ... influences generalisation"" -- Without further explication, this potentially interesting point does not gain any traction.",Does the review address Result?,TRUE,FALSE,This sentence notes unexplored aspects of generalization.
"The approach, extend the model introduced by Vilnis and McCallum (2014) which represented word as unimodal Gaussian distribution.",Does the review address Methodology?,TRUE,FALSE,This sentence mentions the methodology's theoretical basis.
Can PACT be applied to simpler theorem provers like MetaMath where there are no tactics?,Does the review address Theory?,TRUE,FALSE,This sentence poses a question regarding the theory's application.
Pros:  - the paper is well written and very clear - the proposed model has two main advantages: (1) it is very fast to train due to the use of pre-trained BERT representations and (2) it does not depends on any external NLP tool (such as dependency parser)  Cons:   - I think the main source of improvement comes from the BERT representations used as input.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses advantages and drawbacks of the methodology.
"* It will be cool to show some results on BLEU improvement on machine translation, or user studies on language generation tasks, to demonstrate its practical impact, as the quantitative metrics right now are still kind of artificial.",Does the review address Significance?,TRUE,FALSE,This sentence suggests additional experiments for real-world impact.
* There has been so much work on static inference of invariants that it is impossible to list even all the closely related work.,Does the review address Related Work?,TRUE,FALSE,This sentence mentions related work and its breadth.
The cold-start problem is actually an urgent problem to several online review analysis applications.,Does the review address Presentation?,FALSE,TRUE,"This sentence discusses the cold-start problem, but not the presentation of the paper."
A comparison and discussion of this would be really useful.,Does the review address Comparison?,TRUE,FALSE,This sentence suggests providing comparisons.
"Further, the performance improvements are nice, though not impressive.",Does the review address Result?,TRUE,FALSE,This sentence comments on the observed performance improvements.
It's unclear what the contribution of the paper is.,Does the review address Contribution?,TRUE,FALSE,This sentence questions the paper's unique contribution.
"It also further demonstrates that the BERT model, once fully tuned, could achieve SOTA/competitive performance compared to the recent new models (e.g., XLNet).",Does the review address Methodology?,TRUE,FALSE,This sentence highlights the methodology's achievements.
"For example, the full model of wMAN works better than FBW on R@1, but worse on R@5 and R@10.",Does the review address Experiment?,TRUE,FALSE,This sentence discusses experimental results.
_ A question to the authors: What do you attribute the loss of performance of w2gm against w2g in the analysis of SWCS?,Does the review address Analysis?,TRUE,FALSE,This sentence raises a question about performance analysis.
"Since one of the main contributions of this paper is the analysis of the BERT pretraining process, more experimental analysis on the optimizer should also be included.",Does the review address Contribution?,TRUE,FALSE,This sentence suggests additional analysis of the methodology.
I encourage the authors to try out ideas they mentioned as future work to have a substantial contribution for a conference publication.,Does the review address Contribution?,TRUE,FALSE,This sentence encourages extending the contribution.
Energy and policy considerations for deep learning in NLP.,Does the review address Related Work?,TRUE,FALSE,This sentence mentions related considerations in NLP research.
"It is not fully clarified what is the difference between the so-called ""ITM"" (image-text matching) and the contrastive losses used in other  VL pretrained models, such as ALIGN, ALBEF.",Does the review address Methodology?,TRUE,FALSE,This sentence calls for clarification on the methodology.
It especially corroborates that certain tasks are innately more challenging for active learning than others.,Does the review address Data/Task?,TRUE,FALSE,This sentence discusses task difficulty in active learning.
"**Experiment setup** In Table 3, the paper only compares the proposed method against GPT-Neo and GPT-J, which is not sufficient.",Does the review address Comparison?,TRUE,FALSE,This sentence critiques the scope of comparisons made.
"Weakness: - paper title is misleading, not directly related to LM - no citation and description for the baseline method T5+KB (Table 4) - As shown in Table 7, the proposed method is very sensitive to many factors.",Does the review address Related Work?,TRUE,FALSE,This sentence points out weaknesses in related work and citations.
"2.Through a significant number of ablation experiments, this paper extensively researched the model's hyperparameter configurations and training strategies, offering highly instructive guidance for related work.",Does the review address Ablation?,TRUE,FALSE,This sentence commends the ablation studies and insights provided.
I think the setting of checkpoint depends on the hardware specifications and model architectures.,Does the review address Methodology?,TRUE,FALSE,This sentence focuses on the methodology by discussing the impact of hardware and model architectures on checkpoint settings.
"You forgot to bold the best performer in line 3 of Table 2 (in this case, the original compact model).",Does the review address Presentation?,TRUE,FALSE,"This sentence highlights an issue in presentation, suggesting that bolding would improve clarity."
"The authors experiment both with pre-training and fine-tuning of contextual models (BERT-{base,large}) and claim large reduction in training time, with reasonable loss in performance.",Does the review address Result?,TRUE,FALSE,This sentence conveys experimental results regarding training time reduction and performance trade-offs.
"The paper meticulously provides all experimental details, and the ablation study helps to validate the design components, enhancing the overall robustness of the research.",Does the review address Result?,TRUE,FALSE,This sentence emphasizes the experimental results and robustness validation.
"(3) multiSkip : Extends the approach presented by Luong et al. (2015b) for embedding using source and target context (via alignment), to the multilingual case by extending the objective function to include components for all available parallel corpora.",Does the review address Methodology?,TRUE,FALSE,This sentence focuses on the methodology by describing the technical extension and its objective function.
"Do we need better model design, or more data and computations?",Does the review address Data/Task?,TRUE,FALSE,This sentence directly addresses the data/task aspect by raising questions about model improvement strategies.
"Alongside with qualitative analysis, some quantitative analysis would be good to show what the model learns.",Does the review address Methodology?,TRUE,FALSE,This sentence discusses the need for methodology improvements by incorporating quantitative analysis.
"Elaboration on Theorem 1, with an intuitive breakdown of its implications, would significantly enhance the readability and credibility of the results.",Does the review address Presentation?,TRUE,FALSE,This sentence calls for clearer presentation to improve result interpretation.
How effective is the method to capture farther long-term dependencies compared to previous methods?,Does the review address Presentation?,FALSE,TRUE,"This sentence questions the effectiveness of the method, but does not discuss presentation."
Their method MC-LAVE used word embeddings on the language action space to help induce a non-uniform distribution over the action space.,Does the review address Methodology?,TRUE,FALSE,This sentence delves into the methodology by describing the MC-LAVE approach and its objectives.
"The proposed method has achieved SOTA performance on a range of Vision-Language benchmarks, spanning image captioning, Visual Question Answering, and visual grounding tasks.",Does the review address Data/Task?,TRUE,FALSE,This sentence discusses the tasks and benchmarks where the method demonstrated state-of-the-art performance.
"This paper proposes a new understanding of dropout on top of variational dropout, which shows that training with dropout equals to maximizing an empirical variational lower bound on the log-likelihood.",Does the review address Methodology?,TRUE,FALSE,This sentence elaborates on the methodology by describing the theoretical contribution.
"Also, the citation to Universal Dependencies is completely broken.",Does the review address Presentation?,TRUE,FALSE,This sentence critiques the presentation by pointing out citation errors.
My understanding is that contribution of the paper is in exploring using options framework to goal-oriented dialog to handle the issue in question.,Does the review address Contribution?,TRUE,FALSE,This sentence discusses the contribution of the options framework to address a dialogue issue.
So this contribution seems not practically useful according to the empirical result.,Does the review address Result?,TRUE,FALSE,This sentence evaluates the practical impact of the results.
"Perhaps an entropy-regularized setup is a useful comparison to show that it provides marginal benefit over the setup studied, and this might resolve the lack of clarity around the implications of the claims made from the first set of experiments.",Does the review address Comparison?,TRUE,FALSE,This sentence discusses the comparison aspect to address unclear claims.
"Also, the lack of attention mechanism provides a disadvantage to the baseline enc-dec system and it's unclear whether the pRNN can outperform or be an additive feature to the enc-dec system with an attention mechanism.",Does the review address Result?,TRUE,FALSE,This sentence evaluates experimental results concerning baseline weaknesses.
This approach is simple combination of  Doc2Vec and STE.,Does the review address Presentation?,TRUE,FALSE,This sentence describes the presentation of the methodology in a simplified manner.
Why authors consider questions answering and sentiment analysis as the applications?,Does the review address Analysis?,TRUE,FALSE,This sentence analyzes the choice of applications and raises related questions.
It's not really okay to put up the tables and show the perplexity and BLEU scores without some explanation.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence emphasizes the need for detailed explanations accompanying tables.
"However, I feel that experiments can be strengthened, and notations can be improved.",Does the review address Presentation?,TRUE,FALSE,This sentence focuses on enhancing the presentation and clarity of the work.
"I would have thought this was training an LM on progressively larger portions of the relevant data (being used for fine-tuning the others), in which case I'd also expect a downward trend in perplexity.",Does the review address Presentation?,TRUE,FALSE,This sentence discusses potential presentation improvements regarding training insights.
"- In the experiments in MultiWOZ, this paper only evaluates the response generation results.",Does the review address Experiment?,TRUE,FALSE,This sentence specifically mentions experimental evaluation in a task.
"For example, a lot of BERT-style models exploit dense interactions.",Does the review address Related Work?,TRUE,FALSE,This sentence refers to related work by discussing techniques used in other models.
"- Weaknesses: Though it may not be possible in the time remaining, it would be good to see a comparison (i.e. BLEU scores) with previous related work like hierarchical softmax and differentiated softmax.",Does the review address Comparison?,TRUE,FALSE,This sentence emphasizes the need for comparative evaluation.
PUCT-RL is the only directly comparable baseline given action space and other handicap differences.,Does the review address Comparison?,TRUE,FALSE,This sentence focuses on the comparison aspect with specific baselines.
"- After definition 5.1, what does Omega[w] = Omega[w’] mean?",Does the review address Presentation?,TRUE,FALSE,This sentence highlights a presentation issue requiring clarification.
The authors claim that “There are a number of benefits of adopting the adaptive smoothing parameter”.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodology by noting advantages of a specific parameter.
"Publication venues are often missing; e.g., where was Boonkwan & Supnithi (2018) published?",Does the review address Presentation?,TRUE,FALSE,This sentence critiques the presentation by pointing out missing information.
"Details ========= Abstract: In o gur work => In our work P5: ""oracle dialogue state"": What is the oracle dialog state and how is it calculated?",Does the review address Presentation?,TRUE,FALSE,This sentence seeks clarification on an aspect of the presentation.
3) One main contribution of this paper is the power-mean family of dropout.,Does the review address Contribution?,TRUE,FALSE,This sentence clearly states a main contribution.
"The billions that have been pumped into languages like English have in fact resulted in technologies that can be applied at much lower cost to languages like Kanyen’kéha, but there are still costs.",Does the review address Methodology?,FALSE,TRUE,The sentence discusses the impact and costs but not the methodology.
"Publication venues are often missing; e.g., where was Boonkwan & Supnithi (2018) published?",Does the review address Related Work?,TRUE,FALSE,It explicitly mentions the publication venue of related work.
In particular the part that argued why B = O(1/alpha).,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence involves a detailed discussion on why B = O(1/alpha).
"But that would also mean that the phrases are determined by token ngrams which produces a sliding window of the ""pyramid encoders"" for each sentence where there are instance where the parameter for these phrases will be set close to zero to disable the phrases and these phrases would be a good intrinsic evaluation of the pRNN in addition to evaluating it purely on perplexity and BLEU extrinsically.",Does the review address Presentation?,FALSE,TRUE,"This sentence discusses phrases and their evaluation, but not the presentation"
Theorem 1 in Section 5 seems to follow directly from the proposed definition of controllability.,Does the review address Theory?,TRUE,FALSE,It discusses a theorem in the context of theoretical foundations.
"The design of triggers, in this context, warrants a more nuanced discussion by the authors.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence suggests the need for more detailed discussion.
The larger dataset may relate to more many-to-many relationships when training the model.,Does the review address Methodology?,TRUE,FALSE,It talks about the dataset used in the training process.
"The method shows some performance gains over BERT on some GLUE tasks, but these are fairly small for the most part, and BERT outperforms the proposed method by a similar amount on a similar number of tasks.",Does the review address Comparison?,TRUE,FALSE,It compares the performance of the method with BERT.
"There are various weaknesses of the MID data, but the evaluation approach needs to be discussed or justified.",Does the review address Evaluation?,TRUE,FALSE,This sentence addresses the need for evaluation justification.
Weaknesses * Something that stands out is the lack of discussion and comparison to related works that employ recurrent formulations of attention.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It mentions the lack of discussion and comparison to related works.
"However, Section 3.1 cannot demonstrate effective theoretical information.",Does the review address Methodology?,TRUE,FALSE,It discusses the effectiveness of theoretical information presented.
"Even with the results that already exist, it is claimed (for example in section 5.1) that MC-LAVE-RL is the only algorithm to pass bottlenecks such as getting the action ""take lantern"" right.",Does the review address Result?,TRUE,FALSE,This sentence mentions specific results of the algorithm.
"Although I understand the experiment setup, missing reference to more recent VL works prevent readers from getting a good research landscape in the multimodal pre-training.",Does the review address Experiment?,TRUE,FALSE,It mentions the experiment setup and its shortcomings.
- Experiments contain 3 different tasks and each has datasets from different domains.,Does the review address Experiment?,TRUE,FALSE,This sentence is about the different tasks and datasets used in the experiments.
"Even though the results don’t show that the proposed loss function and proposed “conditional mean features” give improvements over baselines, the empirical results show that the basic assumptions and definitions in the theoretical analysis are relatively realistic.",Does the review address Presentation?,FALSE,TRUE,"This sentence discusses empirical results and theoretical analysis, but does not relate to presentation."
- Improvement over previous state-of-the-art models.,Does the review address Result?,TRUE,FALSE,This mentions improvement over previous models.
"The proposed system outperforms or achieves on-par  performance against previous SOTA methods, i.e., AudioGen and AudioLDM, in both objective and subjective metrics.",Does the review address Result?,TRUE,FALSE,This discusses the system's performance compared to other methods.
"What the authors can do is: you can sample some sentences from the test/development set and count how many comparative words are misused in the original model, among which how many are corrected by reranking.",Does the review address Methodology?,TRUE,FALSE,It suggests a methodological approach for improvement.
Introducing the layer and networks in a simple way would help clarify the implementation and other notation.,Does the review address Methodology?,TRUE,FALSE,This sentence suggests simplification for better clarity.
"In the experiment section, the authors only include the CoT annotations from [1] as the most important baseline.",Does the review address Comparison?,TRUE,FALSE,It discusses the importance of certain baselines in the experiment section.
Simply adding related sentences in the pre training input context helps end performance.,Does the review address Methodology?,TRUE,FALSE,It suggests a methodological improvement.
The experimental results mainly address similar networks with similar context lengths.,Does the review address Result?,TRUE,FALSE,It focuses on the experimental results and their context.
"For example, the $p^{\star}$ notation is also defined in Sec 2.1.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It references a definition in a specific section.
Empirical results on MultiWoz 2 and 2.1 shows improvement over other state-of-the-art techniques.,Does the review address Data/Task?,TRUE,FALSE,This mentions the empirical results using specific datasets.
This is a crucial scientific finding: money works!,Does the review address Significance?,FALSE,TRUE,This sentence expresses significance but does not elaborate on it.
"With this definition, a linear model of the logits is also a linear model over the context embeddings f(s) directly.",Does the review address Presentation?,TRUE,FALSE,It explains the relationship between models in a clear manner.
"The model leverage pre-trained BERT language models, making it very fast to train.",Does the review address Methodology?,TRUE,FALSE,It discusses the use of BERT models in the methodology.
Each task is supported with a detailed ablation study to shed light on future research.,Does the review address Data/Task?,TRUE,FALSE,It mentions the use of ablation studies for various tasks.
Making these comparisons would require a heavy rewrite starting from the abstract to the analysis and so I would recommend reject right now but look forward to seeing an updated version of the paper in the future with some of these changes.,Does the review address Analysis?,TRUE,FALSE,It suggests a detailed analysis and comparison.
"Also, possibly figure 3 can be combined into the pyramid part of figure 4.",Does the review address Presentation?,TRUE,FALSE,"This sentence discusses combining figures, which relates to the presentation."
"Especially, in terms novelty, the paper is relatively limited as the RF is explored in Rawat et al., 19.",Does the review address Novelty?,TRUE,FALSE,This sentence explicitly mentions the novelty of the paper.
"Considering this is the combined benefit of multiple techniques, e.g. distillation, text token selection, contrastive learning, I am not fully convinced by the empirical value of the proposed method.",Does the review address Methodology?,TRUE,FALSE,It discusses the combined techniques and empirical value of the method.
"It is not clear how this model would compare to other models using language specific parameters (sparsely gated mixture of experts (Lepikhin et al 2020), light-weight adapters (Bapna et al 2019)  ).",Does the review address Methodology?,TRUE,FALSE,This sentence questions the comparison with models using specific parameters.
2) TDE follows the previous work and aims to learn three different level embeddings.,Does the review address Methodology?,TRUE,FALSE,It describes the approach and aims of TDE in relation to the previous work.
"In comparison, if we look at Page 17, the actual annotations from the authors are very long and detailed.",Does the review address Data/Task?,TRUE,FALSE,"It discusses the length and detail of annotations, relating to the dataset/task."
This paper studies why language model pre-training has been such an effective technique in improving downstream performance across a wide range of NLP tasks recently.,Does the review address Result?,TRUE,FALSE,This sentence mentions the improvement in performance due to pre-training.
I felt this was quite separate from the theoretical analysis.,Does the review address Analysis?,TRUE,FALSE,It contrasts the topic with theoretical analysis.
* Related work: I would also include a discussion of this Lazaridou et al paper where they compare ways of combining EC with non-EC learning signals (e.g. image caption training): https://aclanthology.org/2020.acl-main.685.pdf  * Ethics statement: I appreciate this statement and agree with the possible positive impacts.,Does the review address Related Work?,TRUE,FALSE,This sentence includes a suggestion for discussing related work.
"Reducing these costs could have a very high impact on the field, allowing many more researchers to participate in state-of-the-art research [3].",Does the review address Significance?,TRUE,FALSE,It discusses the potential impact and importance of reducing costs.
There is also a NAACL 2016 paper (https://www.aclweb.org/anthology/N/N16/N16-2016.pdf) which performs relation extraction using a new model based on memory networks… and I’m sure there are more.,Does the review address Related Work?,TRUE,FALSE,This sentence mentions related work and specific papers.
### Notes on text and style  There are parts of the manuscript that felt somewhat informal and confusing to me.,Does the review address Presentation?,TRUE,FALSE,It discusses the text and style of the manuscript.
"The authors cite Bordes et al. (https://arxiv.org/pdf/1506.02075.pdf), who collect a similar dataset and perform relation extraction using memory networks (which are commonly used for reading comprehension).",Does the review address Related Work?,TRUE,FALSE,It references related work and datasets.
"- P24, Figure 2: What are the x and y axis, and what does each dot mean in this figure?",Does the review address Presentation?,TRUE,FALSE,"It asks for clarification on the figure's axes and meaning, relating to presentation."
Why don't the authors of this work do this evaluation as well?,Does the review address Data/Task?,TRUE,FALSE,"It questions the evaluation approach, relating to the task."
"I would have liked to see these results (also, please fix grammar in this sentence) 9.",Does the review address Result?,TRUE,FALSE,This sentence mentions results and suggests a grammar correction.
The findings of (2)(3)(4)(5) mentioned in the summary section above are especially interesting and can be helpful to the community when comparing new VL methods with existing methods.,Does the review address Methodology?,TRUE,FALSE,It discusses the helpfulness of findings in comparing methods.
The introduction of the motivation (the concept of in-context bias) is not easy to understand at the very beginning.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It mentions the difficulty in understanding the motivation.
It would be good if the authors could provide some intuition / insight as to why that might be the case.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It requests intuition or insight to clarify the point.
Towards making the most of bert in neural machine translation.,Does the review address Related Work?,TRUE,FALSE,It mentions the use of BERT in neural machine translation.
"However, there is no discussion on the latency in the proposed method and experiments.",Does the review address Methodology?,TRUE,FALSE,It notes the lack of discussion on latency.
- The ablation experiments and optimized prompt analysis are insightful.,Does the review address Analysis?,TRUE,FALSE,It mentions the insights gained from ablation experiments and prompt analysis.
The paper features extensive experiments that convincingly validate the effectiveness of the proposed method.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It discusses the validation of the method's effectiveness.
A qualitative analysis might be more revealing here.,Does the review address Analysis?,TRUE,FALSE,It suggests a qualitative analysis for better understanding.
This along with a regularization reward using language model the paper aims to improve comprehensibility.,Does the review address Methodology?,TRUE,FALSE,It discusses the aim to improve comprehensibility using a regularization reward.
Otherwise for example it is not clear to me if the improvement in Blue compared to LaRL comes from the extra reward using the language model or from the options framework.,Does the review address Result?,TRUE,FALSE,It questions the source of improvement in results.
"**Strengths** - To the best of my knowledge, this is the first work that _mathematically_ justifies the connection between the pre-training objective and the downstream performance.",Does the review address Methodology?,TRUE,FALSE,It mentions the mathematical justification of the methodology.
Please feel free to highlight other major contributions.,Does the review address Contribution?,TRUE,FALSE,It encourages highlighting other contributions.
"- Less technical comments: The paper writing is fine to me, but I don't like the typesetting.",Does the review address Presentation?,TRUE,FALSE,It discusses the writing and typesetting of the paper.
"Contribution: The authors contribute a new tokenization method, code, and a dataset.",Does the review address Data/Task?,TRUE,FALSE,"It mentions contributions including tokenization method, code, and dataset."
This is the newest of a small but growing body of literature that seeks to connect emergent communication with genuine NLP tasks.,Does the review address Related Work?,TRUE,FALSE,It discusses the connection with related literature and tasks.
"The authors argued that most of the previous multimodal LLMs used shallow connections between vision and models, and thus proposed a new module called visual expert.",Does the review address Related Work?,TRUE,FALSE,It mentions the proposed module and comparison with related work.
"In this case, the authors are using LeetCode coded solutions in MP to compiled the source code into a lower level form (assembly I believe) and then see if N-Bref can return the assembly back to the original form or some semantically equivalent form.",Does the review address Presentation?,TRUE,FALSE,It explains the method of using coded solutions and their presentation.
"- For the classification tasks, what do the label distributions look like over the labeled subset $\mathcal L$?",Does the review address Data/Task?,TRUE,FALSE,It questions the label distributions in the classification tasks.
* Could the authors provide more details about the evaluation settings and number of parameters of each model for the per-token loss comparison?,Does the review address Methodology?,TRUE,FALSE,It requests more details about the evaluation settings and parameters.
The paper has explained and empirically showed that this learned generator needs a resampler.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It mentions the explanation and empirical demonstration of the generator.
Hon downstream tasks with smaller learning rate -> Do you mean smaller datasets?,Does the review address Data/Task?,TRUE,FALSE,It questions the reference to smaller datasets.
"The proposed method has achieved SOTA performance on a range of Vision-Language benchmarks, spanning image captioning, Visual Question Answering, and visual grounding tasks.",Does the review address Methodology?,TRUE,FALSE,It discusses the SOTA performance achieved by the method.
"The studies here show that, pre-training with Inverse Cloze Task (ICT) the two-tower Transformer models significantly outperform the widely used BM-25 algorithm for large-scale information retrieval.",Does the review address Presentation?,TRUE,FALSE,It mentions the outperformance of models in large-scale information retrieval.
### Overall  Authors used BERT alongside to a 2D-position embedding based on a sinusoidal function and a graph-based decoder to improve performance on document information extraction tasks.,Does the review address Result?,TRUE,FALSE,It mentions the improvement in performance on document information extraction tasks.
"According to the evaluation, the proposed method shows its effectiveness especially when the finetuning data is limited.",Does the review address Evaluation?,TRUE,FALSE,It mentions the method's effectiveness based on the evaluation.
I found the result that the LSTM-based model successfully reconstructs the stimuli but fails on the main game to be interesting.,Does the review address Methodology?,TRUE,FALSE,It discusses the results of the LSTM-based model in detail.
It is based on the state-of-the-art language model--augmented memory.,Does the review address Methodology?,TRUE,FALSE,It mentions the state-of-the-art language model and its basis.
"Authors could have provided more in-depth details (visualizations, analysis, examples) to show main differences between the proposed approach and baselines (specially LayoutLM).",Does the review address Comparison?,TRUE,FALSE,It suggests providing more details for comparing the proposed approach with baselines.
"For example, the model hyper-parameters are quite different for different tasks.",Does the review address Data/Task?,TRUE,FALSE,It mentions differences in model hyper-parameters for different tasks.
The use of RNN and Copy RNN in the current context is a new idea.,Does the review address Methodology?,TRUE,FALSE,It discusses the use of RNN and Copy RNN as a new idea.
- There is some missing prior work in creating knowledge graphs from pre-trained language models.,Does the review address Related Work?,TRUE,FALSE,It mentions missing prior work related to knowledge graphs.
"The motivation is very clear, MCTS is generally action agnostic and using language to provide additional semantic information to it can prove to be very effective.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It discusses the clear motivation and effectiveness of using language for semantic information.
"If there is some significant difference here, it is not made clear in the paper.",Does the review address Presentation?,TRUE,FALSE,It mentions a lack of clarity in presenting significant differences.
"Along with ablations and trying LMs of varying sizes, their technique is compared against many other existing selective annotation approaches and shown to consistently outperform the latter.",Does the review address Result?,TRUE,FALSE,It discusses the comparison and performance of the technique against other approaches.
The authors should discuss the relationship with other in-context example selection methods and compare the performance.,Does the review address Result?,FALSE,TRUE,"This sentence suggests discussing relationships and comparing performance, but does not mention specific results."
It does not only require the optimization ability but also the oracle efficiency.,Does the review address Methodology?,TRUE,FALSE,It mentions the need for optimization ability and oracle efficiency.
This paper proposed a new phrasal RNN architecture for sequence to sequence generation.,Does the review address Presentation?,TRUE,FALSE,It mentions the new phrasal RNN architecture.
- Removing the dependence of engineered perceptual models from the CaP method by using foundation models.,Does the review address Methodology?,TRUE,FALSE,It discusses removing dependence on perceptual models by using foundation models.
"Additionally, there are some minor things I would add or improve: - I would add references to multi-task training on different languages (e.g., Task 1 is translation from EN to FR and Task 2 is translation from EN to DE).",Does the review address Methodology?,TRUE,FALSE,It suggests adding references to multi-task training on different languages.
"- Additionally, why isn't the the GRU version of pRNNv reported in the FBIS evaluation in Table 3?",Does the review address Presentation?,TRUE,FALSE,It questions the absence of the GRU version in the evaluation.
"- Weaknesses: In experiments, the author set the window width of the filters in the CNN module to 2.",Does the review address Experiment?,TRUE,FALSE,It mentions the window width setting in the experiments.
Use the text data that comes with each dataset as is and compare this with :   a) Using OpenAQA (current setting)   b) Augmenting the original audio text pairs with OpenAQA  2.,Does the review address Significance?,FALSE,TRUE,It discusses comparison approaches but not their significance.
The methods is evaluated on 5 standard NER+RE datasets with good performances.,Does the review address Data/Task?,TRUE,FALSE,It mentions evaluation on standard datasets with good performance.
"3.The model excels in various audio-related tasks and open-ended question answering, demonstrating its outstanding performance.",Does the review address Data/Task?,TRUE,FALSE,It mentions the model's performance in audio-related tasks.
"However, it seems like there's still some open questions about the types of improvements being made and what this implies about the LM's attention mechanism.",Does the review address Methodology?,FALSE,TRUE,"This sentence discusses open questions about improvements and the LM's attention mechanism, but not methodology."
The authors take time to implement and evaluate several prominent baselines.,Does the review address Presentation?,TRUE,FALSE,It mentions the evaluation of prominent baselines.
"Kernel-based variants of self-attention have a recurrent formulation and lead to linear complexity (see [1,2,3,4]).",Does the review address Methodology?,TRUE,FALSE,It discusses kernel-based variants of self-attention and their formulation.
Statistical comparisons of classifiers over multiple data sets.,Does the review address Related Work?,TRUE,FALSE,It mentions statistical comparisons of classifiers.
"High-level view:  I don’t think this is necessarily a bad paper, but I think it’s unacceptable for ICLR in its current form.",Does the review address Evaluation?,TRUE,FALSE,It evaluates the paper's suitability for ICLR.
This is very likely a confounding factor in the efficacy of active learning and pruning techniques.,Does the review address Methodology?,TRUE,FALSE,It mentions a confounding factor affecting active learning and pruning techniques.
The LSTM classifier is left highly unspecified (L407-409) -- there are multiple different architectures to use an LSTM for classification.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It mentions the unspecified nature of the LSTM classifier and multiple architectures.
"If the authors can adequately address the following issues around presentation and the interpretation of Theorem 5, I would be satisfied raising my score, as I value the research question raised by the paper and believe the conclusions are valid.",Does the review address Theory?,TRUE,FALSE,It mentions issues around presentation and interpretation of Theorem 5.
The findings from the analysis are an important addition to the understanding of the role of language specific parameters in multilingual NMT.,Does the review address Result?,TRUE,FALSE,It discusses the findings from the analysis and their importance.
- The objectives suggested are cheap to compute and seem to increase the signal available in the data.,Does the review address Data/Task?,TRUE,FALSE,It mentions the objectives and their effect on data signal.
"Considering the BERT is leveraged, you should discuss the relation with BERT + NMT [R1,R2].",Does the review address Methodology?,TRUE,FALSE,It suggests discussing the relation with BERT and NMT.
The results presented in the paper show strong gains against baseline methods on 3 different datasets.,Does the review address Result?,TRUE,FALSE,It mentions strong gains in results against baseline methods on multiple datasets.
This paper claims that it is the first time designing a reasoning comprehension-capable model.,Does the review address Methodology?,TRUE,FALSE,It discusses the novelty of designing a reasoning comprehension-capable model.
"Can the theory extend to cross-language clone detection, e.g., one language is tractable, but the other language is not?",Does the review address Theory?,TRUE,FALSE,It questions the application of the theory to cross-language clone detection.
This method should be included in the experiments in order to justify the proposed RL approach is necessary.,Does the review address Methodology?,TRUE,FALSE,It suggests including a method in experiments to justify the RL approach.
Can the proposed theory help explain if the same code modeling task for some languages is strictly easier than the others?,Does the review address Theory?,TRUE,FALSE,It questions the application of the proposed theory to code modeling tasks.
"In terms of strenghs - The paper has a very throughout analysis of different models and positional encodings - It proposes several contributions, including a new probing task and several positional encoding methods.",Does the review address Presentation?,TRUE,FALSE,It highlights the thorough analysis and contributions of the paper.
"- P6, Sec 4.3: ""In fact, $f$ almost always performs better than ..."" This part seems intriguing despite the linear relationship shown in figure 1.",Does the review address Result?,TRUE,FALSE,It mentions the intriguing performance of $f$ despite a linear relationship.
The results on a mixture of the Parity/Sum task are interesting   2.,Does the review address Result?,TRUE,FALSE,It comments on the interesting results of the Parity/Sum task.
"Based on the analysis, recommendations on design of multilingual NMT architectures are proposed and their efficacy validated experimentally.",Does the review address Experiment?,TRUE,FALSE,It mentions recommendations and experimental validation based on analysis.
"Also, can you add a column where a dense linear model over p_f(s) is used?",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It suggests adding a column for a dense linear model over p_f(s).
"* It will be cool to show some results on BLEU improvement on machine translation, or user studies on language generation tasks, to demonstrate its practical impact, as the quantitative metrics right now are still kind of artificial.",Does the review address Data/Task?,TRUE,FALSE,It suggests showing results on BLEU improvement or user studies to demonstrate practical impact.
"Your work is so similar to much of this work that you should really cite and establish novelty wrt at least some of them as early as the introduction -- that's how early I was wondering how your work differed, and it was not made clear.",Does the review address Related Work?,TRUE,FALSE,It suggests citing similar work and establishing novelty early in the introduction.
"The article is well-structured, starting with a thorough discussion on the shortcomings of naive backdoor-type watermarking methods before delving into their novel DOUBLE-I WATERMARKING FRAMEWORK.",Does the review address Methodology?,TRUE,FALSE,It discusses the structure and methodology of the article.
"For CSQA, the best number in this paper is 63.32 vs. 79.5 on the current leaderboard.",Does the review address Result?,TRUE,FALSE,It mentions the results in comparison to the current leaderboard for CSQA.
The experimental results on RoBERTa highlight the applicability and importance of this data augmentation approach on the downstream task of text classification (GLUE).,Does the review address Presentation?,TRUE,FALSE,It discusses the applicability and importance of the data augmentation approach.
"So I suggest that the authors give a technical introduction of the framework and more precisely discuss how it can solve the problem of interest, possibly with visual illustrations.",Does the review address Methodology?,TRUE,FALSE,It suggests giving a technical introduction and discussing the framework with visual illustrations.
"For example, if you could give us one or two sentences of Fon in the beginning of the paper, that demonstrate some of the difficulties of the language, I think this would greatly strengthen the motivation.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It suggests adding sentences to demonstrate the difficulties of the language.
"- The paper is well-written, well-organized, and easy to follow.",Does the review address Presentation?,TRUE,FALSE,"It comments on the writing, organization, and clarity of the paper."
"For instance, they ask how different amounts of data influence model behavior or if their data sampling method can react to task difficulty.",Does the review address Methodology?,TRUE,FALSE,It discusses how different data amounts influence model behavior and task difficulty.
"However, the paper does not include detailed descriptions about the proposed method, making readers not easy to understand.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It notes the lack of detailed descriptions about the proposed method.
"The study sheds light on the amount of language specific parameter sharing, their distribution in the network, impact of language, etc.",Does the review address Methodology?,TRUE,FALSE,It discusses the study's findings on parameter sharing and language impact.
"In a similar vein, I would have appreciated more details about the model architecture and the training regimes used (possibly in the appendix).",Does the review address Presentation?,TRUE,FALSE,It requests more details about the model architecture and training regimes.
"However, there is a lack of unified experimental standards and ablation experiments in this paper.",Does the review address Experiment?,TRUE,FALSE,It mentions the lack of unified standards and ablation experiments.
"These objectives certainly improve over the original T5 base _and_ larges models that are used as initializations, and especially outperform the base model in the low-data regime.",Does the review address Data/Task?,TRUE,FALSE,It mentions the improvement and performance of the objectives over T5 base models.
The authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks.,Does the review address Data/Task?,TRUE,FALSE,It mentions the evaluation on specific tasks.
I found the result that the LSTM-based model successfully reconstructs the stimuli but fails on the main game to be interesting.,Does the review address Result?,TRUE,FALSE,It comments on the interesting results of the LSTM-based model.
(3) The loss for answer prediction in Eq 6 is not clearly described: do you use an aggregated embedding over all nodes for prediction?,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It questions the description of the loss for answer prediction.
"In this way, the conclusion is only supported by the empirical observations but not the presented theoretical analysis.",Does the review address Theory?,TRUE,FALSE,It notes that the conclusion is only supported by empirical observations.
The usage of attention mechanism without some sort of pruning might be problematic at the phrasal level.,Does the review address Methodology?,TRUE,FALSE,It discusses potential problems with the attention mechanism without pruning.
This paper describes four methods of obtaining multilingual word embeddings and a modified QVEC metric for evaluating the efficacy of these embeddings.,Does the review address Evaluation?,TRUE,FALSE,It discusses the methods and metric for evaluating word embeddings.
"Detailed comments:  Training (especially pretraining) costs have been going wild in AI and NLP more particularly, which leads to large costs ([1]) as well as potential environmental problems ([2]).",Does the review address Methodology?,TRUE,FALSE,It mentions the training costs and potential environmental problems.
"I would have thought this was training an LM on progressively larger portions of the relevant data (being used for fine-tuning the others), in which case I'd also expect a downward trend in perplexity.",Does the review address Data/Task?,TRUE,FALSE,It discusses the training process and expected trend in perplexity.
"Also, different programmers write comments very differently.",Does the review address Analysis?,TRUE,FALSE,It mentions the variability in programmer comments.
"3) Continue with 2), as the experiment results shown in Table 2, TS compiler performs poorly.",Does the review address Result?,TRUE,FALSE,It comments on the poor performance of the TS compiler.
The wide range of datasets and active learning techniques they use (including BALD which prior works shows is very competitive) lends credence to the conclusions.,Does the review address Methodology?,TRUE,FALSE,It discusses the range of datasets and active learning techniques.
"In Table 1, the baseline models are TreeRNN and DCNN, they are originally used for sentence embedding but one can easily take the node/substructure embedding from them too.",Does the review address Methodology?,TRUE,FALSE,It mentions the use of baseline models for sentence embedding.
"## Justification As my primary critique concerns the experiments, I will address each individually mentioning any deficiencies and potential improvements.",Does the review address Experiment?,TRUE,FALSE,It mentions addressing critiques related to experiments.
"Result 1: Under the above assumption over the downstream task, this paper provides a bound on the empirical loss of the downstream prediction task.",Does the review address Result?,TRUE,FALSE,It discusses the bound on empirical loss of the downstream task.
"It is unclear to me how the authors are going to justify this ""assumption"".",Does the review address Presentation?,TRUE,FALSE,It questions the justification of an assumption.
"By using a multimodal, the current approach attain the problem of polysemy.",Does the review address Methodology?,TRUE,FALSE,It discusses the use of a multimodal approach to address the problem of polysemy.
"- Figure 1: unclear why certain input sizes have their accuracy going down, even though they have reached 100% in the earlier epochs.",Does the review address Presentation?,TRUE,FALSE,It mentions the accuracy and performance in relation to Figure 1.
Concerns around novelty and the multi-task setup was also raised by another reviewer (e7Hg).,Does the review address Novelty?,TRUE,FALSE,It addresses concerns around the novelty and multi-task setup.
"However, there is no theoretical result about the effectiveness of assigning the self-knowledge distillation to label smoothing.",Does the review address Methodology?,TRUE,FALSE,It mentions the lack of theoretical results regarding self-knowledge distillation.
The proposed method achieves SoTA results on CommonGen with slightly more than half the parameters of the current SoTA model.,Does the review address Result?,TRUE,FALSE,It mentions the SoTA results and parameter efficiency.
"- P6, Sec 4.3: ""In fact, $f$ almost always performs better than ..."" This part seems intriguing despite the linear relationship shown in figure 1.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It highlights the intriguing relationship despite the figure's data.
"- wMAN is evaluated with two publicly available datasets, and is compared with state-of-the-art methods and other ""oracle"" baselines.",Does the review address Evaluation?,TRUE,FALSE,It mentions the evaluation with datasets and comparison with baselines.
It is hard for other users to apply this technique.,Does the review address Methodology?,TRUE,FALSE,It discusses the difficulty in applying the technique.
"Other concerns along these lines: all of this paper's results are averaged over 3 runs while the other baselines are over 5 runs - an indication of variance would be useful to assess whether the differences are significant, especially since some of the margins quite small (23 on MC-LAVE-RL vs 22.8 for the next best on Ludicorp) added to the fact that hyperparameters are different for each game - does that imply that the authors tuned the hyperparameters for each game?",Does the review address Result?,TRUE,FALSE,It mentions the averaging of results and significance of the differences.
One very simple way could be to analyze the occurrence frequency of interleaved natural and programming patterns in the dataset.,Does the review address Analysis?,TRUE,FALSE,It suggests analyzing the occurrence frequency of patterns.
First theoretical definition of equivalence-preserving program embedding problem.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It discusses the first theoretical definition of the problem.
"The first is selecting the most uncertain examples, and the second is making the CoT annotations longer.",Does the review address Data/Task?,TRUE,FALSE,It mentions the selection of examples and CoT annotations.
"The evaluation results are based on the authors' implementations, for both baseline and the proposed method.",Does the review address Methodology?,TRUE,FALSE,It discusses the evaluation based on implementations.
The main weakness of the paper is that it is mainly based on further tuning the existing BERT model and lacks novel contribution in model architecture.,Does the review address Methodology?,TRUE,FALSE,It mentions the lack of novel contribution in the model architecture.
"... bunch of those errors has should be ""errors have"".",Does the review address Presentation?,TRUE,FALSE,It points out a grammar error in the text.
The paper provides a comprehensive study on the two-tower Transformer models in terms of the impact of its pre-training tasks on large-scale retrieval applications.,Does the review address Methodology?,TRUE,FALSE,It discusses the impact of pre-training tasks on retrieval applications.
The method relies much upon manual designs that seem hard to generalize.,Does the review address Presentation?,TRUE,FALSE,It mentions the reliance on manual designs and generalization difficulty.
"So how would it be possible to train with poor annotations, while generalize much better?",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It questions the possibility of training with poor annotations.
What if a similar sample exists in a quite different source task?,Does the review address Data/Task?,TRUE,FALSE,It discusses the existence of similar samples in different tasks.
"In experiments, there is no comparison with previous retrieval based methods.",Does the review address Experiment?,TRUE,FALSE,It mentions the lack of comparison in experiments.
The need for short sequence acceleration needs to be justified IMO.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It mentions the need for justification of short sequence acceleration.
"Indeed, the proposed graph model seems to only implicitly convey knowledge across facts in terms of local reasoning.",Does the review address Methodology?,TRUE,FALSE,It mentions the implicit conveyance of knowledge in the graph model.
The idea was (reasonably) well-positioned with respect to prior work and clearly presented.,Does the review address Related Work?,TRUE,FALSE,It mentions the positioning and presentation of the idea.
Ablation studies show that the model achieves good performance on more complex questions.,Does the review address Result?,TRUE,FALSE,It discusses the performance in ablation studies.
Hon downstream tasks with smaller learning rate -> Do you mean smaller datasets?,Does the review address Presentation?,TRUE,FALSE,It questions the meaning of smaller datasets.
"The paper has ""support set"" and ""support instructions"" at many places but it is unclear to me what it actually means.",Does the review address Presentation?,TRUE,FALSE,It mentions the unclear terms in the text.
### Summary ### The paper presents a technique for inference of certain kinds of program invariants directly from the program’s source code.,Does the review address Methodology?,TRUE,FALSE,It discusses the technique for inference from source code.
"However, the BERT analysis results provided in this paper should also be valuable to the community.",Does the review address Methodology?,TRUE,FALSE,It mentions the value of the BERT analysis results.
Strength: + Describes an important property of program embeddings: they should remain invariant to semantic-preserving transformations.,Does the review address Methodology?,TRUE,FALSE,It discusses the property of program embeddings.
"Theoretically, it shows that language models which are close to the “true” language model are guaranteed to attain strong performance on natural tasks.",Does the review address Presentation?,TRUE,FALSE,It mentions the theoretical guarantee of performance.
"The authors only put a sentence at the end of the Appendix saying that “we still found PEER to generate false statements or claims not backed up by the provided documents in many cases“, but in the main paper there is no discussion or statistics on this weakness.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It mentions the lack of discussion on a weakness.
"- Weaknesses: In experiments, the author set the window width of the filters in the CNN module to 2.",Does the review address Methodology?,TRUE,FALSE,It mentions the setting of filter width in experiments.
"### Cons and aspects to improve  My main concern is that the overall contribution is seems to be limited.In fact, the original paper of the Transformer approach, already proposed such kind of embedding.",Does the review address Contribution?,TRUE,FALSE,It mentions the limited contribution and reference to prior work.
"These are not weakness, but I think some work in this direction may help improve the paper.",Does the review address Presentation?,TRUE,FALSE,It mentions potential improvements for the paper.
"By associating nodes across different fact units based on coreferences and mentions, a supergraph is built that connects all related information and conducts graph reasoning for answer predictions.",Does the review address Methodology?,TRUE,FALSE,It discusses the construction of a supergraph for reasoning.
A three-line explanation at the end of Section 4.1 seems a bit scarce to me.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It mentions the brevity of an explanation in the text.
"## Justification As my primary critique concerns the experiments, I will address each individually mentioning any deficiencies and potential improvements.",Does the review address Result?,FALSE,TRUE,"This sentence discusses critiques and improvements for experiments, but does not mention specific results."
"It provides a nice theoretical framework for thinking about the connection between language models and downstream tasks, which future work could build on.",Does the review address Methodology?,TRUE,FALSE,It mentions the theoretical framework and future work potential.
"Figure from LayoutLM is a good example of that, it comprises the entire process and makes it easier to understand the whole architecture.",Does the review address Methodology?,TRUE,FALSE,It discusses the clarity provided by a figure.
The experimentation is correct and the qualitative analysis made in table 1 shows results as expected from the approach.,Does the review address Analysis?,TRUE,FALSE,It mentions the correctness of experimentation and analysis.
"It's ok for the proposed method to be one particular way, but that discussion would be useful.",Does the review address Methodology?,TRUE,FALSE,It suggests discussing the specific method used.
The difference in the ablation results seem quite small (tables 3 and 4).,Does the review address Ablation?,TRUE,FALSE,It mentions the small difference in ablation results.
3) The ablation study and visualization analysis of the experimental results are sufficient.,Does the review address Experiment?,TRUE,FALSE,It mentions the sufficiency of the ablation study and visualization analysis.
I can not understand why sample-specific rather than task-specific preference is important for prompt tuning.,Does the review address Methodology?,TRUE,FALSE,It questions the importance of sample-specific preference for prompt tuning.
"HRL in general has been used previously for goal-oriented dialog, using language models to regularize RL models has been used and pertaining using SL is widely used.",Does the review address Methodology?,TRUE,FALSE,It discusses the use of HRL and language models in previous work.
The paper does a systematic analysis on the role of language specific parameters using the proposed architecture.,Does the review address Methodology?,TRUE,FALSE,It mentions the systematic analysis of language-specific parameters.
"It would have been better to see the performance gains on more difficult text-classification tasks (non-GLUE), or underperforming models (non-BERT based).",Does the review address Methodology?,TRUE,FALSE,It suggests testing on more difficult tasks and different models.
"For table 4, please also include the significance of the BLEU improvement made by the pRNN with respect to the the baseline, see https://github.com/jhclark/multeval General Discussion ==== As the main contribution of this work is on the phrasal effect of the new RNN architecture, it's rather important to show that the phrases are more coherent than the vanilla LSTM / RNN model.",Does the review address Comparison?,TRUE,FALSE,It requests the significance of the BLEU improvement in comparison.
"In the experiment section, the authors only include the CoT annotations from [1] as the most important baseline.",Does the review address Experiment?,TRUE,FALSE,It mentions the inclusion of CoT annotations as a baseline.
This would probably highlight LTU significantly as the two approaches are contemporary in many ways and are very similar in the overarching goal.,Does the review address Significance?,TRUE,FALSE,It discusses the significance of highlighting LTU.
"Numerous neural architectures have been used to model programs, e.g., large language models, graph neural networks, etc.",Does the review address Presentation?,FALSE,TRUE,This sentence discusses the use of neural architectures to model programs but does not relate to presentation.
**Limitations** - The authors admit that their work is limited to a particular type of downstream tasks.,Does the review address Data/Task?,TRUE,FALSE,It mentions the limitation to specific downstream tasks.
"1.The paper introduces for the first time a large language model that combines both general audio perception capabilities and language reasoning abilities, along with the datasets used for training.",Does the review address Data/Task?,TRUE,FALSE,It mentions the introduction of a large language model and the datasets used.
"There are many confounding factors such as the number and type of pretraining data, the instruction-tuning data, different architecture designs, and finetuning strategies.",Does the review address Methodology?,TRUE,FALSE,It discusses various confounding factors.
- It would be great if authors could incorporate more baseline methods.,Does the review address Methodology?,TRUE,FALSE,It suggests incorporating more baseline methods.
"Comments on the model: - After computing the substructure embeddings, it seems very natural to compute an attention over them at each word.",Does the review address Methodology?,TRUE,FALSE,It discusses computing attention over substructure embeddings.
"It considers the training direction to be ""first to perceive, and then comprehend the sound"" so that the training starts from using close-ended datasets to open-ended datasets.",Does the review address Methodology?,TRUE,FALSE,It discusses the training direction from close-ended to open-ended datasets.
Details of training and dataset are logical and delicate.,Does the review address Methodology?,TRUE,FALSE,It mentions the logical and delicate details of training and dataset.
Would be great to state them upfront to avoid confusion.,Does the review address Presentation?,TRUE,FALSE,It suggests stating details upfront to avoid confusion.
I suspect the size plays an important role in such setups and this hasn't been discussed much in the paper.,Does the review address Significance?,TRUE,FALSE,It discusses the important role of size in setups.
"It provided me with a much more thoughtful explanation for why language model pre-training improves downstream task performance, beyond simply “it helps learn good general representations of language using large amounts of unlabeled text data” (my previous reasoning).",Does the review address Data/Task?,TRUE,FALSE,It mentions the thoughtful explanation for improvements in downstream task performance.
"It requires more analysis about experimental results, such as Figure 1 and tables in Section D.",Does the review address Result?,TRUE,FALSE,It requests more analysis of experimental results.
All of the results in this work seem to be previously known: * Theorem 1 is general to any polynomial-size circuit -- there is nothing special about Transformers.,Does the review address Result?,TRUE,FALSE,It mentions that the results seem to be previously known.
"But I support given a fixed set of phrase pairs at train time, the attention mechanism at the phrasal level can be pre-computed but at inference (apply the attention on new data at test time), this might be kind of problematic when the architecture is scaled to a larger dataset.",Does the review address Data/Task?,TRUE,FALSE,It discusses the potential problems with the attention mechanism.
"Furthermore, when there is a gap between the empirical results and the theoretical results (e.g., validation of Lemma 4.3 at the end of Section 4), the paper makes these limitations clear, which I appreciated very much as a reader (paper does not over-claim its contributions).",Does the review address Contribution?,TRUE,FALSE,It mentions the clarity of limitations and contributions.
"Are there any overheads/disadvantages because of multi-task learning (Like a larger model size, inference time for individual tasks etc)?",Does the review address Methodology?,TRUE,FALSE,It questions the overheads and disadvantages of multi-task learning.
"in Table 2, it's necessary to explain why the LSTM's perplexity from previous work is higher than the author's implementation.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It questions the perplexity differences in Table 2.
One of the important motivations of multi-modal multi-task learning mentioned was to achieve better or on-par performance with a single model (and supposedly fewer computations) which is crucial for devices with limited computing resources.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It discusses the motivation for multi-modal multi-task learning.
"Based on the analysis, recommendations on design of multilingual NMT architectures are proposed and their efficacy validated experimentally.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It mentions recommendations and experimental validation.
"The authors say ""we focus on larger datasets from GLUE (MNLI, QNLI, QQP and SST-2), as it is less meaningful to discuss efficient training"", but then report and analyze results from other GLUE datasets as well.",Does the review address Result?,TRUE,FALSE,It mentions the reporting and analysis of results from GLUE datasets.
I think the story starts with pointing out the importance for long-sequence but turns to the topic on short sequence which is confusing.,Does the review address Presentation?,TRUE,FALSE,It mentions the confusing transition between topics in the presentation.
"The computation of RFA requires outer product, which is O(D^2d) so overall it's O(M D^2 d), if M is around 64 or 128 (common usage) and D is 64, I actually don't see why RFA could improve 2x.",Does the review address Result?,TRUE,FALSE,It discusses the computation and results related to RFA.
"Even though the results don’t show that the proposed loss function and proposed “conditional mean features” give improvements over baselines, the empirical results show that the basic assumptions and definitions in the theoretical analysis are relatively realistic.",Does the review address Comparison?,TRUE,FALSE,It compares the empirical results with baselines and theoretical analysis.
"DP is not ""incorporated"" in a model or multimodality (as the authors mention in different ways a few times throughout the paper), DP is a property of a randomised algorithm (in this context, the training algorithm that produces the distribution of models, not the model).",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It explains the correct context for DP.
I felt this was quite separate from the theoretical analysis.,Does the review address Result?,TRUE,FALSE,It discusses the separation from theoretical analysis.
"Furthermore, they have clarified that on the key metric of CommonGen they achieved SoTA with only slightly more than half the parameters of the current SoTA model.",Does the review address Methodology?,TRUE,FALSE,It mentions achieving SoTA with fewer parameters.
"So I also have a few doubts about this article:  (1) This paper uses 6B T5-V1.1, but the previous baseline work only GTRxxl has the same size, while ColBERTv2 using multi-vector retrieval model has only 110m model size.",Does the review address Methodology?,TRUE,FALSE,It discusses doubts regarding model sizes and baselines.
"Your work obviously is different enough to stand on its own, but it might be good to make a mention of this work and others (e.g. do more of a lit search / related work on low rank compression).",Does the review address Related Work?,TRUE,FALSE,It suggests mentioning related work to establish novelty.
Strengths: - Unifying generative and contrastive training is an important and interesting goal.,Does the review address Presentation?,TRUE,FALSE,It mentions the goal of unifying generative and contrastive training.
The PACT methodology: The paper proposes a methodology for extracting auxiliary tasks that can be trained jointly along with the main task (tactic prediction task).,Does the review address Methodology?,TRUE,FALSE,It discusses the methodology for extracting and training auxiliary tasks.
"However, it seems like there's still some open questions about the types of improvements being made and what this implies about the LM's attention mechanism.",Does the review address Result?,FALSE,TRUE,This sentence discusses open questions about improvements and the LM's attention mechanism but does not directly mention specific results.
"The main contribution are the following innovations: a special attention mechanism called block-diagonal conditional attention, a set of modules for adaptation of a pretrained model, and an uncertainty-based multi-task data sampling method.",Does the review address Methodology?,TRUE,FALSE,It discusses the innovations and contributions.
The evaluation method uses CCA to maximize the correlation between the word embeddings and possibly hand crafted linguistic data.,Does the review address Evaluation?,TRUE,FALSE,It discusses the evaluation method using CCA.
The findings illustrate the substantial impact this choice can have on the final model's behavior.,Does the review address Methodology?,TRUE,FALSE,It mentions the impact of the choice on model behavior.
Both settings show the better performance of the proposed method.,Does the review address Result?,TRUE,FALSE,It mentions the performance in both settings.
"Secondly, the design of the specific neural network cannot describe the theory behind proposed binding-unbinding mechanism properly.",Does the review address Theory?,TRUE,FALSE,It discusses the neural network design and theory.
"* In the abstract, authors say ""BROS utilizes a powerful graph-based decoder that can capture the relation between text segment""* Though in the text such a component (that is from other work) is only mentioned twice without further detail.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It mentions the lack of detail in the description of a component.
The findings of (2)(3)(4)(5) mentioned in the summary section above are especially interesting and can be helpful to the community when comparing new VL methods with existing methods.,Does the review address Result?,TRUE,FALSE,It discusses the interesting findings and their potential help.
"This paper makes comparison with techniques used in active learning (AL), domain shift detection (DS), and multi-domain sampling to combine data from multiple sources.",Does the review address Comparison?,TRUE,FALSE,It mentions the comparison with techniques from different areas.
"Unfortunately, as the paper overall does not seem to contain significant novelty, neither in methodology nor in results, I cannot recommend acceptance at this point.",Does the review address Methodology?,TRUE,FALSE,It mentions the lack of significant novelty in methodology and results.
"First of all, the dense interaction between vision and language tokens has been heavily studied prior to the so-called multimodal LLM era.",Does the review address Methodology?,TRUE,FALSE,It discusses prior studies on vision and language token interaction.
"However, in its current state - the comparisons made are not meaningful which makes the claim of state of the art tenuous (state of the art does not matter so much as showing that you make progress in line with the motivation).",Does the review address Comparison?,TRUE,FALSE,It discusses the importance of meaningful comparisons.
"Additionally, there are some minor things I would add or improve: - I would add references to multi-task training on different languages (e.g., Task 1 is translation from EN to FR and Task 2 is translation from EN to DE).",Does the review address Data/Task?,TRUE,FALSE,It suggests adding references to multi-task training on different languages.
Is it possible to import a public SOTA implementation and conduct comparisons based on that?,Does the review address Comparison?,TRUE,FALSE,It suggests using public SOTA implementation for comparisons.
- I don't see why your theory does not generalize to a _masked_ language modeling (MLM).,Does the review address Methodology?,TRUE,FALSE,It questions the generalization of the theory to MLM.
The paper provides a comprehensive study on the two-tower Transformer models in terms of the impact of its pre-training tasks on large-scale retrieval applications.,Does the review address Data/Task?,TRUE,FALSE,It discusses the comprehensive study on Transformer models.
"- OpenAQA-5M is a good contribution to provide open-ended question answering in audio domain, especially it is verified with human evaluation.",Does the review address Data/Task?,TRUE,FALSE,It mentions the contribution to open-ended question answering.
"Along with ablations and trying LMs of varying sizes, their technique is compared against many other existing selective annotation approaches and shown to consistently outperform the latter.",Does the review address Comparison?,TRUE,FALSE,It mentions the comparison and performance against other approaches.
"How does Theorem 2 come into play when proving Theorem 3, if Theorem 3 can be proved independently?",Does the review address Theory?,TRUE,FALSE,It questions the role of Theorem 2 in proving Theorem 3.
An additional ablation experiment trying different number of tokens to optimize could be illuminating (even for just 1 dataset).,Does the review address Ablation?,TRUE,FALSE,It suggests an additional ablation experiment with different number of tokens.
"That is, by ""general learner"" the authors mean a model that can *express* a universal circuit.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"It explains the meaning of ""general learner"" as a model that can express a universal circuit."
Could the authors list some possible approaches to automatically choose this hyper-parameter please?,Does the review address Methodology?,TRUE,FALSE,It asks for possible approaches to automatically choose the hyper-parameter.
"- From the diversity and representativeness measures shown in Table 10 in Appendix F, the difference between Random and vote-*k* does not appear to be very large.",Does the review address Presentation?,TRUE,FALSE,It mentions the small difference between Random and vote-k in diversity and representativeness measures.
Such a comparison would highlight the advantages of the multi-task model and would be helpful for the relevant audience.,Does the review address Comparison?,TRUE,FALSE,It suggests that a comparison would highlight the advantages of the multi-task model.
"This line of work could be much stronger if the models comprised the whole process (detection, text extraction, recognition) in an end-to-end manner.",Does the review address Methodology?,TRUE,FALSE,It suggests that the work could be stronger if models were end-to-end.
"While the exact choice depends on the dataset characteristics, a framework will be more attractive if it can perform well on different scenarios.",Does the review address Presentation?,TRUE,FALSE,It mentions that a framework will be more attractive if it performs well on different scenarios.
## Strengths - The environment is novel and provides a good basis for studying certain traits of continuous-channel referential games.,Does the review address Novelty?,TRUE,FALSE,It mentions the novelty of the environment and its basis for studying traits.
"The proposed method has achieved SOTA performance on a range of Vision-Language benchmarks, spanning image captioning, Visual Question Answering, and visual grounding tasks.",Does the review address Result?,TRUE,FALSE,It mentions the SOTA performance on Vision-Language benchmarks.
The use of RNN and Copy RNN in the current context is a new idea.,Does the review address Novelty?,TRUE,FALSE,It mentions the novelty of using RNN and Copy RNN.
"Running a baseline model that runs *for the same amount of time* is essential to appreciate the contribution of this work (e.g., repeat the same analysis in Figure 3 for the vanilla BERT).",Does the review address Contribution?,TRUE,FALSE,It mentions that running a baseline model is essential to appreciate the contribution.
"This is immediate from Theorem 2, and this kind of separation appears to have been the implicit the point of Merrill and Sabharwal 2023  * This statement seems overly strong and minimizes prior work: ""This work takes the first step towards rigorously answering the fundamental question by considering a theoretical boundary of model capacity to be general learner""      * e.g., ""Saturated Transformers are Constant-Depth Threshold Circuits"" shows that transformers lie in TC^0 under a saturation condition     * e.g., ""The Parallelism Tradeoff: Limitations of Log-Precision Transformers"" shows that log-precision transformers lie in log-space-uniform TC^0",Does the review address Related Work?,TRUE,FALSE,It mentions prior work and points out an overly strong statement.
"This is mentioned several times, but there are no initial results or anything suggesting that it might be a promising direction to pursue.",Does the review address Result?,TRUE,FALSE,It mentions the lack of initial results suggesting a promising direction.
"The dialog needs to be polite, follow natural language, short, etc which are hard to automatically measure.",Does the review address Evaluation?,TRUE,FALSE,It mentions the difficulty in automatically measuring the quality of dialog.
"In sec3.1, you used S_t for minibatch, but in sec3.4, you use S_i for ""a text sequence"", which is confusing.",Does the review address Presentation?,TRUE,FALSE,It points out the confusing use of different notations.
"The authors switch between using BERT_base and RoBERTa_base without being very clear about when and why, e.g., in Table 2 they have both models, but only in different sections.",Does the review address Comparison?,TRUE,FALSE,It mentions the unclear use of different models in different sections.
The evaluation method uses CCA to maximize the correlation between the word embeddings and possibly hand crafted linguistic data.,Does the review address Methodology?,TRUE,FALSE,It discusses the evaluation method using CCA.
- The title of the paper is a bit strongly worded and may be over-claiming what is shown quantitatively in this paper.,Does the review address Result?,TRUE,FALSE,It mentions that the title may be over-claiming the quantitative results.
"The unfair disadvantage is even more prevalent when the pRNN uses multiple phrasal attention layers within a single sentence while a simple enc-dec system without attention is used as a benchmark =( Question: Wouldn't a simpler way to get phrasal RNN is to put the ""pyramid"" RNNs of a phrase into some soft of a average pooling layer?",Does the review address Methodology?,TRUE,FALSE,It discusses the use of multiple phrasal attention layers and suggests an alternative approach.
How much does the system rely on the capabilities of LLMs?,Does the review address Methodology?,TRUE,FALSE,It questions the reliance on the capabilities of LLMs.
TacticZero by Wu et al 2021 is missing from the related work.,Does the review address Related Work?,TRUE,FALSE,It mentions the omission of TacticZero from the related work.
"If the authors can adequately address the following issues around presentation and the interpretation of Theorem 5, I would be satisfied raising my score, as I value the research question raised by the paper and believe the conclusions are valid.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It mentions the need to address issues around presentation and interpretation.
I felt similar conclusions can be drawn from the results of that paper as well.,Does the review address Comparison?,TRUE,FALSE,It mentions that similar conclusions can be drawn from another paper.
"However, this was already proposed and implemented by an earlier paper (Oikarinen et al.).",Does the review address Contribution?,TRUE,FALSE,It mentions that the contribution was already proposed and implemented by earlier work.
"This is immediate from Theorem 2, and this kind of separation appears to have been the implicit the point of Merrill and Sabharwal 2023  * This statement seems overly strong and minimizes prior work: ""This work takes the first step towards rigorously answering the fundamental question by considering a theoretical boundary of model capacity to be general learner""      * e.g., ""Saturated Transformers are Constant-Depth Threshold Circuits"" shows that transformers lie in TC^0 under a saturation condition     * e.g., ""The Parallelism Tradeoff: Limitations of Log-Precision Transformers"" shows that log-precision transformers lie in log-space-uniform TC^0",Does the review address Theory?,TRUE,FALSE,It mentions prior work related to the theoretical boundary of model capacity.
The community will be interested how the model type / scale affect the LFLL capability.,Does the review address Methodology?,TRUE,FALSE,It discusses the interest in how model type and scale affect LFLL capability.
"With the strengths being said, I hope to also point out that the paper's application of adversarial training is one attempt in many possibilities, and in many cases it is not clear where the improvements come from.",Does the review address Presentation?,TRUE,FALSE,It mentions the unclear source of improvements from adversarial training.
Comparison with GECA: I can read from the paper that the performance is on par with GECA.,Does the review address Comparison?,TRUE,FALSE,It mentions the performance comparison with GECA.
"Clarification of contribution  Eq 6,7 reads like RNN style update but the intuition is lacking.",Does the review address Contribution?,TRUE,FALSE,It mentions the lack of intuition in the contribution.
"For table 4, please also include the significance of the BLEU improvement made by the pRNN with respect to the the baseline, see https://github.com/jhclark/multeval General Discussion ==== As the main contribution of this work is on the phrasal effect of the new RNN architecture, it's rather important to show that the phrases are more coherent than the vanilla LSTM / RNN model.",Does the review address Result?,TRUE,FALSE,It requests the significance of the BLEU improvement in comparison to the baseline.
"The organization is not perfect and readers might find it hard to follow here and there, but the main idea is understandable.",Does the review address Presentation?,TRUE,FALSE,It mentions the imperfect organization and understandability of the main idea.
"It is unclear to me how the authors are going to justify this ""assumption"".",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It questions the justification of an assumption.
5.2 visualizations: this seems pretty ad-hoc without much justification for the choices.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It mentions the lack of justification for visualization choices.
The latter ratio seems proportional to the ratio $\frac{\ell_\mathcal{T}(\\{p_{\cdot\mid s}\\}) - \tau}{\ell_\text{xent}(\\{p_{\cdot\mid s}\\})-\ell_\text{xent}^\ast}$.,Does the review address Methodology?,TRUE,FALSE,It discusses the proportionality of the ratio.
I suspect that a statistical analysis [1] might conclude that BERT and the proposed method are indistinguishable on the GLUE suite.,Does the review address Analysis?,TRUE,FALSE,It suggests that a statistical analysis might show BERT and the proposed method are indistinguishable.
"More importantly, the experiments are not convincing as it is presented now.",Does the review address Presentation?,TRUE,FALSE,It mentions that the experiments are not convincing.
"The experimental settings in Section 3 lack detailed descriptions, potentially making reproduction difficult and potentially misleading.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It mentions the lack of detailed descriptions in experimental settings.
"The theoretical results on the Parity/Sum task reply to some strong assumptions: bilinear parameterization, some initialization (for example, v = 0).",Does the review address Methodology?,TRUE,FALSE,It discusses the strong assumptions in the theoretical results.
Having additional ways to improve data efficiency by changing the model design is definitely of interest.,Does the review address Data/Task?,TRUE,FALSE,It mentions the interest in improving data efficiency by changing the model design.
The paper describes the idea of multiple temporal scales.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It mentions the idea of multiple temporal scales.
"- P8, Table 2: The results from using Quad look worse than the above two.",Does the review address Presentation?,TRUE,FALSE,It mentions the worse results from using Quad.
This is the first such dataset for the Lean Theorem prover.,Does the review address Theory?,FALSE,TRUE,This sentence does not relate to any theoretical concepts or results.
"Since one of the main contributions of this paper is the analysis of the BERT pretraining process, more experimental analysis on the optimizer should also be included.",Does the review address Experiment?,TRUE,FALSE,It suggests including more experimental analysis on the optimizer.
This could be another area to investigate  ---- Misc:  ----  - UnifiedQA seems potentially worth citing as prior work,Does the review address Related Work?,TRUE,FALSE,It suggests citing UnifiedQA as prior work.
I believe RFA should only refer one thing and I don't think eq(6) and eq(5) leads to the same result.,Does the review address Result?,TRUE,FALSE,It questions the consistency of results between eq(6) and eq(5).
"Then, most of the paper is spent discussing preliminaries and introducing notation and definitions.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It mentions the discussion of preliminaries and definitions.
"It describes a mapping of ORCHID, a Thai-specific POS tagset, to the Universal Dependencies (UD) scheme, and evaluates various state-of-the-art POS taggers on this scheme.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It describes the mapping of ORCHID to UD and evaluates POS taggers.
An ablation analysis would be most appropriate for quantifying this.,Does the review address Methodology?,TRUE,FALSE,It suggests that ablation analysis would be appropriate for quantification.
Why do the authors use the Quora dataset in particular?,Does the review address Data/Task?,TRUE,FALSE,It questions the choice of the Quora dataset.
"So, In this case, it will be interesting to see the results (or will be helpful in evaluating ""absent type"" keyphrases): if we identify all the topical phrases of the entire corpus by using tf-idf and relate the document to the high-ranked extracted topical phrases (by using Normalized Google Distance, PMI, etc.).",Does the review address Presentation?,TRUE,FALSE,It discusses the evaluation of keyphrases.
Another limitation is that it is unclear whether the improvement would hold when the size of the model increases; the evaluation is dealing with scaling laws after all.,Does the review address Methodology?,TRUE,FALSE,It mentions the uncertainty of improvement with model size increase.
- Experimental results: I suggest the author to provide more ablation analysis to the experiment section.,Does the review address Experiment?,TRUE,FALSE,It suggests providing more ablation analysis in the experiment section.
Some experimental settings only include the baselines Random and Vote-k. More methods as mentioned in 4.3.2 can be also included.,Does the review address Comparison?,TRUE,FALSE,It suggests including more methods in experimental settings.
"Maybe I've missed this in the paper, if there is, please be more explicit about it because it affects the model quite drastically if for every sentence the largest phrase length is the sentence length.",Does the review address Methodology?,TRUE,FALSE,It discusses the impact of phrase length on the model.
3) The usage of graph is somewhat straightforward to me.,Does the review address Methodology?,TRUE,FALSE,It mentions the straightforward usage of the graph.
It does not follow from the theoretical results that adding similar sentences will be a good thing.,Does the review address Theory?,TRUE,FALSE,It mentions that adding similar sentences is not supported by theoretical results.
"Moreover, I have some comments on the model and experiments.",Does the review address Methodology?,TRUE,FALSE,It mentions having comments on the model and experiments.
"The paper can be better if adding the real-user interactions, because the performance may be different between the simulation environment and the real-user interactions reported by prior results (DSTC in ConvLab).",Does the review address Result?,TRUE,FALSE,It suggests including real-user interactions for better performance evaluation.
*  The claims regarding the 10x better data efficiency are not well supported and I would suggest the authors to compare with transformer models on standard LM benchmarks and potentially to some downstream tasks such as MT to make a stronger case.,Does the review address Comparison?,TRUE,FALSE,It suggests comparing the claims with transformer models.
"Specific criteria: - Correctness: 4     - The claims are supported, but I do not think the claims go far enough (e.g., ""noise has an effect on generalization"" is claimed when instead what that effect is needs to be characterized).",Does the review address Methodology?,TRUE,FALSE,It discusses the need to characterize the effect of noise on generalization.
**Novelty** The idea of utilizing weak supervision of interleaved patterns is intuitive and convincing.,Does the review address Novelty?,TRUE,FALSE,It mentions the novelty of utilizing weak supervision of interleaved patterns.
"This paper suggests a number of cheap-to-compute corruptions of the input data that, when used to reconstruct the input, enrich the underlying model.",Does the review address Data/Task?,TRUE,FALSE,It discusses using data corruptions to enhance the model.
"However, it should be straightforward to extend the results from these papers to the poly(n) case, at least in the nonuniform setting considered here.",Does the review address Result?,TRUE,FALSE,It mentions extending results to the poly(n) case in a nonuniform setting.
*  The evaluation focuses on comparing with an empirical law learned on a different experimental configuration and there is a concern about how comparable are the results to the ones obtained in this study and the validity of the conclusions.,Does the review address Experiment?,TRUE,FALSE,It raises concerns about comparing results from different experimental setups.
"In algorithm 1, in each iteration, only data sample (S_i) is used, how is this choice motivated?",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It questions the motivation behind using a specific data sample in each iteration.
"If not, the presentation requires to change and reflect only the controllability analysis.",Does the review address Analysis?,TRUE,FALSE,It suggests revising the presentation to focus solely on controllability analysis.
I will be more convinced if evaluation is done on a wider range of tasks.,Does the review address Evaluation?,TRUE,FALSE,It calls for evaluation on a broader range of tasks for better validation.
"- wMAN is evaluated with two publicly available datasets, and is compared with state-of-the-art methods and other ""oracle"" baselines.",Does the review address Comparison?,TRUE,FALSE,It discusses the evaluation of wMAN against state-of-the-art methods and baselines.
The authors experiment their method on three datasets and get the state of the art results.,Does the review address Data/Task?,TRUE,FALSE,It mentions achieving state-of-the-art results on three datasets.
How many include simple string operations and/or other simple method calls as implied by Table 2?,Does the review address Methodology?,TRUE,FALSE,It inquires about the inclusion of simple string operations and method calls.
The paper presents a novel way of combining information from text and a KB in a bidirectional way.,Does the review address Novelty?,TRUE,FALSE,It highlights the novelty of combining text and knowledge base information bidirectionally.
"(2) What’is more, if tx contains s_k, can we say that the selected $tx$ is similar to $x$?",Does the review address Presentation?,FALSE,TRUE,This sentence does not mention presentation.
"Why do you choose case-insensitive BLEU score for En->Fr, which is not commonly used in previous baselines.",Does the review address Significance?,FALSE,TRUE,"This sentence questions the choice of BLEU score, but it does not mention significance."
The idea was (reasonably) well-positioned with respect to prior work and clearly presented.,Does the review address Presentation?,TRUE,FALSE,It praises the clear presentation and positioning with respect to prior work.
"- How to find a discriminant for meaning is left out as the authors explicitly assume that “the mechanism is provided by human annotators and other providers of training data.” While the authors emphasize in the introduction that such information can be used in the LLM training without external reward model: “This observation shows that sentence-level annotations can be incorporated directly into the trained model without the need for any external reward model nor external policy model, simply by sentence-level feedback,” I do not see the advantage of this approach over using the very same data to train a reward model and use that either during the training (as in RLHF) or as an augmentation (as in Rectification method), the latter indeed provides quite strong theoretical guarantees.",Does the review address Theory?,TRUE,FALSE,It critiques the advantage of using sentence-level feedback over reward models.
"While the exact choice depends on the dataset characteristics, a framework will be more attractive if it can perform well on different scenarios.",Does the review address Result?,FALSE,TRUE,"This sentence does not mention result, but it discusses the attractiveness of a framework based on dataset characteristics."
I think as an ACL paper there should be more takeaways.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It suggests that the paper should offer more takeaways.
"Informally , a task is defined as natural if, just by using the next word distributions as features, the downstream task can be solved with a small loss.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It defines a task as natural based on next word distributions.
The authors try to interpret the design of the neural networks using the concepts in the proposed binding-unbinding theorybut are not convincible.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It critiques the unconvincing interpretation using binding-unbinding theory.
"I would suggest the following:   * To make the comparison more fair, I would suggest to train transformer models with varying size and derive a power law based on the exact same experimental configuration used for LMU models.",Does the review address Comparison?,TRUE,FALSE,It suggests making the comparison fairer by training transformers with varying sizes.
"While this paper provides extensive empirical results and quantitively demonstrates the effectiveness of RandomMask, there are several areas where it could be further enhanced.",Does the review address Methodology?,TRUE,FALSE,It points out areas for enhancement despite the demonstrated effectiveness of RandomMask.
"The paper meticulously provides all experimental details, and the ablation study helps to validate the design components, enhancing the overall robustness of the research.",Does the review address Ablation?,TRUE,FALSE,It praises the meticulous provision of experimental details and validation through ablation study.
Weaknesses: Some of the design choices need to be elaborated on further and additional analysis-based experiments would also be useful.,Does the review address Experiment?,TRUE,FALSE,It suggests elaborating on design choices and conducting more analysis-based experiments.
There is a lot missing to actually justify this claim: 1.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It points out missing elements needed to justify the claim.
"Before describing your algorithm, humans are only mentioned once in the algorithm.",Does the review address Presentation?,TRUE,FALSE,It notes that humans are mentioned only once before describing the algorithm.
I appreciate the additional figures and other results that you have provided.,Does the review address Result?,TRUE,FALSE,It expresses appreciation for the additional figures and results.
"* Strength     * This paper proposes an interesting idea and interpretation to connect RM and DM paradigm     * This paper proposes a variance reduction method for DPG which demonstrates its improvement on performance, stability and sample efficiency * Weakness     * From my understanding, the baseline mostly comes from the observation in 3.3, which has limited technical novelty.",Does the review address Presentation?,TRUE,FALSE,It praises the proposed ideas but notes limited technical novelty in the baseline.
"Main strengths: - Mapping existing Thai corpora to the UD scheme is a useful contribution for Thai NLP, as the UD scheme has become popular for multilingual work, and this will allow Thai NLP to profit better from advances in this area.",Does the review address Contribution?,TRUE,FALSE,It highlights the contribution of mapping Thai corpora to the UD scheme for Thai NLP.
The new established benchmark is another good contribution.,Does the review address Data/Task?,TRUE,FALSE,It notes the establishment of a new benchmark as a positive contribution.
"Previous work [2] has already shown that by selecting the most complex examples from the training dataset, the performance can be largely improved compared to the original annotations from [1].",Does the review address Result?,TRUE,FALSE,It mentions that selecting complex examples improves performance.
"There is no comparison with other public masking methods in Table 2, such as whole-word-masking, span masking, etc.",Does the review address Comparison?,TRUE,FALSE,It points out the lack of comparison with other public masking methods.
"Though the paper promises faster training speeds in the introduction, Table 3 shows only modest (less than x2) speedups for training.",Does the review address Result?,TRUE,FALSE,It notes that the actual training speedup is less impressive than promised.
- Using “concept” to stand in for verbs and nouns is somewhat confusing.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It mentions that using “concept” for verbs and nouns is confusing.
"For the first ablation, it just gives out the performance of intermediate models on a single task.",Does the review address Data/Task?,TRUE,FALSE,It mentions that the first ablation only reports performance on a single task.
"Overall, the authors show better accuracy for their tested problem set against REWARD, a baseline system (a transformer), lang2logic, and Ins2AST across two dimensions: data type recovery and abstract syntax tree (AST) generation.",Does the review address Comparison?,TRUE,FALSE,It mentions the comparison of accuracy across different systems and dimensions.
"However, there are insufficient experiments and comparison to previous work to convince me that the paper’s contributions are novel and impactful.",Does the review address Novelty?,TRUE,FALSE,It points out the insufficient experiments and comparisons to assert the paper’s novelty and impact.
I had to read it a couple of times before I could fully follow the method.,Does the review address Presentation?,TRUE,FALSE,It mentions the difficulty in following the method described.
The proposed method achieves SoTA results on CommonGen with slightly more than half the parameters of the current SoTA model.,Does the review address Methodology?,TRUE,FALSE,It highlights the achievement of SoTA results with fewer parameters.
"However, there are insufficient experiments and comparison to previous work to convince me that the paper’s contributions are novel and impactful.",Does the review address Comparison?,TRUE,FALSE,It notes the lack of sufficient experiments and comparisons to previous work.
Weaknesses:  - The notation/description of section 4 is not immediately intuitive.,Does the review address Presentation?,TRUE,FALSE,It critiques the non-intuitive notation and description in section 4.
- Each component in perceiver IO is necessary and well defined for the proposed tasks.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It highlights the necessary and well-defined components of perceiver IO for the tasks.
"In addition, the authors empirically demonstrate that the token-level masked-LM model used by BERT is not a good choice as pre-training task for the two-tower Transformer when deployed for large-scale information retrieval applications.",Does the review address Methodology?,TRUE,FALSE,It evaluates the effectiveness of BERT's masked-LM model for pre-training tasks.
"- The paper mentioned that the standard RL methods easily fail and generate responses diverging from human language, even when fine-tuning a pre-trained LM.",Does the review address Methodology?,TRUE,FALSE,It discusses the limitations of standard RL methods in generating human-like responses.
"With these two elements, the approach performs on par with recently proposed GECA (where the data is not augmented via a neural generator) on two datasets.",Does the review address Data/Task?,TRUE,FALSE,It mentions the comparative performance of the approach with GECA on two datasets.
"Last sentence of intro: ""without scarifying accuracy"" seems like an inaccurate description of the results presented in this paper.",Does the review address Presentation?,TRUE,FALSE,It critiques the accuracy of the introductory claim.
Which script did you choose to evaluate BLEU score?,Does the review address Presentation?,TRUE,FALSE,It inquires about the script used for evaluating BLEU scores.
"Strengths: - Thorough theoretical analysis that reveals the connection between (practically-necessary) small learning rates and inability to use dependencies across text chunks - Useful framing and discussion of the ""in-context bias"", where models are more likely to learn dependencies within text chunks seen during pre-training.",Does the review address Analysis?,TRUE,FALSE,"It praises the theoretical analysis and framing of the ""in-context bias."""
"* Why are the lines for ""from scratch"" flat in Figure 2?",Does the review address Methodology?,TRUE,FALSE,It questions the methodology behind the flat lines in Figure 2.
"Although the complexity analysis is thorough, I'd like to see empirical results of memory/compute requirements as a function of the context length.",Does the review address Result?,TRUE,FALSE,It requests empirical results to support the complexity analysis.
"## ""General Learner"": Missing Formal Definition and Misleading Name  The ""general learner"" concept used in the title and throughout is named in a somewhat misleading way, as the results here have to do more with *expressive power* than learning.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"It critiques the use of the term ""general learner"" as misleading."
"The motivation is to reduce the undesirable large variance of MLM objective, based on the hypothesis that randomly sampled masks in MLM would lead to undesirably large gradient variance, which as a result typically hurts the training efficiency with stochastic gradient optimization algorithms.",Does the review address Methodology?,TRUE,FALSE,It explains the motivation behind reducing large variance in MLM objective.
"Empirically, it demonstrates that several NLP tasks are “natural”.",Does the review address Data/Task?,TRUE,FALSE,It empirically demonstrates the naturalness of several NLP tasks.
"After reading other reviews and the authors’ responses to all of the reviewers, I recommend this paper by accepted—extensive results show that the CALM objectives offer more signal from data than current pretraining methods.",Does the review address Data/Task?,TRUE,FALSE,It recommends acceptance based on the effectiveness of CALM objectives.
"- Strengths: Useful modeling contribution, and potentially useful annotated data, for an important problem -- event extraction for the relationships between countries as expressed in news text.",Does the review address Methodology?,TRUE,FALSE,It highlights the usefulness of the modeling contribution and annotated data for event extraction.
"After thinking over the concepts in the paper more, I might lean more strongly toward rejection or toward acceptance (if the authors can address the issues I raise below).",Does the review address Evaluation?,TRUE,FALSE,It expresses indecision based on the need for addressing raised issues.
"- Table 5 the VAE(32) method performs the best overall in ""Wiki section"" although the TC (16) method has been highlighted as the best.",Does the review address Methodology?,TRUE,FALSE,It points out the discrepancy in performance reporting.
Are these rather the ppl resulting from training an LM on the full dataset?,Does the review address Data/Task?,TRUE,FALSE,It questions the results from training on the full dataset.
"There are numerous works and existing methods working on connecting LLM to perform various tasks, e.g., AutoGPT.",Does the review address Methodology?,TRUE,FALSE,It mentions existing works on connecting LLM to perform tasks.
"Other concerns along these lines: all of this paper's results are averaged over 3 runs while the other baselines are over 5 runs - an indication of variance would be useful to assess whether the differences are significant, especially since some of the margins quite small (23 on MC-LAVE-RL vs 22.8 for the next best on Ludicorp) added to the fact that hyperparameters are different for each game - does that imply that the authors tuned the hyperparameters for each game?",Does the review address Presentation?,TRUE,FALSE,It suggests considering the significance of results averaged over different runs.
#### Strength - The idea of perceiver IO is novel and solid -- a general architecture capable of handling general-purpose inputs and outputs across different tasks and modalities.,Does the review address Novelty?,TRUE,FALSE,It praises the novelty and solidity of the perceiver IO architecture.
Their thorough ablation experiments and other analysis yield some interesting findings the authors could emphasize more.,Does the review address Experiment?,TRUE,FALSE,It encourages emphasizing the interesting findings from ablation experiments.
There is limited contribution in terms of machine learning algorithms.,Does the review address Contribution?,TRUE,FALSE,It notes the limited contribution in machine learning algorithms.
"Similar idea also exists in [R3], which is missing from this paper.",Does the review address Related Work?,TRUE,FALSE,It points out missing related work that has a similar idea.
"-----General Discussion----- This paper proposes a practical model which seems working well on one dataset, but the main ideas are not very novel (see comments in Strengths).",Does the review address Data/Task?,TRUE,FALSE,It mentions the practical model and its performance on a dataset but notes the lack of novelty.
It also includes the recurrent memory extension from Transformer-XL from Dai et al.,Does the review address Related Work?,TRUE,FALSE,It mentions the inclusion of the recurrent memory extension from Transformer-XL.
"This is relevant because, by training end to end, that work effectively generates arbitrary amounts of training data through interaction the the HOL4 ITP system (intermediate theorems which are proven give some reward in that work).",Does the review address Theory?,TRUE,FALSE,It discusses the relevance of generating training data through interaction with HOL4 ITP system.
This is an interesting work on the investigation of learning effects with a mix of tasks.,Does the review address Data/Task?,TRUE,FALSE,It mentions the investigation of learning effects with mixed tasks.
"While I appreciate the environment introduced and believe it to be promising, the experiments presented fail to highlight the novelty of the environment.",Does the review address Experiment?,TRUE,FALSE,It critiques the failure to highlight the novelty of the environment in experiments.
The authors present a new approach called: neural-based binary reverse engineering framework (N-Bref).,Does the review address Methodology?,TRUE,FALSE,It introduces a new approach called neural-based binary reverse engineering framework (N-Bref).
"As a result, this paper has a great potential, and its results seem very promising.",Does the review address Result?,TRUE,FALSE,It notes the promising results and potential of the paper.
"There is no comparison with other public masking methods in Table 2, such as whole-word-masking, span masking, etc.",Does the review address Presentation?,TRUE,FALSE,It points out the lack of comparison with other public masking methods.
"Specifically, for Table 1, the inference time of each algorithm should be reported (retrieval time included).",Does the review address Presentation?,TRUE,FALSE,It emphasizes the importance of including inference time to enhance Table 1's informativeness and clarity.
Summary: + Appealing theoretical contributions + Empirical results are encouraging + The use of discriminator for reward shaping in addition to task success rate is interesting  - Writing and explanation can be improved.,Does the review address Result?,TRUE,FALSE,It highlights strong theoretical contributions and empirical results while noting areas for improving writing and explanation.
"If I understand correctly, the sub-linear results depend on particular settings of the memory length and compression rate.",Does the review address Methodology?,TRUE,FALSE,"It questions whether the sub-linear results hinge on specific configurations, hinting at a need for clarity on parameter dependencies."
"* Were the proposed architectural additions conceived with the HANS ""counterexamples"" in mind (i.e. is there a specific reason to think that these types of methods would avoid the ""superficial"" reasoning that these examples are supposed to reveal)?",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"It inquires about the motivation behind architectural choices, particularly their relevance to HANS counterexamples."
"The methods have been tested on two benchmarks focusing on the issue, SCAN and morphological analysis.",Does the review address Analysis?,TRUE,FALSE,"It acknowledges the testing of methods on SCAN and morphological analysis, indicating targeted evaluation."
"At the least, it would be worth proposing that the languages in this study can offer a test of rule-based vs. inference-based processes, and propose performing such comparisons when the data for the study languages is sufficiently mature.",Does the review address Data/Task?,TRUE,FALSE,It suggests using the study languages to compare rule-based and inference-based processes when data matures.
"For example, on VQA, more recent works (BEiT-3) achieve 84+, while the best reported result in the manuscript is ~76.",Does the review address Result?,TRUE,FALSE,It points out a performance gap in VQA results compared to recent works like BEiT-3.
*  The claims regarding the 10x better data efficiency are not well supported and I would suggest the authors to compare with transformer models on standard LM benchmarks and potentially to some downstream tasks such as MT to make a stronger case.,Does the review address Data/Task?,TRUE,FALSE,It critiques unsupported claims of data efficiency and recommends stronger comparisons with standard benchmarks.
"I do not believe that SoTA results are necessary to write a good paper, and indeed the obsession our field has with SoTA is unhealthy.",Does the review address Result?,TRUE,FALSE,"It reflects on the field's overemphasis on SoTA results, advocating for a broader evaluation perspective."
"Secondly, the design of the specific neural network cannot describe the theory behind proposed binding-unbinding mechanism properly.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It critiques the neural network's inability to adequately represent the binding-unbinding mechanism theory.
"3) The experimental results reported are validated on a single dataset, and no human evaluation and error analysis.",Does the review address Analysis?,TRUE,FALSE,It highlights a limitation in validating results without human evaluation or error analysis.
"That raises the question -- Gerrish and O'Connor both conduct evaluations with an external database of country relations developed in political science (""MID"", military interstate disputes).",Does the review address Evaluation?,TRUE,FALSE,It questions the omission of evaluations using an external political science database like MID.
"This paper makes comparison with techniques used in active learning (AL), domain shift detection (DS), and multi-domain sampling to combine data from multiple sources.",Does the review address Data/Task?,TRUE,FALSE,"It notes the comparative analysis with active learning, domain shift detection, and multi-domain sampling methods."
The simple combination of the audio model and LLM does not seem to be novel.,Does the review address Novelty?,TRUE,FALSE,It critiques the combination of the audio model and LLM as lacking novelty.
-  Consistent performance improvement over the baseline models on all of the three QA benchmarks.,Does the review address Methodology?,FALSE,TRUE,"This sentence does not mention methodology, it only discusses performance improvement."
"- The paper is well written: it gives an appropriate context, presents the main theoretical results, and verifies _some_ of the claims experimentally.",Does the review address Theory?,TRUE,FALSE,It praises the paper for presenting theoretical results and partially verifying claims experimentally.
"Additionally, the rationale for choosing (Ma et al., 2023) as a benchmark, along with the significance of the improvements observed, even if minute, should be clearly articulated.",Does the review address Significance?,TRUE,FALSE,It suggests better articulation of the rationale for benchmark selection and observed improvements.
Experimental evaluation shows competitive performance.,Does the review address Result?,TRUE,FALSE,It mentions competitive performance demonstrated in experimental evaluation.
"The authors analyze the in-context bias of the self-attention model, which could inspire some research works on designing training examples.",Does the review address Analysis?,TRUE,FALSE,It discusses the analysis of in-context bias and its potential to influence training example design.
"For example, if for the baseline model, we also only use one data sample and apply different masks, will there be improvement?",Does the review address Result?,TRUE,FALSE,It questions the potential improvement by applying different masks to a single data sample.
"This bound consists of two parts: - The first part measures how ""natural"" the task is, that is, how well can the task be solved using the next word distributions as features.",Does the review address Data/Task?,TRUE,FALSE,"It explains a bound's role in measuring the ""naturalness"" of tasks using next-word distributions."
"However, with most Indigenous languages, existing corpora are not large enough to produce accurate statistical models.""",Does the review address Methodology?,TRUE,FALSE,It identifies the challenge of small corpora leading to limitations in statistical model accuracy for Indigenous languages.
"3.2.2 presents the observation that choosing a set of queries from the dataset a-priori (agnostic to the image), does not result in either an optimal or interpretable query set.",Does the review address Data/Task?,TRUE,FALSE,It critiques the suboptimal and uninterpretable outcomes of agnostic query selection.
This paper provides a mathematical framework to understand this question.,Does the review address Methodology?,TRUE,FALSE,It highlights the contribution of a mathematical framework to address a specific research question.
What happens when the extra reward for using the language model is added to LaRL (that might be tough if you have to modify others code).,Does the review address Methodology?,TRUE,FALSE,It questions the effect of adding an extra reward for language model use to LaRL.
"In Sec2, you said ""yet is outperformed by the proposed fully-explored masking (see Table 2).",Does the review address Related Work?,TRUE,FALSE,It refers to a specific comparison with the proposed fully-explored masking method.
"The main issue of the paper is in the experiments and results reporting, it needs quite a bit of reworking.",Does the review address Experiment?,TRUE,FALSE,It suggests significant reworking of the experiment design and results reporting.
"Contribution: The authors contribute a new tokenization method, code, and a dataset.",Does the review address Methodology?,TRUE,FALSE,"It acknowledges the introduction of a new tokenization method, accompanying code, and a dataset."
# Summary  The authors propose to use corpora generated from _emergent communication_ as a fine-tuning signal for NLP tasks (language modeling and image captioning in particular).,Does the review address Methodology?,TRUE,FALSE,It mentions the innovative use of emergent communication corpora for NLP task fine-tuning.
"- Excellent clarity and presentation of ideas  ## Weaknesses - The experiments provided do not analyze the unique characteristics of the environment introduced; instead, the experiments are similar to the typical gamut for a discrete symbol-based referential game.",Does the review address Presentation?,TRUE,FALSE,It praises the clarity but critiques the lack of unique experimental analysis.
"I also wanted to mention that I appreciate the addition of the suggested related work, but I would still suggest that the authors consider looking into more detailed means of comparison in the future (especially to the Petroni work), since this seemed to be a concern in multiple reviews.",Does the review address Comparison?,TRUE,FALSE,It appreciates the inclusion of related work and suggests exploring more detailed comparisons in future studies.
Weaknesses * Something that stands out is the lack of discussion and comparison to related works that employ recurrent formulations of attention.,Does the review address Related Work?,TRUE,FALSE,It highlights the absence of sufficient discussion and comparison to related works on recurrent formulations of attention.
Other questions for the authors: (1) What is the loss in performance by fixing the word embeddings in the dependency parsing task?,Does the review address Data/Task?,TRUE,FALSE,It raises questions about the performance impact of fixing word embeddings in a specific task.
"So how would it be possible to train with poor annotations, while generalize much better?",Does the review address Methodology?,TRUE,FALSE,It questions the feasibility of achieving strong generalization when training with poor annotations.
"If you remove the RE component, does the NER performance suffer?",Does the review address Ablation?,TRUE,FALSE,It inquires about the effect of removing the RE component on NER performance.
"It also further demonstrates that the BERT model, once fully tuned, could achieve SOTA/competitive performance compared to the recent new models (e.g., XLNet).",Does the review address Result?,TRUE,FALSE,It showcases the competitive or SOTA performance of a fully-tuned BERT model compared to newer models.
"**The method is simple with limited contribution, while performance improvement is not significant.",Does the review address Methodology?,TRUE,FALSE,It critiques the method for being simplistic with minor contributions and insignificant performance improvements.
The theory is a bit complicated and not easy to follow.,Does the review address Theory?,TRUE,FALSE,It comments on the complexity and difficulty in understanding the theoretical framework.
Having additional ways to improve data efficiency by changing the model design is definitely of interest.,Does the review address Methodology?,TRUE,FALSE,It identifies potential interest in exploring model design changes to enhance data efficiency.
"**Baselines are too weak, leading to a misunderstanding of the effectiveness of the proposed method.",Does the review address Methodology?,TRUE,FALSE,It critiques the weak baselines that obscure a proper evaluation of the method's effectiveness.
"A discussion on how to add new tasks to the same framework might help, or a discussion on why the current framework is enough.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It suggests a discussion on adding new tasks or justifying the sufficiency of the current framework.
The authors introduce a BACKDOOR DATA PARADIGM that aptly fulfills the requirements for Uniqueness and Imperceptibility in watermark embedding.,Does the review address Presentation?,TRUE,FALSE,It mentions the BACKDOOR DATA PARADIGM meeting requirements for watermark embedding.
I highly recommend bringing this assumption earlier to avoid readers confusion.,Does the review address Presentation?,TRUE,FALSE,It suggests stating an assumption earlier to prevent reader confusion.
"The result will stand out to compare against Codex, the state-of-the-art program synthesis model.",Does the review address Comparison?,TRUE,FALSE,It highlights the comparison against Codex for better clarity.
Theoretical discussion proves that the gradients derived from the new masking schema have a smaller variance and can lead to more efficient self-supervised training.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It explains that the new masking schema gradients have a smaller variance for efficient training.
(Current experiments only include the results of models that are free from this issue.),Does the review address Experiment?,TRUE,FALSE,It notes that current experiments only show results from models without this issue.
"This isn't clear from the current structure if so, since it's stated as a corollary of Theorem 3.",Does the review address Theory?,TRUE,FALSE,It mentions the lack of clarity in the current structure related to Theorem 3.
lead to performance decrease for individual tasks.,Does the review address Result?,TRUE,FALSE,It notes a performance decrease for individual tasks.
"Moerover, there seem to be some errors about the correctness of the theory (See the first point below).",Does the review address Theory?,TRUE,FALSE,It points out errors in the correctness of the theory.
The source for generating the data is a big contribution to the theorem prover and machine learning community.,Does the review address Contribution?,TRUE,FALSE,It highlights the contribution of data generation to the theorem prover and machine learning community.
- Not accounting for the privacy loss incurred in tuning the hyperparameters is a problem.,Does the review address Methodology?,TRUE,FALSE,It mentions the issue of not accounting for privacy loss in hyperparameter tuning.
Would be great to state them upfront to avoid confusion.,Does the review address Methodology?,TRUE,FALSE,It suggests stating details upfront to avoid confusion.
"In section 3.3.3 ""THE MIX-UP OF MULTIPLE TYPES,"" the authors mention that ""it is possible to embed multiple Double-I watermarks in a model, which theoretically has the potential to enhance the robustness of our watermarking technique.""",Does the review address Methodology?,TRUE,FALSE,It discusses the potential to enhance watermarking robustness by embedding multiple Double-I watermarks.
The authors experiment their method on three datasets and get the state of the art results.,Does the review address Result?,TRUE,FALSE,It mentions achieving state-of-the-art results on three datasets.
"For a fair comparison, I think the baseline should add those methods as claimed in the introduction (Lee et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Beltagy et al., 2020, inter alia), (Kitaev et al., 2020; Wang et al., 2020; Roy et al., 2020, inter alia) and let us know how badly they performed under the short sequence.",Does the review address Comparison?,TRUE,FALSE,It suggests adding the mentioned methods for a fair comparison.
"There is no time complexity in the proposed method, which is very crucial if it needs to run for every inference.",Does the review address Methodology?,TRUE,FALSE,It points out the absence of time complexity in the proposed method.
"The paper demonstrates that naively using CLIP-score between (query-concept, image) does not work well out-of-the-box, and proposes learning a new light-weight network based on pseudo-labels.",Does the review address Contribution?,TRUE,FALSE,It highlights the contribution of proposing a new light-weight network based on pseudo-labels.
"I'm concerned whether these improvements will hold after optimizing BERT carefully like RoBERTa, or using more advanced backbone methods like ALBERT.",Does the review address Result?,TRUE,FALSE,It expresses concerns about the sustainability of improvements with optimized models.
"The experments are fairly convincing, although it is not entirely surprising that this approach works, and to repeat, the basic idea of extracting additional training data in this way is not entirely new.",Does the review address Data/Task?,TRUE,FALSE,"It mentions that while the experiments are convincing, the approach of extracting training data is not new."
Contrastive training (negative sampling) is one of the crucial contributions of this work.,Does the review address Contribution?,TRUE,FALSE,It mentions that contrastive training is a crucial contribution.
"Maybe I've missed this in the paper, if there is, please be more explicit about it because it affects the model quite drastically if for every sentence the largest phrase length is the sentence length.",Does the review address Significance?,TRUE,FALSE,It points out the significance of specifying the largest phrase length.
The first is based on integrating information from all layers of the encoder via a method called Squeeze and Excitation.,Does the review address Methodology?,TRUE,FALSE,It discusses integrating information from all encoder layers using Squeeze and Excitation.
i. e. representing a word by a set of many Gaussian distributions.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It explains the representation of a word using Gaussian distributions.
I would argue that this is not a fair comparison: did the authors intentionally choose weak baselines?,Does the review address Comparison?,TRUE,FALSE,It questions the fairness of the comparison by choosing weak baselines.
"First of all, the dense interaction between vision and language tokens has been heavily studied prior to the so-called multimodal LLM era.",Does the review address Related Work?,TRUE,FALSE,It mentions prior studies on dense interaction between vision and language tokens.
"### Major issue 1 The paper ""**theoretically**"" analyzes the hyper-parameter selection process in Section 3.1 and provides experimental validation in Section 5.1.",Does the review address Experiment?,TRUE,FALSE,It mentions the theoretical analysis and experimental validation of the hyper-parameter selection process.
The graph visualization shown does not seem to illustrate much.,Does the review address Presentation?,TRUE,FALSE,It critiques the lack of informative content in the graph visualization.
The paper's presentation could be improved in several ways:     1.,Does the review address Presentation?,TRUE,FALSE,It suggests several improvements for the paper's presentation.
**Strengths**:  The paper is well-written and easy to follow.,Does the review address Presentation?,TRUE,FALSE,It praises the writing and ease of understanding of the paper.
"The paper shows the reasonable claim that it is necessary to gradually train the model from close-ended datasets to open-ended ones because if the open-ended dataset is trained first, the model is heavily dependent on language capability so it is hard to train the audio representation.",Does the review address Data/Task?,TRUE,FALSE,It explains the necessity of training the model from close-ended to open-ended datasets.
It seems to be making every previously known augmentation approach better.,Does the review address Methodology?,TRUE,FALSE,It highlights that the proposed approach improves existing augmentation methods.
"Conventionally the ITM loss is a binary prediction task, while the particular one used in this work is more often referred as contrastive learning loss.",Does the review address Data/Task?,TRUE,FALSE,"It discusses the type of ITM loss used in the work, which is similar to contrastive learning loss."
"A discussion on how to add new tasks to the same framework might help, or a discussion on why the current framework is enough.",Does the review address Data/Task?,TRUE,FALSE,It suggests discussing how to add new tasks to the framework or justifying the sufficiency of the current framework.
"Only the video-level annotation is available for training, and the goal is retrieving the video segment described by the sentence.",Does the review address Data/Task?,TRUE,FALSE,It mentions that the task is to retrieve video segments based on video-level annotations.
"It might be interesting to see some examples, especially when the annotation budget is 18, of the kinds of instances that get selected depending on the task.",Does the review address Data/Task?,TRUE,FALSE,It suggests showing examples of selected instances with a specific annotation budget.
"Paper is mostly clearly written, and easy to read.",Does the review address Presentation?,TRUE,FALSE,It comments on the clarity and readability of the paper.
Questions for the authors: - How the parameter study was conducted?,Does the review address Presentation?,TRUE,FALSE,It asks for details on how the parameter study was conducted.
Just trying to say that automatic evaluation of dialog systems is a hard problem.,Does the review address Evaluation?,TRUE,FALSE,It acknowledges the difficulty of automatically evaluating dialog systems.
"They show that, especially in small-data regimes, pre-training on an emergent language can yield significant performance boosts in both tasks.",Does the review address Result?,TRUE,FALSE,It mentions significant performance boosts from pre-training on emergent language in small-data regimes.
"Further, how does the network perform when a longer context is obtained *maintaining the same number of parameters* as a network with less temporal scales?",Does the review address Result?,TRUE,FALSE,It questions the performance of the network when using longer contexts with the same number of parameters.
"For example, the model hyper-parameters are quite different for different tasks.",Does the review address Methodology?,TRUE,FALSE,It notes that model hyper-parameters vary between different tasks.
The experimentation is correct and the qualitative analysis made in table 1 shows results as expected from the approach.,Does the review address Experiment?,TRUE,FALSE,It mentions that the experimentation and qualitative analysis results are as expected.
Could you explain more precisely what exactly is new?,Does the review address Presentation?,TRUE,FALSE,It requests a more precise explanation of the novel aspects.
"# Typographic comments  * p 1, ""the input out of detractors"" --> ""the input out of distractors""   * p 2: ""transferable benefits for downstream natural language tasks"" the single hyphens surrounding the subsequent list should be em dashes (three hyphens in TeX)  * p 3: ""uses another GRU layer to decode the message m into a hidden vector hl"" I would use ""encode"" instead of ""decode"" here, since text-->representation is usually what an encoder does  * p 3: ""The most straightforward metric is the accuracy of playing the referential game p(guess = Ii).""",Does the review address Presentation?,TRUE,FALSE,It provides typographic corrections for the paper.
- Any comments / results on the model's sensitivity to parser errors?,Does the review address Methodology?,TRUE,FALSE,It asks for comments or results regarding the model's sensitivity to parser errors.
"Also, some additional experiments need to be added in order to better justify its claims.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It suggests adding more experiments to better justify the claims.
"Thus, it seems to me that you are essentially applying past results to answer a specific question you have (which is still a valuable contribution).",Does the review address Result?,TRUE,FALSE,It acknowledges the valuable contribution of applying past results to answer a specific question.
More theoretical proofs or appropriate literature citations are needed to validate this assertion.,Does the review address Related Work?,TRUE,FALSE,It suggests adding more theoretical proofs or literature citations for validation.
- The ablation experiments and optimized prompt analysis are insightful.,Does the review address Ablation?,TRUE,FALSE,It highlights the insights gained from ablation experiments and prompt analysis.
"References Yining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu, and Chengqing Zong.",Does the review address Related Work?,TRUE,FALSE,It cites relevant references for the related work section.
Authors could have plugged their embedding strategy in LayoutLM to understand the impact of that particular component.,Does the review address Significance?,FALSE,TRUE,This sentence does not mention significance but suggests an embedding strategy.
"Rules may be ""outdated"" because they are inefficient for certain languages with reams of available data and scads of phenomena that don't fit.",Does the review address Data/Task?,TRUE,FALSE,It mentions that rules may be inefficient for certain languages with abundant data and phenomena.
The theory can be more practically useful if it can be extended to quantify the intractability level so the resulting embeddings' error can be bounded or compared.,Does the review address Methodology?,TRUE,FALSE,It suggests extending the theory to quantify intractability and bound embedding errors.
"- Table 5 on the right for the training curriculum, it would be great to also include the language instruction following rate.",Does the review address Methodology?,TRUE,FALSE,It suggests including the language instruction following rate in the training curriculum table.
"Hence, the model relies on whether the parser accurately discovers the crucial information.",Does the review address Methodology?,TRUE,FALSE,It mentions that the model's performance depends on the accuracy of the parser.
The empirical part of the paper shows improved performance of adding similar sentences to the context of LM training.,Does the review address Result?,TRUE,FALSE,It highlights the improved performance from adding similar sentences to the LM training context.
It would be beneficial to further explore whether sparse attention is indeed a problem for DNA sequence representation.,Does the review address Methodology?,TRUE,FALSE,It suggests further exploring the impact of sparse attention on DNA sequence representation.
Does the baseline system (groundhog) contains the attention mechanism?,Does the review address Methodology?,TRUE,FALSE,It asks if the baseline system includes an attention mechanism.
Weaknesses: - Somewhat weaker results on some CommonGen metrics are disappointing.,Does the review address Result?,TRUE,FALSE,It mentions the disappointment in weaker results on some CommonGen metrics.
"You hypothesize that more templates doesn't help because ""models at such scale do not easily overfit to a finetuning single task"" - but my intuition is for an opposite explanation -- that the models at such scale easily memorize a small number of templates!",Does the review address Presentation?,TRUE,FALSE,It challenges the hypothesis on template overfitting and suggests an alternative explanation.
"-----Post-rebuttal----- The authors did not address my main concern, which is whether the baselines (e.g. TreeRNN) are used to compute substructure embeddings independent of the sentence embedding and the joint tagger.",Does the review address Comparison?,TRUE,FALSE,It mentions the main concern about the independence of substructure embeddings from sentence embeddings and joint taggers.
But the diagrams in the appendix for the policy the MC!Q*BERT agent learns as well as the original paper for that agent show otherwise?,Does the review address Methodology?,TRUE,FALSE,It questions the validity of the diagrams in the appendix for the MC!Q*BERT agent's policy.
I also read the original gSCAN paper but they didn't use this term at all.,Does the review address Comparison?,TRUE,FALSE,It mentions that the term was not used in the original gSCAN paper.
"**Related work** The authors clearly describe the related prior works from both the perspectives of program synthesis, large language models, and benchmarks for program synthesis.",Does the review address Methodology?,TRUE,FALSE,It praises the clear description of related prior works in program synthesis and large language models.
* Ablation: the model vs corpus transfer comparison seems unfair to me.,Does the review address Methodology?,TRUE,FALSE,It critiques the fairness of the model versus corpus transfer comparison.
The proposed method is reasonable and moderately novel.,Does the review address Novelty?,TRUE,FALSE,It mentions that the proposed method is reasonable and has moderate novelty.
"The authors say in the introduction that the approach (Andreas, 2020) is task specific which seems not correct.",Does the review address Data/Task?,TRUE,FALSE,It points out an incorrect claim about task specificity in the introduction.
Their findings on learning complex tasks contribute to the understanding of large language model learning and provide valuable insights for future related work on efficient training.,Does the review address Data/Task?,TRUE,FALSE,It highlights the contribution of findings on complex task learning for understanding and efficient training of large language models.
The experiment results in Tables 1 & 2 cannot plausibly prove OTTER is more effective than CLIP.,Does the review address Methodology?,TRUE,FALSE,It questions the validity of experimental results in proving OTTER's effectiveness over CLIP.
"* The paper repeatedly emphasizes ""training"" in FP8 (e.g., in the title, in the abstract, etc.",Does the review address Methodology?,TRUE,FALSE,"It mentions the repeated emphasis on ""training"" in FP8 throughout the paper."
The motivation behind the study is somewhat unclear.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It points out the lack of clarity in the motivation behind the study.
"The motivation is to reduce the undesirable large variance of MLM objective, based on the hypothesis that randomly sampled masks in MLM would lead to undesirably large gradient variance, which as a result typically hurts the training efficiency with stochastic gradient optimization algorithms.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It explains the motivation to reduce large variance in MLM objectives to improve training efficiency.
"However, GTR is a single vector retrieval model, so there is no unified standard to show that the effect of the proposed model in this paper is better than the previous model.",Does the review address Methodology?,FALSE,TRUE,The sentence critiques the lack of a unified standard for comparing the proposed model with previous models.
"The proposed models -- which seem to be an application of various tree-structured recursive neural network models -- demonstrate a nice performance increase compared to a fairly convincing, broad set of baselines (if we are able to trust them; see below).",Does the review address Result?,TRUE,FALSE,It mentions a performance increase of the proposed models compared to baselines.
"3) The experimental results reported are validated on a single dataset, and no human evaluation and error analysis.",Does the review address Data/Task?,TRUE,FALSE,It points out that the experimental results are only validated on a single dataset without human evaluation.
"The layers are described in a textual fashion, barely any math (and extended in the pseudo-code).",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It critiques the textual description of layers with little mathematical explanation.
"However, there are many other tasks that involve sentence pairs.",Does the review address Data/Task?,TRUE,FALSE,It mentions the existence of many other tasks involving sentence pairs.
The problem is a terrific one and the application of the recursive models seems like a contribution to this problem.,Does the review address Contribution?,TRUE,FALSE,It highlights the contribution of applying recursive models to the problem.
I also read the original gSCAN paper but they didn't use this term at all.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It notes that the term was not used in the original gSCAN paper.
"Indeed, it is not clear how one can reformulate e.g. linguistic tasks (like POS-tagging or dependency parsing) as a next word prediction task.",Does the review address Data/Task?,TRUE,FALSE,It questions the reformulation of linguistic tasks as next word prediction tasks.
"Granted, the final effect of MTL depends on task similarities, but that's probably the same for the proposed approach.",Does the review address Data/Task?,TRUE,FALSE,"It mentions that the effect of MTL depends on task similarities, similar to the proposed approach."
"It is good to know that it works for 2D-coordinates for the task at hand, though it seems to be more a marginal improvement on existing work rather than a standalone contribution.",Does the review address Contribution?,TRUE,FALSE,It comments that the improvement for 2D-coordinates is marginal rather than a standalone contribution.
"Similar numbers are true for the rest of the tasks: 60.90 vs. 87 for OBQA, 71.01 vs. 90 for PIQA, and 63.20 vs.  89.70 for aNLI.",Does the review address Data/Task?,TRUE,FALSE,It provides performance numbers for various tasks.
"The key ideas are: (i) training longer with bigger batches over more data, (ii) removing NSP, (iii) training over long sequences, and (iv) dynamically changing the masking pattern.",Does the review address Methodology?,TRUE,FALSE,It outlines the key ideas of the methodology.
"Compared to the related work, the novel part of this work is: (i) a new retrieval way, which is not quite clear and convincing to me.",Does the review address Methodology?,TRUE,FALSE,It critiques the novelty and clarity of the new retrieval method.
"### Cons and aspects to improve  My main concern is that the overall contribution is seems to be limited.In fact, the original paper of the Transformer approach, already proposed such kind of embedding.",Does the review address Result?,FALSE,TRUE,It expresses concern about the limited contribution and notes that the Transformer approach already proposed similar embedding.
"Moreover, I have some comments on the model and experiments.",Does the review address Experiment?,TRUE,FALSE,It indicates that there are comments on the model and experiments.
"It provided me with a much more thoughtful explanation for why language model pre-training improves downstream task performance, beyond simply “it helps learn good general representations of language using large amounts of unlabeled text data” (my previous reasoning).",Does the review address Result?,TRUE,FALSE,It mentions that the paper provides a thoughtful explanation for the improvement in downstream task performance from language model pre-training.
"* Authors perform experimentally sound experiments, following closely LayoutLM.",Does the review address Experiment?,TRUE,FALSE,It praises the soundness of the experiments and their adherence to LayoutLM.
"Theoretical Advantages and Theorem Justification:  While you mention that quantum features are theoretically more expressive, the paper falls short of explaining the underlying intuition and proof for this assertion.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It critiques the lack of explanation and proof for the theoretical advantages of quantum features.
- General Discussion: The authors perform relation extraction as reading comprehension.,Does the review address Methodology?,TRUE,FALSE,It mentions that the authors perform relation extraction as reading comprehension.
And are the previous work using the same training set?,Does the review address Methodology?,TRUE,FALSE,It questions whether the previous work used the same training set.
"2) As the authors claimed in Introduction, ‘plenty of training data is available’.",Does the review address Data/Task?,TRUE,FALSE,It references the claim in the introduction about the availability of plenty of training data.
More discussions on comparing with symbolic logic reasoner model LReasoner are needed.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,It suggests adding more discussions comparing the work with LReasoner.
"Also, can you add a column where a dense linear model over p_f(s) is used?",Does the review address Presentation?,TRUE,FALSE,It asks for the addition of a column for a dense linear model over p_f(s).
The notations in equation 2 and 3 are also inconsistent with equation 5.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"It points out inconsistencies in the notations of equations 2, 3, and 5."
"Altogether, I think this paper makes an interesting contribution to the question of: How can we get the most pretraining signal from unstructured data using off-the-shelf tools?",Does the review address Contribution?,TRUE,FALSE,It acknowledges the contribution of the paper in maximizing pretraining signal from unstructured data using off-the-shelf tools.
"Since the selective annotation is based entirely on similarities derived from sentence embeddings, there is nothing explicit ensuring that the label distribution over the selected subset is not skewed.",Does the review address Data/Task?,TRUE,FALSE,It notes that the selective annotation based on sentence embeddings does not ensure an unbiased label distribution.
"The only two tasks examined are sentence similarity tasks (which seem a bit more like a sanity check), and NaturalQuestions.",Does the review address Data/Task?,TRUE,FALSE,It mentions that only sentence similarity tasks and NaturalQuestions were examined.
"- I think it makes sense in the comparison with CaP to compare using an oracle object detector, but one of the main novelties of the work is replacing the perceptual modules of CaP with foundation models.",Does the review address Comparison?,TRUE,FALSE,It discusses the comparison with CaP and the novelty of replacing perceptual modules with foundation models.
"The authors provide OTTER using hard labels (InfoNCE) as a baseline, but ZSL methods are sensitive to hyper-parameters and training sets.",Does the review address Methodology?,TRUE,FALSE,It mentions the methodology of using OTTER with hard labels and the sensitivity of ZSL methods.
"The billions that have been pumped into languages like English have in fact resulted in technologies that can be applied at much lower cost to languages like Kanyen’kéha, but there are still costs.",Does the review address Significance?,TRUE,FALSE,It discusses the significant impact and costs of applying technologies to languages like Kanyen’kéha.
"Result 1: Under the above assumption over the downstream task, this paper provides a bound on the empirical loss of the downstream prediction task.",Does the review address Presentation?,FALSE,TRUE,It does not discuss how the result is presented but focuses on the empirical loss bound.
- The ablation experiments and optimized prompt analysis are insightful.,Does the review address Experiment?,TRUE,FALSE,It praises the insights gained from ablation experiments and prompt analysis.
"And it's the averaged test scores that pRNN performs better - Please also make it clear whether the ""Test Avg.""",Does the review address Result?,TRUE,FALSE,It highlights the need to clarify the result related to averaged test scores.
"- Additionally, why isn't the the GRU version of pRNNv reported in the FBIS evaluation in Table 3?",Does the review address Evaluation?,TRUE,FALSE,It questions the lack of GRU version reporting in the FBIS evaluation.
"The result will stand out to compare against Codex, the state-of-the-art program synthesis model.",Does the review address Experiment?,TRUE,FALSE,"It suggests the result's prominence in comparison, but does not refer to an experiment."
"Given the issued pointed out in 1 and 2, I am not sure if the results are really sound as the authors claimed.",Does the review address Methodology?,TRUE,FALSE,It expresses doubt about the soundness of the results based on methodology issues.
"- Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train.",Does the review address Result?,TRUE,FALSE,It discusses the positive results of using BERT representations for NER+RE and faster training.
The introduction of two different positional encoding methods in different sections is confusing (even if one only serves the purpose of understanding the role of positional encodings).,Does the review address Presentation?,TRUE,FALSE,It highlights the confusion caused by presenting two different positional encoding methods.
"Unfortunately, as the paper overall does not seem to contain significant novelty, neither in methodology nor in results, I cannot recommend acceptance at this point.",Does the review address Novelty?,TRUE,FALSE,It points out the lack of significant novelty in the paper.
"* Section 6.3 - Please change ""The results from Table 2 and Table 1"" to say ""Table 1 and Table 2"".",Does the review address Presentation?,TRUE,FALSE,It suggests a correction in the presentation order of tables.
"Given the way p(guess=Ii) is used above, I think this should be more like E[argmax(p(guess=Ii)) = i].",Does the review address Presentation?,TRUE,FALSE,It suggests a change in the presentation of the probability function usage.
"Once those are answered, a significant test had better be done since the improvement seems small.",Does the review address Result?,TRUE,FALSE,It calls for a significant test to verify the improvement.
### Positive aspects   * Positional encoder based on sinusoidal function seems to be effective.,Does the review address Methodology?,TRUE,FALSE,It highlights the effectiveness of the sinusoidal function-based positional encoder.
"Without comparison with SOTA's performance, I will try my best to reject this paper.",Does the review address Comparison?,TRUE,FALSE,It mentions the need for comparison with SOTA's performance.
"Furthermore, the authors are neglecting parameter efficient fine-tuning baselines, for instance like [1].",Does the review address Comparison?,TRUE,FALSE,It points out the neglect of comparing with parameter-efficient fine-tuning baselines.
"Weakness: - paper title is misleading, not directly related to LM - no citation and description for the baseline method T5+KB (Table 4) - As shown in Table 7, the proposed method is very sensitive to many factors.",Does the review address Methodology?,TRUE,FALSE,It mentions issues related to methodology including the sensitivity of the proposed method.
** The method is quite intuitive and can be regarded as an in-context example selection method (followed by annotations).,Does the review address Methodology?,TRUE,FALSE,It describes the method as intuitive and an in-context example selection method.
The hypothesis are clearly stated and the experiments are well designed.,Does the review address Experiment?,TRUE,FALSE,It praises the clarity of hypotheses and the design of experiments.
"In Sec2, you said ""yet is outperformed by the proposed fully-explored masking (see Table 2).",Does the review address Result?,TRUE,FALSE,It mentions that the proposed masking method outperformed others.
"* The current analysis doesn’t apply directly to BERT, which is trained to predict masked words in a sentence, instead of the next word.",Does the review address Methodology?,TRUE,FALSE,It points out that the current analysis does not apply directly to BERT.
(4) How exactly is the interaction module processed?,Does the review address Methodology?,TRUE,FALSE,It asks for clarification on the processing of the interaction module.
There are also neural architectures that particularly target to ensure some symbolic famous properties such as [3].,Does the review address Methodology?,TRUE,FALSE,It mentions neural architectures targeting symbolic properties.
Reducing it to 80% seems to be a sweet point with the best balance between performance and efficiency.,Does the review address Methodology?,TRUE,FALSE,It discusses reducing a parameter to 80% for optimal performance and efficiency.
Therefore the novelty of these components of the paper is negligible.,Does the review address Novelty?,TRUE,FALSE,It points out the negligible novelty of the paper's components.
"The architecture does not look entirely novel, but I kind of like the simple and practical approach compared to prior work.",Does the review address Novelty?,TRUE,FALSE,It mentions that the architecture is not entirely novel but appreciates the practical approach.
I believe that this contribution isn't enough for me to recommend acceptance.,Does the review address Contribution?,TRUE,FALSE,It mentions that the contribution is insufficient for recommending acceptance.
"This paper shows that the log posterior have the same lower bound when the inference model p(y|x) is defined by different methods, i.e., the arithmetic mean of predictions with different dropout masks, the geometric mean, and a power-mean family as an interpolation between these two cases.",Does the review address Presentation?,TRUE,FALSE,it shows that the log posterior have have the same lowe bond
I think it will be helpful for authors to have a complete graph of the computational model used instead of only figure 1 concept graph.,Does the review address Methodology?,TRUE,FALSE,It suggests having a complete graph of the computational model for better understanding.
- There is some missing prior work in creating knowledge graphs from pre-trained language models.,Does the review address Methodology?,FALSE,TRUE,It mentions the lack of prior work without detailing the methodology.
"Also it seems that these are not fully annotated, and the ‘forward type inference functionality from TypeScript’ is required to obtain labels.",Does the review address Data/Task?,TRUE,FALSE,It discusses the annotation and label acquisition process.
So this contribution seems not practically useful according to the empirical result.,Does the review address Contribution?,TRUE,FALSE,It critiques the practical usefulness based on empirical results.
"Alongside with qualitative analysis, some quantitative analysis would be good to show what the model learns.",Does the review address Analysis?,TRUE,FALSE,It suggests adding quantitative analysis to show model learning outcomes.
It is a bit hard to identify the interestingness or novelty in the approach.,Does the review address Novelty?,TRUE,FALSE,It mentions difficulty in identifying the novelty of the approach.
2.The authors may add subjective evaluations to the ablation experiments to better demonstrate that the LoRA fine-tuning strategy mitigates catastrophic forgetting issues.,Does the review address Evaluation?,TRUE,FALSE,It suggests adding subjective evaluations to ablation experiments.
The proposed model was evaluated on two publicly-available dataset and achieved reasonable results.,Does the review address Result?,TRUE,FALSE,It mentions the evaluation and results of the proposed model on public datasets.
"They show improved total performance in MultiWoz dataset compared to recent, relevant baselines.",Does the review address Result?,TRUE,FALSE,It highlights improved performance on the MultiWoz dataset compared to baselines.
The key contribution of the paper is the approach to overcome the limitation of annotating  query sets and labels.,Does the review address Methodology?,TRUE,FALSE,It discusses the key contribution related to overcoming annotation limitations.
"For fine-tuning, the authors run their model for 2.2 epochs, while their baseline model runs for 3 epochs, roughly 30% more which accounts for much of the reduction observed in Table 2.",Does the review address Methodology?,TRUE,FALSE,It mentions the difference in fine-tuning epochs between models.
"Particularly because the reported accuracy margins are so slim, either of these variables could modify the empirical conclusions.",Does the review address Result?,TRUE,FALSE,It notes that slim accuracy margins could affect empirical conclusions.
It is hard to know here which one of these actually made the adversarial setup useful.,Does the review address Presentation?,TRUE,FALSE,It questions the presentation of useful aspects in the adversarial setup.
"DP is not ""incorporated"" in a model or multimodality (as the authors mention in different ways a few times throughout the paper), DP is a property of a randomised algorithm (in this context, the training algorithm that produces the distribution of models, not the model).",Does the review address Methodology?,TRUE,FALSE,It clarifies the correct context and property of DP.
The method can select appropriate batch sizes by assessing the working memory requirements per token during benchmarking.,Does the review address Methodology?,TRUE,FALSE,It discusses the method's capability to select batch sizes based on memory requirements.
"Given the nature of the problem statement (with multiple tasks, inputs and outputs), the authors have done a good job in explaining each of them properly.",Does the review address Data/Task?,TRUE,FALSE,"It praises the thorough explanation of tasks, inputs, and outputs."
May I know many questions are in each data split shown in Table 5?,Does the review address Data/Task?,TRUE,FALSE,It inquires about the number of questions in each data split.
"A significant part of the contribution was in the analysis of the results, obtained by this learning-based parameter sharing approach, which was quite informative and revealed some interesting insights about where and when a language-specific computation is required.",Does the review address Contribution?,TRUE,FALSE,It highlights the informative contribution in result analysis and parameter sharing insights.
Maybe add a comment saying Step 2 is the human-in-the-loop step of the algorithm?,Does the review address Methodology?,TRUE,FALSE,It suggests adding a comment about the human-in-the-loop step.
They also empirically show that it is easier and faster for the learner if the signals from easily inferred labels to learn target are provided.,Does the review address Result?,TRUE,FALSE,It mentions the empirical findings on the ease and speed of learning with inferred labels.
The authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks.,Does the review address Evaluation?,TRUE,FALSE,It points out the limited evaluation on specific tasks.
"In a nutshell, aren't you showing that $$\text{downstream error}=\mathcal{O}\left(\sqrt{\text{pre-training error}\cdot\frac{\text{downstream error}}{\text{pre-training error}}}\right)\qquad ?$$  **Issues** - Why don't you verify the main claim---$\epsilon$-optimality in pre-training propagates as $\mathcal{O}(\sqrt{\epsilon})$-optimality on downstream---empirically?For this, you may want to vary the language modeling performance (e.g. by pruning the language model) and then verifying that the downstream loss increase is indeed $\mathcal{O}(\sqrt{\text{pre-training loss increase}})$.",Does the review address Presentation?,TRUE,FALSE,It questions the empirical verification of the main claim and the clarity of the mathematical presentation.
It is highly innovative and holds significant importance for the development of general artificial intelligence.,Does the review address Significance?,TRUE,FALSE,It highlights the innovation and importance for general AI development.
"You are clearly not trying to infer any loop invariants, and it would help clarify that upfront.",Does the review address Presentation?,TRUE,FALSE,It suggests clarifying upfront that loop invariants are not being inferred.
"Also in Table 2, they claim that they outperform other approaches.",Does the review address Comparison?,TRUE,FALSE,It mentions the claim of outperforming other approaches in Table 2.
"The latter is typically used in two different ways in the transformer architecture, each resulting in a different computation for RF  is confusing as the RFA is now redefined.",Does the review address Presentation?,TRUE,FALSE,It notes the confusion in the use of RFA in the transformer architecture.
*  The evaluation focuses on comparing with an empirical law learned on a different experimental configuration and there is a concern about how comparable are the results to the ones obtained in this study and the validity of the conclusions.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,It discusses the evaluation's focus on comparing with an empirical law and the validity of conclusions.
"Given the barrier of reproducing the reported results and also the limited insights delivered by this work, I am sharing a huge concern regarding the current trend of building multimodal LLMs manifested by this work or other related ones.",Does the review address Methodology?,TRUE,FALSE,It expresses concern about the reproducibility and insights of the work in the context of multimodal LLMs.
The authors take time to implement and evaluate several prominent baselines.,Does the review address Comparison?,TRUE,FALSE,It mentions the time taken to implement and evaluate prominent baselines.
"This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.",Does the review address Presentation?,TRUE,FALSE,It points out the misleading use of a term that typically refers to external knowledge.
"For checking the generalization of the method and better comparison w/ InDIGO (though InDIGO also conducted on MSCOCO, Django and the current comparison is sufficiently fair), I would like to increase my rating if seeing more experiments on large scale machine translation benchmarks as those in InDIGO.",Does the review address Experiment?,TRUE,FALSE,It suggests additional experiments on large-scale machine translation benchmarks for better comparison.
I'd like some confirmation that larger batch size won't get much improvement for the baseline model.,Does the review address Methodology?,TRUE,FALSE,It requests confirmation on the impact of larger batch size on the baseline model.
It is unclear which sequences were used for training the Transformer models and how similar they are to test sequences.,Does the review address Methodology?,TRUE,FALSE,It questions the training sequences used for Transformer models.
"Also, the citation to Universal Dependencies is completely broken.",Does the review address Related Work?,TRUE,FALSE,It mentions a broken citation to Universal Dependencies.
"in CVPR, 2020  [2] J. Lu, V. Goswami, M. Rohrbach, D. Parikh, S. Lee, 12-in-1: Multi-Task Vision and Language Representation Learning.",Does the review address Related Work?,TRUE,FALSE,It cites a related work for context.
"Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015 _ Also, the inclusion of the result from those approaches in tables 3 and 4 could be interesting.",Does the review address Presentation?,TRUE,FALSE,It suggests including results from cited approaches in tables 3 and 4.
"Since the data is generated by T-5, couldn't we generate as much as we want?",Does the review address Data/Task?,TRUE,FALSE,It questions the potential to generate unlimited data with T-5.
"Since it's a efficiency paper, I think it should be complete.",Does the review address Methodology?,TRUE,FALSE,It expresses the need for completeness in an efficiency paper.
The paper mentions that the entity extraction was done following Yasunaga et al.,Does the review address Methodology?,TRUE,FALSE,It references the entity extraction methodology following Yasunaga et al.
"- Additionally, in my opinion, the authors are misrepresenting prior work when saying in line 163 that the ""MTL approach has not yet been successful in NLP"".",Does the review address Related Work?,TRUE,FALSE,It mentions the misrepresentation of prior work in the MTL approach claim.
Why authors consider questions answering and sentiment analysis as the applications?,Does the review address Methodology?,TRUE,FALSE,It questions the consideration of specific applications for the methodology.
Such gaps make the main contribution questionable and make it as a pure empirical paper on its value.,Does the review address Contribution?,TRUE,FALSE,"It critiques the contribution, suggesting that it is mainly empirical in nature."
"In the 6.2.3 visualization of clusters, it would be very useful to have a visualization of clusters from some baselines on other ways of learning.",Does the review address Comparison?,TRUE,TRUE,"The sentence suggests an addition to the visualization, not a comparison with other methods."
As the current system captures the semantics through RNN based models.,Does the review address Methodology?,TRUE,TRUE,The sentence describes the current system but doesn't address the methodology of the study.
"- I know GPT3 access is hard to get, but I wish experiments with k=8 prompts could be compared against  Post rebuttal: I think another pass for clarity over the paper would be good for the final version, but otherwise I'm happy with the paper updates and I'm happy to see the ablation numbers aren't too sensitive to token count.",Does the review address Presentation?,FALSE,TRUE,"Focuses on the experimental setup, not the paper's clarity or organization."
"- The experiments contain two setups, one is offline response evaluation via MultiWOZ, and another is interactive simulation via ConvLab.",Does the review address Evaluation?,TRUE,FALSE,Describes the evaluation setups used in the experiments.
"And further discuss the correlation between the classification performance and the instruction following rate, if there is any insights that can be drawn.",Does the review address Result?,TRUE,FALSE,"Suggests an additional analysis, but no results are presented."
Cons: - wMAN model the relation for all possible pairs of the word and the video frame.,Does the review address Methodology?,TRUE,FALSE,"Discusses the model's approach to capturing relationships, relevant to methodology."
"If these method can also achieve very good results, then I feel that the novelty and effectiveness of DeFo may be challenged.",Does the review address Novelty?,TRUE,FALSE,Questions the originality and effectiveness of the proposed method.
"In experimental details, the slightly better performance in Table 2, can it be attributed to finer generation powered by the RNN generator?",Does the review address Experiment?,TRUE,FALSE,Discusses details of an experiment in the study.
This paper proposes a reranking architecture with a LogicForm-to-NaturalLanguage preprocessing step for semantic parsing.,Does the review address Methodology?,TRUE,FALSE,Describes the proposed methodology in the paper.
The motivation behind Transformer-QL is to increase the context length processed beyond what other methods can.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"Provides motivation for the research, clarifying the rationale behind Transformer-QL."
"By using the first inference solution, the performance on PTB and Wikitext2 LM can be improved by 2-3 on perplexity but is still slightly worse than the SOTA achieved by the mixture of softmaxes.",Does the review address Result?,TRUE,FALSE,Discusses performance results compared to the state-of-the-art (SOTA).
Nit: I would have tried to move the (datasets per cluster/templates per dataset) ablation to the main body as well and shortened Section 3  - The 4.2 (scaling laws) ablation is perhaps the most interesting of all.,Does the review address Ablation?,TRUE,FALSE,Suggests changes to the paper's ablation study and its placement in the structure.
"It would be useful if following the suggestions of [2,3] the authors could present results in settings with low data regimes and with significant distribution shift with respect to the pre-training set, which represents a more realistic application setting.",Does the review address Presentation?,FALSE,TRUE,Suggests additional results but doesn't focus on the paper's presentation.
A uniform framework for resampling Different recombinations perform more or less favorably across different datasets.,Does the review address Data/Task?,TRUE,FALSE,"Discusses how recombinations perform across datasets, related to the data/task aspect."
Casting the optimization of discrete latent variables as a one-step MDP is interesting 3.,Does the review address Methodology?,TRUE,FALSE,Describes an interesting methodology used in the study.
"Overall, I thought the paper was easy to read and understand.",Does the review address Presentation?,TRUE,FALSE,Comments on the readability and clarity of the paper.
"In terms of strenghs - The paper has a very throughout analysis of different models and positional encodings - It proposes several contributions, including a new probing task and several positional encoding methods.",Does the review address Contribution?,TRUE,FALSE,Highlights the contributions made by the paper.
Time analysis on language modeling is not presented.,Does the review address Methodology?,TRUE,FALSE,"Notes the absence of a time analysis, which could have been part of the methodology."
"For a more comprehensive analysis, it would be instructive to see how the approach compares with a broader spectrum of state-of-the-art methods.",Does the review address Comparison?,TRUE,FALSE,Suggests a comparison with a broader range of methods for a more comprehensive analysis.
"The proposed system outperforms or achieves on-par  performance against previous SOTA methods, i.e., AudioGen and AudioLDM, in both objective and subjective metrics.",Does the review address Comparison?,TRUE,FALSE,This sentence compares the proposed system with previous state-of-the-art methods.
"However, this does not hold theoretically due to the extra bias on expectation.",Does the review address Analysis?,TRUE,FALSE,This sentence discusses theoretical analysis and extra bias.
- The evaluation on PTB (table 2) isn't a fair one since the model was trained on a larger corpus (FBIS) and then tested on PTB.,Does the review address Evaluation?,TRUE,FALSE,"This sentence critiques the evaluation method, discussing fairness and the training corpus used."
"Strengths: The paper presents an interesting idea for Multiple-Choice Question-Answering (using the answer symbol instead of the answer itself), motivates the idea well and does a thorough analysis over multiple datasets, and LLMs to analyze its performance in different settings (including few-shot settings).",Does the review address Analysis?,TRUE,FALSE,This sentence mentions thorough analysis over multiple datasets and settings.
"As the paper points out, the success rate alone is not enough.",Does the review address Evaluation?,TRUE,FALSE,"This sentence discusses the adequacy of success rate, an evaluation metric."
"- In figure 6A, why was performance not increasing for untuned models w.r.t model size?",Does the review address Result?,TRUE,FALSE,This sentence queries the performance results presented in Figure 6A.
"Since the architecture (ignoring the compression) is similar to multi-scale approaches, it would be good to compare against empirically.",Does the review address Comparison?,TRUE,FALSE,This sentence suggests empirical comparison with multi-scale approaches.
Their thorough ablation experiments and other analysis yield some interesting findings the authors could emphasize more.,Does the review address Ablation?,TRUE,FALSE,This sentence mentions ablation experiments and their findings.
"For example, a Figure to define what are $n_s, n_c, n_m$ would help to understand the paper with a nice visual benefit.",Does the review address Presentation?,TRUE,FALSE,This sentence suggests adding a figure to improve the clarity and understanding of the paper.
"Given the ending of the paper, I interpreted that the set of dialog acts (i.e. options) are learnt automatically but I could not find this to be communicated explicitly.",Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodology used to learn the set of dialog acts.
"With the strengths being said, I hope to also point out that the paper's application of adversarial training is one attempt in many possibilities, and in many cases it is not clear where the improvements come from.",Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the application of adversarial training, a methodological aspect."
"The in-depth experimental analysis of the BERT pretraining process in this paper answers many open questions (e.g., the usefulness of NSP objective) and also provide some guidance in how to effectively tweak the performance of pretrained model (e.g., large batch size).",Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodology related to BERT pretraining analysis.
"Since they both use $m$ nodes, does it mean the supergraph also operates on each sub fact node?",Does the review address Methodology?,TRUE,FALSE,This sentence queries an aspect of the methodology involving supergraph and sub fact nodes.
The authors also introduce the recent developed Gumbel-matching techniques to derive the close-form of the posterior distribution.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodology involving Gumbel-matching techniques.
"After reading it several times, I get the idea that the authors view a model as a general learner if it can simulate a universal circuit for all poly-size circuits.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence explains the authors' interpretation of a model as a general learner.
"For the theoretical analysis, it was not clear to me what is the contribution of the current analysis compared to the Levine 2020 paper.",Does the review address Contribution?,TRUE,FALSE,This sentence questions the contribution of the current analysis compared to previous work.
Experiments on both continual pre-training and general pre-training from scratch show the effectiveness of the proposed method.,Does the review address Experiment?,TRUE,FALSE,This sentence discusses the effectiveness of the experiments conducted.
"Setting and Main Result:  This paper focuses on classification tasks, and the bulk of the work goes into how to model the next word distributions as features or representations.",Does the review address Data/Task?,TRUE,FALSE,This sentence describes the focus on classification tasks and the data used.
Questions:  * How can we be sure that the specific power law derived from a different experimental configuration in Kaplan et al. (2020) corresponds to the empirical law that can be derived for transformers in this particular evaluation setting?,Does the review address Experiment?,TRUE,FALSE,This sentence questions the validity of the experiment in relation to previous work.
"While I think it's nice to analyze the connection between RM and DM, the math provided in the paper is simple, and the main contribution is just to add a baseline to the algorithm of Khalifa et al.",Does the review address Analysis?,TRUE,FALSE,This sentence critiques the analysis and contribution presented in the paper.
- The empirical validation is thoughtful and relatively thorough.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence praises the empirical validation, indicating thoughtful justification and validation."
"However, there is no discussion on the latency in the proposed method and experiments.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence notes the lack of discussion on latency.
The optimality of deterministic inference does not hold empirically due to class imbalance or discrepancy between training and test sets.,Does the review address Analysis?,TRUE,FALSE,This sentence discusses empirical analysis concerning deterministic inference.
"### Weaknesses ###  * The paper falls short on the framing of the invariant inference problem, and on the technical details of what does it mean to infer a meaningful local invariant.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence critiques the framing and technical details related to invariant inference.
"Regarding that CLIP does not release the 400M dataset (mentioned in this paper by the authors), the authors may not be able to train OTTER on the 400M dataset.",Does the review address Data/Task?,TRUE,FALSE,This sentence discusses the dataset limitations affecting the training process.
---------------------------------------------- Original: This paper is aimed at using pre-trained language models to create open-ended knowledge graphs.,Does the review address Methodology?,TRUE,FALSE,This sentence describes the methodology of using pre-trained language models.
"There are various weaknesses of the MID data, but the evaluation approach needs to be discussed or justified.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence critiques the evaluation approach and calls for justification.
"other state-of-the-arts/benchmark systems on only ""present"" type of key phrases.",Does the review address Comparison?,TRUE,FALSE,This sentence suggests comparing with other state-of-the-art systems.
"Important research can make great strides regarding languages that are usually neglected, if and only if funding is available for people to take the time to do the work.",Does the review address Significance?,TRUE,FALSE,This sentence discusses the potential significance and impact of the research.
The notations in equation 2 and 3 are also inconsistent with equation 5.,Does the review address Methodology?,TRUE,FALSE,This sentence points out inconsistencies in the methodology.
"In Sec 5.5 Gradient Variance, $\pi$ is missing in ""$G_\theta(x)=A(x)\nabla_\theta \log_{\theta}(x)$""",Does the review address Presentation?,TRUE,FALSE,This sentence identifies a presentation error in the notation.
"Overall, a very strong paper, well structured and clear.",Does the review address Presentation?,TRUE,FALSE,This sentence praises the overall structure and clarity of the paper.
"I have no experience with these kinds of NLU models, so I can't say with confidence whether the architectural additions proposed are well-motivated, but to me it feels like there is not a strong justification for adding these particular features to the BERT architecture, and the results do not clearly demonstrate their utility except in the ""lexical_overlap"" case.",Does the review address Result?,TRUE,FALSE,This sentence critiques the results and justification for architectural additions.
This paper tackles a very important and under-studied problem: reducing the cost of training NLP models.,Does the review address Presentation?,TRUE,FALSE,This sentence highlights the importance of the research topic.
"**Update (after the author's response)**: During the rebuttal, the authors clarified my major concern, as well as provided additional experiments that verify the main claim of the paper.",Does the review address Experiment?,TRUE,FALSE,This sentence discusses additional experiments conducted to verify the main claim.
"Intuitively, this parameter arises from translating the ""natural"" task assumption, which only guarantees transfer on average to the downstream task.",Does the review address Methodology?,TRUE,FALSE,This sentence explains the methodology involving task assumptions.
"For instance, the greater instability of larger Transformers to active learning bodes poorly for practitioners leveraging ever increasing model sizes for low-resource datasets.",Does the review address Data/Task?,TRUE,FALSE,This sentence discusses the instability of larger transformers in relation to data/task.
"Second, the authors neither 1) evaluate their model on another dataset or 2) evaluate any previously published models on their dataset.",Does the review address Evaluation?,TRUE,FALSE,This sentence critiques the evaluation process for not using multiple datasets or previously published models.
"I understand that a single model is helpful for multiple UI tasks, but I wonder if this approach is scalable beyond the 5 tasks and 5 modalities mentioned.",Does the review address Data/Task?,TRUE,FALSE,This sentence questions the scalability of the model across different tasks and modalities.
"Compared to the related work, the novel part of this work is: (i) a new retrieval way, which is not quite clear and convincing to me.",Does the review address Significance?,TRUE,FALSE,This sentence questions the significance and novelty of the new retrieval method.
"The in-depth experimental analysis of the BERT pretraining process in this paper answers many open questions (e.g., the usefulness of NSP objective) and also provide some guidance in how to effectively tweak the performance of pretrained model (e.g., large batch size).",Does the review address Analysis?,TRUE,FALSE,This sentence discusses in-depth experimental analysis and its findings.
"As pointed by one public comment, the ablation study should show how much improvement is from BERT vectors.",Does the review address Methodology?,TRUE,FALSE,This sentence suggests an ablation study to show improvements from BERT vectors.
"Prior work has explored ""learning-to-share""  strategies for parameter sharing in multi-task learning (see Ruder et al., AAAI 2018), and using gating/masking to control computational paths in a differentiable way (see Fan et al., ICLR 2019, Sukhbaatar et al., ACL 2019); it is clear that the focus is NMT but it should be worth mentioning/discussing such studies to better situate the work and to help the reader assess the actual contributions.",Does the review address Data/Task?,TRUE,FALSE,This sentence discusses prior work related to parameter sharing and task relevance.
"A lot of attention/space is dedicated to locality and symmetry properties of positional encodings, which from my understanding, isn’t very novel and has been explored in previous work.",Does the review address Related Work?,TRUE,FALSE,This sentence critiques the novelty of the positional encodings by comparing them to previous work.
The paper pointed out that ELECTRA framework [1] explored the idea of using REINFORCE [2] as the the way of adding adversarial training signals to the model but observed degenerated results.,Does the review address Comparison?,TRUE,FALSE,This sentence compares the ELECTRA framework to other adversarial training methods.
It is unclear which sequences were used for training the Transformer models and how similar they are to test sequences.,Does the review address Data/Task?,TRUE,FALSE,This sentence questions the data used for training and its similarity to test sequences.
"* Theorem 2 is a restatement of past work, showing that transformers lie in logspace-uniform TC^0 * Theorem 3 assumes TC^0 \neq P / poly, and then derives that transformers cannot simulate any poly-time circuit.",Does the review address Methodology?,TRUE,FALSE,This sentence discusses the theoretical methodology related to transformers.
"Please report the total training and total inference time, and make a comparison with standard Transformer model.",Does the review address Comparison?,TRUE,FALSE,This sentence requests a comparison of training and inference times with a standard Transformer model.
Can the theory guide how to develop new models to learn program representations?,Does the review address Methodology?,TRUE,FALSE,"This sentence asks if the theory can guide the development of new models, relating to methodology."
- Using “concept” to stand in for verbs and nouns is somewhat confusing.,Does the review address Presentation?,TRUE,FALSE,This sentence points out a presentation issue involving terminology.
The authors may add more details about the previous work in the related work section.,Does the review address Related Work?,TRUE,FALSE,This sentence suggests adding more details to the related work section.
"In Section 2, the basics of binding-unbinding are introduced and many mathematical properties are required to make the binding-unbinding work.",Does the review address Theory?,TRUE,FALSE,This sentence discusses the theoretical foundations of binding-unbinding.
This might depend on different sub-sequences and the various functions of different layers when modeling cross-attention.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological aspects of cross-attention modeling.
"- Since a major part of the model contains shared parameters, was there a need for new set of shared parameters along with the language-specific parameters.",Does the review address Methodology?,TRUE,FALSE,This sentence questions the necessity of new shared parameters in the model.
"This paper shows that the log posterior have the same lower bound when the inference model p(y|x) is defined by different methods, i.e., the arithmetic mean of predictions with different dropout masks, the geometric mean, and a power-mean family as an interpolation between these two cases.",Does the review address Evaluation?,TRUE,FALSE,Yes
"The authors switch between using BERT_base and RoBERTa_base without being very clear about when and why, e.g., in Table 2 they have both models, but only in different sections.",Does the review address Methodology?,TRUE,FALSE,This sentence points out a lack of clarity in the methodology regarding model usage.
There's also some missing related work in extracting knowledge from pretrained models that should probably be discussed.,Does the review address Related Work?,TRUE,FALSE,This sentence suggests discussing additional related work on extracting knowledge from pretrained models.
"Although I understand the experiment setup, missing reference to more recent VL works prevent readers from getting a good research landscape in the multimodal pre-training.",Does the review address Methodology?,TRUE,FALSE,This sentence critiques the experiment setup and suggests including more recent references.
"Hence, it will be better to additionally include the results of other standard RL algorithms for better justifying this claim.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence suggests including results from other RL algorithms to justify the claim, indicating validation."
"In the experiments, the authors make comparisons with traditional methods, and show the effectiveness of their model.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the comparison of the proposed model with traditional methods, relating to methodology."
"Strengths: The paper presents an interesting idea for Multiple-Choice Question-Answering (using the answer symbol instead of the answer itself), motivates the idea well and does a thorough analysis over multiple datasets, and LLMs to analyze its performance in different settings (including few-shot settings).",Does the review address Result?,FALSE,TRUE,This sentence does not directly mention specific results.
5.2 visualizations: this seems pretty ad-hoc without much justification for the choices.,Does the review address Presentation?,TRUE,FALSE,"The sentence critiques the lack of justification for the visualizations, which relates to the clarity and presentation of the work."
"In Sec2, you said ""yet is outperformed by the proposed fully-explored masking (see Table 2).",Does the review address Presentation?,TRUE,FALSE,"This sentence refers to Table 2, a part of the paper's presentation."
It is then not clear how each fact block (grey squares in Figure 4) functions as a whole.,Does the review address Presentation?,TRUE,FALSE,"This sentence mentions Figure 4, which is related to the paper's presentation."
How about the benefits compared with the proposed one?,Does the review address Methodology?,TRUE,FALSE,"The sentence is asking for a comparison of methods, which relates to methodology."
"You *tell* us that Fon is ""a language with special tokenization needs"" and that ""standard tokenization methods do not alwaysadequately deal with the grammatical, diacritical, and tonal properties of some African language"", and you cite the relevant papers.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,The sentence provides a detailed explanation about Fon and its tokenization needs.
How do the settings used in the experiments compare to those used for the analysis?,Does the review address Experiment?,TRUE,FALSE,The sentence is asking about the settings used in experiments.
"- Followed by previous question, in the qualitative results, it seems the boundary parts of the predicted video segments are less accurate.",Does the review address Result?,TRUE,FALSE,The sentence discusses the accuracy of results in the qualitative analysis.
"- unconstrained, multi-concept:     This needs a direct comparison to a traditional discrete-channel referential game.",Does the review address Experiment?,TRUE,FALSE,The sentence suggests a direct comparison in experiments.
Introducing the layer and networks in a simple way would help clarify the implementation and other notation.,Does the review address Presentation?,TRUE,FALSE,The sentence suggests improving the clarity of presentation of layers and networks.
The experimental results on RoBERTa highlight the applicability and importance of this data augmentation approach on the downstream task of text classification (GLUE).,Does the review address Significance?,TRUE,FALSE,The sentence highlights the importance and significance of the experimental results.
"First, much of the improvement (I think) comes from reducing the number of epochs and/or the number of steps.",Does the review address Methodology?,TRUE,FALSE,"The sentence discusses factors contributing to improvement, which relate to the methodology."
In general the writing does not make the mechanisms by which proof artifacts may be extracted from Lean clear enough.,Does the review address Presentation?,TRUE,FALSE,"The sentence critiques the clarity of the writing, which is part of the presentation."
- `Table 2`: Are these values computed over a single or multiple runs (in which case include stddev/confidence intervals).,Does the review address Presentation?,TRUE,FALSE,"The sentence asks about the computation values in Table 2, which relates to the presentation of results."
"Does the definition of ""event word""s here come from any particular previous work that motivates it?",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The sentence inquires about the motivation and justification behind the definition of ""event words""."
"- If there is a limit set on the phrase length of pRNN, then it makes the system more tractable.",Does the review address Presentation?,TRUE,FALSE,"The sentence discusses a specific aspect of the system's design, which relates to presentation."
The proposed beam enumeration significantly outperforms REINVENT (the strongest baseline in the existing benchmark).,Does the review address Result?,TRUE,FALSE,"The sentence mentions the outperforming of a method, which relates to the result."
-  Consistent performance improvement over the baseline models on all of the three QA benchmarks.,Does the review address Data/Task?,TRUE,FALSE,"The sentence discusses the performance improvement on benchmarks, which relates to data/task."
The paper contributes LEANSTEP dataset and the Learning environment.,Does the review address Data/Task?,TRUE,FALSE,"The sentence mentions the dataset and learning environment, which relate to data/task."
This method should be included in the experiments in order to justify the proposed RL approach is necessary.,Does the review address Experiment?,TRUE,FALSE,The sentence suggests including a method in experiments.
"It would be to show more experiment results for some settings, for example, performance on Sum task when training with a mixture distribution, or more Sum task samples and fewer Parity task samples ( p range from 0.5 to 1.",Does the review address Result?,TRUE,FALSE,The sentence discusses the need to show more experimental results.
This logical progression effectively addresses the challenges initially posed.,Does the review address Methodology?,TRUE,FALSE,The sentence discusses the logical progression of the methodology.
The model architecture should be better justified.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,The sentence calls for better justification of the model architecture.
This paper describes four methods of obtaining multilingual word embeddings and a modified QVEC metric for evaluating the efficacy of these embeddings.,Does the review address Presentation?,TRUE,FALSE,"The sentence describes the methods and metric, which relates to presentation."
"The authors comment that the model architecture is designed to remain stable for a growing set of tasks, but this seems to be under the assumption that the input and output modalities would remain constant.",Does the review address Methodology?,TRUE,FALSE,"The sentence critiques the model architecture's design under certain assumptions, relating to methodology."
Experiments on LEGO and code interpretation task are done.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,The sentence mentions experiments and describes the tasks.
Possibly a dataset like common crawl or enwiki8 would be more appropriate for language modelling experiments.,Does the review address Methodology?,TRUE,FALSE,"The sentence suggests a different dataset for experiments, which relates to methodology."
"- P2, Sec 2: ""where $p^{\star}_{\cdot | s}$ is used as a vector on the left and distribution on the right"".",Does the review address Presentation?,FALSE,TRUE,The sentence doesn’t mention presentation
"Is there a perplexity/efficiency tradeoff, and can you characterize that experimentally?",Does the review address Experiment?,TRUE,FALSE,The sentence asks about the tradeoff characterization in experiments.
Is it possible to run an ablation study using the combination of REINFORCE and mixture-of-signals to verify whether Gumble-Softmax relaxation is the reason for it to work?,Does the review address Ablation?,TRUE,FALSE,The sentence suggests running an ablation study.
"It's important to discuss why these improvements are non-trivial and how they advance the field, considering the rapidly evolving landscape of both quantum computing and graph analysis.",Does the review address Result?,TRUE,FALSE,"The sentence discusses the importance of the improvements, which relates to results."
"Additionally, showing this for negations and / or examples which GreaseLM gets correct but QA-GNN does not (and vice-versa) can shed some light on what the model improves on (and what are the limitations).",Does the review address Methodology?,TRUE,FALSE,"The sentence discusses the comparison between different models, which relates to methodology."
- The methods and results are presented in an understandable manner.,Does the review address Methodology?,TRUE,FALSE,The sentence mentions the presentation of methods and results.
"LeetCode problems tend to be fairly simple, self-contained, and, to my knowledge, are coding problems that are meant to help train new programmers or prepare software developers for coding interviews, amongst other things.",Does the review address Data/Task?,TRUE,FALSE,"The sentence describes the nature of LeetCode problems, which relates to data/task."
* Ablation: the model vs corpus transfer comparison seems unfair to me.,Does the review address Ablation?,TRUE,FALSE,The sentence mentions an ablation aspect regarding the comparison.
"* ""Moreover, it should be noted that BROS achieves higher f1 score than 79.27 of LayoutLM using visual features"".",Does the review address Comparison?,TRUE,FALSE,"The sentence compares the f1 scores of two methods, which relates to comparison."
"I compared their numbers explicitly to Liu et al. (2019), and RoBERTa_base outperforms their approach on nearly all tasks (and on average).",Does the review address Comparison?,TRUE,FALSE,"The sentence compares the performance of approaches, which relates to comparison."
"This paper finds that the next word distributions of a subset of ""prompt"" words contain discriminative signals and are good features.",Does the review address Methodology?,TRUE,FALSE,"The sentence discusses findings related to word distributions, which relates to methodology."
Strengths * Improving the data efficiency in language models is an important problem that so far studies have shown that can be achieved by scaling the size of the model.,Does the review address Data/Task?,TRUE,FALSE,"The sentence discusses data efficiency in language models, which relates to data/task."
The authors used RNN based generative models (discussed as RNN and Copy RNN) for keyphrase prediction and copy mechanism in RNN to predict the already occurred phrases.,Does the review address Methodology?,TRUE,FALSE,This sentence describes the methodology involving RNN based generative models and copy mechanism.
Theoretical analysis is provided to demonstrate the effectiveness of the proposed method under a greedy search algorithm.,Does the review address Theory?,TRUE,FALSE,This sentence discusses the theoretical analysis demonstrating the method's effectiveness.
"- Easy but probably not great thing to try:  held-out tasks with wrong/useless templates  A final thought:  It's not obvious that using as many training examples per dataset as possible is optimal, given that the model could overfit to dataset-specific spurious correlations.",Does the review address Data/Task?,TRUE,FALSE,This sentence discusses the data/task aspect involving held-out tasks and training examples.
"Given that one of the primary goals of this paper was to create embeddings that perform well under the word translation metric (intra-language), it is disappointing that the method that performs best (by far) is the invariance approach.",Does the review address Methodology?,TRUE,FALSE,This sentence critiques the methodology used for creating embeddings and the invariance approach.
The framework of Variational Information Pursuit is quite similar to the Concept Bottleneck Models.,Does the review address Methodology?,TRUE,FALSE,This sentence compares the methodology of Variational Information Pursuit to Concept Bottleneck Models.
"I think it would improve the paper if you could focus on a certain kind of invariants, and show that these invariants can in fact generalize across programs.",Does the review address Result?,TRUE,FALSE,"This sentence suggests showing the generalization of certain invariants, relating to the results."
But there should be no reason that it is restricted to be so.,Does the review address Data/Task?,TRUE,FALSE,This sentence discusses the restriction related to data/task.
The proposed model is then experimentally verified on three logic-driven datasets which demonstrates some performance gain.,Does the review address Experiment?,TRUE,FALSE,This sentence describes the experimental verification and performance gain of the proposed model.
"Below are my major concerns:  If the major motivation is to reduce gradient variance, can we just use larger mini-batch size?",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence discusses the motivation behind the research, indicating validation."
* Assume T-LLMs are general learners by contradiction.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological assumption about T-LLMs being general learners.
"However, it is unclear how well they perform to the CASP state-of-the art (see also Rives et al, 2020).",Does the review address Related Work?,TRUE,FALSE,"This sentence questions the performance compared to the state-of-the-art, relating to related work."
- Weaknesses: The comparison against similar approaches could be extended.,Does the review address Comparison?,TRUE,FALSE,This sentence suggests extending the comparison with similar approaches.
How effective is the method to capture farther long-term dependencies compared to previous methods?,Does the review address Comparison?,TRUE,FALSE,This sentence questions the method's effectiveness in capturing long-term dependencies compared to previous methods.
"For example, what if the authors don’t use a LogicForm-to-NaturalLanguage conversion?",Does the review address Methodology?,TRUE,FALSE,This sentence questions the methodological choice of using LogicForm-to-NaturalLanguage conversion.
### Overall  Authors used BERT alongside to a 2D-position embedding based on a sinusoidal function and a graph-based decoder to improve performance on document information extraction tasks.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes the methodology involving BERT, 2D-position embedding, and graph-based decoder."
"you explained this in page 6, in Task Description.",Does the review address Presentation?,TRUE,FALSE,This sentence points out a specific location in the paper related to presentation.
"Strengths:  - The paper is generally well-written, with excellent motivation and empirical setup/analysis  - The overall strategy of differentiable prompt optimized to maintain fluency is reasonable and novel.",Does the review address Analysis?,TRUE,FALSE,"This sentence highlights the strengths in motivation, empirical setup, and analysis."
"## (Minor) Imprecise Claim about Poly(n) Size  In Theorem 1, the authors claim: > We consider log-precision, constant-depth, and polynomial-size Transformers: for Transformers whose input is of length n, the values at all neurons are represented with O(log n) bits, the depth is constant, and the number of neurons is O(poly (n)).",Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological claim related to polynomial-size Transformers.
"* Authors perform experimentally sound experiments, following closely LayoutLM.",Does the review address Comparison?,TRUE,FALSE,This sentence mentions the experimental comparison with LayoutLM.
"This paper proposes to model the generation order as latent variables for sequence generation tasks, by optimizing the ELBO involving a proposed process of Variational Order Inference (VOI).",Does the review address Presentation?,TRUE,FALSE,This sentence describes the methodological proposal involving latent variables and ELBO optimization.
- Figure 1 is helpful in comprehending the effect of different loss functions vs robustness.,Does the review address Result?,TRUE,FALSE,This sentence comments on the helpfulness of Figure 1 in understanding the results related to loss functions.
The experimental results on RoBERTa highlight the applicability and importance of this data augmentation approach on the downstream task of text classification (GLUE).,Does the review address Experiment?,TRUE,FALSE,This sentence discusses the experimental results related to data augmentation on text classification tasks.
"Detailed comments:  - P2, Sec 1.1: ""analyze the efficiency language model features"" -> analyze the efficiency of language model features  - P2, Sec 2: you started introducing these notations without explaining what they mean.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence provides detailed comments on the clarity and explanation of notations.
Weaknesses: - I felt like the empirical validation could have been stronger.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence critiques the strength of the empirical validation, indicating validation."
Solid experiments demonstrating the proposed method outperforms other existing approaches.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodology involving solid experiments.
The authors experiment their method on three datasets and get the state of the art results.,Does the review address Experiment?,TRUE,FALSE,This sentence describes the experimental results on three datasets achieving state-of-the-art results.
"The latter aims at avoiding catastrophic forgetting, while the former avoid having to share all (or no) parameters.",Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodology related to avoiding catastrophic forgetting and parameter sharing.
There is limited contribution in terms of machine learning algorithms.,Does the review address Methodology?,TRUE,FALSE,This sentence critiques the limited contribution related to machine learning algorithms.
The source for generating the data is a big contribution to the theorem prover and machine learning community.,Does the review address Theory?,TRUE,FALSE,This sentence discusses the theoretical contribution related to data generation.
"However, the baseline may be much weaker than the current SOTA solution.",Does the review address Comparison?,TRUE,FALSE,This sentence critiques the strength of the baseline compared to the state-of-the-art solution.
Both settings show the better performance of the proposed method.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological results related to performance in different settings.
"However, this does not hold theoretically due to the extra bias on expectation.",Does the review address Theory?,TRUE,FALSE,This sentence discusses the theoretical aspect related to extra bias on expectation.
"Overall the paper presentation is okay, although the clarity could be improved.",Does the review address Presentation?,TRUE,FALSE,This sentence critiques the overall presentation and clarity of the paper.
"You can then argue informally for why you think this definition makes sense, akin to, e.g., the Church-Turing thesis.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence suggests providing an informal argument for the definition, indicating an explanation."
It should be stated more clearly in the text what this means.,Does the review address Presentation?,TRUE,FALSE,This sentence suggests improving the clarity of the presentation.
"**Experiment setup** In Table 3, the paper only compares the proposed method against GPT-Neo and GPT-J, which is not sufficient.",Does the review address Experiment?,TRUE,FALSE,This sentence critiques the experimental setup for not providing sufficient comparisons.
"- I did not learn much from the evaluation; monolingual BERT achieves the highest accuracy, which by itself is not very surprising nor insightful.",Does the review address Evaluation?,TRUE,FALSE,This sentence critiques the evaluation results as not being surprising or insightful.
"Weakness While the ablation study and visualization analysis are done, the key evaluations are missing.",Does the review address Evaluation?,TRUE,FALSE,This sentence critiques the evaluation process for missing key evaluations.
"Even though the results don’t show that the proposed loss function and proposed “conditional mean features” give improvements over baselines, the empirical results show that the basic assumptions and definitions in the theoretical analysis are relatively realistic.",Does the review address Result?,TRUE,FALSE,This sentence discusses the results related to the proposed loss function and conditional mean features.
"It'll be good to have some ablation study of the combined effect of using only one data sample in a mini-batch, and the full-explored masking.",Does the review address Ablation?,TRUE,FALSE,This sentence suggests conducting an ablation study to analyze the combined effect of certain techniques.
The authors investigate how to use multi-task training on top of a pretrained model for a variety of unrelated tasks from the GLUE benchmark.,Does the review address Methodology?,TRUE,FALSE,This sentence describes the methodology involving multi-task training on a pretrained model.
"It is clear that Shaw et al. (2019) didn't experiment on OVERNIGHT dataset, but setting up the baseline on a dataset should not be classified as ``our method’’.",Does the review address Related Work?,TRUE,FALSE,This sentence critiques the related work by Shaw et al. and their experimental setup.
The model is tested in a long range language modeling task.,Does the review address Data/Task?,TRUE,FALSE,This sentence describes the data/task aspect involving long range language modeling.
"Hence, it will be better to additionally include the results of other standard RL algorithms for better justifying this claim.",Does the review address Methodology?,TRUE,FALSE,"This sentence suggests including results from other RL algorithms, indicating a methodological improvement."
My hunch is LTU would be far better than Pengi in open ended tasks although Pengi might be better on Close-ended tasks.,Does the review address Significance?,TRUE,FALSE,This sentence discusses the significance and performance of LTU and Pengi in different tasks.
I suggest to put the model figure more close to the methodology section and the qualitative results on page 8.,Does the review address Result?,TRUE,FALSE,This sentence suggests organizing the paper to improve presentation of results.
5.1 aggregations: this seems fine though fairly ad-hoc.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence evaluates the ad-hoc nature of the aggregations, indicating a validation aspect."
Weaknesses:  - The notation/description of section 4 is not immediately intuitive.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence critiques the clarity and intuitiveness of the notation and description in section 4.
- Too much detail in related work: I think the related work section is too long and just a list of papers rather than helping to place the work in the research area.,Does the review address Related Work?,TRUE,FALSE,This sentence critiques the length and usefulness of the related work section.
"In terms of the experiments, there are only dev results reported on GLUE (Table 2).",Does the review address Experiment?,TRUE,FALSE,This sentence comments on the experimental results reported on the GLUE benchmark.
"In the experiments, the authors make comparisons with traditional methods, and show the effectiveness of their model.",Does the review address Comparison?,TRUE,FALSE,"This sentence discusses the comparison of the proposed model with traditional methods, relating to experiments."
(2) Is table 1 an average over the 17 embeddings described in section 5.1?,Does the review address Presentation?,TRUE,FALSE,This sentence questions the presentation of data in Table 1.
"**Weakness/Suggestions/Questions**  * Although The paper is well written overall, I think section 3.2 Proof Artifact Training can be improved by adding an example explaining the Lean terminology proof term, proof type, tactic, tactic state, etc.",Does the review address Presentation?,TRUE,FALSE,This sentence suggests improving the presentation of section 3.2 by adding examples.
"However, evaluating dialogue policy is also important to justify the learned policy is suitable.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,This sentence discusses the justification and evaluation of dialogue policy.
"This is immediate from Theorem 2, and this kind of separation appears to have been the implicit the point of Merrill and Sabharwal 2023  * This statement seems overly strong and minimizes prior work: ""This work takes the first step towards rigorously answering the fundamental question by considering a theoretical boundary of model capacity to be general learner""      * e.g., ""Saturated Transformers are Constant-Depth Threshold Circuits"" shows that transformers lie in TC^0 under a saturation condition     * e.g., ""The Parallelism Tradeoff: Limitations of Log-Precision Transformers"" shows that log-precision transformers lie in log-space-uniform TC^0",Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological implications of Theorem 2 and prior work.
"- The proposed architecture is tested on massive experiments including language understanding tasks, optical flow, video audio class autoencoding, image classification, and starcraft II and achieves superior performance.",Does the review address Experiment?,TRUE,FALSE,This sentence describes the extensive experimental testing of the proposed architecture.
Massively multilingual neural machine translation.,Does the review address Related Work?,TRUE,FALSE,This sentence references related work in massively multilingual neural machine translation.
I suggest to put the model figure more close to the methodology section and the qualitative results on page 8.,Does the review address Presentation?,TRUE,FALSE,This sentence suggests rearranging the presentation of the model figure and qualitative results.
The main weakness of the paper is that it is mainly based on further tuning the existing BERT model and lacks novel contribution in model architecture.,Does the review address Contribution?,TRUE,FALSE,This sentence critiques the lack of novel contribution in the model architecture.
- Experiments are well-designed: many baselines are implemented to compare proposed method with traditional lifelong learning methods.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the well-designed experiments comparing the proposed method with baselines.
"- §2: ""study that implement"" -> ""study that implements"" - §3: ""adjectives has"" -> ""adjectives have"" - §4: ""analyzing sub-component"" -> ""analyzing sub-components"" - §5: ""Syllables features also generally performs"" -> ""Syllable features also generally perform""",Does the review address Presentation?,TRUE,FALSE,This sentence points out grammatical errors and suggests corrections to improve presentation.
Missing period at the end of the first paragraph in the related work section.,Does the review address Presentation?,TRUE,FALSE,This sentence points out a minor presentation error involving a missing period.
This paper proposed to use Graph Neural Networks (GNN) to do type inference for dynamically typed languages.,Does the review address Methodology?,TRUE,FALSE,This sentence describes the methodology involving Graph Neural Networks for type inference.
"Weaknesses, suggested improvements and requested clarifications  1.",Does the review address Presentation?,TRUE,FALSE,"This sentence introduces weaknesses and suggestions for improvement, indicating a presentation aspect."
"However, Section 3.1 cannot demonstrate effective theoretical information.",Does the review address Theory?,TRUE,FALSE,This sentence critiques the theoretical content presented in Section 3.1.
"If the authors can adequately address the following issues around presentation and the interpretation of Theorem 5, I would be satisfied raising my score, as I value the research question raised by the paper and believe the conclusions are valid.",Does the review address Presentation?,TRUE,FALSE,This sentence discusses issues around the presentation and interpretation of Theorem 5.
"* Strength     * This paper proposes an interesting idea and interpretation to connect RM and DM paradigm     * This paper proposes a variance reduction method for DPG which demonstrates its improvement on performance, stability and sample efficiency * Weakness     * From my understanding, the baseline mostly comes from the observation in 3.3, which has limited technical novelty.",Does the review address Result?,TRUE,FALSE,This sentence discusses the results related to the connection between RM and DM paradigms.
"Other designs include beam size, whether or not to use a pretrained model, etc.",Does the review address Methodology?,TRUE,FALSE,This sentence discusses various design choices related to methodology.
"Hence, the model relies on whether the parser accurately discovers the crucial information.",Does the review address Result?,TRUE,FALSE,"This sentence discusses the dependency of the model's performance on the parser's accuracy, relating to results."
"**The method is simple with limited contribution, while performance improvement is not significant.",Does the review address Result?,TRUE,FALSE,This sentence critiques the method for its simplicity and limited performance improvement.
"Figure 1 is presently not pleasant to look at, even though it has interesting results`!",Does the review address Presentation?,TRUE,FALSE,This sentence critiques the presentation of Figure 1 despite its interesting results.
"Finally, a number of ablation studies are performed and demonstrate the effectiveness of the proposed method to some extent.",Does the review address Ablation?,TRUE,FALSE,This sentence discusses the ablation studies performed to demonstrate the method's effectiveness.
"3) Continue with 2), as the experiment results shown in Table 2, TS compiler performs poorly.",Does the review address Experiment?,TRUE,FALSE,"This sentence critiques the experiment results shown in Table 2, indicating poor performance."
- The methods and results are presented in an understandable manner.,Does the review address Presentation?,TRUE,FALSE,This sentence praises the presentation of methods and results as understandable.
It’s very hard to compare the absolute differences Tables 1 and 2 for ourselves.,Does the review address Comparison?,TRUE,FALSE,This sentence critiques the difficulty of comparing data presented in Tables 1 and 2.
"####Minor Comments:  The paper mentions in several places symbolic scaffolding without citations, literature is certainly rich here, e.g. [1,2] are papers integrating symbolic constraints for semantic parsing.",Does the review address Related Work?,TRUE,FALSE,This sentence critiques the lack of citations related to symbolic scaffolding.
The threshold is definitely related to hardware specifications and model architectures.,Does the review address Presentation?,TRUE,FALSE,"This sentence discusses the relationship between threshold, hardware specifications, and model architectures, relating..."
"- In figure 6A, why was performance not increasing for untuned models w.r.t model size?",Does the review address Methodology?,TRUE,FALSE,This sentence questions the methodological details related to performance in figure 6A.
"If not, the presentation requires to change and reflect only the controllability analysis.",Does the review address Presentation?,TRUE,FALSE,This sentence suggests changing the presentation to focus on controllability analysis.
Weaknesses * Something that stands out is the lack of discussion and comparison to related works that employ recurrent formulations of attention.,Does the review address Methodology?,TRUE,FALSE,This sentence critiques the lack of discussion and comparison related to recurrent formulations of attention.
"- P3, Sec 2.2: ""... achieve lower test perplexity than traditional n-gram models"" Why is this true?",Does the review address Related Work?,TRUE,FALSE,This sentence questions the claim related to lower test perplexity and suggests further discussion on related work.
"However, $R$ is defined as a non-square matrix in the previous paragraph.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence points out a discrepancy in the definition of $R$ as a non-square matrix.
What recommendation the authors would give for those interested in using it?,Does the review address Result?,TRUE,FALSE,"This sentence seeks recommendations related to the practical usage of the method, indicating results."
"In addition, improvements on full-shot cases are mostly marginal.",Does the review address Result?,TRUE,FALSE,This sentence critiques the marginal improvements in full-shot cases.
"However, the contribution of this paper is not clear.",Does the review address Contribution?,TRUE,FALSE,"This sentence explicitly states that the contribution of the paper is not clear, directly addressing the aspect of Contribution."
They introduce an unsupervised approach (MAMA) to construct a knowledge graph in two phases in which they take a target corpus and output a knowledge graph.,Does the review address Methodology?,TRUE,FALSE,This sentence describes the methodology involving the construction of a knowledge graph using an unsupervised approach.
"Along with ablations and trying LMs of varying sizes, their technique is compared against many other existing selective annotation approaches and shown to consistently outperform the latter.",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the comparison of the technique with other selective annotation approaches, relating to Data/Task."
The model is tested in a long range language modeling task.,Does the review address Methodology?,TRUE,FALSE,This sentence describes the methodological aspect involving long range language modeling.
"------- Minors: - figure 1: could consider adding x, which would better match the descriptions of the paper (modeling p(y|x) instead of p(y))  Missing references:   [1] Chan, W., Kitaev, N., Guu, K., Stern, M. and Uszkoreit, J., 2019.",Does the review address Related Work?,TRUE,FALSE,Yes
The task opens up a huge space for AI-powered general audio creation.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological implications for AI-powered audio creation.
The current paper only indicates that a small gap gives more consistency between the true objective and the optimized objective defined on the training set: they can be still far away from the expected posterior over data distribution.,Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the consistency between the true objective and the optimized objective, relating to methodology."
- The experimental analysis focuses exclusively on known computer vision datasets that do not differ from the training distribution of CLIP.,Does the review address Analysis?,TRUE,FALSE,This sentence critiques the experimental analysis focusing on known computer vision datasets.
"- unconstrained, single-concept:     This is adquate to demonstrate that a minimal environment works (as stated in the paper).",Does the review address Experiment?,TRUE,FALSE,This sentence describes the experimental setup and its adequacy for demonstrating a minimal environment.
Weaknesses: There is no contribution/novelty from the modeling/methods side.,Does the review address Methodology?,TRUE,FALSE,This sentence critiques the lack of contribution or novelty from the methodological side.
"Furthermore, the authors deliberately avoid settings where DP is known to be hard due to the relatively low amount of training data per class (e.g. CIFAR-100/ImageNet).",Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the avoidance of difficult settings due to limited training data, relating to methodology."
- Many interesting robustness capabilities of VL models only emerge when trained with hundreds of millions of samples.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological aspect of robustness capabilities emerging from large-scale training.
The proposed RoBERTa achieves/matches state-of-the-art performance on many standard NLU downstream tasks.,Does the review address Result?,TRUE,FALSE,This sentence discusses the results showing state-of-the-art performance achieved by the proposed RoBERTa model.
The authors should conduct experiments beyond English-to-French.,Does the review address Significance?,TRUE,FALSE,"This sentence suggests conducting experiments beyond English-to-French, indicating the significance of broader testing."
"In this paper, the authors present a study of different aspects of language-specific model capacity for massively multilingual machine translation.",Does the review address Methodology?,TRUE,FALSE,This sentence describes the methodological study of language-specific model capacity for multilingual translation.
It is great to have a theoretical analysis of the property of the influence function.,Does the review address Theory?,TRUE,FALSE,This sentence praises the theoretical analysis of the influence function's property.
The applicability and the novelty of the SCS representation seem limited.,Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the applicability and novelty of the SCS representation, relating to methodology."
This paper is an interesting application of a data augmentation or self-supervised learning type of approach for tactic based theorem proving.,Does the review address Theory?,FALSE,TRUE,This sentence does not mention theory.
it is likely a relatively minor effect given the results from Appendix B but it seems like it could slightly prevent overfitting  - The ablation in 4.1 was great (number of clusters).,Does the review address Ablation?,TRUE,FALSE,This sentence discusses the ablation study and its effect on preventing overfitting.
- Experimental results: I suggest the author to provide more ablation analysis to the experiment section.,Does the review address Ablation?,TRUE,FALSE,This sentence suggests providing more ablation analysis in the experiment section.
"### Major issue 1 The paper ""**theoretically**"" analyzes the hyper-parameter selection process in Section 3.1 and provides experimental validation in Section 5.1.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,This sentence discusses the theoretical analysis and experimental validation related to hyper-parameter selection.
"Simply because baselines have done that, it doesn't justify the authors from reporting inflated numbers.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,This sentence critiques the justification of reporting inflated numbers based on baselines.
An additional experimental results with multi-task finetuning should also be added.,Does the review address Data/Task?,TRUE,FALSE,This sentence suggests adding experimental results related to multi-task finetuning.
The state-of-the-art performance should be appreciated.,Does the review address Result?,TRUE,FALSE,This sentence praises the state-of-the-art performance achieved by the proposed method.
QA- GNN: Reasoning with language models and knowledge graphs for question answering.,Does the review address Related Work?,FALSE,TRUE,This sentence does not mention related work directly.
"- General Discussion: This work tackles an important and interesting event extraction problem -- identifying positive and negative interactions between pairs of countries in the world (or rather, between actors affiliated with countries).",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence discusses the event extraction problem, indicating a detailed explanation."
"- Extensive ablation studies to demonstrate the importance of modality interaction layer  - selection, parameter sharing, graph connectivity, and parameter initialization.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the importance of modality interaction layer, relating to methodology."
The paper is mostly clearly written and discusses server interesting ablation experiments.,Does the review address Presentation?,TRUE,FALSE,This sentence praises the clarity of the paper and discusses the ablation experiments.
The results presented in the paper show strong gains against baseline methods on 3 different datasets.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the results showing gains against baselines on different datasets, relating to Data/Task."
"When you map ORCHID to UD, you lose information, since the ORCHID tagset is more fine-grained; does this make a big difference for taggers?",Does the review address Comparison?,TRUE,FALSE,"This sentence questions the impact of mapping ORCHID to UD, relating to comparison."
"According to the evaluation, the proposed method shows its effectiveness especially when the finetuning data is limited.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the effectiveness of the proposed method with limited finetuning data, relating to methodology."
"* It will be cool to show some results on BLEU improvement on machine translation, or user studies on language generation tasks, to demonstrate its practical impact, as the quantitative metrics right now are still kind of artificial.",Does the review address Result?,TRUE,FALSE,"This sentence suggests showing results related to BLEU improvement and practical impact, relating to results."
"Under these assumptions, the gradient over the parity distribution samples is zero.",Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological aspect involving the gradient over parity distribution samples.
What is the number of resulting features that were used to train the logistic regression model?,Does the review address Methodology?,TRUE,FALSE,This sentence questions the methodological detail related to the number of resulting features for logistic regression.
"---- Detailed comments: ----  - ""For each dataset, we manually compose ten unique templates"":  Why not have templates per task cluster instead of per dataset?",Does the review address Data/Task?,TRUE,FALSE,"This sentence suggests an alternative approach for composing templates, relating to Data/Task."
I think if the paper will be improved if it resolves the lack of clarity around the temperature in GS being an implicit entropy-regularization parameter.,Does the review address Result?,TRUE,FALSE,"This sentence suggests resolving clarity issues related to the temperature in GS, indicating results."
The authors have done further experiments and show that there are still gains on these tasks when model sized is increased significantly.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the further experiments and gains related to increased model size, relating to methodology."
In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp.,Does the review address Related Work?,TRUE,FALSE,"This sentence references a conference proceeding, indicating related work."
"- If not, please remove the attention layer after the encoder in figure 5.",Does the review address Presentation?,TRUE,FALSE,This sentence suggests a change to the presentation of figure 5.
"In experiments, mainly accuracy is shown, but since the major motivation to reduce gradient variance, why not show some comparison of gradient variance of MLM and MLM-FE?",Does the review address Experiment?,TRUE,FALSE,This sentence critiques the experimental setup for not comparing gradient variance.
I provide details below examination of how I’ve come to my evaluation rating below.,Does the review address Evaluation?,TRUE,FALSE,"This sentence discusses the evaluation process, indicating evaluation."
"As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.",Does the review address Presentation?,TRUE,FALSE,This sentence suggests a change to the presentation involving BERT representations.
"It would be great to define and identify beyond current close-ended tasks with new lower level tasks which really require using the audio, such as counting sound events, ordering of events, etc.",Does the review address Data/Task?,TRUE,FALSE,"This sentence suggests defining new tasks related to audio, indicating Data/Task."
"It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset.",Does the review address Result?,TRUE,FALSE,"This sentence compares published results with the current system, indicating Result."
"‘Wang & Cho’ were not the first who used Transformers generativity (see Vaswani, 2017).",Does the review address Related Work?,TRUE,FALSE,This sentence references related work involving Transformers generativity.
More importantly the PACT methodology is more general; the idea of incorporating diverse auxiliary tasks as a language modeling task by introducing a distinct token for each task is novel.,Does the review address Novelty?,TRUE,FALSE,This sentence discusses the novelty of the PACT methodology.
Weaknesses: Some of the design choices need to be elaborated on further and additional analysis-based experiments would also be useful.,Does the review address Analysis?,TRUE,FALSE,This sentence critiques the design choices and suggests additional analysis-based experiments.
"| s) which performs well, instead of a model directly over p*( .",Does the review address Result?,TRUE,FALSE,"This sentence discusses the performance of a model, indicating Result."
I therefore consider the contributions as insufficient for an ICLR submission.,Does the review address Contribution?,TRUE,FALSE,"This sentence critiques the contributions as insufficient, indicating Contribution."
More theoretical proofs or appropriate literature citations are needed to validate this assertion.,Does the review address Theory?,TRUE,FALSE,"This sentence suggests providing more theoretical proofs or literature citations, indicating Theory."
"The theoretical results on the Parity/Sum task reply to some strong assumptions: bilinear parameterization, some initialization (for example, v = 0).",Does the review address Result?,TRUE,FALSE,"This sentence discusses the theoretical results related to the Parity/Sum task, indicating Result."
I will be more convinced if evaluation is done on a wider range of tasks.,Does the review address Data/Task?,TRUE,FALSE,"This sentence suggests conducting evaluation on a wider range of tasks, indicating Data/Task."
"Some of the details for model description are missing and confusing, e.g., (1) How the representation is initialized for a supernode corresponding to a fact triplet and how does the supernode propagate information to the sub-nodes?",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence critiques the clarity and completeness of the model description.
The paper use FLOPS to quantify the arithmetic intensity.,Does the review address Evaluation?,TRUE,FALSE,This sentence discusses the evaluation metric involving FLOPS.
"There's not much justification for it, especially given something simpler like a fixed window average could have been used.",Does the review address Methodology?,TRUE,FALSE,This sentence critiques the justification for a methodological choice.
And the results of the proposed model can also be available for downstream detection models.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the applicability of the model's results to downstream detection, indicating Methodology."
"The authors propose to include related texts retrieved by the kNN method in a single training sample, which is proved effective in solving sentence similarity tasks.",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the effectiveness of including related texts in training samples, indicating Data/Task."
"Section 4.8: Using transformers for generating proteins with natural properties is not new (see Madani et al, 2020, ‘ProGen’ or Rives et al, 2020).",Does the review address Novelty?,TRUE,FALSE,"This sentence mentions that the idea is not new, indicating a discussion of Novelty."
"(4) In general, the results in table 3 do not tell a consistent story.",Does the review address Presentation?,TRUE,FALSE,This sentence critiques the presentation of results in table 3.
However in experiment only 300 projects are involved.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the data/task aspect, mentioning the number of projects involved."
"For this purpose, the authors introduced the definition of a ""natural"" task.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence discusses the introduction of a definition, indicating explanation."
Summary ========= Authors applied reinforcement learning framework to the problem of task-oriented dialog.,Does the review address Methodology?,TRUE,FALSE,This sentence summarizes the methodological approach of applying reinforcement learning.
I also think it makes sense to switch Figure 1 and Figure 2 entirely.,Does the review address Presentation?,TRUE,FALSE,This sentence suggests changes to the presentation of figures.
"Leave ""general learner"" imprecise but reframe proposition 1 as your formal definition attempting to capture it: you view an LM hypothesis class is a general learner if it can express a universal circuit family for all poly-size circuits.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence discusses a formal definition, indicating explanation."
"around 1% might be reasonable for 30-40% reduction in training time, but it is certainly a reduction in accuracy.",Does the review address Result?,TRUE,FALSE,This sentence discusses the results related to accuracy and training time.
It's great the authors supplied code for part of the system so I don't want to penalize them for missing it -- but this is relevant since the paper itself has so few details on the baselines that they could not really be replicated based on the explanation in the paper.),Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence discusses the lack of details on baselines, indicating explanation."
i. e. representing a word by a set of many Gaussian distributions.,Does the review address Presentation?,TRUE,FALSE,This sentence describes a presentation aspect involving Gaussian distributions.
"Theoretical Advantages and Theorem Justification:  While you mention that quantum features are theoretically more expressive, the paper falls short of explaining the underlying intuition and proof for this assertion.",Does the review address Theory?,TRUE,FALSE,This sentence critiques the theoretical explanation and proof.
"* p 4: ""es should set a upper bound"" --> ""es should set an upper bound""  * p 5: "" ec perform better than or "" --> "" ec performs better than or """,Does the review address Presentation?,TRUE,FALSE,This sentence points out grammatical errors and suggests corrections to improve presentation.
See section-2.6 of this tutorial for more details about using neural models to rank: https://www.microsoft.com/en-us/research/uploads/prod/2017/06/INR-061-Mitra-neuralir-intro.pdf.,Does the review address Related Work?,TRUE,FALSE,"This sentence references a tutorial, indicating related work."
The performance drops significantly when we change any of them.,Does the review address Result?,TRUE,FALSE,This sentence discusses the results related to performance drops.
- Extensive results show improvements over a base model and a larger model across a range of tasks.,Does the review address Result?,TRUE,FALSE,This sentence discusses extensive results showing improvements.
The author have opted for some sort of greedy pruning as described in the caption of figure 4.,Does the review address Methodology?,TRUE,FALSE,This sentence describes the methodology involving greedy pruning.
"Page 5, line 1: should \tilde{e}^{(l-1)} be \tilde{e}^{l} instead ?",Does the review address Presentation?,TRUE,FALSE,This sentence questions a presentation aspect involving notation.
"If it is the extension to multilingual embeddings, a few lines explaining the novelty would help.",Does the review address Methodology?,TRUE,FALSE,"This sentence suggests explaining the novelty, indicating methodology."
"To make the method more convincing, authors may consider use more recent VL models as student works, and try to further push their limits.",Does the review address Methodology?,TRUE,FALSE,This sentence suggests using more recent models to make the method more convincing.
"In experimental details, the slightly better performance in Table 2, can it be attributed to finer generation powered by the RNN generator?",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence questions the explanation related to performance details.
"The main limitation to me is that, the two novel pre-training tasks proposed in this paper are specific for Wikipedia and they are less effective than the ICT strategy (as shown in Table 5).",Does the review address Presentation?,TRUE,FALSE,This sentence critiques the presentation of novel pre-training tasks.
t-SNE plots of these selected examples in the larger context of unlabeled instances might also be a good visualization to show.,Does the review address Presentation?,TRUE,FALSE,This sentence suggests a visualization technique for presentation.
"- Well Structured Experiments sections with 4 RQs and results that confirm each of the hypotheses  - Reproducibility and Transparency in reporting of experiments in terms of available source code, dataset information, details about human evaluation, generation examples.",Does the review address Evaluation?,TRUE,FALSE,"This sentence praises the structured experiments and results, indicating evaluation."
"Beyond this, the authors conduct a multifaceted set of tests, including a non-harmful test to ensure that the watermark embedding does not significantly degrade model performance, robustness tests against second-time fine-tuning and model quantization, and an ablation study concerning the reference set to further substantiate the rationality of their backdoor data framework design.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,This sentence discusses the validation and justification of the backdoor data framework design.
Enhancing the paper with the aforementioned suggestions could substantially improve its impact and reception by the research community.,Does the review address Result?,TRUE,FALSE,"This sentence discusses the potential impact of improving the paper, indicating results."
There are also some typos to be corrected: Sec 1: ``...making purchase decision...'' should be ``making a/the purchase decision'' Sec 1: ``...are devoted to explore... '' should be `` are devoted to exploring'' Sec 1: ``...there is on sufficient behaviors...'' should be “there are no sufficient behaviors'' Sec 1: ``...on business trip...'' should be ``on a business trip'' Sec 1: ``...there are abundant behavior information...'' should be ``there is abundant behavior'' Sec 3: ``The new reviewer only provide us...'' should be ``...The new reviewer only provides us...'' Sec 3: ``...features need not to take much...'' should be ``...features need not take much...'' Sec 4: ``...there is not any historical reviews...'' should be ``...there are not any historical reviews...'' Sec 4: ``...utilizing a embedding learning model...'' should be ``...utilizing an embedding learning model...'' Sec 5.2 ``...The experiment results proves...'' should be ``...The experiment results prove...'' - General Discussion: It is a good paper and should be accepted by ACL.,Does the review address Presentation?,TRUE,FALSE,This sentence lists typos and suggests corrections to improve presentation.
** I would like to urge the authors to include more powerful baselines in the experiment rather than hide them.,Does the review address Comparison?,TRUE,FALSE,"This sentence suggests including more powerful baselines, indicating Comparison."
"Prior work has explored ""learning-to-share""  strategies for parameter sharing in multi-task learning (see Ruder et al., AAAI 2018), and using gating/masking to control computational paths in a differentiable way (see Fan et al., ICLR 2019, Sukhbaatar et al., ACL 2019); it is clear that the focus is NMT but it should be worth mentioning/discussing such studies to better situate the work and to help the reader assess the actual contributions.",Does the review address Related Work?,TRUE,FALSE,"This sentence discusses prior work related to parameter sharing and gating/masking, indicating Related Work."
Manual parameter sharing schemes are generally costly to come up with and when they are obtained for certain language pairs they do not necessarily generalize well to arbitrary language pairs in multilingual NMT.,Does the review address Methodology?,TRUE,FALSE,"This sentence critiques manual parameter sharing schemes, indicating Methodology."
"However, the paper does not include detailed descriptions about the proposed method, making readers not easy to understand.",Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the lack of detailed descriptions about the proposed method, indicating Methodology."
- multi-concept and noise:     The noise you add to the channel has a number of different components; there should be an ablation study to illustrate the effects of these components.,Does the review address Experiment?,TRUE,FALSE,"This sentence suggests conducting an ablation study to illustrate the effects of noise components, indicating Experiment."
Simply adding related sentences in the pre training input context helps end performance.,Does the review address Result?,TRUE,FALSE,"This sentence discusses the impact of adding related sentences on performance, indicating Result."
The authors refer to the fact that MTL can (and often does!),Does the review address Result?,TRUE,FALSE,"This sentence mentions the impact of MTL on performance, indicating Result."
The submission has evaluated the proposed algorithms on four datasets and improved SOTA performances.,Does the review address Evaluation?,TRUE,FALSE,"This sentence discusses the evaluation of algorithms on multiple datasets, indicating Evaluation."
"Although the presentation can be polished, the overall narrative and explanation is clear and easy to follow.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence praises the clarity of the narrative and explanation, indicating Explanation."
"The authors show that for such tasks, if the pre-training objective is $\epsilon$-optimal, then the downstream objective of a linear classifier is $\mathcal{O}(\sqrt{\epsilon})$-optimal.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the relationship between pre-training and downstream objectives, indicating Methodology."
"Setting and Main Result:  This paper focuses on classification tasks, and the bulk of the work goes into how to model the next word distributions as features or representations.",Does the review address Result?,TRUE,FALSE,"This sentence describes the focus on classification tasks and results, indicating Result."
The paper is well-written and the idea is well-motivated.,Does the review address Presentation?,TRUE,FALSE,"This sentence praises the writing and motivation of the paper, indicating Presentation."
"The annotations from [1] are very simple and short, only including some easy examples as in-context examples.",Does the review address Comparison?,TRUE,FALSE,"This sentence critiques the simplicity of the annotations, indicating Comparison."
"## Logical Flow of Main Results and Relation to Prior Work Should be Clarified  The paper defines T-LLMs as general learners if ""the expressive power of realistic T-LLM model class can cover universal circuits for all circuits of polynomial size"" and says that:  > It is unknown whether the realistic Transformers are expressive enough to be general learners   In fact, it seems that Theorem 3 essentially follows from the TC0 upper bound in previous work.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence discusses the logical flow and relation to prior work, indicating Explanation."
"The design of the architecture is novel, but it is also not groundbreaking.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the novelty and limitations of the architecture design, indicating Methodology."
Summary: + Appealing theoretical contributions + Empirical results are encouraging + The use of discriminator for reward shaping in addition to task success rate is interesting  - Writing and explanation can be improved.,Does the review address Presentation?,TRUE,FALSE,"This sentence summarizes strengths and critiques writing and explanation, indicating Presentation."
- The methods and results are presented in an understandable manner.,Does the review address Result?,TRUE,FALSE,"This sentence praises the presentation of methods and results, indicating Result."
"around 1% might be reasonable for 30-40% reduction in training time, but it is certainly a reduction in accuracy.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the trade-off between training time and accuracy, indicating Methodology."
The proposed ideas in isolation do not yield significant improvements.,Does the review address Result?,TRUE,FALSE,"This sentence critiques the impact of the proposed ideas, indicating Result."
* Related work: I would also include a discussion of this Lazaridou et al paper where they compare ways of combining EC with non-EC learning signals (e.g. image caption training): https://aclanthology.org/2020.acl-main.685.pdf  * Ethics statement: I appreciate this statement and agree with the possible positive impacts.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence suggests including related work discussion, indicating Explanation."
"If that is the case, that should be made more explicit.",Does the review address Data/Task?,TRUE,FALSE,This sentence suggests making the data/task aspect more explicit.
It would be really useful to have an ablation study to disentangle which piece of the training or method is contributing to what and how in the performance measure.,Does the review address Ablation?,TRUE,FALSE,"This sentence suggests conducting an ablation study to analyze contributions to performance, indicating Ablation."
4) It is not clear how the prediction variance is reduced gradually in order to generate the results in Figure 1(b).,Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the clarity of presentation related to prediction variance, indicating Presentation."
After author response: See reply comment in the thread below for further score justification.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence discusses score justification, indicating Validation."
"* Were the proposed architectural additions conceived with the HANS ""counterexamples"" in mind (i.e. is there a specific reason to think that these types of methods would avoid the ""superficial"" reasoning that these examples are supposed to reveal)?",Does the review address Methodology?,TRUE,FALSE,This sentence questions the methodological motivation behind the architectural additions.
"But for some of the words, say ""hold"" and ""sits"", could it play a more important role?",Does the review address Significance?,TRUE,FALSE,This sentence discusses the potential significance of certain words.
"It might be good to find the best hyperparameters for each setting to truly do a fair comparison (avoiding hyperparameter tuning isn't really a fair comparison, IMO -- but this depends on how much compute you have available).",Does the review address Comparison?,TRUE,FALSE,This sentence discusses the importance of hyperparameter tuning for fair comparison.
"With this approach, that requirement doesn’t exist anymore.",Does the review address Significance?,TRUE,FALSE,This sentence discusses the significance of the approach removing a previous requirement.
The experimentation is correct and the qualitative analysis made in table 1 shows results as expected from the approach.,Does the review address Result?,TRUE,FALSE,This sentence discusses the results and qualitative analysis shown in table 1.
What was the gain by simply using these embeddings as alternatives to the random embeddings in the LSTM stack parser?,Does the review address Methodology?,TRUE,FALSE,This sentence questions the methodological gain from using specific embeddings.
"Weaknesses:   The primary weakness of the paper is the lack of convincing justification that the authors have discovered a phenomenon distinct from “collective outliers” (Karamcheti et al., 2021) -- or if it is distinct, how exactly is it distinct?",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,This sentence critiques the justification and distinction of the discovered phenomenon.
The current experimental baseline can't reflect this.,Does the review address Comparison?,TRUE,FALSE,"This sentence critiques the experimental baseline, indicating Comparison."
Figure 1 clearly shows the code generation process.,Does the review address Presentation?,TRUE,FALSE,This sentence discusses the clarity of the presentation of the code generation process in figure 1.
Some comments:  _ It may be interesting to include a brief explanation of the differences between the approach from Tian et al. 2014 and the current one.,Does the review address Comparison?,TRUE,FALSE,This sentence suggests including a comparison between the current approach and a previous one.
"Then, taking inspiration from recent work that shows that many downstream tasks can be reframed as sentence completion tasks, it defines a “natural task” as one on which a sparse linear model over the output of the “true” language model (next word probability distribution, conditioned on context) attains strong performance.",Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological definition of a “natural task.”
The authors find that the main ingredients for the success of in-context learning are a combination of selective annotation with similarity-based prompt retrieval.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the main ingredients for the success of in-context learning, indicating Methodology."
Providing a more comprehensive elucidation of these processes would aid in bridging the gap between classical and quantum approaches.,Does the review address Methodology?,TRUE,FALSE,"This sentence suggests providing a more comprehensive explanation of the processes, indicating Methodology."
"In terms of strenghs - The paper has a very throughout analysis of different models and positional encodings - It proposes several contributions, including a new probing task and several positional encoding methods.",Does the review address Data/Task?,TRUE,FALSE,This sentence discusses the analysis and contributions related to different models and positional encodings.
"Given a validation corpus (X,Y), and the corresponding retrieved (tX, tY), the authors should at least show the similarity between (X,tX), (Y,tY), which measures the retrieval quality.",Does the review address Significance?,TRUE,FALSE,"This sentence suggests showing the similarity between retrieved data to measure retrieval quality, indicating Significance."
"The description of the baselines is lacking and quite confusing: it's not clear why the authors have two DP-SGD baselines, and what they're exactly updating during training.",Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the clarity of the description of baselines, indicating Presentation."
3) This paper shows visualization of the interaction between words and latent topics in the embedding space.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological aspect of visualizing interactions in the embedding space.
- Slight improvements over CaP via the different prompting method.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses slight methodological improvements from a different prompting method.
Clarity on Quantum Enhancements:  Section 3.2.2 seems to lack depth in the explanation of how the quantum correlations are calculated and utilized within the graph transformers.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence critiques the depth of explanation related to quantum enhancements, indicating Explanation."
"While this is not a downside by itself, it is probably something that one implement as a baseline.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the potential implementation of a baseline, indicating Methodology."
"One might hypothesize that if using a (subotimal) template that is less natural for language modeling, that zero-shot performance would suffer, but that FLAN performance wouldn't - One might hypothesize that the ""turn the task around"" templates help more than the other more straightforward templates that don't swap information between the prompt and response.",Does the review address Presentation?,TRUE,FALSE,"This sentence discusses the impact of template choices on performance, indicating Presentation."
## Summary The paper shows that Transformers trained unsupervised on millions of protein sequences learn information about protein contacts by using attention maps for contact prediction.,Does the review address Methodology?,TRUE,FALSE,This sentence summarizes the methodological approach involving Transformers and protein sequences.
"Considering the BERT is leveraged, you should discuss the relation with BERT + NMT [R1,R2].",Does the review address Related Work?,TRUE,FALSE,This sentence suggests discussing the relation with related work involving BERT and NMT.
Is any care taken to handle this in training data?,Does the review address Data/Task?,TRUE,FALSE,"This sentence questions the handling of certain aspects in the training data, indicating Data/Task."
Both split single word representation into multiple prototypes by using a mixture model.,Does the review address Comparison?,TRUE,FALSE,"This sentence discusses the comparison of word representation methods, indicating Comparison."
"Further, the performance improvements are nice, though not impressive.",Does the review address Evaluation?,TRUE,FALSE,"This sentence evaluates the performance improvements, indicating Evaluation."
"* Why are the lines for ""from scratch"" flat in Figure 2?",Does the review address Presentation?,TRUE,FALSE,This sentence questions the presentation of data in Figure 2.
"There are many confounding factors such as the number and type of pretraining data, the instruction-tuning data, different architecture designs, and finetuning strategies.",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the confounding factors related to data and tasks, indicating Data/Task."
The PACT methodology: The paper proposes a methodology for extracting auxiliary tasks that can be trained jointly along with the main task (tactic prediction task).,Does the review address Data/Task?,TRUE,FALSE,This sentence describes the PACT methodology related to data and tasks.
"However, for tasks that require more information on prior decoding context (machine translation or image generation tasks), does the proposed model can still perform well on those tasks?",Does the review address Result?,TRUE,FALSE,"This sentence questions the performance of the proposed model on certain tasks, indicating Result."
--------------------------------------- Strength:  The hypothesis from the variance reduction is interesting.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the hypothesis related to variance reduction, indicating Methodology."
"Result 1: Under the above assumption over the downstream task, this paper provides a bound on the empirical loss of the downstream prediction task.",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the empirical loss bound on the downstream prediction task, indicating Data/Task."
"- If so, please be more specific in describing it in section 4.2 and Table 4.",Does the review address Presentation?,TRUE,FALSE,This sentence suggests being more specific in the description for better Presentation.
"Mainly, for most of the intrinsic metrics, the multilingual embedding techniques do not seem to perform the best.",Does the review address Result?,TRUE,FALSE,"This sentence discusses the performance of multilingual embedding techniques, indicating Result."
Which layers and heads were used and how were they aggregated?,Does the review address Methodology?,TRUE,FALSE,This sentence questions the methodological details related to layers and heads used.
Section 4.5 discusses that Transformers can be also used for secondary structure prediction.,Does the review address Methodology?,TRUE,FALSE,"This sentence mentions the use of Transformers for secondary structure prediction, indicating Methodology."
"I.e., Figure 1 should be your results table, and figure 2 should be the examples for us to see.",Does the review address Presentation?,TRUE,FALSE,This sentence suggests reordering figures for better Presentation.
"- Extensive ablation studies to demonstrate the importance of modality interaction layer  - selection, parameter sharing, graph connectivity, and parameter initialization.",Does the review address Ablation?,TRUE,FALSE,"This sentence discusses the importance of modality interaction layer and ablation studies, indicating Ablation."
"It would have been fascinating if the authors were to do an ablation study to train their model in the format of (Audio, Text) --> Text format - something similar to what PENGI does.",Does the review address Significance?,TRUE,FALSE,"This sentence suggests conducting an ablation study for a specific format, indicating Significance."
"This is a bit confusing and I would suggest either changing the terminology or explicitly clarifying that you are talking about expressive power and not learning (which is fine, limitations on expressive power translate to limitations on what can be learned).",Does the review address Presentation?,TRUE,FALSE,"This sentence suggests clarifying terminology related to expressive power, indicating Presentation."
This paper presents a replication study of BERT pretraining and carefully measures the impact of many key hyperparameters and training data size.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the replication study of BERT pretraining, indicating Data/Task."
Can the proposed theory help explain if the same code modeling task for some languages is strictly easier than the others?,Does the review address Data/Task?,TRUE,FALSE,"This sentence questions the applicability of the proposed theory to different languages, indicating Data/Task."
"**Areas of Enhancement & Questions to authors**  - The information about each of the ablations (ID, BM) could be explained better.",Does the review address Ablation?,TRUE,FALSE,"This sentence suggests improving the explanation of ablations, indicating Ablation."
How to efficiently complete the fine-tuning of the pre-trained model is also a direction worthy of attention.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the importance of efficient fine-tuning, indicating Methodology."
The experiments are conducted on datasets from questions answering and sentiment analysis.,Does the review address Data/Task?,TRUE,FALSE,"This sentence mentions the datasets used for experiments, indicating Data/Task."
"The primary contribution is an application of supervised, structured neural network models for sentence-level event/relation extraction.",Does the review address Contribution?,TRUE,FALSE,"This sentence discusses the primary contribution related to neural network models, indicating Contribution."
What would happen if the number of scales is increased to capture  longer contexts?,Does the review address Experiment?,TRUE,FALSE,"This sentence questions the impact of increasing the number of scales, indicating Experiment."
* The proposed method RandomMask achieves SOTA on various downstream tasks.,Does the review address Methodology?,TRUE,FALSE,"This sentence mentions the state-of-the-art performance of the proposed method, indicating Methodology."
We failed at finding an alpha meeting the requirements for the FT model.,Does the review address Methodology?,TRUE,FALSE,"This sentence mentions the failure to find an alpha for the FT model, indicating Methodology."
The introductions discusses existing work on Transformers for protein languages models.,Does the review address Methodology?,TRUE,FALSE,"This sentence mentions the discussion of existing work, indicating Methodology."
"The selected student networks, VL-BERT, UNITER, VILLA, while they are great and highly reputable works in the community, their performance is not as competitive as for today.",Does the review address Result?,TRUE,FALSE,"This sentence discusses the performance of selected student networks, indicating Result."
The paper does not discuss the computational complexity of the proposed methods.,Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the lack of discussion on computational complexity, indicating Methodology."
"- I also think it is reasonable to include a baseline that just input additional knowledge as features to the RNN, e.g. the head of each word, NER results etc.",Does the review address Comparison?,TRUE,FALSE,"This sentence suggests including a baseline for additional knowledge features, indicating Comparison."
"Although the complexity analysis is thorough, I'd like to see empirical results of memory/compute requirements as a function of the context length.",Does the review address Methodology?,TRUE,FALSE,"This sentence suggests including empirical results related to memory/compute requirements, indicating Methodology."
What recommendation the authors would give for those interested in using it?,Does the review address Methodology?,TRUE,FALSE,"This sentence questions the recommendations for practical usage, indicating Methodology."
"**The method is simple with limited contribution, while performance improvement is not significant.",Does the review address Contribution?,TRUE,FALSE,"This sentence critiques the simplicity and limited performance improvement, indicating Contribution."
The paper has explained and empirically showed that this learned generator needs a resampler.,Does the review address Result?,TRUE,FALSE,"This sentence discusses the empirical results showing the need for a resampler, indicating Result."
- Too much detail in related work: I think the related work section is too long and just a list of papers rather than helping to place the work in the research area.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence critiques the length and usefulness of the related work section, indicating Explanation."
- The visualization section is only a minor contribution; there isn't really any innovation or findings about what works or doesn't work here.,Does the review address Contribution?,TRUE,FALSE,This sentence critiques the minor contribution of the visualization section.
"There are various weaknesses of the MID data, but the evaluation approach needs to be discussed or justified.",Does the review address Methodology?,TRUE,FALSE,"This sentence suggests discussing or justifying the evaluation approach, indicating Methodology."
"Mainly, for most of the intrinsic metrics, the multilingual embedding techniques do not seem to perform the best.",Does the review address Presentation?,TRUE,FALSE,"This sentence discusses the performance of multilingual embedding techniques, indicating Presentation."
"## Logical Flow of Main Results and Relation to Prior Work Should be Clarified  The paper defines T-LLMs as general learners if ""the expressive power of realistic T-LLM model class can cover universal circuits for all circuits of polynomial size"" and says that:  > It is unknown whether the realistic Transformers are expressive enough to be general learners   In fact, it seems that Theorem 3 essentially follows from the TC0 upper bound in previous work.",Does the review address Result?,TRUE,FALSE,"This sentence discusses the logical flow and relation to prior work, indicating Result."
Will the attention model pays more attention to this similar sample resulting in a negative impact on the target task performance since the source task and target task are quite different?,Does the review address Result?,TRUE,FALSE,"This sentence questions the impact of the attention model on performance, indicating Result."
"It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset.",Does the review address Comparison?,TRUE,FALSE,"This sentence compares published results with the current system, indicating Comparison."
"I would have liked to see these results (also, please fix grammar in this sentence) 9.",Does the review address Presentation?,TRUE,FALSE,This sentence critiques the presentation of results and suggests fixing grammar.
I think I'd like to see a discussion of sufficient number D analytically or empirically.,Does the review address Analysis?,TRUE,FALSE,"This sentence suggests discussing sufficient number D analytically or empirically, indicating Analysis."
"Therefore, it is not necessary to carry out the forward of the text encoder every iteration during training.",Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological aspect of the text encoder during training.
The Ablation Study of the paper also provides useful insights about the impact of different pre-training schemas on large-scale information retrieval tasks.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the insights gained from the ablation study, indicating Data/Task."
"Given a validation corpus (X,Y), and the corresponding retrieved (tX, tY), the authors should at least show the similarity between (X,tX), (Y,tY), which measures the retrieval quality.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence suggests showing the similarity to measure retrieval quality, indicating Validation."
- Weaknesses: Comparison and credit to existing work is severely lacking.,Does the review address Comparison?,TRUE,FALSE,"This sentence critiques the lack of comparison and credit to existing work, indicating Comparison."
"A discussion on how to add new tasks to the same framework might help, or a discussion on why the current framework is enough.",Does the review address Methodology?,TRUE,FALSE,"This sentence suggests discussing the addition of new tasks or the sufficiency of the current framework, indicating Methodology."
"Considering the BERT is leveraged, you should discuss the relation with BERT + NMT [R1,R2].",Does the review address Significance?,TRUE,FALSE,"This sentence suggests discussing the relation with BERT + NMT, indicating Significance."
"Summary ------- Using multi-scale hierarchical and compressive techniques, this paper examines a way to increase the context length of transformers.",Does the review address Methodology?,TRUE,FALSE,This sentence describes the methodological approach to increasing the context length of transformers.
An additional experimental results with multi-task finetuning should also be added.,Does the review address Result?,TRUE,FALSE,"This sentence suggests adding experimental results related to multi-task finetuning, indicating Result."
"The techniques used in the paper (multi-branch transformer, pointing mechanism, cross-modal attention, global positional encodings, etc) have been shown to work in the past for image-text tasks [1, 2].",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the techniques used in the paper, indicating Methodology."
"Since one of the main contributions of this paper is the analysis of the BERT pretraining process, more experimental analysis on the optimizer should also be included.",Does the review address Analysis?,TRUE,FALSE,"This sentence suggests including more experimental analysis on the optimizer, indicating Analysis."
The training objective allow budgetary constraints on the amount of language-specific parameters.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the training objective related to budgetary constraints, indicating Methodology."
A layman's explanation or intuitive reasoning behind the adoption of quantum features and their computational advantages in graph analysis would be invaluable for the reader's understanding.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence suggests providing an explanation behind the adoption of quantum features, indicating Explanation."
It appears to vary by orders of magnitude according to Table 10 in the Appendix.,Does the review address Presentation?,TRUE,FALSE,"This sentence discusses the variation in data presented in Table 10, indicating Presentation."
"In experiments, there is no comparison with previous retrieval based methods.",Does the review address Comparison?,TRUE,FALSE,"This sentence critiques the lack of comparison with previous retrieval-based methods, indicating Comparison."
"Thus, while concatenating the audio feature and the text feature can introduce desired performance, there could be some advancements not just combining pretrained audio model and LLM.",Does the review address Methodology?,TRUE,FALSE,"This sentence suggests potential advancements beyond combining pretrained audio model and LLM, indicating Methodology."
"Additionally, the authors conclude that pre-training is not a significant factor in the efficacy of active learning, but their numerical results suggest active learning methods (Entropy and Coreset) narrow the gap with the random baseline significantly from BERT-Base to RoBERTa-Base!",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the impact of pre-training on active learning, indicating Methodology."
Typos: - synthesis -> synthesise - DeepHOLZero -> DeepHOL - wrong bold number in Figure 3,Does the review address Presentation?,TRUE,FALSE,"This sentence points out typos and suggests corrections, indicating Presentation."
Conference on Empirical Methods in Natural Language Processing (2019).,Does the review address Related Work?,TRUE,FALSE,"This sentence references a conference, indicating Related Work."
In my opinion this is a significant paper as it explores one of the straight-forward ways to couple an audio encoder with a trained LLM and carefully examines this coupling from several different viewpoints and contributes the OpenAQA dataset which can be a useful public resource for future research.,Does the review address Significance?,TRUE,FALSE,"This sentence discusses the significance of the paper, indicating Significance."
The proposed RoBERTa achieves/matches state-of-the-art performance on many standard NLU downstream tasks.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the performance of RoBERTa, indicating Methodology."
The paper starts with the motivation of handling comprehensibility.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence discusses the motivation behind the paper, indicating Validation."
"Mainly, Table 4 provides understandable results showing that multi-turn specifications achieve better performance compared to single-turn specifications.",Does the review address Experiment?,TRUE,FALSE,"This sentence discusses the experimental results presented in Table 4, indicating Experiment."
"- The definitions, models, and assumptions in the paper are intuitive and clear (e.g., natural task).",Does the review address Methodology?,TRUE,FALSE,"This sentence praises the clarity of definitions, models, and assumptions, indicating Methodology."
Will the attention model pays more attention to this similar sample resulting in a negative impact on the target task performance since the source task and target task are quite different?,Does the review address Data/Task?,TRUE,FALSE,"This sentence questions the impact of the attention model on task performance, indicating Data/Task."
Some experimental settings only include the baselines Random and Vote-k. More methods as mentioned in 4.3.2 can be also included.,Does the review address Experiment?,TRUE,FALSE,"This sentence suggests including more methods in the experimental settings, indicating Experiment."
"However, if we considered this view, it seems like the problem they are solving should be more impactful than type recovery and AST generation.",Does the review address Significance?,TRUE,FALSE,"This sentence discusses the potential impact of the problem being solved, indicating Significance."
The results on a mixture of the Parity/Sum task are interesting   2.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the results on a specific task, indicating Data/Task."
Strength: - Thorough study on the robustness of recent popular VL models vs conventional CE models.,Does the review address Result?,TRUE,FALSE,"This sentence discusses the robustness study of models, indicating Result."
"Compared to this, the paper made 2 changes to the model: 1) using Gumble-Softmax instead of REINFORCE, and 2) using mixture-of-signals instead of straightforward gradient back-propagation.",Does the review address Presentation?,TRUE,FALSE,"This sentence describes the changes made to the model, indicating Presentation."
The paper describes the idea of multiple temporal scales.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological aspect of multiple temporal scales.
"The main issue of the paper is in the experiments and results reporting, it needs quite a bit of reworking.",Does the review address Result?,TRUE,FALSE,"This sentence critiques the reporting of experiments and results, indicating Result."
"Both those works try to do both (1) extract time series or other statistical information about the polarity of the relationships between countries, and *also* (2) extract topical keywords to explain aspects of the relationships.",Does the review address Related Work?,TRUE,FALSE,"This sentence discusses the related work and their approaches, indicating Related Work."
"However, there are insufficient experiments and comparison to previous work to convince me that the paper’s contributions are novel and impactful.",Does the review address Experiment?,TRUE,FALSE,"This sentence critiques the lack of experiments and comparison to previous work, indicating Experiment."
There are a couple decisions related to methodology that may need some further examination.,Does the review address Methodology?,TRUE,FALSE,"This sentence suggests examining certain methodological decisions, indicating Methodology."
"On top of that, the only data is coming from LeetCode.",Does the review address Data/Task?,TRUE,FALSE,"This sentence mentions the source of the data, indicating Data/Task."
is a micro-average (all testsets are concatenated and evaluated as one set) or macro-average (average taken across the scores of individual test sets) score.,Does the review address Result?,TRUE,FALSE,"This sentence discusses the scoring method, indicating Result."
I understand that LMU might have limited capacity but this is not specifically discussed and it is unclear how each component contributes to the end performance.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence critiques the clarity of explanation regarding component contributions, indicating Explanation."
"While previous work has examined tasks in the overall area, to my knowledge there has not been any publicly availble sentence-level annotated data for the problem -- the authors here make a contribution as well by annotating some data included with the submission; if it is released, it could be useful for future researchers in this area.",Does the review address Related Work?,TRUE,FALSE,"This sentence discusses the contribution of annotated data, indicating Related Work."
Clarification on the task setting: Is it the case that the agent's current utterance does not decide what the next user utterance is?,Does the review address Data/Task?,TRUE,FALSE,"This sentence seeks clarification on the task setting, indicating Data/Task."
"Therefore, the obtained task performance is far from state-of-the-art.",Does the review address Data/Task?,TRUE,FALSE,"This sentence critiques the obtained task performance, indicating Data/Task."
I also encourage the authors to simplify the experiment described in section 3.1 to make it more clear.,Does the review address Experiment?,TRUE,FALSE,"This sentence suggests simplifying the experimental description, indicating Experiment."
It is a bit difficult to understand the task definitions without first understanding the various constituents of the proof.,Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the clarity of task definitions, indicating Presentation."
"Such rarity, however, could potentially be a drawback for these types of watermarking methods.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the potential drawback of watermarking methods, indicating Methodology."
"For example, what is the experiment environment and training receipts.",Does the review address Methodology?,TRUE,FALSE,"This sentence questions the experimental environment and training setup, indicating Methodology."
"This indicates that with the same training objective, different inference methods have different gaps to the posterior lower bound.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the impact of different inference methods, indicating Methodology."
"They train low resource NMTs using 4 different tokenization strategies, to show that their proposed tokenization method leads to the best NMT results by several metrics.",Does the review address Evaluation?,TRUE,FALSE,"This sentence discusses the training and evaluation of NMTs, indicating Evaluation."
Is it a margin of an SVM classifier that solves $\mathcal{T}$?,Does the review address Data/Task?,TRUE,FALSE,"This sentence questions the margin of an SVM classifier, indicating Data/Task."
"-----Weaknesses----- I'm not very convinced by the empirical results, mostly due to the lack of details of the baselines.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence critiques the lack of details on baselines, indicating Explanation."
- The presentation could be improved and distracts from the content at times.,Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the presentation quality, indicating Presentation."
"Given that there is a wealth of existing work that performs the same task and the lack of novelty of this work, the authors need to include experiments that demonstrate that their technique outperforms others on this task, or otherwise show that their dataset is superior to others (e.g. since it is much larger than previous, does it allow for better generalization?)",Does the review address Novelty?,TRUE,FALSE,"This sentence discusses the novelty and suggests including comparative experiments, indicating Novelty."
"- Also, regarding section 3.3, please cite appropriate publications the ""previous work"" presented in the tables.",Does the review address Related Work?,TRUE,FALSE,"This sentence suggests citing appropriate publications, indicating Related Work."
The curriculum learning is yet another contribution which makes a lot of sense and the authors have proposed an intuitive curriculum and backed it up with apt ablation study to show its utility.,Does the review address Novelty?,TRUE,FALSE,"This sentence discusses the contribution and novelty of curriculum learning, indicating Novelty."
"Some kind of analysis of the qualitative strengths and weaknesses of the binary code prediction would be welcome -- what kind of mistakes does the system make, and how does this compare to standard softmax and/or hierarchical and differentiated softmax?",Does the review address Comparison?,TRUE,FALSE,"This sentence suggests analyzing the strengths and weaknesses of binary code prediction, indicating Comparison."
(ii) a new way to aggregate multiple inputs (using M-BERT) and several different decoding methods.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses a new way to aggregate inputs and decoding methods, indicating Methodology."
"I think more could be done here, some ideas, probably there are better ways to test: - Have templates that leave out ""instructions"":  I would guess it wouldn't affect held-in task performance much, but would affect held-out tasks.",Does the review address Data/Task?,TRUE,FALSE,"This sentence suggests testing different template approaches, indicating Data/Task."
They have to tune this parameter empirically instead of configure it theoretically.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the empirical tuning of a parameter, indicating Methodology."
**Experimental results** The presentation of the experimental results is clear.,Does the review address Result?,TRUE,FALSE,"This sentence praises the clarity of the experimental results presentation, indicating Result."
The model sizes are varied across experiments to achieve on par performance which makes the comparison of the computational cost not so obvious.,Does the review address Result?,TRUE,FALSE,"This sentence discusses the comparison of computational cost, indicating Result."
"For experiments, Table 1 shows the FOCAL Reasoner (DeBERTa) brings much performance gain, compared to FOCAL Reasoner (RoBERTa) (the performance seems comparable with DAGN and LReasoner using RoBERTa).",Does the review address Result?,TRUE,FALSE,"This sentence discusses the performance gain shown in Table 1, indicating Result."
"- The authors have evaluated their models on multiple settings, proving that their models trained on Wikipedia can potentially generalize to multiple downstream tasks.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the evaluation of models and their generalization ability, indicating Methodology."
The authors also propose two novel pre-training settings which also show improvement over the baseline BM-25.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses novel pre-training settings and their improvement, which relates to the methodology."
"Before describing your algorithm, humans are only mentioned once in the algorithm.",Does the review address Methodology?,TRUE,FALSE,"This sentence addresses the mentions of humans in the algorithm, which is related to the methodology."
But a direct head to head comparison for the computational cost of a multi-task model and individual models is not provided.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the computational cost comparison, which relates to the data/task."
"It is a report of various NLP efforts for several Indigenous languages of Canada It goes deeply enough into the technical details of the projects to show that the efforts are viable and successful, without getting bogged down in numbers or linguistic details that are unimportant to people external to the projects.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence describes the NLP efforts and technical details, which relate to explanation and interpretation."
The paper is mainly an empirical analysis of a different way of formulating an existing problem.,Does the review address Analysis?,TRUE,FALSE,"This sentence states that the paper is an empirical analysis, which directly relates to the analysis aspect."
"The authors should make it clear that on different evaluation sets, the scores differs.",Does the review address Result?,TRUE,FALSE,"This sentence discusses the scores on different evaluation sets, which relates to the results."
The paper pointed out that ELECTRA framework [1] explored the idea of using REINFORCE [2] as the the way of adding adversarial training signals to the model but observed degenerated results.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes the use of REINFORCE and its results, which relates to the methodology."
The authors should discuss the relationship with other in-context example selection methods and compare the performance.,Does the review address Comparison?,TRUE,FALSE,"This sentence suggests discussing and comparing different methods, which relates to comparison."
The idea that combining the output of several models using the attention strategy is not novel in deep learning.,Does the review address Novelty?,TRUE,FALSE,"This sentence mentions the lack of novelty in using the attention strategy, which relates to the novelty aspect."
"The claimed contributions include: 1) The proposed method is free from the common issue of diverging from human language, because it learns from the sentences sampled from the pre-trained LM.",Does the review address Contribution?,TRUE,FALSE,"This sentence lists the claimed contributions, which directly relates to the contribution aspect."
Experiments on LEGO and code interpretation task are done.,Does the review address Data/Task?,TRUE,FALSE,"This sentence mentions experiments, which relates to the data/task."
"For example, if for the baseline model, we also only use one data sample and apply different masks, will there be improvement?",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses using a single data sample, which relates to the data/task."
The bad: * Figure 1 is difficult to read and messy.,Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the readability of Figure 1, which relates to the presentation."
"However, the BERT analysis results provided in this paper should also be valuable to the community.",Does the review address Analysis?,TRUE,FALSE,"This sentence states the value of the BERT analysis results, which relates to the analysis."
The training procedure mentioned in section 5.2.2 talks about joint training but the procedure followed for training for individual tasks or a subset of tasks is not described in detail.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the training procedure, which relates to the data/task."
- Experiments are well-designed: many baselines are implemented to compare proposed method with traditional lifelong learning methods.,Does the review address Comparison?,TRUE,FALSE,"This sentence mentions the comparison of methods, which relates to comparison."
Did the authors try this as another compared baseline?,Does the review address Comparison?,TRUE,FALSE,"This sentence asks about trying another baseline, which relates to comparison."
Ad-hoc regularization parameter selection is necessary for getting performance gains.,Does the review address Result?,TRUE,FALSE,"This sentence discusses parameter selection for performance gains, which relates to the result."
- Extensive results show improvements over a base model and a larger model across a range of tasks.,Does the review address Methodology?,TRUE,FALSE,"This sentence mentions improvements across tasks, which relates to the methodology."
The proposed ConceptQA architecture is intuitive and quite straightforward.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes the ConceptQA architecture, which relates to the methodology."
The task opens up a huge space for AI-powered general audio creation.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the task's impact, which relates to the data/task."
What recommendation the authors would give for those interested in using it?,Does the review address Contribution?,TRUE,FALSE,"This sentence asks for recommendations, which relates to the contribution aspect."
I think if the paper will be improved if it resolves the lack of clarity around the temperature in GS being an implicit entropy-regularization parameter.,Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the clarity of the paper, which relates to the presentation."
Each task is supported with a detailed ablation study to shed light on future research.,Does the review address Ablation?,TRUE,FALSE,"This sentence mentions ablation studies, which relates to the ablation aspect."
"Figure 1 is presently not pleasant to look at, even though it has interesting results`!",Does the review address Result?,TRUE,FALSE,"This sentence mentions the interesting results, which relates to the result."
Users cannot follow their strategy to set hyper-parameters in an optimal way.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the strategy for setting hyper-parameters, which relates to the methodology."
"Is it possible to split Thai words into syllables without any ambiguity, or do you need a heuristic?",Does the review address Methodology?,TRUE,FALSE,"This sentence questions the method for splitting Thai words, which relates to the methodology."
"Rather than using the ad-hoc approach for selecting which augmentation ""stacking"" scheme is helpful, it would have been better to compare/use an approach highlighted in ""Learning to Compose Domain-Specific Transformations for Data Augmentation"" [NeuRIPS 2017].",Does the review address Comparison?,TRUE,FALSE,"This sentence suggests comparing augmentation schemes, which relates to comparison."
"These prior methods do not incorporate the SFT stage, making it unclear how the model performs before this crucial phase.",Does the review address Presentation?,TRUE,FALSE,"This sentence discusses the lack of clarity regarding the model's performance, which relates to presentation."
"Why do you choose case-insensitive BLEU score for En->Fr, which is not commonly used in previous baselines.",Does the review address Comparison?,TRUE,FALSE,"This sentence questions the choice of BLEU score, which relates to comparison."
"In the experimental sections, it seems that only two scales are used.",Does the review address Experiment?,TRUE,FALSE,This sentence discusses the scales used in experiments.
"In terms of the experiments, there are only dev results reported on GLUE (Table 2).",Does the review address Result?,TRUE,FALSE,This sentence mentions the results reported in experiments.
"Placing Figure 1 and Table 1 on page 1 would improve readability, given that the main content describing Figure 1 and Table 1 is in the first page.",Does the review address Presentation?,TRUE,FALSE,"This sentence suggests improving the readability by changing the placement of figures, which relates to presentation."
"Revisiting GNN for Question Answering](https://openreview.net/forum?id=hzmQ4wOnSb), whose hypothesis seems to be that by only using embeddings for node types and relation types, the models are able to attain good performance (86.67 acc on OpenBookQA) without needing any cross-modal information.",Does the review address Result?,TRUE,FALSE,"This sentence mentions the performance results, which relates to results."
(And the comparison to their BERT_base-based model yields roughly equal scores.),Does the review address Comparison?,TRUE,FALSE,"This sentence compares the performance of models, which relates to comparison."
"**Pros**  - The paper is well structured and easy to follow, the idea of modeling sentences to a Brownian bridge latent space is neat and generic enough to (1) allow for noise given its stochasticity (2) doesn't require explicit domain knowledge for planning.",Does the review address Presentation?,TRUE,FALSE,"This sentence praises the structure and clarity of the paper, which relates to presentation."
"- OpenAQA-5M is a good contribution to provide open-ended question answering in audio domain, especially it is verified with human evaluation.",Does the review address Evaluation?,TRUE,FALSE,"This sentence mentions the evaluation of OpenAQA-5M, which relates to evaluation."
"A qualitative analysis was required in Karamcheti et al., (2021) to explain “collective outliers”.",Does the review address Analysis?,TRUE,FALSE,"This sentence mentions the need for qualitative analysis, which relates to analysis."
"Result 2: The authors further extend this result to word embedding features, which are obtained by a weighted average of word embedding vectors based on the next word distributions.",Does the review address Result?,TRUE,FALSE,This sentence mentions results related to word embedding features.
"However, they only mention a single pruning rate of 50%, which could easily over-prune (all challenging as well as outlier examples) for the NLP datasets used here.",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses pruning rates and dataset characteristics, which relates to data/task."
But a direct head to head comparison for the computational cost of a multi-task model and individual models is not provided.,Does the review address Methodology?,TRUE,FALSE,"This sentence mentions the lack of comparison in computational cost, which relates to methodology."
The authors should discuss the relationship with other in-context example selection methods and compare the performance.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence suggests discussing and comparing methods, which relates to explanation and interpretation."
"Within the evaluation section, there are numerous intriguing findings that hold significant value for dissemination within the wider research community.",Does the review address Evaluation?,TRUE,FALSE,"This sentence mentions the evaluation findings, which relates to evaluation."
"The primary contribution is an application of supervised, structured neural network models for sentence-level event/relation extraction.",Does the review address Methodology?,TRUE,FALSE,"This sentence mentions the application of neural network models, which relates to methodology."
The key contribution of the paper appears to be the formulation of the Concept-QA model based on query information and answers from GPT + CLIP.,Does the review address Contribution?,TRUE,FALSE,"This sentence mentions the key contribution of the Concept-QA model, which relates to contribution."
* Additional general discussion and statistics studies are presented in the Appendix.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence mentions additional discussions and studies, which relates to explanation and interpretation."
"The authors experiment both with pre-training and fine-tuning of contextual models (BERT-{base,large}) and claim large reduction in training time, with reasonable loss in performance.",Does the review address Methodology?,TRUE,FALSE,"This sentence mentions the experimental setup and results, which relates to methodology."
"I thought that this paper was very thought-provoking, and I appreciated the attempts to better understand what is going on with pre-trained language models, why they work well, and what might we be able to improve from theses insights.",Does the review address Methodology?,TRUE,FALSE,"This sentence praises the understanding of pre-trained language models, which relates to methodology."
Will need some clarification to better judge the results.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence asks for clarification to better understand the results, which relates to explanation and interpretation."
The proposed method is simple and relatively straightforward to implement.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes the proposed method, which relates to methodology."
"However, Section 3.1 cannot demonstrate effective theoretical information.",Does the review address Analysis?,TRUE,FALSE,"This sentence mentions the lack of theoretical information, which relates to analysis."
"-----Weaknesses----- I'm not very convinced by the empirical results, mostly due to the lack of details of the baselines.",Does the review address Comparison?,TRUE,FALSE,"This sentence mentions the lack of details in baselines, which relates to comparison."
"I look forward to seeing the authors discuss a comprehensive comparison of DeFo's training time and other methods, such as CoOp and CLIP-adapter.",Does the review address Methodology?,TRUE,FALSE,"This sentence suggests discussing training time comparison, which relates to methodology."
Why are all publications not used in training the baselines?,Does the review address Methodology?,TRUE,FALSE,"This sentence questions the training data for baselines, which relates to methodology."
* The proposed method RandomMask achieves SOTA on various downstream tasks.,Does the review address Data/Task?,TRUE,FALSE,"This sentence mentions the achievements of RandomMask, which relates to data/task."
Compilers tend to lower a representation of a software program into something that is closer to the hardware and therefore potentially more efficient.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence explains the role of compilers, which relates to explanation and interpretation."
"### Major issue 1 The paper ""**theoretically**"" analyzes the hyper-parameter selection process in Section 3.1 and provides experimental validation in Section 5.1.",Does the review address Analysis?,TRUE,FALSE,"This sentence mentions the theoretical and experimental analysis, which relates to analysis."
"Unfortunately, many aspects of the models, experimentation, and evaluation are not explained very well.",Does the review address Evaluation?,TRUE,FALSE,"This sentence mentions the lack of explanation in models, experimentation, and evaluation, which relates to evaluation."
I suspect that a statistical analysis [1] might conclude that BERT and the proposed method are indistinguishable on the GLUE suite.,Does the review address Result?,TRUE,FALSE,"This sentence mentions the statistical analysis and results, which relates to results."
It is a bit difficult to understand the task definitions without first understanding the various constituents of the proof.,Does the review address Data/Task?,TRUE,FALSE,"This sentence mentions the difficulty in understanding task definitions, which relates to data/task."
Maybe adding another ablation on this would be a good idea.,Does the review address Ablation?,TRUE,FALSE,"This sentence suggests adding an ablation study, which relates to ablation."
This makes their empirical results extremely weak.,Does the review address Result?,TRUE,FALSE,"This sentence mentions the weakness of empirical results, which relates to results."
"The experiments in the paper demonstrated the superiority of adding adversarial training to a self-supervision framework, in that significant improvements can be obtained for similar-sized networks.",Does the review address Result?,TRUE,FALSE,This sentence discusses the results showing significant improvements with adversarial training.
Their findings on learning complex tasks contribute to the understanding of large language model learning and provide valuable insights for future related work on efficient training.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological findings on learning complex tasks.
"As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.",Does the review address Presentation?,TRUE,FALSE,"This sentence suggests assessing the use of different representations, indicating Presentation."
Such a comparison would highlight the advantages of the multi-task model and would be helpful for the relevant audience.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the advantages of the multi-task model, indicating Data/Task."
it doesn't seem straightforward for me to use constituent parse as knowledge here.,Does the review address Methodology?,TRUE,FALSE,"This sentence questions the use of constituent parse as knowledge, indicating Methodology."
"This idea is novel and interesting to me, and the derivation and experiment results look encouraging.",Does the review address Novelty?,TRUE,FALSE,This sentence discusses the novelty and encouraging results of the idea.
Weaknesses: - Somewhat weaker results on some CommonGen metrics are disappointing.,Does the review address Evaluation?,TRUE,FALSE,"This sentence critiques the weaker results on some metrics, indicating Evaluation."
"The authors investigate L=12 in the ablation, but in a real setting, it seems unlikely practitioners will label <50 examples before re-training.",Does the review address Ablation?,TRUE,FALSE,"This sentence discusses the ablation study related to labeling examples, indicating Ablation."
"The paper could make more of an advocacy point for what relatively modest funding could do for languages in places where leaders have not yet had the same impetuses as witnessed in Canada, including India and Africa where ""minority"" language is often a misnomer.",Does the review address Significance?,TRUE,FALSE,"This sentence discusses the potential impact of modest funding for languages, indicating Significance."
Theoretical discussion proves that the gradients derived from the new masking schema have a smaller variance and can lead to more efficient self-supervised training.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the theoretical discussion related to gradients and training efficiency, indicating Methodology."
"> However, in every task except CommonGEN the authors do not discuss any methods that are even close to the state of the art.",Does the review address Data/Task?,TRUE,FALSE,"This sentence critiques the lack of discussion on state-of-the-art methods, indicating Data/Task."
The authors clearly present their ideas and describe the technical details.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence praises the clarity of idea presentation and technical details, indicating Explanation."
"Some things that are worth looking into are the work on Scalable static analysis [Scaling], the inference of necessary preconditions [Logozzo], and bug detection that is based on ""belief"" [deviant, belief], which is closely related to your intuition about naturalness and human-written invariants.",Does the review address Related Work?,TRUE,FALSE,"This sentence suggests related work to look into, indicating Related Work."
There are few places (see details) that authors have assumptions in mind but do not provide those assumptions until later.,Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the delay in providing assumptions, indicating Methodology."
The findings of (2)(3)(4)(5) mentioned in the summary section above are especially interesting and can be helpful to the community when comparing new VL methods with existing methods.,Does the review address Comparison?,TRUE,FALSE,"This sentence discusses the helpful findings for method comparison, indicating Comparison."
"- If so, please be more specific in describing it in section 4.2 and Table 4.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence suggests being more specific in descriptions, indicating Explanation."
Empirical results on MultiWoz 2 and 2.1 shows improvement over other state-of-the-art techniques.,Does the review address Result?,TRUE,FALSE,"This sentence discusses the empirical results showing improvement, indicating Result."
"Even, Ref-[2] can be a strong baseline to compare the performance of the current system.",Does the review address Comparison?,TRUE,FALSE,"This sentence suggests using a strong baseline for comparison, indicating Comparison."
"The paper said: “the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples.”  Acutally it seems quite natural for me and I did not realize it is a problem until I saw more explanations in section 1.1.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence discusses the explanation related to pretrained NLM, indicating Explanation."
It does not follow from the theoretical results that adding similar sentences will be a good thing.,Does the review address Result?,TRUE,FALSE,"This sentence critiques the theoretical results related to adding similar sentences, indicating Result."
I would also urge the authors to have a speculative discussion on what successful inductive biases might look like.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence suggests having a discussion on inductive biases, indicating Explanation."
The experimental results are promising for both settings.,Does the review address Experiment?,TRUE,FALSE,"This sentence discusses the promising experimental results, indicating Experiment."
"It is widely acknowledged and studied that the complexity (i.e., the length or reasoning steps of the CoT annotations) significantly influences the performance of the LLMs.",Does the review address Result?,TRUE,FALSE,"This sentence discusses the impact of complexity on performance, indicating Result."
"Typos: 1. compared with Enhanced baseline… -> Comparison with enhanced baseline   ** Refereces **  R1: Zhu, Jinhua, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu.",Does the review address Presentation?,TRUE,FALSE,"This sentence points out typos and suggests corrections, indicating Presentation."
The experimental results on RoBERTa highlight the applicability and importance of this data augmentation approach on the downstream task of text classification (GLUE).,Does the review address Result?,TRUE,FALSE,"This sentence discusses the results highlighting the importance of data augmentation, indicating Result."
Do the authors expect better results with a larger vocab size?,Does the review address Result?,TRUE,FALSE,"This sentence questions the potential impact of a larger vocab size on results, indicating Result."
"For example, Figure 4 validates Assumption 4.1 (log-partition function is roughly quadratic in theta), and Table 1 shows many real tasks are approximately “natural”.",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the validation of assumptions and natural tasks, indicating Data/Task."
"There are various weaknesses of the MID data, but the evaluation approach needs to be discussed or justified.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence suggests discussing or justifying the evaluation approach, indicating Validation."
- Strengths: The paper is well organized and clearly written.,Does the review address Presentation?,TRUE,FALSE,"This sentence praises the organization and clarity of the paper, indicating Presentation."
Can the proposed theory help explain some of the successes of one architecture over others?,Does the review address Theory?,TRUE,FALSE,"This sentence questions the explanatory power of the proposed theory, indicating Theory."
"In particular, it considers language models which compute a probability distribution over the next word in a text, given the previous context.",Does the review address Methodology?,TRUE,FALSE,"This sentence describes the methodological aspect of language models, indicating Methodology."
It seems that the authors have the infrastructure for computing single-model test-set results.,Does the review address Result?,TRUE,FALSE,"This sentence discusses the infrastructure for computing test-set results, indicating Result."
1.The performance of this model is closely related to both the AST encoding frontend and the LLaMA model's performance.,Does the review address Methodology?,TRUE,FALSE,This sentence discusses the methodological aspect related to AST encoding and LLaMA model's performance.
"However, if the video is quite long, say 10 minutes, 30 minutes, or even few hours, will the method still be efficient and effective?",Does the review address Methodology?,TRUE,FALSE,"This sentence questions the efficiency and effectiveness of the method for long videos, indicating Methodology."
"I'm concerned whether these improvements will hold after optimizing BERT carefully like RoBERTa, or using more advanced backbone methods like ALBERT.",Does the review address Methodology?,TRUE,FALSE,"This sentence expresses concern about the improvements holding with different optimizations, indicating Methodology."
Cluster embeddings are then obtained which serve as embeddings for the words that reside in each cluster.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes the process of obtaining cluster embeddings, indicating Methodology."
"The proposed method represents a novel approach to multimodal techniques, distinguishing itself from previous Vision-Language Models (VLMs) like Flamingo and PaLI.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the novel approach of the proposed method, indicating Methodology."
"However, there are unclear parts to be addressed or clarified.",Does the review address Presentation?,TRUE,FALSE,"This sentence points out unclear parts that need to be addressed, indicating Presentation."
"* Extensive empirical results and analysis, providing some findings about overlapping strategy in DNA tokenization, could benefit the community.",Does the review address Analysis?,TRUE,FALSE,"This sentence discusses the findings from empirical results and analysis, indicating Analysis."
"After reading other reviews and the authors’ responses to all of the reviewers, I recommend this paper by accepted—extensive results show that the CALM objectives offer more signal from data than current pretraining methods.",Does the review address Methodology?,TRUE,FALSE,"This sentence recommends the paper based on extensive results, indicating Methodology."
The paper proposes a  HRL/options framework based method to learn a dialog policy over learned latent dialog acts which can then guide the lower level NLG.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes the proposed HRL/options framework method, indicating Methodology."
"I still feel that the authors’ use of “concept” and “commonsense” is vague, when their method can be defined more clearly with more mundane terminology.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence critiques the vague use of terms, indicating Explanation."
"The authors empirically evaluate their N-Bref’s accuracy on a number of problems from the open source LeetCode problem set and generate 25,000 pairs of high-level source and low-level source which are broken into training (60%), validation (20%), and testing (20%).",Does the review address Evaluation?,TRUE,FALSE,"This sentence describes the empirical evaluation process, indicating Evaluation."
"Some of the details for model description are missing and confusing, e.g., (1) How the representation is initialized for a supernode corresponding to a fact triplet and how does the supernode propagate information to the sub-nodes?",Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the missing and confusing details for model description, indicating Methodology."
Will need some clarification to better judge the results.,Does the review address Result?,TRUE,FALSE,"This sentence suggests that clarification is needed to better judge the results, indicating Result."
"* Even though this paper proposes a new efficient transformer, the evaluation does not focus on computational efficiency aspects and comes across as incomplete.",Does the review address Evaluation?,TRUE,FALSE,"This sentence critiques the evaluation for not focusing on computational efficiency, indicating Evaluation."
"The strongest result is the HANS ""lexical_overlap"" case, where the proposed method has a clear advantage.",Does the review address Result?,TRUE,FALSE,"This sentence discusses the strongest result, indicating Result."
"For instance, they ask how different amounts of data influence model behavior or if their data sampling method can react to task difficulty.",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the influence of different amounts of data on model behavior, indicating Data/Task."
"Then it solves two tasks with two network branches: the first branch minimizes the loss for NER, and the second branch minimizes the loss for RE.",Does the review address Data/Task?,TRUE,FALSE,"This sentence describes the task-solving process with two network branches, indicating Data/Task."
And the results of the proposed model can also be available for downstream detection models.,Does the review address Result?,TRUE,FALSE,"This sentence discusses the applicability of the results to downstream detection models, indicating Result."
I highly recommend bringing this assumption earlier to avoid readers confusion.,Does the review address Methodology?,TRUE,FALSE,"This sentence suggests bringing an assumption earlier in the paper, indicating Methodology."
"Minor: Only half the datasets are shown in Tables 3 and 4, but it’s unclear how/why those were chosen?",Does the review address Data/Task?,TRUE,FALSE,"This sentence questions the selection of datasets shown in tables, indicating Data/Task."
"- Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train.",Does the review address Presentation?,TRUE,FALSE,This sentence discusses the contributions related to ablation study and presentation.
The results on Split H are positive and they also conducted a range of ablation and error analysis.,Does the review address Analysis?,TRUE,FALSE,"This sentence discusses the positive results and conducted analyses, indicating Analysis."
I'd like to see another ablation study of whether RE helps NER.,Does the review address Ablation?,TRUE,FALSE,"This sentence suggests conducting another ablation study, indicating Ablation."
This is the newest of a small but growing body of literature that seeks to connect emergent communication with genuine NLP tasks.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the literature connecting emergent communication with NLP tasks, indicating Data/Task."
(ii) a new way to aggregate multiple inputs (using M-BERT) and several different decoding methods.,Does the review address Novelty?,TRUE,FALSE,"This sentence describes a new method, indicating Novelty."
"For table 4, please also include the significance of the BLEU improvement made by the pRNN with respect to the the baseline, see https://github.com/jhclark/multeval General Discussion ==== As the main contribution of this work is on the phrasal effect of the new RNN architecture, it's rather important to show that the phrases are more coherent than the vanilla LSTM / RNN model.",Does the review address Significance?,TRUE,FALSE,"This sentence suggests including the significance of improvement, indicating Significance."
It will be great if authors can justify more on the technical novelty.,Does the review address Novelty?,TRUE,FALSE,"This sentence suggests justifying the technical novelty, indicating Novelty."
(2) The new proposed model in this paper has achieved better results than the previous work in many tasks on MSMARCO.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the improved results on tasks, indicating Data/Task."
"The paper is clear and detailed, and well situated in the literature.",Does the review address Presentation?,TRUE,FALSE,"This sentence praises the clarity and detail of the paper, indicating Presentation."
Definition of Variables and Positional Encodings:  The paper would greatly benefit from a more detailed explanation of the positional encodings used.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence suggests providing a more detailed explanation, indicating Explanation."
The paper then uses an existing result from Liao & Berg (2017) to show that the gap can be bounded by the variance of prediction probability.,Does the review address Result?,TRUE,FALSE,"This sentence references an existing result to support the paper's findings, which is part of the ""Result"" aspect."
"Based on the large-scale training data and the proposed visual expert module, the proposed method achieves a number of state-of-the-art results across a wide range of vision-language tasks.",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the method's performance across various vision-language tasks, thus mentioning ""Data/Task""."
"However, it is confusing to me how this interacts with the prior results in the paper.",Does the review address Result?,TRUE,FALSE,"This sentence mentions confusion about how the results fit with previous ones, which relates to the ""Result"" aspect."
A figure containing the whole process could be helpful to better understand the processing required to train / test such models.,Does the review address Methodology?,TRUE,FALSE,"This sentence suggests improving the methodology presentation through better visual aids, thus referencing ""Methodology""."
"1.The paper introduces for the first time a large language model that combines both general audio perception capabilities and language reasoning abilities, along with the datasets used for training.",Does the review address Methodology?,TRUE,FALSE,"This sentence introduces the new methodology and datasets used in the paper, thus addressing ""Methodology""."
"They do pre-train their model (BROS) on a large dataset with 11M documents, and then used such models to perform downstream tasks in four smaller datasets.",Does the review address Methodology?,TRUE,FALSE,This sentence explains the methodology of pre-training and applying models on various datasets.
"So, it would be better to compare this system, which also captures semantics.",Does the review address Comparison?,TRUE,FALSE,"This sentence suggests a comparison with a system that captures semantics, addressing ""Comparison""."
I can not understand why sample-specific rather than task-specific preference is important for prompt tuning.,Does the review address Related Work?,TRUE,FALSE,This sentence reflects confusion about task-specific versus sample-specific preference in the context of related work.
"Positives --------- Increasing the context length of transformers is an interesting and relevant topic, and the proposed solution can have real impact in moving the state of the art forward.",Does the review address Significance?,TRUE,FALSE,This sentence discusses the significance of the proposed solution in advancing the state of the art.
The embedding methods are:  (1) multiCluster : Uses a dictionary to map words to multilingual clusters.,Does the review address Methodology?,TRUE,FALSE,"This sentence details the methodology behind the ""multiCluster"" embedding method."
"Also, the lack of attention mechanism provides a disadvantage to the baseline enc-dec system and it's unclear whether the pRNN can outperform or be an additive feature to the enc-dec system with an attention mechanism.",Does the review address Methodology?,TRUE,FALSE,This sentence critiques the methodology of the baseline system and questions the pRNN's role in comparison.
The auxiliary tasks themselves will be useful for designing similar tasks for other theorem provers.,Does the review address Data/Task?,TRUE,FALSE,"This sentence suggests the usefulness of auxiliary tasks for designing future tasks, addressing ""Data/Task""."
- Ablation study shown in table 5 provides good insights for choices of LoRA params and the benefit of curriculum in staged training.,Does the review address Presentation?,TRUE,FALSE,"This sentence mentions an ablation study in Table 5, focusing on its presentation of insights into LoRA parameters."
"Empirically, it demonstrates that several NLP tasks are “natural”.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence presents empirical evidence that justifies the assumption that several NLP tasks are ""natural""."
Weaknesses * Something that stands out is the lack of discussion and comparison to related works that employ recurrent formulations of attention.,Does the review address Comparison?,TRUE,FALSE,This sentence critiques the lack of comparison to related works with recurrent formulations of attention.
- General Discussion: The authors perform relation extraction as reading comprehension.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This sentence provides a discussion on the relationship between relation extraction and reading comprehension.
"Moreover, the authors better answer questions in 2 so I can gauge if their hyper-parameters were chosen in the principled ways.",Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the clarity of the authors' answers about hyper-parameters, suggesting improvements in presentation."
"### Major issue 1 The paper ""**theoretically**"" analyzes the hyper-parameter selection process in Section 3.1 and provides experimental validation in Section 5.1.",Does the review address Theory?,TRUE,FALSE,"This sentence mentions the theoretical analysis and experimental validation of hyper-parameter selection, addressing ""Theory""."
"In the ablation study, the authors only consider the fixed \alpha as the base.",Does the review address Ablation?,TRUE,FALSE,"This sentence critiques the ablation study's focus on a fixed α\alpha, addressing ""Ablation""."
It also compares different subword representation strategies and finds that syllable representations perform best (when not using BERT).,Does the review address Comparison?,TRUE,FALSE,This sentence mentions a comparison of different subword representation strategies.
"The motivation is to leverage unimodal data, which are assumed easier to obtain than image-text pairs.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,This sentence explains the motivation behind leveraging unimodal data instead of image-text pairs.
####Pros:  The paper proposes the first RNN based neural generator to perform data augmentation for “extreme” compositionality inference.,Does the review address Data/Task?,TRUE,FALSE,"This sentence highlights the novel RNN-based neural generator, addressing ""Data/Task""."
I am also wondering if the extracted facts could bring too much noise to the question.,Does the review address Methodology?,TRUE,FALSE,This sentence questions the methodology behind fact extraction and its potential impact on the task.
"(In particular, pre-training on emergent language performs on average better than synthetic hierarchical data, but not quite as well as a different natural language, and all of these pre-training methods do better than training from scratch.)",Does the review address Data/Task?,TRUE,FALSE,"This sentence compares different pre-training methods and discusses their performance, addressing ""Data/Task""."
"The authors argued that most of the previous multimodal LLMs used shallow connections between vision and models, and thus proposed a new module called visual expert.",Does the review address Methodology?,TRUE,FALSE,"This sentence critiques prior multimodal LLMs and introduces the ""visual expert"" module, addressing ""Methodology""."
"If the authors would like to compare the number of additional parameters of DeFo with CoOp and CLIP-adapter, I think it may be very helpful for us to comprehensively evaluate and compare these methods.",Does the review address Methodology?,TRUE,FALSE,"This sentence suggests a comparison of additional parameters between different methods, addressing ""Methodology""."
The description of how you compare your invariants to those inferred by Daikon is not clear unless all relevant cases related to (pre)conditions on method parameters.,Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the clarity of the description on how invariants are compared, addressing ""Methodology""."
These papers and other methods for contact prediction beyond Gremlin are not described.,Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the lack of description of alternative methods beyond Gremlin, relating to ""Methodology""."
"The true contribution appears to be the improvement of the overlapping strategy tokenization for DNA pretraining, which diverges from the broader theme of ""rethinking the pretraining for DNA sequence.""",Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the method's true contribution, focusing on the tokenization strategy for DNA pretraining."
"A significant part of the contribution was in the analysis of the results, obtained by this learning-based parameter sharing approach, which was quite informative and revealed some interesting insights about where and when a language-specific computation is required.",Does the review address Result?,TRUE,FALSE,"This sentence discusses the contribution in terms of result analysis, addressing ""Result""."
**Reproducibility** I believe reproducing the results is possible given the clear description provided in the main paper and the appendix.,Does the review address Presentation?,TRUE,FALSE,"This sentence mentions the clarity of the description in the main paper and appendix, which relates to ""Presentation""."
"The layers are described in a textual fashion, barely any math (and extended in the pseudo-code).",Does the review address Presentation?,TRUE,FALSE,"This sentence critiques how the layers are described (mostly text, minimal math), referring to ""Presentation""."
"Given the way p(guess=Ii) is used above, I think this should be more like E[argmax(p(guess=Ii)) = i].",Does the review address Result?,TRUE,FALSE,"This sentence suggests a mathematical modification to the result, which addresses ""Result""."
It is then not clear how each fact block (grey squares in Figure 4) functions as a whole.,Does the review address Methodology?,TRUE,FALSE,This sentence critiques the clarity of how the fact blocks function within the methodology.
"- The paper uses code clone detection and semantic labeling to motivate their theory, but the theory focuses on characterizing language tractability.",Does the review address Theory?,TRUE,FALSE,"This sentence discusses the theory used in the paper, addressing ""Theory""."
Experimental results show improvements over both the base T5 model and the large T5 model.,Does the review address Experiment?,TRUE,FALSE,"This sentence mentions the experimental results, comparing performance improvements over models, relating to ""Experiment""."
"Additionally, there are some minor things I would add or improve: - I would add references to multi-task training on different languages (e.g., Task 1 is translation from EN to FR and Task 2 is translation from EN to DE).",Does the review address Result?,TRUE,FALSE,"This sentence suggests adding references for multi-task training, which addresses the ""Result"" aspect."
"Additionally, the authors conclude that pre-training is not a significant factor in the efficacy of active learning, but their numerical results suggest active learning methods (Entropy and Coreset) narrow the gap with the random baseline significantly from BERT-Base to RoBERTa-Base!",Does the review address Result?,TRUE,FALSE,"This sentence discusses the results of active learning methods and their effectiveness, which is part of ""Result""."
"If these method can also achieve very good results, then I feel that the novelty and effectiveness of DeFo may be challenged.",Does the review address Methodology?,TRUE,FALSE,"This sentence questions the novelty and effectiveness of a method, addressing ""Methodology""."
"Even if pruning is ineffective (at multiple pruning rates), it’s never really explained why, despite this being a core contribution of the paper.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence points out a lack of explanation for pruning's ineffectiveness, which is part of the ""Definition/Discussion"" aspect."
"Strengths: The paper presents an interesting idea for Multiple-Choice Question-Answering (using the answer symbol instead of the answer itself), motivates the idea well and does a thorough analysis over multiple datasets, and LLMs to analyze its performance in different settings (including few-shot settings).",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the idea for MCQA and its performance over multiple datasets, addressing ""Data/Task""."
Section 3.4  does not describe clearly enough how attention maps were used for predicting contact maps.,Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the unclear description of how attention maps are used, which is related to ""Presentation""."
"Regarding that CLIP does not release the 400M dataset (mentioned in this paper by the authors), the authors may not be able to train OTTER on the 400M dataset.",Does the review address Methodology?,TRUE,FALSE,"This sentence raises a concern about the methodology based on dataset availability, addressing ""Methodology""."
- The authors perform quite a lot of ablation studies.,Does the review address Ablation?,TRUE,FALSE,"This sentence mentions the use of ablation studies, addressing ""Ablation""."
Why don't the authors of this work do this evaluation as well?,Does the review address Evaluation?,TRUE,FALSE,"This sentence critiques the evaluation process, suggesting it should be included, which relates to ""Evaluation""."
The provided experiments are more like baselines for self-evaluation other than state-of-the-art performance.,Does the review address Experiment?,TRUE,FALSE,"This sentence critiques the experimental results as being more like baselines than state-of-the-art, related to ""Experiment""."
"For example, the paper https://arxiv.org/abs/1505.06798 also goes beyond simply considering the reconstruction objective on the weights, and includes the nonlinearity as well in the reconstruction objective.",Does the review address Related Work?,TRUE,FALSE,"This sentence references another paper that goes beyond the reconstruction objective, addressing ""Related Work""."
"- Proposition 3.1 is a trivial consequence of the basic theorems of DP, and Algorithm 1 is a simple modification of DP-SGD (that is already available in standard DP-training libraries).",Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the triviality of Proposition 3.1 and Algorithm 1, addressing the ""Methodology"" aspect."
*  The evaluation focuses on comparing with an empirical law learned on a different experimental configuration and there is a concern about how comparable are the results to the ones obtained in this study and the validity of the conclusions.,Does the review address Comparison?,TRUE,FALSE,"This sentence critiques the comparison with an empirical law and questions the validity of the conclusions, addressing ""Comparison""."
"Negatives --------- The experiments do not compare to many other approaches, even though those approaches are cited throughout the paper.",Does the review address Comparison?,TRUE,FALSE,"This sentence critiques the lack of comparison to other approaches, addressing ""Comparison""."
"Contributions: - A new algorithm for unsupervised knowledge graph creation from a target corpus - Demonstrating the utility of large pre-trained language models towards knowledge graph creation (though, there are other works in this area that should probably be discussed more.",Does the review address Contribution?,TRUE,FALSE,"This sentence summarizes the paper's contributions, addressing the ""Contribution"" aspect."
"It's important to discuss why these improvements are non-trivial and how they advance the field, considering the rapidly evolving landscape of both quantum computing and graph analysis.",Does the review address Analysis?,TRUE,FALSE,"This sentence emphasizes the importance of discussing the non-trivial nature of improvements, addressing ""Analysis""."
"It would help to clarify when what you predict is a guard, a precondition, an invariant, or something else.",Does the review address Result?,TRUE,FALSE,"This sentence suggests clarifying the types of predictions made, relating to ""Result""."
- Weaknesses: Many points are not explained well in the paper.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence critiques the lack of explanation for many points in the paper, addressing ""Definition/Discussion""."
"- Extensive results and discussions are put in the Appendix, costing great effort in going back and forth.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence critiques the presentation of results and discussions in the appendix, which affects ""Definition/Discussion""."
More discussions on comparing with symbolic logic reasoner model LReasoner are needed.,Does the review address Comparison?,TRUE,FALSE,"This sentence emphasizes the need for more discussions comparing the proposed work with a symbolic logic reasoner, addressing ""Comparison""."
"* Extensive empirical results and analysis, providing some findings about overlapping strategy in DNA tokenization, could benefit the community.",Does the review address Result?,TRUE,FALSE,"This sentence discusses empirical findings related to DNA tokenization, addressing ""Result""."
"For the theoretical analysis, it was not clear to me what is the contribution of the current analysis compared to the Levine 2020 paper.",Does the review address Analysis?,TRUE,FALSE,"This sentence raises a concern about the contribution of the theoretical analysis, addressing ""Analysis""."
Case studies show that the method can help improve training efficiency.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses how case studies illustrate the method’s ability to improve training efficiency, addressing ""Methodology""."
"- reasonable initial experimental results demonstrating some ways to help models better use cross-text-chunk dependencies (put them into a contiguous text chunk), providing some hope that these results could make models better.",Does the review address Result?,TRUE,FALSE,"This sentence describes initial experimental results showing promise, relating to ""Result""."
This work tries to address the issue by proposing a technique that carefully amalgamates multiple previously known approaches to generate diverse label preserving examples.,Does the review address Methodology?,TRUE,FALSE,"This sentence explains the proposed technique combining multiple methods, addressing ""Methodology""."
Is the speedup over total computational time or just the attention part?,Does the review address Methodology?,TRUE,FALSE,"This sentence questions the specifics of the speedup in terms of computation, addressing ""Methodology""."
(I assumed authors used the same strategy as LayoutLM).,Does the review address Comparison?,TRUE,FALSE,"This sentence assumes that the authors used the same strategy as LayoutLM, addressing ""Comparison""."
"(2) In this paper, similarity and alignment structures are proposed, but no ablation experiments have been carried out to prove the effectiveness of the improved model.",Does the review address Ablation?,TRUE,FALSE,"This sentence critiques the lack of ablation experiments to verify the effectiveness of the proposed model, addressing ""Ablation""."
"Terms such as θ, t, δ, and especially the adjacency matrix A, which are crucial for understanding the method, require clear definitions and contextual usage within the proposed quantum framework.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence points out the need for clear definitions and contextual usage of important terms, addressing ""Definition/Description""."
"The two datasets have some “toy flavor”, while SCAN great favors example combination (with recomb-2 performs much better than recomb-1), recomb-1 seem to perform better for morphological analysis dataset, leaving questions about how to choose the exact models in general.",Does the review address Analysis?,TRUE,FALSE,"This sentence analyzes the performance of different models and raises questions about the model selection, addressing ""Analysis""."
The paper then does a good job of showing how using one-hot encodings compared to SCS change the problem definition leading to a difference in performance on the same task.,Does the review address Comparison?,TRUE,FALSE,"This sentence compares the effects of using one-hot encodings vs SCS on the problem definition and performance, addressing ""Comparison""."
- How will the pseudo data generation amount affect the learning / forgetting performance?,Does the review address Result?,TRUE,FALSE,"This sentence asks about the impact of pseudo data generation on learning and forgetting performance, relating to ""Result""."
This is the first such dataset for the Lean Theorem prover.,Does the review address Data/Task?,TRUE,FALSE,"This sentence introduces a dataset for the Lean Theorem prover, addressing ""Data/Task""."
"It is just a combination of the strong pretrained LLM and the existing audio encoder, AST.",Does the review address Methodology?,TRUE,FALSE,"This sentence summarizes the method as a combination of a pretrained LLM and an audio encoder, addressing ""Methodology""."
The visualization of OTTER’s matching illustrates its effectiveness in handling many-to-many relationships.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes how OTTER handles many-to-many relationships and its effectiveness, addressing ""Methodology""."
It would be impossible to replicate based on the two-line explanation here.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence critiques the insufficient explanation for replication, addressing ""Definition/Discussion""."
The authors should conduct experiments on more types of sentence pair tasks.,Does the review address Data/Task?,TRUE,FALSE,"This sentence suggests expanding experiments to include more types of sentence pair tasks, addressing ""Data/Task""."
The concept of fact units is interesting and novel which are easily constructed via dependency trees.,Does the review address Novelty?,TRUE,FALSE,"This sentence highlights the novelty of the fact units constructed via dependency trees, addressing ""Novelty""."
"Furthermore, when there is a gap between the empirical results and the theoretical results (e.g., validation of Lemma 4.3 at the end of Section 4), the paper makes these limitations clear, which I appreciated very much as a reader (paper does not over-claim its contributions).",Does the review address Presentation?,TRUE,FALSE,"This sentence appreciates the paper's transparency in acknowledging gaps between empirical and theoretical results, addressing ""Presentation""."
"I also believe that the downstream tasks are also somewhat similar (language command grounding, tappability, UI object detection, UI summarization, widget captioning).",Does the review address Data/Task?,TRUE,FALSE,"This sentence compares different downstream tasks to the current study’s tasks, addressing ""Data/Task""."
"I still feel that the authors’ use of “concept” and “commonsense” is vague, when their method can be defined more clearly with more mundane terminology.",Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the vague use of ""concept"" and ""commonsense"" and suggests clearer terminology, addressing ""Presentation""."
Otherwise for example it is not clear to me if the improvement in Blue compared to LaRL comes from the extra reward using the language model or from the options framework.,Does the review address Methodology?,TRUE,FALSE,"This sentence raises a methodological concern about what specifically contributed to the improvement, addressing ""Methodology""."
"However, they merely note that their data was annotated at the “relation” level rather than at the triple (relation, entity pair) level… but couldn’t Bordes et al. have done the same in their annotation?",Does the review address Data/Task?,TRUE,FALSE,"This sentence questions the annotation choice and compares it to prior work, addressing ""Data/Task""."
"Although the individual components are similar to previous work, they are combined in a novel way that shows a path toward longer and more efficient context lengths.",Does the review address Novelty?,TRUE,FALSE,"This sentence points out the novelty in how components are combined to address context length issues, addressing ""Novelty""."
"Running a baseline model that runs *for the same amount of time* is essential to appreciate the contribution of this work (e.g., repeat the same analysis in Figure 3 for the vanilla BERT).",Does the review address Presentation?,TRUE,FALSE,"This sentence emphasizes the importance of using a baseline for comparison, related to ""Presentation""."
The same applies to Table 3: it is unclear to me why or how the baseline T5 model has been chosen.,Does the review address Comparison?,TRUE,FALSE,"This sentence questions the choice of the baseline T5 model, addressing ""Comparison""."
"Summary:  The augmentation of NLP samples is an important task with no clear ""applicable to all"" mechanism.",Does the review address Data/Task?,TRUE,FALSE,"This sentence summarizes the task of NLP sample augmentation, addressing ""Data/Task""."
"- Extensive results and discussions are put in the Appendix, costing great effort in going back and forth.",Does the review address Result?,TRUE,FALSE,"This sentence discusses the extensive results and discussions in the appendix, addressing ""Result""."
(3) Are there any advantages of using the multi-Skip approach instead of learning bilingual embeddings and performing multi-CCA to learning projections across the distinct spaces?,Does the review address Methodology?,TRUE,FALSE,"This sentence questions the advantages of using the multi-Skip approach, addressing ""Methodology""."
The paper made a significant contribution to idea of using adversarial training as part of the self-supervision signal for language learning.,Does the review address Contribution?,TRUE,FALSE,"This sentence highlights a significant contribution related to adversarial training, addressing ""Contribution""."
Summary: + Appealing theoretical contributions + Empirical results are encouraging + The use of discriminator for reward shaping in addition to task success rate is interesting  - Writing and explanation can be improved.,Does the review address Theory?,TRUE,FALSE,"This sentence summarizes the theoretical contributions, addressing ""Theory""."
Clustering to find exemplar terms for keyphrase extraction.,Does the review address Related Work?,TRUE,FALSE,"This sentence mentions clustering for keyphrase extraction, addressing ""Related Work""."
"The second sentence is confusing to me, and I am a native English speaker.",Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the clarity of the second sentence in the paper, addressing ""Presentation""."
P5: Section 4.4: I am still eager to know how you select your dialog actions.,Does the review address Methodology?,TRUE,FALSE,"This sentence expresses interest in the methodology for selecting dialog actions, addressing ""Methodology""."
The performance is impressive and could be a better baseline for the future work.,Does the review address Comparison?,TRUE,FALSE,"This sentence mentions the comparison of the current performance to potential baselines, addressing ""Comparison""."
"Since they both use $m$ nodes, does it mean the supergraph also operates on each sub fact node?",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence seeks clarification on the structure of the supergraph, addressing ""Definition/Discussion""."
"The authors analyze the in-context bias of the self-attention model, which could inspire some research works on designing training examples.",Does the review address Methodology?,TRUE,FALSE,"This sentence describes the analysis of in-context bias in the self-attention model, which could inspire further work, addressing ""Methodology""."
"It may be useful to show how the performance changes when using different M. - According to [2], the CommonsenseQA IH-dev set contains 1,221 questions in total.",Does the review address Result?,TRUE,FALSE,"This sentence proposes showing the impact of different M on performance, relating to ""Result""."
* It is hypothesized that PACT acts a regularizer while imparting useful knowledge to the model due to mutual information across tasks.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes the hypothesis about PACT and its role in regularization, addressing ""Methodology""."
"- Well Structured Experiments sections with 4 RQs and results that confirm each of the hypotheses  - Reproducibility and Transparency in reporting of experiments in terms of available source code, dataset information, details about human evaluation, generation examples.",Does the review address Data/Task?,TRUE,FALSE,"This sentence highlights the structure and reproducibility of the experiments, addressing ""Data/Task""."
"\phi(x) as introduced in eq 2 is in R^{2D} but S_{t-1} is in R{D}, not sure what does + mean in this context.",Does the review address Presentation?,TRUE,FALSE,"This sentence seeks clarification on a mathematical expression, addressing ""Presentation""."
"- Strengths: Useful modeling contribution, and potentially useful annotated data, for an important problem -- event extraction for the relationships between countries as expressed in news text.",Does the review address Contribution?,TRUE,FALSE,"This sentence summarizes the contributions related to event extraction, addressing ""Contribution""."
All of the results in this work seem to be previously known: * Theorem 1 is general to any polynomial-size circuit -- there is nothing special about Transformers.,Does the review address Theory?,TRUE,FALSE,"This sentence critiques the novelty of the results, suggesting they are known and not unique to Transformers, addressing ""Theory""."
"Experiments show that the induced ""best-first"" order outperforms fixed orders, which verifies the motivation of the paper` 4.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence supports the paper's motivation by referencing experimental validation of the ""best-first"" order, addressing ""Motivation""."
"* ""Moreover, it should be noted that BROS achieves higher f1 score than 79.27 of LayoutLM using visual features"".",Does the review address Presentation?,TRUE,FALSE,"This sentence presents a comparison of BROS and LayoutLM with a reported performance metric, addressing ""Presentation""."
More discussion is required on a) what are the reasons of loss in comprehensibility in this case (it is briefly mentioned in the intro) b) why their individual design choices and how they handles the different reasons c) some evaluation to verify this 2.,Does the review address Evaluation?,TRUE,FALSE,"This sentence calls for further discussion and evaluation to clarify design choices and comprehensibility, addressing ""Evaluation""."
"It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines.",Does the review address Methodology?,TRUE,FALSE,"This sentence reports on the training data and system methodology, addressing ""Methodology""."
"For the second ablation, why do all the larger splits lead to similar performance?",Does the review address Result?,TRUE,FALSE,"This sentence raises a question about the results of an ablation study, addressing ""Result""."
"While I think it's nice to analyze the connection between RM and DM, the math provided in the paper is simple, and the main contribution is just to add a baseline to the algorithm of Khalifa et al.",Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the methodological contribution of the paper, addressing ""Methodology""."
"* Strength     * This paper proposes an interesting idea and interpretation to connect RM and DM paradigm     * This paper proposes a variance reduction method for DPG which demonstrates its improvement on performance, stability and sample efficiency * Weakness     * From my understanding, the baseline mostly comes from the observation in 3.3, which has limited technical novelty.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence describes the strengths and weaknesses of the paper’s contributions, addressing ""Definition/Description""."
"Furthermore, BERT doesn’t predict these masked words using a linear softmax model over a contextual embedding for the whole sentence, which is the assumed structure for the softmax language models considered in the analysis.",Does the review address Methodology?,TRUE,FALSE,"This sentence contrasts BERT’s approach to others in terms of methodology, addressing ""Methodology""."
The authors present a simple technique for in-context learning with large language models that achieves consistently good results across a variety of NLP tasks.,Does the review address Data/Task?,TRUE,FALSE,"This sentence introduces a technique for in-context learning applied to NLP tasks, addressing ""Data/Task""."
"The main motivation/result of the paper appears to be that the authors can perform zero-shot relation extraction, extracting relations only seen at test time.",Does the review address Result?,TRUE,FALSE,"This sentence summarizes the main result of the paper, which is zero-shot relation extraction, addressing ""Result""."
"Hence, the overall novelty of the work appears very low.",Does the review address Novelty?,TRUE,FALSE,"This sentence critiques the novelty of the work, suggesting it is low, addressing ""Novelty""."
"It may be useful to show how the performance changes when using different M. - According to [2], the CommonsenseQA IH-dev set contains 1,221 questions in total.",Does the review address Data/Task?,TRUE,FALSE,"This sentence suggests showing how performance changes with different values of M, related to the data set, addressing ""Data/Task""."
Cons:  - The main result (Thm 4.1) applies to next/conditional word distributions that are very close to the optimal distribution.,Does the review address Result?,TRUE,FALSE,"This sentence critiques the result of Theorem 4.1, suggesting its limitation, addressing ""Result""."
- Table 1 would be much more readable if you didn't use horizontal lines after every ORCHID tag.,Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the formatting in Table 1, addressing ""Presentation""."
The submission has evaluated the proposed algorithms on four datasets and improved SOTA performances.,Does the review address Data/Task?,TRUE,FALSE,"This sentence mentions the evaluation on multiple datasets, addressing ""Data/Task""."
"Now consider a prior accepted ICLR 2020 paper, Hoppity (Dinella et al. ), which trained on nearly 300k code change commits in GitHub.",Does the review address Comparison?,TRUE,FALSE,"This sentence compares the current work to a prior paper (Hoppity), addressing ""Comparison""."
"For example, experiments could investigate if using words which are phonologically similar (e.g., ""boat"" and ""moat"") is harder to distinguish than dissimilar words.",Does the review address Experiment?,TRUE,FALSE,"This sentence suggests an experiment to investigate phonologically similar words, addressing ""Experiment""."
Using Transformer attention maps for protein contact prediction is not new.,Does the review address Novelty?,TRUE,FALSE,"This sentence critiques the novelty of using Transformer attention maps for protein contact prediction, addressing ""Novelty""."
"In particular, instead of modeling the context information between the sentence and each video frame, wMAN tried to learn the representation with multi-level and co-attention, which considers all possible pairs between the word and the frame.",Does the review address Presentation?,TRUE,FALSE,"This sentence describes the method used by wMAN, addressing ""Presentation""."
"Clarification of contribution  Eq 6,7 reads like RNN style update but the intuition is lacking.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence asks for clarification of the contribution, particularly in terms of intuition, addressing ""Intuition""."
-  Consistent performance improvement over the baseline models on all of the three QA benchmarks.,Does the review address Result?,TRUE,FALSE,"This sentence mentions consistent improvement in performance, addressing ""Result""."
"For instance, the greater instability of larger Transformers to active learning bodes poorly for practitioners leveraging ever increasing model sizes for low-resource datasets.",Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the instability of larger Transformers in the context of active learning, addressing ""Methodology""."
"Good set of ablation studies to show that each component of the model is necessary, especially because the entire model already has many moving parts in addition to adversarial training.",Does the review address Ablation?,TRUE,FALSE,"This sentence praises the ablation studies showing the necessity of each model component, addressing ""Ablation""."
The key contribution of the paper is the approach to overcome the limitation of annotating  query sets and labels.,Does the review address Contribution?,TRUE,FALSE,"This sentence directly discusses the key contribution of the paper, addressing ""Contribution""."
"Experiments show that the suggested tuning of inference hyperparameters can bring improvements to LM tasks, which is convincing.",Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses experiments related to tuning hyperparameters for language modeling tasks, addressing ""Data/Task""."
This could be done by visualizing the planning trajectory difference between coherent and incoherent text.,Does the review address Presentation?,TRUE,FALSE,"This sentence suggests visualizing a planning trajectory difference, addressing ""Presentation""."
"- grounded language learning:     In both of these experiments, there is analysis provided on what aspects of the audio-based communication channel make the problem harder, easier, or just different from the same experiment with a discrete channel.",Does the review address Experiment?,TRUE,FALSE,"This sentence refers to experiments analyzing the audio-based communication channel, addressing ""Experiment""."
"- Overall the paper would have benefited from an intrinsic visualization of the latent space, to make sure for example that there's no  Information collapse of the embeddings when dealing with long sentences.",Does the review address Presentation?,TRUE,FALSE,"This sentence suggests a visualization of the latent space, addressing ""Presentation""."
It handles the hallucination problem of LLM by training close-ended dataset and then non-answerable question-answer pairs.,Does the review address Methodology?,TRUE,FALSE,"This sentence describes a method to handle hallucination in LLMs, addressing ""Methodology""."
"The two datasets have some “toy flavor”, while SCAN great favors example combination (with recomb-2 performs much better than recomb-1), recomb-1 seem to perform better for morphological analysis dataset, leaving questions about how to choose the exact models in general.",Does the review address Result?,TRUE,FALSE,"This sentence critiques the datasets and results, discussing model performance across datasets, addressing ""Result""."
"I compared their numbers explicitly to Liu et al. (2019), and RoBERTa_base outperforms their approach on nearly all tasks (and on average).",Does the review address Related Work?,TRUE,FALSE,"This sentence compares the paper's results with a previous work (Liu et al. 2019), addressing ""Related Work""."
"- P8, Table 2: The results from using Quad look worse than the above two.",Does the review address Result?,TRUE,FALSE,"This sentence mentions the results of using the ""Quad"" method and compares it to other methods, addressing ""Result""."
"This idea is novel and interesting to me, and the derivation and experiment results look encouraging.",Does the review address Experiment?,TRUE,FALSE,"This sentence describes the novelty of the idea and the encouraging results from experiments, addressing ""Experiment""."
"Therefore it would be interesting to see how this affects performance, i.e. just run the method on the CaP benchmark without the oracle.",Does the review address Methodology?,TRUE,FALSE,"This sentence proposes testing the method on the CaP benchmark, suggesting a methodology to evaluate its performance."
The explanation at the end of Section 4 is not persuasive.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence critiques the explanation in Section 4, suggesting it lacks persuasiveness, addressing ""Explanation""."
"3.The model excels in various audio-related tasks and open-ended question answering, demonstrating its outstanding performance.",Does the review address Result?,TRUE,FALSE,"This sentence presents the model's performance in various tasks as a result, addressing ""Result""."
"- Do you have any initial experiments on the ""self-improving"" aspect of this technique?",Does the review address Experiment?,TRUE,FALSE,"This sentence asks about experiments related to the ""self-improving"" aspect, addressing ""Experiment""."
"- In Table 1, can you explain more explicitly (in caption and text) what “subset” and “class words” means?",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence requests clarification on terms used in Table 1, addressing ""Definition/Explanation""."
The authors present a method that builds on the lottery ticket hypothesis (LTH).,Does the review address Methodology?,TRUE,FALSE,"This sentence describes the authors' method based on the LTH, addressing ""Methodology""."
In case of the textual prompts benchmarks its also not clear to me why no standard errors on results are reported?,Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the lack of standard errors in textual prompt benchmarks, addressing ""Methodology""."
"With this approach, that requirement doesn’t exist anymore.",Does the review address Methodology?,TRUE,FALSE,"This sentence refers to an approach that eliminates a requirement, addressing ""Methodology""."
"I would suggest using the phrase ""weighted SVD"" early on in the introduction (e.g., exactly when you introduce your new method).",Does the review address Methodology?,TRUE,FALSE,"This sentence suggests a change in phrasing for clarity in the introduction, addressing ""Methodology""."
**Empirical**:  One issue with the language modelling experiment is the choice of evaluation and train set.,Does the review address Evaluation?,TRUE,FALSE,"This sentence critiques the experimental setup in language modeling, addressing ""Evaluation""."
- General Discussion: The main focus of this paper is the introduction of a new model for learning multimodal word distributions formed from Gaussian mixtures for multiple word meanings.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence summarizes the general discussion on the model’s focus, addressing ""Definition/Discussion""."
Can the theory guide how to develop new models to learn program representations?,Does the review address Theory?,TRUE,FALSE,"This sentence asks if the theory can guide the development of new models, addressing ""Theory""."
The paper's presentation could be improved in several ways:     1.,Does the review address Result?,TRUE,FALSE,"This sentence refers to presentation improvements but doesn't directly address ""Result""."
This paper studies why language model pre-training has been such an effective technique in improving downstream performance across a wide range of NLP tasks recently.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses the study on language model pre-training and its effectiveness on NLP tasks, addressing ""Data/Task""."
This paper demonstrates why self-knowledge distillation as a prior distribution is a form of regularization with theoretical analysis on the gradients.,Does the review address Theory?,TRUE,FALSE,"This sentence demonstrates the regularization role of self-knowledge distillation with theoretical analysis, addressing ""Theory""."
The model sizes are varied across experiments to achieve on par performance which makes the comparison of the computational cost not so obvious.,Does the review address Presentation?,TRUE,FALSE,"This sentence comments on the varying model sizes and their effect on computational cost, addressing ""Presentation""."
- multi-concept and noise:     The noise you add to the channel has a number of different components; there should be an ablation study to illustrate the effects of these components.,Does the review address Ablation?,TRUE,FALSE,"This sentence suggests an ablation study to explore the effects of different noise components, addressing ""Ablation""."
"- I know GPT3 access is hard to get, but I wish experiments with k=8 prompts could be compared against  Post rebuttal: I think another pass for clarity over the paper would be good for the final version, but otherwise I'm happy with the paper updates and I'm happy to see the ablation numbers aren't too sensitive to token count.",Does the review address Experiment?,TRUE,FALSE,"This sentence suggests experimenting with a comparison of GPT3 prompts, addressing ""Experiment""."
Contrastive training (negative sampling) is one of the crucial contributions of this work.,Does the review address Methodology?,TRUE,FALSE,"This sentence highlights contrastive training (negative sampling) as a key methodological contribution, addressing ""Methodology""."
"- Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train.",Does the review address Presentation?,TRUE,FALSE,"This sentence emphasizes the paper's contributions and how BERT representations improve performance, addressing ""Presentation""."
"In addition, the authors empirically demonstrate that the token-level masked-LM model used by BERT is not a good choice as pre-training task for the two-tower Transformer when deployed for large-scale information retrieval applications.",Does the review address Data/Task?,TRUE,FALSE,"This sentence describes the use of BERT’s masked-LM model and its shortcomings for large-scale information retrieval, addressing ""Data/Task""."
"Strengths: The paper presents an interesting idea for Multiple-Choice Question-Answering (using the answer symbol instead of the answer itself), motivates the idea well and does a thorough analysis over multiple datasets, and LLMs to analyze its performance in different settings (including few-shot settings).",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence discusses the motivation and analysis behind the idea for Multiple-Choice Question-Answering, addressing ""Intuition/Justification""."
- Using capital and lower case tau in Theorem 4.2 is confusing notation.,Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the confusing notation in Theorem 4.2, addressing ""Presentation""."
"While it is also true that corpus transfer _enables_ this divergence in model types, to really test whether transferring the whole model versus using the corpus works or not, I would think you would want to compare fine-tuning the sender GRU on the LM data vs. starting from scratch _with a GRU of the same type_ and pre-training on the EC corpus before fine-tuning.",Does the review address Experiment?,TRUE,FALSE,"This sentence suggests comparing two experimental approaches involving fine-tuning and pre-training, addressing ""Experiment""."
"I will discuss my concerns one-by-one in detail: - Most importantly, the evaluation is confusing.",Does the review address Evaluation?,TRUE,FALSE,"This sentence critiques the evaluation, addressing ""Evaluation""."
"FLOPS is a measure of computer performance, while arithmetic intensity is the ratio of total floating-point operations to total data movement.",Does the review address Data/Task?,TRUE,FALSE,"This sentence explains technical concepts related to data and performance measurement, addressing ""Data/Task""."
The methodology is explained clearly and experiments are executed with a considerable amount of detail.,Does the review address Experiment?,TRUE,FALSE,"This sentence praises the methodology and experimental detail, addressing ""Experiment""."
"Specifically, I would expect authors provide more detailed recommendation for AL, DS, and multi-domain sampling in terms of sampling techniques, and population of different sources for certain application.",Does the review address Methodology?,TRUE,FALSE,"This sentence requests more detailed methodological recommendations, addressing ""Methodology""."
It's great the authors supplied code for part of the system so I don't want to penalize them for missing it -- but this is relevant since the paper itself has so few details on the baselines that they could not really be replicated based on the explanation in the paper.),Does the review address Comparison?,TRUE,FALSE,"This sentence discusses a comparison of baselines and the provided code, addressing ""Comparison""."
- Definition of equivalence seems to be insufficient and lacks important components.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence critiques the definition of equivalence, addressing ""Definition/Explanation""."
I think adapting CaP to the VIMA benchmark would be a more insightful baseline.,Does the review address Data/Task?,TRUE,FALSE,"This sentence suggests adapting CaP to a different benchmark, addressing ""Data/Task""."
"- Extensive ablation studies that showcase specific abilities of the model (types of OOD generalization), investigate the role of the foundation models and study the importance of the components of the prompt.",Does the review address Ablation?,TRUE,FALSE,"This sentence discusses the importance of ablation studies, addressing ""Ablation""."
2.The authors may add subjective evaluations to the ablation experiments to better demonstrate that the LoRA fine-tuning strategy mitigates catastrophic forgetting issues.,Does the review address Experiment?,TRUE,FALSE,"This sentence suggests adding subjective evaluations to ablation experiments, addressing ""Experiment""."
"Another question: does ""prompts of complexity n"" mean prompts of length n?",Does the review address Methodology?,TRUE,FALSE,"This sentence asks for clarification on methodology, addressing ""Methodology""."
The wide range of datasets and active learning techniques they use (including BALD which prior works shows is very competitive) lends credence to the conclusions.,Does the review address Data/Task?,TRUE,FALSE,"This sentence discusses datasets and techniques used to support conclusions, addressing ""Data/Task""."
"There are several follow-up results built on these two results, such as a new loss objective for predicting the downstream task, but to the best of my understanding, these two results are the main claims of this paper.",Does the review address Result?,TRUE,FALSE,"This sentence summarizes the paper’s main results, addressing ""Result""."
The complexity of learning options would be way different in the two different settings.,Does the review address Methodology?,TRUE,FALSE,"This sentence compares different learning options in various settings, addressing ""Methodology""."
"So there's a 3x3 contingency table of gold and predicted (POS, NEU, NEG) classes, but this sentence leaves ambiguous how precision and recall are calculated from this information.",Does the review address Presentation?,TRUE,FALSE,"This sentence critiques the clarity of precision and recall calculation, addressing ""Presentation""."
The paper does a systematic analysis on the role of language specific parameters using the proposed architecture.,Does the review address Analysis?,TRUE,FALSE,"This sentence highlights an analysis of language-specific parameters, addressing ""Analysis""."
I think significant presentation changes are required to clarify that the paper focuses on inference and finetuning.,Does the review address Presentation?,TRUE,FALSE,"This sentence suggests changes to improve presentation, addressing ""Presentation""."
"I think more could be done here, some ideas, probably there are better ways to test: - Have templates that leave out ""instructions"":  I would guess it wouldn't affect held-in task performance much, but would affect held-out tasks.",Does the review address Result?,TRUE,FALSE,"This sentence suggests ways to improve the testing process, addressing ""Result""."
"Second, it is really hard to capture which part is really making the main contribution to the final performance.",Does the review address Result?,TRUE,FALSE,"This sentence critiques the difficulty in identifying the main contribution to performance, addressing ""Result""."
Combining Translation Memory with Neural Machine Translation.,Does the review address Related Work?,TRUE,FALSE,"This sentence references a related work on combining Translation Memory with NMT, addressing ""Related Work""."
"Comparative Analysis and Benchmarking:  The comparisons presented in Tables 1 and 2 focus solely on the improvements over one reference work (Ma et al., 2023).",Does the review address Analysis?,TRUE,FALSE,"This sentence discusses the analysis and comparison in benchmarking, addressing ""Analysis""."
"Yet, it is difficult for me to trust that the effects in this paper will generalize to better performing models without further evidence: what if the CALM intermediate objectives only help with mistakes that larger models do not make in the first place?",Does the review address Methodology?,TRUE,FALSE,"This sentence questions the methodology and generalization of results, addressing ""Methodology""."
"Maybe I've missed this in the paper, if there is, please be more explicit about it because it affects the model quite drastically if for every sentence the largest phrase length is the sentence length.",Does the review address Presentation?,TRUE,FALSE,"This sentence asks for clarity on how sentence length affects the model, addressing ""Presentation""."
"Unfortunately, many aspects of the models, experimentation, and evaluation are not explained very well.",Does the review address Methodology?,TRUE,FALSE,"This sentence critiques the explanation of models, experimentation, and evaluation, addressing ""Methodology""."
"In addition, I have several notation confusions:  Assumption1: What is the hamming distance m_1 and m_2, when m_i are random variables?",Does the review address Presentation?,TRUE,FALSE,"This sentence highlights confusion in notation, addressing ""Presentation""."
"Second paragraph of related work: McCarley et al. (2019) appears twice with different descriptions, is this intentional?",Does the review address Related Work?,TRUE,FALSE,"This sentence critiques a possible inconsistency in related work references, addressing ""Related Work""."
"**Strengths** - To the best of my knowledge, this is the first work that _mathematically_ justifies the connection between the pre-training objective and the downstream performance.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This sentence praises the mathematical justification of the connection, addressing ""Intuition/Justification""."
I find the analysis presented in the paper very interesting and insightful - and distinguishes it from previous work in this area.,Does the review address Related Work?,TRUE,FALSE,"This sentence praises the analysis and its distinction from previous work, addressing ""Related Work""."
"However, there is a lack of unified experimental standards and ablation experiments in this paper.",Does the review address Ablation?,TRUE,FALSE,"This sentence critiques the lack of experimental standards and ablation studies, addressing ""Ablation""."
"- Easy but probably not great thing to try:  held-out tasks with wrong/useless templates  A final thought:  It's not obvious that using as many training examples per dataset as possible is optimal, given that the model could overfit to dataset-specific spurious correlations.",Does the review address Methodology?,TRUE,FALSE,"This sentence suggests an experimental approach to mitigate overfitting, addressing ""Methodology""."
"The presented method chooses action primitives such as ""PickAndPlace"" but does not need much training data (apart from the examples).",Does the review address Data/Task?,TRUE,FALSE,"This sentence describes the method's data requirements for action primitives, addressing ""Data/Task""."
"These prior methods do not incorporate the SFT stage, making it unclear how the model performs before this crucial phase.",Does the review address Result?,TRUE,FALSE,"This sentence questions the performance of prior methods without the SFT stage, addressing ""Result""."
"Ablation studies on the varying parameter counts of these two components would be valuable, if possible.",Does the review address Presentation?,TRUE,FALSE,"This sentence suggests the value of ablation studies, addressing ""Presentation""."
"The paper in general is well-written and easy to follow, the qualitative analysis and the additional diagrams in the appendix illustrating the variations in policies are appreciated.",Does the review address Analysis?,TRUE,FALSE,"This sentence praises the qualitative analysis and diagrams, addressing ""Analysis""."
"Comparative Analysis and Benchmarking:  The comparisons presented in Tables 1 and 2 focus solely on the improvements over one reference work (Ma et al., 2023).",Does the review address Result?,TRUE,FALSE,It mentions results by comparing the proposed approach to Ma et al. (2023).
"Additionally,        The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,The review discusses missing details regarding the dataset used for training.
The analysis shows that it outperforms only all the other baselines only on 5/9 games and matches on 3.,Does the review address Result?,TRUE,FALSE,It mentions specific performance results across games.
"In algorithm 1, in each iteration, only data sample (S_i) is used, how is this choice motivated?",Does the review address Data/Task?,TRUE,FALSE,The review questions the reasoning behind using only one data sample per iteration.
The comparison in Observation 1 does not seem to be an apples-to-apples comparison.,Does the review address Comparison?,TRUE,FALSE,The review points out an issue with the fairness of the comparison in Observation 1.
"what part of the performance is coming from pretraining (especially if using VAE type is novel, then quantifying that is important with and without VAE type SL), etc.",Does the review address Result?,TRUE,FALSE,The review asks about the contribution of pretraining to performance.
"Therefore it would be interesting to see how this affects performance, i.e. just run the method on the CaP benchmark without the oracle.",Does the review address Result?,TRUE,FALSE,The review suggests an experiment to further test performance without the oracle.
"* Most of the methodology appears to me like standard low quantized training techniques, e.g., using additional scales that are determined dynamically, adapted directly to FP8.",Does the review address Methodology?,TRUE,FALSE,"The review comments on the familiarity of the methodology, which seems to use standard techniques."
It is better to compare with more demonstration selection methods such as similarity-based and diversity-based methods which are widely used in practice.,Does the review address Methodology?,TRUE,FALSE,The review suggests using a broader set of comparison methods in the experiments.
"Running a baseline model that runs *for the same amount of time* is essential to appreciate the contribution of this work (e.g., repeat the same analysis in Figure 3 for the vanilla BERT).",Does the review address Analysis?,TRUE,FALSE,The review highlights the importance of time-matching baselines for a fair analysis.
* It is interesting to know that WebMath pre-training is still helpful even in the presence of PACT.,Does the review address Methodology?,TRUE,FALSE,The review comments on the utility of WebMath pre-training despite PACT's presence.
"Related Work: Contrastive learning - Under an unsupervised setting, ontrastive -> contrastive  Overall:  This work highlights the importance of incorporating contrastive training for data augmentation.",Does the review address Related Work?,TRUE,FALSE,The review discusses related work on contrastive learning and its relevance to data augmentation.
"i,.e the agent is given the ground truth context every time and asked to predict the correct next utterance.",Does the review address Methodology?,TRUE,FALSE,The review explains a specific methodology used in the proposed approach.
"You *tell* us that Fon is ""a language with special tokenization needs"" and that ""standard tokenization methods do not alwaysadequately deal with the grammatical, diacritical, and tonal properties of some African language"", and you cite the relevant papers.",Does the review address Related Work?,TRUE,FALSE,The review references related work that addresses tokenization issues for specific languages.
It seems possible that these improvements could instead be due to the high quality semantic/syntactic relations encoded in the attention mechanism.,Does the review address Presentation?,TRUE,FALSE,The review suggests that improvements could be attributed to attention mechanisms rather than the proposed method.
"* ""Moreover, it should be noted that BROS achieves higher f1 score than 79.27 of LayoutLM using visual features"".",Does the review address Result?,TRUE,FALSE,The review mentions a specific result where BROS outperforms LayoutLM on F1 score.
"Experiments show that the suggested tuning of inference hyperparameters can bring improvements to LM tasks, which is convincing.",Does the review address Experiment?,TRUE,FALSE,The review notes convincing results from experiments with tuned inference hyperparameters.
"Previous work [2] has already shown that by selecting the most complex examples from the training dataset, the performance can be largely improved compared to the original annotations from [1].",Does the review address Related Work?,TRUE,FALSE,The review mentions previous work that demonstrates the benefit of selecting complex examples for better performance.
It would be nice if these baselines are described before Fig.,Does the review address Comparison?,TRUE,FALSE,The review points out that baseline descriptions should come before presenting the figure for clarity.
"Was this using the development set, and if so for which dataset?",Does the review address Data/Task?,TRUE,FALSE,The review asks clarification on the dataset and whether the development set was used.
More specifically description would help the readers to understand the task clearly.,Does the review address Presentation?,TRUE,FALSE,The review asks for more detailed description to aid in the reader's understanding of the task.
"However, all the parameters/variables in the neural networks are freely designated and are not correlated to each other, thus they cannot work together to meet the requirements in the binding-unbinding mechanism.",Does the review address Methodology?,TRUE,FALSE,"The review critiques the lack of correlation between parameters in the neural networks, impacting task performance."
"For the first ablation, it just gives out the performance of intermediate models on a single task.",Does the review address Methodology?,TRUE,FALSE,The review critiques the methodology of reporting performance from intermediate models in an ablation study.
Theoretical analysis is provided to demonstrate the effectiveness of the proposed method under a greedy search algorithm.,Does the review address Analysis?,TRUE,FALSE,The review highlights the theoretical analysis validating the proposed method under specific conditions.
The results are intriguing and promising and should be of interest both to the emergent communication community as well as to the broader community working on low-resource NLP.,Does the review address Result?,TRUE,FALSE,The review acknowledges the results as promising and of broader interest.
"Beyond this, the authors conduct a multifaceted set of tests, including a non-harmful test to ensure that the watermark embedding does not significantly degrade model performance, robustness tests against second-time fine-tuning and model quantization, and an ablation study concerning the reference set to further substantiate the rationality of their backdoor data framework design.",Does the review address Experiment?,TRUE,FALSE,The review outlines a wide range of experiments conducted by the authors to validate their method.
"Second, it is really hard to capture which part is really making the main contribution to the final performance.",Does the review address Contribution?,TRUE,FALSE,The review expresses difficulty in identifying the main contributor to performance in the proposed method.
It would be great if the authors discuss this or provide some supporting evidence about its correctness.,Does the review address Comparison?,TRUE,FALSE,The review requests further clarification or evidence on the correctness of the approach.
Implementation details are only given for the vanilla BERT Are they similar to the EarlyBERT model as well?,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,The review seeks clarification on whether the implementation details for BERT apply to EarlyBERT.
Strengths:   The authors connect active learning example selection to more severe training instability than random selection.,Does the review address Methodology?,TRUE,FALSE,The review highlights a strength related to the methodology of active learning.
My another is concern is that the motivation of the experimental design is not clear.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,The review mentions concerns about the unclear motivation behind the experimental design.
It is nontrivial to gather the data as it involves hooking into the Lean's compilation process.,Does the review address Data/Task?,TRUE,FALSE,"The review comments on the difficulty of data gathering, which involves Lean's compilation process."
"The main limitations seem to be: (1) the proposed method is a bit limited in that it can only be used with a corpus in which the target head, relation, and tail spans need to be directly mentioned in a single sentence (2) it’s not clear whether the quantitative improvements are due to factual knowledge in the pretrained model or the syntactic/semantic relationships encoded in the self-attention.",Does the review address Methodology?,TRUE,FALSE,"The review critiques the limitations of the method, especially its reliance on specific sentence structure and unclear improvement sources."
"I know you cite the Abbott & Martinus, 2018 paper, stating that BPE is bad for analytical languages, but I still think it would prove a point to show BPE performing badly for your data.",Does the review address Comparison?,TRUE,FALSE,"The review suggests conducting a test to show that BPE is unsuitable for the data, referencing prior work."
The current experimental baseline can't reflect this.,Does the review address Experiment?,TRUE,FALSE,The review mentions that the experimental baseline does not appropriately reflect the claim.
I liked the idea of removing conditionals to infer likely necessary preconditions.,Does the review address Result?,TRUE,FALSE,The review praises a specific idea in the results.
"The proposed models -- which seem to be an application of various tree-structured recursive neural network models -- demonstrate a nice performance increase compared to a fairly convincing, broad set of baselines (if we are able to trust them; see below).",Does the review address Comparison?,TRUE,FALSE,The review praises the performance increase of the proposed model over a broad set of baselines.
The system performs on par with recently proposed GECA for SCAN and favorably to GECA on morphological analysis.,Does the review address Result?,TRUE,FALSE,The review compares the performance of the system to GECA on two tasks.
"Contributions: - A new algorithm for unsupervised knowledge graph creation from a target corpus - Demonstrating the utility of large pre-trained language models towards knowledge graph creation (though, there are other works in this area that should probably be discussed more.",Does the review address Presentation?,TRUE,FALSE,"The review mentions the contributions of the paper, but also highlights a gap in discussing related works."
Is there any special reason for restraining it to single-task finetuning if earlier results demonstrates multi-task finetuning is better?,Does the review address Data/Task?,TRUE,FALSE,The review raises a question about the choice of single-task finetuning versus multi-task finetuning.
"However, they only show that the hyperparameter search on \alpha is removed and the adaptive smoothing parameter can be connected to the gradient rescaling effect on self-distillation.",Does the review address Methodology?,TRUE,FALSE,The review discusses a methodological aspect related to the removal of a hyperparameter search and connection to gradient rescaling in self-distillation.
"The methods have been tested on two benchmarks focusing on the issue, SCAN and morphological analysis.",Does the review address Data/Task?,TRUE,FALSE,The review mentions the datasets (SCAN and morphological analysis) used for testing.
"- It's because if the largest phrase length is the sentence length, then model can be simplified into a some sort of convolution RNN where the each state of the RNN goes through some convolution layer before a final softmax and attention.",Does the review address Presentation?,TRUE,FALSE,The review explains a method for simplifying a model into a convolutional RNN structure.
Were the same hyperparameters used for all configurations?,Does the review address Methodology?,TRUE,FALSE,The review asks a methodological question about the consistency of hyperparameter usage across configurations.
"In terms of modelling, the work follows in the line of recent work on language-specific parameters for multilingual NMT.",Does the review address Methodology?,TRUE,FALSE,"The review comments on the modeling approach, stating it aligns with recent work on language-specific parameters for multilingual NMT."
"- Because the policy learning procedure utilizes the additionally generated dialogue acts and corresponding responses, it is easy to think that naively fine-tuning the GPT-2 model on the additional generated data may also improve the dialogue model performance in terms of its policy and responses (similar to a data augmentation method).",Does the review address Presentation?,TRUE,FALSE,The review suggests a potential improvement method for the dialogue model through fine-tuning and data augmentation.
Theorem 2 is presented with no intuition and the proof in the appendix is only for a special case.,Does the review address Theory?,TRUE,FALSE,The review critiques the lack of intuition behind Theorem 2 and notes that the proof is only valid for a special case.
"2) As the authors claimed in Introduction, ‘plenty of training data is available’.",Does the review address Methodology?,TRUE,FALSE,"The review refers to the availability of training data, which is an aspect of the methodology."
"Rather than using the ad-hoc approach for selecting which augmentation ""stacking"" scheme is helpful, it would have been better to compare/use an approach highlighted in ""Learning to Compose Domain-Specific Transformations for Data Augmentation"" [NeuRIPS 2017].",Does the review address Methodology?,TRUE,FALSE,The review suggests using a more systematic approach to data augmentation based on existing work.
"Reasons for Score ----------------- The idea proposed in the paper is novel and exciting, but I have some concerns about whether the gains promised by the theoretical analysis can be realized while maintaining modeling quality.",Does the review address Analysis?,TRUE,FALSE,The review raises concerns about the theoretical analysis and its implications for modeling quality.
- Discussion in Section 4.1 - I think Figure 4 should be explained in more detail (in caption and/or text).,Does the review address Presentation?,TRUE,FALSE,The review suggests providing more detail for Figure 4 in the text or caption.
Weaknesses: There is no contribution/novelty from the modeling/methods side.,Does the review address Novelty?,TRUE,FALSE,The review critiques the lack of novelty or contribution from the methods side.
# Summary  The authors propose to use corpora generated from _emergent communication_ as a fine-tuning signal for NLP tasks (language modeling and image captioning in particular).,Does the review address Data/Task?,TRUE,FALSE,The review summarizes the task and application domains (language modeling and image captioning).
"While the authors describe two applications (Section 2), these applications often deal with common programming languages that are intractable, e.g., code clones across binary code for vulnerability detection.",Does the review address Methodology?,TRUE,FALSE,The review comments on the intractability of certain applications described in the paper.
"In semantic parsing problem, langugae to programatic language is a typical task.",Does the review address Data/Task?,TRUE,FALSE,"The review highlights the task of semantic parsing, particularly the mapping of language to programmatic language."
"However, we should credit the core idea and (part of the implementation) to the earlier work on label-free CBMs.",Does the review address Presentation?,TRUE,FALSE,The review credits earlier work for the core idea and part of the implementation.
CALM shows better results with less data than the base model.,Does the review address Data/Task?,TRUE,FALSE,The review notes that CALM performs better with less data compared to the base model.
"Where the paper does get technical is in a discussion of the differing difficulties of speech recognition for different languages, providing a useful case study to demonstrate that one-size technology approaches are not necessarily universal stand-alone solutions.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review discusses the challenges of speech recognition across different languages, highlighting a case study."
But the authors didn’t compare against these agent-based system design.,Does the review address Comparison?,TRUE,FALSE,The review notes that the authors did not compare their approach against agent-based system designs.
"I appreciate that the authors do not read too much into it and focus more on the analysis of the results, but one thing that remains unanswered in this paper is how the proposed method fairs against multilingual baselines that utilize (roughly) the same number of parameters; currently, the best models outperform the LS baseline by ~28M and ~10M parameters on OPUS-100 and WMT-14 respectively.",Does the review address Comparison?,TRUE,FALSE,The review compares the proposed method against multilingual baselines with a similar number of parameters.
That more templates per dataset didn't help is particularly interesting and suggests some questions.,Does the review address Data/Task?,TRUE,FALSE,The review suggests interesting questions based on the results of using more templates per dataset.
"Given the issued pointed out in 1 and 2, I am not sure if the results are really sound as the authors claimed.",Does the review address Result?,TRUE,FALSE,"The review questions the validity of the results, based on earlier concerns."
"Put in another way, using RFA in transformer is from Rawat et al., 19 so do you think your major contribution is to design such  a gated usage of RFA?",Does the review address Methodology?,TRUE,FALSE,"The review asks whether the gated usage of RFA in transformers is the primary contribution, referencing Rawat et al. (2019)."
- Section 3: ...append a prompt like “This movie is” (the final quotation mark is on the next line).,Does the review address Methodology?,TRUE,FALSE,"The review suggests an adjustment to the methodology, specifically how prompts are appended in Section 3."
"Overall, I think this paper has a clear motivation and some interesting ideas on how to incorporate semantic language information into planning algorithms.",Does the review address Methodology?,TRUE,FALSE,The review summarizes the motivation and ideas in the paper related to planning algorithms.
"**Updates after rebuttal period**  The authors addressed some of the concerns -- showing inference time, model size and a discussion about training details and hyperparameters in the appendix.",Does the review address Methodology?,FALSE,TRUE,This sentence discusses the addressed concerns related to methodology.
"- Even though there is a “explain” component in PEER, it is not evaluated and studied regarding its correctness.",Does the review address Evaluation?,TRUE,FALSE,"The review points out that the ""explain"" component in PEER was not evaluated for correctness."
The authors did not provide what the retrieved sentences are like.,Does the review address Significance?,FALSE,TRUE,This sentence does not mention significance.
"It would be to show more experiment results for some settings, for example, performance on Sum task when training with a mixture distribution, or more Sum task samples and fewer Parity task samples ( p range from 0.5 to 1.",Does the review address Experiment?,TRUE,FALSE,The review requests more experimental results for certain settings.
- Experimental results: I suggest the author to provide more ablation analysis to the experiment section.,Does the review address Result?,FALSE,TRUE,This sentence does not mention result.
Experiments on LEGO and code interpretation task are done.,Does the review address Experiment?,TRUE,FALSE,The review mentions the completion of experiments on LEGO and code interpretation tasks.
The experiment results in Tables 1 & 2 cannot plausibly prove OTTER is more effective than CLIP.,Does the review address Result?,TRUE,FALSE,"The review critiques the experimental results, stating that they do not prove the effectiveness of OTTER over CLIP."
"Weakness While the ablation study and visualization analysis are done, the key evaluations are missing.",Does the review address Ablation?,TRUE,FALSE,The review mentions weaknesses in the ablation study and the lack of key evaluations.
"The main motivation/result of the paper appears to be that the authors can perform zero-shot relation extraction, extracting relations only seen at test time.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,The review explains the main motivation of the paper related to
"As I understand it, this result shows that any part of p_f(s) orthogonal to row-span(Phi) doesn’t affect the cross-entropy of the language model (first order optimality condition would still be satisfied).",Does the review address Methodology?,TRUE,FALSE,"This sentence describes an observation on the result, which refers to the Result aspect."
"While previous work has examined tasks in the overall area, to my knowledge there has not been any publicly availble sentence-level annotated data for the problem -- the authors here make a contribution as well by annotating some data included with the submission; if it is released, it could be useful for future researchers in this area.",Does the review address Contribution?,TRUE,FALSE,This sentence mentions the Contribution of annotating data for the problem.
Ablation studies show that the model achieves good performance on more complex questions.,Does the review address Methodology?,TRUE,FALSE,This sentence directly mentions Ablation studies.
The authors utilize figures and tables well to illustrate the ideas.,Does the review address Presentation?,TRUE,FALSE,"This refers to the Presentation of the paper, including figures and tables."
"The introduction is somewhat verbose, indirectly causing the first two weaknesses and making the paper hard to read.",Does the review address Presentation?,TRUE,FALSE,"This addresses Presentation, specifically the clarity and readability of the introduction."
-  The proposed model has two main parts: sentence embedding and substructure embedding.,Does the review address Methodology?,TRUE,FALSE,This mentions the Methodology used in the proposed model.
"- What is the ""margin of task $\mathcal{T}$"" mentioned on p.5?",Does the review address Data/Task?,TRUE,FALSE,This is a question about a concept related to the Data/Task being discussed in the paper.
"Additionally,        The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing.",Does the review address Data/Task?,TRUE,FALSE,This addresses Data/Task as it mentions missing details about the dataset used in the study.
"For example, what if the authors don’t use a LogicForm-to-NaturalLanguage conversion?",Does the review address Result?,TRUE,FALSE,"This question touches on a Result, as it is asking about the impact of a potential methodological change."
"If the tasks are not similar, and the learning objectives are not aligned, then the motivation for multi-task learning is solely for reducing memory footprint and computational cost.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This addresses Intuition/Justification/Motivation/Validation, as it discusses the justification behind the method."
"Starting from trivialities like the fact that the problem is generally undecidable (and not as stated in Section 2), through the use of incorrect terminology for invariants, guards, pre/post conditions, etc.",Does the review address Presentation?,TRUE,FALSE,"This addresses Presentation, specifically the clarity and correctness of terminology and explanation."
"First, much of the improvement (I think) comes from reducing the number of epochs and/or the number of steps.",Does the review address Result?,TRUE,FALSE,This refers to a Result related to performance improvement.
Since the authors point out that using class labels to generate text embeddings may bring challenges with expressive sensitivity.,Does the review address Methodology?,TRUE,FALSE,"This mentions Methodology, specifically the challenges with using class labels for generating embeddings."
"- Consider HellaSwag/PiQA/etc, where FLAN underperformed few-shot and even zero-shot.",Does the review address Data/Task?,TRUE,FALSE,"This refers to Data/Task, as it mentions specific datasets used in the task."
"Your work obviously is different enough to stand on its own, but it might be good to make a mention of this work and others (e.g. do more of a lit search / related work on low rank compression).",Does the review address Methodology?,TRUE,FALSE,"This addresses Related Work, as it mentions the importance of reviewing similar studies and comparing them."
"I also wanted to mention that I appreciate the addition of the suggested related work, but I would still suggest that the authors consider looking into more detailed means of comparison in the future (especially to the Petroni work), since this seemed to be a concern in multiple reviews.",Does the review address Related Work?,TRUE,FALSE,"This mentions Related Work, specifically the suggestion to include more related work."
"It requires more analysis about experimental results, such as Figure 1 and tables in Section D.",Does the review address Analysis?,TRUE,FALSE,"This refers to Analysis, as it asks for a deeper evaluation of experimental results."
"This is in sharp contrast to computer vision where techniques like rotation, modification of hue, saturation as well as umpteen other techniques exist.",Does the review address Presentation?,TRUE,FALSE,"This addresses Presentation, comparing techniques in computer vision and discussing differences in methods."
I would recommend the authors to at least assume the availability of some public data that is kept out of training and evaluations and to run all the baselines fairly in this setting.,Does the review address Methodology?,TRUE,FALSE,"This addresses Methodology, suggesting a change in the approach to data usage."
is a micro-average (all testsets are concatenated and evaluated as one set) or macro-average (average taken across the scores of individual test sets) score.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, as it discusses different averaging techniques for evaluating results."
* Section 4 - I think you really need to re-state that the algorithm has a human-in-the-loop for clarity.,Does the review address Presentation?,TRUE,FALSE,"This is related to Presentation, as it suggests making the description of the methodology clearer."
"I appreciate that the authors do not read too much into it and focus more on the analysis of the results, but one thing that remains unanswered in this paper is how the proposed method fairs against multilingual baselines that utilize (roughly) the same number of parameters; currently, the best models outperform the LS baseline by ~28M and ~10M parameters on OPUS-100 and WMT-14 respectively.",Does the review address Result?,TRUE,FALSE,"This is about Result, specifically analyzing and interpreting the results."
"The authors say ""we focus on larger datasets from GLUE (MNLI, QNLI, QQP and SST-2), as it is less meaningful to discuss efficient training"", but then report and analyze results from other GLUE datasets as well.",Does the review address Analysis?,TRUE,FALSE,"This addresses Analysis, as it critiques the results and the datasets used for analysis."
There are several parts to the method and there are I assume several differences in the architecture etc with baselines etc.,Does the review address Methodology?,TRUE,FALSE,"This discusses the Methodology, especially differences between the proposed model and baselines."
"These objectives certainly improve over the original T5 base _and_ larges models that are used as initializations, and especially outperform the base model in the low-data regime.",Does the review address Methodology?,TRUE,FALSE,"This is related to Methodology, comparing the proposed method with others."
"Given the success of MLM as a powerful pre-training objective, please consider formulating your claims in a more general way.",Does the review address Methodology?,TRUE,FALSE,"This is a suggestion related to Methodology, asking the authors to make broader claims."
The Ablation Study of the paper also provides useful insights about the impact of different pre-training schemas on large-scale information retrieval tasks.,Does the review address Ablation?,TRUE,FALSE,"This is about Ablation, mentioning insights from an ablation study."
Do you want to claim that this structure design is inspired by RNN and it leads to a better result?,Does the review address Result?,TRUE,FALSE,"This question refers to Result, as it asks about the impact of a specific design on performance."
"Provide a formal definition for ""general learner"" before stating the proposition.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This addresses Definition/Description/Detail/Discussion/Explanation/Interpretation, asking for a definition of a term."
* One idea I had here: Could you define a natural task as one for which there exists a sparse linear model over the *logits* of p*( .,Does the review address Result?,TRUE,FALSE,"This refers to Result, as it explores the nature of a ""natural task"" in relation to the results."
"But from the paper, I can hardly tell what the researchers should proceed to further improve the performance.",Does the review address Result?,TRUE,FALSE,"This is about Result, discussing areas where performance improvements are unclear."
"Concerning the claim, “the proposed fully-explored masking strategies lead to pre-trained models with stronger generalization ability.”, it is not clear how the proposed method yields stronger generalization ability.",Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, questioning the effectiveness of the proposed masking strategies."
"- wMAN explicitly utilized multi-level context information between the sentence and the video frame, and used the graph neural network and the message passing to model the representation.",Does the review address Presentation?,TRUE,FALSE,"This is related to Presentation, describing how wMAN uses multi-level context information."
"Without the experimental results using the same training settings, I do not think the authors are able to prove the superiority of OTTER over CLIP fully.",Does the review address Result?,TRUE,FALSE,"This addresses Result, as it questions the validity of the results without consistent experimental settings."
"Also it seems that these are not fully annotated, and the ‘forward type inference functionality from TypeScript’ is required to obtain labels.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This addresses Definition/Description/Detail/Discussion/Explanation/Interpretation, pointing out missing definitions or details."
CALM shows better results with less data than the base model.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, describing a result with fewer data."
"What is ""in-distribution"" instructions and how would an instruction from non-oracle look to an instruction from oracle?",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This refers to Definition/Description/Detail/Discussion/Explanation/Interpretation, asking for clarification on terminology."
"It's ok for the proposed method to be one particular way, but that discussion would be useful.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is about Definition/Description/Detail/Discussion/Explanation/Interpretation, suggesting further discussion of the method."
"Indeed, the proposed graph model seems to only implicitly convey knowledge across facts in terms of local reasoning.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is related to Definition/Description/Detail/Discussion/Explanation/Interpretation, discussing the capabilities of the proposed model."
Can the proposed theory help explain if the same code modeling task for some languages is strictly easier than the others?,Does the review address Methodology?,TRUE,FALSE,"This is about Methodology, asking if the proposed theory can be applied to explain task difficulty."
Their models achieve better quantitative results when compared to the provided baselines.,Does the review address Comparison?,TRUE,FALSE,"This directly mentions Comparison, as it compares the models against baselines."
"Also, the authors did a great job in terms of conducting evaluations from different angles.",Does the review address Evaluation?,TRUE,FALSE,"This is related to Evaluation, as it acknowledges the evaluation approach taken"
- Choice of baselines: For the VIMABench I find the choice of baselines not insightful.,Does the review address Comparison?,TRUE,FALSE,"This sentence addresses the comparison between the proposed work and baselines, which corresponds to the Comparison aspect."
Using a model capable of streaming would make sense from the point of view of inductive biases.,Does the review address Methodology?,TRUE,FALSE,"This sentence discusses the use of a model in terms of its methodology, which corresponds to the Methodology aspect."
Will need some clarification to better judge the results.,Does the review address Presentation?,TRUE,FALSE,"The mention of needing clarification to judge results refers to the Presentation aspect, as it deals with how results are presented."
I suggest the authors investigate the effect of such differences.,Does the review address Analysis?,TRUE,FALSE,"This sentence suggests further investigation into differences, which involves the Analysis aspect."
"- (The supplied code does not seem to include the baselines, just the recursive NN models.",Does the review address Comparison?,TRUE,FALSE,"The mention of baselines relates to Comparison, as it contrasts the proposed model against others."
"With the strengths being said, I hope to also point out that the paper's application of adversarial training is one attempt in many possibilities, and in many cases it is not clear where the improvements come from.",Does the review address Result?,TRUE,FALSE,"This discusses the results or improvements from the paper's methods, which corresponds to the Result aspect."
The methods is evaluated on 5 standard NER+RE datasets with good performances.,Does the review address Result?,TRUE,FALSE,"This sentence mentions the evaluation of the method and its performance, which directly corresponds to the Result aspect."
"The architecture does not look entirely novel, but I kind of like the simple and practical approach compared to prior work.",Does the review address Methodology?,TRUE,FALSE,"The comparison to prior work relates to Methodology, as it refers to the approach and techniques used."
"Given the nature of the problem statement (with multiple tasks, inputs and outputs), the authors have done a good job in explaining each of them properly.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This sentence is about the clear explanation of the problem, which corresponds to Definition/Description/Detail/Discussion/Explanation/Interpretation."
"PE seems to be important for wMAN, and the authors provides few sentences analysis about this, but I don't think I fully understand this part.",Does the review address Analysis?,TRUE,FALSE,"This discusses an analysis of an important element of the method, which refers to Analysis."
"This is especially disappointing as the objectives introduced _directly_ match the task in CommonGEN, making this intermediate training a form of noisy training data rather than pretraining.",Does the review address Methodology?,TRUE,FALSE,"This sentence analyzes the relationship between tasks and objectives, which corresponds to Methodology."
"Within the evaluation section, there are numerous intriguing findings that hold significant value for dissemination within the wider research community.",Does the review address Result?,TRUE,FALSE,The mention of findings with value for dissemination refers to Result.
The authors present a simple technique for in-context learning with large language models that achieves consistently good results across a variety of NLP tasks.,Does the review address Result?,TRUE,FALSE,"This highlights the results across various tasks, so it relates to Result."
"Weakness While the ablation study and visualization analysis are done, the key evaluations are missing.",Does the review address Analysis?,TRUE,FALSE,This refers to an Analysis of the experiments and evaluations performed in the study.
Some important concepts are repeatedly used without a definition.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This refers to the Definition/Description/Detail/Discussion/Explanation/Interpretation aspect, as it discusses the need for clearer definitions."
lead to performance decrease for individual tasks.,Does the review address Data/Task?,TRUE,FALSE,"This sentence refers to the effect on Data/Task, as it mentions the impact on task performance."
"For example, is there any restriction on the parameters in encoder and decoder respectively to reflect the property $UR=I$ as in Section 2.",Does the review address Theory?,TRUE,FALSE,"This sentence involves a theoretical concept, relating to Theory."
"Indeed, at first, the words ""The following algorithm"" confused me, because I thought it was more a ""methodology"", since Step 2 is where the humans are in the loop, unless you have a Fon POS tagger and I am misunderstanding?",Does the review address Methodology?,TRUE,FALSE,"This sentence reflects on the methodology, so it refers to Methodology."
"(3) This model still needs to calculate the similarity matrix for queries and documents, so the computational efficiency is not improved compared with other multi-vector retrieval models in the training stage.",Does the review address Methodology?,TRUE,FALSE,"This mentions the model's methodology and computational efficiency, referring to Methodology."
"This is relevant because, by training end to end, that work effectively generates arbitrary amounts of training data through interaction the the HOL4 ITP system (intermediate theorems which are proven give some reward in that work).",Does the review address Methodology?,TRUE,FALSE,"This relates to Methodology, discussing how training data is generated through an approach in the methodology."
Concerns around novelty and the multi-task setup was also raised by another reviewer (e7Hg).,Does the review address Data/Task?,TRUE,FALSE,"This refers to concerns about the Data/Task setup, as it mentions the multi-task setup."
"- P3, Sec 2.2: ""... achieve lower test perplexity than traditional n-gram models"" Why is this true?",Does the review address Methodology?,TRUE,FALSE,"This question asks about the methodology behind a result, so it refers to Methodology."
There’s not much that can be faulted and all my comments below are meant to help the paper gain additional clarity.,Does the review address Presentation?,TRUE,FALSE,"This refers to the Presentation aspect, as it discusses the paper's clarity."
The findings illustrate the substantial impact this choice can have on the final model's behavior.,Does the review address Result?,TRUE,FALSE,"This sentence is about the results, relating to the Result aspect."
"For example, is there any restriction on the parameters in encoder and decoder respectively to reflect the property $UR=I$ as in Section 2.",Does the review address Methodology?,TRUE,FALSE,"This is again related to Methodology, as it discusses the parameters in the method."
Weakness   The paper is poorly organized and very hard to follow.,Does the review address Presentation?,TRUE,FALSE,"This addresses the Presentation aspect, focusing on the paper's organization and readability."
They have evaluated their architecture based on (i) the language modelling test evaluated on PTB and FBIS and (ii) Chinese-English machine translation task on NIST MT02-08 evaluation sets.,Does the review address Evaluation?,TRUE,FALSE,"This directly relates to Evaluation, as it mentions the evaluation of the architecture."
- How will the pseudo data generation amount affect the learning / forgetting performance?,Does the review address Data/Task?,TRUE,FALSE,"This refers to a Data/Task question, addressing how data generation affects performance."
NIT:  - Grammar last sentence of Section 1.1 (“…analyze the efficiency *of* …”) - Proposition 2.2: Maybe write “\forall s \in S” instead of “\forall s ~ p_L”.,Does the review address Presentation?,TRUE,FALSE,"This refers to Presentation, as it mentions grammar and formatting issues."
An emerging use of LeetCode is to use it as a baseline for machine programming (MP) in a variety of different ways.,Does the review address Data/Task?,TRUE,FALSE,"This refers to the Data/Task aspect, as it discusses the use of LeetCode as a baseline."
"- Technical Novelty and Significance: 3 - Empirical Novelty and Significance: 1  ## Questions _What_ is unique about the environment has been presented well, so my primary question is _how_ is it unique?",Does the review address Novelty?,TRUE,FALSE,"This directly addresses Novelty, questioning the uniqueness of the environment presented."
"While this is not a downside by itself, it is probably something that one implement as a baseline.",Does the review address Comparison?,TRUE,FALSE,"This refers to Comparison, as it discusses implementing a baseline."
"- After definition 5.1, what does Omega[w] = Omega[w’] mean?",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is about Definition/Description/Detail/Discussion/Explanation/Interpretation, as it seeks clarification on a definition."
Conducting real-human interactions can better justify the effectiveness of the proposed RL method in practical scenarios.,Does the review address Presentation?,TRUE,FALSE,"This involves Presentation, focusing on how the effectiveness of the method is justified."
"In the first phase (match), they extract knowledge tuples from each sentence in a corpus using a beam search over the self-attention within a pre-trained language model.",Does the review address Methodology?,TRUE,FALSE,"This refers to the Methodology, discussing the process used in the study."
Since we observe that the randomly pruned models do not competitive performance ...: how uncompetitive?,Does the review address Presentation?,TRUE,FALSE,"This refers to the Presentation, asking for clarification on how uncompetitive the models are."
"To name a few:   -“Language Models as Knowledge Bases?” EMNLP 2019   -“Commonsense Knowledge Mining from Pretrained Models.” EMNLP 2019   -“Comet: Commonsense Transformers for Automatic Knowledge Graph Construction.” ACL 2019 - Although the proposed model achieves quantitative improvements over Angeli et al. (2015), it seems unclear whether these improvements are due to the factual knowledge encoded in the pre-trained LM, as claimed.",Does the review address Related Work?,TRUE,FALSE,This directly discusses prior related works in the context of improvements over them.
The theory is a bit complicated and not easy to follow.,Does the review address Methodology?,TRUE,FALSE,This refers to difficulty understanding the theory but does not describe the methodology or theory in detail.
"- Similarly, using bold and not-bold B in Theorem 5.2 is confusing notation.",Does the review address Theory?,TRUE,FALSE,"This addresses notation confusion, which is part of the Theory aspect."
I feel that the conclusions are in general well supported by the results.,Does the review address Result?,TRUE,FALSE,"This statement evaluates how well the results support the conclusions, fitting the Result aspect."
"Then it is reasonable to include the baseline suggest above, i.e. input additional features.",Does the review address Comparison?,TRUE,FALSE,"This suggests including a baseline comparison, which is part of the Comparison aspect."
"Writing:  The writing is overall clear and easy to follow, although it took me quite some time to map out the definitions of various notations.",Does the review address Presentation?,TRUE,FALSE,"This refers to the clarity and structure of the writing, fitting the Presentation aspect."
"## Justification As my primary critique concerns the experiments, I will address each individually mentioning any deficiencies and potential improvements.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This refers to the critique and justification of experiments, corresponding to Justification/Motivation."
The method proposed has limited methodology contribution to the research community.,Does the review address Methodology?,TRUE,FALSE,"This discusses the methodology and its contributions, fitting the Methodology aspect."
* One idea I had here: Could you define a natural task as one for which there exists a sparse linear model over the *logits* of p*( .,Does the review address Data/Task?,TRUE,FALSE,"This is proposing a new task, relating to the Data/Task aspect."
This paper conducts comprehensive evaluations on the influence of selecting specific examples for inclusion in the prompt.,Does the review address Evaluation?,TRUE,FALSE,"This statement refers to evaluating the method, fitting the Evaluation aspect."
The authors refer to the fact that MTL can (and often does!),Does the review address Related Work?,TRUE,FALSE,"This refers to multi-task learning (MTL) and mentions related work, fitting the Related Work aspect."
"Without the experimental results using the same training settings, I do not think the authors are able to prove the superiority of OTTER over CLIP fully.",Does the review address Experiment?,TRUE,FALSE,"This critiques the experiments and their settings, relating to the Experiment aspect."
---- Appendix: ----  I liked the section B ablations (as implied above).,Does the review address Ablation?,TRUE,FALSE,"This refers to the ablation studies conducted in the paper, corresponding to the Ablation aspect."
"In my opinion, this idea has a potential to be applied to domains other than theorem proving.",Does the review address Methodology?,TRUE,FALSE,"This refers to potential applications of the methodology, fitting the Methodology aspect."
Should also discuss related work in 2d spatial visualization of country-country relationships by Peter Hoff and Michael Ward.,Does the review address Related Work?,TRUE,FALSE,"This is a suggestion to reference related work, fitting the Related Work aspect."
"The paper focuses on a critical and urgent issue, the training and inference efficiency of LLMs.",Does the review address Methodology?,TRUE,FALSE,"This is a description of the paper's focus on methodology, fitting the Methodology aspect."
The empirical part of the paper shows improved performance of adding similar sentences to the context of LM training.,Does the review address Methodology?,TRUE,FALSE,"This discusses empirical results, corresponding to the Methodology aspect."
"Additionally, there are some minor things I would add or improve: - I would add references to multi-task training on different languages (e.g., Task 1 is translation from EN to FR and Task 2 is translation from EN to DE).",Does the review address Related Work?,TRUE,FALSE,"This refers to related work in the context of multi-task training, fitting the Related Work aspect."
"As, deep recurrent neural networks are already used in keyphrase extraction (shows very good performance also), so, it will be interesting to have a proper motivation to justify the use of  RNN and Copy RNN over deep recurrent neural networks.",Does the review address Methodology?,TRUE,FALSE,"This suggests a justification for using RNNs, which fits the Methodology aspect."
"- I think it makes sense in the comparison with CaP to compare using an oracle object detector, but one of the main novelties of the work is replacing the perceptual modules of CaP with foundation models.",Does the review address Methodology?,TRUE,FALSE,"This compares methodologies, fitting the Methodology aspect."
"The proposed method represents a novel approach to multimodal techniques, distinguishing itself from previous Vision-Language Models (VLMs) like Flamingo and PaLI.",Does the review address Novelty?,TRUE,FALSE,"This addresses the originality and novelty of the method, fitting the Novelty aspect."
This reminds us of the sensitivity of BERT pretraining to the optimizers.,Does the review address Methodology?,TRUE,FALSE,"This refers to the methodology used in pretraining BERT, fitting the Methodology aspect."
Such choice and associated thresholds seem arbitrary: how were they actually found out?,Does the review address Presentation?,TRUE,FALSE,"This critiques the methodological choices, fitting the Presentation aspect."
"As I understand it, it’s called decompilation because it tends to do the opposite of what a compiler does.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is an explanation of a term, fitting the Definition/Description/Detail aspect."
"\phi(x) as introduced in eq 2 is in R^{2D} but S_{t-1} is in R{D}, not sure what does + mean in this context.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is a clarification query regarding the notation, fitting the Definition/Description/Detail aspect."
Strength: - Thorough study on the robustness of recent popular VL models vs conventional CE models.,Does the review address Presentation?,TRUE,FALSE,"This refers to an evaluation of robustness, which fits the Presentation aspect."
The Human evaluation study adds a ton of value in judging LTU in my perspective.,Does the review address Novelty?,TRUE,FALSE,"This refers to a human evaluation study, fitting the Novelty aspect."
**SCS**  The authors propose a new method to represent symbolic stimuli as continuous representations as an alternative to one-hot encodings.,Does the review address Methodology?,TRUE,FALSE,"This describes the methodology being proposed, fitting the Methodology aspect."
Experiments on both continual pre-training and general pre-training from scratch show the effectiveness of the proposed method.,Does the review address Methodology?,TRUE,FALSE,"This evaluates the method's effectiveness through experiments, fitting the Methodology aspect."
"Reasons for Score ----------------- The idea proposed in the paper is novel and exciting, but I have some concerns about whether the gains promised by the theoretical analysis can be realized while maintaining modeling quality.",Does the review address Novelty?,TRUE,FALSE,"This addresses novelty and theoretical concerns, fitting the Novelty and Methodology aspects."
* Using the Legendre Memory Unit to substitute self-attention in transformers is interesting and has several potential merits: it can reduce the complexity and does not increase the size of the layer.,Does the review address Methodology?,TRUE,FALSE,"This discusses a specific methodological improvement, fitting the Methodology aspect."
"Experiments show that the induced ""best-first"" order outperforms fixed orders, which verifies the motivation of the paper` 4.",Does the review address Experiment?,TRUE,FALSE,"This reports experimental results verifying the motivation of the method, fitting the Experiment aspect."
"Additionally, showing this for negations and / or examples which GreaseLM gets correct but QA-GNN does not (and vice-versa) can shed some light on what the model improves on (and what are the limitations).",Does the review address Result?,TRUE,FALSE,"This suggests additional analysis of model performance, fitting the Result aspect."
"Even if pruning is ineffective (at multiple pruning rates), it’s never really explained why, despite this being a core contribution of the paper.",Does the review address Contribution?,TRUE,FALSE,"This critiques the explanation of a core contribution, fitting the Contribution aspect."
"Overall, I like the idea of the paper: it is important to reduce the parameters needed for sets of tasks, to enable NLP models to be deployed in a larger variety of settings, and reducing the amount of data needed.",Does the review address Methodology?,TRUE,FALSE,"This is a discussion of the methodology and its importance, fitting the Methodology aspect."
"The authors provide several reasonable insights that might be relevant to the user interface modeling community — using object detection as part of multi-task learning instead of standalone pre-training task, design choices to create a single unified architecture for all the tasks, multi-task learning outperforming single-task learning etc.",Does the review address Methodology?,TRUE,FALSE,"This refers to the methodology and insights provided, fitting the Methodology aspect."
The paper presents a novel way of combining information from text and a KB in a bidirectional way.,Does the review address Methodology?,TRUE,FALSE,"This describes the novel methodology of the paper, fitting the Methodology aspect."
"With the proposed CaptionNet dataset, it is now possible to have a fairer comparison between VL models and CE models.",Does the review address Comparison?,TRUE,FALSE,"This suggests the use of a new dataset for comparison, fitting the Comparison aspect."
This work tries to address the issue by proposing a technique that carefully amalgamates multiple previously known approaches to generate diverse label preserving examples.,Does the review address Data/Task?,TRUE,FALSE,"This describes the methodology of the paper, fitting the Data/Task aspect."
"For a more comprehensive analysis, it would be instructive to see how the approach compares with a broader spectrum of state-of-the-art methods.",Does the review address Analysis?,TRUE,FALSE,"This suggests a more comprehensive analysis with comparisons, fitting the Analysis aspect."
I think adapting CaP to the VIMA benchmark would be a more insightful baseline.,Does the review address Comparison?,TRUE,FALSE,"This suggests a more insightful comparison baseline, fitting the"
**Experimental results** The presentation of the experimental results is clear.,Does the review address Experiment?,TRUE,FALSE,"The review mentions ""experimental results,"" indicating that the presentation is being addressed."
What did the authors do in their attempt to find it?,Does the review address Methodology?,TRUE,FALSE,This relates to the authors' methodology in conducting the experiment.
It is not intuitive to understand to what extent the model relies on the input audio versus on the common sense knowledge that is already encoded in the LLMs.,Does the review address Presentation?,TRUE,FALSE,This statement is concerned with presentation and how well the model dependencies are explained.
"Especially because of the surprising magnitude by which this pruning degrades absolute performance, it is unfortunately necessary to try more pruning rates for a fair comparison.",Does the review address Comparison?,TRUE,FALSE,This highlights comparison and the need to explore additional pruning rates to evaluate fairness.
It paves the way for more widespread application of VIP in scenarios where interpretable-by-design approaches are critical.,Does the review address Methodology?,TRUE,FALSE,This speaks to the methodology and its implications for applications requiring interpretability.
I felt similar conclusions can be drawn from the results of that paper as well.,Does the review address Result?,TRUE,FALSE,The statement addresses the results and conclusions drawn from the experiments.
All of the results in this work seem to be previously known: * Theorem 1 is general to any polynomial-size circuit -- there is nothing special about Transformers.,Does the review address Methodology?,TRUE,FALSE,"The review is referring to methodology, noting that the results are not novel."
"Unfortunately, many aspects of the models, experimentation, and evaluation are not explained very well.",Does the review address Experiment?,TRUE,FALSE,This refers to the lack of explanation in the experimental process and evaluation of the models.
This paper set an assumption that the teacher network makes a less confident prediction than that of the student and extends gradient analysis in the perspective of regularization effect in the proposed adaptive label smoothing.,Does the review address Analysis?,TRUE,FALSE,"This involves the analysis within the methodology, explaining the assumption and gradient analysis."
Both split single word representation into multiple prototypes by using a mixture model.,Does the review address Methodology?,TRUE,FALSE,This describes the methodology used to represent words in multiple prototypes.
There seems to be a lack of thought about model structure and loss.,Does the review address Methodology?,TRUE,FALSE,"This is a critique of the methodology, focusing on the model structure and loss considerations."
It would be great if the authors discuss this or provide some supporting evidence about its correctness.,Does the review address Evaluation?,TRUE,FALSE,This relates to evaluation and the need for further supporting evidence.
The key technique is to construct a type dependency graph and infer the type on top of it.,Does the review address Methodology?,TRUE,FALSE,This highlights a methodology technique used in the research.
"- Technical Novelty and Significance: 3 - Empirical Novelty and Significance: 1  ## Questions _What_ is unique about the environment has been presented well, so my primary question is _how_ is it unique?",Does the review address Significance?,TRUE,FALSE,This involves the significance of the work and the question of its uniqueness.
"For instance, ensuring that comparisons are fair (same number of parameters, etc) for the object detection task.",Does the review address Comparison?,TRUE,FALSE,"This is related to comparison, ensuring fair evaluation metrics like parameters are consistent."
"Try using horizontal lines only after each UD tag category, and consider the ""booktabs"" guidelines for making good-looking tables.",Does the review address Presentation?,TRUE,FALSE,This is a suggestion related to the presentation of the paper.
The idea of using cosine similarity in word embeddings is a simple but effective way of biasing the MCTS in the right directions.,Does the review address Methodology?,TRUE,FALSE,This discusses a methodology used to bias the model with cosine similarity.
Their findings on learning complex tasks contribute to the understanding of large language model learning and provide valuable insights for future related work on efficient training.,Does the review address Result?,TRUE,FALSE,This refers to the results and how they contribute to understanding model learning.
The experimental results also look promising in compared with the exsiting models.,Does the review address Experiment?,TRUE,FALSE,This evaluates the experiment results and their comparison to existing models.
"The paper appraisal therefore rests on the clarity of presentation, how convincing the experiments are, and how reproducible.",Does the review address Experiment?,TRUE,FALSE,"This is about the experimental design, the presentation, and reproducibility of the work."
- Experiments contain 3 different tasks and each has datasets from different domains.,Does the review address Data/Task?,TRUE,FALSE,This addresses the data/tasks used in the experiments.
"For polysynthetic languages, though, one could posit that a fairly small set of rules might be highly predictive - humans invoke algorithms to construct patterned speech that would otherwise be incomprehensible for the listener to deconstruct, and those same algorithms can be encoded for use by machines.",Does the review address Methodology?,TRUE,FALSE,"This is part of the methodology, suggesting how polysynthetic languages can be modeled."
"It seems like a hard task (there are hundreds of those CAMEO categories....) Did the authors consider using the Goldstein scaling, which has been used in political science, as well as the cited work by O'Connor et al.?",Does the review address Related Work?,TRUE,FALSE,"This relates to related work, suggesting prior methods for handling CAMEO categories."
"**Strengths** - To the best of my knowledge, this is the first work that _mathematically_ justifies the connection between the pre-training objective and the downstream performance.",Does the review address Result?,TRUE,FALSE,This pertains to the results and the novel contribution of the paper.
"The results are highly correlated with concepts that are actually present in the images; i.e., there seems to be very little confabulation (often called “hallucination”).",Does the review address Result?,TRUE,FALSE,This evaluates the results and their alignment with actual concepts in the images.
"Other designs include beam size, whether or not to use a pretrained model, etc.",Does the review address Experiment?,TRUE,FALSE,This is part of the experimental setup and design choices.
A single value of $k=150$ was chosen for experiments across all tasks.,Does the review address Experiment?,TRUE,FALSE,This is part of the experimental setup.
"In addition, I have several notation confusions:  Assumption1: What is the hamming distance m_1 and m_2, when m_i are random variables?",Does the review address Methodology?,TRUE,FALSE,This raises a methodological clarification about notation.
One may also refer to https://opencompass.org.cn/leaderboard-llm for the performance of LLMs (I acknowledge that the performance of ChatGPT on GSM8K from that website is possibly still underestimated).,Does the review address Result?,TRUE,FALSE,This relates to the results and compares them to a leaderboard of LLM performance.
"Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained?",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is a request for further discussion and explanation, focusing on methodology."
"Their goal is to reduce the amount of parameters and data needed, while improving (or maintaining) state-of-the-art performance.",Does the review address Result?,TRUE,FALSE,This addresses the results and the goal of reducing parameters while maintaining performance.
"- There are also many typos in the text, for example page 2: “Our results that when” -> “Our results show that when”, page 5: “CaptionNet subset can be found in Section ?",Does the review address Presentation?,TRUE,FALSE,This is a critique related to the presentation of the paper.
"For example, the full model of wMAN works better than FBW on R@1, but worse on R@5 and R@10.",Does the review address Result?,TRUE,FALSE,"This is part of the results, comparing performance across different metrics."
"The authors switch between using BERT_base and RoBERTa_base without being very clear about when and why, e.g., in Table 2 they have both models, but only in different sections.",Does the review address Evaluation?,TRUE,FALSE,"This is related to evaluation, as it concerns clarity in the choice of models used in the experiment."
I would appreciate the explanation and further evidence to address these concerns.,Does the review address Result?,TRUE,FALSE,"The reviewer requests further explanation, which is related to the results section and the need for more evidence to back up conclusions."
In case of the textual prompts benchmarks its also not clear to me why no standard errors on results are reported?,Does the review address Result?,TRUE,FALSE,"This relates to results, as the reviewer is questioning the lack of standard error reporting for the benchmarks."
"The experimental settings in Section 3 lack detailed descriptions, potentially making reproduction difficult and potentially misleading.",Does the review address Experiment?,TRUE,FALSE,"This is related to the experimental setup, which is lacking sufficient detail for replication."
"For checking the generalization of the method and better comparison w/ InDIGO (though InDIGO also conducted on MSCOCO, Django and the current comparison is sufficiently fair), I would like to increase my rating if seeing more experiments on large scale machine translation benchmarks as those in InDIGO.",Does the review address Comparison?,TRUE,FALSE,This involves comparison and the need for further experimentation with large-scale machine translation benchmarks to improve the evaluation.
A key parameter that occurs in obtaining the above results is a worst-case coefficient that bounds the distributional shift between language model distributions of the training dataset and that of the downstream task.,Does the review address Methodology?,TRUE,FALSE,"This is a methodological discussion, explaining a key parameter used to address distribution shifts."
"- There's a clear Inconsistency in the best TC method between different latent dimensions (8,6,32), in most of the experiments there's at least one of the 3 that is performing drastically worse than the other baselines, while there's overall no clear winner.",Does the review address Result?,TRUE,FALSE,"This is a result-related observation, noting inconsistencies in performance across different latent dimensions."
"It's also not explained how Theorem 2 justifies the main conclusion: that ""a prompt engineer aided by enough time and memory can force an LLM to output an arbitrary sequence of ℓ tokens.""",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is a critique of the methodology or theory, questioning how the conclusions of the theorem are justified."
It is hard to know here which one of these actually made the adversarial setup useful.,Does the review address Result?,TRUE,FALSE,"This is a result-related question, highlighting confusion over which components of the adversarial setup were effective."
"Altogether, I think this paper makes an interesting contribution to the question of: How can we get the most pretraining signal from unstructured data using off-the-shelf tools?",Does the review address Data/Task?,TRUE,FALSE,"This is about the data/task, exploring how the work contributes to utilizing unstructured data efficiently."
The evaluation method uses CCA to maximize the correlation between the word embeddings and possibly hand crafted linguistic data.,Does the review address Data/Task?,TRUE,FALSE,"This is part of the data/task, explaining how evaluation was done using CCA to correlate word embeddings."
"To best of my knowledge, under many circumstances in particular for short sequence, attention alone might not be the  most time-consuming part of the model.",Does the review address Methodology?,TRUE,FALSE,"This is related to methodology, providing insight into the time complexity of using attention mechanisms."
This may strengthen the contribution of the paper.,Does the review address Contribution?,TRUE,FALSE,"This is about the contribution of the paper, suggesting that additional details or findings could enhance its impact."
"The evaluation results are based on the authors' implementations, for both baseline and the proposed method.",Does the review address Evaluation?,TRUE,FALSE,"This is related to evaluation, pointing out that results are based on the authors' own implementations."
- Using capital and lower case tau in Theorem 4.2 is confusing notation.,Does the review address Theory?,TRUE,FALSE,"This is a critique of theory, pointing out confusing notation used in the paper."
"In algorithm 1, in each iteration, only data sample (S_i) is used, how is this choice motivated?",Does the review address Methodology?,TRUE,FALSE,This is a methodological query about the choice to use a single data sample in each iteration and its justification.
There's also some missing related work in extracting knowledge from pretrained models that should probably be discussed.,Does the review address Presentation?,TRUE,FALSE,"This is a critique of the presentation, noting that relevant prior work in extracting knowledge from pretrained models is missing."
"As such, in my opinion this is unquestionably an important subtopic for the field of machine programming and the authors approach also seems satisfactory to me for ICLR (described below).",Does the review address Significance?,TRUE,FALSE,"This addresses the significance of the work, affirming its importance to the field of machine programming."
"More importantly, the experiments are not convincing as it is presented now.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is a critique of the experimental results, suggesting that the current presentation is not convincing."
"If the tasks are not similar, and the learning objectives are not aligned, then the motivation for multi-task learning is solely for reducing memory footprint and computational cost.",Does the review address Data/Task?,TRUE,FALSE,"This is related to data/task, focusing on the motivation for multi-task learning based on task similarity and objectives."
"Further, how does the network perform when a longer context is obtained *maintaining the same number of parameters* as a network with less temporal scales?",Does the review address Methodology?,TRUE,FALSE,This is a methodological question about how the network behaves when maintaining parameters while extending context.
"This paper proposes the adaptive \alpha computed by the entropic level of model probability distribution per sample, which leads to updating the model parameters to lower the predictive score on the ground-truth target, as opposed to the effect of the cross-entropy with hard targets.",Does the review address Methodology?,TRUE,FALSE,"This is a description of the methodology used, explaining the adaptive α\alpha used for updating model parameters."
"Compared to Pengi, the closed-ended audio task performances are lower.",Does the review address Result?,TRUE,FALSE,"This refers to the results, comparing performance on the closed-ended audio task to another system (Pengi)."
"In sum, the proposed method is relatively novel and the idea is reasonable.",Does the review address Novelty?,TRUE,FALSE,The review mentions the novelty of the proposed method and its reasonable foundation.
- What is it about an audio channel model that makes compositionality easier or more difficult to learn?,Does the review address Methodology?,TRUE,FALSE,The review raises a methodological question regarding the audio channel model and its effects on compositionality learning.
"Their goal is to reduce the amount of parameters and data needed, while improving (or maintaining) state-of-the-art performance.",Does the review address Data/Task?,TRUE,FALSE,"This review discusses the task of reducing parameters/data while maintaining performance, which is related to the data/task."
The authors chose to not rerank if the candidates' scores are too low or high but close.,Does the review address Experiment?,TRUE,FALSE,This question pertains to the experimental design and methods used in reranking candidates during the experiment.
"In the least, it's unclear how to assess the differences shown in this table.",Does the review address Presentation?,TRUE,FALSE,"This is a presentation issue, pointing out that the differences in the table are not clear or properly assessed."
My main problem with this paper at the moment is that it could be much better written: The overall narrative of the paper is unpolished and it’s hard at first to understand the contributions of the paper.,Does the review address Presentation?,TRUE,FALSE,"This is a criticism of the presentation, noting that the paper is not clearly written and that the contributions are hard to understand."
The idea of learning which parameters to share across languages in multilingual transformer models is original and potentially useful for designing and analyzing multilingual models in the context of NMT.,Does the review address Methodology?,TRUE,FALSE,"This discusses a methodological idea, proposing an approach for sharing parameters in multilingual transformer models."
"I do understand that the main point is the reduction of the amount of parameters (per task), but this doesn't mean that the evaluation should paint a wrong picture.",Does the review address Methodology?,TRUE,FALSE,"The review critiques the methodology, stating that reducing parameters does not justify misleading evaluation."
It is therefore not surprising that multi-task learning should help these tasks.,Does the review address Data/Task?,TRUE,FALSE,"This is related to the data/task, explaining that multi-task learning is expected to benefit tasks with similar learning objectives."
"Had `calculateTime()` been part of some standard library shared between programs, the case for generalization would have been much stronger.",Does the review address Methodology?,TRUE,FALSE,"This is a methodological issue, questioning the generalization of the method without a standard library."
- General Discussion: It would be nice if the survey of prior work in 2.2 explicitly related those methods to the desiderata in the introduction (i.e. specify which they satisfy).,Does the review address Related Work?,TRUE,FALSE,"This is about related work, suggesting that prior work should be explicitly connected to the goals outlined in the introduction."
"Beyond this, the authors conduct a multifaceted set of tests, including a non-harmful test to ensure that the watermark embedding does not significantly degrade model performance, robustness tests against second-time fine-tuning and model quantization, and an ablation study concerning the reference set to further substantiate the rationality of their backdoor data framework design.",Does the review address Ablation?,TRUE,FALSE,"This is about ablation studies and experimentation, discussing different robustness and performance tests conducted in the study."
This paper demonstrates why self-knowledge distillation as a prior distribution is a form of regularization with theoretical analysis on the gradients.,Does the review address Analysis?,TRUE,FALSE,"This refers to analysis, explaining how self-knowledge distillation is regularized through gradient analysis."
Performance better than previous approaches (although minor).,Does the review address Comparison?,TRUE,FALSE,"This is a comparison, noting that the method performs slightly better than prior approaches."
"Steinert-Threlkeld (2020) ""Towards the Emergence of Non-trivial Compositionality"" makes similar points and could be cited here as well.",Does the review address Related Work?,TRUE,FALSE,"This refers to related work, suggesting that relevant prior work should be cited."
It could be easy to be re-implemented and deployed for further research.,Does the review address Methodology?,TRUE,FALSE,"This is a methodological point, indicating that the method could be easily re-implemented for future research."
"Additionally, it would be good if the authors could report the number of examples (either raw number or as a fraction of the total dev set) for each of the categories: having that would help draw better conclusions.",Does the review address Data/Task?,TRUE,FALSE,"This is about data/task, suggesting that more detailed reporting on example counts would help interpret the results."
"For example, the $p^{\star}$ notation is also defined in Sec 2.1.",Does the review address Presentation?,TRUE,FALSE,"This is a presentation issue, pointing out that notation needs clearer definition."
The evaluation process shows that the current system (which extracts 1.,Does the review address Evaluation?,TRUE,FALSE,"This refers to evaluation, discussing the evaluation process for the proposed system."
I think the presentation of the paper needs to be improved.,Does the review address Presentation?,TRUE,FALSE,"This is a presentation critique, suggesting that the paper's clarity and presentation need improvement."
So my point 5 is important to answer and I would like to see all the details are clarified in order to make the contribution stronger.,Does the review address Presentation?,TRUE,FALSE,"This is a presentation issue, requesting clarification of details to strengthen the paper’s contribution."
Is this threshold a hyper-parameter need to be configured?,Does the review address Presentation?,TRUE,FALSE,"This refers to the presentation of methods, asking whether a threshold parameter needs configuration."
"-----General Discussion----- This paper proposes a practical model which seems working well on one dataset, but the main ideas are not very novel (see comments in Strengths).",Does the review address Methodology?,TRUE,FALSE,"This is a methodological comment, noting the model's practicality but questioning the novelty of the approach."
"The overall problem is framed as a judgment question, further enhancing the method's Uniqueness and Efficiency.",Does the review address Methodology?,TRUE,FALSE,"This relates to methodology, framing the problem in a way that highlights uniqueness and efficiency."
And more space can be freed up to further explain the results section.,Does the review address Result?,TRUE,FALSE,"This refers to the results, suggesting that more explanation of the results section would improve the paper."
"As similar efforts are already applied in several query expansion techniques (with the aim to relate the document with the query, if matching terms are absent in document).",Does the review address Related Work?,TRUE,FALSE,"This is related work, noting that similar techniques have been applied in query expansion efforts."
The authors also propose two novel pre-training settings which also show improvement over the baseline BM-25.,Does the review address Result?,TRUE,FALSE,"This refers to results, showing that the proposed pre-training settings perform better than BM-25."
There are some unclear expressions and inconsistent explanations.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is a critique of definition/description, noting unclear expressions and inconsistencies."
"In order to train reading comprehension models to perform relation extraction, they create a large dataset of 30m “querified” (converted to natural language) relations by asking mechanical turk annotators to write natural language queries for relations from a schema.",Does the review address Data/Task?,TRUE,FALSE,"This relates to data/task, explaining the creation of a large dataset for training reading comprehension models."
I also did not find a Related Work section discussing this in more detail.,Does the review address Related Work?,TRUE,FALSE,"This is about related work, indicating that the relevant prior work should be more thoroughly discussed."
"The description of the baselines is lacking and quite confusing: it's not clear why the authors have two DP-SGD baselines, and what they're exactly updating during training.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is about definition/description, pointing out confusion in the baseline description and the use of DP-SGD."
Experimental results show improvements over both the base T5 model and the large T5 model.,Does the review address Methodology?,TRUE,FALSE,"This refers to methodology, highlighting improvements in experimental results over T5 models."
This is very promising to simplify the construction of highly tuned task-specific neural pipelines and improve the multimodal and multi-task problems.,Does the review address Methodology?,TRUE,FALSE,"This discusses methodology, noting the promise of simplifying the construction of neural pipelines for multimodal and multi-task problems."
strengths 1) This paper is well written and easy to read.,Does the review address Presentation?,TRUE,FALSE,"This is a presentation note, complimenting the clarity and readability of the paper."
The world oracle conveys the meaning of being absolute truth which sounds a bit unexpected.,Does the review address Presentation?,TRUE,FALSE,"This refers to presentation, critiquing the use of the term ""oracle"" for being unclear or unexpected."
The method proposed has limited methodology contribution to the research community.,Does the review address Contribution?,TRUE,FALSE,"This is about contribution, noting that the method has limited methodological contributions."
"We expect that the model can not only achieve good performance on a single dataset, but also have the potential to transfer beyond a single dataset.",Does the review address Data/Task?,TRUE,FALSE,"This is related to data/task, suggesting the model’s potential for transferability beyond a single dataset."
"I think authors wanted to say that even though BROS does not rely on visual features, it does outperform LayoutLM which, in turn, uses visual features.",Does the review address Result?,TRUE,FALSE,"This refers to results, comparing the performance of BROS with LayoutLM."
"Authors could have provided more in-depth details (visualizations, analysis, examples) to show main differences between the proposed approach and baselines (specially LayoutLM).",Does the review address Analysis?,TRUE,FALSE,"This is analysis, suggesting that more in-depth analysis and examples could help highlight differences."
"Please report the total training and total inference time, and make a comparison with standard Transformer model.",Does the review address Methodology?,TRUE,FALSE,"This is methodological, suggesting the reporting of training and inference times, with a comparison to the standard Transformer model."
"Generally speaking, this paper puts forward a universal retrieval scheme, and achieves good results, the specific advantages are as follows:  (1) The sparse alignment of multi-vector retrieval is a good solution to solve the retrieval effect and efficiency.",Does the review address Result?,TRUE,FALSE,"This is a result-related point, describing the advantages and improvements achieved with the proposed retrieval scheme."
"Are the authors using off-the-shelf code (in which case, please refer and cite, which would also make it easier for the reader to understand and replicate if necessary)?",Does the review address Related Work?,TRUE,FALSE,"This relates to related work, asking whether the authors used off-the-shelf code and if it should be cited for ease of understanding and replication."
"I am concerned this is quite low and could yield exaggerated instability, especially for large Transformer models.",Does the review address Methodology?,TRUE,FALSE,"This is a methodological concern, expressing worries about the low value leading to instability, particularly for large Transformer models."
"Although the presentation can be polished, the overall narrative and explanation is clear and easy to follow.",Does the review address Presentation?,TRUE,FALSE,"This is a presentation-related comment, suggesting that while polishing is needed, the overall explanation is clear and understandable."
"Theoretically, they showed that synchronized updates to the low-level and high-level policy may never converge, yet asynchronized updates guarantees convergence.",Does the review address Methodology?,TRUE,FALSE,"This is a methodological point, discussing the theoretical justification for the updates to low- and high-level policies in the system."
"The authors say ""we focus on larger datasets from GLUE (MNLI, QNLI, QQP and SST-2), as it is less meaningful to discuss efficient training"", but then report and analyze results from other GLUE datasets as well.",Does the review address Data/Task?,TRUE,FALSE,"This addresses the data/task, noting the inconsistency between the stated focus and the datasets actually used in the analysis."
", do you mean Table 3?Does the review address Presentation? 1936The Non-trivial Sub-network"" paragraph feels like it should be part of the Experiments section.",Does the review address Experiment?,TRUE,FALSE,"This is a presentation issue, clarifying which table is being referred to in the review."
The way in which you have collected these samples is likely to create a bias towards simple missing conditions.,Does the review address Methodology?,TRUE,FALSE,"This refers to experimentation, suggesting that the ""Non-trivial Sub-network"" paragraph would be more appropriately placed in the Experiments section."
They should have comparable numbers of parameters.,Does the review address Methodology?,TRUE,FALSE,"This is a methodological concern, pointing out the potential bias introduced by the way samples were collected."
They provide a training that guarantees convergence to local maxima.,Does the review address Methodology?,TRUE,FALSE,"This refers to methodology, suggesting that models should have comparable parameters for fair comparison."
"####Summary:  To tackle situations where compositionality is mostly required at inference time, the paper proposes a novel data augmentation method with an RNN based generator (recombination); to make the generator generate highly compositional patterns, the paper proposes a resampling method.",Does the review address Data/Task?,TRUE,FALSE,"This is related to methodology, noting that the training process guarantees convergence to local maxima."
* The paper is well motivated and easy to understand and follow.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This is about data/task, explaining the novel approach to data augmentation and the resampling method to improve compositionality at inference time."
"If successful, the approach could be impactful because it speeds up prediction.",Does the review address Significance?,TRUE,FALSE,"This is a positive comment on the intuition/motivation, validating the clarity and motivation behind the paper's approach."
It is then not clear how each fact block (grey squares in Figure 4) functions as a whole.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This refers to significance, suggesting the potential impact of the approach in speeding up prediction."
"* Because BPE is such a standard baseline, why do you not include it as a baseline?",Does the review address Comparison?,TRUE,FALSE,"This is a definition/description issue, indicating that the function of each fact block in Figure 4 is unclear."
1.The performance of this model is closely related to both the AST encoding frontend and the LLaMA model's performance.,Does the review address Result?,TRUE,FALSE,"This is a comparison point, questioning why BPE, a standard baseline, was not included in the evaluation."
"The paper meticulously provides all experimental details, and the ablation study helps to validate the design components, enhancing the overall robustness of the research.",Does the review address Experiment?,TRUE,FALSE,"This is a result, noting the relationship between the model's performance and the AST encoding frontend and LLaMA model."
"Due to the very “flat” portions of the softmax function, there can be meaningful differences between the logits corresponding to 2 different words, but the LM probabilities for those words are extremely similar (and thus, harder for a linear model to distinguish).",Does the review address Presentation?,TRUE,FALSE,"This refers to experimentation, praising the experimental details and ablation study that validate the design components."
"The work introduces the Transformer-QL, a transformer-based model that aims to capture long distance dependencies in the input.",Does the review address Methodology?,TRUE,FALSE,"This is a presentation issue, discussing a challenge with the softmax function that makes distinguishing between words difficult."
Some discussion on the current design choices and why making the proposed methods features to some of the other baselines is not a way to achieve some benefits is not the right way to do it.,Does the review address Comparison?,TRUE,FALSE,"This is methodology, introducing the Transformer-QL model that aims to capture long-distance dependencies."
Weaknesses: There is no contribution/novelty from the modeling/methods side.,Does the review address Contribution?,TRUE,FALSE,"This refers to comparison, suggesting that a discussion on design choices and why certain methods are not beneficial would strengthen the paper."
The fact that the previous study reported a 126 perplexity baseline using LSTM and the LSTM's perplexity of 106.9 provided by the author showed that the FBIS gives an advantage to computing the language model's perplexity when tested on PTB.,Does the review address Methodology?,TRUE,FALSE,"This is a critique of contribution, claiming that the paper does not offer any significant contribution in terms of modeling or methods."
I understand that LMU might have limited capacity but this is not specifically discussed and it is unclear how each component contributes to the end performance.,Does the review address Result?,TRUE,FALSE,"This is a methodology point, demonstrating the advantage of FBIS in computing perplexity on the PTB dataset."
"In sec4.1, the authors said ""A batch size of 256 is employed, "", does that mean K=256 in Algorithm 1?",Does the review address Presentation?,TRUE,FALSE,"This is a result, expressing concern over the unclear contribution of each component in the LMU to the end performance."
Many of the notations look cumbersome and I suspect that there is still room for making the notations more accessible for new readers.,Does the review address Presentation?,TRUE,FALSE,"This is a presentation question, clarifying the relationship between batch size and the parameter KK in Algorithm 1."
* It is explained in the paper that the runtime environment ensures that the proofs are never circular.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is a presentation issue, suggesting that the notations could be simplified for better accessibility, especially for new readers."
It seems to be making every previously known augmentation approach better.,Does the review address Result?,TRUE,FALSE,"This is a definition/description issue, clarifying the runtime environment's role in ensuring the proofs are not circular."
3) The ablation study and visualization analysis of the experimental results are sufficient.,Does the review address Ablation?,TRUE,FALSE,"This is a result, suggesting that the paper's approach improves upon existing augmentation methods."
"Prior work has explored ""learning-to-share""  strategies for parameter sharing in multi-task learning (see Ruder et al., AAAI 2018), and using gating/masking to control computational paths in a differentiable way (see Fan et al., ICLR 2019, Sukhbaatar et al., ACL 2019); it is clear that the focus is NMT but it should be worth mentioning/discussing such studies to better situate the work and to help the reader assess the actual contributions.",Does the review address Methodology?,TRUE,FALSE,"This refers to ablation, noting that the ablation study and visualizations sufficiently validate the experimental results."
"In section 3.3.3 ""THE MIX-UP OF MULTIPLE TYPES,"" the authors mention that ""it is possible to embed multiple Double-I watermarks in a model, which theoretically has the potential to enhance the robustness of our watermarking technique.""",Does the review address Result?,TRUE,FALSE,"This refers to methodology, suggesting that prior work on parameter sharing and gating/masking in multi-task learning should be discussed to better situate the current work."
This paper conducts comprehensive evaluations on the influence of selecting specific examples for inclusion in the prompt.,Does the review address Presentation?,TRUE,FALSE,"This is a result, discussing the potential for embedding multiple Double-I watermarks to enhance the robustness of the technique."
"2016 [2] A syntactic neural model for general-purpose code generation, Yin and Neubig 2017 [3] Making Neural Programming Architectures Generalize via Recursion, Cai et al. 2017  ####Authors have engaged in the discussion, clarified questions about the paper and addressed comments in its newest revision.",Does the review address Methodology?,TRUE,FALSE,"This refers to presentation, noting that the paper evaluates the impact of selecting specific examples for the prompt."
Pros: - Weakly-supervised method for video moment localization is a reasonable and important direction.,Does the review address Presentation?,TRUE,FALSE,"This refers to methodology, indicating that the authors have responded to feedback and clarified aspects of the paper in its latest revision."
2) The presentation is very good and easy to follow.,Does the review address Presentation?,TRUE,FALSE,"This is a presentation point, highlighting the promising direction of weakly-supervised methods for video moment localization."
"Finally, writing in general can be made clearer:  1.",Does the review address Presentation?,TRUE,FALSE,"This is a presentation-related point, suggesting that clarity in writing could be improved."
I think I'd like to see a discussion of sufficient number D analytically or empirically.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This refers to definition/description/detail/discussion, suggesting an analytical or empirical discussion of the number DD."
"## (Minor) Imprecise Claim about Poly(n) Size  In Theorem 1, the authors claim: > We consider log-precision, constant-depth, and polynomial-size Transformers: for Transformers whose input is of length n, the values at all neurons are represented with O(log n) bits, the depth is constant, and the number of neurons is O(poly (n)).",Does the review address Theory?,TRUE,FALSE,"This is related to theory, pointing out an imprecise claim in Theorem 1 regarding polynomial-size Transformers."
It's not really okay to put up the tables and show the perplexity and BLEU scores without some explanation.,Does the review address Result?,TRUE,FALSE,"This refers to results, suggesting that tables showing perplexity and BLEU scores should be accompanied by explanations."
"Edit after seeing others reviews -- I think I gave this paper a MUCH higher score than the other reviewers, simply because it is very novel with Fon language.",Does the review address Novelty?,TRUE,FALSE,"This refers to novelty, highlighting the reviewer's higher score due to the paper's novelty, particularly with Fon language."
"First, the authors are missing a great deal of related work: Neelakantan at al. 2015 (https://arxiv.org/abs/1504.06662) perform zero-shot relation extraction using RNNs over KB paths.",Does the review address Related Work?,TRUE,FALSE,"This is related to related work, pointing out the absence of important prior work in the paper."
"But then at the end, I saw you include Encode as step 4, so it is the machine...",Does the review address Methodology?,TRUE,FALSE,"This refers to methodology, pointing out an inconsistency in the description of steps in the methodology section."
The graph indicates that for MNLI and QNLI 60% seems like a better choice.,Does the review address Result?,TRUE,FALSE,"This refers to results, discussing a specific result and the best choice for MNLI and QNLI."
The difference in the ablation results seem quite small (tables 3 and 4).,Does the review address Result?,TRUE,FALSE,"This is related to results, suggesting that the differences in ablation results are too small."
"Considering the actual experimental setting is significantly different from the VIP setting, the title “Answering Queries with CLIP Adversely affects VIPs explanations” and the conclusions seem a bit misleading.",Does the review address Experiment?,TRUE,FALSE,"This refers to experiment, indicating that the experimental setting is different and thus misleading conclusions may be drawn."
The difference in the ablation results seem quite small (tables 3 and 4).,Does the review address Significance?,TRUE,FALSE,"This is related to significance, pointing out that the differences in the ablation results are minimal."
Margins are very small for the Average differences across all datasets as well -- have you considered confidence intervals on those as well?,Does the review address Data/Task?,TRUE,FALSE,"This refers to data/task, suggesting the inclusion of confidence intervals to account for small margins in the results."
"Consequently, there is a possibility that it could be utilized in real-world applications of in-context learning.",Does the review address Methodology?,TRUE,FALSE,"This refers to methodology, suggesting potential real-world applications for in-context learning."
"I look forward to the author's discussion of the additional learnable parameters introduced in addition to CLIP's pre-trained model, and compare the number with other methods.",Does the review address Comparison?,TRUE,FALSE,"This refers to comparison, asking for a discussion of additional learnable parameters and how they compare with other methods."
(2) Is table 1 an average over the 17 embeddings described in section 5.1?,Does the review address Methodology?,TRUE,FALSE,"This refers to methodology, seeking clarification on whether Table 1 is an average over 17 embeddings."
Solid experiments demonstrating the proposed method outperforms other existing approaches.,Does the review address Experiment?,TRUE,FALSE,"This is related to experiment, praising the solid experiments and the method's superior performance over existing approaches."
(3) The loss for answer prediction in Eq 6 is not clearly described: do you use an aggregated embedding over all nodes for prediction?,Does the review address Methodology?,TRUE,FALSE,"This refers to methodology, asking for clarification on how the loss for answer prediction is computed."
The provided experiments are more like baselines for self-evaluation other than state-of-the-art performance.,Does the review address Evaluation?,TRUE,FALSE,"This refers to evaluation, pointing out that the experiments are more self-evaluation than a comparison to state-of-the-art performance."
"Consider, for a moment, that they are using only 25,000 input/output pairs for their training/validation/testing.",Does the review address Data/Task?,TRUE,FALSE,"This refers to data/task, highlighting concerns about the small dataset size used for training and evaluation."
"Did the author try other window widths, for example width `1' to extract unigram features, `3' to trigram, or use them together?",Does the review address Presentation?,TRUE,FALSE,"This refers to presentation, asking if the authors considered using different window widths for feature extraction."
An ablation study is carried out to rule out the possibility that the benefits from PACT come from simply regularizing the model.,Does the review address Ablation?,TRUE,FALSE,"This refers to ablation, noting the ablation study aimed at ruling out simple regularization as the source of benefits."
Another limitation is that it is unclear whether the improvement would hold when the size of the model increases; the evaluation is dealing with scaling laws after all.,Does the review address Evaluation?,TRUE,FALSE,"This refers to evaluation, pointing out the uncertainty about whether improvements hold when the model size increases."
The optimality of deterministic inference does not hold empirically due to class imbalance or discrepancy between training and test sets.,Does the review address Methodology?,TRUE,FALSE,"This refers to methodology, discussing issues with the empirical validity of deterministic inference."
Experimental evaluation shows competitive performance.,Does the review address Evaluation?,TRUE,FALSE,"This is related to evaluation, noting that the experimental evaluation demonstrates competitive performance."
I agree with the authors that extending the context is important.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This refers to intuition/justification/motivation/validation, agreeing with the importance of extending the context."
"Along with ablations and trying LMs of varying sizes, their technique is compared against many other existing selective annotation approaches and shown to consistently outperform the latter.",Does the review address Methodology?,TRUE,FALSE,"This refers to methodology, indicating that the technique was compared with many existing approaches and showed superior performance."
"Sec 3.1 contains results on captionnet, Sec 4 also contains results on captionnet.",Does the review address Result?,TRUE,FALSE,"This refers to results, pointing out that results related to CaptionNet are presented in multiple sections."
"Some of the details for model description are missing and confusing, e.g., (1) How the representation is initialized for a supernode corresponding to a fact triplet and how does the supernode propagate information to the sub-nodes?",Does the review address Presentation?,TRUE,FALSE,"This refers to presentation, indicating that some details in the model description are unclear or missing."
Strengths * Improving the data efficiency in language models is an important problem that so far studies have shown that can be achieved by scaling the size of the model.,Does the review address Methodology?,TRUE,FALSE,"This refers to methodology, highlighting the importance of improving data efficiency in language models, typically achieved by scaling the model."
"It is important to note that while BLEU of other methods reduced on the Fr→Fon task, WB improved on it.",Does the review address Presentation?,TRUE,FALSE,"This refers to presentation, highlighting the improvement of WB in the Fr→Fon task compared to other methods."
- During the training of the decoder how do you make sure that the decoder uses the information given by the latent plan?,Does the review address Methodology?,TRUE,FALSE,"This refers to methodology, asking for clarification on how the decoder utilizes information from the latent plan during training."
"It is also strange that the multi-cluster approach, which discards inter-cluster (word and language) semantic information performs the best with respect to the extrinsic metrics.",Does the review address Evaluation?,TRUE,FALSE,"This refers to evaluation, discussing the unexpected performance of the multi-cluster approach when it discards certain semantic information."
"For example, sentence inference tasks such as MNLI and RTE are common tasks in NLP field.",Does the review address Data/Task?,TRUE,FALSE,"This refers to data/task, noting that MNLI and RTE are common tasks used for sentence inference in NLP."
Explicit modeling the generation order is not a very novel idea that there have been many works on this topic.,Does the review address Novelty?,TRUE,FALSE,"This refers to novelty, pointing out that explicitly modeling the generation order is not a new idea."
"It would help to clarify when what you predict is a guard, a precondition, an invariant, or something else.",Does the review address Presentation?,TRUE,FALSE,"This refers to presentation, suggesting that clarifying what is predicted (guard, precondition, invariant) would improve the paper."
"I do understand that the main point is the reduction of the amount of parameters (per task), but this doesn't mean that the evaluation should paint a wrong picture.",Does the review address Evaluation?,TRUE,FALSE,"This refers to evaluation, suggesting that the evaluation might misrepresent the results despite the reduction in parameters."
"Second, the authors neither 1) evaluate their model on another dataset or 2) evaluate any previously published models on their dataset.",Does the review address Data/Task?,TRUE,FALSE,"This refers to data/task, noting that the paper lacks evaluation on other datasets and comparison with existing models on the proposed dataset."
Hyper-parameter balancing strategies in Section 3.1 is not a solid theoretical analysis.,Does the review address Theory?,TRUE,FALSE,"This refers to theory, pointing out that the hyperparameter balancing strategies lack solid theoretical backing."
"I would suggest the following:   * To make the comparison more fair, I would suggest to train transformer models with varying size and derive a power law based on the exact same experimental configuration used for LMU models.",Does the review address Presentation?,TRUE,FALSE,"This refers to presentation,"
- I don't see why your theory does not generalize to a _masked_ language modeling (MLM).,Does the review address Theory?,TRUE,FALSE,This comment questions why the theory doesn’t generalize to MLM.
"The paper has ""support set"" and ""support instructions"" at many places but it is unclear to me what it actually means.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The reviewer expresses confusion about the meaning of ""support set"" and ""support instructions,"" requesting further clarification."
"-----General Discussion----- This paper proposes a practical model which seems working well on one dataset, but the main ideas are not very novel (see comments in Strengths).",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This is a general discussion about the novelty of the ideas presented in the paper.
Details / Questions: * It seems to me that the GLUE results might be within the margin of error.,Does the review address Result?,TRUE,FALSE,The reviewer questions whether the GLUE results are statistically significant or just within the margin of error.
Might be useful to define what is exactly meant by 'comprehensibility' 4.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The reviewer asks for a clearer definition of ""comprehensibility."""
"But from Table 5., the trend of performance seems to increase with the increased prepositional phrases (with 84.7 being the max for 4 PPs).",Does the review address Methodology?,TRUE,FALSE,"This refers to the methodology, specifically the impact of prepositional phrases on performance as seen in Table 5."
"**Major concern** If I understand correctly (and please correct me if I am wrong), in Theorem B.1, the ratio between the downstream error $\ell_\mathcal{T}(\\{p_{\cdot\mid s}\\}) - \tau$ and the pre-training error $\ell_\text{xent}(\\{p_{\cdot\mid s}\\})-\ell_\text{xent}^\ast$ is _hidden_ in the $\gamma(p_{\mathcal{T}}; \\{p_{\cdot\mid s}\\})$ coefficient.",Does the review address Methodology?,TRUE,FALSE,The comment addresses a methodological concern related to the details of Theorem B.1.
- Improvement over previous state-of-the-art models.,Does the review address Methodology?,TRUE,FALSE,This discusses a comparative improvement in the methodology over previous state-of-the-art models.
"Therefore, it is necessary to compare OTTER and CLIP on the same-scaled datasets.",Does the review address Data/Task?,TRUE,FALSE,"The comment is related to data/task, suggesting a fair comparison using the same-scaled datasets."
"Overall, I think the paper provides an interesting view of discussion, but there are many flaws in the current version which needs to be corrected before a more serious consideration.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This is a general discussion of the paper’s strengths and weaknesses.
I suppose it would be hard to define auxiliary tasks for simpler systems.,Does the review address Methodology?,TRUE,FALSE,"The reviewer discusses the difficulty of defining auxiliary tasks in simpler systems, which is a methodological concern."
(4) How exactly is the interaction module processed?,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The reviewer seeks clarification on the processing of the interaction module, asking for more details."
* Description of refl and tidy-bfs baselines appears much late in the paper.,Does the review address Comparison?,TRUE,FALSE,"This comment addresses the delayed mention of baselines, suggesting an issue with the comparison section."
"According to the evaluation, the proposed method shows its effectiveness especially when the finetuning data is limited.",Does the review address Data/Task?,TRUE,FALSE,"The reviewer discusses the effectiveness of the proposed method with limited fine-tuning data, a data/task concern."
Could you explain the significance of this result again?,Does the review address Significance?,TRUE,FALSE,The reviewer asks for clarification on the significance of the result.
"But I support given a fixed set of phrase pairs at train time, the attention mechanism at the phrasal level can be pre-computed but at inference (apply the attention on new data at test time), this might be kind of problematic when the architecture is scaled to a larger dataset.",Does the review address Methodology?,TRUE,FALSE,"The reviewer discusses a potential issue with the attention mechanism as the model scales, focusing on methodology."
"-----General Discussion----- This paper proposes a practical model which seems working well on one dataset, but the main ideas are not very novel (see comments in Strengths).",Does the review address Novelty?,TRUE,FALSE,This comment directly addresses the novelty of the ideas in the paper.
• It is stated that (page 7) the submission to GLUE leaderboard uses only single-task finetuning.,Does the review address Data/Task?,TRUE,FALSE,"This refers to the use of single-task fine-tuning in the GLUE submission, a data/task comment."
I suggest to put the model figure more close to the methodology section and the qualitative results on page 8.,Does the review address Methodology?,TRUE,FALSE,The reviewer suggests a reorganization of the figures and results to better align with the methodology.
It is not clear to me what the value of OpenAQA dataset is on top of of the textual metadata available with most of these datasets.,Does the review address Significance?,TRUE,FALSE,The reviewer questions the added value of the OpenAQA dataset in terms of its significance.
"‘Wang & Cho’ were not the first who used Transformers generativity (see Vaswani, 2017).",Does the review address Methodology?,TRUE,FALSE,"This comment points out that Wang & Cho were not the first to use Transformer-based generative models, addressing methodology."
"For this purpose, the authors introduced the definition of a ""natural"" task.",Does the review address Data/Task?,TRUE,FALSE,"The reviewer refers to the introduction of the definition of a ""natural"" task, related to data/task."
The findings from the analysis are an important addition to the understanding of the role of language specific parameters in multilingual NMT.,Does the review address Analysis?,TRUE,FALSE,The reviewer mentions the importance of the analysis in understanding multilingual NMT.
"Theoretically, it shows that language models which are close to the “true” language model are guaranteed to attain strong performance on natural tasks.",Does the review address Analysis?,TRUE,FALSE,This comment addresses theoretical findings related to the analysis of language models and their performance on natural tasks.
"The true contribution appears to be the improvement of the overlapping strategy tokenization for DNA pretraining, which diverges from the broader theme of ""rethinking the pretraining for DNA sequence.""",Does the review address Contribution?,TRUE,FALSE,"The reviewer discusses the contribution of the paper, focusing on the improvement of tokenization strategy for DNA pretraining."
"The paper made a impactful finding for practicing adversarial training, that mixture of signals at different depth of of the generator can stabilize ELECTRA-style models trained adversarially using Gumble-Softmax relaxation.",Does the review address Methodology?,TRUE,FALSE,The reviewer highlights a significant methodological contribution regarding adversarial training with ELECTRA-style models.
"I would suggest using the phrase ""weighted SVD"" early on in the introduction (e.g., exactly when you introduce your new method).",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This comment suggests introducing the term ""weighted SVD"" earlier in the paper to clarify the method."
Chain-of-thought prompting elicits reasoning in large language models.,Does the review address Related Work?,TRUE,FALSE,"This comment refers to chain-of-thought prompting, discussing relevant related work."
"If the experiment does not peer inside the structure of the noise and treats it as a gray box instead, it is not much different than a discrete-channel environment where random edits are made to the message.",Does the review address Experiment?,TRUE,FALSE,"The reviewer comments on the experimental design, specifically the treatment of noise in the experiment."
Why isn’t an asymmetric architecture more natural?,Does the review address Methodology?,TRUE,FALSE,This comment raises a methodological question about the architecture choice.
- The ablation experiments and optimized prompt analysis are insightful.,Does the review address Methodology?,TRUE,FALSE,This praises the methodological choices made in the ablation experiments and prompt analysis.
"*  What is the contribution of each specific design choice, such as the FFN/global attention and implicit self-attention, to the end performance?",Does the review address Ablation?,TRUE,FALSE,The reviewer asks for an ablation study to determine the contribution of specific design choices.
"- P5, Sec 4.1: ""The result suggests that small test cross-entropy (hence test perplexity)..."" Same question as above.",Does the review address Presentation?,TRUE,FALSE,"The reviewer raises a question about the result presented in Section 4.1, focusing on its presentation."
"Main weaknesses (for each of them, see detailed comments below): - The description of the corpus and the experimental setup frequently lacked some details; there is also no comparison to an existing Thai UD resource.",Does the review address Comparison?,TRUE,FALSE,The reviewer notes that the paper lacks comparison to an existing Thai UD resource.
**Experimental results** The presentation of the experimental results is clear.,Does the review address Presentation?,TRUE,FALSE,The reviewer states that the experimental results are clearly presented.
- The analysis of compositionality given is insufficient.,Does the review address Analysis?,TRUE,FALSE,The reviewer points out that the analysis of compositionality is inadequate.
The cost of training NLP models: A concise overview.,Does the review address Related Work?,TRUE,FALSE,This comment refers to related work regarding the cost of training NLP models.
"Despite very encouraging results, several important methodological questions about the source of the efficiency gains and other aspects of the paper are left unanswered.",Does the review address Result?,TRUE,FALSE,The reviewer mentions unanswered methodological questions about the source of efficiency gains.
"To name a few:   -“Language Models as Knowledge Bases?” EMNLP 2019   -“Commonsense Knowledge Mining from Pretrained Models.” EMNLP 2019   -“Comet: Commonsense Transformers for Automatic Knowledge Graph Construction.” ACL 2019 - Although the proposed model achieves quantitative improvements over Angeli et al. (2015), it seems unclear whether these improvements are due to the factual knowledge encoded in the pre-trained LM, as claimed.",Does the review address Methodology?,TRUE,FALSE,"The comment questions the methodology used in the paper, specifically the claimed improvements due to factual knowledge in the pre-trained language model."
Section 3.4  does not describe clearly enough how attention maps were used for predicting contact maps.,Does the review address Presentation?,TRUE,FALSE,"The review mentions the presentation of how attention maps are used, and the lack of clarity is pointed out."
"#### Weakness - In Table 1, the WNLI task is excluded from the GLUE benchmark, I wonder what is the reason this task is removed?",Does the review address Data/Task?,TRUE,FALSE,This question concerns the data/task and the reason behind excluding a particular task from the benchmark.
- The visualization section is only a minor contribution; there isn't really any innovation or findings about what works or doesn't work here.,Does the review address Result?,TRUE,FALSE,"The comment critiques the result section, highlighting the lack of innovative findings or contributions in the visualization."
And there is no further explanation and ablation study on the design of the dynamic threshold.,Does the review address Ablation?,TRUE,FALSE,"The reviewer points out that there is no ablation study on the dynamic threshold, which is related to the design choices and their evaluation."
The algorithms were clear and the comments were useful for understanding the proposed idea.,Does the review address Methodology?,TRUE,FALSE,"This comment praises the methodology, emphasizing the clarity of the algorithms and their explanation."
"It requires more analysis about experimental results, such as Figure 1 and tables in Section D.",Does the review address Experiment?,TRUE,FALSE,"This review mentions the experiment section, requesting more analysis of the experimental results."
"In contrast, focusing on a solid target and ignoring the rest of potential targets, CLIP may still infer the generalized information provided by many-to-many relationships due to the wide range of data collection.",Does the review address Data/Task?,TRUE,FALSE,"This review addresses the data/task aspect, discussing the CLIP model’s approach to data and task relationships."
"Discussion of D  Since RF is not the major contribution, you summarize existing results of FA in sec2.2.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This review mentions the discussion of existing work, specifically in section 2.2, concerning FA and RF."
"It is widely acknowledged and studied that the complexity (i.e., the length or reasoning steps of the CoT annotations) significantly influences the performance of the LLMs.",Does the review address Presentation?,FALSE,TRUE,This sentence does not mention presentation.
"Even though the results don’t show that the proposed loss function and proposed “conditional mean features” give improvements over baselines, the empirical results show that the basic assumptions and definitions in the theoretical analysis are relatively realistic.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This comment is related to the theoretical analysis and the assumptions in the model, touching on definitions and their explanations."
It is also hard to navigate through all the information due to different ablation targets.,Does the review address Ablation?,TRUE,FALSE,"This comment discusses the difficulty of navigating the ablation study, pointing to the varying targets as a challenge."
"The description of the baselines is lacking and quite confusing: it's not clear why the authors have two DP-SGD baselines, and what they're exactly updating during training.",Does the review address Comparison?,TRUE,FALSE,"This comment addresses comparison, questioning the clarity and rationale behind the baselines used."
"Comparative Analysis and Benchmarking:  The comparisons presented in Tables 1 and 2 focus solely on the improvements over one reference work (Ma et al., 2023).",Does the review address Data/Task?,TRUE,FALSE,"The comment refers to the data/task, discussing the limited comparative analysis and its focus on one reference."
"This idea is novel and interesting to me, and the derivation and experiment results look encouraging.",Does the review address Result?,TRUE,FALSE,"This review touches on the result section, noting the novelty and encouraging results from the derivation and experiments."
Using Transformer attention maps for protein contact prediction is not new.,Does the review address Methodology?,TRUE,FALSE,"This comment discusses the methodology, critiquing the novelty of using Transformer attention maps for protein contact prediction."
"That is, unless I’ve just missed something, it seems that all of the core components of N-Bref are lifted from prior work with perhaps some minor augmentation.",Does the review address Novelty?,TRUE,FALSE,"The review questions the novelty of the N-Bref components, indicating that they are largely taken from prior work."
"For a more comprehensive analysis, it would be instructive to see how the approach compares with a broader spectrum of state-of-the-art methods.",Does the review address Methodology?,TRUE,FALSE,"This review touches on methodology, suggesting that a broader comparison with other methods would provide a more comprehensive analysis."
"Along with ablations and trying LMs of varying sizes, their technique is compared against many other existing selective annotation approaches and shown to consistently outperform the latter.",Does the review address Ablation?,TRUE,FALSE,"This comment mentions ablation, referring to the use of ablation studies and comparisons with other approaches."
This paper presents an effective way to make use of this idea.,Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, praising the approach taken in the paper."
The proposed method improves modestly on BERT on the GLUE suite of problems.,Does the review address Result?,TRUE,FALSE,"The comment refers to results, discussing the improvement of the proposed method over BERT on GLUE."
"* It will be cool to show some results on BLEU improvement on machine translation, or user studies on language generation tasks, to demonstrate its practical impact, as the quantitative metrics right now are still kind of artificial.",Does the review address Evaluation?,TRUE,FALSE,"This review mentions evaluation, suggesting the inclusion of practical results, such as BLEU scores or user studies."
**Empirical**:  One issue with the language modelling experiment is the choice of evaluation and train set.,Does the review address Methodology?,TRUE,FALSE,"This comment touches on methodology, critiquing the choice of evaluation and training sets used in the experiment."
Ablations show the necessity of applying a 2-step intermediate training scheme with mixed training followed by joint training.,Does the review address Methodology?,TRUE,FALSE,"This review mentions methodology, highlighting the importance of a 2-step intermediate training scheme."
"The key ideas are: (i) training longer with bigger batches over more data, (ii) removing NSP, (iii) training over long sequences, and (iv) dynamically changing the masking pattern.",Does the review address Data/Task?,FALSE,TRUE,"This sentence discusses key ideas, which are more related to methodology."
"In fact, empirical evidence suggest that LMs do memorize n-grams from their training data somewhat, but not full examples (see [McCoy et al.",Does the review address Result?,TRUE,FALSE,"This review discusses results, referencing empirical evidence on language models' memorization abilities."
"Although the individual components are similar to previous work, they are combined in a novel way that shows a path toward longer and more efficient context lengths.",Does the review address Methodology?,TRUE,FALSE,"This review discusses methodology, highlighting the novel combination of components and their implications."
"Finally, the CALM intermediate objectives share many properties with all of the datasets tested on and are likely calibrating the model to the kind of correlations they should expect to predict in advance of finetuning.",Does the review address Methodology?,TRUE,FALSE,"This review mentions methodology, explaining the role of CALM objectives in the model’s calibration."
It might be valuable to evaluate PENGI on Open ended tasks.,Does the review address Significance?,TRUE,FALSE,"This review mentions significance, suggesting the evaluation of PENGI on open-ended tasks."
"I have no experience with these kinds of NLU models, so I can't say with confidence whether the architectural additions proposed are well-motivated, but to me it feels like there is not a strong justification for adding these particular features to the BERT architecture, and the results do not clearly demonstrate their utility except in the ""lexical_overlap"" case.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,This review questions the intuition/justification for the architectural additions to BERT.
"The only interesting aspect is adapting the training procedure to contrastive learning, which is trivial.",Does the review address Methodology?,TRUE,FALSE,"This review discusses methodology, critiquing the novelty and value of the contrastive learning adaptation."
"In sec4.1, the authors said ""A batch size of 256 is employed, "", does that mean K=256 in Algorithm 1?",Does the review address Methodology?,TRUE,FALSE,"This comment is about methodology, specifically questioning the consistency between the reported batch size and an algorithm."
It also substantially improves on BERT with respect to a class of examples that are designed to confound models that learn superficial heuristics based on word occurrence.,Does the review address Result?,TRUE,FALSE,"This comment refers to the result, highlighting improvements over BERT in specific example classes."
"About type dependency graph: 1) Comparing to previous work (e.g, Allamanis et.al, ICLR 18), it seems the construction of the task specific graph is the major contribution, where the novelty is a bit limited.",Does the review address Novelty?,TRUE,FALSE,"This review touches on novelty, questioning the extent of innovation in the task-specific graph construction."
"This setting captures the design and the hardware restriction of realistic T-LLMs and is common in the literature on the theoretical power of T-LLMs (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022; Merrill & Sabharwal, 2023).",Does the review address Presentation?,FALSE,TRUE,This sentence does not mention presentation.
"This is especially disappointing as the objectives introduced _directly_ match the task in CommonGEN, making this intermediate training a form of noisy training data rather than pretraining.",Does the review address Data/Task?,TRUE,FALSE,"This review touches on data/task, questioning the use of intermediate training as noisy data rather than pretraining."
Both publications appeared on arXiv at least one month before the ICLR submission deadline and are not clearly discussed in the paper.,Does the review address Related Work?,TRUE,FALSE,"The review mentions that related work is missing from the paper, referring to previous publications that were not cited or discussed."
It does not follow from the theoretical results that adding similar sentences will be a good thing.,Does the review address Methodology?,TRUE,FALSE,"The review questions the methodology, specifically how the theoretical results translate to practical outcomes."
"However, the teacher networks used in the experiments are not always uni-modal.",Does the review address Experiment?,TRUE,FALSE,"The review addresses an experiment-related issue, mentioning the use of multi-modal teacher networks instead of uni-modal ones."
The methodology is explained clearly and experiments are executed with a considerable amount of detail.,Does the review address Methodology?,TRUE,FALSE,The review highlights the clarity of the methodology and the detail in which the experiments are conducted.
"I think authors wanted to say that even though BROS does not rely on visual features, it does outperform LayoutLM which, in turn, uses visual features.",Does the review address Comparison?,TRUE,FALSE,This mentions a comparison between the proposed method (BROS) and an existing model (LayoutLM).
"Weakness While the ablation study and visualization analysis are done, the key evaluations are missing.",Does the review address Presentation?,FALSE,TRUE,"This sentence discusses missing key evaluations, which does not relate to presentation."
"- P6, Sec 4.3: ""In fact, $f$ almost always performs better than ..."" This part seems intriguing despite the linear relationship shown in figure 1.",Does the review address Presentation?,TRUE,FALSE,The review brings attention to a specific result in the presentation that seems interesting but needs further clarification.
"- P8, Table 2: The results from using Quad look worse than the above two.",Does the review address Significance?,TRUE,FALSE,"The review refers to a result in Table 2, discussing the poor performance of a particular method (Quad)."
"In my knowledge, the previous work has not yet attempted to tackle this problem.",Does the review address Novelty?,TRUE,FALSE,"The review highlights the novelty of the paper, claiming that the problem addressed has not been tackled in prior work."
- The choice in deciding how many template tokens are used is unclear.,Does the review address Presentation?,TRUE,FALSE,This concerns the unclear experimental design related to the number of template tokens used.
It might be good to address the existing literature on compressing trained neural networks which also goes beyond simply trying to minimize the Frobenius norm of the difference between the weights.,Does the review address Related Work?,TRUE,FALSE,The review suggests referencing related work in neural network compression.
Since we observe that the randomly pruned models do not competitive performance ...: how uncompetitive?,Does the review address Result?,TRUE,FALSE,The review requests further clarification on the uncompetitive performance of randomly pruned models.
"Nonetheless, the current paper leaves too many open questions regarding the validity of the experiments.",Does the review address Experiment?,TRUE,FALSE,This points out that there are unresolved issues in the experiment's validity.
"The proposed models -- which seem to be an application of various tree-structured recursive neural network models -- demonstrate a nice performance increase compared to a fairly convincing, broad set of baselines (if we are able to trust them; see below).",Does the review address Methodology?,TRUE,FALSE,"This describes the methodology used and highlights the model's performance, though with some skepticism about trusting the results."
"This paper makes comparison with techniques used in active learning (AL), domain shift detection (DS), and multi-domain sampling to combine data from multiple sources.",Does the review address Methodology?,TRUE,FALSE,The review mentions a comparison with techniques from different areas like active learning and domain shift detection.
"This setting captures the design and the hardware restriction of realistic T-LLMs and is common in the literature on the theoretical power of T-LLMs (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022; Merrill & Sabharwal, 2023).",Does the review address Related Work?,TRUE,FALSE,The review refers to related work on the theoretical aspects of T-LLMs.
Performance: Concept-QA appears to perform well when evaluated along multiple axes.,Does the review address Result?,TRUE,FALSE,The review mentions positive results from experiments (performance of Concept-QA) along multiple evaluation axes.
The results on Split H are positive and they also conducted a range of ablation and error analysis.,Does the review address Ablation?,TRUE,FALSE,The review acknowledges positive results and the inclusion of ablation and error analysis.
"in Table 2, it's necessary to explain why the LSTM's perplexity from previous work is higher than the author's implementation.",Does the review address Result?,TRUE,FALSE,"This asks for clarification of results, particularly concerning the perplexity of the LSTM."
"* In the results section there is a typo: *""performances with a large margins of 2.32pp in""*.",Does the review address Result?,TRUE,FALSE,The review points out a typographical error in the results section.
"- Strengths: This paper has high originality, proposing a fundamentally different way of predicting words from a vocabulary that is more efficient than a softmax layer and has comparable performance on NMT.",Does the review address Novelty?,TRUE,FALSE,The review praises the paper’s novelty and the proposed method’s efficiency.
"Based on the analysis, recommendations on design of multilingual NMT architectures are proposed and their efficacy validated experimentally.",Does the review address Analysis?,TRUE,FALSE,The review mentions analysis and validation of the proposed design for multilingual NMT architectures.
The idea of learning which parameters to share across languages in multilingual transformer models is original and potentially useful for designing and analyzing multilingual models in the context of NMT.,Does the review address Novelty?,TRUE,FALSE,The review points out the novelty and potential utility of the proposed idea for multilingual models.
"Overall, I think this paper has a clear motivation and some interesting ideas on how to incorporate semantic language information into planning algorithms.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,The review appreciates the motivation and ideas presented in the paper.
"It is good to know that it works for 2D-coordinates for the task at hand, though it seems to be more a marginal improvement on existing work rather than a standalone contribution.",Does the review address Result?,TRUE,FALSE,The review mentions that the paper’s contribution appears marginal compared to existing work.
I would appreciate further elaboration on the limitations of sparse attention in DNA sequence representation.,Does the review address Methodology?,TRUE,FALSE,The review asks for further clarification of the limitations of the sparse attention approach.
"Specifically, having the BFS analysis of the attention weights as a function of different GreaseLM layers (as done by Yasunaga et al.)",Does the review address Analysis?,TRUE,FALSE,"The review mentions the need for additional analysis, like BFS on attention weights."
"Experiments on type predictions for TypeScript have shown better performance than the previous methods, with or without user specified types.",Does the review address Experiment?,TRUE,FALSE,The review highlights promising experimental results for TypeScript type predictions.
The experimental results also look promising in compared with the exsiting models.,Does the review address Comparison?,TRUE,FALSE,The review compares the results favorably with existing models.
"Given that there is a wealth of existing work that performs the same task and the lack of novelty of this work, the authors need to include experiments that demonstrate that their technique outperforms others on this task, or otherwise show that their dataset is superior to others (e.g. since it is much larger than previous, does it allow for better generalization?)",Does the review address Methodology?,TRUE,FALSE,The review emphasizes the lack of novelty and suggests additional experiments to compare the proposed method with others.
"These are not weakness, but I think some work in this direction may help improve the paper.",Does the review address Experiment?,TRUE,FALSE,"The review suggests areas for improvement, though it does not frame them as weaknesses."
The authors have somewhat clarified in this in their updated version.,Does the review address Presentation?,TRUE,FALSE,The review mentions that the authors have clarified some points in the updated version.
"Theoretically, it shows that language models which are close to the “true” language model are guaranteed to attain strong performance on natural tasks.",Does the review address Theory?,TRUE,FALSE,The review refers to the theoretical insights provided in the paper.
"Some things that are worth looking into are the work on Scalable static analysis [Scaling], the inference of necessary preconditions [Logozzo], and bug detection that is based on ""belief"" [deviant, belief], which is closely related to your intuition about naturalness and human-written invariants.",Does the review address Analysis?,TRUE,FALSE,"The review suggests related areas for further exploration, which tie into the intuition behind the paper's approach."
The cold-start problem is actually an urgent problem to several online review analysis applications.,Does the review address Significance?,TRUE,FALSE,The review emphasizes the significance of the cold-start problem in online review analysis applications.
My another is concern is that the motivation of the experimental design is not clear.,Does the review address Experiment?,TRUE,FALSE,The review points out a lack of clarity in the motivation behind the experimental design.
"3) The experimental results reported are validated on a single dataset, and no human evaluation and error analysis.",Does the review address Result?,TRUE,FALSE,"The review highlights the lack of validation on multiple datasets, human evaluation, and error analysis."
"I would suggest the following:   * To make the comparison more fair, I would suggest to train transformer models with varying size and derive a power law based on the exact same experimental configuration used for LMU models.",Does the review address Experiment?,TRUE,FALSE,The review suggests a more rigorous experimental comparison.
"The term ""intermediate neurons"" (section 3.2) was unclear to me.",Does the review address Presentation?,TRUE,FALSE,The review asks for clarification of the term “intermediate neurons.”
"Discussion of D  Since RF is not the major contribution, you summarize existing results of FA in sec2.2.",Does the review address Result?,TRUE,FALSE,"The review comments on the results section, suggesting a better summary of related works."
The hypothesis are clearly stated and the experiments are well designed.,Does the review address Methodology?,TRUE,FALSE,The review appreciates the clarity of hypotheses and the design of experiments.
**Student networks** are too weak to prove the proposed techniques are useful for the more recent (and more powerful) models.,Does the review address Presentation?,TRUE,FALSE,"The review criticizes the use of student networks, suggesting they are not suitable for evaluating newer, more powerful models"
"For example, besides the quantative improvements, does the rapid convergence and under-training still exist after applying RandomMask?",Does the review address Result?,TRUE,FALSE,The review mentions whether the rapid convergence and under-training persist after applying RandomMask.
"**Related work** The authors clearly describe the related prior works from both the perspectives of program synthesis, large language models, and benchmarks for program synthesis.",Does the review address Related Work?,TRUE,FALSE,"The review highlights the authors' clear description of related prior works on program synthesis, large language models, and benchmarks."
"New Outlooks for Low-Bit Quantization on Large Language Models, Zhang et al. [3] FP8 Quantization: The Power of the Exponent, Kuzmin et al.",Does the review address Related Work?,TRUE,FALSE,The review mentions relevant related work on low-bit quantization for large language models.
It is not clear how certain experimental designs were made.,Does the review address Presentation?,TRUE,FALSE,The review points out that the experimental design was not clearly explained.
- The algorithm presented here is able to be used in an unsupervised way and can work with both open-ended and more structured knowledge graph schema.,Does the review address Methodology?,TRUE,FALSE,"The review discusses the algorithm’s ability to work in unsupervised settings and with knowledge graphs, which is part of the methodology."
- The analysis which builds on these definitions/models/assumptions provides meaningful theoretical insight into why language model pre-training may be so beneficial for downstream training.,Does the review address Theory?,TRUE,FALSE,The review mentions the theoretical insights related to language model pre-training and its benefits.
"Having an ablation on the number of GreaseLM layers would also be quite useful to answer if performance improves with more GreaseLM layers, are there diminishing returns or do we need just a few GreaseLM layers, beyond which it is detrimental to the model's performance.",Does the review address Result?,TRUE,FALSE,The review suggests that an ablation study on the number of GreaseLM layers would provide useful information about performance and diminishing returns.
S: - The idea of controlling the generation of language models step-by-step in a recurred manner is interesting.,Does the review address Methodology?,TRUE,FALSE,"The review mentions that the idea of controlling the generation of language models step-by-step is interesting, which pertains to the methodology."
I felt this was quite separate from the theoretical analysis.,Does the review address Theory?,TRUE,FALSE,The review indicates that the step-by-step generation concept seems separate from the theoretical analysis.
The paper proposes Options framework based method for using the hierarchical structure in dialog to learn the dialog policy and NLG in a hierarchical fashion.,Does the review address Methodology?,TRUE,FALSE,"The review mentions the proposal of an Options framework-based method for dialog policy and natural language generation, which is part of the methodology."
"But from Table 5., the trend of performance seems to increase with the increased prepositional phrases (with 84.7 being the max for 4 PPs).",Does the review address Result?,TRUE,FALSE,"The review points out the observed trend in performance with increasing prepositional phrases, which is related to results."
I would recommend include some references in semantic parsing.,Does the review address Related Work?,TRUE,FALSE,"The review suggests adding references related to semantic parsing, which pertains to related work."
Time analysis on language modeling is not presented.,Does the review address Analysis?,TRUE,FALSE,"The review mentions the absence of time analysis in language modeling, which is a form of analysis."
"This is the key weakness, but this makes me find it really difficult to judge the overall technical quality and significance.",Does the review address Significance?,TRUE,FALSE,The review mentions a key weakness that makes it difficult to assess the overall technical quality and significance.
Also the pointer mechanism used for predicting user specified types is a good strategy that advances the previous method.,Does the review address Result?,TRUE,FALSE,"The review acknowledges the effectiveness of the pointer mechanism for predicting user-specified types, which is part of the result."
The description of how you compare your invariants to those inferred by Daikon is not clear unless all relevant cases related to (pre)conditions on method parameters.,Does the review address Comparison?,TRUE,FALSE,"The review notes that the comparison between the invariants inferred by Daikon and those proposed in the paper is unclear, which is a comparison issue."
"- Secondly, this main result depends on the worst-case coefficient, which is also unclear to me.",Does the review address Result?,TRUE,FALSE,The review raises concerns about the unclear worst-case coefficient in the main result.
"Based on the large-scale training data and the proposed visual expert module, the proposed method achieves a number of state-of-the-art results across a wide range of vision-language tasks.",Does the review address Result?,TRUE,FALSE,"The review mentions that the proposed method achieves state-of-the-art results in vision-language tasks, which is part of the results."
"While I appreciate the environment introduced and believe it to be promising, the experiments presented fail to highlight the novelty of the environment.",Does the review address Novelty?,TRUE,FALSE,The review acknowledges the promising environment introduced but states that the experiments fail to emphasize its novelty.
The work successfully leverages the framing of general learners in terms of circuits to conclude that transformer LMs are not universal learners.,Does the review address Methodology?,TRUE,FALSE,"The review mentions that the work successfully uses the circuit-based framing of general learners, which is part of the methodology."
The submission has evaluated the proposed algorithms on four datasets and improved SOTA performances.,Does the review address Result?,TRUE,FALSE,"The review mentions that the proposed algorithms were evaluated on four datasets and achieved state-of-the-art performance, which is related to results."
"Typos: In Sec 5.2 Tasks (f), ""$\phi_2(x)=1$"" should be ""$\bar \mu_2=1$"".",Does the review address Presentation?,TRUE,FALSE,The review points out a typo in the paper's presentation.
"On the one hand the baselines make use of a large set of training trajectories, but on the other hand these methods train policies that choose low-level actions.",Does the review address Comparison?,TRUE,FALSE,The review mentions a comparison between baselines using a large set of training trajectories and methods choosing low-level actions.
"The claimed contributions include: 1) The proposed method is free from the common issue of diverging from human language, because it learns from the sentences sampled from the pre-trained LM.",Does the review address Methodology?,TRUE,FALSE,"The review mentions the methodology, stating the proposed method learns from pre-trained LM sentences to avoid divergence from human language."
The proposed model was evaluated on two publicly-available dataset and achieved reasonable results.,Does the review address Data/Task?,TRUE,FALSE,The review mentions the evaluation of the model on publicly available datasets and the reasonable results achieved.
"Last sentence of intro: ""without scarifying accuracy"" seems like an inaccurate description of the results presented in this paper.",Does the review address Result?,TRUE,FALSE,"The review questions the accuracy of the statement in the introduction, pointing out a potential inaccuracy in describing the results."
Here there is no direct comparison of the performance of the current system w.r.t.,Does the review address Comparison?,TRUE,FALSE,The review notes the absence of a direct comparison of the current system's performance.
"This makes it difficult to conclusively prove that this is an ""applicable to all"" data augmentation scheme.",Does the review address Methodology?,TRUE,FALSE,The review mentions the difficulty of proving the universal applicability of the data augmentation scheme.
I suppose it would be hard to define auxiliary tasks for simpler systems.,Does the review address Data/Task?,TRUE,FALSE,"The review acknowledges that defining auxiliary tasks may be difficult for simpler systems, which is related to the data/task."
"The architecture does not look entirely novel, but I kind of like the simple and practical approach compared to prior work.",Does the review address Related Work?,TRUE,FALSE,"The review comments on the simplicity and practicality of the approach, comparing it to prior work."
"This makes it difficult to conclusively prove that this is an ""applicable to all"" data augmentation scheme.",Does the review address Data/Task?,TRUE,FALSE,This point is repeated and still relates to the difficulty of proving the general applicability of the data augmentation scheme.
"- Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train.",Does the review address Ablation?,TRUE,FALSE,"The review mentions that the paper’s contributions are based on showing the benefits of using BERT representations for better performance and faster training, but without an ablation study."
"But using neural models to rank (or rerank) is a long-existing technique, regardless of the chosen parametrization of the reranking model.",Does the review address Methodology?,TRUE,FALSE,"The review points out that the use of neural models for ranking (or reranking) is a long-existing technique, which pertains to methodology."
"Sometimes, sparse attention can improve generalization [1].",Does the review address Methodology?,TRUE,FALSE,"The review mentions sparse attention and its potential to improve generalization, which is part of the methodology."
Please let me know if I have misunderstood something(s),Does the review address Presentation?,TRUE,FALSE,"The review asks for clarification on certain points, related to presentation."
- The proof technique (pre-training performance $\to$ covariance of pre-training errors $\to$ covariance of downstream errors $\to$ downstream performance) is itself interesting.,Does the review address Result?,TRUE,FALSE,"The review finds the proof technique interesting, which pertains to the results section."
"However, I am unable to grasp nuances, leaving important questions untouched such as: In what scenarios do we expect the model to perform better than GECA?",Does the review address Comparison?,TRUE,FALSE,"The review mentions the difficulty of understanding the nuances, especially about when the model would outperform GECA, which pertains to comparison."
"- The definitions, models, and assumptions in the paper are intuitive and clear (e.g., natural task).",Does the review address Presentation?,TRUE,FALSE,"The review finds the definitions, models, and assumptions clear and intuitive, related to the presentation section."
The authors should conduct experiments on more types of sentence pair tasks.,Does the review address Experiment?,TRUE,FALSE,"The review suggests that more experiments should be conducted on sentence pair tasks, which pertains to experiment."
One issue is that the main contribution is  mostly condensed into section 3.2 which is less than one page.,Does the review address Contribution?,TRUE,FALSE,"The review notes that the main contribution is condensed into a short section, which pertains to the contribution."
"For the first ablation, it just gives out the performance of intermediate models on a single task.",Does the review address Ablation?,TRUE,FALSE,"The review critiques the first ablation study, stating it only gives performance on a single task."
Strengths: The paper is well-written with clear motivations and structure.,Does the review address Presentation?,TRUE,FALSE,"The review highlights the strengths of the paper, particularly its clarity and structure, which pertains to presentation."
"The authors experiment both with pre-training and fine-tuning of contextual models (BERT-{base,large}) and claim large reduction in training time, with reasonable loss in performance.",Does the review address Experiment?,TRUE,FALSE,"The review mentions the authors’ experiments with pre-training and fine-tuning of contextual models and the resulting performance trade-offs, related to the experiment section."
Incorporating bert into neural machine translation.,Does the review address Related Work?,FALSE,TRUE,This sentence does not mention related work.
The idea itself is not completely new as the authors readily explain in the paragraph MACHINE LEARNING WITH PROOF ARTIFACTS on page 2.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review provides a discussion and explanation of the idea being not new, referencing a specific paragraph for clarification."
Existing performance improvement is quite limited.,Does the review address Result?,TRUE,FALSE,"The review mentions the result, stating that the performance improvement achieved by the proposed method is limited."
* I found the discussion in Section 4.1 pretty confusing.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review expresses confusion regarding the discussion in Section 4.1, which pertains to interpretation of the paper's arguments."
This seems to contradict findings from Brown et al where larger models did better on essentially all tasks.,Does the review address Methodology?,TRUE,FALSE,"The review discusses a contradiction with previous findings from Brown et al., which is related to methodology."
"Minor Issues  ==== Figure 2 is a little redundant, I think figure 1 is enough to compare it against the pRNN (figure3 and 4).",Does the review address Presentation?,TRUE,FALSE,"The review mentions redundancy in figures, specifically comparing figures and making a point about presentation."
This could be tested by ablating elements from the input or ablating the recurrent memory vectors (setting them to zero during inference).,Does the review address Experiment?,TRUE,FALSE,"The review suggests an experiment (ablating elements), which pertains to experimentation methodology."
"Strengths:  - The paper is generally well-written, with excellent motivation and empirical setup/analysis  - The overall strategy of differentiable prompt optimized to maintain fluency is reasonable and novel.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The review highlights the paper's strengths, emphasizing motivation, empirical setup, and methodology."
- Experiments are well-designed: many baselines are implemented to compare proposed method with traditional lifelong learning methods.,Does the review address Experiment?,TRUE,FALSE,"The review mentions the experimental design, noting a comparison with traditional methods."
"Detailed comments:  1) Reducing the variance of output prediction can reduce the gap on variational posterior, but how does the gap relate to the generalization error?",Does the review address Methodology?,TRUE,FALSE,"The review asks for clarification on how reducing the variance relates to generalization error, which is a methodology question."
This paper presents a replication study of BERT pretraining and carefully measures the impact of many key hyperparameters and training data size.,Does the review address Methodology?,TRUE,FALSE,The review describes the methodology of replicating BERT pretraining and measuring its impact.
- The data preprocessing and training steps are complex.,Does the review address Methodology?,TRUE,FALSE,The review mentions that the methodology involves complex preprocessing and training steps.
The theory can be more practically useful if it can be extended to quantify the intractability level so the resulting embeddings' error can be bounded or compared.,Does the review address Theory?,TRUE,FALSE,"The review discusses the potential for theoretical improvement, relating to theory."
"As the main contribution of this paper is the increased efficiency of the proposed approach, it must be clear how efficiency is measured.",Does the review address Contribution?,TRUE,FALSE,"The review comments on the need for clearer measurement of efficiency, which is related to the paper’s contribution."
The paper is mostly clearly written and discusses server interesting ablation experiments.,Does the review address Ablation?,TRUE,FALSE,The review praises the clarity of the paper and discusses the ablation experiments.
- Standard Errors: The VIMABench experiments were run over three random seeds for each meta-task but results are reported without any standard errors?,Does the review address Experiment?,TRUE,FALSE,"The review points out the lack of standard errors in the experimental results, which should have been included."
"Furthermore, they have clarified that on the key metric of CommonGen they achieved SoTA with only slightly more than half the parameters of the current SoTA model.",Does the review address Presentation?,TRUE,FALSE,"The review discusses the reported results of achieving state-of-the-art (SoTA) performance with fewer parameters, related to presentation."
"However, two recent papers that appeared on arXiv before the ICLR submission deadlines also use Transformers for protein contact prediction.",Does the review address Methodology?,FALSE,TRUE,This sentence does not mention methodology.
"- It's because if the largest phrase length is the sentence length, then model can be simplified into a some sort of convolution RNN where the each state of the RNN goes through some convolution layer before a final softmax and attention.",Does the review address Presentation?,FALSE,TRUE,This sentence does not mention presentation.
"In addition, the explanations (query-answer chains) are shorter and more human-interpretable.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions shorter, more human-interpretable explanations, which is related to definition or explanation."
"Compared to a softmax layer and hierarchical/differentiated softmax, is binary code prediction a natural way to predict words?",Does the review address Comparison?,TRUE,FALSE,"The review compares binary code prediction to other methods such as softmax, which is part of the comparison section."
"Intuitively, a smaller gap might lead to better performance.",Does the review address Result?,TRUE,FALSE,"The review suggests an intuition about performance, which relates to result."
"However, the way they convert the logic forms is different for each dataset and they have to manually design rules for each logic form.",Does the review address Methodology?,TRUE,FALSE,The review discusses a difference in methodology regarding logic form conversion.
More importantly the PACT methodology is more general; the idea of incorporating diverse auxiliary tasks as a language modeling task by introducing a distinct token for each task is novel.,Does the review address Methodology?,TRUE,FALSE,"The review discusses the novelty of the PACT methodology and its generalizability, which pertains to methodology."
"In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266.",Does the review address Related Work?,TRUE,FALSE,The review references related work presented in a past conference.
**Summary** This work relates a pre-training performance with a downstream performance for tasks that _can_ be reformulated as next word prediction tasks.,Does the review address Result?,TRUE,FALSE,"The review summarizes the relationship between pre-training and downstream performance, relating to result."
"(iii) Some suggested baselines that make these assumptions would be a heuristic A\* search, or modifying any of the existing algorithms to use smaller action spaces and/or apply alternative exploration strategies seen in previous works such as modular policy chaining (that MC!Q*BERT uses) or Go-Explore (Madotto et al.",Does the review address Comparison?,TRUE,FALSE,"The review mentions comparison with baseline methods, which suggests a methodology question."
"3) The experimental results reported are validated on a single dataset, and no human evaluation and error analysis.",Does the review address Experiment?,TRUE,FALSE,"The review points out the limited validation of experimental results, relating to experiment."
"I’m not sure about this, so any discussion would be appreciated.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review asks for clarification on certain aspects, which relates to explanation."
"For example, if you could give us one or two sentences of Fon in the beginning of the paper, that demonstrate some of the difficulties of the language, I think this would greatly strengthen the motivation.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The review suggests strengthening the motivation by providing examples, which is related to intuition/motivation."
Questions:  * How can we be sure that the specific power law derived from a different experimental configuration in Kaplan et al. (2020) corresponds to the empirical law that can be derived for transformers in this particular evaluation setting?,Does the review address Evaluation?,TRUE,FALSE,"The review asks a question related to evaluation, comparing results from different experiments."
This looks like an order of magnitude difference in dataset empirical evaluation to me.,Does the review address Data/Task?,TRUE,FALSE,"The review compares the dataset evaluation, referring to data/task."
"- The definitions, models, and assumptions in the paper are intuitive and clear (e.g., natural task).",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review praises the clarity of definitions, models, and assumptions, which is related to definition/description."
Could you explain the significance of this result again?,Does the review address Presentation?,TRUE,FALSE,"The review requests further explanation of the significance of a result, which pertains to presentation."
In case of the textual prompts benchmarks its also not clear to me why no standard errors on results are reported?,Does the review address Data/Task?,TRUE,FALSE,"The review questions why standard errors are missing, which pertains to data/task."
"Hence, to my understanding, it may not be suitable to use the term ""global reasoning"" in this work.",Does the review address Presentation?,TRUE,FALSE,"The review questions the terminology used, which pertains to presentation."
The show improved performance in MultiWoz dataset.,Does the review address Result?,TRUE,FALSE,"The review mentions improved performance, which pertains to results."
More details should be reported to show the benefits of adopting the adaptive smoothing parameter.,Does the review address Methodology?,TRUE,FALSE,"The review suggests reporting more details, which is related to methodology."
"Specifically, having the BFS analysis of the attention weights as a function of different GreaseLM layers (as done by Yasunaga et al.)",Does the review address Methodology?,TRUE,FALSE,The review suggests a more detailed methodology involving BFS analysis.
"This paper suggests a number of cheap-to-compute corruptions of the input data that, when used to reconstruct the input, enrich the underlying model.",Does the review address Methodology?,TRUE,FALSE,The review describes a methodology suggestion regarding input data corruption.
"For the theoretical analysis, it was not clear to me what is the contribution of the current analysis compared to the Levine 2020 paper.",Does the review address Theory?,TRUE,FALSE,The review requests clarification on the theoretical contribution compared to existing work.
"In particular, they used the option framework to represent the connection between the dialog policy and the natural language generation.",Does the review address Methodology?,TRUE,FALSE,"The review describes the methodology used by the authors, explaining the connection between dialog policy and language generation."
Performance better than previous approaches (although minor).,Does the review address Result?,TRUE,FALSE,"The review mentions performance improvements over previous methods, related to result."
"It provides a nice theoretical framework for thinking about the connection between language models and downstream tasks, which future work could build on.",Does the review address Data/Task?,TRUE,FALSE,"The sentence discusses a theoretical framework related to downstream tasks, which is a key aspect of the Data/Task section."
Some experimental settings only include the baselines Random and Vote-k. More methods as mentioned in 4.3.2 can be also included.,Does the review address Methodology?,TRUE,FALSE,"The sentence mentions the experimental methodology and the inclusion of additional methods, relevant to Methodology."
* The paper doesn’t explain why learning a linear model directly on the context embeddings f(s) performs better than using the contextual mean embeddings.,Does the review address Result?,TRUE,FALSE,"The sentence highlights an unexplained result, which relates to the Result section, focusing on why one method performs better than another."
"The first use of the proposed definitions to make a non-trivial claim is in Section 5 with Postulate 1, but this claim is not justified.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The sentence questions the justification of a claim, which ties into the Intuition/Justification/Motivation/Validation section."
"Thus, it seems to me that you are essentially applying past results to answer a specific question you have (which is still a valuable contribution).",Does the review address Contribution?,TRUE,FALSE,"The sentence is commenting on the contribution made by the paper, fitting into the Contribution section."
"*  What is the contribution of each specific design choice, such as the FFN/global attention and implicit self-attention, to the end performance?",Does the review address Analysis?,TRUE,FALSE,"This question addresses the specific design choices and their contribution to performance, which aligns with Analysis."
- No error analysis about the generated plans and the edited text.,Does the review address Analysis?,TRUE,FALSE,"The sentence identifies the absence of error analysis, which falls under Analysis."
"The presentation can be improved, all the definitions are hard to follow.",Does the review address Presentation?,TRUE,FALSE,"The sentence critiques the clarity of the presentation, relevant to Presentation."
Summary of review:  There have been lots of interests to understand why self-supervised learning approaches such as the next word prediction task learn a useful representation for downstream tasks.,Does the review address Data/Task?,TRUE,FALSE,"This summary touches on Data/Task, as it addresses why self-supervised learning is useful for downstream tasks."
- Experimental results: I suggest the author to provide more ablation analysis to the experiment section.,Does the review address Analysis?,TRUE,FALSE,"The suggestion pertains to improving the Analysis section, particularly regarding ablation experiments."
"Yet, it is difficult for me to trust that the effects in this paper will generalize to better performing models without further evidence: what if the CALM intermediate objectives only help with mistakes that larger models do not make in the first place?",Does the review address Result?,TRUE,FALSE,This is questioning the Result and its generalizability to better-performing models.
So a very straightforward idea is that we can directly set an independently learnable parameter as the prototype of each category to calculate the cosine similarity with image embeddings.,Does the review address Methodology?,TRUE,FALSE,"The idea describes an experimental methodology to calculate similarities, which fits within Methodology."
(2) Another weakness is that the comparison with the vanilla and LS baselines does not seem to be properly controlled in terms of parameters.,Does the review address Methodology?,TRUE,FALSE,"The sentence addresses an issue with experimental control, which pertains to Methodology."
As noted in contemporary works such assumptions dramatically reduce the difficulty and language understanding capabilities of text games (Yao et al. (ii) The second issue is that MC-LAVE assumes that the simulator is deterministic and can conduct rollouts and reset within the span of an episode - standard planning assumptions but incompatible with all other baselines (except for MC!Q\*BERT) which do not use this handicap.,Does the review address Presentation?,TRUE,FALSE,"The sentence touches on assumptions affecting text games, related to Presentation and Methodology."
The separate table on the left in Table 2 appears to be redundant.,Does the review address Presentation?,TRUE,FALSE,"This comment addresses an issue with presentation and redundancy in a table, fitting into Presentation."
"These prior methods do not incorporate the SFT stage, making it unclear how the model performs before this crucial phase.",Does the review address Comparison?,TRUE,FALSE,This refers to the lack of comparison with prior methods and relates to Comparison.
"- Similarly, using bold and not-bold B in Theorem 5.2 is confusing notation.",Does the review address Presentation?,TRUE,FALSE,"This addresses a presentation issue regarding confusing notation, relevant to Presentation."
"In experimental details, the slightly better performance in Table 2, can it be attributed to finer generation powered by the RNN generator?",Does the review address Result?,TRUE,FALSE,The question pertains to the Result and explores the potential reason behind the performance differences in the experimental results.
"Overall, I like the idea of the paper: it is important to reduce the parameters needed for sets of tasks, to enable NLP models to be deployed in a larger variety of settings, and reducing the amount of data needed.",Does the review address Data/Task?,TRUE,FALSE,"This positive comment relates to Data/Task, emphasizing the importance of reducing parameters and data for broader NLP deployment."
"It might be good to find the best hyperparameters for each setting to truly do a fair comparison (avoiding hyperparameter tuning isn't really a fair comparison, IMO -- but this depends on how much compute you have available).",Does the review address Methodology?,TRUE,FALSE,The suggestion about fair comparison and hyperparameter tuning is tied to Methodology.
- The analysis which builds on these definitions/models/assumptions provides meaningful theoretical insight into why language model pre-training may be so beneficial for downstream training.,Does the review address Methodology?,TRUE,FALSE,"The comment highlights theoretical insights, fitting within Methodology."
"With the proposed CaptionNet dataset, it is now possible to have a fairer comparison between VL models and CE models.",Does the review address Methodology?,TRUE,FALSE,"The mention of a new dataset ties into Methodology, suggesting better comparative analysis between models."
"The theoretical results on the Parity/Sum task reply to some strong assumptions: bilinear parameterization, some initialization (for example, v = 0).",Does the review address Data/Task?,TRUE,FALSE,"This comment on assumptions for theoretical results relates to Data/Task, as it addresses the assumptions in the task's design."
"Firstly, in the last paragraph of Section 2, the authors claim that the role matrix $R$ would be invertible such that there exists a matrix $U = R^{-1}$ such that the fillers would be recovered.",Does the review address Theory?,TRUE,FALSE,"This comment addresses a theoretical aspect of the paper, which fits into Theory."
* The paper is well motivated and easy to understand and follow.,Does the review address Presentation?,TRUE,FALSE,"This comment praises the clarity and presentation, which relates to Presentation."
The authors have created a large dataset for relation extraction as question answering which would likely be useful to the community.,Does the review address Data/Task?,TRUE,FALSE,"The creation of a dataset fits within Data/Task, as it contributes to the field by providing useful data."
"Section 4.8: Using transformers for generating proteins with natural properties is not new (see Madani et al, 2020, ‘ProGen’ or Rives et al, 2020).",Does the review address Methodology?,TRUE,FALSE,This comment critiques the novelty of the methodology used in Methodology.
"Similarly, for pretraining, the model runs 80% of the training steps (20% reduction), which accounts much of the training time reduction reported on section 4.3.",Does the review address Methodology?,TRUE,FALSE,"The mention of pretraining steps ties into Methodology, discussing training procedures."
"**Pros**  - The paper is well structured and easy to follow, the idea of modeling sentences to a Brownian bridge latent space is neat and generic enough to (1) allow for noise given its stochasticity (2) doesn't require explicit domain knowledge for planning.",Does the review address Methodology?,TRUE,FALSE,"The comment praises the methodology, indicating clarity and creativity, relevant to Methodology."
"However, the main driving force in the choice of the language-specific computation is currently a single hyper-parameter p which is the same across languages; so, this will lead to choices that are good on average for all language pairs involved for a given *universal* budget.",Does the review address Methodology?,TRUE,FALSE,"This critique is about methodology and the use of hyperparameters in language-specific computation, fitting into Methodology."
"Unfortunately, many aspects of the models, experimentation, and evaluation are not explained very well.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This highlights issues in Definition/Description/Detail/Discussion/Explanation/Interpretation, addressing lack of clarity."
The method's innovative and effective feature fusion into the language model sets it apart.,Does the review address Presentation?,TRUE,FALSE,"The comment praises the innovative methodology used, fitting into Presentation."
This paper is meaningful and presents a reasonable analysis.,Does the review address Analysis?,TRUE,FALSE,"This comment refers to the Analysis section, praising the quality of the analysis provided."
"Detailed comments:  - P2, Sec 1.1: ""analyze the efficiency language model features"" -> analyze the efficiency of language model features  - P2, Sec 2: you started introducing these notations without explaining what they mean.",Does the review address Presentation?,TRUE,FALSE,"The comments offer suggestions on improving Presentation, focusing on clarity and definition."
It is not clear how certain experimental designs were made.,Does the review address Experiment?,TRUE,FALSE,"The comment refers to Experiment, questioning the clarity of experimental design."
"- I know GPT3 access is hard to get, but I wish experiments with k=8 prompts could be compared against  Post rebuttal: I think another pass for clarity over the paper would be good for the final version, but otherwise I'm happy with the paper updates and I'm happy to see the ablation numbers aren't too sensitive to token count.",Does the review address Methodology?,TRUE,FALSE,"This review mentions Methodology, suggesting improvements in the clarity of experimental designs and comparisons."
"It is also strange that the multi-cluster approach, which discards inter-cluster (word and language) semantic information performs the best with respect to the extrinsic metrics.",Does the review address Result?,TRUE,FALSE,"The sentence discusses the outcome of an experiment, addressing unexpected results, which falls under Result."
"- One huge benefit of perceiver IO is to train different tasks together and explore the transfer between different tasks/modalities, which is not explored in this paper.",Does the review address Methodology?,TRUE,FALSE,"The comment refers to Methodology, suggesting that the authors did not explore the transfer between different tasks/modalities."
Their models achieve better quantitative results when compared to the provided baselines.,Does the review address Result?,TRUE,FALSE,"The statement provides results from the experiments, which is part of the Result section."
The authors find that the main ingredients for the success of in-context learning are a combination of selective annotation with similarity-based prompt retrieval.,Does the review address Data/Task?,TRUE,FALSE,"This refers to the data and task configuration in the study, which fits into Data/Task."
"Some kind of analysis of the qualitative strengths and weaknesses of the binary code prediction would be welcome -- what kind of mistakes does the system make, and how does this compare to standard softmax and/or hierarchical and differentiated softmax?",Does the review address Analysis?,TRUE,FALSE,"This suggestion is asking for further Analysis, particularly focused on the model's mistakes and comparisons with other methods."
"While previous work has examined tasks in the overall area, to my knowledge there has not been any publicly availble sentence-level annotated data for the problem -- the authors here make a contribution as well by annotating some data included with the submission; if it is released, it could be useful for future researchers in this area.",Does the review address Data/Task?,TRUE,FALSE,"This comment is about the contribution of annotated data, which is part of Data/Task."
"(2) In this paper, similarity and alignment structures are proposed, but no ablation experiments have been carried out to prove the effectiveness of the improved model.",Does the review address Experiment?,TRUE,FALSE,"This critique highlights the absence of ablation studies, relevant to Experiment."
* Section 4 - I think you really need to re-state that the algorithm has a human-in-the-loop for clarity.,Does the review address Methodology?,TRUE,FALSE,"The comment addresses a Methodology clarification, specifically regarding the human-in-the-loop concept."
The provided experiments are more like baselines for self-evaluation other than state-of-the-art performance.,Does the review address Result?,TRUE,FALSE,"This comment critiques the Result, stating that the experiments mainly focus on self-evaluation rather than achieving state-of-the-art results."
"Authors could have provided more in-depth details (visualizations, analysis, examples) to show main differences between the proposed approach and baselines (specially LayoutLM).",Does the review address Presentation?,TRUE,FALSE,"This comment calls for improvement in the Presentation of the paper, specifically asking for more visual aids and examples for clearer comparisons."
The analysis shows that it outperforms only all the other baselines only on 5/9 games and matches on 3.,Does the review address Comparison?,TRUE,FALSE,"This comment is an evaluation of the Comparison between the model and its baselines, specifically pointing out the limited number of tasks where it outperforms them."
The paper presents an end-to-end methods for jointly training named entity recognition (NER) and relation extraction (RE).,Does the review address Methodology?,TRUE,FALSE,"This statement describes the Methodology used in the paper, specifically regarding joint training for NER and RE."
"Comparing the proposed method to earlier approaches such as PaLI, CoCa, and Flamingo may not be entirely fair.",Does the review address Comparison?,TRUE,FALSE,"This is a critique related to the Comparison, questioning the fairness of comparing the proposed method to earlier approaches."
A single value of $k=150$ was chosen for experiments across all tasks.,Does the review address Data/Task?,TRUE,FALSE,"The comment refers to a Data/Task design choice, discussing the uniform selection of a hyperparameter ($k=150$) for all tasks."
"However, all the parameters/variables in the neural networks are freely designated and are not correlated to each other, thus they cannot work together to meet the requirements in the binding-unbinding mechanism.",Does the review address Theory?,TRUE,FALSE,"This addresses a Theory issue, questioning the lack of correlation among parameters in the neural network design."
"In terms of strenghs - The paper has a very throughout analysis of different models and positional encodings - It proposes several contributions, including a new probing task and several positional encoding methods.",Does the review address Analysis?,TRUE,FALSE,"This comment praises the Analysis section of the paper, highlighting its thorough examination of models and positional encodings."
"* I think it should be discussed earlier (in intro/related work) why the paper focuses on language models which do next word prediction via linear softmax models over fixed dimensional context embeddings, and that BERT is out of scope.",Does the review address Methodology?,TRUE,FALSE,"This suggests a Methodology improvement, recommending that the paper clarify its focus on specific types of language models earlier in the text."
"By converting the logic forms to natural languages, the authors can leverage paraphrase datasets and pre-train the critic as a paraphrase model.",Does the review address Presentation?,TRUE,FALSE,"This suggestion involves Presentation, discussing the use of paraphrase datasets and a pre-trained critic."
"- The paper is well written: it gives an appropriate context, presents the main theoretical results, and verifies _some_ of the claims experimentally.",Does the review address Experiment?,TRUE,FALSE,"This is a positive comment on the Experiment, noting that some of the theoretical claims have been experimentally verified."
"335: consider defining GPGPU Table 3: Highlight the best BLEU scores in bold Equation 15: remind the reader that q is defined in equation 6 and b is a function of w. I was confused by this at first because w and h appear on the LHS but don't appear on the right, and I didn't know what b and q were.",Does the review address Presentation?,TRUE,FALSE,"This is a Presentation suggestion, aiming to clarify definitions and highlight key results in the paper."
Should also discuss related work in 2d spatial visualization of country-country relationships by Peter Hoff and Michael Ward.,Does the review address Presentation?,TRUE,FALSE,"This is a suggestion for Presentation, recommending that related work be discussed in more detail."
"It would be to show more experiment results for some settings, for example, performance on Sum task when training with a mixture distribution, or more Sum task samples and fewer Parity task samples ( p range from 0.5 to 1.",Does the review address Data/Task?,TRUE,FALSE,"This is a Data/Task suggestion, asking for more experiment results and varied task configurations."
"They use the reading comprehension model of Seo et al. 2016, adding the ability to return “no relation,” as the original model must always return an answer.",Does the review address Methodology?,TRUE,FALSE,"This describes the Methodology used in the paper, noting that the authors adapted a model to return ""no relation"" when needed."
It paves the way for more widespread application of VIP in scenarios where interpretable-by-design approaches are critical.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This fits under Definition/Description/Detail/Discussion/Explanation/Interpretation, discussing the broader implications and applications of the work."
* Related work: I would also include a discussion of this Lazaridou et al paper where they compare ways of combining EC with non-EC learning signals (e.g. image caption training): https://aclanthology.org/2020.acl-main.685.pdf  * Ethics statement: I appreciate this statement and agree with the possible positive impacts.,Does the review address Methodology?,TRUE,FALSE,"This is a Methodology suggestion, recommending that related work be discussed for a more comprehensive review."
"According to Table 6, I can hardly see a clear improvement brought by the introduced new VE modules.",Does the review address Result?,TRUE,FALSE,"This is a Result critique, highlighting the unclear improvement with the new modules in the experiments."
A uniform framework for resampling Different recombinations perform more or less favorably across different datasets.,Does the review address Methodology?,TRUE,FALSE,"This relates to Methodology, discussing a uniform framework for resampling and its varying effectiveness."
Why don't the authors of this work do this evaluation as well?,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This is a critique on Intuition/Justification/Motivation/Validation, questioning why a specific evaluation wasn't included in the paper."
It is unclear why the authors only show the response generation results.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,This is a Definition/Description/Detail/Discussion/Explanation/Interpretation query about the focus of the results in the paper.
The proposed model was evaluated on two publicly-available dataset and achieved reasonable results.,Does the review address Evaluation?,TRUE,FALSE,"This is an Evaluation statement, noting the evaluation of the model on publicly available datasets and"
Their thorough ablation experiments and other analysis yield some interesting findings the authors could emphasize more.,Does the review address Result?,TRUE,FALSE,"This refers to the Result, where the reviewer suggests that the authors could emphasize their findings from ablation experiments more."
The authors curated a large-scale dataset for first-stage pretraining and second-stage instruction tuning.,Does the review address Data/Task?,TRUE,FALSE,"This refers to Data/Task, where the authors' contribution of curating a large-scale dataset for pretraining and tuning is mentioned."
"(In particular, pre-training on emergent language performs on average better than synthetic hierarchical data, but not quite as well as a different natural language, and all of these pre-training methods do better than training from scratch.)",Does the review address Result?,TRUE,FALSE,This is a Result describing the performance of different pre-training methods.
"Overall, the authors show better accuracy for their tested problem set against REWARD, a baseline system (a transformer), lang2logic, and Ins2AST across two dimensions: data type recovery and abstract syntax tree (AST) generation.",Does the review address Result?,TRUE,FALSE,"This is a Result, comparing the authors' model to several baseline systems and reporting superior performance across multiple dimensions."
"It would be useful if following the suggestions of [2,3] the authors could present results in settings with low data regimes and with significant distribution shift with respect to the pre-training set, which represents a more realistic application setting.",Does the review address Data/Task?,TRUE,FALSE,"This refers to Data/Task, suggesting that the authors present results for more realistic application settings, such as low data regimes and distribution shifts."
Why does Recomb-2 perform less well than GECA in SCAN?,Does the review address Result?,TRUE,FALSE,"This is a Result question, asking for an explanation about why one model performs worse than another in a specific task (SCAN)."
"In the 6.2.3 visualization of clusters, it would be very useful to have a visualization of clusters from some baselines on other ways of learning.",Does the review address Presentation?,TRUE,FALSE,"This is a Presentation suggestion, recommending that the authors include visualizations of clusters from baseline methods for comparison."
Some theoretical and empirical evidence is shown for the learning effect.,Does the review address Result?,TRUE,FALSE,"This is a Result statement, noting that theoretical and empirical evidence is provided for the learning effect."
"The authors propose to include related texts retrieved by the kNN method in a single training sample, which is proved effective in solving sentence similarity tasks.",Does the review address Methodology?,TRUE,FALSE,"This is related to Methodology, discussing the authors' approach of using kNN for sentence similarity tasks."
- The experimental analysis focuses exclusively on known computer vision datasets that do not differ from the training distribution of CLIP.,Does the review address Experiment?,TRUE,FALSE,"This is a critique of the Experiment, highlighting that the authors focus on known datasets that are similar to the CLIP training distribution."
But the authors didn’t compare against these agent-based system design.,Does the review address Methodology?,TRUE,FALSE,"This is a critique related to Methodology, noting that the authors did not compare their approach with agent-based system designs."
"The authors could add more content to figure 1, which may resolve this issue.",Does the review address Presentation?,TRUE,FALSE,"This is a Presentation suggestion, proposing that the authors improve Figure 1 to resolve issues."
Minor issues that did not affect score ------------------ Figure 1 has some scaling/resolution issues that make it hard to read.,Does the review address Presentation?,TRUE,FALSE,"This refers to a Presentation issue, where the reviewer points out readability problems with Figure 1."
The paper provides an image on GCP to reproduce their partial results.,Does the review address Result?,TRUE,FALSE,"This is related to Result, as it mentions providing an image on GCP to help reproduce the results."
"> On the generative task CALM performs closer to SOTA, but it improves only slightly on T5.",Does the review address Data/Task?,TRUE,FALSE,"This is a Data/Task discussion, mentioning the performance of the CALM model on a generative task and its comparison to T5."
"Contribution: The authors contribute a new tokenization method, code, and a dataset.",Does the review address Contribution?,TRUE,FALSE,"This is related to Contribution, highlighting the authors' contributions in tokenization methods, code, and a dataset."
The model architecture should be better justified.,Does the review address Methodology?,TRUE,FALSE,"This is a critique of the Methodology, suggesting that the model architecture should be better explained and justified."
Also they could visually demonstrate the advantages of their approach.,Does the review address Presentation?,TRUE,FALSE,"This refers to Presentation, recommending that the authors visually demonstrate the advantages of their approach."
Such a comparison would highlight the advantages of the multi-task model and would be helpful for the relevant audience.,Does the review address Methodology?,TRUE,FALSE,"This is a suggestion for Methodology, encouraging the authors to compare their multi-task model to highlight its advantages."
There are few places (see details) that authors have assumptions in mind but do not provide those assumptions until later.,Does the review address Presentation?,TRUE,FALSE,"This refers to a Presentation issue, where the reviewer points out that assumptions should be mentioned earlier in the paper."
"(2) multiCCA : Extends the approach presented by Faruqui and Dyer (2014) for embedding bilingual words, to multilingual words by using English embeddings as the anchor space.",Does the review address Methodology?,TRUE,FALSE,"This is a Methodology statement, explaining how the authors extend a previous approach to multilingual words."
"It is not clear whether the proposed method can be applied to other hardware settings, such as other GPUs, TPUs and large scale.",Does the review address Methodology?,TRUE,FALSE,"This is a Methodology critique, questioning the applicability of the proposed method to different hardware settings."
"In experiments, there is no comparison with previous retrieval based methods.",Does the review address Significance?,TRUE,FALSE,"This is a Significance critique, pointing out that the authors didn't compare their method with previous retrieval-based methods."
* I found the discussion in Section 4.1 pretty confusing.,Does the review address Presentation?,TRUE,FALSE,"This is a Presentation critique, expressing confusion about the discussion in Section 4.1."
The visualization of OTTER’s matching illustrates its effectiveness in handling many-to-many relationships.,Does the review address Presentation?,TRUE,FALSE,"This is a Presentation statement, highlighting how the visualization of OTTER’s matching is effective."
"Negatives --------- The experiments do not compare to many other approaches, even though those approaches are cited throughout the paper.",Does the review address Experiment?,TRUE,FALSE,"This is a critique of the Experiment, suggesting that the authors did not sufficiently compare to other approaches."
(2) The new proposed model in this paper has achieved better results than the previous work in many tasks on MSMARCO.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, reporting that the new model outperforms previous work in multiple tasks on MSMARCO."
Hyper-parameter balancing strategies in Section 3.1 is not a solid theoretical analysis.,Does the review address Analysis?,TRUE,FALSE,"This is an Analysis critique, questioning the theoretical soundness of the hyperparameter balancing strategies."
The proposed model is then experimentally verified on three logic-driven datasets which demonstrates some performance gain.,Does the review address Result?,TRUE,FALSE,"This is a Result, stating that the proposed model was experimentally verified and showed performance improvements."
Some comments:  _ It may be interesting to include a brief explanation of the differences between the approach from Tian et al. 2014 and the current one.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is a Definition/Description/Detail/Discussion/Explanation/Interpretation suggestion, recommending a brief explanation of the differences between the current approach and Tian et al. (2014)."
"For the second ablation, why do all the larger splits lead to similar performance?",Does the review address Ablation?,TRUE,FALSE,"This is a Ablation question, asking why larger splits in the second ablation lead to similar performance."
Experimental evaluation shows competitive performance.,Does the review address Experiment?,TRUE,FALSE,"This is a Experiment statement, highlighting that the experimental evaluation shows competitive performance."
"(3) The proposed model in this paper uses sparse alignment matrix to aggregate token-level similarity, where each element represents the alignment of a pair of tokens, which can develop different retrieval models in a unified way and identify the shortcomings of existing models.",Does the review address Presentation?,FALSE,TRUE,"This sentence describes the model's methodology, but it does not discuss the clarity, structure, or organization of the paper."
"For example, in the visualization in Figure 2, the maximum many-to-many relationship of a sample is 3.",Does the review address Presentation?,TRUE,FALSE,"This refers to Presentation, describing a specific visualization in Figure 2 and explaining a characteristic of the many-to-many relationship."
"However, there are many numbers in different tables.",Does the review address Presentation?,TRUE,FALSE,"This is a Presentation critique, noting that the many tables in the paper can be overwhelming or difficult to interpret."
"what part of the performance is coming from pretraining (especially if using VAE type is novel, then quantifying that is important with and without VAE type SL), etc.",Does the review address Methodology?,TRUE,FALSE,"This is related to Methodology, suggesting that the authors clarify the contribution of pretraining and the VAE type to the overall performance."
"However, they compare to BERT models and build themselves on RoBERTa_base; how are the results meaningful if they use a stronger model to start with?",Does the review address Comparison?,TRUE,FALSE,"This refers to Comparison, questioning whether the results are meaningful given the use of a stronger model (RoBERTa_base) as a starting point."
So my point 5 is important to answer and I would like to see all the details are clarified in order to make the contribution stronger.,Does the review address Contribution?,TRUE,FALSE,"This refers to Contribution, emphasizing the need for clarification of details to strengthen the paper's contribution."
"**Updates after rebuttal period**  The authors addressed some of the concerns -- showing inference time, model size and a discussion about training details and hyperparameters in the appendix.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This is related to Definition/Description/Detail/Discussion/Explanation/Interpretation, as the authors addressed concerns regarding inference time, model size, and training details in the appendix."
The paper made a significant contribution to idea of using adversarial training as part of the self-supervision signal for language learning.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, acknowledging the paper's significant contribution in using adversarial training for self-supervision in language learning."
Making these comparisons would require a heavy rewrite starting from the abstract to the analysis and so I would recommend reject right now but look forward to seeing an updated version of the paper in the future with some of these changes.,Does the review address Comparison?,TRUE,FALSE,"This is a Comparison suggestion, stating that the paper would require significant revisions before making meaningful comparisons to other methods."
"However, the way they convert the logic forms is different for each dataset and they have to manually design rules for each logic form.",Does the review address Data/Task?,TRUE,FALSE,"This refers to Data/Task, noting that the logic form conversion differs per dataset and requires manual rule design."
- Major contributions of the work should be described in the main paper.,Does the review address Contribution?,TRUE,FALSE,"This is a Contribution critique, suggesting that the major contributions should be explicitly described in the main paper."
Solid experiments demonstrating the proposed method outperforms other existing approaches.,Does the review address Result?,TRUE,FALSE,"This is a Result, stating that the experiments demonstrate the proposed method outperforming other existing approaches."
Human evaluation is costly and could also have bias like the paper points out.,Does the review address Evaluation?,TRUE,FALSE,"This is an Evaluation comment, pointing out the potential costs and biases of human evaluation."
"-----Strengths----- I think the main contribution of this paper is a simple way to ""flatten"" structured information to an array of vectors (the memory), which is then connected to the tagger as additional knowledge.",Does the review address Contribution?,TRUE,FALSE,"This is a Contribution, identifying the main contribution as a method to flatten structured information for tagging tasks."
"If there are particular differences in the above, it would nice to clearly state them and also say why the different choices and verify if the different choices are beneficial compared to the previous ones.",Does the review address Evaluation?,TRUE,FALSE,"This refers to Evaluation, suggesting that the authors clarify and justify their choices in comparison to previous approaches."
"Finally, a number of ablation studies are performed and demonstrate the effectiveness of the proposed method to some extent.",Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, stating that the authors performed ablation studies showing the effectiveness of their proposed method."
How can the authors use a pair of logic forms as negative examples (in figure-2)?,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, asking about the use of negative examples in figure-2 and how they are applied in the methodology."
A key parameter that occurs in obtaining the above results is a worst-case coefficient that bounds the distributional shift between language model distributions of the training dataset and that of the downstream task.,Does the review address Data/Task?,TRUE,FALSE,"This refers to Data/Task, describing a key parameter used to measure the distributional shift between training and downstream task distributions."
"In comparison, if we look at Page 17, the actual annotations from the authors are very long and detailed.",Does the review address Comparison?,TRUE,FALSE,"This refers to Comparison, contrasting the level of detail in the authors' annotations to other sections or works."
The experiment results in Tables 1 & 2 cannot plausibly prove OTTER is more effective than CLIP.,Does the review address Experiment?,TRUE,FALSE,"This refers to Experiment, stating that the experimental results in Tables 1 & 2 fail to convincingly prove the effectiveness of OTTER over CLIP."
"), yet I could not find any actual training experiments, that is training a large LLM from scratch, in the paper.",Does the review address Experiment?,TRUE,FALSE,"This refers to Experiment, criticizing the lack of actual training experiments for large LLMs from scratch in the paper."
"Writing:  The writing is overall clear and easy to follow, although it took me quite some time to map out the definitions of various notations.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This refers to Definition/Description/Detail/Discussion/Explanation/Interpretation, noting that while the writing is clear, the definitions of notations took time to understand."
"It would have been better to see the performance gains on more difficult text-classification tasks (non-GLUE), or underperforming models (non-BERT based).",Does the review address Result?,TRUE,FALSE,"This refers to Result, suggesting that the authors should show performance gains on more challenging tasks or underperforming models beyond GLUE and BERT-based models."
(4) Translation invariance : Uses a low rank decomposition of the word PMI matrix with an objective with includes bilingual alignment frequency components.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, describing the authors' use of a low-rank decomposition of the PMI matrix for translation invariance."
"Of course, it is valuable to see the great performance achieved by single-task finetuning for RoBERTa.",Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, praising the performance achieved by single-task finetuning for RoBERTa."
A closer look at evaluating the phrases in a subset of the evaluation set would be necessary to support the claims.,Does the review address Evaluation?,TRUE,FALSE,"This refers to Evaluation, suggesting that a closer look at evaluating phrases in the subset of the evaluation set is necessary for supporting the claims."
I would recommend the authors to at least assume the availability of some public data that is kept out of training and evaluations and to run all the baselines fairly in this setting.,Does the review address Evaluation?,TRUE,FALSE,"This refers to Evaluation, recommending the use of public data and fair baseline comparisons in the evaluation setting."
"As far as I know, this is indeed the first work for handling this task using binding-unbinding mechanism.",Does the review address Novelty?,TRUE,FALSE,"This refers to Novelty, acknowledging that this paper is the first to address the task using a binding-unbinding mechanism."
"How would it perform if LLM is not GPT-4, but rather those open-source alternatives like Llama.",Does the review address Result?,TRUE,FALSE,"This refers to Result, suggesting an evaluation of the model's performance using open-source LLM alternatives like Llama."
Using meta-learning for compositional generalization is reasonable.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, stating that using meta-learning for compositional generalization is a reasonable approach."
"In Figure 1, results with p ranges from 0.1 to 0.5 are shown.",Does the review address Result?,TRUE,FALSE,"This refers to Result, noting that results with a range of p values from 0.1 to 0.5 are shown in Figure 1."
Authors also used a discriminator reward signal to cope with sparse reward (dialog success rate) and better representation of the human evaluation.,Does the review address Evaluation?,TRUE,FALSE,"This refers to Evaluation, noting that the authors used a discriminator reward signal to address sparse rewards and improve human evaluation representation."
(2) The new proposed model in this paper has achieved better results than the previous work in many tasks on MSMARCO.,Does the review address Related Work?,TRUE,FALSE,"This refers to Related Work, comparing the proposed model to previous work on MSMARCO and demonstrating better performance."
(2) The new proposed model in this paper has achieved better results than the previous work in many tasks on MSMARCO.,Does the review address Result?,TRUE,FALSE,"This refers to Result, stating that the proposed model outperforms previous work in many tasks on MSMARCO."
It also compares different subword representation strategies and finds that syllable representations perform best (when not using BERT).,Does the review address Result?,TRUE,FALSE,"This refers to Result, discussing the comparison of subword representation strategies and finding that syllable representations perform the best when BERT is not used."
"Its not that I/readers dont believe you when we are *told*, but being *shown* makes it much more interesting and give people an appreciation for Fon tokenization challenges!",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This refers to Intuition/Justification/Motivation/Validation, suggesting that showing the challenges of Fon tokenization would make the claims more interesting and easier to appreciate."
How do the settings used in the experiments compare to those used for the analysis?,Does the review address Analysis?,TRUE,FALSE,"This refers to Analysis, questioning how the settings used in the experiments compare to those used in the analysis."
N-Bref has a number of components that it relies on to perform its decompilation.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, as it discusses the components N-Bref relies on for decompilation."
Although it turns out that additional components need to be introduced for good performance.,Does the review address Result?,TRUE,FALSE,"This refers to Result, noting that additional components are needed for achieving good performance."
"The idea, at the time the paper was originally written, was indeed very novel as there were not many audio language models around back in May.",Does the review address Novelty?,TRUE,FALSE,"This refers to Novelty, recognizing the paper's novelty in using audio language models, which were rare at the time of writing."
"To reduce the variance due to the sampling of masks, the authors propose a fully-explored masking strategy, where a text sequence is divided into a certain number of non-overlapping segments.",Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, describing the authors' proposed fully-explored masking strategy to reduce variance in sampling."
* One idea I had here: Could you define a natural task as one for which there exists a sparse linear model over the *logits* of p*( .,Does the review address Presentation?,FALSE,TRUE,"This sentence discusses an idea but does not address the clarity, structure, or organization of the paper."
- Perhaps adding more pre-trained LMs such as GPT-2 and different sizes of T-5.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, suggesting the inclusion of additional pre-trained language models like GPT-2 and different sizes of T-5."
"5.3 L638-639: ""unions of countries"" isn't a well defined concept.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This refers to Definition/Description/Detail/Discussion/Explanation/Interpretation, pointing out that the concept of ""unions of countries"" isn't well-defined."
"About type dependency graph: 1) Comparing to previous work (e.g, Allamanis et.al, ICLR 18), it seems the construction of the task specific graph is the major contribution, where the novelty is a bit limited.",Does the review address Contribution?,TRUE,FALSE,"This refers to Contribution, suggesting that the construction of the task-specific graph is the major contribution, though its novelty is limited compared to prior work."
"Though the paper promises faster training speeds in the introduction, Table 3 shows only modest (less than x2) speedups for training.",Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, noting that the claimed faster training speeds in the paper are only modest and not as significant as expected."
- The visualization section is only a minor contribution; there isn't really any innovation or findings about what works or doesn't work here.,Does the review address Presentation?,TRUE,FALSE,"This refers to Presentation, indicating that the visualization section does not contribute much in terms of innovation or findings."
* The paper doesn’t explain why learning a linear model directly on the context embeddings f(s) performs better than using the contextual mean embeddings.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, questioning the reasoning behind learning a linear model directly on context embeddings rather than using the contextual mean embeddings."
"Hence, it is hard to directly relate ""reducing the gap"" and ""improve the test-set performance"".",Does the review address Result?,TRUE,FALSE,"This refers to Result, highlighting the difficulty of relating the reduction in gap to actual improvement in test-set performance."
"As mentioned in the paper, the authors also used some in-house data, which I guess cannot be released to the public.",Does the review address Data/Task?,TRUE,FALSE,"This refers to Data/Task, referring to the use of in-house data that cannot be publicly released."
"Based on the analysis, recommendations on design of multilingual NMT architectures are proposed and their efficacy validated experimentally.",Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, stating that recommendations on the design of multilingual NMT architectures are proposed and validated through experiments."
"The paper here is only concerned with #1 and less concerned with #2, but certainly the previous work addresses #1.",Does the review address Related Work?,TRUE,FALSE,"This refers to Related Work, explaining that the paper focuses on one aspect (e.g., #1) while previous work addresses another aspect (#2)."
There are several parts to the method and there are I assume several differences in the architecture etc with baselines etc.,Does the review address Comparison?,TRUE,FALSE,"This refers to Comparison, noting that there are several differences in the method and architecture compared to baselines."
The hyperparameter search on \alpha in label smoothing is removed.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, pointing out that the hyperparameter search for α\alpha in label smoothing was removed."
"- reasonable initial experimental results demonstrating some ways to help models better use cross-text-chunk dependencies (put them into a contiguous text chunk), providing some hope that these results could make models better.",Does the review address Experiment?,TRUE,FALSE,"This refers to Experiment, noting the initial experimental results that show promising ways to improve model performance by using cross-text-chunk dependencies."
Is any care taken to handle this in training data?,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, questioning whether the training data handling takes care of certain issues."
This paper studies why language model pre-training has been such an effective technique in improving downstream performance across a wide range of NLP tasks recently.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, explaining the study of why language model pre-training has been effective for improving performance on downstream NLP tasks."
"For example, besides the quantative improvements, does the rapid convergence and under-training still exist after applying RandomMask?",Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, questioning if issues like rapid convergence and under-training still persist after applying RandomMask."
weaknesses 1) Approaches are straightforward and lack originality.,Does the review address Novelty?,TRUE,FALSE,"This refers to Novelty, criticizing the paper for lacking originality in its approach."
"Of course, it is valuable to see the great performance achieved by single-task finetuning for RoBERTa.",Does the review address Result?,TRUE,FALSE,"This refers to Result, acknowledging the good performance achieved by single-task finetuning for RoBERTa."
"- Figure 1: unclear why certain input sizes have their accuracy going down, even though they have reached 100% in the earlier epochs.",Does the review address Result?,TRUE,FALSE,"This refers to Result, discussing the unclear relationship between input sizes and accuracy over time."
"I think this paper can reasonably be rejected, but I'd like to give actionable of constructive criticism, since I do think the work on this low resource language is important for the NLP community.",Does the review address Significance?,TRUE,FALSE,"This refers to Significance, emphasizing the importance of the work on a low-resource language for the NLP community."
"The first is selecting the most uncertain examples, and the second is making the CoT annotations longer.",Does the review address Result?,TRUE,FALSE,"This refers to Result, specifying changes or approaches that could impact the results, such as selecting uncertain examples and making CoT annotations longer."
Did you use dynamic masking as that was previous used in RoBERTa?,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, asking if dynamic masking (used in RoBERTa) was employed in the paper."
"While it is hard to formally define meaningful comments, it would be insightful to at least calculate the document frequency of interleaved natural and programming language.",Does the review address Analysis?,TRUE,FALSE,"This refers to Analysis, suggesting that calculating document frequency could provide insight into meaningful comments."
Its current form doesn’t seem to give insight on how the proposed method really helps.,Does the review address Analysis?,TRUE,FALSE,"This refers to Analysis, pointing out that the current form of the paper lacks insight into how the proposed method is beneficial."
"Similar numbers are true for the rest of the tasks: 60.90 vs. 87 for OBQA, 71.01 vs. 90 for PIQA, and 63.20 vs.  89.70 for aNLI.",Does the review address Result?,TRUE,FALSE,"This refers to Result, comparing the performance on different tasks and providing quantitative results."
The experimental results mainly address similar networks with similar context lengths.,Does the review address Comparison?,TRUE,FALSE,"This refers to Comparison, noting that the experimental results are focused on similar networks with similar context lengths."
"Absent both kinds of keyphrases) is evaluated against baselines (which contains only ""present"" type of keyphrases).",Does the review address Evaluation?,TRUE,FALSE,"This refers to Evaluation, discussing the evaluation of different types of keyphrases against baselines."
It is unclear for readers the details of the selection.,Does the review address Presentation?,TRUE,FALSE,"This refers to Presentation, stating that the details of the selection process are unclear to readers."
About experiments: 1) I think one ablation study I’m most interested in is to simply run GNN on the AST (or simply use Allamanis et.al’s method).,Does the review address Experiment?,TRUE,FALSE,"This refers to Experiment, requesting a specific ablation study related to running GNN on the AST or using Allamanis et al.'s method."
"On the other hand, eq 7 should be the same as eq 5.",Does the review address Comparison?,TRUE,FALSE,"This refers to Comparison, pointing out that equations 7 and 5 should be the same."
Look forward to the author discussing in following version.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This refers to Definition/Description/Detail/Discussion/Explanation/Interpretation, as it suggests a discussion in the next version."
"While I think it's nice to analyze the connection between RM and DM, the math provided in the paper is simple, and the main contribution is just to add a baseline to the algorithm of Khalifa et al.",Does the review address Contribution?,TRUE,FALSE,"This refers to Contribution, suggesting that the paper's main contribution is adding a baseline to Khalifa et al.'s algorithm."
"- The paper also emphasizes sample efficiency, which is an essential problem in practical molecular design.",Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, mentioning the emphasis on sample efficiency, which is important in practical molecular design."
The other baselines all use the full template-based action space (except the DRRN) of size 10^8 - a auxiliary entropy loss is used there derived from the valid actions but it is not a hard constraint.,Does the review address Comparison?,TRUE,FALSE,"This refers to Comparison, comparing the use of a full template-based action space with other baselines."
Very well written manuscript with clear non-monotonic description of details.,Does the review address Presentation?,TRUE,FALSE,"This refers to Presentation, commenting on the clarity and quality of writing in the manuscript."
"* Theorem 2 is a restatement of past work, showing that transformers lie in logspace-uniform TC^0 * Theorem 3 assumes TC^0 \neq P / poly, and then derives that transformers cannot simulate any poly-time circuit.",Does the review address Theory?,TRUE,FALSE,"This refers to Theory, discussing theorems related to transformers and computational complexity."
Also helpful to look at [loopInvariant] and the related work mentioned there.,Does the review address Related Work?,TRUE,FALSE,"This refers to Related Work, suggesting looking at loopInvariant and the related work mentioned."
It seems like more quantitative analysis would be needed to determine how much the LM's attention is correlating empirically to factual knowledge or if there are other factors that are affecting the downstream improvements.,Does the review address Analysis?,TRUE,FALSE,"This refers to Analysis, suggesting that more quantitative analysis is needed to determine the correlation between LM's attention and factual knowledge."
I think this means that some amount of claim rewriting is required in addition to the changed baselines.,Does the review address Comparison?,TRUE,FALSE,"This refers to Comparison, stating that claim rewriting is necessary due to changes in the baselines."
"Weaknesses, suggested improvements and requested clarifications  1.",Does the review address Result?,FALSE,TRUE,This sentence does not provide information about results.
An additional ablation experiment trying different number of tokens to optimize could be illuminating (even for just 1 dataset).,Does the review address Experiment?,TRUE,FALSE,"This refers to Experiment, suggesting an additional ablation experiment with varying the number of tokens to optimize."
"As sort of an ensemble of expert models, the paper does not include any system-level ablation studies by comparing against other design choices.",Does the review address Ablation?,TRUE,FALSE,"This refers to Ablation, noting the absence of system-level ablation studies comparing different design choices."
The provided experiments are more like baselines for self-evaluation other than state-of-the-art performance.,Does the review address Comparison?,TRUE,FALSE,"This refers to Comparison, suggesting that the experiments mainly serve as baselines rather than showing state-of-the-art performance."
It would be good if the authors could provide some discussion around this observation.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This refers to Definition/Description/Detail/Discussion/Explanation/Interpretation, requesting further discussion to explain an observation."
"If it is the extension to multilingual embeddings, a few lines explaining the novelty would help.",Does the review address Novelty?,TRUE,FALSE,"This refers to Novelty, requesting clarification on the novelty of the extension to multilingual embeddings."
It is a bit difficult to understand the task definitions without first understanding the various constituents of the proof.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This refers to Definition/Description/Detail/Discussion/Explanation/Interpretation, as it discusses the clarity and understanding of task definitions in the proof."
"Especially because of the surprising magnitude by which this pruning degrades absolute performance, it is unfortunately necessary to try more pruning rates for a fair comparison.",Does the review address Result?,TRUE,FALSE,"This refers to Result, addressing how pruning affects performance and suggesting the need for a more thorough comparison with additional pruning rates."
"In Figure 1, results with p ranges from 0.1 to 0.5 are shown.",Does the review address Presentation?,TRUE,FALSE,"This refers to Presentation, simply noting the presentation of results with specific ranges of p in Figure 1."
"- Definition of meaning seems to defeat the whole purpose of this paper, as it allows for any gibberish/random sequence of tokens to still induce a meaning and possibly a set of many other gibberish sequences to be in their equivalent class.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This refers to Definition/Description/Detail/Discussion/Explanation/Interpretation, critiquing the definition of meaning in the paper and its implications."
Implementation details are only given for the vanilla BERT Are they similar to the EarlyBERT model as well?,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, asking for clarification on whether the implementation details provided are similar to the EarlyBERT model."
"####Summary:  To tackle situations where compositionality is mostly required at inference time, the paper proposes a novel data augmentation method with an RNN based generator (recombination); to make the generator generate highly compositional patterns, the paper proposes a resampling method.",Does the review address Presentation?,TRUE,FALSE,"This refers to Presentation, summarizing the proposed method and its key components."
"- In Table 1, can you explain more explicitly (in caption and text) what “subset” and “class words” means?",Does the review address Presentation?,TRUE,FALSE,"This refers to Presentation, requesting clarification on the terms ""subset"" and ""class words"" in the caption and text of Table 1."
"What the authors can do is: you can sample some sentences from the test/development set and count how many comparative words are misused in the original model, among which how many are corrected by reranking.",Does the review address Analysis?,TRUE,FALSE,"This refers to Analysis, suggesting an approach to analyze the model's performance by examining comparative word misuse and reranking corrections."
They have evaluated their architecture based on (i) the language modelling test evaluated on PTB and FBIS and (ii) Chinese-English machine translation task on NIST MT02-08 evaluation sets.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, describing the evaluation of the architecture on specific tasks (language modeling and machine translation)."
"- The literature of DP in multi-modality is lacking, and therefore the work could be interesting  - There are several factually inappropriate usages of the notion of DP.",Does the review address Related Work?,TRUE,FALSE,"This refers to Related Work, discussing the lack of literature on DP in multi-modality and inappropriate usages of the term."
I realize that PENGI's checkpoint was probably not available when this paper was submitted but it is now.,Does the review address Significance?,TRUE,FALSE,"This refers to Significance, noting the importance of the PENGI checkpoint being available now and how it affects the paper's significance."
An additional ablation experiment trying different number of tokens to optimize could be illuminating (even for just 1 dataset).,Does the review address Data/Task?,TRUE,FALSE,"This refers to Data/Task, suggesting an additional ablation experiment to optimize the number of tokens using a specific dataset."
*  The claims regarding the 10x better data efficiency are not well supported and I would suggest the authors to compare with transformer models on standard LM benchmarks and potentially to some downstream tasks such as MT to make a stronger case.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, criticizing the claims regarding data efficiency and recommending comparisons with transformer models on standard benchmarks and downstream tasks."
The error analysis might be better to be a bit more quantitative.,Does the review address Analysis?,TRUE,FALSE,"This refers to Analysis, suggesting that the error analysis could benefit from being more quantitative."
- You provide the mapped ORCHID corpus in JSON format in the Supplementary Material.,Does the review address Data/Task?,TRUE,FALSE,"This refers to Data/Task, mentioning the availability of the ORCHID corpus in the Supplementary Material."
"Without the ablation studies on these two aspects, we cannot determine whether the performance improvement truly comes from the author's contribution or just longer CoT annotations.",Does the review address Ablation?,TRUE,FALSE,"This refers to Ablation, pointing out the lack of ablation studies to determine the true source of performance improvement."
The results on Split H are positive and they also conducted a range of ablation and error analysis.,Does the review address Result?,TRUE,FALSE,"This refers to Result, stating that the results on Split H are positive and mentioning conducted ablation and error analysis."
"I really like the research question and goals of this work, as it draws an interesting connection between transformers' ability to execute instructions (posed as circuits) and existing results analyzing transformers via circuits and classical work on universal circuits.",Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, expressing appreciation for the research question and the connection between transformers' ability to execute instructions and prior work on circuits."
The simple combination of the audio model and LLM does not seem to be novel.,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, suggesting that the combination of the audio model and LLM is not novel."
"There's not much justification for it, especially given something simpler like a fixed window average could have been used.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"This refers to Intuition/Justification/Motivation/Validation, critiquing the lack of justification for a specific method and suggesting simpler alternatives."
"_ There are some missing citations that could me mentioned in related work as : Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space Neelakantan, A., Shankar.",Does the review address Related Work?,TRUE,FALSE,"This refers to Related Work, noting missing citations that should be included in the related work section."
Authors could have plugged their embedding strategy in LayoutLM to understand the impact of that particular component.,Does the review address Comparison?,TRUE,FALSE,"This refers to Comparison, suggesting that the authors could have tested their embedding strategy in LayoutLM to assess its impact."
"2016 [2] A syntactic neural model for general-purpose code generation, Yin and Neubig 2017 [3] Making Neural Programming Architectures Generalize via Recursion, Cai et al. 2017  ####Authors have engaged in the discussion, clarified questions about the paper and addressed comments in its newest revision.",Does the review address Related Work?,TRUE,FALSE,"This refers to Related Work, citing relevant previous work and noting that the authors have addressed questions in the latest revision."
"If not, it seems unfair to compare with PMO's best baseline REINVENT.",Does the review address Comparison?,TRUE,FALSE,"This refers to Comparison, pointing out the unfairness of comparing with PMO's best baseline REINVENT if not done properly."
"** Again, the performance improvement may come from two aspects.",Does the review address Result?,TRUE,FALSE,"This refers to Result, acknowledging that performance improvement could stem from multiple factors."
The authors have done further experiments and show that there are still gains on these tasks when model sized is increased significantly.,Does the review address Experiment?,TRUE,FALSE,"This refers to Experiment, noting the authors' further experiments showing performance gains with increased model size."
A thorough proofreading could enhance the clarity of writing and word choice.,Does the review address Presentation?,TRUE,FALSE,"This refers to Presentation, suggesting that proofreading could improve the manuscript's clarity and word choice."
"They do pre-train their model (BROS) on a large dataset with 11M documents, and then used such models to perform downstream tasks in four smaller datasets.",Does the review address Data/Task?,TRUE,FALSE,"This refers to Data/Task, describing the pre-training of a model (BROS) on a large dataset and its use on smaller downstream tasks."
Can the proposed theory help explain some of the successes of one architecture over others?,Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, questioning if the proposed theory can explain the success of one architecture over others."
"Specifically, I would expect authors provide more detailed recommendation for AL, DS, and multi-domain sampling in terms of sampling techniques, and population of different sources for certain application.",Does the review address Data/Task?,TRUE,FALSE,"This refers to Data/Task, asking for more detailed recommendations on sampling techniques and populations for specific applications."
May I know many questions are in each data split shown in Table 5?,Does the review address Presentation?,TRUE,FALSE,"This refers to Presentation, asking for clarification on the number of questions in each data split in Table 5."
- Performance with relatively little finetuning data are encouraging.,Does the review address Data/Task?,TRUE,FALSE,"This refers to Data/Task, mentioning that performance with minimal fine-tuning data is encouraging."
"However, they merely note that their data was annotated at the “relation” level rather than at the triple (relation, entity pair) level… but couldn’t Bordes et al. have done the same in their annotation?",Does the review address Related Work?,TRUE,FALSE,This refers to Related Work
I see this work more as an analysis on language-specific parameters for a particular LS-model rather than a novel architecture.,Does the review address Methodology?,TRUE,FALSE,The review mentions analysis of language-specific parameters rather than introducing a novel architecture.
"To me, D looks to be an important efficiency tradeoff.",Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, specifically addressing the importance of efficiency trade-offs in the model."
(ii) a new way to aggregate multiple inputs (using M-BERT) and several different decoding methods.,Does the review address Significance?,TRUE,FALSE,The review touches on the significance of the novel approach of aggregating inputs and decoding methods.
I’m not saying these problems aren’t important – especially type recovery (I think this problem is deeply important) – but that it should go further to demonstrate more dimensions of decompilation.,Does the review address Significance?,TRUE,FALSE,The review mentions the significance of type recovery but suggests the paper should explore more dimensions.
- The model uses two RNNs: a chain-based one and a knowledge guided one.,Does the review address Methodology?,TRUE,FALSE,"The review points to the methodology, detailing the use of two RNNs."
And there is no further explanation and ablation study on the design of the dynamic threshold.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,The review critiques the lack of explanation or ablation study on dynamic threshold design.
"Their goal is to reduce the amount of parameters and data needed, while improving (or maintaining) state-of-the-art performance.",Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, specifically addressing the goal of reducing parameters and data while maintaining performance."
"If that is the case, that should be made more explicit.",Does the review address Presentation?,TRUE,FALSE,The review asks for clearer presentation of the methodology.
"[5] FP8 Formats for Deep Learning, Micikevicius et al. 1- The paper is well-written and organized.",Does the review address Related Work?,TRUE,FALSE,"The review mentions the paper's presentation, noting it is well-written and organized."
"The experiments in the paper demonstrated the superiority of adding adversarial training to a self-supervision framework, in that significant improvements can be obtained for similar-sized networks.",Does the review address Methodology?,TRUE,FALSE,"The review discusses methodology, specifically the impact of adversarial training on self-supervision."
"* In the results section there is a typo: *""performances with a large margins of 2.32pp in""*.",Does the review address Presentation?,TRUE,FALSE,The review critiques the presentation due to a typo in the results section.
"In particular, using a carefully selected subset of ""prompt"" words, the authors observe that learning a linear predictor over the next word distributions of these words achieves performance close to a pre-trained GPT-2 model.",Does the review address Presentation?,FALSE,TRUE,"This sentence discusses the methodology involving ""prompt"" words and predictors, but does not address the clarity or organization of the paper."
"* Strength     * This paper proposes an interesting idea and interpretation to connect RM and DM paradigm     * This paper proposes a variance reduction method for DPG which demonstrates its improvement on performance, stability and sample efficiency * Weakness     * From my understanding, the baseline mostly comes from the observation in 3.3, which has limited technical novelty.",Does the review address Novelty?,TRUE,FALSE,"The review assesses the novelty of the paper, pointing out limited technical novelty due to baseline reliance."
"Why are the backbone models (RoBERTa and BERT, respectively) different in Table 1 and Table 2?",Does the review address Presentation?,TRUE,FALSE,The review raises a question regarding the presentation of results in different tables.
"It will be interesting to see the impact of the RNN and Copy RNN based model on automatic extraction of local or ""present"" type of key phrases.",Does the review address Significance?,TRUE,FALSE,The review refers to the significance of RNN-based models in automatic extraction of key phrases.
"Once more baselines are included, it is very possible that the performance will be surpassed.",Does the review address Comparison?,TRUE,FALSE,"The review provides a comparative analysis, noting that performance might be surpassed with additional baselines."
The authors try to interpret the design of the neural networks using the concepts in the proposed binding-unbinding theorybut are not convincible.,Does the review address Theory?,TRUE,FALSE,"The review addresses the theory behind the neural network design, suggesting that the interpretation is unconvincing."
It would be great if the authors discuss this or provide some supporting evidence about its correctness.,Does the review address Methodology?,TRUE,FALSE,The review calls for more discussion or evidence supporting the theoretical claims.
I had to go multiple times back-and-forward in this paper to understand what was new in it.,Does the review address Presentation?,TRUE,FALSE,"The review mentions presentation issues, noting difficulty in following the new contributions."
* The rationale behind the architectural choices for the self-attention component is not well explained or empirically verified.,Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, criticizing the lack of explanation or verification of architectural choices."
"It is not clear how this model would compare to other models using language specific parameters (sparsely gated mixture of experts (Lepikhin et al 2020), light-weight adapters (Bapna et al 2019)  ).",Does the review address Comparison?,TRUE,FALSE,The review suggests the need for comparison to other models using similar parameters.
"How would it perform if LLM is not GPT-4, but rather those open-source alternatives like Llama.",Does the review address Methodology?,TRUE,FALSE,"The review asks for clarification on how the model would perform with different LLMs, which touches on methodology."
"Or if they are measuring the probability assigned to the true image and not just accuracy, the name shoudl be changed from accuracy.",Does the review address Presentation?,TRUE,FALSE,The review points out a potential issue with terminology in the evaluation section.
"The presented method chooses action primitives such as ""PickAndPlace"" but does not need much training data (apart from the examples).",Does the review address Methodology?,TRUE,FALSE,The review mentions the methodology and the limited training data required for the presented method.
Details of training and dataset are logical and delicate.,Does the review address Data/Task?,TRUE,FALSE,The review appreciates the logical structure of the training and dataset details.
"Or, is the label distribution on the annotated subsets derived via vote-*k* indeed skewed for some tasks and the performance improvements are mainly coming from improvements on the labels that are well-represented?",Does the review address Result?,TRUE,FALSE,"The review questions the data distribution, suggesting the need for further clarification."
Pros:  - A new framework for understanding why learning how to predict the next word helps the downstream task.,Does the review address Methodology?,TRUE,FALSE,"The review discusses the methodology, emphasizing a novel framework for understanding downstream task improvement."
"I think authors wanted to say that even though BROS does not rely on visual features, it does outperform LayoutLM which, in turn, uses visual features.",Does the review address Presentation?,TRUE,FALSE,"The review highlights presentation, suggesting the authors’ intent regarding comparison with LayoutLM."
"Another problem is that there is only few qualitative results, and in both these two examples, predicted results cover the GT segments.",Does the review address Result?,TRUE,FALSE,"The review critiques the presentation of the results, noting the lack of qualitative diversity."
"The two datasets have some “toy flavor”, while SCAN great favors example combination (with recomb-2 performs much better than recomb-1), recomb-1 seem to perform better for morphological analysis dataset, leaving questions about how to choose the exact models in general.",Does the review address Data/Task?,TRUE,FALSE,"The review discusses data/task concerns, questioning the generalization of results across datasets."
"T-LLMs are trained with huge batches, and it can be hard to pick out all the information about one example from a batch.",Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, pointing out the challenges with large batch training."
"FLOPS is a measure of computer performance, while arithmetic intensity is the ratio of total floating-point operations to total data movement.",Does the review address Result?,TRUE,FALSE,"The review briefly mentions performance metrics, particularly in the context of result analysis."
"3) The experimental results reported are validated on a single dataset, and no human evaluation and error analysis.",Does the review address Evaluation?,TRUE,FALSE,"The review points to limitations in the experimental validation, lacking human evaluation and error analysis."
In Proceedings of the 6th Workshop on Asian Translation (pp.,Does the review address Related Work?,TRUE,FALSE,The review refers to related work.
"As far as I know, this is indeed the first work for handling this task using binding-unbinding mechanism.",Does the review address Data/Task?,TRUE,FALSE,"The review discusses data/task novelty, highlighting the use of binding-unbinding in this work."
"Ablation studies on the varying parameter counts of these two components would be valuable, if possible.",Does the review address Ablation?,TRUE,FALSE,The review calls for more ablation studies to assess the impact of varying parameters.
"I appreciate that the authors do not read too much into it and focus more on the analysis of the results, but one thing that remains unanswered in this paper is how the proposed method fairs against multilingual baselines that utilize (roughly) the same number of parameters; currently, the best models outperform the LS baseline by ~28M and ~10M parameters on OPUS-100 and WMT-14 respectively.",Does the review address Analysis?,TRUE,FALSE,The review asks for a comparison to multilingual baselines to substantiate the model's performance.
The performance is impressive and could be a better baseline for the future work.,Does the review address Result?,TRUE,FALSE,The review acknowledges the strong performance and potential future relevance of the work.
"Also, what is the meaning of the two segments of ""suppress""?",Does the review address Presentation?,TRUE,FALSE,The review questions the clarity of terminology used in the presentation.
Is there any reason to use a static attention for all words?,Does the review address Methodology?,TRUE,FALSE,The review raises a methodological question regarding the use of static attention across words.
"The paper in general is well-written and easy to follow, the qualitative analysis and the additional diagrams in the appendix illustrating the variations in policies are appreciated.",Does the review address Presentation?,TRUE,FALSE,"The review praises the presentation, particularly the qualitative analysis and diagrams."
"The evaluation results are based on the authors' implementations, for both baseline and the proposed method.",Does the review address Comparison?,TRUE,FALSE,"The review touches on comparison, noting the use of the authors' own implementations for evaluation."
"The latter is typically used in two different ways in the transformer architecture, each resulting in a different computation for RF  is confusing as the RFA is now redefined.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,The review mentions confusion regarding the redefinition of RFA and its impact on computations.
"I think it would improve the paper if you could focus on a certain kind of invariants, and show that these invariants can in fact generalize across programs.",Does the review address Methodology?,TRUE,FALSE,The review suggests a methodological improvement to enhance generalization across programs.
"Further, [5] finetunes (and even trains from scratch) large Transformers in FP8.",Does the review address Methodology?,TRUE,FALSE,The review mentions the methodology of finetuning large Transformers in FP8.
"* The current analysis doesn’t apply directly to BERT, which is trained to predict masked words in a sentence, instead of the next word.",Does the review address Analysis?,TRUE,FALSE,The review discusses an analysis issue related to BERT's training setup.
I had to read it a couple of times before I could fully follow the method.,Does the review address Methodology?,TRUE,FALSE,"The review points out difficulty in following the methodology, suggesting potential clarity issues."
This could be tested by ablating elements from the input or ablating the recurrent memory vectors (setting them to zero during inference).,Does the review address Ablation?,TRUE,FALSE,The review suggests performing an ablation experiment to validate the model.
And are the previous work using the same training set?,Does the review address Related Work?,TRUE,FALSE,The review inquires about the consistency of the training set with related work.
This paper set an assumption that the teacher network makes a less confident prediction than that of the student and extends gradient analysis in the perspective of regularization effect in the proposed adaptive label smoothing.,Does the review address Methodology?,TRUE,FALSE,The review discusses the methodology of teacher-student networks and adaptive label smoothing.
The paper proposed a weakly-supervised wMAN model for moment localization in untrimmed videos.,Does the review address Methodology?,TRUE,FALSE,The review mentions the methodology of the weakly-supervised wMAN model.
"Other smaller suggested fixes:  * Section 5, near the end - Little grammatical mistake.",Does the review address Presentation?,TRUE,FALSE,"The review mentions a minor issue related to presentation, such as grammar."
The paper sets out to formally characterize controllability of LLMs which is an important issue in preventing adversarial attacks on language models and preventing LLMs from producing undesirable content.,Does the review address Methodology?,TRUE,FALSE,The review highlights a methodology-related contribution regarding the controllability of LLMs.
"The paper proposes a new joint learning algorithm that works for two tasks, NER and RE.",Does the review address Data/Task?,TRUE,FALSE,The review mentions the proposed joint learning algorithm related to NER and RE tasks.
I doubt that InfoNCE can represent the best performance of CLIP trained on CC (3M) and WIT(5M).,Does the review address Result?,TRUE,FALSE,The review raises concerns about the results related to InfoNCE performance in CLIP.
The Graph connectivity ablation states that connecting the e_int node to all entities (instead of just the input text entities) hurts performance.,Does the review address Result?,TRUE,FALSE,The review refers to an ablation analysis and its findings about graph connectivity.
"- (The supplied code does not seem to include the baselines, just the recursive NN models.",Does the review address Methodology?,TRUE,FALSE,The review points out a methodological issue regarding the supplied code.
## Paper strengths and contributions **Motivation and intuition** The motivation for multi-turn code generation is convincing.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,The review emphasizes the motivation and intuition behind the proposed approach.
"Assuming that these are two different programs, there is no reason to assume that the contract of `calculateTime()` remains the same.",Does the review address Presentation?,TRUE,FALSE,The review mentions presentation issues in understanding assumptions about program contracts.
Other questions for the authors: (1) What is the loss in performance by fixing the word embeddings in the dependency parsing task?,Does the review address Result?,TRUE,FALSE,The review queries about the result of fixing word embeddings in dependency parsing tasks.
Do you think the conclusion would be still the same if a language-specific hyper-parameter p_l was used instead?,Does the review address Methodology?,TRUE,FALSE,The review discusses a methodological alternative regarding the use of language-specific hyper-parameters.
"Neither the proposed “Quad” loss function, nor the theoretically inspired “conditional mean features”, perform better than the baselines.",Does the review address Comparison?,TRUE,FALSE,The review compares the performance of the proposed methods against baselines.
The experimental results are promising for both settings.,Does the review address Result?,TRUE,FALSE,The review highlights promising results from the experiments.
An ablation analysis would be most appropriate for quantifying this.,Does the review address Evaluation?,TRUE,FALSE,The review suggests using ablation analysis to quantify certain aspects of the model.
"The paper meticulously provides all experimental details, and the ablation study helps to validate the design components, enhancing the overall robustness of the research.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,The review appreciates the detailed experimental setup and ablation study used for validation.
LOW LEVEL COMMENTS Equation 5: what's the difference between id(w) = id(w') and w = w' ?,Does the review address Presentation?,TRUE,FALSE,"The review points out presentation-related issues, such as clarifying Equation 5."
"I know you cite the Abbott & Martinus, 2018 paper, stating that BPE is bad for analytical languages, but I still think it would prove a point to show BPE performing badly for your data.",Does the review address Result?,TRUE,FALSE,The review challenges the results and suggests more evidence regarding BPE's performance on the data.
- The experimental analysis focuses exclusively on known computer vision datasets that do not differ from the training distribution of CLIP.,Does the review address Data/Task?,TRUE,FALSE,The review critiques the experimental setup for lacking diversity in datasets.
"In the experiments, the authors make comparisons with traditional methods, and show the effectiveness of their model.",Does the review address Experiment?,TRUE,FALSE,The review acknowledges that comparisons with traditional methods are made in the experiments.
"Additionally, the rationale for choosing (Ma et al., 2023) as a benchmark, along with the significance of the improvements observed, even if minute, should be clearly articulated.",Does the review address Data/Task?,TRUE,FALSE,The review suggests improving the explanation for the choice of benchmarks and observed improvements.
"The idea is similar to structured / syntax-based attention (i.e. attention over nodes from treeLSTM); related work includes Zhao et al on textual entailment, Liu et al. on natural language inference, and Eriguchi et al.",Does the review address Methodology?,TRUE,FALSE,The review mentions related works that share similarities with the proposed method.
Section 3.4  does not describe clearly enough how attention maps were used for predicting contact maps.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,The review points out that Section 3.4 lacks clarity in describing the use of attention maps.
"Conventionally the ITM loss is a binary prediction task, while the particular one used in this work is more often referred as contrastive learning loss.",Does the review address Presentation?,TRUE,FALSE,The review discusses a presentation issue regarding the classification of the ITM loss function.
- Figure 1 is helpful in comprehending the effect of different loss functions vs robustness.,Does the review address Presentation?,TRUE,FALSE,The review acknowledges the utility of Figure 1 in explaining the effect of loss functions on robustness.
"There is a lack of experimental analysis supporting the source of the observed improvements, which is crucial for substantiating the paper's main claims.",Does the review address Result?,TRUE,FALSE,The review criticizes the lack of experimental analysis to support the paper's main claims.
"Each network branch is from known structures, but the combination is not proposed before.",Does the review address Novelty?,TRUE,FALSE,The review comments on the novelty of the network branch combination.
It also exhibits explainability during the generation.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,The review highlights the model's explainability during generation.
"More importantly, the experiments are not convincing as it is presented now.",Does the review address Experiment?,TRUE,FALSE,The review expresses doubt about the convincing nature of the experimental results.
This paper is meaningful and presents a reasonable analysis.,Does the review address Significance?,TRUE,FALSE,The review acknowledges the paper’s significance and the reasoned analysis.
(3) How much does coverage affect the score in table 2?,Does the review address Presentation?,TRUE,FALSE,The review queries the impact of coverage on the results presented in Table 2.
"Then, most of the paper is spent discussing preliminaries and introducing notation and definitions.",Does the review address Presentation?,TRUE,FALSE,The review points out a potential issue with spending too much time on preliminaries.
"- For the open-ended questions, this work seems to focus mainly on solely LLMs assisted question answer generation.",Does the review address Methodology?,TRUE,FALSE,The review mentions the focus of the paper on LLM-assisted question answering.
"Strengths:  - The paper is generally well-written, with excellent motivation and empirical setup/analysis  - The overall strategy of differentiable prompt optimized to maintain fluency is reasonable and novel.",Does the review address Presentation?,TRUE,FALSE,"The review highlights strengths in the motivation, setup, and fluency of the strategy."
- The analysis which builds on these definitions/models/assumptions provides meaningful theoretical insight into why language model pre-training may be so beneficial for downstream training.,Does the review address Analysis?,TRUE,FALSE,The review praises the theoretical insight provided by the analysis.
"In the 2nd paragraph of Section 4.1, ``For each s_k in s, we try to find a matched example (tx, ty) from D what tx contains the s_k’’: (1) There should be many sentence pairs (tx,ty) that tx contains s_k.",Does the review address Presentation?,TRUE,FALSE,The review points out a presentation issue related to the explanation in Section 4.1.
"2.Through a significant number of ablation experiments, this paper extensively researched the model's hyperparameter configurations and training strategies, offering highly instructive guidance for related work.",Does the review address Methodology?,TRUE,FALSE,The review appreciates the use of ablation experiments for model research.
"There is a lack of experimental analysis supporting the source of the observed improvements, which is crucial for substantiating the paper's main claims.",Does the review address Analysis?,TRUE,FALSE,The review again emphasizes the need for better experimental analysis to back
There is a lot missing to actually justify this claim: 1.,Does the review address Result?,TRUE,FALSE,"The statement discusses a missing justification for a claim, implying that the review refers to a result that is not well-supported."
Here's how I would reconstruct the proof of Theorem 3:  *Proof.,Does the review address Theory?,TRUE,FALSE,"The review refers to the proof of a theorem, which involves a theoretical discussion."
"But that would also mean that the phrases are determined by token ngrams which produces a sliding window of the ""pyramid encoders"" for each sentence where there are instance where the parameter for these phrases will be set close to zero to disable the phrases and these phrases would be a good intrinsic evaluation of the pRNN in addition to evaluating it purely on perplexity and BLEU extrinsically.",Does the review address Evaluation?,TRUE,FALSE,The review includes a discussion of evaluating the pRNN based on intrinsic and extrinsic measures.
"- The proposed architecture is tested on massive experiments including language understanding tasks, optical flow, video audio class autoencoding, image classification, and starcraft II and achieves superior performance.",Does the review address Result?,TRUE,FALSE,"The review discusses the performance of the proposed architecture on various tasks, which is a result."
or adopt the exponential-moving-average (EMA) manner [3].,Does the review address Methodology?,TRUE,FALSE,"The review mentions adopting the exponential-moving-average method, which refers to methodology."
I suggest authors to add discussion about the perfomance of DeFo for domain generalization.,Does the review address Methodology?,TRUE,FALSE,"The review suggests adding a discussion about a specific method (DeFo), relating to methodology."
The second uses Gaussian blurring to encourage information sharing among neighboring words.,Does the review address Methodology?,TRUE,FALSE,The review discusses a methodological approach using Gaussian blurring.
Perhaps there is some visual representation that could help demonstrate the comparisons you make in the text?,Does the review address Comparison?,TRUE,FALSE,"The review suggests a visual representation for comparisons, indicating a reference to comparison."
- General Discussion: The main focus of this paper is the introduction of a new model for learning multimodal word distributions formed from Gaussian mixtures for multiple word meanings.,Does the review address Methodology?,TRUE,FALSE,"The review introduces a new model, which is part of the methodology."
"This paper proposes a neural network architecture that represent structural linguistic knowledge in a memory network for sequence tagging tasks (in particular, slot-filling of the natural language understanding unit in conversation systems).",Does the review address Presentation?,TRUE,FALSE,"The review discusses the proposed architecture, which is a form of presentation."
- It seems like the optimal value of $k$ in vote-*k* would depend on the number of instances in the unlabeled set that changes with the tasks.,Does the review address Data/Task?,TRUE,FALSE,"The review discusses the dependency on the number of instances in the task, indicating a data/task-related discussion."
The paper is mostly clearly written and discusses server interesting ablation experiments.,Does the review address Experiment?,TRUE,FALSE,"The review mentions ablation experiments, which are part of the experiment section."
"Did the author try other window widths, for example width `1' to extract unigram features, `3' to trigram, or use them together?",Does the review address Experiment?,TRUE,FALSE,"The review refers to experimenting with different window widths, indicating an experimental approach."
2.The authors may add subjective evaluations to the ablation experiments to better demonstrate that the LoRA fine-tuning strategy mitigates catastrophic forgetting issues.,Does the review address Ablation?,TRUE,FALSE,The review discusses adding subjective evaluations to ablation experiments.
"Theoretically, they showed that synchronized updates to the low-level and high-level policy may never converge, yet asynchronized updates guarantees convergence.",Does the review address Contribution?,TRUE,FALSE,"The review mentions theoretical results about updates, indicating a contribution to the field."
We have no empirical demonstration that this approach will work on other datasets outside of LeetCode.,Does the review address Data/Task?,TRUE,FALSE,"The review references the lack of empirical data for generalization across datasets, pointing to a data/task issue."
"Questions:   - Previous work has tried to combine both language-specific and shared parameters (Wang et al 2018), rather than making a binary choice between these.",Does the review address Related Work?,TRUE,FALSE,"The review references prior work, indicating a related work discussion."
"This is relevant because, by training end to end, that work effectively generates arbitrary amounts of training data through interaction the the HOL4 ITP system (intermediate theorems which are proven give some reward in that work).",Does the review address Data/Task?,TRUE,FALSE,"The review refers to the generation of training data, indicating a focus on data/task."
The authors first qualitatively and quantitatively analyze the cold-start problem.,Does the review address Analysis?,TRUE,FALSE,"The review discusses analysis of the cold-start problem, referring to analysis."
"- wMAN is evaluated with two publicly available datasets, and is compared with state-of-the-art methods and other ""oracle"" baselines.",Does the review address Methodology?,TRUE,FALSE,"The review discusses the evaluation process, indicating a methodological focus."
The result presented in Table 4 don't match the description in Section 4.3:  - It's not true that the pRNN outperforms both PBSMT and Enc-Dec model.,Does the review address Result?,TRUE,FALSE,"The review identifies a discrepancy in the results, indicating a result-based discussion."
"Given that one of the primary goals of this paper was to create embeddings that perform well under the word translation metric (intra-language), it is disappointing that the method that performs best (by far) is the invariance approach.",Does the review address Evaluation?,TRUE,FALSE,"The review evaluates the performance of different approaches, indicating an evaluation focus."
"A similar analysis here could greatly demystify why these sets of examples cause instability, and whether they are indeed “informative”.",Does the review address Analysis?,TRUE,FALSE,"The review suggests an analysis of instability, referring to analysis."
(4) The dictionary extraction approach (from parallel corpora via alignments or from google translate) may not reflect the challenges of using real lexicons.,Does the review address Methodology?,TRUE,FALSE,The review discusses the methodology of dictionary extraction and its limitations.
"Presentation: - The ""Crime Suppression Division"" example would be clearer if you showed it graphically in a figure.",Does the review address Presentation?,TRUE,FALSE,"The review suggests a graphical presentation of the example, referring to presentation."
The idea is straightforward and the motivation is clear.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The review acknowledges the clarity of the idea and motivation, focusing on intuition/justification/motivation."
The proposed beam enumeration significantly outperforms REINVENT (the strongest baseline in the existing benchmark).,Does the review address Comparison?,TRUE,FALSE,"The review compares the proposed method with a benchmark, indicating a comparison."
"I would recommend to put the citation around it (Andreas, 2000) although previously cited.",Does the review address Related Work?,TRUE,FALSE,"The review suggests referencing prior work, indicating related work."
"Good set of ablation studies to show that each component of the model is necessary, especially because the entire model already has many moving parts in addition to adversarial training.",Does the review address Methodology?,TRUE,FALSE,"The review praises ablation studies, indicating a methodological approach."
"You are clearly not trying to infer any loop invariants, and it would help clarify that upfront.",Does the review address Result?,TRUE,FALSE,"The review suggests clarifying the goal, focusing on the result."
"In Empirical Methods in Natural Language Processing (EMNLP), 2019.",Does the review address Related Work?,TRUE,FALSE,"The review references a related conference, indicating related work."
3) This paper shows visualization of the interaction between words and latent topics in the embedding space.,Does the review address Presentation?,TRUE,FALSE,"The review mentions visualization, referring to presentation."
"Furthermore, the authors deliberately avoid settings where DP is known to be hard due to the relatively low amount of training data per class (e.g. CIFAR-100/ImageNet).",Does the review address Data/Task?,TRUE,FALSE,The review addresses data and task-specific challenges.
"Summary: This paper presents a method of incorporating prior knowledge into MCTS via language, using interactive fiction games as a test bed.",Does the review address Methodology?,TRUE,FALSE,The review refers to a methodology incorporating prior knowledge.
Some discussion on the current design choices and why making the proposed methods features to some of the other baselines is not a way to achieve some benefits is not the right way to do it.,Does the review address Methodology?,TRUE,FALSE,"The review suggests further discussion on design choices, referring to methodology."
Novel weighting scheme for SVD for low-rank weight compression (though should double check this more thoroughly).,Does the review address Novelty?,TRUE,FALSE,"The review mentions a novel weighting scheme, indicating novelty."
"In the presence of definitive results, this might be acceptable, but in its absence, as in this paper, there needs to be quantitative analysis across multiple runs to demonstrate the robustness of the phenomenon.",Does the review address Result?,TRUE,FALSE,"The review emphasizes the need for quantitative analysis, focusing on results."
"Experiments on type predictions for TypeScript have shown better performance than the previous methods, with or without user specified types.",Does the review address Result?,TRUE,FALSE,"The review mentions the performance improvement in TypeScript predictions, which is a result of the experiments."
"However, in its current state - the comparisons made are not meaningful which makes the claim of state of the art tenuous (state of the art does not matter so much as showing that you make progress in line with the motivation).",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The review questions the meaningfulness of comparisons, addressing the motivation and justification for the claims."
The authors present a simple technique for in-context learning with large language models that achieves consistently good results across a variety of NLP tasks.,Does the review address Methodology?,TRUE,FALSE,"The review discusses the in-context learning technique as a methodology, focusing on its broad applicability across NLP tasks."
**Strengths** - The paper is generally quite clearly written and the claims are well-validated.,Does the review address Presentation?,TRUE,FALSE,"The review praises the clarity and validation of the claims, which refers to the presentation of the paper."
"I look forward to the author's discussion of the additional learnable parameters introduced in addition to CLIP's pre-trained model, and compare the number with other methods.",Does the review address Methodology?,TRUE,FALSE,"The review discusses additional learnable parameters, which relates to the methodology section."
"The authors first identify redundant structures early during training, then prune these structures, which leads to faster training.",Does the review address Methodology?,TRUE,FALSE,The review highlights the methodology involving identifying and pruning redundant structures to optimize training speed.
"I think just including two sentences that have some of these features, and that gets the point accross of ""how would we tokenize this?""",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The review suggests improving clarity in explaining the tokenization process, which ties to intuition and motivation."
"The experments are fairly convincing, although it is not entirely surprising that this approach works, and to repeat, the basic idea of extracting additional training data in this way is not entirely new.",Does the review address Methodology?,TRUE,FALSE,"The review mentions experiments and provides insight into the methodology behind extracting additional training data, though it notes the idea isn't novel."
"In the experiments, it is not reported that the learning rate or the mini-batch size is well tuned for the baseline.",Does the review address Experiment?,TRUE,FALSE,"The review points out issues with experimental setup, specifically tuning of learning rate and mini-batch size."
"In the conclusion, the paper mentions comparison with Multi-Scale approaches, but that is not present in the experiments.",Does the review address Comparison?,TRUE,FALSE,"The review highlights the missing comparison with Multi-Scale approaches, referencing a gap in the experimental setup."
"It may be useful to show how the performance changes when using different M. - According to [2], the CommonsenseQA IH-dev set contains 1,221 questions in total.",Does the review address Methodology?,TRUE,FALSE,"The review suggests exploring the effect of different values for M, which is part of the experimental methodology."
Suggested additions: * I think more specific linguistic details about Fon are missing.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review suggests providing more detailed linguistic information, which refers to further explanation or description of the Fon language."
"The main limitations seem to be: (1) the proposed method is a bit limited in that it can only be used with a corpus in which the target head, relation, and tail spans need to be directly mentioned in a single sentence (2) it’s not clear whether the quantitative improvements are due to factual knowledge in the pretrained model or the syntactic/semantic relationships encoded in the self-attention.",Does the review address Result?,TRUE,FALSE,"The review discusses the limitations of the proposed method, referring to result-related issues in terms of scope and impact of improvements."
"**Please put the verbose caption description in the main text for Figure 3, 4, 5 and Table 4** Spacing in between some of the equations can also be reduced (e.g. in latex use \vspace{-5mm} )",Does the review address Presentation?,TRUE,FALSE,The review suggests improving presentation by adjusting the figure captions and reducing spacing in equations.
_ A question to the authors: What do you attribute the loss of performance of w2gm against w2g in the analysis of SWCS?,Does the review address Result?,TRUE,FALSE,"The review raises a question regarding performance loss, which relates to analyzing experimental results."
"It'll be good to have some ablation study of the combined effect of using only one data sample in a mini-batch, and the full-explored masking.",Does the review address Data/Task?,TRUE,FALSE,"The review suggests conducting an ablation study, referencing both data and task components in the methodology."
"In addition, the notation in (6) looks wrong to me.",Does the review address Presentation?,TRUE,FALSE,"The review points out issues with the notation, which is a presentation-related concern."
"Or if they are measuring the probability assigned to the true image and not just accuracy, the name shoudl be changed from accuracy.",Does the review address Result?,TRUE,FALSE,The review questions the accuracy metric and suggests clarifying the result measurement.
The larger dataset may relate to more many-to-many relationships when training the model.,Does the review address Data/Task?,TRUE,FALSE,"The review discusses the relationship between dataset size and training dynamics, addressing data and task aspects."
The empirical analysis section answers several interesting questions.,Does the review address Analysis?,TRUE,FALSE,"The review highlights the importance of the empirical analysis, pointing out that it addresses significant research questions."
"The fact that I flustered a bit with my understanding here, was confused, and had to spend a few minutes thinking about it, means it needs a bit of tweaking.",Does the review address Presentation?,TRUE,FALSE,The review mentions confusion and suggests enhancing the presentation for clarity.
"To this end, language-specific behaviour is achieved via a combination of conditional computation to decide whether to use language-specific parameters or not and statically assigning experts for each languages.",Does the review address Presentation?,TRUE,FALSE,The review provides insights into the methodology that enables language-specific behavior via conditional computation and parameter assignment.
"However, there is no theoretical result about the effectiveness of assigning the self-knowledge distillation to label smoothing.",Does the review address Theory?,TRUE,FALSE,"The review notes the absence of theoretical justification, which is a concern related to theory."
I would shorten it and move interesting results from the Appendix to the main paper.,Does the review address Related Work?,TRUE,FALSE,"The review suggests moving important results to the main paper, which relates to related work and presentation."
"The comparison of some other important baseline is missing, such as Tip-adapter [1] and CoCoOp [2].",Does the review address Comparison?,TRUE,FALSE,"The review mentions the absence of important baseline comparisons, suggesting an area for improvement in comparison analysis."
It becomes difficult to fathom if the gains are actually due to good objective function or a case of chance for choosing better examples.,Does the review address Result?,TRUE,FALSE,"The review questions whether performance gains are due to the objective function or chance, focusing on result-related concerns."
Another limitation is that it is unclear whether the improvement would hold when the size of the model increases; the evaluation is dealing with scaling laws after all.,Does the review address Result?,TRUE,FALSE,"The review raises concerns about whether improvements will scale with larger models, referring to results."
The authors clearly present their ideas and describe the technical details.,Does the review address Presentation?,TRUE,FALSE,The review praises the clarity of the paper’s presentation and technical descriptions.
"Despite very encouraging results, several important methodological questions about the source of the efficiency gains and other aspects of the paper are left unanswered.",Does the review address Methodology?,TRUE,FALSE,"The review points out methodological questions left unanswered, referencing the need for deeper analysis."
"However, I feel that experiments can be strengthened, and notations can be improved.",Does the review address Experiment?,TRUE,FALSE,"The review highlights the need for stronger experiments and clearer notation, which pertain to experiment and presentation issues."
"- I know GPT3 access is hard to get, but I wish experiments with k=8 prompts could be compared against  Post rebuttal: I think another pass for clarity over the paper would be good for the final version, but otherwise I'm happy with the paper updates and I'm happy to see the ablation numbers aren't too sensitive to token count.",Does the review address Ablation?,TRUE,FALSE,"The review suggests further experimentation with different prompt configurations, which ties to experiment and methodology."
"Compared to Pengi, the closed-ended audio task performances are lower.",Does the review address Comparison?,TRUE,FALSE,"The review recommends improving clarity and notes satisfaction with updates, focusing on presentation and ablation study results."
"In the second phase (match), they ground facts to a knowledge graph schema by using combinations of entity linking and relation matching techniques from previous work.",Does the review address Methodology?,TRUE,FALSE,"The review compares performance with Pengi, noting a specific limitation in task results, which is a comparison-related point."
"However, \alpha could also be changed in the training process.",Does the review address Methodology?,TRUE,FALSE,"The review discusses grounding facts using knowledge graph techniques, which refers to methodology."
The proposed RoBERTa achieves/matches state-of-the-art performance on many standard NLU downstream tasks.,Does the review address Data/Task?,TRUE,FALSE,"The review mentions a potential modification in the training process (\alpha), which is a methodological suggestion."
"Questions: - According to the parameters presented in Table 8, the knowledge from LM and GNN are only fused at the last 5 layers (parameter M) when 24-layer LMs are used, and at the last 3 layers when 12-layer LMs are used.",Does the review address Methodology?,TRUE,FALSE,"The review discusses the performance of RoBERTa on NLU tasks, which refers to result-related findings."
(2) The use of super-sense annotations across multiple languages is a problem.,Does the review address Data/Task?,TRUE,FALSE,"The review references methodological details from the paper, specifically regarding layer fusion."
It is not clear to me why we cannot use HDSA+R or LARL + NLG + language model reward.,Does the review address Methodology?,TRUE,FALSE,"The review mentions the methodology and raises concerns about the approach taken, indicating it pertains to model techniques."
"To really become a benchmark to measure the progress of LFLL, more tasks/datasets will be needed.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions the need for more tasks and datasets to assess the model, indicating it relates to Data/Task."
Ablations show the necessity of applying a 2-step intermediate training scheme with mixed training followed by joint training.,Does the review address Ablation?,TRUE,FALSE,"The review specifically discusses the necessity of an ablation study, which is related to experimental analysis."
"For example, a lot of BERT-style models exploit dense interactions.",Does the review address Methodology?,TRUE,FALSE,"This is directly related to the methodology, particularly the architecture of models like BERT."
"Please don't abuse figure/table captions, whenever possible, please try to keep the description of the tables and figures in-text.",Does the review address Presentation?,TRUE,FALSE,"This is a presentation-related concern, asking for better use of captions and inline explanations for clarity."
It would helpful to place it more clearly where the contribution of the paper lies in the related work.,Does the review address Related Work?,TRUE,FALSE,"The review suggests the paper should better position its contributions in the context of existing literature, addressing Related Work."
"The same work, with a more carefully written paper, could be really great.",Does the review address Presentation?,TRUE,FALSE,This is a general comment on the presentation of the work and is related to the clarity and structure of the paper.
The research shows how low-level proof artifact data may be used to significantly boost performance on high-level theorem proving by co-training auxiliary tasks.,Does the review address Data/Task?,TRUE,FALSE,"This review point discusses the use of data to enhance model performance, relating to the Data/Task category."
It’s not easy to follow what the authors try to convey quickly at first glance.,Does the review address Presentation?,TRUE,FALSE,"This comment refers to the presentation of the paper, highlighting a need for clearer communication."
Will the attention model pays more attention to this similar sample resulting in a negative impact on the target task performance since the source task and target task are quite different?,Does the review address Methodology?,TRUE,FALSE,"This question pertains to the methodology of the model, specifically concerning task differences and attention mechanisms."
"Prior work has explored ""learning-to-share""  strategies for parameter sharing in multi-task learning (see Ruder et al., AAAI 2018), and using gating/masking to control computational paths in a differentiable way (see Fan et al., ICLR 2019, Sukhbaatar et al., ACL 2019); it is clear that the focus is NMT but it should be worth mentioning/discussing such studies to better situate the work and to help the reader assess the actual contributions.",Does the review address Contribution?,TRUE,FALSE,"This review highlights the importance of discussing related prior work and situating the current study in the context of existing literature, addressing the Contribution category."
"Strengths:  - The paper is generally well-written, with excellent motivation and empirical setup/analysis  - The overall strategy of differentiable prompt optimized to maintain fluency is reasonable and novel.",Does the review address Novelty?,TRUE,FALSE,"The review explicitly praises the novelty of the approach used in the paper, referring to Novelty."
"However, there are still major gaps between the theoretical analysis, the conclusion and the empirical solution (please see the detailed comments).",Does the review address Analysis?,TRUE,FALSE,"This comment critiques the alignment between theory and empirical results, which points to the need for better theoretical analysis, related to Analysis."
"We expect that the model can not only achieve good performance on a single dataset, but also have the potential to transfer beyond a single dataset.",Does the review address Result?,TRUE,FALSE,"This expectation is related to the potential generalization of the model, and thus it touches upon Results."
"That raises the question -- Gerrish and O'Connor both conduct evaluations with an external database of country relations developed in political science (""MID"", military interstate disputes).",Does the review address Data/Task?,TRUE,FALSE,"This question mentions an external dataset and evaluation, which falls under Data/Task."
"- Strengths: This paper has high originality, proposing a fundamentally different way of predicting words from a vocabulary that is more efficient than a softmax layer and has comparable performance on NMT.",Does the review address Result?,TRUE,FALSE,"This review recognizes the original contribution, highlighting the efficiency of the proposed method, addressing Result."
"Instead, they turn to rely on the abundant textual and behavioral information of the existing reviewer to augment the information of a new user.",Does the review address Methodology?,TRUE,FALSE,"This comment refers to the methodology used in augmenting information from reviewers, which relates to how the model processes input data."
This would be much more useful for other researchers as it is the file format used by UD.,Does the review address Data/Task?,TRUE,FALSE,"This refers to the data format, suggesting a more accessible file format for broader use, addressing Data/Task."
"The selected student networks, VL-BERT, UNITER, VILLA, while they are great and highly reputable works in the community, their performance is not as competitive as for today.",Does the review address Presentation?,TRUE,FALSE,"This compares the performance of selected networks, focusing on Data/Task."
How many include simple string operations and/or other simple method calls as implied by Table 2?,Does the review address Presentation?,TRUE,FALSE,"This concerns the presentation of the paper, specifically questioning the clarity of explanations related to string operations in the models."
"Overview: This paper discusses the problems of common tokenization strategies for low resource african languages, and proposes a new tokenization method to overcome these problems.",Does the review address Methodology?,TRUE,FALSE,This is a methodology-related point focusing on the tokenization approach proposed.
"The techniques used in the paper (multi-branch transformer, pointing mechanism, cross-modal attention, global positional encodings, etc) have been shown to work in the past for image-text tasks [1, 2].",Does the review address Data/Task?,TRUE,FALSE,"This is a reference to established methodologies used in the paper, particularly concerning Data/Task."
Contributions of the paper don't seen particularly novel.,Does the review address Contribution?,TRUE,FALSE,"This comment addresses the novelty of the contributions, pointing out a lack of distinctiveness."
"Overall, I like the idea of the paper: it is important to reduce the parameters needed for sets of tasks, to enable NLP models to be deployed in a larger variety of settings, and reducing the amount of data needed.",Does the review address Significance?,TRUE,FALSE,"This reflects on the significance of the paper’s contributions, focusing on its broader impact, addressing Significance."
"If so, does the global node $V_g$ connect to all the sub fact nodes?",Does the review address Methodology?,TRUE,FALSE,"This question relates to the methodology of the model, asking about the network architecture."
"I agree with all of your points about what is lacking, but in my mind, the novelty was enough to still give a 7.",Does the review address Novelty?,TRUE,FALSE,"The reviewer acknowledges the novelty of the approach, mentioning Novelty."
- The data preprocessing and training steps are complex.,Does the review address Data/Task?,TRUE,FALSE,"This comment mentions the complexity of the data processing and training steps, linking to Data/Task."
"The paper shows the reasonable claim that it is necessary to gradually train the model from close-ended datasets to open-ended ones because if the open-ended dataset is trained first, the model is heavily dependent on language capability so it is hard to train the audio representation.",Does the review address Methodology?,TRUE,FALSE,"This point relates to the methodology behind the training process, specifically addressing how datasets are used in the process."
- Slight improvements over CaP via the different prompting method.,Does the review address Result?,TRUE,FALSE,"This is a result-oriented comment, mentioning a slight improvement with the new method, addressing Results."
"Regarding preposition phrases as a proxy for complexity: since the hypothesis is that the more the number of prepositional phrases in a question, the harder it is to answer.",Does the review address Methodology?,TRUE,FALSE,This is a methodological consideration related to task difficulty in NLP.
it's very clearly presented -- I like cross-referencing the models with the diagrams in Table 2.,Does the review address Presentation?,TRUE,FALSE,"This is a positive comment about the paper’s presentation, specifically the clarity of figures and cross-referencing."
"Experiments show that the suggested tuning of inference hyperparameters can bring improvements to LM tasks, which is convincing.",Does the review address Methodology?,TRUE,FALSE,"This comment refers to experimental results demonstrating the effectiveness of hyperparameter tuning, related to Methodology."
The proposed model was evaluated on two publicly-available dataset and achieved reasonable results.,Does the review address Methodology?,TRUE,FALSE,"This refers to the model evaluation and results achieved on publicly available datasets, falling under Methodology."
"The studies here show that, pre-training with Inverse Cloze Task (ICT) the two-tower Transformer models significantly outperform the widely used BM-25 algorithm for large-scale information retrieval.",Does the review address Data/Task?,TRUE,FALSE,"This is related to Data/Task, as it compares model performance on a large-scale information retrieval task."
"The paper only performs some finetuning on GLUE tasks, which is significantly less interesting given that it is comparatively cheap and FP8 speedups thus not so crucial while, in many cases, even more affordable finetuning techniques like QLoRA also work well.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions that the evaluation on GLUE tasks is less interesting, touching on the data/task focus of the study."
It is a bit hard to identify the interestingness or novelty in the approach.,Does the review address Methodology?,TRUE,FALSE,"This is a methodology critique, questioning the novelty and interest of the approach used in the paper."
"Page 5, Equations (6, 7, 8): should e^{l}_{s} and e^{l}_{j} be e^{(l-1)}_{s} and e^{(l-1)}_{j} respectively ?",Does the review address Presentation?,TRUE,FALSE,"This refers to a specific issue in the presentation of the paper, asking for clarification on notation used in equations."
* Ablation: the model vs corpus transfer comparison seems unfair to me.,Does the review address Comparison?,TRUE,FALSE,"The review points out an issue with the fairness of the ablation study, which is related to the comparison between models and datasets."
"For instance: Roee Aharoni, Melvin Johnson, and Orhan Firat.",Does the review address Related Work?,TRUE,FALSE,"This is a reference to related work, indicating the need to acknowledge prior research in the relevant domain."
"After reading other reviews and the authors’ responses to all of the reviewers, I recommend this paper by accepted—extensive results show that the CALM objectives offer more signal from data than current pretraining methods.",Does the review address Result?,TRUE,FALSE,"The review praises the paper's results and supports its acceptance, touching on the significance and results."
"As such, in my opinion this is unquestionably an important subtopic for the field of machine programming and the authors approach also seems satisfactory to me for ICLR (described below).",Does the review address Presentation?,TRUE,FALSE,"This is a positive evaluation of the significance and relevance of the paper in the context of machine programming, falling under Presentation and Significance."
This is very promising to simplify the construction of highly tuned task-specific neural pipelines and improve the multimodal and multi-task problems.,Does the review address Data/Task?,TRUE,FALSE,"This is a comment on the potential impact of the paper on task-specific neural pipelines, touching on the Data/Task section."
"It is hard to tell what are the standalone contributions of the paper, and what is coming from other works.",Does the review address Novelty?,TRUE,FALSE,"The review discusses the unclear novelty of the contributions, pointing to the distinction between original work and previous methods, related to Novelty."
"The authors say ""the axes in the plots are the number of training steps finished.""",Does the review address Presentation?,TRUE,FALSE,This is a presentation-related clarification about the labeling of the axes in the plots.
"The input would be the source sentence with its appropriate tokenization, no?",Does the review address Presentation?,TRUE,FALSE,"This question addresses a methodological aspect, specifically focusing on input processing in the paper."
- The evaluation on PTB (table 2) isn't a fair one since the model was trained on a larger corpus (FBIS) and then tested on PTB.,Does the review address Methodology?,TRUE,FALSE,"This is a methodological critique of the evaluation setup, highlighting a potential issue with the fairness of the data/task evaluation."
I think significant presentation changes are required to clarify that the paper focuses on inference and finetuning.,Does the review address Methodology?,TRUE,FALSE,"This review comment suggests significant changes to the paper's presentation to clarify its focus on inference and fine-tuning, related to Methodology and Presentation."
"Experiments show that the suggested tuning of inference hyperparameters can bring improvements to LM tasks, which is convincing.",Does the review address Result?,TRUE,FALSE,"This refers to the experiments and their results, focusing on improvements to language model tasks, relating to Results and Methodology."
I would like the authors to have a more extended discussion of how SCS can be used outside of their work.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The reviewer asks for a more detailed discussion on the broader applications of the method, relating to Definition/Description/Detail/Discussion."
The methods is evaluated on 5 standard NER+RE datasets with good performances.,Does the review address Evaluation?,TRUE,FALSE,"The review mentions the method's evaluation on multiple datasets, addressing Evaluation and Data/Task."
"The paper is clear and detailed, and well situated in the literature.",Does the review address Related Work?,TRUE,FALSE,"This comment praises the clarity of the presentation and the proper positioning of the work in the related literature, addressing Related Work and Presentation."
"How does the method work if the candidate with the highest score is always picked: in the end, this is what the model is supposed to learn, correct?",Does the review address Methodology?,TRUE,FALSE,"This question concerns the methodology of the model, questioning how the method works in specific conditions, like selecting the highest score candidate."
Nit: I would have tried to move the (datasets per cluster/templates per dataset) ablation to the main body as well and shortened Section 3  - The 4.2 (scaling laws) ablation is perhaps the most interesting of all.,Does the review address Data/Task?,TRUE,FALSE,"This suggests a revision in the paper's structure, focusing on the placement of the ablation study, relating to Data/Task."
"Furthermore, the authors are neglecting parameter efficient fine-tuning baselines, for instance like [1].",Does the review address Methodology?,TRUE,FALSE,"The review criticizes the lack of consideration for parameter-efficient fine-tuning baselines, addressing Methodology."
It is not clear how the proposed method considers the correlations among the retrieved data points.,Does the review address Data/Task?,TRUE,FALSE,"This question pertains to how the methodology handles data correlations, touching on Data/Task."
I can not understand why sample-specific rather than task-specific preference is important for prompt tuning.,Does the review address Data/Task?,TRUE,FALSE,This question asks for clarification on the methodology regarding the preference for sample-specific tuning.
"If the authors would like to compare the number of additional parameters of DeFo with CoOp and CLIP-adapter, I think it may be very helpful for us to comprehensively evaluate and compare these methods.",Does the review address Comparison?,TRUE,FALSE,"This suggestion is related to comparison studies between models and methods, addressing Comparison."
"Finally, the CALM intermediate objectives share many properties with all of the datasets tested on and are likely calibrating the model to the kind of correlations they should expect to predict in advance of finetuning.",Does the review address Data/Task?,TRUE,FALSE,"The review points out the relationship between CALM objectives and datasets, which addresses Data/Task."
* I suggest using the same x-axis scale on the two charts in Figure 3 to avoid confusion about the magnitudes of the differences.,Does the review address Presentation?,TRUE,FALSE,"This is a presentation suggestion about the use of consistent scales in graphs, related to Presentation."
- The number of tasks and domains is minimal setting.,Does the review address Data/Task?,TRUE,FALSE,"This comment addresses the limited scope of the tasks and domains used in the study, related to Data/Task."
"- unconstrained, multi-concept:     This needs a direct comparison to a traditional discrete-channel referential game.",Does the review address Comparison?,TRUE,FALSE,"This review suggests a comparison to traditional methods, addressing the Comparison section."
"For example, the sentiment lexicon is not explained for the SVM.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This critique asks for clarification of the sentiment lexicon used, related to Definition/Description/Detail/Discussion/Explanation/Interpretation."
"Granted, the final effect of MTL depends on task similarities, but that's probably the same for the proposed approach.",Does the review address Methodology?,TRUE,FALSE,"This comment questions the methodology, specifically in terms of multi-task learning (MTL) and task similarities."
The paper features extensive experiments that convincingly validate the effectiveness of the proposed method.,Does the review address Experiment?,TRUE,FALSE,"This comment praises the extensive experiments conducted, which support the method’s effectiveness, touching on Experiment and Result."
- The captions are too small to read in Figure 2 & 3.,Does the review address Presentation?,TRUE,FALSE,"This comment addresses a presentation issue related to the readability of figure captions, referring to Presentation."
"## ""General Learner"": Missing Formal Definition and Misleading Name  The ""general learner"" concept used in the title and throughout is named in a somewhat misleading way, as the results here have to do more with *expressive power* than learning.",Does the review address Presentation?,TRUE,FALSE,"This comment critiques the terminology used in the paper, specifically questioning the definition and naming of the ""general learner,"" addressing Presentation."
"Existing methods for contact prediction (beyond Gremlin), however, are not described sufficiently.",Does the review address Methodology?,TRUE,FALSE,"This comment points to the insufficient discussion of existing methods, addressing Methodology."
It might be better to give out the trend of training loss and validation loss.,Does the review address Methodology?,TRUE,FALSE,"This suggests including more detailed results, specifically about training and validation loss trends, related to Methodology."
"Although I understand the experiment setup, missing reference to more recent VL works prevent readers from getting a good research landscape in the multimodal pre-training.",Does the review address Related Work?,TRUE,FALSE,This comment highlights the lack of reference to recent works
"- Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train.",Does the review address Contribution?,TRUE,FALSE,"The review discusses the contributions and the importance of an ablation study, which falls under the Contribution section."
"Once more baselines are included, it is very possible that the performance will be surpassed.",Does the review address Result?,TRUE,FALSE,"This comment addresses the Result section, suggesting that the results may be surpassed with more baselines."
"- Consider HellaSwag/PiQA/etc, where FLAN underperformed few-shot and even zero-shot.",Does the review address Result?,TRUE,FALSE,"This critique addresses the Result section, pointing out areas where the proposed method underperforms compared to others."
Comments below are ranked by decreasing importance.,Does the review address Significance?,TRUE,FALSE,"This is a general statement about the significance of the comments, related to Significance."
"Also, please show the performance trends based on different augmentation sizes.",Does the review address Result?,TRUE,FALSE,"The review requests additional performance data with respect to augmentation sizes, falling under Result."
"Comparing the proposed method to earlier approaches such as PaLI, CoCa, and Flamingo may not be entirely fair.",Does the review address Methodology?,TRUE,FALSE,"This comment addresses the Methodology, suggesting that comparisons to these methods may not be fair."
"However, the dataset with 400M may contain too many many-to-many relationships like 7 or 8 (maybe more, the data scale is about 100 times the used datasets in this paper).",Does the review address Data/Task?,TRUE,FALSE,"This comment pertains to the Data/Task, highlighting potential issues with the dataset's scale."
"Strengths: - Thorough theoretical analysis that reveals the connection between (practically-necessary) small learning rates and inability to use dependencies across text chunks - Useful framing and discussion of the ""in-context bias"", where models are more likely to learn dependencies within text chunks seen during pre-training.",Does the review address Theory?,TRUE,FALSE,"This is a praise for the Theory section, discussing the theoretical insights provided in the paper."
"As pointed out by the authors in section 3.3.1 ""TRIGGER IN 'INPUT' KEY,"" decorations can utilize specific keywords or phrases that are rare in regular instructions.",Does the review address Methodology?,TRUE,FALSE,"This refers to Methodology, specifically the use of specific keywords and decorations as part of the approach."
"Elaboration on Theorem 1, with an intuitive breakdown of its implications, would significantly enhance the readability and credibility of the results.",Does the review address Theory?,TRUE,FALSE,This is a suggestion to improve Theory by elaborating on the implications of Theorem 1.
"For one, it would require doing some experiments with trained LMs and finding evidence of memorization.",Does the review address Experiment?,TRUE,FALSE,"This comment addresses Experiment, suggesting further experiments on memorization with trained language models."
"Since the architecture (ignoring the compression) is similar to multi-scale approaches, it would be good to compare against empirically.",Does the review address Methodology?,TRUE,FALSE,"This pertains to Methodology, recommending empirical comparisons with similar multi-scale architectures."
"- Excellent clarity and presentation of ideas  ## Weaknesses - The experiments provided do not analyze the unique characteristics of the environment introduced; instead, the experiments are similar to the typical gamut for a discrete symbol-based referential game.",Does the review address Experiment?,TRUE,FALSE,"This critique falls under Experiment, noting that the experiments are generic and don't focus on the unique aspects of the environment."
The curriculum training is yet another aspect that makes this effort worthy as it shows that a brute force approach to just wrap in all possible audio-text paired data may not be as good overall.,Does the review address Significance?,TRUE,FALSE,"This comment is related to Significance, highlighting the relevance of curriculum training."
"The theoretical results on the Parity/Sum task reply to some strong assumptions: bilinear parameterization, some initialization (for example, v = 0).",Does the review address Theory?,TRUE,FALSE,"This discusses Theory, critiquing some strong assumptions in the theoretical analysis."
This paper focuses on improving the dialogue policy together with the responses by utilizing a pre-trained language model and offline RL.,Does the review address Methodology?,TRUE,FALSE,"This is a comment on Methodology, describing the paper's focus on dialogue policy improvement."
Experimental results show improvements over both the base T5 model and the large T5 model.,Does the review address Result?,TRUE,FALSE,"This refers to the Result section, discussing improvements over base models."
The applicability and the novelty of the SCS representation seem limited.,Does the review address Novelty?,TRUE,FALSE,"This is a critique related to Novelty, suggesting that the SCS representation has limited applicability."
"Additionally, there's some limitations to the way the language model is being leveraged and the types of knowledge it can extract.",Does the review address Methodology?,TRUE,FALSE,"This critique falls under Methodology, addressing limitations in the usage of the language model."
"Even though the results don’t show that the proposed loss function and proposed “conditional mean features” give improvements over baselines, the empirical results show that the basic assumptions and definitions in the theoretical analysis are relatively realistic.",Does the review address Theory?,TRUE,FALSE,"This relates to Theory, acknowledging that the theoretical assumptions were realistic, despite empirical shortcomings."
"It is not the case that I find the experimental results inadequate, rather, the experiments run in the first place are generic and do not illustrate the points of interest with a continuous-channel referential game.",Does the review address Experiment?,TRUE,FALSE,"This comment critiques the Experiment, noting that the experiments are too generic."
The fact that the previous study reported a 126 perplexity baseline using LSTM and the LSTM's perplexity of 106.9 provided by the author showed that the FBIS gives an advantage to computing the language model's perplexity when tested on PTB.,Does the review address Evaluation?,TRUE,FALSE,"This discusses Evaluation, focusing on how FBIS influences the language model’s perplexity score."
The system performs on par with recently proposed GECA for SCAN and favorably to GECA on morphological analysis.,Does the review address Analysis?,TRUE,FALSE,"This comment pertains to Analysis, comparing the system's performance with the GECA method."
I would recommend the authors to at least assume the availability of some public data that is kept out of training and evaluations and to run all the baselines fairly in this setting.,Does the review address Data/Task?,TRUE,FALSE,"This is a suggestion under Data/Task, advocating for fair baseline comparisons using public data."
"Furthermore, an explicit definition of the feature matrix X of the nodes would help in understanding how these features interact with the quantum-inspired positional encodings.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This comment refers to Definition/Description/Detail/Discussion/Explanation/Interpretation, suggesting more clarity in the feature matrix."
"There is no comparison provided with any baseline for 2/5 tasks (Language grounding, Tappability) 5.",Does the review address Comparison?,TRUE,FALSE,"This critique relates to Comparison, pointing out the lack of baseline comparison in certain tasks."
Strengths:   - Combining lifelong and few-shot learning is a new setting.,Does the review address Novelty?,TRUE,FALSE,This praise is directed toward the Novelty of combining lifelong and few-shot learning.
"Moreover, the authors better answer questions in 2 so I can gauge if their hyper-parameters were chosen in the principled ways.",Does the review address Presentation?,TRUE,FALSE,"This falls under Presentation, suggesting that the authors clarify their choices of hyperparameters."
"As pointed by one public comment, the ablation study should show how much improvement is from BERT vectors.",Does the review address Ablation?,TRUE,FALSE,"This critique concerns Ablation, suggesting that the study should clarify the contribution of BERT vectors."
"While I understand this is contemporaneous work, but since the work is so relevant to this paper and seems to directly contradict the premise of this paper, it might be good to have a short discussion on this (just a suggestion).",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"This suggestion pertains to Definition/Description/Detail/Discussion/Explanation/Interpretation, recommending a discussion on contradicting works."
"The experiments in the paper demonstrated the superiority of adding adversarial training to a self-supervision framework, in that significant improvements can be obtained for similar-sized networks.",Does the review address Experiment?,TRUE,FALSE,"This refers to Experiment, discussing the superiority of adversarial training in the experiments."
"The paper appraisal therefore rests on the clarity of presentation, how convincing the experiments are, and how reproducible.",Does the review address Presentation?,TRUE,FALSE,"This comment is related to Presentation, discussing the importance of clarity and reproducibility in the paper."
"Put in another way, using RFA in transformer is from Rawat et al., 19 so do you think your major contribution is to design such  a gated usage of RFA?",Does the review address Contribution?,TRUE,FALSE,"This question addresses Contribution, questioning the originality of the gated usage of RFA in transformers."
"Therefore, the dynamic \alpha by hyperparameter searching should also be added as a base.",Does the review address Methodology?,TRUE,FALSE,"This comment suggests an improvement to the Methodology, recommending dynamic hyperparameter searching."
Keyphrase extraction using deep recurrent neural networks on Twitter.,Does the review address Related Work?,TRUE,FALSE,"This is a reference to Related Work, mentioning keyphrase extraction using deep recurrent networks on Twitter."
"However, the designed specific neural network does not support the claimed binding-unbinding theory very well.",Does the review address Theory?,TRUE,FALSE,"This critique concerns Theory, pointing out the failure of the designed neural network to support the binding-unbinding theory."
"For example, the full model of wMAN works better than FBW on R@1, but worse on R@5 and R@10.",Does the review address Comparison?,TRUE,FALSE,The review compares the performance of two models (wMAN vs. FBW) at different recall rates.
The idea that combining the output of several models using the attention strategy is not novel in deep learning.,Does the review address Methodology?,TRUE,FALSE,The review comments on the novelty (or lack thereof) of using attention strategy to combine models.
- The experimental results are thorough and solid.,Does the review address Experiment?,TRUE,FALSE,The review praises the thoroughness and solidity of the experimental results.
"Therefore, it's not ideal that the uncertainty sampling algorithm has been moved to the appendix.",Does the review address Methodology?,TRUE,FALSE,The review critiques the placement of the uncertainty sampling algorithm in the appendix.
One of the important motivations of multi-modal multi-task learning mentioned was to achieve better or on-par performance with a single model (and supposedly fewer computations) which is crucial for devices with limited computing resources.,Does the review address Result?,TRUE,FALSE,Discusses the motivation behind using multi-modal multi-task learning to achieve better performance and reduce computation.
"However, I am unable to grasp nuances, leaving important questions untouched such as: In what scenarios do we expect the model to perform better than GECA?",Does the review address Result?,TRUE,FALSE,Raises concerns about the model’s performance and its comparison with GECA.
"**Major concern** If I understand correctly (and please correct me if I am wrong), in Theorem B.1, the ratio between the downstream error $\ell_\mathcal{T}(\\{p_{\cdot\mid s}\\}) - \tau$ and the pre-training error $\ell_\text{xent}(\\{p_{\cdot\mid s}\\})-\ell_\text{xent}^\ast$ is _hidden_ in the $\gamma(p_{\mathcal{T}}; \\{p_{\cdot\mid s}\\})$ coefficient.",Does the review address Theory?,TRUE,FALSE,Raises a concern about the formulation and interpretation of the ratio in Theorem B.1.
"While this paper provides extensive empirical results and quantitively demonstrates the effectiveness of RandomMask, there are several areas where it could be further enhanced.",Does the review address Result?,TRUE,FALSE,Acknowledges the empirical results but suggests there is room for enhancement.
"In the conclusion, the paper mentions comparison with Multi-Scale approaches, but that is not present in the experiments.",Does the review address Experiment?,TRUE,FALSE,"Notes that while the paper mentions comparison with Multi-Scale approaches, the experiments do not reflect this."
"The in-depth experimental analysis of the BERT pretraining process in this paper answers many open questions (e.g., the usefulness of NSP objective) and also provide some guidance in how to effectively tweak the performance of pretrained model (e.g., large batch size).",Does the review address Experiment?,TRUE,FALSE,Highlights the in-depth analysis and how it addresses open questions about BERT pretraining.
"(4) In general, the results in table 3 do not tell a consistent story.",Does the review address Result?,TRUE,FALSE,Comments that the results in Table 3 lack consistency.
"## Paper weaknesses and questions  **Code comment analysis**  Some programmers like to write comments, while some are not.",Does the review address Analysis?,TRUE,FALSE,"Briefly mentions the issue of code comment analysis, which falls under the analysis of programming practices."
"Mainly, for most of the intrinsic metrics, the multilingual embedding techniques do not seem to perform the best.",Does the review address Evaluation?,TRUE,FALSE,"Evaluates the performance of multilingual embedding techniques, stating that they do not perform the best on metrics."
"- If not, please remove the attention layer after the encoder in figure 5.",Does the review address Methodology?,TRUE,FALSE,"Suggests changes to the model architecture, specifically regarding the attention layer."
"Do we need better model design, or more data and computations?",Does the review address Methodology?,TRUE,FALSE,"Questions whether improvements should come from model design, more data, or increased computation."
What is the result if we directly learn to match input and logic forms?,Does the review address Methodology?,TRUE,FALSE,Asks a question about exploring different methods for matching inputs and logic forms.
"Furthermore, when there is a gap between the empirical results and the theoretical results (e.g., validation of Lemma 4.3 at the end of Section 4), the paper makes these limitations clear, which I appreciated very much as a reader (paper does not over-claim its contributions).",Does the review address Result?,TRUE,FALSE,Appreciates the transparency of the authors regarding the gap between empirical and theoretical results.
3) The ablation study and visualization analysis of the experimental results are sufficient.,Does the review address Result?,TRUE,FALSE,Considers the ablation study and visualization to be sufficient in the experimental analysis.
"- Strengths: Useful modeling contribution, and potentially useful annotated data, for an important problem -- event extraction for the relationships between countries as expressed in news text.",Does the review address Data/Task?,TRUE,FALSE,Praises the modeling contribution and the use of annotated data for event extraction in the task of relationship extraction.
"Perhaps an entropy-regularized setup is a useful comparison to show that it provides marginal benefit over the setup studied, and this might resolve the lack of clarity around the implications of the claims made from the first set of experiments.",Does the review address Presentation?,TRUE,FALSE,Suggests an alternative setup for comparison to improve the clarity of the paper’s claims.
Pros: - Weakly-supervised method for video moment localization is a reasonable and important direction.,Does the review address Significance?,TRUE,FALSE,Acknowledges the significance of weakly-supervised methods for video moment localization.
Weaknesses - The writing and the paper organization can be improved.,Does the review address Presentation?,TRUE,FALSE,Suggests improvements in writing and organization of the paper.
"1 ""Rule based approaches may seem outdated in contrast to statistical or neural methods.",Does the review address Methodology?,TRUE,FALSE,Mentions that rule-based approaches may appear outdated compared to newer methods like statistical or neural models.
"It describes a mapping of ORCHID, a Thai-specific POS tagset, to the Universal Dependencies (UD) scheme, and evaluates various state-of-the-art POS taggers on this scheme.",Does the review address Evaluation?,TRUE,FALSE,Discusses the evaluation of POS taggers on the ORCHID scheme.
It also includes the recurrent memory extension from Transformer-XL from Dai et al.,Does the review address Methodology?,TRUE,FALSE,Notes the inclusion of the Transformer-XL recurrent memory extension in the methodology.
"On the other hand, the symbolic logic rules are able to express global patterns.",Does the review address Comparison?,TRUE,FALSE,"Compares the symbolic logic rules to other methods, noting their ability to express global patterns."
Contributions of the paper don't seen particularly novel.,Does the review address Novelty?,TRUE,FALSE,Critiques the novelty of the paper’s contributions.
"Thus, while concatenating the audio feature and the text feature can introduce desired performance, there could be some advancements not just combining pretrained audio model and LLM.",Does the review address Result?,TRUE,FALSE,Discusses potential improvements beyond just combining pretrained audio and language models.
It would helpful to place it more clearly where the contribution of the paper lies in the related work.,Does the review address Contribution?,TRUE,FALSE,Suggests clarifying the paper's contribution in relation to the related work.
"However, there is no theoretical result about the effectiveness of assigning the self-knowledge distillation to label smoothing.",Does the review address Result?,TRUE,FALSE,Points out the lack of theoretical results on self-knowledge distillation in the paper.
* Can we get any information about how the annotators were trained?,Does the review address Methodology?,TRUE,FALSE,Requests information on how annotators were trained for the task.
"It would be great to define and identify beyond current close-ended tasks with new lower level tasks which really require using the audio, such as counting sound events, ordering of events, etc.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,Suggests expanding task definitions to include more complex tasks.
"- wMAN is evaluated with two publicly available datasets, and is compared with state-of-the-art methods and other ""oracle"" baselines.",Does the review address Data/Task?,TRUE,FALSE,Mentions the datasets and baselines used for evaluating wMAN.
The ablations cannot serve the topic of this paper well.,Does the review address Ablation?,TRUE,FALSE,Critiques the relevance and utility of the ablation study.
But a direct head to head comparison for the computational cost of a multi-task model and individual models is not provided.,Does the review address Comparison?,TRUE,FALSE,Notes the absence of a direct comparison of computational costs between multi-task and individual models.
* Authors reproduced results from their strongest baseline.,Does the review address Result?,TRUE,FALSE,Points out that the authors reproduced results from their strongest baseline.
The paper is taking all the lessons from past works and applying it to a new domain.,Does the review address Methodology?,TRUE,FALSE,Notes that the paper applies previous lessons to a new domain.
"A significant part of the contribution was in the analysis of the results, obtained by this learning-based parameter sharing approach, which was quite informative and revealed some interesting insights about where and when a language-specific computation is required.",Does the review address Analysis?,TRUE,FALSE,Appreciates the analysis of the results and insights from parameter sharing in the model.
"2. the FlexGen proposed in (Sheng et al., 2023) have showed -> has shown 3.",Does the review address Presentation?,TRUE,FALSE,Comments on a grammatical error in the presentation (tense issue).
The proposed distillation loss is standard and by itself is not technically new.,Does the review address Novelty?,TRUE,FALSE,Comments that the distillation loss is not a new contribution in itself.
"Strengths: The paper presents an interesting idea for Multiple-Choice Question-Answering (using the answer symbol instead of the answer itself), motivates the idea well and does a thorough analysis over multiple datasets, and LLMs to analyze its performance in different settings (including few-shot settings).",Does the review address Methodology?,TRUE,FALSE,"The review mentions the methodology, particularly the analysis over multiple datasets and LLMs."
"3.The model excels in various audio-related tasks and open-ended question answering, demonstrating its outstanding performance.",Does the review address Methodology?,TRUE,FALSE,"The review mentions the methodology, focusing on the model’s performance in audio-related tasks and open-ended QA."
"Additionally, the rationale for choosing (Ma et al., 2023) as a benchmark, along with the significance of the improvements observed, even if minute, should be clearly articulated.",Does the review address Presentation?,TRUE,FALSE,"The review mentions the presentation, particularly how benchmarks and improvements should be clarified."
- Discussion in Section 4.1 - I think Figure 4 should be explained in more detail (in caption and/or text).,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions definition/description/detail/discussion/explanation/interpretation, calling for more clarity on Figure 4."
Such choice and associated thresholds seem arbitrary: how were they actually found out?,Does the review address Experiment?,TRUE,FALSE,"The review mentions the experiment, questioning how the thresholds were determined in the experiment."
The same comment is applicable to oracle database (DB) results.,Does the review address Presentation?,TRUE,FALSE,"The review mentions presentation, particularly regarding the oracle database results."
It will be good to rewrite highlighting the contributions.,Does the review address Presentation?,TRUE,FALSE,"The review mentions presentation, suggesting that the contributions be better highlighted."
"Experiments show that the induced ""best-first"" order outperforms fixed orders, which verifies the motivation of the paper` 4.",Does the review address Result?,TRUE,FALSE,"The review mentions the result, focusing on the experimental comparison of different orders."
Theoretical analysis is provided to demonstrate the effectiveness of the proposed method under a greedy search algorithm.,Does the review address Presentation?,TRUE,FALSE,"The review mentions presentation, specifically pointing out the theoretical analysis of the proposed method."
"In its current form, the two arguments (input query and output sequence translated from a logic form) are interchangeable.",Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, pointing out an issue with how the arguments are used in the model."
The findings from the analysis are an important addition to the understanding of the role of language specific parameters in multilingual NMT.,Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, focusing on the analysis of language-specific parameters in multilingual NMT."
"I recommend this paper for acceptance, though I encourage the authors to revise their paper to make this the focus of the story, rather than the vaguely defined notion of “concept”.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions definition/description/detail/discussion/explanation/interpretation, calling for clearer focus in the paper."
The main weakness of the paper is that it is mainly based on further tuning the existing BERT model and lacks novel contribution in model architecture.,Does the review address Novelty?,TRUE,FALSE,"The review mentions novelty, critiquing the paper for lacking novelty in model architecture."
"Therefore, the training is stable and guarantee to converge.",Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, emphasizing the stability and convergence of the training process."
Strengths: The paper is well-written with clear motivations and structure.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The review mentions intuition/justification/motivation/validation, highlighting the clarity and structure of the paper."
"- The authors have evaluated their models on multiple settings, proving that their models trained on Wikipedia can potentially generalize to multiple downstream tasks.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions data/task, noting the generalizability of models trained on Wikipedia."
"However, there are insufficient experiments and comparison to previous work to convince me that the paper’s contributions are novel and impactful.",Does the review address Contribution?,TRUE,FALSE,"The review mentions contribution, raising concerns about the novelty and impact of the paper’s contributions."
I also encourage the authors to simplify the experiment described in section 3.1 to make it more clear.,Does the review address Presentation?,TRUE,FALSE,"The review mentions presentation, suggesting a clearer explanation of the experiment."
"The motivation is to leverage unimodal data, which are assumed easier to obtain than image-text pairs.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions data/task, discussing the motivation to use unimodal data."
"One novel finding of this paper is that the distribution of the next word, conditional on the context, can provide a strong discriminative signal for the downstream task.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions data/task, highlighting a key novel finding related to next-word prediction."
"A lot of attention/space is dedicated to locality and symmetry properties of positional encodings, which from my understanding, isn’t very novel and has been explored in previous work.",Does the review address Novelty?,TRUE,FALSE,"The review mentions novelty, critiquing the originality of the focus on locality and symmetry properties."
The empirical study that compares the prediction ensemble with the prompt ensemble is quite interesting and can inspire many related fields.,Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, appreciating the empirical study comparing prediction and prompt ensembles."
About experiments: 1) I think one ablation study I’m most interested in is to simply run GNN on the AST (or simply use Allamanis et.al’s method).,Does the review address Ablation?,TRUE,FALSE,"The review mentions ablation, expressing interest in a potential ablation study with GNN on the AST."
"I appreciate that the authors do not read too much into it and focus more on the analysis of the results, but one thing that remains unanswered in this paper is how the proposed method fairs against multilingual baselines that utilize (roughly) the same number of parameters; currently, the best models outperform the LS baseline by ~28M and ~10M parameters on OPUS-100 and WMT-14 respectively.",Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, suggesting a comparison with multilingual baselines."
W: - Human evaluation is missing and could add more insights to the interactive process.,Does the review address Evaluation?,TRUE,FALSE,"The review mentions evaluation, highlighting the absence of human evaluation and suggesting its inclusion."
Could you explain more precisely what exactly is new?,Does the review address Novelty?,TRUE,FALSE,"The review mentions novelty, asking for more clarity about what is new in the paper."
"Theoretically, they showed that synchronized updates to the low-level and high-level policy may never converge, yet asynchronized updates guarantees convergence.",Does the review address Theory?,TRUE,FALSE,"The review mentions theory, discussing the theoretical analysis on policy updates."
This paper is an interesting application of a data augmentation or self-supervised learning type of approach for tactic based theorem proving.,Does the review address Data/Task?,TRUE,FALSE,"The review mentions data/task, noting the use of a data augmentation/self-supervised approach for theorem proving."
Authors also used a discriminator reward signal to cope with sparse reward (dialog success rate) and better representation of the human evaluation.,Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, focusing on the use of a discriminator reward signal in the model."
"I also appreciate that due care has been taken to present the work as understanding a phenomenon, to avoid any misconceptions about a new method being proposed.",Does the review address Presentation?,TRUE,FALSE,"The review mentions presentation, appreciating the care taken to avoid misconceptions about the new method."
- The experimental results are thorough and solid.,Does the review address Result?,TRUE,FALSE,"The review mentions result, praising the thoroughness and solidity of the experimental results."
"Therefore, it is necessary to compare OTTER and CLIP on the same-scaled datasets.",Does the review address Comparison?,TRUE,FALSE,"The review mentions comparison, suggesting that OTTER and CLIP should be compared on the same datasets."
"For the transferability coefficient proposed in Section 5.1, is it possible to measure it in experiments?",Does the review address Experiment?,TRUE,FALSE,"The review mentions experiment, questioning whether the transferability coefficient can be measured experimentally."
"The annotations from [1] are very simple and short, only including some easy examples as in-context examples.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions data/task, commenting on the simplicity of the annotations used in the experiments."
"The main limitation to me is that, the two novel pre-training tasks proposed in this paper are specific for Wikipedia and they are less effective than the ICT strategy (as shown in Table 5).",Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, focusing on the limitations of the pre-training tasks for Wikipedia."
It shows that BERT was significantly undertrained and propose an improved training recipe called RoBERTa.,Does the review address Methodology?,TRUE,FALSE,"The review mentions methodology, describing the improvements in BERT’s training with RoBERTa."
"There are several follow-up results built on these two results, such as a new loss objective for predicting the downstream task, but to the best of my understanding, these two results are the main claims of this paper.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions data/task, clarifying the main claims of the paper related to loss objectives."
This seems to contradict findings from Brown et al where larger models did better on essentially all tasks.,Does the review address Result?,TRUE,FALSE,"The review mentions result, contrasting the findings of the paper with previous work."
"2) The proposed method outperforms other SOTA models in offline and interactive online settings, MultiWOZ and ConvLab respectively.",Does the review address Result?,TRUE,FALSE,"The review mentions result, praising the method for outperforming other models in specific settings."
"Also, I think putting the english translation in a different font or color would be greatly helpful to our eyes.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, suggesting improved readability through formatting changes."
I understand that LMU might have limited capacity but this is not specifically discussed and it is unclear how each component contributes to the end performance.,Does the review address Contribution?,TRUE,FALSE,"The review mentions Contribution, questioning the contribution of individual components to the overall performance."
This work has the potential to set a precedent in the fusion of quantum computing with graph transformers.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, discussing the potential of the work in a new research direction."
"Reasons for Score ----------------- The idea proposed in the paper is novel and exciting, but I have some concerns about whether the gains promised by the theoretical analysis can be realized while maintaining modeling quality.",Does the review address Theory?,TRUE,FALSE,"The review mentions Theory, expressing concern about the practical realization of theoretical claims."
- The experimental analysis focuses exclusively on known computer vision datasets that do not differ from the training distribution of CLIP.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, critiquing the use of well-known datasets that may not generalize to new settings."
"* ""By achieving the best, these results prove that BROS"" this sentence can be improved.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, suggesting improvement in the clarity of a specific sentence."
"2) The proposed method outperforms other SOTA models in offline and interactive online settings, MultiWOZ and ConvLab respectively.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, summarizing the findings from experimental results in various settings."
"However, only one member (alpha=0.5) from the family has been evaluated in the experiments, and it does not achieve the best performance in most experiments.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, pointing out the limitations of the experimental evaluation focusing on only one configuration."
3) The ablation study and visualization analysis of the experimental results are sufficient.,Does the review address Analysis?,TRUE,FALSE,"The review mentions Analysis, appreciating the thoroughness of the ablation study and visualization."
"According to my understanding, at least there should be some direct connections between the parameters in the encoder and decoder.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, suggesting that there should be more explicit connections between encoder and decoder."
However in experiment only 300 projects are involved.,Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, questioning the scale of the experiment with only 300 projects."
"### Major issue 1 The paper ""**theoretically**"" analyzes the hyper-parameter selection process in Section 3.1 and provides experimental validation in Section 5.1.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, describing the theoretical and experimental approach used for hyperparameter selection."
"They show improved total performance in MultiWoz dataset compared to recent, relevant baselines.",Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, highlighting the improved performance compared to baselines on MultiWoz."
"The experments are fairly convincing, although it is not entirely surprising that this approach works, and to repeat, the basic idea of extracting additional training data in this way is not entirely new.",Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, acknowledging the convincing results while noting that the idea is not novel."
What is the result if we directly learn to match input and logic forms?,Does the review address Result?,TRUE,FALSE,"The review mentions Result, querying the outcome of learning to match input and logic forms directly."
"Based on the large-scale training data and the proposed visual expert module, the proposed method achieves a number of state-of-the-art results across a wide range of vision-language tasks.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, highlighting the approach's effectiveness on vision-language tasks using large-scale data."
"The main limitation to me is that, the two novel pre-training tasks proposed in this paper are specific for Wikipedia and they are less effective than the ICT strategy (as shown in Table 5).",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, discussing the limitations of pre-training tasks specific to Wikipedia."
This is very promising to simplify the construction of highly tuned task-specific neural pipelines and improve the multimodal and multi-task problems.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, expressing optimism about simplifying task-specific pipelines and improving multimodal problems."
Clarification on the task setting: Is it the case that the agent's current utterance does not decide what the next user utterance is?,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, seeking clarification on task settings in the experiment."
"- Well Structured Experiments sections with 4 RQs and results that confirm each of the hypotheses  - Reproducibility and Transparency in reporting of experiments in terms of available source code, dataset information, details about human evaluation, generation examples.",Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, praising the structured and transparent presentation of experimental results."
"(3) In the experiment about linguistic similarity, it appears that the capacity schedule is the same across languages and the authors conclude from this that the schedule has little to do with linguistic characteristics.",Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, critiquing the experiment on linguistic similarity and the conclusion drawn."
"(In particular, pre-training on emergent language performs on average better than synthetic hierarchical data, but not quite as well as a different natural language, and all of these pre-training methods do better than training from scratch.)",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, comparing the performance of different pre-training approaches."
"(2) Specifically, in the encoder side, the M-BERT model is leveraged to jointly encode the $x,tx,ty$.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, explaining the use of M-BERT in encoding the data in the encoder."
**Limited technical novelty and incremental empirical gains**.,Does the review address Novelty?,TRUE,FALSE,"The review mentions Novelty, noting the paper's limited technical novelty and incremental gains."
Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?,Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, discussing the method of learning vectors and applying weighted sums in the approach."
"Altogether, I think this paper makes an interesting contribution to the question of: How can we get the most pretraining signal from unstructured data using off-the-shelf tools?",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, highlighting the contribution to improving pretraining with off-the-shelf tools."
"In practice, the authors use nouns and verbs as their concepts, which is fine in terms of pretraining objectives, but surely does not capture the generality of concepts.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, critiquing the limited choice of concepts (nouns and verbs) in the pretraining objectives."
Could you explain the significance of this result again?,Does the review address Result?,TRUE,FALSE,"The review mentions Result, requesting further explanation of the significance of a result."
"For checking the generalization of the method and better comparison w/ InDIGO (though InDIGO also conducted on MSCOCO, Django and the current comparison is sufficiently fair), I would like to increase my rating if seeing more experiments on large scale machine translation benchmarks as those in InDIGO.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, suggesting additional experiments for a better comparison with InDIGO."
*  The evaluation focuses on comparing with an empirical law learned on a different experimental configuration and there is a concern about how comparable are the results to the ones obtained in this study and the validity of the conclusions.,Does the review address Evaluation?,TRUE,FALSE,"The review mentions Evaluation, questioning the validity and comparability of the results with an empirical law."
CALM shows better results with less data than the base model.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, noting the improved performance of CALM with less data compared to the base model."
"It provided me with a much more thoughtful explanation for why language model pre-training improves downstream task performance, beyond simply “it helps learn good general representations of language using large amounts of unlabeled text data” (my previous reasoning).",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, appreciating the deeper explanation of language model pre-training's impact on downstream tasks."
The paper is well-written and the idea is well-motivated.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The review mentions Intuition/Justification/Motivation/Validation, praising the clarity and motivation of the paper's ideas."
Strengths: - The algorithms presented here are relatively straightforward but surprisingly effective.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, appreciating the simplicity and effectiveness of the proposed algorithms."
"Although the authors identify three potential challenges -- rapid convergence, the risk of under-training, and the potential for sparse attention -- they do not adequately explain how RandomMask addresses or mitigates these issues.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, critiquing the lack of explanation on how RandomMask addresses key challenges."
"If so, does the global node $V_g$ connect to all the sub fact nodes?",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions Definition/Description/Detail/Discussion/Explanation/Interpretation, seeking clarification on model details."
"Overall this paper tackles a nice application of GNN, which is the type prediction problem that utilizes structural information of the code.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, highlighting the use of GNN in the type prediction problem."
"- Strengths: This paper has high originality, proposing a fundamentally different way of predicting words from a vocabulary that is more efficient than a softmax layer and has comparable performance on NMT.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, discussing the novelty and efficiency of the proposed approach in word prediction for NMT."
"They show improved total performance in MultiWoz dataset compared to recent, relevant baselines.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, highlighting the improved performance on MultiWoz dataset."
"Your work is so similar to much of this work that you should really cite and establish novelty wrt at least some of them as early as the introduction -- that's how early I was wondering how your work differed, and it was not made clear.",Does the review address Novelty?,TRUE,FALSE,"The review mentions Novelty, pointing out the need to establish novelty and differentiate the work in the introduction."
The experiments are conducted on datasets from questions answering and sentiment analysis.,Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, listing the datasets used in the experiments."
"The paper claims that Balance Beam is ""an workflow to optimize the trade-off between latency and throughput performance of LLMs"".",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, critiquing the claim made about the Balance Beam framework."
3) The ablation study and visualization analysis of the experimental results are sufficient.,Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, appreciating the ablation study and visualization analysis."
"**Weaknesses**:   (1) Even though it is the first time such a method is applied in the context of NMT, the idea is not as much novel in the broader context of deep learning.",Does the review address Novelty?,TRUE,FALSE,"The review mentions Novelty, highlighting the lack of novelty within the broader deep learning field."
The current paper only indicates that a small gap gives more consistency between the true objective and the optimized objective defined on the training set: they can be still far away from the expected posterior over data distribution.,Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, pointing out the gap between the true and optimized objectives."
"However, only one member (alpha=0.5) from the family has been evaluated in the experiments, and it does not achieve the best performance in most experiments.",Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, criticizing the narrow evaluation of only one member from the family."
The experiments are conducted on datasets from questions answering and sentiment analysis.,Does the review address Analysis?,TRUE,FALSE,"The review mentions Analysis, confirming the use of Q&A and sentiment analysis datasets in the experiments."
It is better to compare with more demonstration selection methods such as similarity-based and diversity-based methods which are widely used in practice.,Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, suggesting better comparison methods for more comprehensive analysis."
"- The experiments contain two setups, one is offline response evaluation via MultiWOZ, and another is interactive simulation via ConvLab.",Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, describing the two experimental setups."
"Related Work: Contrastive learning - Under an unsupervised setting, ontrastive -> contrastive  Overall:  This work highlights the importance of incorporating contrastive training for data augmentation.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, noting the relevance of contrastive learning in data augmentation."
Presumably this is because much of the training iteration time is consumed by other parts of the network.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, speculating on why training iteration time is consumed by other parts of the network."
One of the important motivations of multi-modal multi-task learning mentioned was to achieve better or on-par performance with a single model (and supposedly fewer computations) which is crucial for devices with limited computing resources.,Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, discussing the motivation for multi-task learning in resource-limited devices."
This is to verify and support the usage of proposed type dependency graph.,Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, discussing the verification of the proposed type dependency graph."
"The two datasets have some “toy flavor”, while SCAN great favors example combination (with recomb-2 performs much better than recomb-1), recomb-1 seem to perform better for morphological analysis dataset, leaving questions about how to choose the exact models in general.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, critiquing the dataset choice and model selection."
"For instance, they ask how different amounts of data influence model behavior or if their data sampling method can react to task difficulty.",Does the review address Evaluation?,TRUE,FALSE,"The review mentions Evaluation, discussing how different data amounts influence model behavior."
"In experiments, mainly accuracy is shown, but since the major motivation to reduce gradient variance, why not show some comparison of gradient variance of MLM and MLM-FE?",Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, suggesting a comparison of gradient variance for more detailed analysis."
"For example, by selecting the most complex examples, the performance of ChatGPT (i.e., gpt-3.5-turbo) can easily achieve more than 80% accuracy (without self-consistency) compared to the number 77.1% in Table 1.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, discussing the potential performance improvement in ChatGPT with specific example selection."
"Without the experimental results using the same training settings, I do not think the authors are able to prove the superiority of OTTER over CLIP fully.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, questioning the comparison between OTTER and CLIP without using the same training settings."
Questions:  * How can we be sure that the specific power law derived from a different experimental configuration in Kaplan et al. (2020) corresponds to the empirical law that can be derived for transformers in this particular evaluation setting?,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, questioning the applicability of a power law derived from different experimental settings."
"They show that, especially in small-data regimes, pre-training on an emergent language can yield significant performance boosts in both tasks.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, discussing the performance boost of emergent language pre-training in small-data regimes."
Cons: Some of the claims are not quite accurate even when compared to the works already cited here - 1.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, pointing out inaccuracies in claims when compared to other cited works."
"It is hard to tell what are the standalone contributions of the paper, and what is coming from other works.",Does the review address Contribution?,TRUE,FALSE,"The review mentions Contribution, critiquing the clarity of standalone contributions in the paper."
"Furthermore, when there is a gap between the empirical results and the theoretical results (e.g., validation of Lemma 4.3 at the end of Section 4), the paper makes these limitations clear, which I appreciated very much as a reader (paper does not over-claim its contributions).",Does the review address Theory?,TRUE,FALSE,"The review mentions Theory, appreciating the transparency about limitations between empirical and theoretical results."
"The authors should make it clear that on different evaluation sets, the scores differs.",Does the review address Evaluation?,TRUE,FALSE,"The review mentions Evaluation, advising clarity on score variations across different evaluation sets."
"Terms such as θ, t, δ, and especially the adjacency matrix A, which are crucial for understanding the method, require clear definitions and contextual usage within the proposed quantum framework.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, suggesting the need for clear definitions and context for specific terms."
"Therefore, I think it is very necessary to supplement this experiment.",Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, recommending supplementary experiments to enhance the study."
And they show this technique improves accuracy in downstream tasks.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, highlighting the improvement in accuracy for downstream tasks."
The problem is a terrific one and the application of the recursive models seems like a contribution to this problem.,Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, appreciating the application of recursive models to the problem."
Enhancing the paper with the aforementioned suggestions could substantially improve its impact and reception by the research community.,Does the review address Significance?,TRUE,FALSE,"The review mentions Significance, suggesting that the paper's impact could be improved with the suggested enhancements."
* I think there should be more discussion about the implications of Proposition 2.2.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions Definition/Description/Detail/Discussion/Explanation/Interpretation, recommending more discussion on Proposition 2.2."
- The proof technique (pre-training performance $\to$ covariance of pre-training errors $\to$ covariance of downstream errors $\to$ downstream performance) is itself interesting.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, praising the proof technique used in the study."
"Of course, it is valuable to see the great performance achieved by single-task finetuning for RoBERTa.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, recognizing the performance of single-task fine-tuning for RoBERTa."
"The authors use objectives which capture both generative and discriminative information, which some have suggested contain mutually beneficial signal but have not been unified in a single training method.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, appreciating the unified generative and discriminative approach."
- The original option framework assumes given options.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, discussing assumptions made in the original option framework."
"335: consider defining GPGPU Table 3: Highlight the best BLEU scores in bold Equation 15: remind the reader that q is defined in equation 6 and b is a function of w. I was confused by this at first because w and h appear on the LHS but don't appear on the right, and I didn't know what b and q were.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions Definition/Description/Detail/Discussion/Explanation/Interpretation, suggesting clarification on terms and equations."
The authors take time to implement and evaluate several prominent baselines.,Does the review address Evaluation?,TRUE,FALSE,"The review mentions Evaluation, appreciating the evaluation of multiple baselines."
"> However, in every task except CommonGEN the authors do not discuss any methods that are even close to the state of the art.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, critiquing the lack of state-of-the-art methods discussed."
This paper suggestsan intermediate training regime that can be used between pretraining and the end-task finetuning.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, highlighting the intermediate training regime introduced in the paper."
"* Why does the EC pre-training use |V| = 4035, as opposed to 50 that's used in the other tasks and the fine-tuning corpora?",Does the review address Methodology?,TRUE,FALSE,"The methodology shows that |v|= 4035, as opposed to 50 that's used in the other tasks and the fine-tuning corpora?"
"The in-depth experimental analysis of the BERT pretraining process in this paper answers many open questions (e.g., the usefulness of NSP objective) and also provide some guidance in how to effectively tweak the performance of pretrained model (e.g., large batch size).",Does the review address Result?,TRUE,FALSE,"The review mentions Result, recognizing the experimental analysis and insights provided."
"While the authors say that they use vocabulary permutation, it is not clear what the size of the vocabulary is.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, pointing out the lack of clarity in the vocabulary size."
Were there perhaps some poor datasets that happened to be in the held-in split (since the held-out tasks don't seem to have the same trend)?,Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, questioning dataset quality in the held-in split and its impact on the results."
"In this way, the conclusion is only supported by the empirical observations but not the presented theoretical analysis.",Does the review address Analysis?,TRUE,FALSE,"The review mentions Analysis, noting a disconnect between theoretical analysis and empirical observations."
"To alleviate the difficulty of optimizing discrete latent variables, the authors propose to cast it as a one-step Markov Decision problem and optimize it using the policy gradient.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, explaining the proposal of using policy gradients for optimization."
(2) Equation 2 and 3 use the same graph encoder $F_{G}$.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, discussing the use of the same graph encoder across equations."
Weaknesses:   - The empirical study is not convincing by only evaluating BERT-Tiny.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, critiquing the empirical study's reliance on a single model."
* The rationale behind the architectural choices for the self-attention component is not well explained or empirically verified.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions Definition/Description/Detail/Discussion/Explanation/Interpretation, noting a lack of explanation for the self-attention architecture."
The result presented in Table 4 don't match the description in Section 4.3:  - It's not true that the pRNN outperforms both PBSMT and Enc-Dec model.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions Definition/Description/Detail/Discussion/Explanation/Interpretation, pointing out a mismatch between results and description."
It will be good to rewrite highlighting the contributions.,Does the review address Contribution?,TRUE,FALSE,"The review mentions Contribution, suggesting a rewrite to better highlight the contributions of the paper."
"Since the selective annotation is based entirely on similarities derived from sentence embeddings, there is nothing explicit ensuring that the label distribution over the selected subset is not skewed.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, discussing potential issues with the label distribution in selective annotation."
The authors suggest that their method captures more commonsense knowledge by being focused on capturing knowledge about “concepts”.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, noting the focus on commonsense knowledge in the proposed method."
"The result section cannot be simply presenting a table without explanation:  - Still on the result sections, although it's clear that BLEU and perplexity are objective automatic measure to evaluate the new architecture.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions Definition/Description/Detail/Discussion/Explanation/Interpretation, critiquing the lack of explanation for result metrics."
"Without the ablation studies on these two aspects, we cannot determine whether the performance improvement truly comes from the author's contribution or just longer CoT annotations.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, suggesting the need for ablation studies to confirm the source of performance improvements."
Summary: + Appealing theoretical contributions + Empirical results are encouraging + The use of discriminator for reward shaping in addition to task success rate is interesting  - Writing and explanation can be improved.,Does the review address Contribution?,TRUE,FALSE,"The review mentions Contribution, summarizing the paper's contributions and areas of improvement."
"In particular, you are comparing a small GRU LM to a larger transformer LM, where the latter is, as you mention, a much more powerful model.",Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, discussing the comparison between GRU LM and transformer LM."
"Other concerns along these lines: all of this paper's results are averaged over 3 runs while the other baselines are over 5 runs - an indication of variance would be useful to assess whether the differences are significant, especially since some of the margins quite small (23 on MC-LAVE-RL vs 22.8 for the next best on Ludicorp) added to the fact that hyperparameters are different for each game - does that imply that the authors tuned the hyperparameters for each game?",Does the review address Significance?,TRUE,FALSE,"The review mentions Significance, questioning the statistical significance of results given the variance in runs and hyperparameters."
"The result section cannot be simply presenting a table without explanation:  - Still on the result sections, although it's clear that BLEU and perplexity are objective automatic measure to evaluate the new architecture.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, noting the lack of explanation accompanying result tables and objective measures."
"Rules may be ""outdated"" because they are inefficient for certain languages with reams of available data and scads of phenomena that don't fit.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, suggesting that rules may be inefficient for some languages in certain contexts."
The methodology is explained clearly and experiments are executed with a considerable amount of detail.,Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, praising the clarity and detail in methodology and experiments."
"Does the definition of ""event word""s here come from any particular previous work that motivates it?",Does the review address Related Work?,TRUE,FALSE,"The review mentions Related Work, questioning whether the definition of ""event words"" is grounded in prior work."
"Then, taking inspiration from recent work that shows that many downstream tasks can be reframed as sentence completion tasks, it defines a “natural task” as one on which a sparse linear model over the output of the “true” language model (next word probability distribution, conditioned on context) attains strong performance.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, discussing the reframing of downstream tasks as sentence completion tasks."
Are these rather the ppl resulting from training an LM on the full dataset?,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, questioning the nature of the perplexity values (ppl) in the study."
"To me saying ""BLUE reduced for the other methods"" means that you have some other baseline you are comparing to.",Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, noting that the comparison to a baseline is implied by the BLUE reduction."
"However, they compare to BERT models and build themselves on RoBERTa_base; how are the results meaningful if they use a stronger model to start with?",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, critiquing the use of a stronger base model (RoBERTa) in the comparisons."
"**Related work** The authors clearly describe the related prior works from both the perspectives of program synthesis, large language models, and benchmarks for program synthesis.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, appreciating the clear description of related work in the paper."
"It is hard to infer the test gains, given the possibly significant hyperparameter optimization on the dev set.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, questioning the inference of test gains given potential hyperparameter tuning."
It is great to have a theoretical analysis of the property of the influence function.,Does the review address Analysis?,TRUE,FALSE,"The review mentions Analysis, appreciating the theoretical analysis of the influence function."
"However, there are still major gaps between the theoretical analysis, the conclusion and the empirical solution (please see the detailed comments).",Does the review address Theory?,TRUE,FALSE,"The review mentions Theory, pointing out gaps between the theoretical analysis and conclusions."
- It is unclear why the zero-shot prediction of CLIP is not used as a baseline (as it would be equivalent to epsilon = 0).,Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, questioning the exclusion of a zero-shot prediction baseline (CLIP)."
"Compared to the related work, the novel part of this work is: (i) a new retrieval way, which is not quite clear and convincing to me.",Does the review address Novelty?,TRUE,FALSE,"The review mentions Novelty, expressing a lack of clarity and conviction regarding the new retrieval method proposed."
This is an interesting topic that can spur further research and help predictability and understand the LLM's behaviours in general.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, highlighting the potential of the topic to inspire further research and contribute to understanding LLM behavior."
The proposed model explored to utilize better context information and captured the relation between video and sentence/word via graph neural networks.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, discussing the use of graph neural networks to improve context information representation."
The annotations in baseline [1] are much shorter compared to the annotations by the authors.,Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, comparing the annotation lengths between the baseline and the proposed method."
#### Strength - The idea of perceiver IO is novel and solid -- a general architecture capable of handling general-purpose inputs and outputs across different tasks and modalities.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, emphasizing the novelty and strength of the perceiver IO architecture."
It handles the hallucination problem of LLM by training close-ended dataset and then non-answerable question-answer pairs.,Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, discussing the dataset handling strategy to address hallucination in language models."
(2) Another weakness is that the comparison with the vanilla and LS baselines does not seem to be properly controlled in terms of parameters.,Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, critiquing the baseline comparison in terms of parameter control."
Their thorough ablation experiments and other analysis yield some interesting findings the authors could emphasize more.,Does the review address Analysis?,TRUE,FALSE,"The review mentions Analysis, suggesting that the ablation experiments provide interesting findings that should be emphasized."
"On the other hand, one could argue that N-Bref is novel because it combines a number of existing components in a unique way to achieve better performance that prior work.",Does the review address Novelty?,TRUE,FALSE,"The review mentions Novelty, discussing how the N-Bref method combines existing components in a novel way to achieve improved performance."
The margin of change seems even larger than some results which are discussed in the paper as significant.,Does the review address Significance?,TRUE,FALSE,"The review mentions Significance, pointing out that the change margin seems significant in comparison to results discussed as significant in the paper."
"It is also strange that the multi-cluster approach, which discards inter-cluster (word and language) semantic information performs the best with respect to the extrinsic metrics.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, questioning the efficacy of the multi-cluster approach and its performance on extrinsic metrics."
"you explained this in page 6, in Task Description.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, noting that the explanation of the task description was already provided in the paper."
Is this strategy guaranteed optimal theoretically?,Does the review address Theory?,TRUE,FALSE,"The review mentions Theory, asking about the theoretical guarantees of the strategy proposed."
"It is not the case that I find the experimental results inadequate, rather, the experiments run in the first place are generic and do not illustrate the points of interest with a continuous-channel referential game.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, critiquing the experiments as being too generic and not illustrating the key points effectively."
"The authors cite Bordes et al. (https://arxiv.org/pdf/1506.02075.pdf), who collect a similar dataset and perform relation extraction using memory networks (which are commonly used for reading comprehension).",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, discussing the related work cited and the use of memory networks in relation extraction."
Impact: The labeling requirement was a huge bottleneck.,Does the review address Significance?,TRUE,FALSE,"The review mentions Significance, identifying the labeling requirement as a significant bottleneck in the study."
"On the one hand the baselines make use of a large set of training trajectories, but on the other hand these methods train policies that choose low-level actions.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, contrasting the use of large training trajectories with low-level action selection in the baselines."
Or an experiment could address if using the phonotactics from a natural language results is more effective than using a randomly selected phonemes due to necessity for natural languages to have acoustically distinct words.,Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, suggesting an experiment to compare phonotactics from natural languages versus randomly selected phonemes."
My understanding is that contribution of the paper is in exploring using options framework to goal-oriented dialog to handle the issue in question.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, summarizing the contribution as using an options framework for goal-oriented dialogue."
"Are there any overheads/disadvantages because of multi-task learning (Like a larger model size, inference time for individual tasks etc)?",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, raising concerns about the potential overheads of multi-task learning such as model size and inference time."
More of an issue is that the fact that the definition of general learner in Section 2.2 is not clearly structured.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions Definition/Description/Detail/Discussion/Explanation/Interpretation, pointing out that the definition of general learner is unclear."
"The main contribution are the following innovations: a special attention mechanism called block-diagonal conditional attention, a set of modules for adaptation of a pretrained model, and an uncertainty-based multi-task data sampling method.",Does the review address Contribution?,TRUE,FALSE,"The review mentions Contribution, summarizing the paper's contributions in terms of innovation in attention mechanisms, model adaptation, and multi-task data sampling."
"If this secondary goal is valid, the paper requires sufficient reasoning for why the presented approach is superior.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, questioning the validity and reasoning behind the secondary goal and its superiority."
"It provided me with a much more thoughtful explanation for why language model pre-training improves downstream task performance, beyond simply “it helps learn good general representations of language using large amounts of unlabeled text data” (my previous reasoning).",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions Definition/Description/Detail/Discussion/Explanation/Interpretation, praising the explanation of why language model pre-training improves downstream performance."
Strength: - A novel architecture to enable deeper interaction between LM and GCN.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, highlighting the novelty of the architecture enabling deeper interaction between language models and graph convolutional networks (GCN)."
**Strengths** - The paper is generally quite clearly written and the claims are well-validated.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The review mentions Intuition/Justification/Motivation/Validation, praising the clarity of the paper and the validation of claims."
The research shows how low-level proof artifact data may be used to significantly boost performance on high-level theorem proving by co-training auxiliary tasks.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, explaining how low-level proof artifact data boosts performance in high-level theorem proving."
Clarity on Quantum Enhancements:  Section 3.2.2 seems to lack depth in the explanation of how the quantum correlations are calculated and utilized within the graph transformers.,Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, noting a lack of clarity in explaining the quantum enhancements in the context of graph transformers."
"Mainly, Table 4 provides understandable results showing that multi-turn specifications achieve better performance compared to single-turn specifications.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, discussing the results in Table 4 that show multi-turn specifications outperform single-turn ones."
"Yet it seems inconsistent at times whether this is a formal definition of the concept or a necessary condition obtained from some other (unprovided) definition, as Proposition 1 suggests.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions Definition/Description/Detail/Discussion/Explanation/Interpretation, pointing out inconsistencies in the formal definition of the concept."
The new established benchmark is another good contribution.,Does the review address Contribution?,TRUE,FALSE,"The review mentions Contribution, acknowledging the newly established benchmark as an important contribution."
More discussion is required on a) what are the reasons of loss in comprehensibility in this case (it is briefly mentioned in the intro) b) why their individual design choices and how they handles the different reasons c) some evaluation to verify this 2.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions Definition/Description/Detail/Discussion/Explanation/Interpretation, recommending further discussion on comprehensibility loss and design choices."
"Moreover, the authors better answer questions in 2 so I can gauge if their hyper-parameters were chosen in the principled ways.",Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, suggesting more clarification on the hyperparameter choices in the experiment."
Perhaps the authors can consider adding them to setup a more comprehensive benchmark.,Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, recommending that the authors consider adding additional data for a more comprehensive benchmark."
The same applies to Table 3: it is unclear to me why or how the baseline T5 model has been chosen.,Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, questioning the choice of the T5 model as a baseline in Table 3."
The description of how you compare your invariants to those inferred by Daikon is not clear unless all relevant cases related to (pre)conditions on method parameters.,Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, pointing out the lack of clarity in how invariants are compared to Daikon."
Can PACT be applied to simpler theorem provers like MetaMath where there are no tactics?,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, asking if PACT can be applied to simpler theorem provers."
"This work tries a single pruning rate and finds it significantly degrades, rather than improves results: for both active learning and random selection.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, discussing how the pruning rate negatively impacts results in the experiments."
"According to the description, the fact units are constructed using the dependency parser.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, explaining the construction of fact units using a dependency parser."
"For a fair comparison, I think the baseline should add those methods as claimed in the introduction (Lee et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Beltagy et al., 2020, inter alia), (Kitaev et al., 2020; Wang et al., 2020; Roy et al., 2020, inter alia) and let us know how badly they performed under the short sequence.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, suggesting additional baseline methods for a fair comparison."
"- Extensive ablation studies that showcase specific abilities of the model (types of OOD generalization), investigate the role of the foundation models and study the importance of the components of the prompt.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, advocating for extensive ablation studies to explore specific model abilities and components."
The proposed method is reasonable and moderately novel.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, offering a reasonable and moderately novel assessment of the proposed method."
It seems like more quantitative analysis would be needed to determine how much the LM's attention is correlating empirically to factual knowledge or if there are other factors that are affecting the downstream improvements.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, recommending more quantitative analysis to explore the correlation between the LM's attention and factual knowledge."
"* Even though this paper proposes a new efficient transformer, the evaluation does not focus on computational efficiency aspects and comes across as incomplete.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, criticizing the lack of focus on computational efficiency in the evaluation."
I would encourage the authors to do this comparison to show how LTU could be a more generic model which can be super useful for users to interact with an audio language model through natural text.,Does the review address Significance?,TRUE,FALSE,"The review mentions Significance, recommending a comparison to demonstrate the utility of LTU for interacting with audio language models through natural text."
"The only advantage of using two is an increase of model capacity, i.e. Furthermore, what are the hyper-parameters / size of the baseline neural networks?",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, questioning the advantage of using two models and requesting details on the baseline's hyperparameters and size."
"Compared with commonsense and entity-relation knowledge, the fact units are more informative to each specific question.",Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, stating that fact units provide more specific information compared to commonsense and entity-relation knowledge."
"-----Weaknesses----- I'm not very convinced by the empirical results, mostly due to the lack of details of the baselines.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, expressing doubt about the empirical results due to insufficient baseline details."
It paves the way for more widespread application of VIP in scenarios where interpretable-by-design approaches are critical.,Does the review address Significance?,TRUE,FALSE,"The review mentions Significance, highlighting the broader application potential of VIP in interpretable-by-design approaches."
"Footnote 2 -- ""the tensor version"" - needs citation to explain what's being referred to.",Does the review address Related Work?,TRUE,FALSE,"The review mentions Related Work, recommending a citation to explain the ""tensor version"" mentioned in Footnote 2."
My main problem with this paper at the moment is that it could be much better written: The overall narrative of the paper is unpolished and it’s hard at first to understand the contributions of the paper.,Does the review address Contribution?,TRUE,FALSE,"The review mentions Contribution, criticizing the clarity and polish of the paper's narrative and the contributions presented."
"There is no comparison with other public masking methods in Table 2, such as whole-word-masking, span masking, etc.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, noting the lack of comparison with other public masking methods in Table 2."
"It requires more analysis about experimental results, such as Figure 1 and tables in Section D.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, recommending additional analysis of experimental results in Figure 1 and Section D."
"I look forward to seeing the authors discuss a comprehensive comparison of DeFo's training time and other methods, such as CoOp and CLIP-adapter.",Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, suggesting a comparison of training times between DeFo and other methods like CoOp and CLIP-adapter."
"Second, the authors neither 1) evaluate their model on another dataset or 2) evaluate any previously published models on their dataset.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, highlighting the lack of evaluation on other datasets and the failure to compare with previously published models."
"Strengths: * Novel method of using emergent language for pre-training (as opposed to transferring an entire artificial agent) * Some good ablations to identify what contributes to successful transfer * A new evaluation metric (emergent --> NL translation performance) that best correlates with fine-tuning performance  Weaknesses: * Some parameter choices and the design of some ablations are not completely justified * Some additional related works could be included   # Minor comments / questions  * ""However, this metric is too rigid in its definition of compositionality, ignoring aspects like argument structure, context or morphology which play a key role in determining the combination of word semantics (Goldberg, 2015).""",Does the review address Evaluation?,TRUE,FALSE,"The review mentions Evaluation, discussing the novel evaluation metric and critiquing its rigid definition of compositionality."
"The paper proposes a novel task, text-to-audio storytelling.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, recognizing the novel task of text-to-audio storytelling."
"Given a sentence $x$ to be translated, they first retrieve a $(tx, ty)$ sentence pair from the training set through ``SEGMENT-BASED TM RETRIEVAL’’ defined in Section 4.1, where $tx$ and $ty$ are from source and target languages respectively.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, explaining the sentence pair retrieval method for translation."
"The authors provide OTTER using hard labels (InfoNCE) as a baseline, but ZSL methods are sensitive to hyper-parameters and training sets.",Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, noting the use of hard labels as a baseline and pointing out the sensitivity of ZSL methods to hyperparameters and training sets."
"Theoretically, it shows that language models which are close to the “true” language model are guaranteed to attain strong performance on natural tasks.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, explaining how language models close to the ""true"" language model perform well on natural tasks."
It is not clear how the proposed method considers the correlations among the retrieved data points.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, questioning how the proposed method accounts for correlations between retrieved data points."
"Generally speaking, this paper puts forward a universal retrieval scheme, and achieves good results, the specific advantages are as follows:  (1) The sparse alignment of multi-vector retrieval is a good solution to solve the retrieval effect and efficiency.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, highlighting the advantages of the proposed retrieval scheme."
* There are some points in the paper that could be made clearer.,Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, suggesting areas where clarity could be improved in the paper."
"Regarding the proof, the notations in Section 3 are quite loose, especially for Section 3.1.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, criticizing the loose notation in Section 3, particularly in Section 3.1."
"Unfortunately, as the paper overall does not seem to contain significant novelty, neither in methodology nor in results, I cannot recommend acceptance at this point.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, stating that the paper lacks novelty in methodology and results, leading to the rejection recommendation."
"## Logical Flow of Main Results and Relation to Prior Work Should be Clarified  The paper defines T-LLMs as general learners if ""the expressive power of realistic T-LLM model class can cover universal circuits for all circuits of polynomial size"" and says that:  > It is unknown whether the realistic Transformers are expressive enough to be general learners   In fact, it seems that Theorem 3 essentially follows from the TC0 upper bound in previous work.",Does the review address Related Work?,TRUE,FALSE,"The review mentions Related Work, suggesting that Theorem 3 follows from existing work and should be better connected to prior research."
"Given that there is a wealth of existing work that performs the same task and the lack of novelty of this work, the authors need to include experiments that demonstrate that their technique outperforms others on this task, or otherwise show that their dataset is superior to others (e.g. since it is much larger than previous, does it allow for better generalization?)",Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, recommending that the authors demonstrate how their technique outperforms others or show the superiority of their dataset."
Reducing it to 80% seems to be a sweet point with the best balance between performance and efficiency.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, discussing the balance between performance and efficiency achieved by reducing to 80%."
"They show that, especially in small-data regimes, pre-training on an emergent language can yield significant performance boosts in both tasks.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, explaining how pre-training on an emergent language boosts performance in small-data regimes."
"- The authors have evaluated their models on multiple settings, proving that their models trained on Wikipedia can potentially generalize to multiple downstream tasks.",Does the review address Evaluation?,TRUE,FALSE,"The review mentions Evaluation, noting that the authors evaluated their models across multiple settings to test generalization."
"**Technical contribution** Codegen for program synthesis seems effective, especially when the user wants to generate pieces of code from input/output examples or natural language descriptions.",Does the review address Contribution?,TRUE,FALSE,"The review mentions Contribution, highlighting the effectiveness of Codegen in program synthesis for generating code from natural language descriptions."
The related work section does not connect this paper to specific prior work (only citing two survey papers).,Does the review address Related Work?,TRUE,FALSE,"The review mentions Related Work, criticizing the lack of connection to specific prior work and reliance on only two survey papers."
One may also refer to https://opencompass.org.cn/leaderboard-llm for the performance of LLMs (I acknowledge that the performance of ChatGPT on GSM8K from that website is possibly still underestimated).,Does the review address Related Work?,TRUE,FALSE,"The review mentions Related Work, pointing to additional resources on LLM performance for further context."
"With these two elements, the approach performs on par with recently proposed GECA (where the data is not augmented via a neural generator) on two datasets.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, comparing the approach's performance with GECA across two datasets."
This paper focuses on improving the dialogue policy together with the responses by utilizing a pre-trained language model and offline RL.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, summarizing the paper's focus on improving dialogue policy and responses using a pre-trained language model and offline RL."
See section-2.6 of this tutorial for more details about using neural models to rank: https://www.microsoft.com/en-us/research/uploads/prod/2017/06/INR-061-Mitra-neuralir-intro.pdf.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, suggesting a resource for more details on neural models used for ranking."
The authors also seem to have carefully designed their experiments.,Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, affirming that the authors have designed their experiments carefully."
Questions & Comments: • It is stated that the performance is sensitive to epsilon in AdamW.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, pointing out the sensitivity of performance to epsilon in AdamW."
"Also, can you add a column where a dense linear model over p_f(s) is used?",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, suggesting adding a column for dense linear model analysis."
"It considers the training direction to be ""first to perceive, and then comprehend the sound"" so that the training starts from using close-ended datasets to open-ended datasets.",Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, describing the approach to training using close-ended to open-ended datasets."
"Intuitively, this parameter arises from translating the ""natural"" task assumption, which only guarantees transfer on average to the downstream task.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The review mentions Intuition/Justification/Motivation/Validation, explaining the reasoning behind a particular parameter."
"-----Post-rebuttal----- The authors did not address my main concern, which is whether the baselines (e.g. TreeRNN) are used to compute substructure embeddings independent of the sentence embedding and the joint tagger.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, raising a concern about how baselines like TreeRNN are used in computing substructure embeddings."
"As, deep recurrent neural networks are already used in keyphrase extraction (shows very good performance also), so, it will be interesting to have a proper motivation to justify the use of  RNN and Copy RNN over deep recurrent neural networks.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The review mentions Intuition/Justification/Motivation/Validation, requesting a justification for using RNN and Copy RNN over existing deep recurrent neural networks."
"However, there is no discussion on the latency in the proposed method and experiments.",Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, criticizing the lack of discussion about latency in the proposed method and experiments."
(Current experiments only include the results of models that are free from this issue.),Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, stating that the current experiments exclude results from models affected by the issue."
An additional experimental results with multi-task finetuning should also be added.,Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, suggesting the inclusion of multi-task finetuning results."
"Beyond this, the authors conduct a multifaceted set of tests, including a non-harmful test to ensure that the watermark embedding does not significantly degrade model performance, robustness tests against second-time fine-tuning and model quantization, and an ablation study concerning the reference set to further substantiate the rationality of their backdoor data framework design.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, emphasizing the authors' thorough set of tests, including robustness and ablation studies."
The second major concern I have with this paper is the small dataset they are using.,Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, expressing concern about the use of a small dataset."
- Extensive results show improvements over a base model and a larger model across a range of tasks.,Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, discussing how extensive results show improvements over base and larger models across tasks."
Perhaps the biggest reason is I can’t seem to understand what is novel about the system.,Does the review address Novelty?,TRUE,FALSE,"The review mentions Novelty, indicating that the reviewer cannot identify the novelty in the system."
"I thought that this paper was very thought-provoking, and I appreciated the attempts to better understand what is going on with pre-trained language models, why they work well, and what might we be able to improve from theses insights.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, praising the paper's thought-provoking analysis of pre-trained language models and their improvements."
The paper then does a good job of showing how using one-hot encodings compared to SCS change the problem definition leading to a difference in performance on the same task.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, noting the difference in performance based on the use of one-hot encodings versus SCS."
The idea of analyzing the gap to variational posterior lower bound for different dropout inference model is interesting.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, finding the analysis of the gap to the variational posterior lower bound interesting."
"We observe empirically that if pruned globally, the attention heads in some layers may be completely removed, making the network un-trainable.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, observing the effect of global pruning on attention heads and network trainability."
It is therefore not surprising that multi-task learning should help these tasks.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, noting that multi-task learning is expected to help the tasks."
"Specifically, is there any feed-forward computation involved and how many layers of the models used in comparison.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, asking about the feed-forward computation and model layers involved in the comparison."
"From what I can tell, Appendix C on prompt tuning (which is very interesting) is maybe the primary evidence the instructions are important.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, noting that Appendix C on prompt tuning provides important evidence."
"While it is also true that corpus transfer _enables_ this divergence in model types, to really test whether transferring the whole model versus using the corpus works or not, I would think you would want to compare fine-tuning the sender GRU on the LM data vs. starting from scratch _with a GRU of the same type_ and pre-training on the EC corpus before fine-tuning.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, suggesting a comparison between fine-tuning a GRU on LM data and starting from scratch with a GRU pre-trained on the EC corpus."
"They train low resource NMTs using 4 different tokenization strategies, to show that their proposed tokenization method leads to the best NMT results by several metrics.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, noting that training NMTs with different tokenization strategies shows that the proposed method yields the best results."
- Many interesting robustness capabilities of VL models only emerge when trained with hundreds of millions of samples.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, pointing out that robustness in VL models is only seen when trained on large datasets."
An ablation analysis would be most appropriate for quantifying this.,Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, recommending the use of an ablation analysis to quantify the robustness and results."
"I.e., Figure 1 should be your results table, and figure 2 should be the examples for us to see.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, suggesting the reorganization of figures for clearer presentation of results and examples."
The show improved performance in MultiWoz dataset.,Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, highlighting improved performance in the MultiWoz dataset."
"As shown in Figure 6, when more than 12 facts are constructed, the performance becomes worse.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, discussing the drop in performance when more than 12 facts are used, as shown in Figure 6."
"For example, you explained why ADJ is not used in your mapping, but ADJ does appear in this UD treebank.",Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, comparing the absence of ADJ in the paper’s mapping to its appearance in a UD treebank."
"However, there are insufficient experiments and comparison to previous work to convince me that the paper’s contributions are novel and impactful.",Does the review address Significance?,TRUE,FALSE,"The review mentions Significance, stating that the lack of experiments and comparisons to prior work diminishes the novelty and impact of the paper."
This paper is an interesting application of a data augmentation or self-supervised learning type of approach for tactic based theorem proving.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, describing the use of data augmentation or self-supervised learning for tactic-based theorem proving as an interesting application."
The method can select appropriate batch sizes by assessing the working memory requirements per token during benchmarking.,Does the review address Data/Task?,TRUE,FALSE,"The review mentions Data/Task, explaining how the method assesses memory requirements per token to determine batch sizes during benchmarking."
The question about whether realistic transformers can represent any polynomial-time-computable function is interesting.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, noting the interesting question regarding whether transformers can represent polynomial-time computable functions."
"For fine-tuning, the authors run their model for 2.2 epochs, while their baseline model runs for 3 epochs, roughly 30% more which accounts for much of the reduction observed in Table 2.",Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, highlighting the difference in training duration between the model and the baseline, which may explain the observed performance reduction."
"The authors start with some good motivation for building more intimate interaction between vision and language, but it finally becomes the emphasis of the benefit of scaling up.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,"The review mentions Intuition/Justification/Motivation/Validation, discussing how the motivation to integrate vision and language eventually emphasizes the benefit of scaling up."
"* page 3: s/""It has also been previous observed""/""It has also been previously observed""",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, pointing out a typo on page 3 that should be corrected."
Another major concern is the use of two separate RNNs which gives the proposed model more parameters than the baselines.,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, raising concerns about using two separate RNNs, increasing the number of parameters in the model compared to baselines."
Detailed comments: - There already is a small UD treebank for Thai: <https://github.com/UniversalDependencies/UD_Thai-PUD/>   How does your mapped tagset compare to the annotation scheme chosen there?,Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, suggesting that the authors compare their mapped tagset with the annotation scheme used in the Thai UD treebank."
"There is a lack of experimental analysis supporting the source of the observed improvements, which is crucial for substantiating the paper's main claims.",Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, noting the lack of experimental analysis supporting the source of improvements in the paper’s claims."
"The paper only performs some finetuning on GLUE tasks, which is significantly less interesting given that it is comparatively cheap and FP8 speedups thus not so crucial while, in many cases, even more affordable finetuning techniques like QLoRA also work well.",Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, criticizing the paper for performing relatively cheap finetuning on GLUE tasks and not considering more advanced or cheaper techniques like QLoRA."
"The closest baseline is simply a comparison against a CLIP(image, concept) similarity score.",Does the review address Comparison?,TRUE,FALSE,"The review mentions Comparison, pointing out that the closest baseline used in the paper is a comparison against a CLIP similarity score."
"Where the paper does get technical is in a discussion of the differing difficulties of speech recognition for different languages, providing a useful case study to demonstrate that one-size technology approaches are not necessarily universal stand-alone solutions.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, discussing the technical details about speech recognition for different languages and the limitations of one-size-fits-all approaches."
"The experments are fairly convincing, although it is not entirely surprising that this approach works, and to repeat, the basic idea of extracting additional training data in this way is not entirely new.",Does the review address Novelty?,TRUE,FALSE,"The review mentions Novelty, acknowledging the approach works but noting that extracting additional training data is not a new idea."
"It would be useful if following the suggestions of [2,3] the authors could present results in settings with low data regimes and with significant distribution shift with respect to the pre-training set, which represents a more realistic application setting.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, suggesting that results should be presented in low-data and distribution-shift settings to better reflect realistic applications."
The results demonstrated in the experiments show an improvement over previous models.,Does the review address Result?,TRUE,FALSE,"The review mentions Result, confirming that the results show improvements over previous models."
"See Rives et al, 2020, ‘Biological structures and functions emerge…’, section 5.2, and Vig et al, 2020, ‘Bertology’ section 4.2.",Does the review address Related Work?,TRUE,FALSE,"The review mentions Related Work, referencing related works by Rives et al., and Vig et al., as relevant prior research."
"Thus, the experiment could answer questions such as:     - Does compositionality emerge at the same rate in continuous- and discrete-channel games?",Does the review address Experiment?,TRUE,FALSE,"The review mentions Experiment, suggesting experiments to answer specific questions about compositionality in different game setups."
Is it possible to import a public SOTA implementation and conduct comparisons based on that?,Does the review address Methodology?,TRUE,FALSE,"The review mentions Methodology, suggesting the use of a public SOTA implementation for comparison."
The proposed method also extends this to multilingual evaluations.,Does the review address Evaluation?,TRUE,FALSE,"The review mentions Evaluation, noting that the proposed method includes multilingual evaluations."
Ablation studies show that the model achieves good performance on more complex questions.,Does the review address Ablation?,TRUE,FALSE,"The review mentions Ablation, noting that ablation studies show good performance on more complex questions."
"Theoretically, it shows that language models which are close to the “true” language model are guaranteed to attain strong performance on natural tasks.",Does the review address Result?,TRUE,FALSE,"The review mentions Result, highlighting the theoretical guarantee that language models close to the ""true"" language model perform well on natural tasks."
"In general, the paper is well written and describes the work clearly.",Does the review address Presentation?,TRUE,FALSE,"The review mentions Presentation, noting the clarity and quality of the paper’s writing."
L680-683: This needs more examples or explanation of what it means to judge the polarity of a peak.,Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,"The review mentions Definition/Description/Detail/Discussion/Explanation/Interpretation, requesting more explanation or examples for the peak polarity judgement."
"So, In this case, it will be interesting to see the results (or will be helpful in evaluating ""absent type"" keyphrases): if we identify all the topical phrases of the entire corpus by using tf-idf and relate the document to the high-ranked extracted topical phrases (by using Normalized Google Distance, PMI, etc.).",Does the review address Evaluation?,TRUE,FALSE,Mentions that evaluating absent-type keyphrases by relating documents to high-ranked extracted topical phrases would be helpful.
"For example, Figure 4 validates Assumption 4.1 (log-partition function is roughly quadratic in theta), and Table 1 shows many real tasks are approximately “natural”.",Does the review address Methodology?,TRUE,FALSE,Refers to how Figure 4 and Table 1 validate assumptions about the log-partition function and task characteristics.
"I would like to have seen qualitative examples of model predictions, and more examples from the dataset.",Does the review address Data/Task?,TRUE,FALSE,Indicates the need for more qualitative examples of model predictions and additional dataset examples for better clarity.
"For example, if for the baseline model, we also only use one data sample and apply different masks, will there be improvement?",Does the review address Methodology?,TRUE,FALSE,Suggests experimenting with applying different masks to the baseline model using a single data sample.
- Performance with relatively little finetuning data are encouraging.,Does the review address Result?,TRUE,FALSE,Notes that performance with minimal finetuning data is encouraging.
"It's important to discuss why these improvements are non-trivial and how they advance the field, considering the rapidly evolving landscape of both quantum computing and graph analysis.",Does the review address Definition/Description/Detail/Discussion/Explanation/Interpretation?,TRUE,FALSE,Stresses the importance of discussing why the improvements are non-trivial and how they advance the field in the context of rapidly evolving quantum computing and graph analysis.
It seems completely unnecessary to me to have separate weights for the two RNNs.,Does the review address Methodology?,TRUE,FALSE,Argues that it is unnecessary to use separate weights for the two RNNs in the proposed model.
The authors should conduct experiments beyond English-to-French.,Does the review address Experiment?,TRUE,FALSE,Suggests extending experiments beyond just English-to-French language pairs.
Is Theorem 2 a stronger version in some sense because it applies for polylog input size?,Does the review address Theory?,TRUE,FALSE,Questions whether Theorem 2 is stronger due to its application to polylog input sizes.
Theorem 2 is presented with no intuition and the proof in the appendix is only for a special case.,Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,Points out the lack of intuition behind Theorem 2 and the limited scope of its proof.
- The ablation study would be better if specific numbers were provided.,Does the review address Ablation?,TRUE,FALSE,Suggests that the ablation study would be improved with the inclusion of specific numbers for clarity.
"Even though the results don’t show that the proposed loss function and proposed “conditional mean features” give improvements over baselines, the empirical results show that the basic assumptions and definitions in the theoretical analysis are relatively realistic.",Does the review address Analysis?,TRUE,FALSE,"Acknowledges that the empirical results show realistic assumptions and definitions, even if the proposed loss function and features don't outperform baselines."
"By converting the logic forms to natural languages, the authors can leverage paraphrase datasets and pre-train the critic as a paraphrase model.",Does the review address Data/Task?,TRUE,FALSE,Suggests using paraphrase datasets to pre-train the critic for improved performance.
"Nonetheless, the current paper leaves too many open questions regarding the validity of the experiments.",Does the review address Intuition/Justification/Motivation/Validation?,TRUE,FALSE,Expresses concerns about the validity of the experiments and the need for further justification.
"There are various weaknesses of the MID data, but the evaluation approach needs to be discussed or justified.",Does the review address Data/Task?,TRUE,FALSE,Points out weaknesses in the MID data and the need to discuss or justify the evaluation approach.
"While it is also true that corpus transfer _enables_ this divergence in model types, to really test whether transferring the whole model versus using the corpus works or not, I would think you would want to compare fine-tuning the sender GRU on the LM data vs. starting from scratch _with a GRU of the same type_ and pre-training on the EC corpus before fine-tuning.",Does the review address Comparison?,TRUE,FALSE,Suggests comparing fine-tuning the sender GRU on the LM data with training from scratch with a GRU pre-trained on the EC corpus.
"This is not new (see Rives 2020 and Vig 2020) and does not fit well to the rest of the paper, which is about contact prediction.",Does the review address Novelty?,TRUE,FALSE,States that the method discussed is not new and doesn't align well with the focus of the paper on contact prediction.
I find the analysis presented in the paper very interesting and insightful - and distinguishes it from previous work in this area.,Does the review address Analysis?,TRUE,FALSE,"Praises the interesting and insightful analysis presented in the paper, noting how it distinguishes itself from previous work."
"It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines.",Does the review address Data/Task?,TRUE,FALSE,"Highlights the large training dataset used in the system, with a subset of 40,000 publications held out for baseline training."
Audio Instruction Generation (AIG) is also quite nice and interesting.,Does the review address Novelty?,TRUE,FALSE,Acknowledges the interesting and novel aspect of Audio Instruction Generation (AIG).
There are several such insights in the experiments section that will be helpful to the community.,Does the review address Experiment?,TRUE,FALSE,Notes the helpful insights presented in the experiments section.
"Questions:   - Previous work has tried to combine both language-specific and shared parameters (Wang et al 2018), rather than making a binary choice between these.",Does the review address Methodology?,TRUE,FALSE,"References previous work that combined both language-specific and shared parameters, contrasting it with the paper's approach."
"- Table 5 the VAE(32) method performs the best overall in ""Wiki section"" although the TC (16) method has been highlighted as the best.",Does the review address Presentation?,TRUE,FALSE,"Mentions that VAE(32) performs the best overall in the ""Wiki section,"" while TC(16) is highlighted as the best in Table 5."
