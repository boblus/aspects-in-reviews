{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700b0d7-4e75-419e-b0f2-4b2b7016fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c392e-38b0-42b0-aff3-1ba20678a6c2",
   "metadata": {},
   "source": [
    "# review comparison\n",
    "\n",
    "This corresponds to **Section 5.2 Review comparison** in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474ec55-daf2-4a95-9476-dee9969165b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d24497-4b67-4b8c-855e-e483ccc08273",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'jaccard'\n",
    "if metric == 'jaccard':\n",
    "    similarity_metric = jaccard_similarity\n",
    "if metric == 'dice':\n",
    "    similarity_metric = dice_coefficient\n",
    "if metric == 'overlap':\n",
    "    similarity_metric = overlap_coefficient\n",
    "if metric == 'kulczynski':\n",
    "    similarity_metric = kulczynski_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa685e6-d787-4cb3-9658-6ac48e54095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pd.read_csv('config-inference.txt', sep='\\t')\n",
    "run_id = config['run_id'][(config['type_of_labels'] == 'fine') & (config['source'] == 'llm_generated_reviews_iclr24_ours')].to_list()[0]\n",
    "source = config['source'][config['run_id'] == run_id].to_list()[0]\n",
    "number_of_data = config['number_of_data'][config['run_id'] == run_id].to_list()[0]\n",
    "type_of_labels = config['type_of_labels'][config['run_id'] == run_id].to_list()[0]\n",
    "\n",
    "with open(f'results/agr_detection-{run_id}.json') as file:\n",
    "    results = json.loads(file.read())\n",
    "\n",
    "review_aspects = defaultdict()\n",
    "for paper_id in list(results.keys()):\n",
    "    review_aspects[paper_id] = defaultdict(list)\n",
    "    for reviewer_id in results[paper_id]:\n",
    "        aspects = set()\n",
    "        for _, values in results[paper_id][reviewer_id].items():\n",
    "            aspects.update(values)\n",
    "        if '-' in aspects:\n",
    "            aspects.remove('-')\n",
    "        if 'O' in aspects:\n",
    "            aspects.remove('O')\n",
    "        review_aspects[paper_id][reviewer_id] = aspects\n",
    "\n",
    "similarities_inter = defaultdict()\n",
    "similarities_inter['human_review'] = defaultdict()\n",
    "for paper_id_i in review_aspects:\n",
    "    similarities_inter['human_review'][paper_id_i] = defaultdict(list)\n",
    "    for i in review_aspects[paper_id_i]:\n",
    "        if i != 'llm_review':\n",
    "            for paper_id_j in review_aspects:\n",
    "                for j in review_aspects[paper_id_j]:\n",
    "                    if j != 'llm_review':\n",
    "                        similarities_inter['human_review'][paper_id_i][i].append(similarity_metric(review_aspects[paper_id_i][i], review_aspects[paper_id_j][j]))\n",
    "\n",
    "similarities_inter['llm_review'] = defaultdict()\n",
    "for paper_id_i in review_aspects:\n",
    "    similarities_inter['llm_review'][paper_id_i] = defaultdict(list)\n",
    "    for i in review_aspects[paper_id_i]:\n",
    "        if i == 'llm_review':\n",
    "            for paper_id_j in review_aspects:\n",
    "                for j in review_aspects[paper_id_j]:\n",
    "                    if j == 'llm_review':\n",
    "                        similarities_inter['llm_review'][paper_id_i][i].append(similarity_metric(review_aspects[paper_id_i][i], review_aspects[paper_id_j][j]))\n",
    "\n",
    "\n",
    "plt.rcdefaults()\n",
    "plt.rcParams['font.size'] = 12\n",
    "fig = plt.figure(figsize=(5.6, 2))\n",
    "gs = fig.add_gridspec(1, 3, width_ratios=[1, 1, 0.05])\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "cbar_ax = fig.add_subplot(gs[0, 2])\n",
    "for ax, review_type in zip([ax1, ax2], ['human_review', 'llm_review']):\n",
    "\n",
    "    similarities, temp = [], []\n",
    "    for paper_id in similarities_inter[review_type]:\n",
    "        for reviewer_id, scores in similarities_inter[review_type][paper_id].items():\n",
    "            similarities.append(scores)\n",
    "            temp.append((paper_id, reviewer_id))\n",
    "    \n",
    "    df = pd.DataFrame(similarities, index=[i for i in range(len(similarities))])\n",
    "\n",
    "    if review_type == 'human_review':\n",
    "        sns.heatmap(df, cmap='coolwarm', vmin=0, vmax=1.0, cbar=False, ax=ax)\n",
    "    else:\n",
    "        sns.heatmap(df, cmap='coolwarm', vmin=0, vmax=1.0, cbar_ax=cbar_ax, cbar_kws={'ticks': [0, 0.5, 1.0]}, ax=ax)\n",
    "\n",
    "    if review_type == 'human_review':\n",
    "        ticks = [0, 160, 315]\n",
    "        if source == 'ReviewCritique':\n",
    "            ticks = [0, 38, 76]\n",
    "    else:\n",
    "        ticks = [i for i in range(0, len(similarities)+50, 50)]\n",
    "        if source == 'ReviewCritique':\n",
    "            ticks = [0, 10, 20]\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(ticks, rotation=0)\n",
    "    avg = round(df.values.mean(), 4)\n",
    "    ax.set_xlabel(f'average={avg:.4f}')\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticks(ticks)\n",
    "    if review_type == 'human_review':\n",
    "        ax.set_ylabel('paper')\n",
    "        ax.set_yticklabels(ticks, rotation=0)\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    \n",
    "    ax.set_title({'human_review': 'human-written', 'llm_review': 'LLM-generated'}[review_type], fontweight='bold')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "pos = cbar_ax.get_position()\n",
    "cbar_ax.set_position([pos.x0-0.025, pos.y0, pos.width, pos.height])\n",
    "plt.savefig(f'plots/heatmap-comparison-{source}-{type_of_labels}.png', format='png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741186f-2c4e-46b9-879c-40d4e1cfe7ee",
   "metadata": {},
   "source": [
    "# LLM-generated review detection\n",
    "\n",
    "This corresponds to **Section 5.3 LLM-generated review detection** in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614d8d1-71bc-4eae-86df-4047a0d1f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'jaccard'\n",
    "if metric == 'jaccard':\n",
    "    similarity_metric = jaccard_similarity\n",
    "if metric == 'dice':\n",
    "    similarity_metric = dice_coefficient\n",
    "if metric == 'overlap':\n",
    "    similarity_metric = overlap_coefficient\n",
    "if metric == 'kulczynski':\n",
    "    similarity_metric = kulczynski_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0332b-7b31-402f-a74d-8d6d40534727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random baseline\n",
    "sources = ['ReviewCritique']\n",
    "random_baselines_1, random_baselines_2 = defaultdict(list), defaultdict(list)\n",
    "for source in sources:\n",
    "    for seed in [2266,105,86379]:\n",
    "        random.seed(seed)\n",
    "        with open(f'preprocessed/preprocessed-{source}.json') as file:\n",
    "            data = json.loads(file.read())\n",
    "    \n",
    "        detection = []\n",
    "        for paper_id in data:\n",
    "            if len(data[paper_id].keys()) >= 2:\n",
    "                detection.append(random.sample(list(data[paper_id].keys()), 2))\n",
    "            else:\n",
    "                detection.append(random.sample(list(data[paper_id].keys()), 1))\n",
    "        \n",
    "        correct_at_1, correct_at_2 = 0, 0\n",
    "        for item in detection:\n",
    "            if item[0] == 'llm_review':\n",
    "                correct_at_1 += 1\n",
    "            if 'llm_review' in item:\n",
    "                correct_at_2 += 1\n",
    "        \n",
    "        correctness_at_1 = correct_at_1 / len(detection)\n",
    "        correctness_at_2 = correct_at_2 / len(detection)\n",
    "\n",
    "        random_baselines_1[source].append(correctness_at_1)\n",
    "        random_baselines_2[source].append(correctness_at_2)\n",
    "\n",
    "for source in sources:\n",
    "    random_baselines_1[source] = sum(random_baselines_1[source]) / len(random_baselines_1[source])\n",
    "    random_baselines_2[source] = sum(random_baselines_2[source]) / len(random_baselines_2[source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14487718-3b20-4111-8403-08ffed7f2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baselines_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74c098-f5e1-4087-bb51-0afa134d3d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pd.read_csv('config-inference.txt', sep='\\t')\n",
    "for run_id in config['run_id'].to_list():\n",
    "\n",
    "    source = config['source'][config['run_id'] == run_id].to_list()[0]\n",
    "    number_of_data = config['number_of_data'][config['run_id'] == run_id].to_list()[0]\n",
    "    type_of_labels = config['type_of_labels'][config['run_id'] == run_id].to_list()[0]\n",
    "\n",
    "    with open(f'results/agr_detection-{run_id}.json') as file:\n",
    "        results = json.loads(file.read())\n",
    "\n",
    "    review_aspects = defaultdict()\n",
    "    for paper_id in list(results.keys()):\n",
    "        review_aspects[paper_id] = defaultdict(list)\n",
    "        for reviewer_id in results[paper_id]:\n",
    "            aspects = set()\n",
    "            for _, values in results[paper_id][reviewer_id].items():\n",
    "                aspects.update(values)\n",
    "            if '-' in aspects:\n",
    "                aspects.remove('-')\n",
    "            if 'O' in aspects:\n",
    "                aspects.remove('O')\n",
    "            review_aspects[paper_id][reviewer_id] = aspects\n",
    "    \n",
    "    similarities_intra = defaultdict()\n",
    "    for paper_id in review_aspects:\n",
    "        similarities_intra[paper_id] = defaultdict(list)\n",
    "        for i in review_aspects[paper_id]:\n",
    "            for j in review_aspects[paper_id]:\n",
    "                if j != i:\n",
    "                    similarities_intra[paper_id][i].append(similarity_metric(review_aspects[paper_id][i], review_aspects[paper_id][j]))\n",
    "            if len(similarities_intra[paper_id][i]) == 0:\n",
    "                similarities_intra[paper_id][i] = 0\n",
    "            else:\n",
    "                similarities_intra[paper_id][i] = sum(similarities_intra[paper_id][i]) / len(similarities_intra[paper_id][i])\n",
    "    \n",
    "    similarities_inter = defaultdict()\n",
    "    for paper_id_i in review_aspects:\n",
    "        similarities_inter[paper_id_i] = defaultdict(list)\n",
    "        for i in review_aspects[paper_id_i]:\n",
    "            for paper_id_j in review_aspects:\n",
    "                if paper_id_j != paper_id_i:\n",
    "                    for j in review_aspects[paper_id_j]:\n",
    "                        similarities_inter[paper_id_i][i].append(similarity_metric(review_aspects[paper_id_i][i], review_aspects[paper_id_j][j]))\n",
    "            similarities_inter[paper_id_i][i] = sum(similarities_inter[paper_id_i][i]) / len(similarities_inter[paper_id_i][i])\n",
    "    \n",
    "    detection = defaultdict()\n",
    "    for paper_id in similarities_intra:\n",
    "        detection[paper_id] = defaultdict()\n",
    "        for reviewer_id in similarities_intra[paper_id]:\n",
    "            diff = similarities_intra[paper_id][reviewer_id] - similarities_inter[paper_id][reviewer_id]\n",
    "            detection[paper_id][reviewer_id] = diff\n",
    "        detection[paper_id] = dict(sorted(detection[paper_id].items(), key=lambda x: x[1]))\n",
    "    \n",
    "    correct_at_1, correct_at_2 = 0, 0\n",
    "    for paper_id, item in detection.items():\n",
    "        if len(detection[paper_id]) != 0:\n",
    "            if sorted(detection[paper_id].items(), key=lambda x: x[1])[0][0] == 'llm_review':\n",
    "                correct_at_1 += 1\n",
    "            if sorted(detection[paper_id].items(), key=lambda x: x[1])[0][0] == 'llm_review' or sorted(detection[paper_id].items(), key=lambda x: x[1])[1][0] == 'llm_review':\n",
    "                correct_at_2 += 1\n",
    "    \n",
    "    correctness_at_1 = correct_at_1 / len(detection)\n",
    "    correctness_at_2 = correct_at_2 / len(detection)\n",
    "    \n",
    "    with open('evaluation_scores-llm_generated_review_detection.txt', 'a') as file:\n",
    "        file.write(f'{run_id}\\t{source}\\t{number_of_data}\\t{type_of_labels}\\t{metric}\\t{correctness_at_1}\\t{correctness_at_2}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
